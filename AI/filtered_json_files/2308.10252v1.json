{
  "title": "LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models",
  "authors": [
    "Yixuan Weng",
    "Zhiqi Wang",
    "Huanxuan Liao",
    "Shizhu He",
    "Shengping Liu",
    "Kang Liu",
    "Jun Zhao"
  ],
  "abstract": "\n With the burgeoning development in the realm of large language models (LLMs), the demand for efficient incremental training tailored to specific industries and domains continues to increase. Currently, the predominantly employed frameworks lack modular design, it often takes a lot of coding work to kickstart the training of LLM. To address this, we present \"LMTuner\", a highly usable, integrable, and scalable system for training LLMs expeditiously and with minimal user-input. LMTuner comprises three main modules -the Interaction, Training, and Inference Modules. We advocate that LMTuner's usability and integrality alleviate the complexities in training large language models. Remarkably, even a novice user could commence training large language models within five minutes. Furthermore, it integrates DeepSpeed frameworks and supports Efficient Fine-Tuning methodologies like Low Rank Adaptation (LoRA), Quantized LoRA (QLoRA), etc., enabling the training of language models scaling from 300M to a whopping 130B parameters using a single server. The LMTuner's homepage 1 and screencast video 2 are now publicly available. \n",
  "references": [
    {
      "id": null,
      "title": "LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models",
      "authors": [
        "Yixuan Weng",
        "Zhiqi Wang",
        "Huanxuan Liao",
        "Shizhu He",
        "Shengping Liu",
        "Kang Liu",
        "Jun Zhao"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata",
      "authors": [
        "Sid Black",
        "Gao Leo",
        "Phil Wang",
        "Connor Leahy",
        "Stella Biderman"
      ],
      "year": "2021",
      "venue": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata",
      "doi": "10.5281/zenodo.5297715"
    },
    {
      "id": "b1",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Large language models as tool makers",
      "authors": [
        "Tianle Cai",
        "Xuezhi Wang",
        "Tengyu Ma",
        "Xinyun Chen",
        "Denny Zhou"
      ],
      "year": "2023",
      "venue": "Large language models as tool makers",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "",
      "authors": [
        "Arno Candel",
        "Jon Mckinney",
        "Philipp Singer",
        "Pascal Pfeiffer",
        "Maximilian Jeblick",
        "Prithvi Prabhu",
        "Jeff Gambera",
        "Mark Landry",
        "Shivam Bansal",
        "Ryan Chesler",
        "Chun",
        "Ming Lee",
        "Marcos V Conde",
        "Pasha Stetsenko",
        "Olivier Grellier",
        "Srisatish Ambati"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "",
      "authors": [
        "Mark Chen",
        "Jerry Tworek",
        "Heewoo Jun",
        "Qiming Yuan",
        "Henrique Ponde De Oliveira Pinto",
        "Jared Kaplan",
        "Harrison Edwards",
        "Yuri Burda",
        "Nicholas Joseph",
        "Greg Brockman",
        "Alex Ray",
        "Raul Puri",
        "Gretchen Krueger",
        "Michael Petrov",
        "Heidy Khlaaf",
        "Girish Sastry",
        "Pamela Mishkin",
        "Brooke Chan",
        "Scott Gray",
        "Nick Ryder",
        "Mikhail Pavlov",
        "Alethea Power",
        "Lukasz Kaiser",
        "Mohammad Bavarian",
        "Clemens Winter",
        "Philippe Tillet",
        "Felipe Petroski Such",
        "Dave Cummings",
        "Matthias Plappert",
        "Fotios Chantzis",
        "Elizabeth Barnes",
        "Ariel Herbert-Voss",
        "William Hebgen Guss",
        "Alex Nichol",
        "Alex Paino",
        "Nikolas Tezak",
        "Jie Tang",
        "Igor Babuschkin",
        "Suchir Balaji",
        "Shantanu Jain",
        "William Saunders",
        "Christopher Hesse",
        "Andrew N Carr",
        "Jan Leike",
        "Joshua Achiam",
        "Vedant Misra",
        "Evan Morikawa",
        "Alec Radford",
        "Matthew Knight",
        "Miles Brundage",
        "Mira Murati",
        "Katie Mayer",
        "Peter Welinder",
        "Bob Mcgrew",
        "Dario Amodei",
        "Sam Mccandlish"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "2023a. Extending context window of large language models via positional interpolation",
      "authors": [
        "Shouyuan Chen",
        "Sherman Wong",
        "Liangjian Chen",
        "Yuandong Tian"
      ],
      "year": "",
      "venue": "2023a. Extending context window of large language models via positional interpolation",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Symbolic discovery of optimization algorithms",
      "authors": [
        "Xiangning Chen",
        "Chen Liang",
        "Da Huang",
        "Esteban Real",
        "Kaiyuan Wang",
        "Yao Liu",
        "Hieu Pham",
        "Xuanyi Dong",
        "Thang Luong",
        "Cho-Jui Hsieh",
        "Yifeng Lu",
        "V Quoc",
        "Le"
      ],
      "year": "2023",
      "venue": "Symbolic discovery of optimization algorithms",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Dissecting transformer length extrapolation via the lens of receptive field analysis",
      "authors": [
        "Ta-Chung Chi",
        "Alexander I Ting-Han Fan",
        "Peter J Rudnicky",
        "Ramadge"
      ],
      "year": "2023",
      "venue": "Dissecting transformer length extrapolation via the lens of receptive field analysis",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma",
        "Albert Webson",
        "Shane Shixiang",
        "Zhuyun Gu",
        "Mirac Dai",
        "Xinyun Suzgun",
        "Aakanksha Chen",
        "Alex Chowdhery",
        "Marie Castro-Ros",
        "Kevin Pellat",
        "Dasha Robinson",
        "Sharan Valter",
        "Gaurav Narang",
        "Adams Mishra",
        "Vincent Yu",
        "Yanping Zhao",
        "Andrew Huang",
        "Hongkun Dai",
        "Slav Yu",
        "Ed H Petrov",
        "Jeff Chi",
        "Jacob Dean",
        "Adam Devlin",
        "Denny Roberts",
        "Quoc V Zhou",
        "Jason Le",
        "Wei"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Chatlaw: Open-source legal large language model with integrated external knowledge bases",
      "authors": [
        "Jiaxi Cui",
        "Zongjian Li",
        "Yang Yan",
        "Bohua Chen",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Chatlaw: Open-source legal large language model with integrated external knowledge bases",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness",
      "authors": [
        "Tri Dao",
        "Daniel Y Fu",
        "Stefano Ermon",
        "Atri Rudra",
        "Christopher Ré"
      ],
      "year": "2022",
      "venue": "Flashattention: Fast and memory-efficient exact attention with io-awareness",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "-bit matrix multiplication for transformers at scale",
      "authors": [
        "Tim Dettmers",
        "Mike Lewis",
        "Younes Belkada",
        "Luke Zettlemoyer"
      ],
      "year": "2022",
      "venue": "Llm.int",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Qlora: Efficient finetuning of quantized llms",
      "authors": [
        "Tim Dettmers",
        "Artidoro Pagnoni",
        "Ari Holtzman",
        "Luke Zettlemoyer"
      ],
      "year": "2023",
      "venue": "Qlora: Efficient finetuning of quantized llms",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Lamini: The llm engine for rapidly customizing models",
      "authors": [
        "Greg Diamos",
        "Sharon Zhou",
        "Samee Ibraheem",
        "Arman Daniel"
      ],
      "year": "2023",
      "venue": "Lamini: The llm engine for rapidly customizing models",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "A survey of quantization methods for efficient neural network inference",
      "authors": [
        "Amir Gholami",
        "Sehoon Kim",
        "Zhen Dong",
        "Zhewei Yao",
        "Michael W Mahoney",
        "Kurt Keutzer"
      ],
      "year": "2021",
      "venue": "A survey of quantization methods for efficient neural network inference",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "",
      "authors": [
        "Suriya Gunasekar",
        "Yi Zhang",
        "Jyoti Aneja",
        "Caio César",
        "Teodoro Mendes",
        "Allie Del Giorno",
        "Sivakanth Gopi",
        "Mojan Javaheripi",
        "Piero Kauffmann"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "OpenDelta: A plug-and-play library for parameterefficient adaptation of pre-trained models",
      "authors": [
        "Shengding Hu",
        "Ning Ding",
        "Weilin Zhao",
        "Xingtai Lv",
        "Zhen Zhang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "A phd student's perspective on",
      "authors": [
        "Oana Ignat",
        "Zhijing Jin",
        "Artem Abzaliev",
        "Laura Biester",
        "Santiago Castro",
        "Naihao Deng",
        "Xinyi Gao",
        "Aylin Gunal",
        "Jacky He",
        "Ashkan Kazemi",
        "Muhammad Khalifa",
        "Namho Koh",
        "Andrew Lee",
        "Siyang Liu",
        "June Do",
        "Shinka Min",
        "Joan Mori",
        "Veronica Nwatu",
        "Siqi Perez-Rosas",
        "Zekun Shen",
        "Winston Wang",
        "Rada Wu",
        "Mihalcea"
      ],
      "year": "2023",
      "venue": "A phd student's perspective on",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2017",
      "venue": "Adam: A method for stochastic optimization",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Do emergent abilities exist in quantized large language models: An empirical study",
      "authors": [
        "Peiyu Liu",
        "Zikang Liu",
        "Ze-Feng Gao",
        "Dawei Gao",
        "Wayne Xin Zhao",
        "Yaliang Li",
        "Bolin Ding",
        "Ji-Rong Wen"
      ],
      "year": "2023",
      "venue": "Do emergent abilities exist in quantized large language models: An empirical study",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Came: Confidenceguided adaptive memory efficient optimization",
      "authors": [
        "Yang Luo",
        "Xiaozhe Ren",
        "Zangwei Zheng",
        "Zhuo Jiang",
        "Xin Jiang",
        "Yang You"
      ],
      "year": "2023",
      "venue": "Came: Confidenceguided adaptive memory efficient optimization",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Qi jie Gao, Qipeng Guo, and Xipeng Qiu. 2023. Full parameter finetuning for large language models with limited resources",
      "authors": [
        "Kai Lv",
        "Yuqing Yang",
        "Tengxiao Liu"
      ],
      "year": "",
      "venue": "Qi jie Gao, Qipeng Guo, and Xipeng Qiu. 2023. Full parameter finetuning for large language models with limited resources",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Peft: Stateof-the-art parameter-efficient fine-tuning methods",
      "authors": [
        "Sourab Mangrulkar",
        "Sylvain Gugger"
      ],
      "year": "2022",
      "venue": "Lysandre Debut, Younes Belkada, and Sayak Paul",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "fairseq: A fast, extensible toolkit for sequence modeling",
      "authors": [
        "Myle Ott",
        "Sergey Edunov",
        "Alexei Baevski",
        "Angela Fan",
        "Sam Gross",
        "Nathan Ng",
        "David Grangier",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)",
      "doi": "10.18653/v1/N19-4009"
    },
    {
      "id": "b27",
      "title": "",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "year": "2022",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "2022b. Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Gray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "year": "",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeff Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "Language models are unsupervised multitask learners",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Crest-copilot for real-world experimental scientist",
      "authors": [
        "Zhen Zhichu Ren",
        "Yunsheng Zhang",
        "Ju Tian",
        "Li"
      ],
      "year": "2023",
      "venue": "Crest-copilot for real-world experimental scientist",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Bloom: A 176bparameter open-access multilingual language model",
      "authors": [
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilić",
        "Roman Hesslow",
        "Alexandra Castagné",
        "François Sasha Luccioni",
        "Matthias Yvon",
        "Gallé"
      ],
      "year": "2022",
      "venue": "Bloom: A 176bparameter open-access multilingual language model",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Fast transformer decoding: One write-head is all you need",
      "authors": [
        "Noam Shazeer"
      ],
      "year": "2019",
      "venue": "Fast transformer decoding: One write-head is all you need",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
      "authors": [
        "Mohammad Shoeybi",
        "Mostofa Patwary",
        "Raul Puri",
        "Patrick Legresley",
        "Jared Casper",
        "Bryan Catanzaro"
      ],
      "year": "2020",
      "venue": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Roformer: Enhanced transformer with rotary position embedding",
      "authors": [
        "Jianlin Su",
        "Yu Lu",
        "Shengfeng Pan",
        "Ahmed Murtadha",
        "Bo Wen",
        "Yunfeng Liu"
      ],
      "year": "2022",
      "venue": "Roformer: Enhanced transformer with rotary position embedding",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "",
      "authors": [
        "Yutao Sun",
        "Li Dong",
        "Barun Patra",
        "Shuming Ma",
        "Shaohan Huang",
        "Alon Benhaim",
        "Vishrav Chaudhary",
        "Xia Song",
        "Furu Wei"
      ],
      "year": "2022",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Stanford alpaca: An instruction-following llama model",
      "authors": [
        "Rohan Taori",
        "Ishaan Gulrajani",
        "Tianyi Zhang",
        "Yann Dubois",
        "Xuechen Li",
        "Carlos Guestrin",
        "Percy Liang",
        "Tatsunori B Hashimoto"
      ],
      "year": "2023",
      "venue": "Stanford alpaca: An instruction-following llama model",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurelien Rodriguez",
        "Armand Joulin"
      ],
      "year": "",
      "venue": "Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Igor Molybog",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale",
        "Dan Bikel",
        "Lukas Blecher",
        "Cristian Canton Ferrer",
        "Moya Chen",
        "Guillem Cucurull",
        "David Esiobu",
        "Jude Fernandes",
        "Jeremy Fu",
        "Wenyin Fu",
        "Brian Fuller",
        "Cynthia Gao",
        "Vedanuj Goswami",
        "Naman Goyal",
        "Anthony Hartshorn",
        "Saghar Hosseini",
        "Rui Hou",
        "Hakan Inan",
        "Marcin Kardas",
        "Viktor Kerkez",
        "Madian Khabsa",
        "Isabel Kloumann",
        "Artem Korenev",
        "Punit Singh Koura",
        "Marie-Anne Lachaux",
        "Thibaut Lavril",
        "Jenya Lee",
        "Diana Liskovich",
        "Yinghai Lu",
        "Yuning Mao",
        "Xavier Martinet",
        "Todor Mihaylov",
        "Pushkar Mishra"
      ],
      "year": "",
      "venue": "Igor Molybog",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Focused transformer: Contrastive training for context scaling",
      "authors": [
        "Szymon Tworkowski",
        "Konrad Staniszewski",
        "Mikołaj Pacek",
        "Yuhuai Wu",
        "Henryk Michalewski",
        "Piotr Miłoś"
      ],
      "year": "2023",
      "venue": "Focused transformer: Contrastive training for context scaling",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Lei Yang, and Yuxiong He. 2023a. Zero++: Extremely efficient collective communication for giant model training",
      "authors": [
        "Guanhua Wang",
        "Heyang Qin",
        "Sam Ade Jacobs",
        "Connor Holmes",
        "Samyam Rajbhandari",
        "Olatunji Ruwase",
        "Feng Yan"
      ],
      "year": "",
      "venue": "Lei Yang, and Yuxiong He. 2023a. Zero++: Extremely efficient collective communication for giant model training",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Linxi Fan, and Anima Anandkumar. 2023b. Voyager: An open-ended embodied agent with large language models",
      "authors": [
        "Guanzhi Wang",
        "Yuqi Xie",
        "Yunfan Jiang",
        "Ajay Mandlekar",
        "Chaowei Xiao",
        "Yuke Zhu"
      ],
      "year": "",
      "venue": "Linxi Fan, and Anima Anandkumar. 2023b. Voyager: An open-ended embodied agent with large language models",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "",
      "authors": [
        "Zekun Wang",
        "Ge Zhang",
        "Kexin Yang",
        "Ning Shi",
        "Wangchunshu Zhou",
        "Shaochun Hao",
        "Guangzheng Xiong",
        "Yizhi Li",
        "Mong Yuan Sim",
        "Xiuying Chen",
        "Qingqing Zhu",
        "Zhenzhu Yang",
        "Adam Nik",
        "Qi Liu",
        "Chenghua Lin",
        "Shi Wang",
        "Ruibo Liu",
        "Wenhu Chen",
        "Ke Xu",
        "Dayiheng Liu",
        "Yike Guo"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Vincent Zhao",
        "Kelvin Guu",
        "Adams Wei Yu",
        "Brian Lester",
        "Nan Du",
        "Andrew M Dai",
        "Quoc V Le"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Emergent abilities of large language models",
      "authors": [
        "Jason Wei",
        "Yi Tay",
        "Rishi Bommasani",
        "Colin Raffel",
        "Barret Zoph",
        "Sebastian Borgeaud",
        "Dani Yogatama",
        "Maarten Bosma",
        "Denny Zhou",
        "Donald Metzler"
      ],
      "year": "2022",
      "venue": "Emergent abilities of large language models",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Chain of thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Ed Chi",
        "Quoc Le",
        "Denny Zhou"
      ],
      "year": "2022",
      "venue": "Chain of thought prompting elicits reasoning in large language models",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "2023a. Large language models need holistically thought in medical conversational qa",
      "authors": [
        "Yixuan Weng",
        "Bin Li",
        "Fei Xia",
        "Minjun Zhu",
        "Bing Sun",
        "Shizhu He",
        "Kang Liu",
        "Jun Zhao"
      ],
      "year": "",
      "venue": "2023a. Large language models need holistically thought in medical conversational qa",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Large language models are reasoners with self-verification",
      "authors": [
        "Yixuan Weng",
        "Minjun Zhu",
        "Shizhu He",
        "Kang Liu",
        "Jun Zhao"
      ],
      "year": "2022",
      "venue": "Large language models are reasoners with self-verification",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Neural comprehension: Language models with compiled neural networks",
      "authors": [
        "Yixuan Weng",
        "Minjun Zhu",
        "Fei Xia",
        "Bin Li",
        "Shizhu He",
        "Kang Liu",
        "Jun Zhao"
      ],
      "year": "2023",
      "venue": "Neural comprehension: Language models with compiled neural networks",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Remi Louf",
        "Morgan Funtowicz",
        "Joe Davison",
        "Sam Shleifer",
        "Clara Patrick Von Platen",
        "Yacine Ma",
        "Julien Jernite",
        "Canwen Plu",
        "Teven Xu",
        "Sylvain Le Scao",
        "Mariama Gugger",
        "Quentin Drame",
        "Alexander Lhoest",
        "Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/2020.emnlp-demos.6"
    },
    {
      "id": "b51",
      "title": "Training transformers with 4-bit integers",
      "authors": [
        "Haocheng Xi",
        "Changhao Li",
        "Jianfei Chen",
        "Jun Zhu"
      ],
      "year": "2023",
      "venue": "Training transformers with 4-bit integers",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "2022a. Med-ConQA: Medical conversational question answering system based on knowledge graphs",
      "authors": [
        "Fei Xia",
        "Bin Li",
        "Yixuan Weng",
        "Shizhu He",
        "Kang Liu",
        "Bin Sun",
        "Shutao Li",
        "Jun Zhao"
      ],
      "year": "",
      "venue": "Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "2022b. Medconqa: Medical conversational question answering system based on knowledge graphs",
      "authors": [
        "Fei Xia",
        "Bin Li",
        "Yixuan Weng",
        "Shizhu He",
        "Kang Liu",
        "Bin Sun",
        "Shutao Li",
        "Jun Zhao"
      ],
      "year": "",
      "venue": "Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
      "authors": [
        "Canwen Xu",
        "Daya Guo",
        "Nan Duan",
        "Julian Mcauley"
      ],
      "year": "2023",
      "venue": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "React: Synergizing reasoning and acting in language models",
      "authors": [
        "Shunyu Yao",
        "Jeffrey Zhao",
        "Dian Yu",
        "Nan Du",
        "Izhak Shafran",
        "Karthik Narasimhan",
        "Yuan Cao"
      ],
      "year": "2023",
      "venue": "React: Synergizing reasoning and acting in language models",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "authors": [
        "Aohan Zeng",
        "Xiao Liu",
        "Zhengxiao Du",
        "Zihan Wang",
        "Hanyu Lai",
        "Ming Ding",
        "Zhuoyi Yang",
        "Yifan Xu",
        "Wendi Zheng",
        "Xiao Xia"
      ],
      "year": "2022",
      "venue": "Glm-130b: An open bilingual pre-trained model",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Meddialog: Large-scale medical dialogue dataset",
      "authors": [
        "Guangtao Zeng",
        "Wenmian Yang",
        "Zeqian Ju",
        "Yue Yang",
        "Sicheng Wang",
        "Ruisi Zhang",
        "Meng Zhou",
        "Jiaqi Zeng",
        "Xiangyu Dong",
        "Ruoyu Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Opt: Open pre-trained transformer language models",
      "authors": [
        "Susan Zhang",
        "Stephen Roller",
        "Naman Goyal",
        "Mikel Artetxe",
        "Moya Chen",
        "Shuohui Chen",
        "Christopher Dewan",
        "Mona Diab",
        "Xian Li",
        "Xi Victoria Lin"
      ],
      "year": "2022",
      "venue": "Opt: Open pre-trained transformer language models",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Yifan Dong",
        "Chen Du",
        "Yushuo Yang",
        "Zhipeng Chen",
        "Jinhao Chen",
        "Ruiyang Jiang",
        "Yifan Ren",
        "Xinyu Li",
        "Zikang Tang",
        "Peiyu Liu",
        "Jian-Yun Liu",
        "Ji-Rong Nie",
        "Wen"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "2023b. Ten-centPretrain: A scalable and flexible toolkit for pretraining models of different modalities",
      "authors": [
        "Zhe Zhao",
        "Yudong Li",
        "Cheng Hou",
        "Jing Zhao",
        "Rong Tian",
        "Weijie Liu",
        "Yiren Chen",
        "Ningyuan Sun",
        "Haoyan Liu",
        "Weiquan Mao",
        "Han Guo",
        "Weigang Gou",
        "Taiqiang Wu",
        "Tao Zhu",
        "Wenhang Shi",
        "Chen Chen",
        "Shan Huang",
        "Sihong Chen",
        "Liqun Liu",
        "Feifei Li",
        "Xiaoshuai Chen",
        "Xingwu Sun",
        "Zhanhui Kang",
        "Xiaoyong Du",
        "Linlin Shen",
        "Kimmo Yan"
      ],
      "year": "",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Less is more for alignment",
      "authors": [
        "Chunting Zhou",
        "Pengfei Liu",
        "Puxin Xu",
        "Srini Iyer",
        "Jiao Sun",
        "Yuning Mao",
        "Xuezhe Ma",
        "Avia Efrat",
        "Ping Yu",
        "Lili Yu",
        "Susan Zhang",
        "Gargi Ghosh",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Omer Levy"
      ],
      "year": "2023",
      "venue": "Less is more for alignment",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Towards graph-hop retrieval and reasoning in complex question answering over textual database",
      "authors": [
        "Minjun Zhu",
        "Yixuan Weng",
        "Shizhu He",
        "Kang Liu"
      ],
      "year": "2023",
      "venue": "Towards graph-hop retrieval and reasoning in complex question answering over textual database",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Lmtuner: An User-Friendly And Highly-Integrable Training Framework",
      "text": "for fine-tuning Large Language Models Yixuan Weng \\({}^{1}\\), Zhiqi Wang\\({}^{1,2}\\), Huanxuan Liao\\({}^{1,2}\\), Shizhu He\\({}^{1,2}\\), Shengping Liu\\({}^{3}\\), Kang Liu\\({}^{1,2}\\), Jun Zhao\\({}^{1,2}\\) \\({}^{1}\\) The Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences \\({}^{2}\\)School of Artificial Intelligence, University of Chinese Academy of Sciences \\({}^{3}\\)Unisound, Beijing, China wengsyx@gmail.com, {wangzhiqi2022, liaohuanxuan2023}@ia.ac.cn, liushengping@unisound.com, {shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn"
    },
    {
      "title": "Abstract",
      "text": "With the burgeoning development in the realm of large language models (LLMs), the demand for efficient incremental training tailored to specific industries and domains continues to increase. Currently, the predominantly employed frameworks lack modular design, it often takes a lot of coding work to kickstart the training of LLM. To address this, we present \"LMTuner\", a highly usable, integrable, and scalable system for training LLMs expeditiously and with minimal user-input. LMTuner comprises three main modules - the Interaction, Training, and Inference Modules. We advocate that LMTuner's usability and integrality alleviate the complexities in training large language models. Remarkably, even a novice user could commence training large language models within five minutes. Furthermore, it integrates DeepSpeed frameworks and supports Efficient Fine-Tuning methodologies like Low Rank Adaptation (LoRA), Quantized LoRA (QLoRA), etc., enabling the training of language models scaling from 300M to a whopping 130B parameters using a single server. The LMTuner's homepage1 and screenscat video 2are now publicly available. Footnote 1: [https://wengsyx.github.io/LMTuner/](https://wengsyx.github.io/LMTuner/) Footnote 2: [https://youtu.be/nsXmWOnN3rE](https://youtu.be/nsXmWOnN3rE)"
    },
    {
      "title": "1 Introduction",
      "text": "Large language models (LLMs) are demonstrating unprecedented performance in numerous natural language understanding and generation tasks Wei et al. (2022); Weng et al. (2023); Weng et al. (2023) thanks to their capacity to learn from extensive text data with generative manner Brown et al. (2020); Scao et al. (2022). This has led to a rising number of researchers and engineers embarking on the training of their own language models for specific industries and domains Taori et al. (2023); Xu et al. (2023); Cui et al. (2023). However, training LLMs imposes high demands on engineering skills Zhang et al. (2022), and the various techniques applicable to such training are mostly disparate Dao et al. (2022); Xi et al. (2023); Luo et al. (2023). This not only increases the complexity of related projects but also adds to the learning cost required for training language models. With the progressive development of generative large language model technology, various techniques have emerged, incorporated into different toolkits Zhao et al. (2023). As shown in Table 1, when aiming for model parallelism, MegatronLM Shoeybi et al. (2020) stands as a preferred choice, while bitsandbytes Dettmers et al. (2022) serves the purpose of model quantization, and Opendelta Hu et al. (2023) facilitates the implementation of Efficient Fine-Tuning technology. Nonetheless, the flexibility offered by these individual tools comes at the cost of developers spending significant time coordinating the usage of diverse tool modules, rendering direct application challenging. To address these challenges, several frameworks, such as h2oGPT Candel et al. (2023) and Lamini Diamos et al. (2023), have attempted to consolidate some of these functionalities. However, these frameworks typically integrate only a subset of the available techniques, often lacking comprehensive coverage of commonly used model technologies. Therefore, in this paper, we present LMTuner, a novel framework that significantly minimizes these Figure 1: The architecture of the LMTuner. LMTuner can be invoked through a single line of code Let_Tune0. The overall process is top-down, sequentially going through the Interaction Module, Training Module, and Inference Module. obstacles by offering an easy-to-use, scalable, and integrative modular system. As depicted in Figure 1, LMTuner is comprised of three modules. 1) The Interaction Module enables user-friendly communication, automatically adjusting parameters based on user needs and context, ideal for non-technical users. 2) The Training Module autonomously processes training using these parameters, saving users the complexity of setup. 3) The Inference Module utilizes the trained models for various tasks upon training completion. With these modules, developers can effortlessly generate their desired models and automatically pass on the parameters to the next module for seamless training. This streamlines the entire process, allowing the system to be readily deployed and utilized upon completion of training. In summary, our proposed LMTuner system offers greater usability and flexibility, allowing users to swiftly configure parameters for training language models according to specific needs and effectively initiate the training. Our contributions are as follows: * We have proposed LMTuner, which is the highly usable and integrated training system for LLMs. It is free to use and license-friendly (Apache 2.0). And we open source code at [https://github.com/WENGSYX/LMTuner](https://github.com/WENGSYX/LMTuner). * LMTuner boasts high usability, only needing a single code (**Let_Tune**0) to be launched. It facilitates quick start-up for large language model training by allowing users to interact using natural language with LMTuner. * We have incorporated a wide range of techniques suited for training large language models, including models, domain QA datasets, Efficient Fine-Tuning methods, and specific hyperparameters. This integration fosters the research and development of large language models."
    },
    {
      "title": "2 Related Work",
      "text": "The development of pre-training language models has brought about numerous language model tools and a flourishing NLP community, among which are \"Transformers\" (Wolf et al., 2020). It establishes a range of model classes and provides APIs for implementing easily extendable transformer models. Other tools closely associated with language model building include Fairseq Ott et al. (2019), MegatronLM Shoeybi et al. (2020), MedConQA Xia et al. (2022), OpenDelta Hu et al. (2023) and h2oGPT Candel et al. (2023). Differing from these, LMTuner is specifically designed for training auto-regressive LLMs. With its modular design, it allows for a free combination of different pre-training models, datasets, model frameworks, automatic length extrapolation settings, and PEFT methods within one framework. With the assistance of dialogue-type LLMs such as GPT-4 (OpenAI, 2023), users can accomplish the entire training process with just a single line of code. Recent researches have proposed many directions for training LLMs Ignat et al. (2023), including high-quality data Zhou et al. (2023); Gunasekar et al. (2023), efficient fine-tuning methods Mangrulkar et al. (2022), model structures Shazeer (2019); Dao et al. (2022), and length extrapolation Chi et al. (2023); Tworkowski et al. (2023); Su et al. (2022); Chen et al. (2023). The LMTuner system integrates the most advanced techniques in these fields, allowing users to make choices according to their needs. LMTuner integrates these techniques into separate modules, facilitating user selection and usage Wang et al. (2023); Dao et al. (2022); Dettmers et al. (2023). LLMs undergoing instruction pre-training can align with human instructions Ouyang et al. (2022); Chung et al. (2022) and have been discovered to possess many capabilities not present in smaller language models Zhao et al. (2023), such as tool creation Cai et al. (2023), environment \\begin{table} \\begin{tabular}{l|c c c c c c c c} \\hline & \\multicolumn{8}{c}{**Highly-intergrable**} & \\multicolumn{3}{c}{**User-friendly**} \\\\ \\cline{2-10} & Model & Parallelism & Quantization & PEFT & MEFT & ZeRO & Load & Dataset & Position Interpolation & AI Assistant & Code Concise \\\\ \\hline MegatronLM Shoeybi et al. (2020) & ✓ & & & & & & & \\\\ Huggingface Wolf et al. (2020) & ✓ & & ✓ & ✓ & ✓ & ✓ & & & ✓ \\\\ bitaudyves Dettmers et al. (2022) & & ✓ & & ✓ & & & & & ✓ \\\\ OpenDelta Hu et al. (2023) & & & & ✓ & & & & & ✓ \\\\ Lamini Diames et al. (2023) & & & & & & ✓ & & & ✓ \\\\ h2oGPT Candel et al. (2023) & & ✓ & ✓ & & & ✓ & & ✓ \\\\ \\hline **LMTuner (Ours)** & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ \\hline \\end{tabular} \\end{table} Table 1: Compared to other commonly used language model training systems, LMTuner system has highly integrable and user-friendly. [MISSING_PAGE_FAIL:3] such expertise among researchers and engineers without backgrounds in large language model training. The Interaction Module, while offering immense flexibility and ease of use, prevents potential issues such as user configuration errors. As shown in Figure 3, users with different requirements only need to express their needs in natural language. The LMTuner system is capable of analyzing and recommending appropriate training parameters. If the user is not inclined to train the model on the current device, LMTuner will automatically generate a Readme.md file that includes environment configuration, model processing, and training code instructions like Figure 4, facilitating a swift setup for the user on a new device. Once all training parameters are finalized, LMTuner saves a copy of these in an ARGS.json file. If one wishes to initiate training quickly with the same parameters, they only need to pass the path name of ARGS.json to the **Let_Tune0** function, thereby avoiding redundant repeated dialogues."
    },
    {
      "title": "Training Module",
      "text": "The training module in LMTuner has highly integrated, easy to invoke, and extensible features. Currently, the mainstream LLMs are mostly similar in architecture, and the training process and loss calculation are generally consistent. Therefore, we construct the required techniques in a modularized manner at the code level according to different requirements. Meanwhile, such design also facilitates engineers with coding skills to directly replace the corresponding modules through hooks. In the rest of this section, we will introduce each technical module separately. **Datasets.** The availability of high-quality training data is crucial for developing capable LLMs. To Figure 4: An example of the Readme.md automatically generated by LMTuner. Figure 3: Depending on the requirements specified, LMTuner can automatically suggest different training plans. facilitate affordable access to suitable datasets for question answering, we have curated and prepared a collection of QA datasets covering diverse domains, including English, Chinese, medical, and legal fields. Furthermore, to enable the development of models with customizable names and personas, the datasets have been augmented with synthetic question-answer pairs inquiring about the model's identity (e.g. \"Hi I'm [MODEL NAME]\"). During training, the [MODEL NAME] tokens can be dynamically substituted with the preferred name for each model instance. ``` 1fromLMTuner.datasetimportLMTunerDataset 2 3dataset=LMTunerDataset() 4 5#Giveyourmodelaname 6dataset.set_model_name('LMTuner') 7 8#AddQAdatasetsamples 9dataset.add_sample(['Whoareyou?', 10~I'mLMTuner,yourpersonalsidekick!\"]) ``` However, despite the richness of our curated datasets, they inherently possess certain limitations in coverage and diversity. To augment the built-in datasets and account for user-specific needs, our system also provides seamless support for customized training data. Users can simply provide the local path to their own JSONL-formatted question answering data files. This design choice provides greater flexibility to users, empowering them to tailor the training distribution to their unique application requirements. For instance, users can provide proprietary datasets containing sensitive or confidential information not suitable for public release. The ability to directly use local JSONL files avoids the need to port datasets to external platforms. ``` 1#PretrainedCustom-DatasetFormat 2{ 3\"input\":\"\", 4\"output\":\"Withthebburgeoningdevelopmenttherealmof...\", 5} 6 7#InstructCustom-DatasetFormat 8{ 9\"input\":\"Human:Whoareyou?\", 10\"output\":\"Assistant:I'mLMTuner,yourpersonalsidekick!\", 11} ``` **Models.** Recent advancements in natural language processing have been enabled by the transformer architecture [17]. The SwissArmyTransformer 3 framework facilitates efficient development of diverse transformer models by decoupling reusable core components from interchangeable model-specific modules. These lightweight modules attach to the shared backbone via hooks, enabling rapid iteration and customization. In contrast, Transformers [18] provides optimized implementations of canonical architectures and pretrained models for production use. Footnote 3: [https://github.com/THUDM/SwissArmyTransformer](https://github.com/THUDM/SwissArmyTransformer) The LMTuner system combines these complementary strengths for flexible model development and deployment. It utilizes SwissArmyTransformer to construct tailored architectures and seamlessly integrates Transformers' pretrained models. This synthesis of code modularization and extensive pretrained models promises to enhance productivity, accelerate innovation, and improve real-world language understanding. LMTuner promotes exploratory modeling by innovating new designs built on transformer infrastructure, while benefiting from cutting-edge advancements in language model pretraining. Selectively utilizing both libraries stands to meaningfully advance natural language processing systems through rapid prototyping of specialized models and accessible deployment of state-of-the-art capabilities. **Efficient Fine-Tuning.** LMTuner uses ZeRO technology of Deepspeed by default to unload parameters and improve training throughput. In addition, LMTuner provides parameters-efficient fine-tuning (PEFT) methods including LoRA [10] and QLoRA [14], and memory-efficient fine-tuning (MEFT) methods including LOMO [11] and Quantization[15]. They support training LLMs with low memory usage. These methods are implemented in a modularized manner at the code level of LMTuner, so they can be easily combined and used freely. **Position Interpolation.** To better support long-context modeling, LMTuner has integrated some scaling of RoPE. We implemented Xpos [17] and some recent position interpolation methods like linear interpolation [13], dynamic interpolation, NTK-Aware Scaled RoPE (NTKv1) and NTK-By-Parts (NTKv2). The dynamic methods choose the correct _scale_ parameter based on sequence length, rather than having to settle for a fixed trade-off between maximum sequence length and performance on shorter sequences, i.e., use the exact position values for the first 2048 contexts and then recalculate the position vectors for each new sequence length as the model generates the markers one by one. A certain degree of long-context modeling can be achieved by choosing different scaling methods. **Other Details.** We use the probability distribution over sequences of tokens as the optimization objective with cross-entropy loss [14] and use Lion optimizer [3] by default to optimize LLMs, because it has been proven to be more memory-efficient than Adam [15]. During training, we record the loss, learning rate, and number of tokens respectively at each step using wandb4, and display them in the browser through line charts, which helps users observe the training status during training. Footnote 4: [https://wandb.ai/](https://wandb.ai/)"
    },
    {
      "title": "Inference Module",
      "text": "LMTuner loads the final model weights after training and generates continuations conditioned on given contexts until reaching the maximum length. To speed up inference, LMTuner provides model quantization methods including INT8 and INT4 quantization [22]. By quantizing 16-bit floating-point weights into lower bitwidth integers such as 8-bit or 4-bit, the computation time and memory usage during inference can be reduced. LMTuner quantizes weights of selected layers in a trained model, while keeping activations in 16-bit floating-point format. After quantization, the inference latency on CPU and throughput on GPU can be improved significantly with little degradation in model quality [16]."
    },
    {
      "title": "4 A Running Case",
      "text": "The target audience of LMTuner is machine learning engineers and researchers across academia and industry. Novice users can leverage LMTuner's guided interaction while experts retain full control over implementation details. It compares favorably to current systems by combining user-friendliness, scalability, and integrability within a unified interface. LMTuner is open-source under the Apache 2.0 license, allowing free use in commercial products. By open-sourcing LMTuner, we hope to catalyze progress in large language model training and lower the barriers to leveraging these transformative technologies. The availability of an easy-to-use, highly customizable system should benefit the broader community. Assuming we need to train a medical LLM that can assist in patient diagnosis, and we have two A6000 GPUs with 48GB VRAM each, as well as a medical QA dataset [13]). Using LMTuner, we can automatically determine the training process, including the selection of the LLama-7B model and a set of corresponding hyperparameters5. Footnote 5: We released the code at [https://github.com/WENGSYX/LMTuner/tree/main/Example/English_Medical](https://github.com/WENGSYX/LMTuner/tree/main/Example/English_Medical) while Table 2 showcases the performance of the MedDialog test set (without manual model selection). Instead, we chose the final model obtained after LMTuner training, which completed 10 epochs. We observed that the LMTuner-trained model surpasses existing models in metrics such as Bleu, Meteor, and NIST. This indicates that the trained model is ready for direct utilization."
    },
    {
      "title": "5 Conclusion",
      "text": "LMTuner represents a pioneering effort to facilitate large language model training through enhanced usability and modularity. We believe LMTuner represents a significant step towards realizing the full potential of large language models. By continuing to integrate emerging techniques and community feedback, its capabilities will only grow over time. The availability of LMTuner as an open source project presents exciting opportunities for LLMs enhancement. We hope its emphasis on usability and extensibility will meaningfully accelerate future work in this paradigm-defining domain. \\begin{table} \\begin{tabular}{c|c c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multicolumn{5}{c}{**MedDialog**} \\\\ & BLEU-2 & BLEU-4 & Meteor & Nist-2 & Nist-4 \\\\ \\hline GPT-3 (175B) + Direct & 14.93 & 7.52 & 4.55 & 0.814 & 0.852 \\\\ GPT-3 (175B) + CoT & 9.18 & 4.61 & 3.54 & 0.546 & 0.569 \\\\ Insortort-GPT (175B) + Direct & 15.93 & 8.21 & 5.74 & 0.874 & 0.913 \\\\ Insort-GPT (175B) + CoT & 16.49 & 7.80 & 4.94 & 1.020 & 1.059 \\\\ GLM (130B) + Direct & 12.16 & 6.02 & 5.31 & 0.577 & 0.601 \\\\ GLM (130B) + CoT & 30.02 & 15.61 & 8.73 & 1.909 & 2.017 \\\\ \\hline Zeng et al. (2020) & 31.67 & 16.88 & 9.57 & 1.981 & 2.076 \\\\ LMTuner (**Ours**) & **35.62** & **19.02** & **9.47** & **2.377** & **2.517** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: We present the results of several language models on the MedDialog task, including the zero-shot (Direct) and CoT [13, 14] performance of some LLMs (GPT-3:code-davinci-001 [3]; Instruct-GPT: code-davinci-002 [3]), as well as the performance of previous Finetune state-of-the-art models."
    },
    {
      "title": "Limitations",
      "text": "While LMTuner is designed to be user-friendly and intuitive, it may not capture all specific user requirements in its current version. The training process might need to be adjusted or additional techniques might need to be integrated to achieve optimal performance for certain specialized tasks. Some complex requirements could necessitate manual code modification, which might increase the learning curve for non-expert users. However, this limitation is also being actively addressed through continuous development and updates to the system to enhance its comprehension of user requirements."
    },
    {
      "title": "Acknowledgments",
      "text": "This work was supported by the Strategic Priority Research Program of Chinese Academy of Sciences (No. XDA27020100) and the National Natural Science Foundation of China (No.U1936207, No.61976211). This work was supported by the Youth Innovation Promotion Association CAS and Yunnan Provincial Major Science and Technology Special Plan Projects (No.202202AD080004)."
    },
    {
      "title": "References",
      "text": "* Black et al. (2021) Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata. * Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901. * Cai et al. (2023) Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2023. Large language models as tool makers. * Candel et al. (2023) Arno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jelbicki, Prithvi Prabhu, Jeff Gambera, Mark Landry, Shivam Bansal, Ryan Chesler, Chun Ming Lee, Marcos V. Conde, Pasha Stetsenko, Olivier Grellier, and SriSatish Ambati. 2023. h2gept: Democctizing large language models. * Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Petros Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikos Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. _CoRR_, abs/2107.03374. * Chen et al. (2023a) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a. Extending context window of large language models via positional interpolation. * Chen et al. (2023b) Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. 2023b. Symbolic discovery of optimization algorithms. * Chi et al. (2023) Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky, and Peter J. Ramadge. 2023. Dissecting transformer length extrapolation via the lens of receptive field analysis. * Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. * Cui et al. (2023) Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023. Chatlaw: Open-source legal large language model with integrated external knowledge bases. * Dao et al. (2022) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. * Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Llm.int8(): 8-bit matrix multiplication for transformers at scale. * Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. * Diamos et al. (2023) Greg Diamos, Sharon Zhou, Samee Ibraheem, Daniel, and Arman. 2023. Lamini: The llm engine for rapidly customizing models. [https://github.com/lamini-ai/lamini](https://github.com/lamini-ai/lamini). Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335. * Gholami et al. (2021) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. 2021. A survey of quantization methods for efficient neural network inference. * Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks are all you need. * Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_. * Hu et al. (2023) Shengding Hu, Ning Ding, Weilin Zhao, Xingtai Lv, Zhen Zhang, Zhiyuan Liu, and Maosong Sun. 2023. OpenDelta: A plug-and-play library for parameter-efficient adaptation of pre-trained models. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)_, pages 274-281, Toronto, Canada. Association for Computational Linguistics. * Ignat et al. (2023) Oana Ignat, Zhijing Jin, Artem Abzaliev, Laura Biester, Santiago Castro, Naihao Deng, Xinyi Gao, Aylin Gunal, Jacky He, Ashkan Kazemi, Muhammad Khalifa, Namho Koh, Andrew Lee, Siyang Liu, Do June Min, Shinka Mori, Joan Nwatu, Veronica Perez-Rosas, Siqi Shen, Zekun Wang, Winston Wu, and Rada Mihalcea. 2023. A phd student's perspective on research in nlp in the era of very large language models. * Kingma and Ba (2017) Diederik P. Kingma and Jimmy Ba. 2017. Adam: A method for stochastic optimization. * Liu et al. (2023) Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Jie-Rong Wen. 2023. Do emergent abilities exist in quantized large language models: An empirical study. * Luo et al. (2023) Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, and Yang You. 2023. Came: Confidence-guided adaptive memory efficient optimization. * Lv et al. (2023) Kai Lv, Yuqing Yang, Tengxiao Liu, Qi jie Gao, Qipeng Guo, and Xipeng Qiu. 2023. Full parameter fine-tuning for large language models with limited resources. * Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debutt, Younes Belkada, and Sayak Paul. 2022. Peft: State-of-the-art parameter-efficient fine-tuning methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft). * OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. * Ott et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)_, pages 48-53, Minneapolis, Minnesota. Association for Computational Linguistics. * Ouyang et al. (2022a) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022a. Training language models to follow instructions with human feedback. * Ouyang et al. (2022b) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022b. Training language models to follow instructions with human feedback. In _Advances in Neural Information Processing Systems_. * Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. * Ren et al. (2023) Zhichu Ren, Zhen Zhang, Yunsheng Tian, and Ju Li. 2023. Crest-copilot for real-world experimental scientist. * Le Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Elie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_. * Shazeer (2019) Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. * Shoeybi et al. (2020) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2020. Megatron-lm: Training multi-billion parameter language models using model parallelism. * Su et al. (2022) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2022. Roformer: Enhanced transformer with rotary position embedding. * Sun et al. (2022) Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable transformer. * Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca). Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. * Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jens Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihalyo, Pushkar Mishra, Igor Molybg, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashin Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. * Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milos. 2023. Focused transformer: Contrastive training for context scaling. * Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. * Wang et al. (2023a) Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, and Yuxiong He. 2023a. Zero++: Extremely efficient collective communication for giant model training. * Wang et al. (2023b) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023b. Voyager: An open-ended embodied agent with large language models. * Wang et al. (2023c) Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchunshu Zhou, Shaochun Hao, Guangzhou Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, Zhenzhu Yang, Adam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen, Ke Xu, Dayiheng Liu, Yike Guo, and Jie Fu. 2023c. Interactive natural language processing. * Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In _International Conference on Learning Representations_. * Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_. * Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_. * Weng et al. (2023a) Yixuan Weng, Bin Li, Fei Xia, Minjun Zhu, Bing Sun, Shizhu He, Kang Liu, and Jun Zhao. 2023a. Large language models need holistically thought in medical conversational qa. _ArXiv_, abs/2305.05410. * Weng et al. (2022) Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. 2022. Large language models are reasoners with self-verification. _arXiv preprint arXiv:2212.09561_. * Weng et al. (2023b) Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. 2023b. Neural comprehension: Language models with compiled neural networks. _arXiv preprint arXiv:2304.01665_. * Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online. Association for Computational Linguistics. * Xi et al. (2023) Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. 2023. Training transformers with 4-bit integers. * Xia et al. (2022a) Fei Xia, Bin Li, Yixuan Weng, Shizhu He, Kang Liu, Bin Sun, Shutao Li, and Jun Zhao. 2022a. MedConQA: Medical conversational question answering system based on knowledge graphs. In _Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 148-158, Abu Dhabi, UAE. Association for Computational Linguistics. * Xia et al. (2022b) Fei Xia, Bin Li, Yixuan Weng, Shizhu He, Kang Liu, Bin Sun, Shutao Li, and Jun Zhao. 2022b. Medconqa: Medical conversational question answering system based on knowledge graphs. In _Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 148-158. * Xu et al. (2023) Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. _arXiv preprint arXiv:2304.01196_. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. * Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. _arXiv preprint arXiv:2210.02414_. * Zeng et al. (2020) Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, et al. 2020. Meddialog: Large-scale medical dialogue dataset. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_. * Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_. * Zhao et al. (2023a) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023a. A survey of large language models. * Zhao et al. (2023) Zhe Zhao, Yudong Li, Cheng Hou, Jing Zhao, Rong Tian, Weijie Liu, Yiren Chen, Ningyuan Sun, Haoyan Liu, Weiquan Mao, Han Guo, Weigang Gou, Taiqiang Wu, Tao Zhu, Wenhang Shi, Chen Chen, Shan Huang, Sihong Chen, Liqun Liu, Feifei Li, Xiaoshai Chen, Xingwu Sun, Zhanhui Kang, Xiaoyong Du, Linlin Shen, and Kimmo Yan. 2023b. TencentPretrain: A scalable and flexible toolkit for pre-training models of different modalities. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)_, pages 217-225, Toronto, Canada. Association for Computational Linguistics. * Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment. * Zhu et al. (2023) Minjun Zhu, Yixuan Weng, Shizhu He, Kang Liu, and Jun Zhao. 2023. Towards graph-hop retrieval and reasoning in complex question answering over textual database. _arXiv preprint arXiv:2305.14211_."
    },
    {
      "title": "Appendix A Support Models",
      "text": "LMTuner enables rapid deployment of large language models ranging from 300 million to 130 billion parameters, facilitating their application to textual data tasks. We list the models supported for training in LMTuner in Table 4. (Please note that some of these models are not fully allowed for commercial use.) LMTuner also provides capabilities to load pre-trained models from Transformers, however this necessitates redefining the data tokenizer. In addition, LMTuner facilitates training models from scratch to incorporate specialized architectures such as FlashAttention and Multi-Query Attention through its flexible hook. The modular hook-based approach streamlines implementing customized attentional mechanisms and training objectives. This balancing of usability and flexibility makes LMTuner a promising platform for rapidly prototyping."
    },
    {
      "title": "Appendix B Support Datasets",
      "text": "We have supported multiple QA datasets in different fields for quick training. The provision of ready-to-use domain-specific datasets eliminates the need for users to invest significant time and effort in dataset curation and preprocessing. With high-quality training data covering key domains, users can promptly initialize model training and optimization workflows. * For English datasets, we default to loading the LIMA6Zhou et al. (2023) dataset, which includes 1,030 diverse and high-quality QA samples. Footnote 6: [https://huggingface.co/datasets/GAIR/lima](https://huggingface.co/datasets/GAIR/lima) * For Chinese datasets, we manually translated the LIMA dataset and added an additional 60 Chinese samples covering different fields such as Chinese history, Marxism, essay writing, and Chinese humor, making it a QA dataset with 1,090 samples covering multiple fields and Chinese characteristics. * For Chinese Medical datasets, We selected about 60,000 medical consultation dialogues from CMCQAXia et al. (2022), which contained QA samples from over 30 different departments, including pediatrics, gynecology, internal medicine, oncology and more."
    },
    {
      "title": "Appendix C Lmtuner'S Gpu Memory Usage",
      "text": "Normally, training LLMs requires a huge amount of GPU memory. Recent advancements in EFT techniques such as LoRA and LOMO have enabled the training of extremely large language models on consumer GPUs with as little as 24GB of memory. In addition, tensor parallelism techniques can split a model across different GPUs, making it possible to scale up training of even larger LLMs. In Table 3 we list the minimum recommended GPU memory configuration by LMTuner for training models of different sizes."
    },
    {
      "title": "Appendix D System Configuration And Requirements",
      "text": "LMTuner ([https://github.com/WENGSYX/LMTuner](https://github.com/WENGSYX/LMTuner)) is an open-source system that offers a command-line interface for training LLMs, without requiring any coding experience. The system requirements include: Ubuntu 14.04+, Debian 8+, CentOS 6+, or Fedora 27+. And an NVIDIA GPU with driver version >= 460.32.03 or AMD GPU with ROMc >= 4.0. In addition, a series of Python libraries, including Apex7, Pytorch8 and DeepSpeed 9, are also required. For details, please refer to [https://wengsyx.github.io/LMTuner/install.html](https://wengsyx.github.io/LMTuner/install.html). Footnote 7: [https://github.com/NVIDIA/apex](https://github.com/NVIDIA/apex) Footnote 8: [https://pytorch.org/](https://pytorch.org/) Footnote 9: [https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed) After installation is complete, you can directly launch via Command-Line Interface without modifying any code by using the one-line code \\(\\textbf{Let\\_Lingo}0\\). LMTuner requires the provision of an OpenAI Key to facilitate the collaborative determination of training parameters with GPT-4 through dialogue. However, we have also pre-defined ten sets of configuration questions related to training LLMs in a format similar to a questionnaire, to enable users in countries that do not support GPT-4 to quickly initiate LMTuner 10. Footnote 10: Take a look at the current list of Supported Countries and Territories for OpenAI: [https://platform.openai.com/docs/supported-countries](https://platform.openai.com/docs/supported-countries) Finally, LMTuner will automatically generate and run a command line to invoke deepspeed according to the requirements without needing to modify at the code level. This not only simplifies the training process, but also facilitates modifying and recording the configuration of hyperparameters, helping users without basic knowledge to also train LLMs. \\begin{table} \\begin{tabular}{l c} \\hline \\hline GPT-2 & Radford et al. (2019) \\\\ GPT-Neo-1.3B & Black et al. (2021) \\\\ ChatGLM-6B & Du et al. (2022) \\\\ ChatGLM2-6B & Du et al. (2022) \\\\ Llama-7B & Touvron et al. (2023a) \\\\ Llama-13B & Touvron et al. (2023a) \\\\ Llama-33B & Touvron et al. (2023a) \\\\ Llama-65B & Touvron et al. (2023a) \\\\ Llama2-7B & Touvron et al. (2023b) \\\\ Llama2-13B & Touvron et al. (2023b) \\\\ Llama2-70B & Touvron et al. (2023b) \\\\ GLM-130B & Zeng et al. (2022) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: LMTuner supports pre-trained language models for rapid deployment. \\begin{table} \\begin{tabular}{c|c|c|c|c|c} \\hline Model Size & 16-bit Finestone & 16-bit LOMO & 16-bit L6A & 8-bit L6A & 4-bit L6A \\\\ \\hline \\(\\sim\\)1B & 8 GB & 4 GB & 4 GB & 6 GB & 6 GB \\\\ TB & 5 GB & 6 GB & 6 GB & 8 GB & 8 GB \\\\ 13B & 16 GB & 10 GB & 10 GB & 12 GB & 10 GB \\\\ 33B & 24 GB GB & 24 GB & 24 GB & 32 GB & 32 GB \\\\ 70B & 8,24 GB & 448 GB & 2,82 GB & 2,24 GB & 2,32 GB & 80 GB & 2,24 GB & 48 GB \\\\ 130B & 8,48 GB & 48.0 GB & 8,24 GB & 448 GB & 2,80 GB & 8,24 GB & 448 GB & 2,80 GB \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: The minimum configuration required for different model size in LMTuner (when Batchsize=1). We suggest appropriately increasing the number of GPUs to accelerate training the models."
    }
  ]
}