{
  "title": "Locally Differentially Private Document Generation Using Zero Shot Prompting",
  "authors": [
    "Saiteja Utpala",
    "Sara Hooker",
    "Yu Chen"
  ],
  "abstract": "\n Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of deanonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46% reduction in author identification F1 score against static attackers and a 26% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacyutility tradeoff. Code is avaliable at  https:  //github.com/SaitejaUtpala/dp_prompt  Mechanism Privacy Level Requires fine-tuning Generates sanitized doc Madlib (Feyisetan et al., 2020) Word level Metric-DP No Yes Mahanolbis (Xu et al., 2020) Word level Metric-DP No Yes TEM (Carvalho et al. \n",
  "references": [
    {
      "id": null,
      "title": "Locally Differentially Private Document Generation Using Zero Shot Prompting",
      "authors": [
        "Saiteja Utpala",
        "Sara Hooker",
        "Yu Chen"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "The US census bureau adopts differential privacy",
      "authors": [
        "John M Abowd"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "",
      "authors": [
        "Nicolás E Miguel E Andrés",
        "Konstantinos Bordenabe",
        "Catuscia Chatzikokolakis",
        "Palamidessi"
      ],
      "year": "2013",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Differential privacy for location-based systems",
      "authors": [
        "Geo-Indistinguishability"
      ],
      "year": "",
      "venue": "Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Large-scale differentially private bert",
      "authors": [
        "Rohan Anil",
        "Badih Ghazi",
        "Vineet Gupta",
        "Ravi Kumar",
        "Pasin Manurangsi"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Learning with privacy at scale",
      "authors": [
        "Apple"
      ],
      "year": "2017",
      "venue": "Apple Machine Learning Journal",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Lamp: Extracting text from gradients with language model priors",
      "authors": [
        "Mislav Balunovic",
        "Dimitar Iliev Dimitrov",
        "Nikola Jovanović",
        "Martin Vechev"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "A face is exposed for aol searcher no. 4417749",
      "authors": [
        "Michael Barbaro",
        "Tom Zeller"
      ],
      "year": "2006",
      "venue": "New York Times",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Heuristic authorship obfuscation",
      "authors": [
        "Janek Bevendorff",
        "Martin Potthast",
        "Matthias Hagen",
        "Benno Stein"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Gpt-neox-20b: An open-source autoregressive language model",
      "authors": [
        "Sid Black",
        "Stella Biderman",
        "Eric Hallahan",
        "Quentin Anthony",
        "Leo Gao",
        "Laurence Golding",
        "Horace He",
        "Connor Leahy",
        "Kyle Mcdonell",
        "Jason Phang"
      ],
      "year": "2022",
      "venue": "Gpt-neox-20b: An open-source autoregressive language model",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "A critical review on the use (and misuse) of differential privacy in machine learning",
      "authors": [
        "Alberto Blanco-Justicia",
        "David Sánchez",
        "Josep Domingo-Ferrer",
        "Krishnamurty Muralidhar"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "The secret sharer: Evaluating and testing unintended memorization in neural networks",
      "authors": [
        "Nicholas Carlini",
        "Chang Liu",
        "Úlfar Erlingsson",
        "Jernej Kos",
        "Dawn Song"
      ],
      "year": "2019",
      "venue": "USENIX Security Symposium",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Extracting training data from large language models",
      "authors": [
        "Nicholas Carlini",
        "Florian Tramer",
        "Eric Wallace"
      ],
      "year": "2021",
      "venue": "USENIX Security Symposium",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Tem: high utility metric differential privacy on text",
      "authors": [
        "Ricardo Silva Carvalho",
        "Theodore Vasiloudis",
        "Oluwaseyi Feyisetan"
      ],
      "year": "2021",
      "venue": "Tem: high utility metric differential privacy on text",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Broadening the scope of differential privacy using metrics",
      "authors": [
        "Konstantinos Chatzikokolakis",
        "Miguel E Andrés",
        "Nicolás Emilio Bordenabe",
        "Catuscia Palamidessi"
      ],
      "year": "2013",
      "venue": "Privacy Enhancing Technologies: 13th International Symposium",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "A customized text sanitization mechanism with differential privacy",
      "authors": [
        "Sai Chen",
        "Fengran Mo",
        "Yanhao Wang",
        "Cen Chen",
        "Jian-Yun Nie",
        "Chengyu Wang",
        "Jamie Cui"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2022",
      "venue": "Palm: Scaling language modeling with pathways",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Eric Li",
        "Xuezhi Wang",
        "Mostafa Dehghani"
      ],
      "year": "",
      "venue": "Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Tag: Gradient attack on transformerbased language models",
      "authors": [
        "Jieren Deng",
        "Yijue Wang",
        "Ji Li",
        "Chenghong Wang",
        "Chao Shang",
        "Hang Liu",
        "Sanguthevar Rajasekaran",
        "Caiwen Ding"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Local privacy and statistical minimax rates",
      "authors": [
        "Michael I John C Duchi",
        "Martin J Jordan",
        "Wainwright"
      ],
      "year": "2013",
      "venue": "2013 IEEE 54th Annual Symposium on Foundations of Computer Science",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Calibrating noise to sensitivity in private data analysis",
      "authors": [
        "Cynthia Dwork",
        "Frank Mcsherry",
        "Kobbi Nissim",
        "Adam Smith"
      ],
      "year": "2006",
      "venue": "Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "The algorithmic foundations of differential privacy",
      "authors": [
        "Cynthia Dwork",
        "Aaron Roth"
      ],
      "year": "2014",
      "venue": "Foundations and Trends® in Theoretical Computer Science",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Rappor: Randomized aggregatable privacypreserving ordinal response",
      "authors": [
        "Úlfar Erlingsson",
        "Vasyl Pihur",
        "Aleksandra Korolova"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 ACM SIGSAC conference on computer and communications security",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Privacy-and utility-preserving textual analysis via calibrated multivariate perturbations",
      "authors": [
        "Borja Oluwaseyi Feyisetan",
        "Thomas Balle",
        "Tom Drake",
        "Diethe"
      ],
      "year": "2020",
      "venue": "Proceedings of the 13th International Conference on Web Search and Data Mining",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "The median hypothesis",
      "authors": [
        "Ran Gilad",
        "- Bachrach",
        "Chris J C Burges"
      ],
      "year": "2012",
      "venue": "The median hypothesis",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "User review sites as a resource for large-scale sociolinguistic studies",
      "authors": [
        "Dirk Hovy",
        "Anders Johannsen",
        "Anders Søgaard"
      ],
      "year": "2015",
      "venue": "Proceedings of the 24th international conference on World Wide Web",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "DP-BART for privatized text rewriting under local differential privacy",
      "authors": [
        "Timour Igamberdiev",
        "Ivan Habernal"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Evaluating differentially private machine learning in practice",
      "authors": [
        "Bargav Jayaraman",
        "David Evans"
      ],
      "year": "2019",
      "venue": "USENIX Security Symposium",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Mistral 7b",
      "authors": [
        "Alexandre Albert Q Jiang",
        "Arthur Sablayrolles",
        "Chris Mensch",
        "Devendra Bamford",
        "Diego Singh Chaplot",
        "Florian De Las Casas",
        "Gianna Bressand",
        "Guillaume Lengyel",
        "Lucile Lample",
        "Saulnier"
      ],
      "year": "2023",
      "venue": "Mistral 7b",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
      "authors": [
        "Di Jin",
        "Zhijing Jin",
        "Joey Tianyi Zhou",
        "Peter Szolovits"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "I know what you did last summer: query logs and user privacy",
      "authors": [
        "Rosie Jones",
        "Ravi Kumar",
        "Bo Pang",
        "Andrew Tomkins"
      ],
      "year": "2007",
      "venue": "Proceedings of the sixteenth ACM conference on Conference on information and knowledge management",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "What can we learn privately? SIAM Journal on Computing",
      "authors": [
        "Shiva Prasad Kasiviswanathan",
        "K Homin",
        "Kobbi Lee",
        "Sofya Nissim",
        "Adam Raskhodnikova",
        "Smith"
      ],
      "year": "2011",
      "venue": "What can we learn privately? SIAM Journal on Computing",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Differentially private language models benefit from public pre-training",
      "authors": [
        "Gavin Kerrigan",
        "Dylan Slack",
        "Jens Tuyls"
      ],
      "year": "2020",
      "venue": "Proceedings of the Second Workshop on Privacy in NLP",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "N-gram-based author profiles for authorship attribution",
      "authors": [
        "Vlado Kešelj",
        "Fuchun Peng",
        "Nick Cercone",
        "Calvin Thomas"
      ],
      "year": "2003",
      "venue": "Proceedings of the conference pacific association for computational linguistics",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Large language models are zero-shot reasoners",
      "authors": [
        "Takeshi Kojima",
        "Shane Shixiang",
        "Machel Gu",
        "Yutaka Reid",
        "Yusuke Matsuo",
        "Iwasawa"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "Mike Lewis",
        "Yinhan Liu",
        "Naman Goyal",
        "Marjan Ghazvininejad",
        "Abdelrahman Mohamed",
        "Omer Levy",
        "Veselin Stoyanov",
        "Luke Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Differentially private imaging via latent space manipulation",
      "authors": [
        "Tao Li",
        "Chris Clifton"
      ],
      "year": "2021",
      "venue": "Differentially private imaging via latent space manipulation",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Large language models can be strong differentially private learners",
      "authors": [
        "Xuechen Li",
        "Florian Tramer",
        "Percy Liang",
        "Tatsunori Hashimoto"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee",
      "authors": [
        "Yuanzhi Li",
        "Sébastien Bubeck",
        "Ronen Eldan"
      ],
      "year": "2023",
      "venue": "Textbooks are all you need ii: phi-1.5 technical report",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "2022a. Differentially private language models for secure data sharing",
      "authors": [
        "Zhijing Justus Mattern",
        "Benjamin Jin",
        "Bernhard Weggenmann",
        "Mrinmaya Schoelkopf",
        "Sachan"
      ],
      "year": "",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "2022b. The limits of word level differential privacy",
      "authors": [
        "Benjamin Justus Mattern",
        "Florian Weggenmann",
        "Kerschbaum"
      ],
      "year": "",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2022",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Mechanism design via differential privacy",
      "authors": [
        "Frank Mcsherry",
        "Kunal Talwar"
      ],
      "year": "2007",
      "venue": "48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07)",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Sentence-level privacy for document embeddings",
      "authors": [
        "Casey Meehan",
        "Khalil Mrini",
        "Kamalika Chaudhuri"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Differential privacy at scale: Uber and Berkeley collaboration",
      "authors": [
        "Joe Near"
      ],
      "year": "2018",
      "venue": "Enigma 2018",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "A picture of search",
      "authors": [
        "Greg Pass",
        "Abdur Chowdhury",
        "Cayley Torgeson"
      ],
      "year": "2006",
      "venue": "Proceedings of the 1st international conference on Scalable information systems",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher D Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "On the challenges of using blackbox apis for toxicity evaluation in research",
      "authors": [
        "Luiza Pozzobon",
        "Beyza Ermis",
        "Patrick Lewis",
        "Sara Hooker"
      ],
      "year": "2023",
      "venue": "On the challenges of using blackbox apis for toxicity evaluation in research",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "An analysis of the user occupational class through twitter content",
      "authors": [
        "Daniel Preoţiuc-Pietro",
        "Vasileios Lampos",
        "Nikolaos Aletras"
      ],
      "year": "2015",
      "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Misleading authorship attribution of source code using adversarial learning",
      "authors": [
        "Erwin Quiring",
        "Alwin Maier",
        "Konrad Rieck"
      ],
      "year": "2019",
      "venue": "28th USENIX Security Symposium (USENIX Security 19)",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "Language models are unsupervised multitask learners",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Exploring limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter J Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Can pseudonymity really guarantee privacy?",
      "authors": [
        "Pankaj Josyula R Rao",
        "Rohatgi"
      ],
      "year": "2000",
      "venue": "USENIX Security Symposium",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Bloom: A 176bparameter open-access multilingual language model",
      "authors": [
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilić",
        "Roman Hesslow",
        "Alexandra Castagné",
        "François Sasha Luccioni",
        "Matthias Yvon",
        "Gallé"
      ],
      "year": "2022",
      "venue": "Bloom: A 176bparameter open-access multilingual language model",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "{A4NT}: Author attribute anonymity by adversarial training of neural machine translation",
      "authors": [
        "Rakshith Shetty",
        "Bernt Schiele",
        "Mario Fritz"
      ],
      "year": "2018",
      "venue": "27th USENIX Security Symposium (USENIX Security 18)",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Convolutional neural networks for authorship attribution of short texts",
      "authors": [
        "Prasha Shrestha",
        "Sebastian Sierra",
        "Fabio A González",
        "Manuel Montes",
        "Paolo Rosso",
        "Thamar Solorio"
      ],
      "year": "2017",
      "venue": "Proceedings of the 15th conference of the European chapter of the association for computational linguistics",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Mpnet: Masked and permuted pretraining for language understanding",
      "authors": [
        "Kaitao Song",
        "Xu Tan",
        "Tao Qin",
        "Jianfeng Lu",
        "Tie-Yan Liu"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "year": "",
      "venue": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Mathematics and the picturing of data",
      "authors": [
        "John W Tukey"
      ],
      "year": "1975",
      "venue": "Proceedings of the International Congress of Mathematicians",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Upton: Unattributable authorship text via data poisoning",
      "authors": [
        "Ziyao Wang",
        "Thai Le",
        "Dongwon Lee"
      ],
      "year": "2022",
      "venue": "Upton: Unattributable authorship text via data poisoning",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Vincent Zhao",
        "Kelvin Guu",
        "Adams Wei Yu",
        "Brian Lester",
        "Nan Du",
        "Andrew M Dai",
        "Quoc V Le"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Paraphrasing with large language models",
      "authors": [
        "Sam Witteveen",
        "Martin Andrews"
      ],
      "year": "2019",
      "venue": "Proceedings of the 3rd Workshop on Neural Generation and Translation",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "Huggingface's transformers: State-ofthe-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Rémi Louf",
        "Morgan Funtowicz"
      ],
      "year": "2019",
      "venue": "Huggingface's transformers: State-ofthe-art natural language processing",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "A comprehensive survey on local differential privacy. Security and Communication Networks",
      "authors": [
        "Xingxing Xiong",
        "Shubo Liu",
        "Dan Li",
        "Zhaohui Cai",
        "Xiaoguang Niu"
      ],
      "year": "2020",
      "venue": "A comprehensive survey on local differential privacy. Security and Communication Networks",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "A differentially private text perturbation method using regularized mahalanobis metric",
      "authors": [
        "Zekun Xu",
        "Abhinav Aggarwal",
        "Oluwaseyi Feyisetan",
        "Nathanael Teissier"
      ],
      "year": "2020",
      "venue": "Proceedings of the Second Workshop on Privacy in NLP",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "Differentially private fine-tuning of language models",
      "authors": [
        "Da Yu",
        "Saurabh Naik",
        "Arturs Backurs",
        "Sivakanth Gopi",
        "Gautam Huseyin A Inan",
        "Janardhan Kamath",
        "Yin Tat Kulkarni",
        "Andre Lee",
        "Lukas Manoel",
        "Wutschitz"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "Paws: Paraphrase adversaries from word scrambling",
      "authors": [
        "Yuan Zhang",
        "Jason Baldridge",
        "Luheng He"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Locally Differentially Private Document Generation Using",
      "text": "Zero Shot Prompting Saiteja Utpala Cohere For AI saitejautpala@gmail.com &Sara Hooker Cohere For AI sarahooker@cohere.com &Pin Yu Chen IBM Research pin-yu.chen@ibm.com"
    },
    {
      "title": "Abstract",
      "text": "Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46% reduction in author identification F1 score against static attackers and a 26% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff. Code is available at [https://github.com/SaitejaUtpala/dp_prompt](https://github.com/SaitejaUtpala/dp_prompt)"
    },
    {
      "title": "1 Introduction",
      "text": "The vast amount of online text data has the potential to reveal numerous user attributes, making individuals easily identifiable Rao et al. (2000); Hovy et al. (2015); Preotiuc-Pietro et al. (2015). While private information can be directly disclosed through specific phrases in the text, it can also be implicitly inferred. For instance, linguistic patterns embedded within the text can inadvertently facilitate authorship attribution Keselj et al. (2003); Shrestha et al. (2017), leading to unintended privacy leakage. An illustrative real-world scenario is the AOL search data leak in August 2006 Pass et al. (2006). The incident unfolded when AOL mistakenly released detailed search logs of their users, wrongly assuming that the data had been adequately anonymized through the use of random user IDs. Unfortunately, the released logs contained sufficient personally identifiable information, leading to the identification of numerous individuals Barbaro and Jr. (2006); Jones et al. (2007). This breach of privacy triggered widespread public outry and led to the initiation of class action lawsuits. This case is just one among many that highlights the limitations of ad-hoc privacy approaches that may give the impression of providing privacy but ultimately fall short. Differential privacy (DP) provides a rigorous treatment for the notion of data privacy by providing plausible deniability by precisely quantifying the deviation in the model's output distribution under modification of a small number of data points Dwork et al. (2006, 2014). The provable guarantees offered by DP, coupled with its compelling properties such as immunity to arbi Figure 1: Overview of the proposed DP-Prompt mechanism. Given a private document (D), DP-Prompt generates a sanitized document (P) while ensuring local differential privacy. The level of privacy guarantee is controlled by adjusting the sampling temperature (T) during the decoding process. trary post-processing and graceful composability, have established it as the de facto standard for privacy. DP has witnessed widespread adoption and numerous deployments in both private (Erlingsson et al., 2014; Apple, 2017; Near, 2018) and public organizations (Abowd, 2018). To address the issue of deanonymization attacks, various approaches have been proposed within the DP framework. These approaches encompass word-level strategies (Feysietan et al., 2020; Xu et al., 2020; Carvalho et al., 2021) where noise is added at the word level, as well as sentence-level techniques (Meehan et al., 2022) where noise is added at the sentence level. However, recent research by (Mattern et al., 2022) has identified limitations in word-level approaches, particularly their disregard for contextual information. To overcome these limitations, Mattern introduced a mechanism that fine-tunes the GPT-2 model (Radford et al., 2019) specifically for paraphrasing tasks, resulting in the generation of sanitized versions of documents. While promising, the approach is limited by their reliance on annotated paraphrasing data, extensive computing resources for larger models, and the quality of annotations. We propose **DP-Prompt**, a novel and straightforward solution to address deanonymization attacks. Our method leverages pretrained large language models by directly prompting them to generate paraphrases. These paraphrases are then released as sanitized documents in a zero-shot manner (See Figure 1). Our motivation for this approach stems from two important factors. Firstly, recent research (Bevendorff et al., 2019; Mattern et al., 2022) has shown that paraphrasing is a robust defense mechanism against deanonymization attacks. Secondly, growing evidence suggests that pretrained large language models can effectively tackle complex tasks without the need for task-specific and expensive fine-tuning (Brown et al., 2020; Chowdhery et al., 2022; Chung et al., 2022; Kojima et al., 2022; OpenAI, 2023), through zero-shot prompting. By harnessing the capabilities of pretrained large language models, DP-Prompt offers a straightforward and powerful solution to mitigate the risk of deanonymization. It provides a promising alternative that can be widely applicable, particularly in the context of on-device large language models where text completion tasks require significantly fewer resources. We summarize the contributions as follows: * We propose DP-Prompt, a new, simple, and computationally effective differentially private (DP) mechanism designed as a defense against de-anonymization attacks. DP-Prompt takes a private document and generates a paraphrased version using zero-shot prompting. The resulting paraphrased document is then released as a sanitized document, as illustrated in Figure 1. * We demonstrate that DP-Prompt, in combination with ChatGPT (gpt-3.5), surpasses all current methods in terms of utility for any level of privacy. Our approach successfully recovers clean sentiment F1 score while significantly reducing the accuracy of author de-anonymization attacks. Refer to Figure 2 for an overview of these results. * To demonstrate the broad applicability of DP-prompt, We conduct extensive experiments with \\(6\\) open source models ranging upto \\(7\\) billion parameters to study the privacy-utiliy tradeoff."
    },
    {
      "title": "2 Preliminaries",
      "text": "A mechanism \\(\\mathcal{M}:\\mathcal{D}\\rightarrow\\mathcal{V}\\) achieves \\(\\epsilon\\)-PureDP if, for all inputs \\(\\mathsf{D},\\mathsf{D}^{\\prime}\\in\\mathcal{D}\\) that differ in one element, and for all \\(V\\subseteq\\text{Range}(\\mathcal{M})\\), \\(\\text{Pr}\\left[\\mathcal{M}(\\mathsf{D})\\in V\\right]\\leq\\exp\\left(\\epsilon \\right)\\text{Pr}\\left[\\mathcal{M}(\\mathsf{D}^{\\prime})\\in V\\right]\\)(Dwork et al., 2006). Figure 2: Overview of the privacy-utility tradeoff with DP-Prompt (using ChatGPT) for the IMDB and Yelp datasets, conducted at a temperature of \\(1.5\\). The terms ‘Static’ and ‘Adaptive’ refer to the attack models defined in Definition 2. Metric Differential Privacy (Metric-DP) [11, 12] is a relaxation of Pure-DP that applies to data represented in a general metric space. For a given distance metric \\(d:\\mathcal{D}\\times\\mathcal{D}\\rightarrow\\mathbb{R}_{+}\\), a mechanism \\(\\mathcal{M}:\\mathcal{D}\\rightarrow\\mathcal{V}\\) achieves \\(\\epsilon d\\)-MetricDP if, for any \\(\\mathrm{D},\\mathrm{D}^{\\prime}\\in\\mathcal{D}\\) and for all \\(V\\subseteq\\text{Range}(\\mathcal{M})\\), \\(\\text{Pr}\\left[\\mathcal{M}(\\mathrm{D})\\in V\\right]\\leq\\exp\\left(d(\\mathrm{D}, \\mathrm{D}^{\\prime})\\right)\\text{Pr}\\left[\\mathcal{M}(\\mathrm{D}^{\\prime})\\in V\\right]\\). Local differential privacy (LDP) [11, 12, 13] is a privacy framework where data is locally perturbed before transmission, considering the presence of an untrusted data collector or server. The formal definition of LDP is as follows: **Definition 1** (PureLDP).: _A randomized mechanism \\(\\mathcal{M}:\\mathcal{D}\\rightarrow\\mathcal{V}\\) is said to be \\(\\epsilon\\)-PureLDP if for any pair of inputs \\(\\mathrm{D},\\mathrm{D}^{\\prime}\\in\\mathcal{D}\\) and for all \\(V\\subseteq\\text{Range}(\\mathcal{M})\\)_ \\[\\text{Pr}[\\mathcal{M}(\\mathrm{D})\\in V]\\leq\\exp\\left(\\epsilon\\right)\\text{Pr} [\\mathcal{M}(\\mathrm{D}^{\\prime})\\in V].\\] There is a growing consensus that, despite the assurance of formal guarantees, it is imperative to subject differentially private mechanisms to robust privacy attacks that simulate strong and malicious adversaries [1, 12]. Such evaluation allows to effectively assess the empirical privacy provided by the mechanism in real-world scenario. To this end we define four attack models depending its adaptivity and mode of access. **Definition 2** (Attack Models).: _Consider a collection of private documents \\((D_{1},\\ldots,D_{n})\\) from distribution \\(\\mathcal{D}\\) with associated author identities \\((a_{1},\\ldots,a_{n})\\) and embeddings \\((E_{1},\\ldots,E_{n})\\sim\\mathcal{E}\\)._ _For text-to-text sanitization using mechanism \\(\\mathcal{M}_{\\text{text}}\\), the sanitized documents are represented as \\((P_{1},\\ldots,P_{n})\\sim\\mathcal{P}_{\\mathcal{M}_{\\text{text}}}\\). For text-to-embedding sanitization via mechanism \\(\\mathcal{M}_{\\text{embedding}}\\), the sanitized embeddings are denoted as \\((N_{1},\\ldots,N_{n})\\sim\\mathcal{N}_{\\mathcal{M}_{\\text{embedding}}}\\)_ * **Static Attacker with Embedding Access**_: Has access to clean documents_ \\((D_{1},\\ldots,D_{n})\\) _but lacks access to sanitized versions_ \\((P_{1},\\ldots,P_{n})\\)_._ * **Static Attacker with Text Access**_: Doesn't have access to sanitized embeddings_ \\((N_{1},\\ldots,N_{n})\\) _but only to the clean embeddings_ \\((E_{1},\\ldots,E_{n})\\)_._ * **Adaptive Attacker with Embedding Access**_: Has access to sanitized embeddings_ \\((N_{1},\\ldots,N_{n})\\)_. Hence, trains a de-anonymization model to adapt to the DP mechanism_ \\(\\mathcal{M}_{\\text{embedding}}\\)_._ * **Adaptive Attacker with Text Access**_: Has access to sanitized text_ \\((P_{1},\\ldots,P_{n})\\)_. Consequently, trains a de-anonymization model to adapt to the DP mechanism_ \\(\\mathcal{M}_{\\text{text}}\\)_._ It is important to note that the adaptive attacker is a more formidable adversary since it adapts to the characteristics of the mechanism \\(\\mathcal{M}\\), whereas the static attacker only has access to clean documents/clean embeddings without any added noise. The mode of access--either raw text or abstracted embeddings--offers further nuances, determining the exact nature of the data an attacker can exploit."
    },
    {
      "title": "3 Dp Prompt",
      "text": "Language models use a decoder network to generate sequential text output. Given a context or prompt represented by a sequence of tokens \\(\\text{C}=(c_{1},\\ldots,c_{m})\\), the language model generates text by sampling tokens from a conditional distribution \\(\\text{Pr}_{|\\text{C}}(x_{1},\\ldots,x_{n})=\\prod_{i=1}^{n}\\text{Pr}_{|\\text{C }}(x_{i}|x_{1},\\ldots,x_{i-1})\\). In this distribution, the logits \\(\\textbf{u}\\in\\mathbb{R}^{|\\mathcal{V}|}\\) are transformed using the softmax function with a temperature \\(T\\), where \\(p_{ij}=\\frac{\\exp(\\frac{u_{ij}}{T})}{\\sum_{j=1}^{|\\mathcal{V}|}\\exp(\\frac{u_{ ij}}{T})}\\), and \\(\\mathcal{V}\\) represents the vocabulary. \\begin{table} \\begin{tabular}{c c c c} \\hline \\hline Mechanism & Privacy Level & Requires fine-tuning & Generates sanitized doc \\\\ \\hline \\hline Madlib [14] & Word level Metric-DP & No & Yes \\\\ Mahanobis [11] & Word level Metric-DP & No & Yes \\\\ TEM [13] & Word level Metric-DP & No & Yes \\\\ Truncated Laplace [12] & Sentence level Pure-DP & No & No \\\\ Deep Candidate [12] & Sentence level Pure-DP & Yes & No \\\\ Paraphraser [13] & Document level Pure-LDP & Yes & Yes \\\\ \\hline DP Prompt (Ours) & Document level Pure-LDP & No & Yes \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: We compare our proposed method, DP-Prompt, with related work on various factors. The “Privacy level” indicates the privacy guarantee provided by each mechanism. “Fine-tuning” denotes whether the mechanism involves fine-tuning a model as an intermediate step. The last column, ”Generates sanitized doc,” indicates whether the mechanism can output a fully sanitized document instead of just sanitized embeddings. This process of sequentially generating text can be regarded as a problem of selecting tokens at each step. Hence, to make the generation step differentially private, one must replace it with a differentially private version of the selection process. One commonly used and well-known differentially private mechanism is the exponential mechanism [13], which is defined as follows: **Definition 3** (Exponential Mechanism).: _Given an utility function \\(\\mathsf{u}:\\mathcal{D}\\times\\mathcal{V}\\rightarrow\\mathcal{V}\\). The exponential mechanism \\(\\mathcal{M}_{\\text{Exp}}:\\mathcal{D}\\rightarrow\\mathcal{V}\\) is a randomized algorithm with output distribution \\(\\mathsf{P}\\left[\\mathcal{M}_{\\text{Exp}}(\\mathsf{D})=v\\right]\\propto\\exp \\left(\\frac{\\mathsf{u}(\\mathsf{D},v)}{2\\Delta u}\\right)\\), where \\(\\Delta\\mathsf{u}=\\max_{\\mathsf{D},\\mathsf{D}^{\\prime},v}|\\mathsf{u}(\\mathsf{D},v)-\\mathsf{u}(\\mathsf{D}^{\\prime},v)|\\) is sensitivity._ In our case, the utility of token \\(v_{j}\\in\\mathcal{V}\\) at each step \\(i\\) is simply the logit \\(u_{ij}\\in\\mathbb{R}\\). Hence, one can make text generation differentially private using the exponential mechanism. Extensive research has shown that paraphrasing documents helps conceal author identity [14, 15, 16]. Considering recent advancements where tasks are formulated as prompts and language models are tasked to complete them [13, 15, 14], we directly prompt the language model to generate paraphrases. Therefore, given a private document D and a specific prompt template instructing the language model to generate a paraphrase, such as \\(\\mathsf{T}:=\\) \"Paraphrase of the document:\" we combine \\(\\mathsf{D}\\) and \\(\\mathsf{T}\\) to create a context \\(\\mathsf{C}\\). By utilizing this context, we execute the text generation procedure in a differentially private manner to produce a paraphrase. We refer to this procedure as DP-Prompt. Algorithm 1 outlines the specific steps of our proposed DP-Prompt, and the code for implementing DP-Prompt in HuggingFace [11] is provided in Appendix B. The formal guarantee of achieving \\(\\epsilon\\)-PureLDP is provided by the following theorem: **Theorem 1**.: _Suppose the language model has not been pretrained on the private documents distribution \\(\\mathcal{D}\\). If the final logits \\(\\mathbf{u}\\in\\mathbb{R}^{|\\mathcal{V}|}\\) satisfy the condition \\(b_{1}\\leq u_{i}\\leq b_{2},\\forall i,\\) and the DP-Prompt run with a temperature \\(T\\) for generating \\(n\\) tokens, then it can be proven that the generated output satisfies \\((2n(b_{2}-b_{1})/T)\\)-LDP._ See the Appendix A for the proof. ``` Input: language model (LM), private document (D), prompt template (T), clipping vector \\(\\mathbf{b}\\in\\mathbb{R}^{|\\mathcal{V}|}\\), temperature \\(T\\in\\mathbb{R}_{+}\\), paraphrase tokens \\(\\mathbf{n}\\). Output: Sanitized Doc (P) 1P \\(\\leftarrow\\) [], C \\(\\leftarrow\\) GeneratePrompt(D,T) 2C\\(\\leftarrow\\) Tokenize(C) 3for\\(i\\gets 1\\)to\\(n\\)do 4\\(\\mathbf{u}\\leftarrow\\) LM(C\\({}_{\\text{tokens}}\\)) 5\\(\\mathbf{u}^{\\prime}\\leftarrow\\) ClipAndScale(\\(\\mathbf{u},\\mathbf{b},T\\)) 6\\(\\mathbf{p}\\leftarrow\\) ConvertToProbabilities(\\(\\mathbf{u}^{\\prime}\\)) 7\\(v\\leftarrow\\) SampleToken(\\(\\mathbf{p}\\)) 8\\(\\mathbf{p}\\leftarrow\\) P\\(\\cup\\) [\\(v\\)], C\\({}_{\\text{tokens}}\\leftarrow\\) C\\({}_{\\text{tokens}}\\cup[v]\\) 9 end for 10P \\(\\leftarrow\\) Detokenize(P) returnP ``` **Algorithm 1**DP-Prompt"
    },
    {
      "title": "4 Experiments",
      "text": ""
    },
    {
      "title": "Experiment Setup",
      "text": "**Evaluation:** Note that we are comparing DP-mechanisms with different levels of differential privacy. Therefore, in our experiments, we focus on evaluating the empirical privacy rather than the theoretical privacy\\((\\epsilon)\\) for effective and realistic as Figure 3: Sample illustration of clean and sanitized documents for various mechanisms. For ChatGPT, prompt is “Review:[review]Paraphrase of the Review:” where review is the clean review. sessment. As a result, we plot the author identification F1 score, which is calculated by conducting de-anonymization attacks on the sanitized documents. This score indicates the potential for privacy breaches. On the other hand, the y-axis represents the sentiment F1 score, which measures the utility of the sanitized documents. **Datasets:** We conduct experiments using IMDB movie reviews and Yelp business reviews, both of which contain author and sentiment labels. The IMDB dataset has a size of 15,000, while the Yelp dataset has 17,336 samples. For both datasets, sentiment analysis is a 2-class classification task, and the author identification task is a 10-class classification task. **Implementation Details:** For the embedding-level attacker, we utilize 3-Layer MLPs with ReLU activation functions and train them on sentence embeddings (Reimers and Gurevych, 2019). For the text-level attacker, we fine-tune BERT (Devlin et al., 2018). More details can be found in Appendix C. Regarding the static attacker, the clean set of documents is used for training and validation, while the sanitized documents serve as the test set. On the other hand, for the adaptive attacker, all three sets (training, validation, and testing) consist of sanitized documents. * For each of word level mechanisms, (Madlib (Feysietan et al., 2020), Mahalanobis (Xu et al., 2020), TEM (Carvalho et al., 2021)) we run the mechanisms for \\(8\\)\\(\\epsilon\\)'s given \\(\\epsilon=\\{2,5,8,11,14,17,20,25\\}\\) * For each of sentence level mechanisms (Truncated-Laplace (Meehan et al., 2022), Deep-Candidate (Meehan et al., 2022)): we run the mechanisms for \\(11\\)\\(\\epsilon\\)'s given by \\(\\epsilon=\\{5,10,20,30,40,50,75,100,150,200\\}\\). * For Paraphraser (Mattern et al., 2022) and DP-Prompt with open source models we run decoding at \\(5\\) temperatures \\(\\{0.75,1.0,1.25,1.5,1.75\\}\\). For DP-Prompt we run ChatGPT at temperatures \\(\\{1.0,1.25,1.5,1.75,2.0\\}\\). Further we also consider F1 scores on Clean (without noise added) embeddings/documents and performance of uniformly random classifier (for more details, refer to Appendix C.6)."
    },
    {
      "title": "Dp-Prompt With Chatgpt (Gpt-3.5)",
      "text": "In this section we compare \\(6\\) baselines (Madlib, Mahalanobis, Tem, Truncated-laplace, Deep-candidate, Paraphraser) run with configurations above (for more details also refer to Appendix C) with DP-Prompt with ChatGPT. Except for DP-Prompt, we run each mechanism to \\(3\\) times to produce \\(3\\) different sanitized documents and plot mean author F1 identification score on x-axis and show \\(2\\sigma\\) band around mean sentiment F1 score. Results are show in Figure 4 The results clearly demonstrate the superior performance of DP-Prompt with ChatGPT (GPT-3.5). Figure 4: Comparison of DP-Prompt (with ChatGPT) with various baselines. The top row shows results for an attacker with embedding access, while the row below presents results for an attacker with text access. Notably, it is evident that regardless of the chosen privacy level, DP-Prompt, when utilized with ChatGPT (GPT-3.5), exhibits significantly better utility compared to all baseline mechanisms. Notably, DP-Prompt exhibits significantly higher utility on the y-axis for a chosen empirical privacy value on the x-axis. All word-level mechanisms show a similar privacy-utility tradeoff. Regarding sentence-level mechanisms, the truncated Laplace mechanism performs decently, while in the static attack experiments, Deep-candidate is reduced to a random classifier due to the distribution shift caused by sentence recoding. Furthermore, in the case of clean reviews (i.e., without any noise), the embedding-level attacker can accurately identify the author among 10 different options with a high F1 score of 0.93 in IMDB and 0.86 in Yelp. However, when DP-Prompt is employed, the sentiment F1 scores remain unchanged, while the author identification scores decrease by 46% and 25% in the case of IMDB, and 53% and 29% in the case of Yelp. The text-level models are more accurate than the embedding-level models, with author identification scores of 0.99 (as opposed to 0.93) and 0.97 (as opposed to 0.86) in IMDB and Yelp, respectively, for clean reviews. When DP-Prompt is employed, the sentiment F1 scores remain unchanged, while the author identification scores decrease by 54% and 10% in the case of IMDB, and 73% and 24% in the case of Yelp. This illustrates that text-level attackers are more powerful."
    },
    {
      "title": "Dp-Prompt With Open Source Models",
      "text": "It is important to note that models such as ChatGPT (gpt-3.5) are proprietary and can only be accessed through APIs, necessitating the uploading of user documents to the language model provider. Although DP-Prompt with such proprietary models provide LDP guarantee, this defeats the fundamental motivation of LDP, which is to achieve privacy guarantees without relying on a trusted data curator. The objective of the experiments presented in the preceding section aim to demonstrate that DP-Prompt, when combined with a powerful language model like ChatGPT, can outperform existing methods by a significant margin. Considering the increasing interest in building high-quality open-source large language models (Scao et al., 2022; Black et al., 2022; Touvron et al., 2023; Li et al., 2023; Jiang et al., 2023), we expand our evaluation of DP-Prompt to include six open-source models, ranging in size up to seven billion parameters. Our evaluation takes into account two factors: (i) architecture, where we consider both encoder-decoder and decoder-only models, and (ii) the level of fine-tuning, including models that are fine-tuned using instructions and/or Reinforcement Learning with Human Feedback (RLHF). Specifically 6 models are as follows, _Base_: T5 (3b), Stable lm base (3b, 7b),_Instruction finetuned/RLHF tuned:_ Flan T5 (3b), Stable lm tuned (3b,7b). While T5, Flan T5 are encoder-decoder models, rest of the models are decoder-only. In contrast to DP-Prompt with ChatGPT, we perform DP-Prompt using the aforementioned open source language models three times for each dataset and temperature, resulting in three sanitized documents. Fur Figure 5: Illustration of privacy-utility tradeoff in DP-Prompt with open source models and ChatGPT(gpt-3.5). The top row shows results for an attacker with embedding access, while the row below presents results for an attacker with text access. ther we all run these open source models at half precision (torch.float16) and with max number of tokens in paraphrase to \\(150\\). The obtained results are presented in Figure 5. Now we compare open source models along various factors **Base vs Instruction/RLHF tuned:** It is evident that the base models perform poorly in comparison to the models that underwent instruction fine-tuning/RLHF tuning. One important point to mention is that both StableLM-Based (3b, 7b) perform significantly worse against adaptive attackers. **Scale:** We observe that scale plays a key role, as StableLM-tuned (7b) demonstrates better utility than StableLM-Tuned (3b) across all levels of empirical privacy, with ChatGPT outperforming both. **Variance:** It is worth noting that we did not obtain variance for DP-Prompt with ChatGPT in Figure 4 through multiple runs. However, our analysis in Figure 5 suggests that there is no significant variance in the Sentiment F1 score, at least for open source models. **Encoder-Decoder / Decoder only:** Flan-T5(3b), an encoder-decoder model, outperforms the larger Stablelm-Tuned (7b) model. Notably, Flan-T5(3b) is fine-tuned exclusively on academic datasets, resulting in shorter paraphrases compared to Stablelm-Tuned (7b). **ChatGPT vs Rest:** While the open-source models we considered demonstrate competitiveness with ChatGPT, a notable gap remains. The key finding is that even at a higher temperature of 1.5, ChatGPT is capable of recovering a clean sentiment F1 score, while none of the open-source models can achieve a matching clean sentiment F1 score, even at a significantly lower temperature of 0.75. See Figure 7, which presents the paraphrases output by ChatGPT and Stablelm-Tuned (7B)."
    },
    {
      "title": "Effect Of Clipping Logits",
      "text": "In both sections (Section 4.2 and Section 4.3) DP-Prompt is run without logit clipping. This is primarily because ChatGPT doesn't expose logits, and for a fair comparison with ChatGPT, we didn't clip logits for open-source models in Section 4.3. However, the LDP guarantee still holds because, in practice, logits are always bounded within a certain precision, even though they may have large values. In this section, we examine the impact of logit clipping on the tradeoff between privacy and utility using FlanT5(3b) and Stablelm Tuned(7b) models against embedding-level attacks. We adopt the approach of (Li and Clifton, 2021) by learning the clipping boundaries on additional data (more details can be found in Appendix C). The results of this analysis are presented in Table 2. The maximum difference observed with and without clipping occurs for Stablelm-Tuned(7b) on the IMDB dataset at a temperature of \\(0.75\\). In this case, the sentiment F1 score drops from \\(0.71\\) to \\(0.65\\), and the author identification F1 score drops from \\(0.53\\) to \\(0.45\\). Based on these findings, we recommend using logit clipping when higher privacy is required and not using clipping when higher utility is desired."
    },
    {
      "title": "Effect Of Top-K Sampling",
      "text": "For the differential privacy guarantee to hold, sampling from probabilities must be done over the entire vocabulary according to score probabilities. While it is common to use top-k sampling in practice to improve generation (the default value in Hugging Face (Wolf et al., 2019) is 40). It is important to note that the ChatGPT chat completion API does not include a top-k parameter. In this section, using open-source models, we examine the impact of top-k sampling on utility and investigate whether it provides any empirical privacy, even in the _absence_ of the differential privacy guarantee. In addition, we aim to assess whether top-k sampling can effectively help open source models to recover the clean sentiment F1-score and narrow the gap compared to DP-Prompt when used with ChatGPT. We consider Flan-T5(3b) and Stablelm \\begin{table} \\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \\hline \\multirow{3}{*}{} & \\multicolumn{3}{c|}{**Image**} & \\multicolumn{3}{c|}{**Image**} & \\multicolumn{3}{c|}{**Image**} & \\multicolumn{3}{c|}{**Image**} & \\multicolumn{3}{c|}{**Image**} \\\\ \\cline{3-15} & \\multicolumn{3}{c|}{**Metrics**} & \\multicolumn{3}{c|}{**Nonlinear F1 score} & \\multicolumn{3}{c|}{**Additive Randomization F1 score} & \\multicolumn{3}{c|}{**Softening F1 score} & \\multicolumn{3}{c|}{**Additive Randomization F1 score**} \\\\ \\cline{2-15} & \\multicolumn{3}{c|}{**Metrics**} & **Origes** & **4.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** & **1.5** \\\\ \\hline \\multirow{3}{*}{**Embed**} & **Rank** & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ \\cline{2-15} & **Multi** & 0.72 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ \\cline{2-15} & **Multi** & 0.72 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ \\cline{2-15} & **Algebra** & 0.75 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ \\cline{2-15} & **Algebra** & 0.75 & 0.7 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0 [MISSING_PAGE_FAIL:8] selection problem Carvalho et al. (2021). Furthermore, there is a recent development known as CusText Chen et al. (2023), which focuses on developing customized mapping mechanisms for each individual word in the vocabulary Chen et al. (2023). All of these approaches are word-level mechanisms and have been shown to have significant limitations, such as their disregard for contextual information Mattern et al. (2022). **Sentence-level Approaches**: Sentence-level mechanisms based on Sentence Transformer Reimers and Gurevych (2019) were introduced in Maehan et al. (2022). They proposed two approaches: one approach where noise is added to sentence embeddings, and another more complicated approach based on maximizing Tukey depth Tukey (1975); Gilad-Bachrach and Burges (2012). **Document-level Approaches**: A document-level Local Differential Privacy (LDP) mechanism was introduced, where GPT-2 is fine-tuned for a paraphrasing task Mattern et al. (2022). Our approach, DP-Prompt, draws inspiration from their work, but instead of resource-intensive fine-tuning, we use a zero-shot approach with pretrained models for efficient and effective generation of sanitized documents. Furthermore, the recently proposed DP-BART Igamberdiev and Habernal (2023) employs BART Lewis et al. (2020), an encoder-decoder model. In DP-BART, noise is added to the encoder's output, and the decoder is fine-tuned to adapt to this noisy encoder output. **Adversarial Methods:** Parallel to differentially private approaches, other techniques have been proposed that utilize Adversarial Learning Shetty et al. (2018); Quiring et al. (2019) and Data Poisoning Wang et al. (2022); Jin et al. (2020). However, these methods typically require access to a surrogate classifier. In contrast, our method is zero-shot, requiring neither fine-tuning nor access to a classifier. **Differentially Private Training/Fine Tuning:** There is extensive research on differentially private training or fine-tuning of language models Kerrigan et al. (2020); Li et al. (2021); Yu et al. (2021); Anil et al. (2022); Mattern et al. (2022). They aim to make language models resistant to various kinds of data leakage attacks Carlini et al. (2019, 2021); Deng et al. (2021); Balunovic et al. (2022). It is important to emphasize that this line of work is completely distinct from our own, as it focuses on training language models on private data, while our goal is to generate sanitized documents from private documents using pretrained language models."
    },
    {
      "title": "7 Conclusion",
      "text": "This paper introduces DP-Prompt, a locally differentially mechanism called DP-Prompt that generates sanitized versions of private documents by prompting large language models to generate paraphrases. Notably, our method offers a simpler approach compared to existing methods. Through extensive experiments, we show that our approach achieves significantly improved utility compared to current methods for any required level of privacy. As the demand for on-device large language models (LLMs) continues to grow, our method emerges as a reliable safeguard for users' privacy and provides robust defense against de-anonymization attacks."
    },
    {
      "title": "8 Limitations",
      "text": "In our study, we explored the initial step of harnessing large language models and zero-shot prompting to generate sanitized documents. While this approach effectively conceals the specific writing style of authors, there is still a potential risk of revealing explicit personal information, such as zip codes, bank details, or gender, especially when naively prompting a language model. This risk is particularly relevant in alternative text formats like messages or emails compared to online reviews. For future work, an important direction would be to define a set of sensitive attributes and directly prompt the language model to replace these attributes with the identifier \"X\" while generating paraphrases. This approach would help improve the safeguarding of personal information. Additionally, it would be worthwhile to investigate the potential side effects of hallucination and the impact of different prompt templates on the generation of paraphrases, specifically within the context of the privacy-utility tradeoff. Additionally, more robust attacks that measure privacy leakage at the text level should be explored."
    },
    {
      "title": "References",
      "text": "* M. Abowd (2018)The US census bureau adopts differential privacy. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2867-2867. Cited by: SS1. * M. E. Andres, N. E. Bordenabe, K. Chatzikokolakis, and C. Palamidessi (2013)Geo-indistinguishability: Differential privacy for location-based systems. In _Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security_, pages 901-914. * Anil et al. (2022) Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. 2022. Large-scale differentially private bert. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 6481-6491. * Apple (2017) D Apple. 2017. Learning with privacy at scale. _Apple Machine Learning Journal_, 1(8). * Balunovic et al. (2022) Mislav Balunovic, Dimitar Iliev Dimitrov, Nikola Jovanovic, and Martin Vechev. 2022. Lamp: Extracting text from gradients with language model priors. In _Advances in Neural Information Processing Systems_. * Barbaro and Jr. (2006) Michael Barbaro and Tom Zeller Jr. 2006. A face is exposed for aol searcher no. 4417749. _New York Times_. * Bevendorff et al. (2019) Janek Bevendorff, Martin Potthast, Matthias Hagen, and Benno Stein. 2019. Heuristic authorship obfuscation. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 1098-1108. * Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. _arXiv preprint arXiv:2204.06745_. * Blanco-Justicia et al. (2022) Alberto Blanco-Justicia, David Sanchez, Josep Domingo-Ferrer, and Krishnamurty Muralidhar. 2022. A critical review on the use (and misuse) of differential privacy in machine learning. _ACM Computing Surveys_, 55(8):1-16. * Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901. * Carlini et al. (2019) Nicholas Carlini, Chang Liu, Ulfar Erlingsson, Jernej Kos, and Dawn Song. 2019. The secret sharer: Evaluating and testing unintended memorization in neural networks. In _USENIX Security Symposium_, volume 267. * Carlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, et al. 2021. Extracting training data from large language models. In _USENIX Security Symposium_, volume 6. * Carvalho et al. (2021) Ricardo Silva Carvalho, Theodore Vasiloudis, and Oluwaseyi Feyisetan. 2021. Tem: high utility metric differential privacy on text. _arXiv preprint arXiv:2107.07928_. * Chatzikokolakis et al. (2013) Konstantinos Chatzikokolakis, Miguel E Andres, Nicolas Emilio Bordenabe, and Catuscia Palamidessi. 2013. Broadening the scope of differential privacy using metrics. In _Privacy Enhancing Technologies: 13th International Symposium, PETS 2013, Bloomington, IN, USA, July 10-12, 2013. Proceedings 13_, pages 82-102. Springer. * Chen et al. (2023) Sai Chen, Fengran Mo, Yanhao Wang, Cen Chen, Jian-Yun Nie, Chengyu Wang, and Jamie Cui. 2023. A customized text sanitization mechanism with differential privacy. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 5747-5758. * Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_. * Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_. * Deng et al. (2021) Jieren Deng, Yijue Wang, Ji Li, Chenghong Wang, Chao Shang, Hang Liu, Sanguthevar Rajasekaran, and Caiwen Ding. 2021. Tag: Gradient attack on transformer-based language models. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 3600-3610. * Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_. * Duchi et al. (2013) John C Duchi, Michael I Jordan, and Martin J Wainwright. 2013. Local privacy and statistical minimax rates. In _2013 IEEE 54th Annual Symposium on Foundations of Computer Science_, pages 429-438. IEEE. * Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrating noise to sensitivity in private data analysis. In _Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3_, pages 265-284. Springer. * Dwork et al. (2014) Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407. * Erlingsson et al. (2014) Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. 2014. Rappor: Randomized aggregatable privacy-preserving ordinal response. In _Proceedings of the 2014 ACM SIGSAC conference on computer and communications security_, pages 1054-1067. * Feyisetan et al. (2020) Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, and Tom Diethe. 2020. Privacy-and utility-preserving textual analysis via calibrated multivariate perturbations. In _Proceedings of the 13th International Conference on Web Search and Data Mining_, pages 178-186. Ran Gilad-Bachrach and Chris J.C. Burges. 2012. The median hypothesis. Technical Report MSR-TR-2012-56. * Hovy et al. (2015) Dirk Hovy, Anders Johannsen, and Anders Sogaard. 2015. User review sites as a resource for large-scale sociolinguistic studies. In _Proceedings of the 24th international conference on World Wide Web_, pages 452-461. * Igamberdiev and Habernal (2023) Timour Igamberdiev and Ivan Habernal. 2023. DP-BART for privatized text rewriting under local differential privacy. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 13914-13934, Toronto, Canada. Association for Computational Linguistics. * Jayaraman and Evans (2019) Bargav Jayaraman and David Evans. 2019. Evaluating differentially private machine learning in practice. In _USENIX Security Symposium_. * Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. _arXiv preprint arXiv:2310.06825_. * Jin et al. (2020) Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 8018-8025. * Jones et al. (2007) Rosie Jones, Ravi Kumar, Bo Pang, and Andrew Tomkins. 2007. I know what you did last summer: query logs and user privacy. In _Proceedings of the sixteenth ACM conference on Conference on information and knowledge management_, pages 909-914. * Kasiviswanathan et al. (2011) Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. 2011. What can we learn privately? _SIAM Journal on Computing_, 40(3):793-826. * Kerrigan et al. (2020) Gavin Kerrigan, Dylan Slack, and Jens Tuyls. 2020. Differentially private language models benefit from public pre-training. In _Proceedings of the Second Workshop on Privacy in NLP_, pages 39-45. * Keselj et al. (2003) Vlado Keselj, Fuchun Peng, Nick Cercone, and Calvin Thomas. 2003. N-gram-based author profiles for authorship attribution. In _Proceedings of the conference pacific association for computational linguistics, PACLING_, volume 3, pages 255-264. * Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In _Advances in Neural Information Processing Systems_. * Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7871-7880. * Li and Clifton (2021) Tao Li and Chris Clifton. 2021. Differentially private imaging via latent space manipulation. _arXiv preprint arXiv:2103.05472_. * Li et al. (2021) Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. 2021. Large language models can be strong differentially private learners. In _International Conference on Learning Representations_. * Li et al. (2023) Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks are all you need ii: phi-1.5 technical report. _arXiv preprint arXiv:2309.05463_. * Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_. * Mattern et al. (2022a) Justus Mattern, Zhijing Jin, Benjamin Weggenmann, Bernhard Schoelkopf, and Mrinmaya Sachan. 2022a. Differentially private language models for secure data sharing. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 4860-4873. * Mattern et al. (2022b) Justus Mattern, Benjamin Weggenmann, and Florian Kerschbaum. 2022b. The limits of word level differential privacy. In _Findings of the Association for Computational Linguistics: NAACL 2022_, pages 867-881. * McSherry and Talwar (2007) Frank McSherry and Kunal Talwar. 2007. Mechanism design via differential privacy. In _48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07)_, pages 94-103. IEEE. * Meehan et al. (2022) Casey Meehan, Khalil Mrini, and Kamalika Chaudhuri. 2022. Sentence-level privacy for document embeddings. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3367-3380. * Near (2018) Joe Near. 2018. Differential privacy at scale: Uber and Berkeley collaboration. In _Enigma 2018 (Enigma 2018)_. * OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_. * Pass et al. (2006) Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A picture of search. In _Proceedings of the 1st international conference on Scalable information systems_, pages 1-es. * Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_, pages 1532-1543. * Pennington et al. (2015)Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker. 2023. On the challenges of using black-box apis for toxicity evaluation in research. _arXiv preprint arXiv:2304.12397_. * Preotiuc-Pietro et al. (2015) Daniel Preotiuc-Pietro, Vasileios Lampos, and Nikolaos Aletras. 2015. An analysis of the user occupational class through twitter content. In _Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 1754-1764. * Quiring et al. (2019) Erwin Quiring, Alwin Maier, and Konrad Rieck. 2019. Misleading authorship attribution of source code using adversarial learning. In _28th USENIX Security Symposium (USENIX Security 19)_, pages 479-496. * Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. * Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551. * Rao et al. (2000) Josyula R Rao, Pankaj Rohatgi, et al. 2000. Can pseudonymity really guarantee privacy? In _USENIX Security Symposium_, pages 85-96. * Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3982-3992. * Le Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_. * Shetty et al. (2018) Rakshith Shetty, Bernt Schiele, and Mario Fritz. 2018. {A4NT}: Author attribute anonymity by adversarial training of neural machine translation. In _27th USENIX Security Symposium (USENIX Security 18)_, pages 1633-1650. * Shrestha et al. (2017) Prasha Shrestha, Sebastian Sierra, Fabio A Gonzalez, Manuel Montes, Paolo Rosso, and Thamar Solorio. 2017. Convolutional neural networks for authorship attribution of short texts. In _Proceedings of the 15th conference of the European chapter of the association for computational linguistics: Volume 2, short papers_, pages 669-674. * Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. Mpnet: Masked and permuted pretraining for language understanding. _Advances in Neural Information Processing Systems_, 33:16857-16867. * Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_. * Tukey (1975) John W Tukey. 1975. Mathematics and the picturing of data. In _Proceedings of the International Congress of Mathematicians, Vancouver, 1975_, volume 2, pages 523-531. * Wang et al. (2022) Ziyao Wang, Thai Le, and Dongwon Lee. 2022. Upton: Unattributable authorship text via data poisoning. _arXiv preprint arXiv:2211.09717_. * Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In _International Conference on Learning Representations_. * Witteveen and Andrews (2019) Sam Witteveen and Martin Andrews. 2019. Paraphrasing with large language models. In _Proceedings of the 3rd Workshop on Neural Generation and Translation_, pages 215-220. * Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. _arXiv preprint arXiv:1910.03771_. * Xiong et al. (2020) Xingxing Xiong, Shubo Liu, Dan Li, Zhaohui Cai, and Xiaoguang Niu. 2020. A comprehensive survey on local differential privacy. _Security and Communication Networks_, 2020:1-29. * Xu et al. (2020) Zekun Xu, Abhinav Aggarwal, Oluwaseyi Fejisetan, and Nathanael Teissier. 2020. A differentially private text perturbation method using regularized malandanobis metric. In _Proceedings of the Second Workshop on Privacy in NLP_, pages 7-17. * Yu et al. (2021) Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. 2021. Differentially private fine-tuning of language models. In _International Conference on Learning Representations_. * Zhang et al. (2019) Yuan Zhang, Jason Baldridge, and Luheng He. 2019. Paws: Paraphrase adversaries from word scrambling. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 1298-1308."
    },
    {
      "title": "Appendix A Proof Of Privacy",
      "text": "See 2 See 2 See 2 See 3 See 4 See 5 See 6 See 7 See 8 See 9 See 10 See 11 See 12 See 13 See 14 See 15 See 16 See 17 See 18 See 19 See 20 See 21 See 22 See 23 See 24 See 25 See 26 See 27 See 28 See 29 See 28 See 29 See 30 See 31 See 32 See 33 See 34 See 35 See 36 See 37 See 38 See 39 See 38 See 39 See 40 See 41 See 42 See 43 See 44 See 45 See 46 See 47 See 48\\([|\\mathcal{V}|]\\), and the DP-Prompt run with a temperature \\(T\\) for generating \\(n\\) tokens, then it can be proven that the generated output satisfies \\((n(b_{2}-b_{1})/T)\\)-LDP. Proof.: Let \\(\\mathcal{D}\\) and \\(\\mathcal{D}^{\\prime}\\) be any two documents, and \\(\\mathbf{u}\\) and \\(\\mathbf{u}^{\\prime}\\in\\mathbb{R}^{|\\mathcal{V}|}\\) be their corresponding logits. Let \\(v\\in\\mathcal{V}\\), and \\(k\\) be its index, with \\(u_{k}\\) being its corresponding logit. We then have that, \\[\\frac{\\text{Pr}[\\mathcal{M}(\\text{D})=v]}{\\text{Pr}[\\mathcal{M}( \\text{D}^{\\prime})=v]} =\\frac{\\frac{\\exp(\\frac{u_{k}}{\\mathcal{D}})}{\\sum_{j=1}^{| \\mathcal{V}|}\\exp(\\frac{u_{j}^{\\prime}}{T})}}{\\frac{\\exp(\\frac{u_{j}^{\\prime} }{T})}{\\sum_{j=1}^{|\\mathcal{V}|}\\exp(\\frac{u_{j}^{\\prime}}{T})}}\\] \\[=\\exp\\left(\\frac{u_{k}-u_{k}^{\\prime}}{T}\\right)\\frac{\\sum_{j=1}^ {|\\mathcal{V}|}\\exp(\\frac{u_{j}^{\\prime}}{T})}{\\sum_{j=1}^{|\\mathcal{V}|}\\exp( \\frac{u_{j}}{T})}\\] \\[\\leq\\exp\\left(\\frac{b_{2}-b1}{T}\\right)\\exp\\left(\\frac{b_{2}-b1} {T}\\right)\\] \\[\\leq\\exp\\left(2(b_{2}-b_{1})/T\\right).\\] Now by using sequential composition law of DP (Dwork et al., 2006), we can set \\(\\epsilon=(2n(b_{2}-b_{1})/T)\\) to conclude the proof."
    },
    {
      "title": "Appendix B Code",
      "text": "In this section, we showcase the code for DP-Prompt implemented using the Hugging Face library (Wolf et al., 2019). The code can be found in Listing 1."
    },
    {
      "title": "Appendix C Implementation Details And Additional Experiments",
      "text": ""
    },
    {
      "title": "Word-Level Mechanisms",
      "text": "For all word-level mechanisms, we utilize 50-dimensional glove embeddings (Pennington et al., 2014), following the approach of Mattern et al. (Mattern et al., 2022). The projection step, which requires approximate nearest neighbor search, is performed using an Annoy indexer with 500 trees."
    },
    {
      "title": "Sentence-Level Mechanisms",
      "text": "Both the Truncated-Laplace and Deep-Candidate mechanisms require additional publicly available data. In the case of Truncated-Laplace, this data is used to determine truncated boundaries, while for Deep-Candidate, it is used to train the sentence recoder and obtain the output. We randomly sample 5,000 documents from both the IMDB and Yelp reviews, which are not part of the data used for privacy-utility experiments. The sentence recoder architecture is trained with three layers of MLP, incorporating dropout and selecting the best model. We choose 50 clusters for sentence recoding and employ 100 random projections to estimate the approximate Tukey depth. Sentence embeddings (Reimers and Gurevych, 2019) of dimension 768 are obtained from (Song et al., 2020)."
    },
    {
      "title": "Document-Level Mechanisms",
      "text": "For the paraphrasing mechanism, we fine-tune the gprt2-xl(1.5b) parameter model on the PAWS dataset (Zhang et al., 2019). The training set is constructed by combining the train and val sets, and the test set is used for validation to save the best model. The training set consists of 25,368 examples, and the validation set consists of 3,536 examples. We follow the procedure outlined in (Mattern et al., 2022), which builds upon (Witteveen and Andrews, 2019). To set clip thresholds in Section 4.4, we employ a process similar to Truncated-Laplace (Meehan et al., 2022) with a slight difference. While (Meehan et al., 2022) calculates the \\(75\\%\\) quantile and utilizes it as the clipping threshold, we calculate the minimum and maximum values.Both Truncated-Laplace and Dp-Prompt employ the exact same additional data. The resulting min and max clip threshold vector is then used to clip the logits before scaling them by temperature."
    },
    {
      "title": "Static And Adaptive Attacker Architecture",
      "text": "Word level and document level output documents, to simulate embedding-level attacker we employ sentence transformer all-mpnet-base-v2 (Reimers and Gurevych, 2019; Song et al., 2020) to convert sanitized document to sanitized embedding. For sentence level, directly sanitized embeddings are used. The embedding-level attackers employ a three-layer MLP with a hidden dimension of 768. We use the ReLU activation function and incorporate dropout of 0.5. The models are trained for 50 epochs with a batch size of 32, using the Adam optimizer with a StepLR learning scheduler. The initial learning rate is set to \\(10^{-3}\\), and the gamma value is 0.95. The text-level attackers use bert-base-cased and fine-tune it for 3 epochs with a batch size of 16, using the AdamW optimizer with a linear scheduler and a starting learning rate of \\(5\\times 10^{-5}\\)."
    },
    {
      "title": "Code And Reproducibility",
      "text": "The code will be publicly released. Considering the reproducibility challenges associated with closed APIs (Pozzobon et al., 2023), we also plan to release the paraphrased documents that were generated using ChatGPT."
    },
    {
      "title": "Expected F1 Score Of Random Classifier",
      "text": "Let \\(p_{i}\\) represent the fraction of documents with label \\(y_{i}\\), where \\(i\\) ranges from \\(0\\) to \\(\\ell-1\\). A uniformly random classifier predicts class \\(y_{i}\\) with a probability of \\(1/\\ell\\). Based on this setup, we can derive the following metrics: \\[\\text{True Positives(TP)} =\\frac{p_{i}}{\\ell}\\] \\[\\text{True Negatives(TN)} =\\frac{(1-p_{i})(\\ell-1)}{\\ell}\\] \\[\\text{False Positives(FP)} =\\frac{1-p_{i}}{\\ell}\\] \\[\\text{False Negatives(FN)} =\\frac{p_{i}(\\ell-1)}{\\ell}\\] From these metrics, we can calculate the F1 Score for sentiment analysis, which is a binary classification task, as follows: \\[\\text{F1 Score}=\\frac{\\text{TP}}{\\text{TP}+\\frac{1}{2}\\left(\\text{FP}+\\text{ FN}\\right)}=\\frac{2p_{1}}{1+p_{1}\\ell}\\] For author identification, which is a multi-class classification task, we utilize the F1 Score with a macro average. In the case of a random classifier, the expected F1 score can be calculated as:\\[\\text{F1 Score}_{\\text{macro avg}}=\\frac{1}{\\ell}\\sum_{i=0}^{\\ell-1}\\frac{2p_{i}}{1 +p_{i}\\ell}\\] By employing this approach, we can effectively evaluate the performance of a random classifier for author identification in terms of the F1 Score."
    },
    {
      "title": "Appendix D Additional Results",
      "text": ""
    },
    {
      "title": "Comparing 50 And 300-Dimensional Glove Embeddings For Word-Level Mechanism",
      "text": "Note that in Section 4.2, we used 50-dimensional Glove embeddings (glove-wiki-gigaword-50) (Pennington et al., 2014) for Madlib, Mahalanobis, and TEM mechanisms. In this section, we demonstrate that there is no significant benefit to using 300-dimensional glove embeddings (glove-wiki-gigaword-300). We show this for IMDB dataset against embedding-level attackers. Results are show in Figure 6."
    },
    {
      "title": "Bert Score For Paraphrasing Models",
      "text": "We use BERTScore, which has been demonstrated to correlate with human judgments, in conjunction with the RoBerta model (Liu et al., 2019) (roberta-large) to assess the similarity between a review and its paraphrase. The results for Chat-GPT are presented in Table 4, while those for the open-source model can be found in Table 5."
    },
    {
      "title": "Appendix E Sample Illustration Of Paraphrases",
      "text": "In this section, we present a comparison of paraphrases generated from ChatGPT and Stablelm at various temperatures, as illustrated in Figure 7. It is evident from the results that ChatGPT produces higher-quality paraphrases compared to Stablelm-Tuned (7B). \\begin{table} \\begin{tabular}{l|c c c c|c c c c c} \\hline \\hline & \\multicolumn{6}{c}{**BERT Score**} \\\\ \\hline & \\multicolumn{3}{c}{**IMDB**} & \\multicolumn{3}{c}{**Velp**} \\\\ \\hline & **t=1.0** & **t=1.25** & **t=1.5** & **t=1.75** & **t=2.0** & **t=1.0** & **t=1.25** & **t=1.75** & **t=2.0** \\\\ \\hline **Cat GPT** & \\(0.882\\) & 0.879 & 0.863 & 0.805 & 0.765 & 0.89 & 0.887 & 0.876 & 0.83 & 0.777 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Bert Score for ChatGPT for different sampling temperatures \\begin{table} \\begin{tabular}{l|c c c c|c c c c c} \\hline \\hline & \\multicolumn{6}{c}{**BERT Score**} \\\\ \\hline & \\multicolumn{3}{c}{**IMDB**} & \\multicolumn{3}{c}{**Velp**} \\\\ \\hline & **t=0.75** & **t=1.0** & **t=1.25** & **t=1.5** & **t=1.75** & **t=0.75** & **t=1.0** & **t=1.25** & **t=1.5** & **t=1.75** \\\\ \\hline **GPT-2 (a1) (line tuned)** & 0.838 & 0.822 & 0.794 & 0.762 & 0.748 & 0.852 & 0.837 & 0.804 & 0.765 & 0.750 \\\\ **T5 (a1)** & 0.831 & 0.812 & 0.790 & 0.775 & 0.763 & 0.834 & 0.817 & 0.797 & 0.782 & 0.769 \\\\ **Stablelm-Base (3b)** & 0.814 & 0.805 & 0.785 & 0.762 & 0.749 & 0.835 & 0.817 & 0.789 & 0.757 & 0.747 \\\\ **Stablelm-Base (7b)** & 0.813 & 0.803 & 0.779 & 0.759 & 0.748 & 0.835 & 0.819 & 0.788 & 0.757 & 0.746 \\\\ **Plam T5 (a1)** & 0.843 & 0.823 & 0.792 & 0.762 & 0.750 & 0.849 & 0.830 & 0.801 & 0.769 & 0.753 \\\\ **Stablelm Tuned (3b)** & 0.846 & 0.830 & 0.795 & 0.757 & 0.743 & 0.849 & 0.836 & 0.800 & 0.760 & 0.746 \\\\ **Stablelm Tuned (7b)** & 0.854 & 0.839 & 0.800 & 0.757 & 0.743 & 0.858 & 0.845 & 0.806 & 0.761 & 0.747 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: Bert Score for Open Source Models for different sampling temperatures Figure 6: Comparision of glove-wiki-gigaword-50 with glove-wiki-gigaword-300 for word level mechanism (Madlib, Mahalonobis, TEM) for IMDB dataset against embedding level attackers. Figure 7: Comparison of paraphrase results between ChatGPT (gpt-3.5) and Stablelm-Tuned-7B at different temperatures. ChatGPT consistently generates more readable and superior quality text at higher temperatures compared to Stablelm-Tuned-7B."
    }
  ]
}