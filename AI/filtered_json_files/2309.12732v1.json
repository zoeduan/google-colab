{
  "title": "OPENAI'S GPT4 AS CODING ASSISTANT",
  "authors": [
    "Lefteris Moussiades",
    "George Zografos"
  ],
  "abstract": "\n Lately, Large Language Models have been widely used in code generation. GPT4 is considered the most potent Large Language Model from Openai. In this paper, we examine GPT3.5 and GPT4 as coding assistants. More specifically, we have constructed appropriate tests to check whether the two systems can a) answer typical questions that can arise during the code development, b) produce reliable code, and c) contribute to code debugging. The test results are impressive. The performance of GPT4 is outstanding and signals an increase in the productivity of programmers and the reorganization of software development procedures based on these new tools. \n",
  "references": [
    {
      "id": null,
      "title": "OPENAI'S GPT4 AS CODING ASSISTANT",
      "authors": [
        "Lefteris Moussiades",
        "George Zografos"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models",
      "authors": [
        "A Tamkin",
        "M Brundage",
        "J Clark",
        "D Ganguli"
      ],
      "year": "2021",
      "venue": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Attention Is All You Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A N Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2023",
      "venue": "Attention Is All You Need",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
      "authors": [
        "Z Feng",
        "D Guo",
        "D Tang",
        "N Duan",
        "X Feng",
        "M Gong",
        "L Shou",
        "B Qin",
        "T Liu",
        "D Jiang",
        "M Zhou"
      ],
      "year": "2020",
      "venue": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Evaluating Large Language Models Trained on Code",
      "authors": [
        "M Chen",
        "J Tworek",
        "H Jun",
        "Q Yuan",
        "H P D O Pinto",
        "J Kaplan",
        "H Edwards",
        "Y Burda",
        "N Joseph",
        "G Brockman",
        "A Ray",
        "R Puri",
        "G Krueger",
        "M Petrov",
        "H Khlaaf",
        "G Sastry",
        "P Mishkin",
        "B Chan",
        "S Gray",
        "N Ryder",
        "M Pavlov",
        "A Power",
        "L Kaiser",
        "M Bavarian",
        "C Winter",
        "P Tillet",
        "F P Such",
        "D Cummings",
        "M Plappert",
        "F Chantzis",
        "E Barnes",
        "A Herbert-Voss",
        "W H Guss",
        "A Nichol",
        "A Paino",
        "N Tezak",
        "J Tang",
        "I Babuschkin",
        "S Balaji",
        "S Jain",
        "W Saunders",
        "C Hesse",
        "A N Carr",
        "J Leike",
        "J Achiam",
        "V Misra",
        "E Morikawa",
        "A Radford",
        "M Knight",
        "M Brundage",
        "M Murati",
        "K Mayer",
        "P Welinder",
        "B Mcgrew",
        "D Amodei",
        "S Mccandlish",
        "I Sutskever",
        "W Zaremba"
      ],
      "year": "2021",
      "venue": "Evaluating Large Language Models Trained on Code",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Program Synthesis with Large Language Models",
      "authors": [
        "J Austin",
        "A Odena",
        "M Nye",
        "M Bosma",
        "H Michalewski",
        "D Dohan",
        "E Jiang",
        "C Cai",
        "M Terry",
        "Q Le",
        "C Sutton"
      ],
      "year": "2021",
      "venue": "Program Synthesis with Large Language Models",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "ReACC: A Retrieval-Augmented Code Completion Framework",
      "authors": [
        "S Lu",
        "N Duan",
        "H Han",
        "D Guo",
        "S.-W Hwang",
        "A Svyatkovskiy"
      ],
      "year": "2022",
      "venue": "ReACC: A Retrieval-Augmented Code Completion Framework",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation",
      "authors": [
        "D Zan",
        "B Chen",
        "D Yang",
        "Z Lin",
        "M Kim",
        "B Guan",
        "Y Wang",
        "W Chen",
        "J.-G Lou"
      ],
      "year": "2022",
      "venue": "CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "PanGu-Coder: Program Synthesis with Function-Level Language Modeling",
      "authors": [
        "F Christopoulou",
        "G Lampouras",
        "M Gritta",
        "G Zhang",
        "Y Guo",
        "Z Li",
        "Q Zhang",
        "M Xiao",
        "B Shen",
        "L Li",
        "H Yu",
        "L Yan",
        "P Zhou",
        "X Wang",
        "Y Ma",
        "I Iacobacci",
        "Y Wang",
        "G Liang",
        "J Wei",
        "X Jiang",
        "Q Wang",
        "Q Liu"
      ],
      "year": "2022",
      "venue": "PanGu-Coder: Program Synthesis with Function-Level Language Modeling",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Competition-level code generation with AlphaCode",
      "authors": [
        "Y Li",
        "D Choi",
        "J Chung",
        "N Kushman",
        "J Schrittwieser",
        "R Leblond",
        "T Eccles",
        "J Keeling",
        "F Gimeno",
        "A Lago",
        "T Hubert",
        "P Choy",
        "C De Masson D'autume",
        "I Babuschkin",
        "X Chen",
        "P.-S Huang",
        "J Welbl",
        "S Gowal",
        "A Cherepanov",
        "J Molloy",
        "D J Mankowitz",
        "E Sutherland Robson",
        "P Kohli",
        "N De Freitas",
        "K Kavukcuoglu",
        "O Vinyals"
      ],
      "year": "2022",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis",
      "authors": [
        "E Nijkamp",
        "B Pang",
        "H Hayashi",
        "L Tu",
        "H Wang",
        "Y Zhou",
        "S Savarese",
        "C Xiong"
      ],
      "year": "2022",
      "venue": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Lost at C: A User Study on the Security Implications of Large Language Model Code Assistants",
      "authors": [
        "G Sandoval",
        "H Pearce",
        "T Nys",
        "R Karri",
        "S Garg",
        "B Dolan-Gavitt"
      ],
      "year": "2022",
      "venue": "Lost at C: A User Study on the Security Implications of Large Language Model Code Assistants",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation",
      "authors": [
        "F Zhang",
        "B Chen",
        "Y Zhang",
        "J Liu",
        "D Zan",
        "Y Mao",
        "J.-G Lou",
        "W Chen"
      ],
      "year": "2023",
      "venue": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Large Language Models Meet NL2Code: A Survey",
      "authors": [
        "D Zan",
        "B Chen",
        "F Zhang",
        "D Lu",
        "B Wu",
        "B Guan",
        "Y Wang",
        "J.-G Lou"
      ],
      "year": "2022",
      "venue": "Large Language Models Meet NL2Code: A Survey",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "",
      "authors": [
        "R Li",
        "L B Allal",
        "Y Zi",
        "N Muennighoff",
        "D Kocetkov",
        "C Mou",
        "M Marone",
        "C Akiki",
        "J Li",
        "J Chim",
        "Q Liu",
        "E Zheltonozhskii",
        "T Y Zhuo",
        "T Wang",
        "O Dehaene",
        "M Davaadorj",
        "J Lamy-Poirier",
        "J Monteiro",
        "O Shliazhko",
        "N Gontier",
        "N Meade",
        "A Zebaze",
        "M.-H Yee",
        "L K Umapathi",
        "J Zhu",
        "B Lipkin",
        "M Oblokulov",
        "Z Wang",
        "R Murthy",
        "J Stillerman",
        "S S Patel",
        "D Abulkhanov",
        "M Zocca",
        "M Dey",
        "Z Zhang",
        "N Fahmy",
        "U Bhattacharyya",
        "W Yu",
        "S Singh",
        "S Luccioni",
        "P Villegas",
        "M Kunakov",
        "F Zhdanov",
        "M Romero",
        "T Lee",
        "N Timor",
        "J Ding",
        "C Schlesinger",
        "H Schoelkopf",
        "J Ebert",
        "T Dao",
        "M Mishra",
        "A Gu",
        "J Robinson",
        "C J Anderson",
        "B Dolan-Gavitt",
        "D Contractor",
        "S Reddy",
        "D Fried",
        "D Bahdanau",
        "Y Jernite",
        "C M Ferrandis",
        "S Hughes",
        "T Wolf",
        "A Guha",
        "L Werra",
        "H De Vries"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
      "authors": [
        "Z Luo",
        "C Xu",
        "P Zhao",
        "Q Sun",
        "X Geng",
        "W Hu",
        "C Tao",
        "J Ma",
        "Q Lin",
        "D Jiang"
      ],
      "year": "2023",
      "venue": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding",
      "authors": [
        "Z Xiao",
        "X Yuan",
        "Q V Liao",
        "R Abdelghani",
        "P.-Y Oudeyer"
      ],
      "year": "2023",
      "venue": "28th International Conference on Intelligent User Interfaces",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "The 12 best IDEs for programming",
      "authors": [
        "F Okeke"
      ],
      "year": "2022",
      "venue": "The 12 best IDEs for programming",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "GPT-4 System Card | Data Science Association",
      "authors": [],
      "year": "",
      "venue": "GPT-4 System Card | Data Science Association",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "OpenAI Platform",
      "authors": [],
      "year": "",
      "venue": "OpenAI Platform",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "GitHub -lmous/openai-gpt4-coding-assistantt -github.com",
      "authors": [],
      "year": "",
      "venue": "GitHub -lmous/openai-gpt4-coding-assistantt -github.com",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "BigDecimal (Java Platform SE 8 )",
      "authors": [],
      "year": "",
      "venue": "BigDecimal (Java Platform SE 8 )",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Theory of Games and Economic Behavior (60th Anniversary Commemorative Edition)",
      "authors": [
        "J Neumann",
        "O Morgenstern",
        "A Rubinstein"
      ],
      "year": "1944",
      "venue": "Theory of Games and Economic Behavior (60th Anniversary Commemorative Edition)",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Openai'S Gpt4 As Coding Assistant",
      "text": "Lefteris Moussiades Computer Science Department International Hellenic University Greece, Kavala PA 65404 lmous@cs.ihu.gr &George Zografos Computer Science Department International Hellenic University Greece, Kavala PA 65404 gezozra@cs.ihu.gr"
    },
    {
      "title": "Abstract",
      "text": "Lately, Large Language Models have been widely used in code generation. GPT4 is considered the most potent Large Language Model from Openai. In this paper, we examine GPT3.5 and GPT4 as coding assistants. More specifically, we have constructed appropriate tests to check whether the two systems can a) answer typical questions that can arise during the code development, b) produce reliable code, and c) contribute to code debugging. The test results are impressive. The performance of GPT4 is outstanding and signals an increase in the productivity of programmers and the reorganization of software development procedures based on these new tools."
    },
    {
      "title": "1 Introduction",
      "text": "Among other features, Large Language Models (LLM) can generate code in various programming languages [1]. Recently, many publications have recommended and evaluated LLMs specialized in code generation. CodeBERT is a bimodal pre-trained model designed for programming and natural language tasks, like code search and documentation generation. It's developed using a Transformer-based architecture [2] and trained with a unique objective function to effectively use paired and unpaired data from programming and natural language sources [3]. Codex is a GPT language model fine-tuned on public GitHub code, and a version of it powers GitHub Copilot. When evaluated on the HumanEval set, designed to gauge program synthesis from docstrings, Codex solves 28.8% of the tasks, outperforming GPT-3 and GPT-J. The study also uncovers that multiple samplings from Codex enhance problem-solving success rates. Additionally, the paper discusses the challenges and broader implications of advanced code generation technologies [4]. The capabilities of large language models in synthesizing Python programs from natural language prompts using two new benchmarks, MBPP and MathQA-Python, are explored by [5]. The study reveals that as model size increases, synthesis performance also improves, with the largest models being able to correctly generate solutions to nearly 60% of MBPP problems through few-shot learning. The models also benefit from human feedback, cutting error rates in half, but struggle to predict the outputs of the generated programs when provided with specific inputs. Study [6] introduces a novel approach to code completion using an \"external\" context, emulating human behaviour of referencing related code snippets. The proposed framework combines retrieval techniques with traditional language models to better predict code, factoring in direct copying and semantically similar code references. When tested on Python and Java, this method achieves state-of-the-art performance on the CodeXGLUE benchmark. Paper [7] explores LLMs trained on unlabeled code corpora for code generation. It introduces CERT, a two-step method that creates a basic code outline and then fills in the details. The study also presents two new benchmarks, PandasEval and NumpyEval, for evaluating library-oriented code generation. PanGu-Coder is a pre-trained language model built on the PanGu-Alpha architecture designed to generate code from natural language descriptions. The model is trained using a two-stage strategy, starting with raw programming data, followed by task-focused training using Causal and Masked Language Modelling objectives [8]Li et al. introduced AlphaCode, a deep-learning model built with self-supervised learning and an encoder-decoder transformer, which approximates human-level performance in computer programming competitions on the Codeforces platform. Authors argue that this advancement could significantly boost programmers' productivity and reshape programming culture, where humans primarily define problems and machine learning handles code generation and execution [9]. CODEGEN is a family of large language models trained on natural language and programming data to advance program synthesis. The study also explores a multi-step approach to program synthesis, revealing improved performance when tasks are broken down into multiple prompts, and introduces an open benchmark, the Multi-Turn Programming Benchmark (MTPB), for this purpose [10]. Paper [11] investigates the impact of LLMs, like OpenAI Codex, on developers' code security. Through a user study involving 58 student programmers, the research examines the code's security when implementing a specific C-based task with the assistance of LLMs. The findings suggest that using LLMs does not substantially increase the risk of introducing critical security vulnerabilities in such coding tasks. RepoCoder [12] is a framework designed for repository-level code completion that efficiently leverages information scattered across different files in a repository. RepoCoder uses a combination of a similarity-based retriever and a pre-trained code language model, along with an innovative iterative retrieval-generation approach, to improve code completion at various levels of granularity. RepoCoder has been tested on a new benchmark called RepoEval. Paper [13] thoroughly surveys 27 large language models geared explicitly towards the NL2Code task, which involves generating code from natural language descriptions. The study evaluates these models using the HumanEval benchmark and derives that success in this domain hinges on \"Large Size, Premium Data, Expert Tuning\". The authors also introduce a dedicated website to monitor ongoing advancements and discuss the gap between model performance and human capabilities in the NL2Code realm. The BigCode community has unveiled StarCoder and StarCoderBase, advanced Large Language Models designed for code generation and infilling, with StarCoderBase trained on a vast dataset called The Stack and StarCoder being a fine-tuned version for Python [14]. WizardCoder is a model that empowers Code Large Language Models (Code LLMs) with complex instruction fine-tuning by adapting the Evol-Instruct method to the domain of code. It has been introduced in a paper [15] and has demonstrated exceptional performance in code-related tasks. Study [16] investigates the use of large language models (LLMs) to aid in deductive coding, a method in qualitative analysis where data is labelled based on predetermined codebooks. The approach reached satisfactory alignment with expert-labelled outcomes by integrating GPT-3 with expert-created codebooks for a specific task related to coding curiosity-driven questions. The paper highlights the potential and challenges of employing LLMs in qualitative data coding and broader applications. One result of all this development is the addition of intelligent assistants to many well-known IDEs. For example, Visual Studio Code is supported by IntelliCode, PyCharm by Code With Me, Eclipse by Code Recommenders, NetBeans by Deep Learning, IntelliJ IDEA by Code With Me, and Xcode by SourceKit-LSP [17]. In March 2023, Openai published the GPT-4 system card, which [18] analyzes the capabilities of GPT-4, including code generation. However, to date, we have not found any publication evaluating the coding capabilities of GPT-4. This paper evaluates GPT-4 and GPT-3.5 as coding assistants."
    },
    {
      "title": "2 Methodology",
      "text": "We consider three tasks for which a coding assistant should be helpful: Code development, Code Debugging, and answering questions related to code. Code development and Code debugging are self-explanatory concepts. The human programmer often has questions during code writing, such as details on the syntax of a command. For this reason, we check that GPT-3.5 and 4 can answer questions about the code satisfactorily. There are many source code datasets, several mentioned in the introduction. However, these are geared to check LLMs' code production specifically. In addition, problems of a prototypical nature often arise in the production environment. Although we do not know exactly which data sets GPT-3.5 and 4 are trained on, it is reasonable to assume that they are trained on public data sets whose purpose is to evaluate LLMs' coding capabilities. For the reasons above, our tests do not rely on such data sets. Instead, we have carefully constructed 3 test suites: one for testing code generation capabilities, one for testing debugging capabilities, and one for answering questions. The tests were designed to limit the chances that GPT3.5 and 4 were trained on exactly those requested codes. The tests were submitted through the web interface of GPT3.5 and 4. The prompt engineering of the tests follows the GPT best practices of Openai [19]. The results were evaluated based on an expert human reviewer or compared to another reliable source. As the tests are about checking different capabilities, more details about the test configuration and the evaluation of the results are given with the description of each test. Java was used as the programming language. All code and other answers generated by GPT3.5 and 4 is on GitHub [19]."
    },
    {
      "title": "3 Answering Questions",
      "text": "In this task, we test the assistants to see if they can answer questions that often arise for developers when developing code. For this purpose, we constructed three questions of relative difficulty. We list the relevant prompts and then evaluate the assistants' answers. * _Question 1 (Prompt)_: Does Java support passing a function as an argument to a function? What is the syntax? * _Question 2 (Prompt)_: Consider the code System.out.print(s==s1+\" \"+s.equals(s1)); I expected it to display two boolean values, but it displays only one. Explain why? * _Question 3 (Prompt)_: Non-abstract methods have an implementation. The same applies to the default methods. Non-abstract methods are inherited and can be overwritten. The same applies to default methods. What is the difference between default methods and non-abstract ones? Answer briefly. Response GPT3.5 and 4 responses were evaluated by a human expert and found to answer all three questions satisfactorily. Responses can be found on Github [20]."
    },
    {
      "title": "4 Code Development Assistance",
      "text": "For code development, we constructed two tests. The first asks for developing a power function, and the second for implementing a tic-tac-toe application with predetermined classes."
    },
    {
      "title": "Power Function (Pf)",
      "text": "In this task, we asked GPT3.5 and 4 to implement a function that calculates the power of a real number raised to an integer exponent. Although the task seems simple at first glance, it is demanding when high calculation precision is required. The difficulty arises from the approximate nature of real numbers. Due to the approximate nature of real numbers, the results of operations lack precision. When there are many intermediate operations, the deviations from each operation accumulate, and the final result may present a significant deviation. So, this is a complex implementation when precision is required in the calculations. Moreover, it is a feature, not a concern for application developers, as all languages provide a ready-made power function. Besides, after an exhaustive search on the web, we could not find a high-precision implementation. Evaluation The generated functions were compared with the Java Math.pow function. The Math.pow() function is implemented in Java as a native method, which means that it is implemented in the underlying platform's native code. The implementation of Math.pow() varies depending on the platform and the underlying hardware architecture. The algorithm is optimized for speed and accuracy and is presumed to be relatively accurate. The results were checked based on the following procedure. Let GPT4.pow be the function produced by GPT4 and r(f,b,e) the result of the function f with base b and exponent e. For each b from 500 to 1000 with step 1 and each e from 0 to 9 with step 1, the values r(GPT4.pow,b,e) and r(Math.pow,b,e) are calculated. Assume that for each pair of these values, even one is non-infinite, and they differ from each other by more than 4.9E-324 (the smallest real value represented by Java double type). In that case, the absolute value of their difference is added to an appropriate adder. Then, the adder is divided by the number of terms in the sum and, thus, the average deviation of the GPT4.pow results from the Math.pow results are calculated. The same process is repeated to compare GPT3.5.pow to Math.pow. The whole process is repeated for exponents from -1 to -9."
    },
    {
      "title": "Pf Prompt #1",
      "text": "Develop a Java function that calculates the power of a real number raised to an integer exponent. Specifications: 1. Interface: public static double pow(double b, int e) 2. Don't use Math.pow or BigDecimal.pow 3. Achieve the maximum possible precision"
    },
    {
      "title": "Response",
      "text": "Both systems responded by providing a satisfactory implementation based on the exponentiation by squaring algorithm. The algorithm has time complexity O(log n), where n is the exponent. The implementations are almost identical, with only two minor differences: * GPT4 checks if the exponent is odd by performing a bitwise and with 1 \\(((e\\&1)==1)\\) while GPT3.5 performs an integer division remainder calculation \\((e\\%2==1)\\) * GPT4 performs a right shift by 1 to divide the exponent by 2 \\((e>>=1)\\), whereas GPT3.5 performs integer division \\((e/=2)\\) for the same purpose. The algorithms presented the same average deviation with respect to Math.pow, which was 2.356527240763158E10 for positive exponents and 1.7112490986192953E-22 for negative exponents."
    },
    {
      "title": "Pf Prompt #2",
      "text": "Can you improve the precision of your function? I checked it against Math.pow and found significant discrepancies. Examples: base = 502, exponent= 9, GPT.pow = 2.0245730632526733E24, Math.pow = 2.024573063252673E24, difference = 2.68435456E8 base = 504, exponent = 9, GPT.pow = 2.098335016107156E24, Math.pow = 2.0983350161071556E24, difference = 2.68435456E8"
    },
    {
      "title": "Response",
      "text": "GPT3.5 responded with a function that implements the Taylor series expansion [21] algorithm, which increases time complexity to O(e2). GPT4 again used exponentiation by squaring but used the BigDecimal class [22], recommended for cases requiring precision in calculations. The mean deviation of GPT3.5 worsened to 2.2292150579952536E25 for positive exponents and 1.0012331308931004 for negative ones. The mean deviation of GPT4 improved to 2.3037066373333335E9 for positive exponents and 2.1726446876877912E-2 for negative ones."
    },
    {
      "title": "Tic-Tac-Toe Application (Ttt)",
      "text": "In this task, we asked GPT to develop a tic-tac-toe application following especial specifications. We set certain specifications to minimize the chance that a tic-tac-toe app would be found ready-made and delivered intact."
    },
    {
      "title": "Ttt Prompt #1",
      "text": "Develop a command-line tic-tac-toe application consisting of the following classes: Player, Board, LivePlayer, RBPlayer, and Game. * _Player_: Is an Abstract class containing, final char id, abstract method Board move(Board board)* _Class Board_: Represents the game board. It contains the following public function members: void displayBoard(): It displays the game board on its current status char win(): It returns the winner's id. If there is no winner, it returns a white character. * _Class LivePlayer_: Represents a human player. It is a concrete class implementation inherited from Player. * _Class RBPlayer_: Represents an artificial Rule-based Player. It is based on the following rules: A. If there is a movement to win, select it. B. If the opponent has a movement to win, select it to block the opponent from winning. * _Game_: Uses the above-described classes to implement a tic-tac-toe game."
    },
    {
      "title": "Response",
      "text": "GPT4 respond with a fully functional application that meets all our requirements. The code quality is good, including a warning that the used Board object could have been declared final. GPT3.5 responded with code that contained compile time errors. We performed the following communication to investigate its ability to produce correct code."
    },
    {
      "title": "Ttt Prompt #1.1",
      "text": "Your code compiles with errors. Examples: * error: cells has private access in Board board.cells[i][j] = id; * error: cannot assign a value to final variable board board = currentPlayer.move(board); Rewrite code to avoid compile-time errors. GPT3.5 replied with code containing logical errors. We prompt it as follows:"
    },
    {
      "title": "Ttt Prompt #1.2",
      "text": "Your code has logical errors. Here is the output of your code after two movements of each player Player X, enter your move (row [0-2] and column [0-2]): **1 1** After the second fix, in the third version of the application, GPT3.5 responded with functional code. GPT4 respond with a fully functional application that meets all our requirements. The code quality is good, including a warning that the used Board object could have been declared final. Next, we requested a new class representing an artificial player based on the minimax [23] algorithm. The minimax implements a perfect player, i.e., a player who never loses. Therefore, the worst possible outcome minimax may give is a draw."
    },
    {
      "title": "Ttt Prompt #2",
      "text": "Can you add the class MinimaxPlayer representing an artificial player based on the well-known minimax algorithm?"
    },
    {
      "title": "Response",
      "text": "GPT4 responded with a fully functional minimax player. GPT3.5 replayed with an erroneous version of a minimax player. A communication ensued in which we attempted to inform GPT3.5 of its errors, but it failed to present a satisfactory solution. Finally, we prompt GPT3.5 as follows:"
    },
    {
      "title": "Ttt Prompt #2.1",
      "text": "No improvement. It's still straightforward for anyone to win your MinimaxPlayer. I'm giving you the game board if it can help you. Please don't give me the same wrong algorithm again. If you can't do better, just let me know. Player X, enter your move (row [0-2] and column [0-2]): **2 0** Player X wins! Here, GPT3.5 explained the difficulties of implementing the algorithm and suggested that we study the matter more or look for a ready-made solution on GitHub."
    },
    {
      "title": "5 Debugging Assistance (Da)",
      "text": "To test the debugging capabilities, we designed two tests. One includes code that throws an exception, and the other includes code containing a logic error."
    },
    {
      "title": "Exception (E)",
      "text": "In this task, we provided a code that crashes with IndexOutOfBoundsException and asked GPT3.5 and 4 to explain the problem and fix the code."
    },
    {
      "title": "Da-E Prompt #1",
      "text": "The Code below fails with IndexOutOfBoundsException. ``` importjava.util.ArrayList; importjava.util.List; publicclassDebug2{ staticArrayList<String>l=newArrayList<>(); staticvoidload(){ l.add(\"Green\"); l.add(\"Black\"); l.add(\"Blue\"); l.add(\"White\"); l.add(\"Pink\"); l.add(\"Black\"); } staticvoiddelAll(List<String>l,Stringtarget){``` intsize=l.size(); for(inti=0;i<size;i++) if(target.equals(l.get(i))){ l.remove(i); } } } publicstaticvoidmain(String[]args){ load(); delAll(l,\"Black\"); } } ``` Explain the error and correct the code."
    },
    {
      "title": "Explanation Of The Error",
      "text": "First, the exception is raised in the delAll function, which is responsible for deleting all the target elements from the list l. The function stores the list size in the local variable size and then, in the iterative process, tries to delete every element equal to the target. However, after deleting the first element, the list size is reduced by 1. However, delAll tries to access the list for its original size, which leads to the exception."
    },
    {
      "title": "Responce",
      "text": "Both assistants solved the problem successfully. While GPT3.5 proposed a solution based on an Iterator, GPT4 proposed two alternatives. In the first solution, the for control expression replaces the size variable with the function that returns the list size (l.size()); inside the for, decrements i by one each time it deletes an element. The second solution traverses the list from the end (l.size()-1) to the beginning, thus ensuring no IndexOutOfBoundsException issue."
    },
    {
      "title": "Logical Error (Le)",
      "text": ""
    },
    {
      "title": "Da-Le Prompt #1",
      "text": "The code below contains logical errors. Expected Output: [1, 2, 3, 4, 0, 5, 6] Actual Output: [1, 2, 3, 4, 5, 6, 0, 0, 0, 0, 0] Explain the errors and correct the code. ``` //Codecontaininglogicalerror importjava.util.Arrays; publicclassDebugging{ staticint[]resize(int[]input,intnewSize){ returnArrays.copyOf(input,newSize<input.length?newSize:input.length); } staticintadd(int[]array,intdata,intindex){ for(inti=0;i<=index;i++){ if(array[i]==data){ returnindex; } } array[index++]=data; returnindex; } staticint[]generateSet(int...array){ int[]set=newint[array.length]; intidx=0; for(intelement:array){idx = add(set, element, idx); } resize(set, idx); return set; } static int[] concat(int[] array1, int[] array2) { int[] rslt = new int[array1.length + array2.length]; System.arraycopy(array1, 0, rslt, 0, array1.length); System.arraycopy(array2, 0, rslt, array1.length, array2.length); return generateSet(rslt); } public static void main(String[] args) { int[] set1 = generateSet(1, 2, 3, 4, 0), set2 = generateSet(0, 3, 4, 5, 6); int[] union = concat(set1, set2); System.out.println(Arrays.toString(union)); } } ```"
    },
    {
      "title": "Explanation Of The Error",
      "text": "There are two bugs in the code. The first one is found in generateSet, which calls the function resize but does not assign the array returned by resize to the set variable. Thus, the set retains its original size and data. So the fix needed here is return resize(set, idx); instead of resize(set,idx); return set; The second error is within the add function, which iterates while i<=index, whereas the correct condition is i<index."
    },
    {
      "title": "Response",
      "text": "First, GPT3.5 and GPT4 correctly explained the problems in the add and generateSet functions. In addition, they identified a resize problem when there is none. More specifically, GPT3.5 commented: 1. The resize method is not updating the size of the array correctly. It creates a new array of the specified size but doesn't copy the elements from the original array. 2. Use Arrays.copyOf to create a new array of the desired size and copy the elements from the original array to the new one. And GPT4 commented: 1. Resize method: In the current implementation, if newSize is larger than input.length, it would return an array of the same size as input. This does not match the intended behavior of resizing the array to newSize. These comments are wrong. However, the generated codes are functional as they correctly fix both add and generateSet, while the change they make to resize does not affect the specific code. More specifically, both systems converted resize so that it does not support reducing the size of the input table. Indeed, size reduction is not needed in this code. Of course, a resize that helps reduce a table's length (with possible data loss) might be helpful elsewhere."
    },
    {
      "title": "6 Conclusions",
      "text": "In this work, we examined the potential of GPT3.5 and 4 as coding assistants for three distinct tasks: Answering questions and providing Development and Debugging assistance. In answering questions, both LLMs proved to be efficient. In Development assistance, GPT4 proved superior to GPT3.5. Both in creating the pow function, it achieved a significant improvement in accuracy, and in the requirements for the tic-tac-toe application, it immediately responded with complete success. Moreover, it added a player based on the Minimax algorithm with ease. This is a requirement, according to our estimation, that is far from easy to implement. GPT3.5 failed to meet this requirement. In testing the debugging capabilities, GPT3.5 and 4 responded promptly and successfully to exception and logical error investigations. These conclude that GPT4 can provide substantial and reliable help as a coding assistant for all three properties tested. As expected, GPT3.5 appeared inferior to GPT4, but its capabilities are still impressive. Recently, a heated debate has been about whether artificial intelligence will replace human programmers. We believe the answer to this question is impossible, as no one can predict the future. However, currently, GPT4 can provide meaningful and reliable assistance to coding and dramatically improve the productivity of human developers. Such a thing is sure to reorganize the software production processes and possibly will not leave the job market of programmers unaffected. Whether its effect will increase the amount of software produced or unemployment in the developer industry remains to be seen."
    },
    {
      "title": "References",
      "text": "* [1]A. Tamkin, M. Brundage, J. Clark, and D. Ganguli (2021) Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models. External Links: 2103.03766 Cited by: SS1. * [2]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin (2023-02) Attention Is All You Need. External Links: 1706.03762 Cited by: SS1. * [3]Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou (2020) CodeBERT: A Pre-Trained Model for Programming and Natural Languages. External Links: 2002.01880 Cited by: SS1. * [4]M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba (2021) Evaluating Large Language Models Trained on Code. External Links: 2103.03766 Cited by: SS1. * [5]J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton (2021) Program Synthesis with Large Language Models. External Links: 2103.03766 Cited by: SS1. * [6]S. Lu, N. Duan, H. Han, D. Guo, S. Hwang, and A. Svyatkovskiy (2022) ReACC: A Retrieval-Augmented Code Completion Framework. External Links: 2103.03766 Cited by: SS1. * [7]Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, T. Hubert, P. Choy, C. De Masson d'Autume, I. Babuschkin, X. Chen, P. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. Sutherland Robson, P. Kohli, N. De Freitas, K. Kavukcuoglu, and O. Vinyals (2022-10) Competition-level code generation with AlphaCode. Science378, pp. 1092-1097. External Links: Document, Link Cited by: SS1. * [8]F. Christopoulou, G. Lampouras, M. Gritta, G. Zhang, Y. Guo, Z. Li, Q. Zhang, M. Xiao, B. Shen, L. Li, H. Yu, L. Yan, P. Zhou, X. Wang, Y. Ma, I. Iacobacci, Y. Wang, G. Liang, J. Wei, X. Jiang, Q. Wang, and Q. Liu (2022) PanGu-Coder: Program Synthesis with Function-Level Language Modeling. External Links: 2103.03766 Cited by: SS1. * [9]F. Zhang, B. Chen, Y. Zhang, J. Liu, D. Zan, Y. Mao, J. Lou, and W. Chen (2023) RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. External Links: 2103.03766 Cited by: SS1. * [10]E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong (2022-10) CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. External Links: 2103.03766 Cited by: SS1. * [11]F. Zhang, B. Chen, Y. Zhang, J. Liu, D. Zan, Y. Mao, J. Lou, and W. Chen (2023) RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. External Links: 2103.03766 Cited by: SS1. * [12]D. Zan, B. Chen, Y. Zhang, J. Liu, D. Zan, Y. Mao, J. Lou, and W. Chen (2023) RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. External Links: 2103.03766 Cited by: SS1. * [13]D. Zan, B. Chen, Y. Zhang, D. Lu, B. Wu, B. Guan, Y. Wang, and J. Lou (2022) Large Language Models Meet NL2Code: A Survey. External Links: 2103.03766 Cited by: SS1. [MISSING_PAGE_POST] . Yuan, Q. V. Liao, R. Abdelghani, and P. Oudeyer (2023-02) Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding. In* [17] F. Okeke, \"The 12 best IDEs for programming.\" [https://www.techrepublic.com/article/best-ide-software/](https://www.techrepublic.com/article/best-ide-software/), July 2022. [Accessed 14-09-2023]. * [18] \"GPT-4 System Card | Data Science Association.\" [http://www.datascienceassn.org/content/gpt-4-system-card](http://www.datascienceassn.org/content/gpt-4-system-card). [Accessed 14-09-2023]. * [19] \"OpenAI Platform.\" [https://platform.openai.com/docs/guides/gpt-best-practices](https://platform.openai.com/docs/guides/gpt-best-practices). [Accessed 14-09-2023]. * lmous/openai-gpt4-coding-assistantt - - github.com.\" [https://github.com/lmous/openai-gpt4-coding-assistantt](https://github.com/lmous/openai-gpt4-coding-assistantt). [Accessed 15-09-2023]. * from Wolfram MathWorld - - mathworld.wolfram.com.\" [https://mathworld.wolfram.com/TaylorSeries.html](https://mathworld.wolfram.com/TaylorSeries.html). [Accessed 15-09-2023]. * [22] \"BigDecimal (Java Platform SE 8 ).\" [https://docs.oracle.com/javase/8/docs/api/java/math/BigDecimal.html](https://docs.oracle.com/javase/8/docs/api/java/math/BigDecimal.html). [Accessed 15-09-2023]. * [23] J. von Neumann, O. Morgenstern, and A. Rubinstein, _Theory of Games and Economic Behavior (60th Anniversary Commemorative Edition)_. Princeton University Press, 1944."
    }
  ]
}