{
  "title": "Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval",
  "authors": [
    "Shengjie Ma",
    "Chong Chen",
    "Qi Chu",
    "Jiaxin Mao",
    "Leveraging Large"
  ],
  "abstract": "\n Collecting relevant judgments for legal case retrieval is a challenging and time-consuming task. Accurately judging the relevance between two legal cases requires a considerable effort to read the lengthy text and a high level of domain expertise to extract Legal Facts and make juridical judgments. With the advent of advanced large language models, some recent studies have suggested that it is promising to use LLMs for relevance judgment. Nonetheless, the method of employing a general large language model for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored. To fill this research gap, we devise a novel few-shot workflow tailored to the relevant judgment of legal cases. The proposed workflow breaks down the annotation process into a series of stages, imitating the process employed by human annotators and enabling a flexible integration of expert reasoning to enhance the accuracy of relevance judgments. By comparing the relevance judgments of LLMs and human experts, we empirically show that we can obtain reliable relevance judgments with the proposed workflow. Furthermore, we demonstrate the capacity to augment existing legal case retrieval models through the synthesis of data generated by the large language model. \n CCS CONCEPTS • Information systems → Specialized information retrieval. \n",
  "references": [
    {
      "id": null,
      "title": "Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval",
      "authors": [
        "Shengjie Ma",
        "Chong Chen",
        "Qi Chu",
        "Jiaxin Mao",
        "Leveraging Large"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Longformer: The longdocument transformer",
      "authors": [
        "Iz Beltagy",
        "Matthew E Peters",
        "Arman Cohan"
      ],
      "year": "2020",
      "venue": "Longformer: The longdocument transformer",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos",
      "authors": [
        "Ilias Chalkidis",
        "Manos Fergadiotis"
      ],
      "year": "2020",
      "venue": "LEGAL-BERT: The muppets straight out of law school",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Chatlaw: Opensource legal large language model with integrated external knowledge bases",
      "authors": [
        "Jiaxi Cui",
        "Zongjian Li",
        "Yang Yan",
        "Bohua Chen",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Chatlaw: Opensource legal large language model with integrated external knowledge bases",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Perspectives on Large Language Models for Relevance Judgment",
      "authors": [
        "Guglielmo Faggioli",
        "Laura Dietz",
        "L A Charles",
        "Gianluca Clarke",
        "Matthias Demartini",
        "Claudia Hagen",
        "Noriko Hauff",
        "Evangelos Kando",
        "Martin Kanoulas",
        "Benno Potthast",
        "Henning Stein",
        "Wachsmuth"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR '23)",
      "doi": "10.1145/3578337.3605136"
    },
    {
      "id": "b5",
      "title": "Chatgpt outperforms crowd-workers for text-annotation tasks",
      "authors": [
        "Fabrizio Gilardi",
        "Meysam Alizadeh",
        "Maël Kubli"
      ],
      "year": "2023",
      "venue": "Chatgpt outperforms crowd-workers for text-annotation tasks",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Empirical slant delay model for radio space geodetic techniques",
      "authors": [
        "Klemens Lagler",
        "Michael Schindelegger",
        "Johannes Böhm",
        "Hana Krásná",
        "Tobias Nilsson"
      ],
      "year": "2013",
      "venue": "Geophysical research letters",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval",
      "authors": [
        "Haitao Li",
        "Qingyao Ai",
        "Jia Chen",
        "Qian Dong",
        "Yueyue Wu",
        "Yiqun Liu",
        "Chong Chen",
        "Qi Tian"
      ],
      "year": "2023",
      "venue": "SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "LawGPT: 中文法律对话语言模型",
      "authors": [
        "Meng Yutong",
        "Liu Hongcheng",
        "Liao Yusheng",
        "Wang Yuhao"
      ],
      "year": "2023",
      "venue": "LawGPT: 中文法律对话语言模型",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "7th International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "LeCaRD: a legal case retrieval dataset for Chinese law system",
      "authors": [
        "Yixiao Ma",
        "Yunqiu Shao",
        "Yueyue Wu",
        "Yiqun Liu",
        "Ruizhe Zhang",
        "Min Zhang",
        "Shaoping Ma"
      ],
      "year": "2021",
      "venue": "Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Do we still need human assessors? prompt-based gpt-3 user simulation in conversational ai",
      "authors": [
        "Selina Meyer",
        "David Elsweiler",
        "Bernd Ludwig",
        "Marcos Fernandez-Pichel",
        "David E Losada"
      ],
      "year": "2022",
      "venue": "Proceedings of the 4th Conference on Conversational User Interfaces",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval",
      "authors": [
        "E Stephen",
        "Steve Robertson",
        "Walker"
      ],
      "year": "1994",
      "venue": "SIGIR'94",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers",
      "authors": [
        "Jon Saad-Falcon",
        "Omar Khattab",
        "Keshav Santhanam",
        "Radu Florian",
        "Martin Franz",
        "Salim Roukos",
        "Avirup Sil",
        "Md Arafat Sultan",
        "Christopher Potts"
      ],
      "year": "2023",
      "venue": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval",
      "authors": [
        "Yunqiu Shao",
        "Jiaxin Mao",
        "Yiqun Liu",
        "Weizhi Ma",
        "Ken Satoh",
        "Min Zhang",
        "Shaoping Ma"
      ],
      "year": "2020",
      "venue": "IJCAI",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Understanding Relevance Judgments in Legal Case Retrieval",
      "authors": [
        "Yunqiu Shao",
        "Yueyue Wu",
        "Yiqun Liu",
        "Jiaxin Mao",
        "Shaoping Ma"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Information Systems",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement",
      "authors": [
        "Rosamond Thalken",
        "Edward H Stiglitz",
        "David Mimno",
        "Matthew Wilkens"
      ],
      "year": "2023",
      "venue": "Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Large language models can accurately predict searcher preferences",
      "authors": [
        "Paul Thomas",
        "Seth Spielman",
        "Nick Craswell",
        "Bhaskar Mitra"
      ],
      "year": "2023",
      "venue": "Large language models can accurately predict searcher preferences",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Edouard Grave, and Guillaume Lample",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurelien Rodriguez",
        "Armand Joulin"
      ],
      "year": "2023",
      "venue": "LLaMA: Open and Efficient Foundation Language Models",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Semantic data augmentation based distance metric learning for domain generalization",
      "authors": [
        "Mengzhu Wang",
        "Jianlong Yuan",
        "Qi Qian",
        "Zhibin Wang",
        "Hao Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Promda: Prompt-based data augmentation for low-resource nlu tasks",
      "authors": [
        "Yufei Wang",
        "Can Xu",
        "Qingfeng Sun",
        "Huang Hu",
        "Chongyang Tao",
        "Xiubo Geng",
        "Daxin Jiang"
      ],
      "year": "2022",
      "venue": "Promda: Prompt-based data augmentation for low-resource nlu tasks",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "V Quoc",
        "Denny Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Lawformer: A pre-trained language model for chinese legal long documents",
      "authors": [
        "Chaojun Xiao",
        "Xueyu Hu",
        "Zhiyuan Liu",
        "Cunchao Tu",
        "Maosong Sun"
      ],
      "year": "2021",
      "venue": "AI Open",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Leveraging Large Language Models For Relevance Judgments In Legal Case Retrieval",
      "text": "Shengjie Ma Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China msl@ruc.edu.cn Chong Chen Huawei Cloud BU Beijing, China chenchong55@huawei.com Qi Chu Beijing Normal University Beijing, China Jiaxin Mao Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China maojiaxin@gmail.com"
    },
    {
      "title": "Abstract.",
      "text": "Collecting relevant judgments for legal case retrieval is a challenging and time-consuming task. Accurately judging the relevance between two legal cases requires a considerable effort to read the lengthy text and a high level of domain expertise to extract Legal Facts and make juridiad judgments. With the advent of advanced large language models, some recent studies have suggested that it is promising to use LLMs for relevance judgment. Nonetheless, the method of employing a general large language model for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored. To fill this research gap, we devise a novel few-shot workflow tailored to the relevant judgment of legal cases. The proposed workflow breaks down the annotation process into a series of stages, imitating the process employed by human annotators and enabling a flexible integration of expert reasoning to enhance the accuracy of relevance judgments. By comparing the relevance judgments of LLMs and human experts, we empirically show that we can obtain reliable relevance judgments with the proposed workflow. Furthermore, we demonstrate the capacity to augment existing legal case retrieval models through the synthesis of data generated by the large language model. Large Language Models, Data Annotation, Legal Case Retrieval 2018 978-1-4503-XXXX-X/18/06. $15.00 2"
    },
    {
      "title": "1. Introduction",
      "text": "Legal case retrieval (LCR) is a critical component of modern legal systems, for ensuring fairness and justice. It enables legal professionals to find relevant cases related to their current cases. According to \"Guiding Opinions of the Supreme People's Court of China on Unified Legal Application and Strengthening Case Retrieval\": the term \"similar cases\" refers to cases that share similarities in basic facts, points of dispute, issues of legal application, and other aspects with the pending case; when the pending case lacks clear judicial rules or has not yet formed unified judicial rules, case retrieval for similar cases should be conducted. Many practitioners still rely on keyword-based retrieval systems when searching for similar cases. However, this approach is cumbersome, has a steep learning curve, and offers low efficiency and accuracy. The researchers have made extensive attempts at the LCR task. BERT-PLI[18] models the relevance by aggregating paragraph-level interactions of case pairs. SAILER[8] aims to maximize the utilization of information in annotated data. Specifically, it utilizes an encoder-decoder architecture for pre-training, which encodes the facts into vectors and uses a decoder to re-construct the vectors to the original decisions and reasoning. In addition, Wei et al. [25] propose IOT-Match, which learns to find rationales from paired legal cases based on semantics and legal elements of sentences, with the assistance of numerous expert-annotated interpretable relevance labels. Despite the promising results of these models on specific test sets, they still rely heavily on extensively high-quality annotated data. Owing to the scarcity of annotated data, a notable concern arises regarding the limited generalizability of these models across diverse data. These concerns highlight the role of a more reliable and valid method to collect the legal relevance annotations, which is vital not only for the evaluation of models but also for the training of more sophisticated models. However, to judge relevance, one necessitatessubstantial domain expertise and a significant effort to comprehend lengthy texts (Kumar et al., 2017). This means that manual annotations require much more complex preliminary instructions for annotators, leading to higher learning effort, greater demands on their abilities, and difficulties in expanding the scale of annotated data. In particular, the major challenges in legal cases relevance annotation are threefold: **1. Expertise-intensive**: The requirement for high-level domain knowledge for accurate annotation, including the intrinsic connection between the factual scenario and the corresponding judicial interpretation, and the interplay of objective conduct and subjective attitudes in shaping the ultimate legal conclusion. **2. Lengthy-text**: The query and candidates often contain thousands of words. **3. Nuance-sensitive**: As shown in Figure 1, besides a comprehensive understanding of the overall context, sensitivity to nuances in context and terms is equally important. Because such minor distinctions can significantly impact judicial interpretations and decisions. E.g., \"homicide\" vs \"mansl daughter\", \"minor injury\" vs \"minor assault\", and \"robbery\" and \"Theft\". (Please refer to Section 3.1 Relevance part for further details of legal relevance). Existing work(Kumar et al., 2017; Li et al., 2018) has shown that the annotation capabilities of large language models (LLMs) in general NLP tasks can rival those of crowd workers. Most efforts often involved general relevance or direct relevance signals, like question-answering, and text classification(Kumar et al., 2017; Li et al., 2018). Since relevance labels in legal cases are far more complicated as mentioned earlier, our major research question is whether LLM can perform relevance annotation in the legal field. Although researchers have attempted to build legal domain-specific LLMs such as Chatlaw(Chatlaw, 2018) and LawGPT(Li et al., 2018), they are primarily fine-tuned for general legal Q&A tasks; moreover, due to limitations in parameters and training data volume, their understanding and reasoning capabilities are inferior to general LLMs like GPT-3.5, making it hard to employ them for processing long-text content such as legal case annotation. In this work, we will focus on developing an automatic relevance annotation method for legal case retrieval. Instead of utilizing or building specialized LLMs, which carry many serious constraints in cost and capabilities (Please refer to the next section for details), we employ one of the most powerful general LLMs, GPT-3.5, for domain annotation. To take advantage of the understanding and reasoning capabilities of the general LLM, we instruct it step-by-step(Kumar et al., 2017) with minimal expert guidance. As a result, our approach can activate domain-specific knowledge within the LLM and then it can annotate relevance to align with the criteria used by experts when making judgments. Specifically, we address the challenge of **Lengthy-text** by decomposing the annotation process into sub-progresses, including distinct views in facts extractions and relevance annotations. In each sub-process, we elaborate expert's reasoning process as instructions to ease the **Expertise-Intensive** challenge and mitigate the **Nuance-Sensitivity** issue. In addition, we employ adaptive retrieval of effective demonstrations to further tackle the challenge of **Expertise-Intensive**. Given a group of unlabeled legal cases, we also designed a strategy for efficiently collecting possible positive case pairs. Through empirical experiments, our method demonstrates a high level of consistency with human labels. Furthermore, when applying the generated synthetic data for downstream model fine-tuning, we observed a significant improvement in performance on case retrieval tasks, indirectly validating the effectiveness of our approach. We summarize the major contributions of the paper as follows: * We design an automatic workflow to leverage a general LLM to make relevance judgments for legal cases. To the best of our knowledge, this is the first attempt at automated legal case relevance annotation. * We evaluate and analyze the quality of automatic relevance annotations by comparing them with expert annotations. Empirical experiments demonstrate that our approach can achieve high consistency with expert annotations. * We use the proposed method to generate a synthetic dataset, which can be used to further fine-tune legal case retrieval models, resulting in a significant performance improvement."
    },
    {
      "title": "2. Related Works",
      "text": ""
    },
    {
      "title": "Pre-Trained Language Models (Plms) In Legal Case Retrieval",
      "text": "Pre-trained Language Models (PLMs) like BERT(Devlin et al., 2017), GPT-2(Devlin et al., 2017), and RoBERTa(Kumar et al., 2017) have set new standards in a variety of NLP tasks. These models are trained on vast corpora to learn linguistic patterns and can be fine-tuned on specific tasks. The strength of PLMs lies in their ability to understand context, making them effective for tasks ranging from sentiment analysis to machine translation. However, in the legal domain such as legal case retrieval, general PLMs still struggle with the complex nuances and intensive background domain knowledge of legal jargon and may produce biased or asymmetric predictions. Additionally, legal documents typically consist of thousands of tokens, far exceeding the length that mainstream PLMs can handle(Kumar et al., 2017). Researchers are dedicated to designing specialized models(Devlin et al., 2017; Li et al., 2018; Li et al., 2018; Li et al., 2018) tailored to legal case matching, underscoring the limitations of generic PLMs in accurately interpreting and classifying extensive legal content. Large-scale Language Models (LLMs) are an extension of the PLM concept but on a much grander scale, such as GPT4(Li et al., 2018) and LLaMa(Li et al., 2018). With billions or even trillions of parameters, LLMs are designed to understand and generate human-like text with little to no fine-tuning required for specific tasks, showing significant potential in various fields, including law. Figure 1. An example of a challenge of legal relevance annotation. Although researchers have attempted to train domain-specific LLMs for the legal field, such as Chalaw(Chatlaw, 2018), LawGPT(Lewis et al., 2019), and others, there are still several drawbacks in directly training a dedicated large language model for this domain: 1. The process requires a substantial amount of high-quality data and expensive computational resources. Legal texts can be vast and diverse, necessitating extensive training data to ensure the model's proficiency in understanding the nuances of legal language and reasoning. 2. The legal domain encompasses numerous downstream tasks, each with its own specific requirements and nuances. As a result, domain-specific large-scale language models lack generalizability and reliability across various downstream tasks. Adapting these models to each specific task requires fine-tuning with task-specific data, leading to considerable efforts for collecting specialized training data for each task. 3. Compared to general large models like GPT-3.5, these domain-specific large models suffer from insufficient training scale and data volume, leading to weaker model memorization, comprehension, and reasoning capabilities. Additionally, most of these domain-specific models have mainly undergone fine-tuning for legal question-answering tasks. In this article, we present a lightweight approach that enables general large-scale language models for legal relevance annotation."
    },
    {
      "title": "Prompt Engineering In Data Augmentation",
      "text": "Prompt-based data augmentation is a promising avenue, especially for tasks where data collection is challenging. The \"PromDA\" model, introduced by (Zhou et al., 2017), leverages soft prompts in pre-trained language models to generate high-quality synthetic data for low-resource natural language understanding tasks. Similarly, (Zhou et al., 2017) proposed a method that combines semantic data augmentation with distance metric learning for domain generalization. (Zhou et al., 2017) explored the feasibility of GPT-3 generated synthetic data for training conversational AI classifiers, revealing that while synthetic data offers advantages, it doesn't surpass real user data in performance. In addition, (Zhou et al., 2017) develop a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply, which generates a small number of synthetic queries using an expensive LLM and then uses a much less expensive one to create large numbers of synthetic queries that are used to fine-tune re-rankers. These studies underscore the growing importance and potential of prompt-based data augmentation in enhancing model performance, especially in scenarios with limited data resources. While it offers a novel way to leverage the capabilities of large-scale language models, care must be taken to ensure that the generated data aligns well with real-world scenarios and doesn't introduce biases or errors. As with any data augmentation technique, it's crucial to validate its effectiveness empirically on the target task. Therefore, in this work, we will evaluate our automatic annotation approach from two levels: the consistency with human labeling and the empirical effectiveness of data-augmented models on the LCR task."
    },
    {
      "title": "3. Methodology",
      "text": "In this section, we will first provide a brief overview of the background knowledge, including an introduction to the forms of Chinese legal cases and the factors that experts consider when determining the relevance of legal cases. Then, we will provide a detailed explanation of each step in the proposed automated annotation workflow. Subsequently, we address some questions that readers may be concerned about. Finally, we introduce an application of the proposed annotation method: the construction of a synthetic dataset for training LCR models."
    },
    {
      "title": "Preliminary",
      "text": "**Legal case documents** diverge significantly from the general domain content. Their distinctive feature lies in their structured composition, providing both clarity and complexity. This structure is pivotal for the legal profession, as it captures the essence and progression of a case, from presenting the parties involved to the court's final decision, which basically includes five parts: _Procedure_: This is the foundational section of any legal case document. It introduces essential information about the parties involved and outlines the procedural posture. _Fact_: Beyond mere introductions, this section delves into the core contentions of the parties. Here, one finds descriptions of arguments, evidence presented, and pivotal events forming the crux of the legal dispute. This is the only information that a pending (query) case has. _Judgment process_: It explains the selection of specific rules and their application to the case facts. _Verdict_: This section offers the court's final conclusive response to the case. We will evaluate our annotation method on LeCaRD(Zhou et al., 2017), the Chinese legal case retrieval dataset, of which the data format is shown in Figure 3. **Relevance**: According to LeCaRD, the determination of case relevance primarily depends on the key facts of the case, which are composed of \"Material Facts (MF)\" and \"Legal Facts (LF)\". Legal Facts are the legal evaluations of Material Facts. For example, in a hypothetical case: Fact: A had persistent conflicts with B. Consequently, A found a pretext to provoke B and then assaulted him. B's injuries were classified as \"Level 2 minor injury. Legal Facts: Arbitrary assault on others; causing minor injuries to others; acts that disrupt social order. Material Facts: Occasional conflicts, seeking trouble under pretense, unprovoked assault; \"Level 2 minor injury.\" To determine whether different cases are related, one should deep into the Material Facts and the Legal Facts. The Material Facts of a case include the identity of multiple factors such as the defendant and the victim, the cause, process, outcome, time, and location of the crime committed by the defendant. Judging the relevance of MF involves a \"factual level\" assessment of the relatedness of different cases, focusing on whether there is \"identity\" between multiple factors. Judging the relevance of LF includes the subjective motive of the defendant, whether the victim is at fault, and whether there is a causal relationship between the criminal act and the damage caused. This involves an adaptive \"legal level\" assessment of in different legal aspects, focusing on whether there is \"homogeneity\" between two cases. In practice, the judgment should follow the order of \"Material Facts first, Legal Facts second.\" Both are necessary and sufficient conditions for the assessment of case-relevance. **Problem Formulation**: The legal case retrieval task is defined as finding cases related to a given query case from a set of candidate cases. Specifically, given a query case \\(q\\) and a set of candidate cases\\(D\\), the objective is to retrieve the top-\\(k\\) relevant cases from a vast candidate pool, denoted as \\(D_{q}=d_{1},d_{2},\\ldots,d_{k}\\)."
    },
    {
      "title": "Automated Relevance Annotation For Legal Cases",
      "text": "We have meticulously designed a prompt workflow for legal case retrieval, which can be segmented into four steps: 1) Preliminary factual analysis by legal professionals, 2) Adaptive Demo-Matching (ADM), 3) Fact Extraction (FE), and 4) Fact Annotation (FA). Figure 2 shows the overall annotation workflow. In the subsequent sections, we will delve into each of these components in detail. **Preliminary factual analysis by legal professionals**: While the relevance annotations of the original data were made based on the criteria mentioned in Section 3.1, the annotated data only contain relevance scores without explicit matching labels, preventing the understanding of the exact facts that determine the similarity between two cases. Accurate and direct indications of what the 'Material Fact' in the current case is, its connection to the details in the text, followed by additional legal knowledge and reasoning logic used for identifying such connections, can trigger LLMs to utilize their inherent knowledge and comprehension abilities to mimic human experts' cognitive processes. Such an approach is deemed necessary because we observed that without detailed relevance indications, the model's judgment could deviate. For instance, if case 1 involves person A stealing a car and case 2 involves person B stealing a wallet, the LLM sometimes perceives a car and a wallet as dissimilar items, focusing on trivial facts, thereby simply judging the two cases as irrelevant. Thus We consider such legal reasoning order as a logical chain, prompting the LLM to make judgments consistent with those of the professionals. Figure 4 and Figure 5 shows examples of experts' demonstrations for extractions of MF and LF. It's worth mentioning that, as stated in section 3.1, in practice, the judgment should follow the order of 'Material Facts first, Legal Facts second'. The extraction of Legal Facts, therefore, involves a multi-dimensional analysis built upon Material Facts, encompassing considerations of the defendant's subjective motive, the potential fault of the victim, and the existence of a causal relationship between the criminal act and the resulting harm. Then during the annotation phase, it is crucial to perform adaptive relevance assessments tailored to different case pairs, based on the Figure 3. An example of data format in LeCaRD. Note that the original language is Chinese and the English texts are translations. Figure 2. The proposed workflow of data annotation for legal case retrieval task. connection between Material Facts (MF) and Legal Facts (LF). We incorporate carefully designed expert reasoning logic of analyzing relevance to instruct the LLm in adhering to the criterion of experts. Figure 6 shows an example of demonstrations for the fact annotation. **Adaptive Demo-Matching (ADM)**: When applying few-shot prompting to LLMs, we adaptively fit appropriate demonstrations for input cases by retrieval in each prompting step from the small-scale pre-built expertise demonstration set. By doing so, as opposed to random selection, we can minimize the disparity between the demonstration and the input cases, facilitating the LLM to better follow legal experts. In this work, we employ a simple but effective method, BM25[16], to rapidly retrieve demonstrations, which could be improved by more sophisticated models in the future. Figure 4. The demonstration of extractions of MF for few-shot prompting. (Translation of Chinese texts.) Figure 5. The demonstration of extractions of LF for few-shot prompting. (Translation of Chinese texts.) Figure 6. The demonstrations of MF and LF annotation for few-shot prompting. (Translation of Chinese texts.) **Fact extraction (FE)**: As illustrated in Figure 7, we present an framework of FE prompting. Following the order of 'Material Facts first, Legal Facts second', we separately extract MF and LF sequentially. In the prompt, we provide the task description, definitions of Material Facts or Legal Facts, and the top 2 demos collected by ADM. Finally, the target case is input into the prompt. Specifically, for the MF extraction, the input is the case details, and for the LF extraction is the extracted MF. **Few-shot annotation (FA)**: As illustrated in Figure 8, we present an framework of FA prompting. Similarly, we separately annotate MF and LF sequentially. Given as input the facts of a pair of cases to be labeled, the prompt contains relevant as well as irrelevant demonstrations. Ultimately, the LLMs are instructed to assess the relevance of the current input pair, drawing upon the expert's rationale. The calculation of the final relevance score follows the method of LeCaRD: score 1 for MF relevance, and score 2 for LF relevance, with the total relevance score calculated as the sum of MF and LF relevance scores."
    },
    {
      "title": "Other Statements",
      "text": "Our work is currently focused on Chinese law due to the easy access to Chinese legal experts, who can help us build a small yet high-quality set of expert demonstrations. Additionally, given the scarcity of annotated legal data (the very issue we aim to address), we conduct research on LeCaRD, the sole case retrieval dataset in the Chinese legal domain. To apply our approach to other countries' legal cases, the same process can be utilized. The only necessary change is to refine the content under the framework of our demonstrations, with the help of domain experts in the corresponding legal field and languages. Apart from that, the data of Chinese legal documents includes more than just the \"basic case information\", but we only utilize this part for relevance annotation. There are two main reasons for this: * **Fact-Driven**: In practical scenarios for judges and lawyers, it is necessary to retrieve similar cases based on the details of new cases that only contain case facts. It's the same in the query cases of the Lecard dataset, which only contain case facts. The goal of legal case retrieval is to ensure similar cases receive fair and consistent adjudication, which is why the analysis starts from the case itself, rather than basing it on the outcomes of judgments. Our approach is thus in line with practical requirements. * **Consistency with Dataset Standards**: By focusing on case facts, our approach aligns with the Lecard dataset's method of annotation, which is based on key facts (MF and LF) within case details. This ensures that our method aligns with the dataset's standards for relevance annotation."
    },
    {
      "title": "Application Of Annotated Data",
      "text": "An application of our method is data augmentation: to construct a synthetic dataset from unlabeled legal cases for model training. Take LeCaRD as an example, where there are a total of 10,700 candidate cases. By randomly selecting two candidate cases each time for relevance annotation, theoretically, we can generate over 50 million pairs of data. This could significantly scale the annotated dataset. We tried this on a small scale: We first randomly generated 200k candidate case pairs. Our goal is to produce more high-quality positive examples. Considering that the case pairs randomly extracted have a high probability of being irrelevant, the 200k case pairs were pre-sorted by a BERT model fine-tuned on the original dataset (see the Experiment section for implementation details) and we performed relevance annotation on the top 50k data. After obtaining these annotations, we constructed two augmented datasets intended for fine-tuning the retrieval models: the first, a 20k dataset with a distribution that closely mirrors the label distribution of the original dataset; and the second, a 40k dataset selected randomly from the 50k annotations."
    },
    {
      "title": "4. Experiment",
      "text": "In this section, we conduct various empirical experiments to answer the following research questions (RQs): **RQ 1** How reliable are the relevance judgments obtained with the proposed workflow? **RQ 2** How do the LLM-based relevance judgments align with the judgments from human annotators? **RQ 3**How can we leverage the LLM-based relevance judgments to augment existing legal case retrieval models? Figure 8. The example of few-shot annotation FA. (Translation of Chinese texts.) Figure 7. The example of fact extraction FE. (Translation of Chinese texts.) [MISSING_PAGE_FAIL:7]"
    },
    {
      "title": "Baselines For Data Augmentation Experiments",
      "text": "To further validate the annotation quality, we examine the improvements achieved when using annotated synthetic data for training different retrievers in LCR tasks. This will further explore the effectiveness of this automatic annotation method, which will be detailed in subsequent chapters. Here, we select only two basic deep models but not more complicated models like (Dosov et al., 2017). Since our goal is to compare the relative differences in model performance before and after using the proposed annotation for data augmentation, to validate whether downstream models can be effectively augmented, rather than pursuing a state-of-the-art model for the LCR task. * **BM25**(Kang et al., 2018) is a simple but effective baseline for LCR. * **BERT**(Devlin et al., 2017) is built on the Transformer architecture, which employs self-attention mechanisms to weigh input tokens differently, allowing for a more nuanced representation of context. * **Longformer**(Devlin et al., 2017) is an adaptation of the Transformer architecture designed to process exceptionally long sequences efficiently. It achieves this by combining local attention with a limited number of global attention mechanisms, reducing the computational complexity from quadratic to linear for most tokens. This design enables direct processing of tasks involving extensive documents without the need for chunking or hierarchical methods."
    },
    {
      "title": "Implementation Details",
      "text": "All experiments are conducted on the LeCaRD dataset. The size of the demonstration set we built is 40 groups. Each group consists of a query case and four candidate cases with different relevance levels, which are evaluated in both FE and FA. For the annotation evaluation, we compare the annotation of the proposed method with the golden labels of the top 30 candidate cases in every query case. In the heatmap experiment, we randomly sample 100 human-labeled pairs for each relevance, from 0 to 3, 400 in total. For the data augmentation experiments, we split LeCaRD in a ratio of 8:1:1 for training, validation, and testing. The augmented data pairs are sampled from the candidate case corpus from the training set. We use PyTorch (Paszke et al., 2017) to implement our method. We use Nvidia 3090 with 24G GPU memory. We use the Chinese tokenizer jieba 1 in data pre-processing and use AdamW (Kingmae et al., 2014) optimizer in fine-tuning procedure. All hyper-parameters are tuned based on the performance of the validation set. Specifically, when using the BERT baseline model, We adopted a segmentation method similar to BERT-PLI, splitting long texts into chunks of length 256 with overlapping window sizes of 128. The [CLS] token serves as the embedding for each block, and during relevance computation, we cross-calculate the dot product of the query and candidate block vectors. Furthermore, the LeCaRD is in Chinese, so we use the Chinese version of Figure 11. The heatmap of the annotated four-level relevance vs golden labels. Figure 12. The heatmap of the annotated MF&LF relevance vs golden labels. Figure 10. The Cohen’s Kappa between the MF&LF and the golden labels over different temperature settings: The x-axis represents the temperature, while the y-axis indicates Cohen’s Kappa value. ’Avg.’ represents the average of Kappa values between multiple annotation results and the golden labels, measuring the consistency of our annotations with golden labels, which indicates the validity of our approach. Longformer, longformer-chinese. For large language models, we opted for the robust GPT-3.5-turbo 2. Footnote 2: [https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)"
    },
    {
      "title": "Results Of Data Augmentation Experiments",
      "text": "Table 3 illustrates the performance of several baseline models on the task of legal case retrieval. We compared the performance changes of Bert and Longformer before and after fine-tuning on synthetic data, with BM25 retrieval results only used as a performance reference here. According to the results, we have some observations as follows: We pre-trained the BERT-based case retrieval model on three different scales of synthetic datasets. The 2k and the 20k datasets have a similar positive-to-negative ratio based on the distribution of the training set. The 40k dataset is randomly collected from all generated data. It can be observed that both BERT and Longformer, when trained on the 2k synthetic dataset of the same scale as the training set, already achieve significant improvements. The performance further significantly improves when trained on the larger 20k synthetic dataset. This demonstrates the data augmented by our method has effectively simulated real data. Intriguingly, even with twice the data expansion, the performance on the 40k dataset was notably worse than on the 20k dataset. There might be two explanations for this: A training dataset closer to the testing dataset in distribution benefits the model more significantly. Random sampling without selection or filtering retains fewer positives and more unnecessary negatives. An excessive number of negative examples will not necessarily bring more value to the model."
    },
    {
      "title": "Ablation Study",
      "text": "In this section, we aim to explore the impact of each stage in our proposed data annotation workflow on the final quality of the generated data. The ablation study is conducted for both annotation and augmentation. According to Table 4, removing ADM impacts the annotation of MF notably. This indicates that the extraction and relevance assessment of Material Facts relies on more pertinent expert guidance. This may be due to the variable nature of Material Facts. Meanwhile, Legal Facts remain unaffected due to their normative formats. This also highlights that the understanding of Legal Facts in the case benefits more from the FE and FA modules. When FE is removed, the Kappa scores for both types of facts significantly decrease, and the effect is similar when both FE and FA are removed. This demonstrates that the fact extraction step we designed is the most crucial part of the entire annotation process. Based on this observation, it can be inferred that LIM faces difficulty in accurately identifying key legal elements. Removing FA has little impact on LF annotation, but it does negatively affect MF. This indirectly confirms that extracting MF from case pairs and assessing their relevance is a challenging aspect of judging legal case relevance, and our method addresses this challenge through a series of carefully designed processes. For the augmentation ablation shown in Table 5, when not using ADM, the randomly selected demo might differ significantly from the current query case in content. This weakens the guiding role of expert knowledge for the LIM during information extraction and annotation, leading to a noticeable impact on the performance of the downstream task model. Also, by eliminating the expert knowledge guidance in the FA phase, the performance of the downstream model is similarly weakened. We speculate that the criteria for similarity judgment in the last step are clearer with expert guidance. The performance of the downstream model is greatly affected when \\begin{table} \\begin{tabular}{l l} \\hline \\hline **MODEL\\&DATA** & **NDCG@30** \\\\ \\hline BM25 & 0.8532 \\\\ BERT & 0.8816 \\\\ BERT+2k & 0.8979 \\\\ BERT+20k & 0.9305 \\\\ BERT+40k & 0.9156 \\\\ Longformer & 0.9019 \\\\ Longformer+20k & 0.9458 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4. The impact of different components for annotation quality: Original indicates the proposed approach. w/o ADM indicates the method of selecting examples from the demo library using random sampling; w/o FE means letting the LLM extract the two types of facts directly in the prompting workflow without prompt guidance and demo examples; w/o FA indicates that after extracting the two types of facts from each case, the similarity is directly labeled without the guidance of positive and negative demos in the original FA progress. \\begin{table} \\begin{tabular}{l l} \\hline \\hline Annotation methods & MF Kappa & LF Kappa \\\\ \\hline Original & 0.34 & 0.53 \\\\ w/o ADM & 0.23 & 0.54 \\\\ w/o FE & 0.09 & 0.21 \\\\ w/o FA & 0.19 & 0.50 \\\\ w/o FE\\&FA & 0.13 & 0.17 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5. The impact of different components for augmentation effectiveness. \\begin{table} \\begin{tabular}{l l} \\hline \\hline Model & NDCG@30 \\\\ \\hline BERT2K & 0.8979 \\\\ BERT2K w/o ADM & 0.8874 \\\\ BERT2K w/o FE & 0.8548 \\\\ BERT2K w/o FA & 0.8812 \\\\ BERT2K w/o FA\\&FA & 0.8501 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5. The impact of different components for augmentation effectiveness. FE is removed, even worse than when no synthetic data is applied. We believe this is because, when identifying the two Legal Facts that have a decisive impact on case similarity, the LLM struggles to independently recognize and judge key legal elements and their interconnections. Our few-prompt method inspires the LLM to mimic experts. Every stage within the prompt workflow contributes significantly to enhancing the quality of domain data annotation. Both Adaptive Demo-Matching (ADM) and Few-shot Annotation (FA) moderately influence the quality of annotations, with the impact of ADM being more pronounced. ADM validates that relevant demonstrations indeed provide LLMs with a more robust basis for reasoning. On the other hand, FA introduces clearer constraint conditions for relevance annotations. Notably, the most pivotal process is FE, involving the most specialized knowledge Furthermore. It also suggests that our FE method can effectively leverage minimal expert instructions to inspire and guide LLMs in capturing subtle but vital nuances within professional data."
    },
    {
      "title": "5. Conclusions And Future Works",
      "text": "In this paper, we proposed an automated relevance annotation method for legal cases. Through empirical experiments, we demonstrate its good reliability and high consistency to human labeling. With minimal expert knowledge adaption, he proposed workflow could be applied to other legal areas such as civil law, contract law, and the law of other countries. Hopefully, it can also be used as a supplement in other legal tasks in the future, such as automated judgment and defense. These are promising and meaningful directions we can explore."
    },
    {
      "title": "References",
      "text": "* (1) * L. Beltagy, Matthew E Peters, and Arman Cohan (2020)Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. Cited by: SS1. * I. Chalidis, M. Fergergiola, P. Malakasiotis, N. Aletras, and R. Androutsopoulos (2020)LEGAL-BERT: the muppetski straight out of law school. arXiv preprint arXiv:2010.02559. Cited by: SS1. * J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan (2023)Challaw: open-source legal large language model with integrated external knowledge bases. arXiv preprint arXiv:2306.10028. Cited by: SS1. * J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018)Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Cited by: SS1. * G. Fargooli, L. Dietz, C. L. A. Clarke, G. Demartini, M. Hagen, C. Hauff, N. Kando, E. Kanolakis, M. Pothast, B. Stein, and H. Wachsmuth (2023)Perspectives on large language models for relevance judgment. In Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval (ICITR 23), pp. 10145/357837.3605136. * F. Glorland, M. Alizadeh, and M. Kubli (2023)Clostup outperforms crowd-swortless for text-and-attention tasks. arXiv preprint arXiv:2303.15056. Cited by: SS1. * K. Lagler, M. Schindelegger, J. Bohm, H. Krasian, and T. Nilsson (2013)GPTE: empirical stellar delay model for radio space geodetic techniques. Geophysical research letters40 (6), pp. 1069-1073. Cited by: SS1. * H. Li, Q. Ai, J. Chen, Q. Dong, Y. Wu, Y. Liu, C. Chen, and Q. Tian (2023)SAILER: structure-aware free-trained language model for legal data retrieval. arXiv preprint arXiv:2304.11769. Cited by: SS1. * Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov (2019)Robert: a robustly optimized perft pretraining approach. arXiv preprint arXiv:1907.11626. Cited by: SS1. * M. Y. Liu, H. Cheng, L. Pusheny and W. Yuhao (2023)LawFGTE (2023). Note: [https://github.com/LiuHC0428/LAW_GPT](https://github.com/LiuHC0428/LAW_GPT) Cited by: SS1. * I. Loshchilov and F. Hutter (2019)Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, pp. 1105-1116. External Links: Link Cited by: SS1. * Y. Ma, Y. Shao, Y. Wu, Y. Liu, R. Zhang, M. Zhang, and S. Ma (2021)LeCaRD: a legal case retrieval dataset for chinese law system. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pp. 2342-2348. Cited by: SS1. * S. Meyer, D. Elsweiler, B. Ludwig, M. Fernandez-Pichel, and D. E. Loosda (2022)Do we still need human assess? prompt-based gpt-based gpt-aware simulation in conversational ai. In Proceedings of the 4th Conference on Conversational User Interfaces, pp. 1-6. Cited by: SS1. * O. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. (2019)Pytorch: an imperative style, high-performance deep learning library. Advances in neural information processing systems32. Cited by: SS1. * S. E. Robertson and S. Walker (1994)Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR'94, pp. 232-241. Cited by: SS1. * J. Saad-Falcon, O. Khattels, K. Santhamam, R. Florian, M. Franz, S. Roukos, A. Sil, M. M. Sultan, and C. Potts (2023)UDAPDR: unsupervised domain adaptation via lll prompting and distillation of kernelness. arXiv preprint arXiv:2303.08087. Cited by: SS1. * Y. Shao, J. Xiao, Y. Liu, W. Liu, K. Satoh, M. Zhang, and S. Ma (2020)BERT-PLL: modeling paragraph-level interactions for legal case retrieval. In IJCAI, pp. 3501-3507. Cited by: SS1. * Y. Shao, Y. Wu, Y. Liu, J. Mao, and S. Ma (2023)Understanding relevance judgments in legal case retrieval. ACM Transactions on Information Systems41 (3), pp. 1-32. Cited by: SS1. * R. Thalken, E. H. Siglitz, D. Mimno, and M. Wilkens (2023)Modeling legal reasoning: ll annotation at the edge of human agreement. arXiv preprint arXiv:2301.10445 (csL). Cited by: SS1. * P. Thomas, S. Spikman, N. Craswell, and B. Mitra (2023)Large language models can accurately predict searcher preferences. arXiv:2309.10621 (csL). Cited by: SS1. * H. Touyron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hamb, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lumez (2023)ILAMA: open and efficient foundation language models. Note: [http://arxiv.org/abs/2302.13971](http://arxiv.org/abs/2302.13971) cite arxiv:2302.13971 Cited by: SS1. * M. Wang, J. Yuan, Q. Qian, Z. Wang, and H. Li (2022)Semantic data augmentation based instance metric learning for domain generalization. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 3214-3223. Cited by: SS1. * Y. Wang, C. Xu, Q. Sun, H. Hu, C. Tao, X. Gong, and D. Jiang (2022)Promix: prompt-based data augmentation for low-resource nluk tasks. arXiv preprint arXiv:2202.12490. Cited by: SS1. * J. Wei, X. Wang, D. Schummurs, M. Bosma, F. Xia, E. Chi, O. Y. Le, D. Zhou, et al. (2022)Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems35. Cited by: SS1. * C. Xiao, X. Hu, Z. Liu, C. Tu, and M. Sun (2021)Lawformer: a pre-trained language model for chinese legal long documents. AI Open2. Cited by: SS1."
    }
  ]
}