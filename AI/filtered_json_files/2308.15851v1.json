{
  "title": "Prompting Vision Language Model with Knowledge from Large Language Model for Knowledge-Based VQA",
  "authors": [
    "Yang Zhou",
    "Pengfei Cao",
    "Yubo Chen",
    "Kang Liu",
    "Jun Zhao"
  ],
  "abstract": "\n Knowledge-based visual question answering is a very challenging and widely concerned task. Previous methods adopt the implicit knowledge in large language models (LLM) to achieve excellent results, but we argue that existing methods may suffer from biasing understanding of the image and insufficient knowledge to solve the problem. In this paper, we propose PROOFREAD -PROmpting vision language model with knOwledge From laRgE lAnguage moDel, a novel, lightweight and efficient knowledge-based VQA framework, which make the vision language model and the large language model cooperate to give full play to their respective strengths and bootstrap each other. In detail, our proposed method uses LLM to obtain knowledge explicitly, uses the vision language model which can see the image to get the knowledge answer, and introduces knowledge perceiver to filter out knowledge that is harmful for getting the correct final answer. Experimental results on two datasets prove the effectiveness of our approach. Our method outperforms all state-of-the-art methods on the A-OKVQA dataset in two settings and also achieves relatively good performance on the OKVQA dataset. \n",
  "references": [
    {
      "id": null,
      "title": "Prompting Vision Language Model with Knowledge from Large Language Model for Knowledge-Based VQA",
      "authors": [
        "Yang Zhou",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": [
        "J Alayrac",
        "J Donahue",
        "P Luc",
        "A Miech",
        "I Barr",
        "Y Hasson",
        "K Lenc",
        "A Mensch",
        "K Millican",
        "M Reynolds",
        "R Ring",
        "E Rutherford",
        "S Cabi",
        "T Han",
        "Z Gong",
        "S Samangooei",
        "M Monteiro",
        "J L Menick",
        "S Borgeaud",
        "A Brock",
        "A Nematzadeh",
        "S Sharifzadeh",
        "M Binkowski",
        "R Barreira",
        "O Vinyals",
        "A Zisserman",
        "K Simonyan"
      ],
      "year": "2022",
      "venue": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "VQA: Visual Question Answering",
      "authors": [
        "S Antol",
        "A Agrawal",
        "J Lu",
        "M Mitchell",
        "D Batra",
        "C L Zitnick",
        "D Parikh"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "T B Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D M Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "XGBoost: A Scalable Tree Boosting System",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Scaling Instruction-Finetuned Language Models",
      "authors": [
        "H W Chung",
        "L Hou",
        "S Longpre",
        "B Zoph",
        "Y Tay",
        "W Fedus",
        "E Li",
        "X Wang",
        "M Dehghani",
        "S Brahma",
        "A Webson",
        "S S Gu",
        "Z Dai",
        "M Suzgun",
        "X Chen",
        "A Chowdhery",
        "S Narang",
        "G Mishra",
        "A Yu",
        "V Y Zhao",
        "Y Huang",
        "A M Dai",
        "H Yu",
        "S Petrov",
        "E H Chi",
        "J Dean",
        "J Devlin",
        "A Roberts",
        "D Zhou",
        "Q V Le",
        "J Wei"
      ],
      "year": "2022",
      "venue": "Scaling Instruction-Finetuned Language Models",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering",
      "authors": [
        "Y Ding",
        "J Yu",
        "B Liu",
        "Y Hu",
        "M Cui",
        "Q Wu"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
      "authors": [
        "Y Fang",
        "W Wang",
        "B Xie",
        "Q Sun",
        "L Wu",
        "X Wang",
        "T Huang",
        "X Wang",
        "Y Cao"
      ],
      "year": "2022",
      "venue": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "KAT: A Knowledge Augmented Transformer for Vision-and-Language",
      "authors": [
        "L Gui",
        "B Wang",
        "Q Huang",
        "A Hauptmann",
        "Y Bisk",
        "J Gao",
        "J Hoffmann",
        "S Borgeaud",
        "A Mensch",
        "E Buchatskaya",
        "T Cai",
        "E Rutherford",
        "D De Las Casas",
        "L A Hendricks",
        "J Welbl",
        "A Clark",
        "T Hennigan",
        "E Noland",
        "K Millican",
        "G Van Den Driessche",
        "B Damoc",
        "A Guy",
        "S Osindero",
        "K Simonyan",
        "E Elsen",
        "J W Rae",
        "Vinyals"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "$Qˆ2$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering",
      "authors": [
        "O Honovich",
        "L Choshen",
        "R Aharoni",
        "E Neeman",
        "I Szpektor",
        "O Abend"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "PromptCap: Prompt-Guided Task-Aware Image Captioning",
      "authors": [
        "Y Hu",
        "H Hua",
        "Z Yang",
        "W Shi",
        "N A Smith",
        "J Luo"
      ],
      "year": "2022",
      "venue": "PromptCap: Prompt-Guided Task-Aware Image Captioning",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S C H Hoi"
      ],
      "year": "2023",
      "venue": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Unified Demonstration Retriever for In-Context Learning",
      "authors": [
        "X Li",
        "K Lv",
        "H Yan",
        "T Lin",
        "W Zhu",
        "Y Ni",
        "G Xie",
        "X Wang",
        "X Qiu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Microsoft COCO: Common Objects in Context",
      "authors": [
        "T Lin",
        "M Maire",
        "S J Belongie",
        "J Hays",
        "P Perona",
        "D Ramanan",
        "P Dollár",
        "C L Zitnick"
      ],
      "year": "2014",
      "venue": "Computer Vision -ECCV 2014 -13th European Conference",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "What Makes Good In-Context Examples for GPT-3?",
      "authors": [
        "J Liu",
        "D Shen",
        "Y Zhang",
        "B Dolan",
        "L Carin",
        "W Chen"
      ],
      "year": "2022",
      "venue": "Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA",
      "authors": [
        "K Marino",
        "X Chen",
        "D Parikh",
        "A Gupta",
        "M Rohrbach"
      ],
      "year": "2021",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge",
      "authors": [
        "K Marino",
        "M Rastegari",
        "A Farhadi",
        "R Mottaghi"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C L Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray",
        "J Schulman",
        "J Hilton",
        "F Kelton",
        "L Miller",
        "M Simens",
        "A Askell",
        "P Welinder",
        "P F Christiano",
        "J Leike",
        "R Lowe"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
      "authors": [
        "Z Peng",
        "W Wang",
        "L Dong",
        "Y Hao",
        "S Huang",
        "S Ma",
        "F Wei"
      ],
      "year": "2023",
      "venue": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Learning To Retrieve Prompts for In-Context Learning",
      "authors": [
        "O Rubin",
        "J Herzig",
        "J Berant"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "A-OKVQA: A Benchmark for Visual Question Answering Using World Knowledge",
      "authors": [
        "D Schwenk",
        "A Khandelwal",
        "C Clark",
        "K Marino",
        "R Mottaghi"
      ],
      "year": "2022",
      "venue": "Computer Vision -ECCV 2022 -17th European Conference",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Get To The Point: Summarization with Pointer-Generator Networks",
      "authors": [
        "A See",
        "P J Liu",
        "C D Manning"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering",
      "authors": [
        "Z Shao",
        "Z Yu",
        "M Wang",
        "J Yu"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "MP-Net: Masked and Permuted Pre-training for Language Understanding",
      "authors": [
        "K Song",
        "X Tan",
        "T Qin",
        "J Lu",
        "T Liu",
        "H Larochelle",
        "M Ranzato",
        "R Hadsell",
        "M Balcan",
        "H Lin"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
      "authors": [
        "R Speer",
        "J Chin",
        "C Havasi"
      ],
      "year": "2017",
      "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Com-monsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge",
      "authors": [
        "A Talmor",
        "J Herzig",
        "N Lourie",
        "J Berant"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "The TREC-8 Question Answering Track",
      "authors": [
        "E M Voorhees",
        "D M Tice"
      ],
      "year": "2000",
      "venue": "Proceedings of the Second International Conference on Language Resources and Evaluation, LREC 2000",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Explicit Knowledge-based Reasoning for Visual Question Answering",
      "authors": [
        "P Wang",
        "Q Wu",
        "C Shen",
        "A R Dick",
        "Van Den Hen"
      ],
      "year": "2017",
      "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "FVQA: Fact-Based Visual Question Answering",
      "authors": [
        "P Wang",
        "Q Wu",
        "C Shen",
        "A R Dick",
        "A Van Den Hengel"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data",
      "authors": [
        "S Wang",
        "Y Xu",
        "Y Fang",
        "Y Liu",
        "S Sun",
        "R Xu",
        "C Zhu",
        "M Zeng"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Multi-Modal Answer Validation for Knowledge-Based VQA",
      "authors": [
        "J Wu",
        "J Lu",
        "A Sabharwal",
        "R Mottaghi"
      ],
      "year": "2022",
      "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA",
      "authors": [
        "Z Yang",
        "Z Gan",
        "J Wang",
        "X Hu",
        "Y Lu",
        "Z Liu",
        "L Wang"
      ],
      "year": "2022",
      "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Deep Multimodal Neural Architecture Search",
      "authors": [
        "Z Yu",
        "Y Cui",
        "J Yu",
        "M Wang",
        "D Tao",
        "Q Tian",
        "C W Chen",
        "R Cucchiara",
        "X Hua",
        "G Qi",
        "E Ricci",
        "Z Zhang",
        "R Zimmermann"
      ],
      "year": "2020",
      "venue": "MM '20: The 28th ACM International Conference on Multimedia",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "OPT: Open Pre-trained Transformer Language Models",
      "authors": [
        "S Zhang",
        "S Roller",
        "N Goyal",
        "M Artetxe",
        "M Chen",
        "S Chen",
        "C Dewan",
        "M T Diab",
        "X Li",
        "X V Lin",
        "T Mihaylov",
        "M Ott",
        "S Shleifer",
        "K Shuster",
        "D Simig",
        "P S Koura",
        "A Sridhar",
        "T Wang",
        "L Zettlemoyer"
      ],
      "year": "2022",
      "venue": "OPT: Open Pre-trained Transformer Language Models",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Active Example Selection for In-Context Learning",
      "authors": [
        "Y Zhang",
        "S Feng",
        "C Tan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
      "authors": [
        "D Zhu",
        "J Chen",
        "X Shen",
        "X Li",
        "M Elhoseiny"
      ],
      "year": "2023",
      "venue": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual Question Answering",
      "authors": [
        "Z Zhu",
        "J Yu",
        "Y Wang",
        "Y Sun",
        "Y Hu",
        "Q Wu"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Prompting Vision Language Model With Knowledge From Large Language Model For Knowledge-Based Vqa",
      "text": "Yang Zhou\\({}^{1,2}\\), Pengfei Cao\\({}^{1,2}\\), Yubo Chen\\({}^{1,2}\\), Kang Liu\\({}^{1,2}\\), Jun Zhao\\({}^{1,2}\\) \\({}^{1}\\) The Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China \\({}^{2}\\) School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, 100049, China {yang.zhou2020, pengfei.cao, yubo.chen, kliu, jzhao}@nlpr.ia.ac.cn"
    },
    {
      "title": "Abstract",
      "text": "Knowledge-based visual question answering is a very challenging and widely concerned task. Previous methods adopt the implicit knowledge in large language models (LLM) to achieve excellent results, but we argue that existing methods may suffer from biasing understanding of the image and insufficient knowledge to solve the problem. In this paper, we propose PROOfreeAD -**PRO**mpting vision language model with knOvidedge from **laRgE**L**anguage moDel, a novel, lightweight and efficient knowledge-based VQA framework, which make the vision language model and the large language model cooperate to give full play to their respective strengths and bootstrap each other. In detail, our proposed method uses LLM to obtain knowledge explicitly, uses the vision language model which can see the image to get the knowledge answer, and introduces knowledge perceive to filter out knowledge that is harmful for getting the correct final answer. Experimental results on two datasets prove the effectiveness of our approach. Our method outperforms all state-of-the-art methods on the A-OKVQA dataset in two settings and also achieves relatively good performance on the OKVQA dataset."
    },
    {
      "title": "Introduction",
      "text": "Visual question answering (VQA) [1] aims to allow machines to understand images and answer free-form questions by reasoning on given images. However, in real scenarios, it is not enough to answer questions only based on the content of images. Machines also need to know some world knowledge or commonsense knowledge of the objects involved in the image. Motivated by this, knowledge-based VQA [22, 16, 17, 18] has received increasing interest these years, which focuses on questions that require knowledge to answer. For example, as shown in Figure 1, to answer the question \"_Which American president is associated with the stuffed animal seen here?_\", it does not only need to recognize that the image is about teddy bears but also should know the relationship between \"_teddy bear_\" and \"_George Roosevelt_\" (George Roosevelt named the teddy bear). Traditional methods [14, 15, 16, 17, 18] model this problem as a two-stage process, where the first stage retrieves knowledge from an explicit external knowledge resources (e.g., Wikipedia, ConceptNet [16]), and the second stage fuses knowledge with images and questions to predict answers. However, due to the incompletion of knowledge bases and the poor generalization of these methods which are limited by the training data scale, it is difficult for them to achieve excellent performance. With the development of large language models (LLMs) [15, 16, 17, 18], in the field of natural language processing (NLP), some knowledge-intensive tasks (e.g., commonsense question answering [16], open domain question answering [15], document summarization [14] and etc.) no longer rely on external knowledge bases and can still achieve human-like performance. Motivated by this, some studies [22, 16, 17, 18, 19] attempt to take advantage of large amount of knowledge stored in the LLMs and its powerful reasoning ability to solve knowledge-based VQA. However, since existing LLMs are basically trained on text, the information in the images cannot be directly understood by text-only Figure 1: Examples of different approaches based on LLMs. Snowflakes represent that the module is frozen, and flames represent that the module is trainable. (a) An example of prompting-based methods, which convert images to captions fed into the LLM. (b) An example of Multimodal interface-based methods, which train a multimodal interface for large models that can understand image inputs. (c) Part of PROOfreeAD (our method), the frozen VLM that leverages knowledge can answer questions correctly. LLMs (e.g., GPT-3 Brown et al.2020), ChatGPT (Ouyang et al.2022)). In order to make up for this gap, some methods have been proposed, which can be roughly divided into two categories: prompting-based methods, and multimodal interface-based methods. **Prompting-based methods**: Some methods Yang et al.2022; Shao et al.2023; Hu et al.2022 try to provide information about the image in the prompt to the LLM and predict the answer. For example, as illustrated in Figure 1(a), PICA Yang et al.2022 prompts a frozen GPT-3 with the caption of the image \"_Lots of toys on a pink blanket_\" to provide the information of the image. However, these methods may suffer from **biasing understanding of the image**, because the caption information of the image is not enough to elicit all the knowledge needed to solve the problem in the LLM. In other words, the limitation of these methods is that the LLM cannot really see the image, and information (captions or answer candidates) is always difficult to exhaust all the key content in the image. Under this condition, it is probably wrong to rely on the caption to prompt LLM to get the final answer. For instance, the caption about the image in Figure 1 is \"_Lots of toys on a pink blanket_\", the contribution of which is very limited to help answer the question \"_Which American president is associated with the stuffed animal seen here?_\". With the caption as the prompt, it is difficult for LLMs to have an accurate and perfect understanding of image content, and then LLMs will be confused to obtain the answer. **Multimodal interface-based methods**: Some studies Li et al.2023; Peng et al.2023; Alayrac et al.2022 attempt to train a multimodal interface for the LLM so that they can directly see the image to obtain the answer, which is also a very important way in constructing vision language model (VLM). As Figure illustrated in 1(b), this kind of VLM often freezes the image encoder and language model, and maps the image representation to the embedding space of the language model by training a multimodal interface on multimodal data. In this framework, almost all knowledge is stored in the language model, if the language model does not contain the knowledge needed to solve the problem, then the question remains unanswerable. In fact, existing VLMs that be publicly available to access still rely on relatively small-scale language models (e.g., FlanT5 Chung et al.2022), OPT Zhang et al.2022)), and the amount of knowledge stored in the model is limited. In summary, these methods may suffer from **insufficient knowledge** to solve the knowledge-based VQA problem. As shown in Figure 1(b), asking questions to the VLM directly will be answered incorrectly. But as shown in Figure 1(c), with the help of appropriate knowledge, the problem can be solved. And according to our sampling statistics, 43% of the error of BLIP2 Li et al.2023) on the AOKVQA dataset can be corrected by providing appropriate knowledge. This phenomenon shows that only relying on the implicit knowledge in the current VLM is not enough to answer the question. Intuitively, in this paper, we propose **PROOFREAD -PRO**mpting vision language model with kn**O**wledge **F**on **laRgE** **la**nguage moDel**, a lightweight and efficient knowledge-based VQA framework. Firstly, to avoid the **biasing understanding of the image**, PROOFREAD adopts a frozen VLM to predict the answers, which can see the image. Secondly, to alleviate the **insufficient knowledge** of existing VLMs, PROOFREAD leverages an LLM to make up for this gap. Apart from making up for the weakness of the above methods, PROOFREAD also has the following two advantages. Firstly, PROOFREAD bridges LLMs and VLMs without a complex training process (Only 3000 samples were selected to adjust a small number of parameters), during which the parameters of LLM and VLM are frozen. Secondly, PROOFREAD provides a paradigm for efficiently generating knowledge by large models, and proposes Knowledge Perceiver, a novel knowledge filtering mechanism, to minimize the impact of harmful knowledge. Experimental results demonstrate the effectiveness of our proposed method. Our contribution can be summarized as follows: * We design a novel lightweight framework PROOFREAD to bridge LLM with the vision language model, which can help VLM with the knowledge from LLM, without complex training process. * PROOFREAD can efficiently generate knowledge by LLMs. The proposed knowledge perceiver can effectively filter harmful knowledge. * We conduct extensive experiments on two public open-domain knowledge-based VQA datasets. Experimental results prove the effectiveness of our method. Our code will be publicly available soon."
    },
    {
      "title": "Related Work",
      "text": "**Knowledge-based Visual Question Answering.** Knowledge-based visual question answering Marino et al.2019; Schwenk et al.2022; Wang et al.2018, 2017) aims to solve questions that cannot be answered based on images alone and requires some knowledge or commonsense. Early works rely heavily on retrieving knowledge from knowledge bases and then training to get answers. Mucko Zhu et al.2020 first proposes to depict an image by a multi-modal heterogeneous graph containing multiple layers of information based on visual, semantic, and knowledge modalities, and then answer questions. KRISP Marino et al.2021 combines implicit knowledge in Transformer and symbolic knowledge in the knowledge bases for the knowledge-based VQA. However, due to the incompleteness of the existing knowledge resources, these methods are not able to achieve decent performance in the open domain knowledge-based VQA Marino et al.2019; Schwenk et al.2022). Benefiting from the powerful reasoning ability and abundant knowledge of large language models(e.g., GPT-3 Brown et al.2020), ChatGPT Ouyang et al.2022), some studies Alayrac et al.2022; Yang et al.2022; Shao et al.2023; Gui et al.2022; Hu et al.2022 utilize large language models as part of their methods and achieve excellent performance. PICA Yang et al.2022 converts images in knowledge-based VQA into captions and uses GPT3 and in-context learning to get the final answer. Flamingo Alayrac et al.2022 proposes a vision language model, which freezes the language model to save all language model capabilities, and trains the multimodal interface on the basis of it so that the model can cope with image input. Prophet (Shao et al., 2023) uses the VQA model to get candidate answers and then leverages GPT-3 to give the final answer through in-context learning. Our method also exploits the large language model. In-Context Learning.The large language model, especially the large language model after instruction tuning, shows a powerful few-shot learning ability, giving some input and output demonstrations to the large language model, it can generate high-quality answers without training. This training-free few-shot learning capability is called in-context learning. And this capability also exists in multimodal large language models trained on multimodal data (Alayrac et al., 2022; Peng et al., 2023; Zhu et al., 2023). Some studies(Liu et al., 2022; Wang et al., 2022; Rubin, Herzig, and Berant, 2022; Zhang, Feng, and Tan, 2022; Li et al., 2023) have shown that constructing a suitable example set can help enhance the ability of in-context learning. REINA (Wang et al., 2022) retrieves samples similar to the test samples on the training set and then performs in-context learning to achieve good performance. Zhang, Feng, and Tan (2022) propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. UDR (Li et al., 2023) proposes a unified model to retrieve demonstrations for a wide range of NLP tasks."
    },
    {
      "title": "Methodology",
      "text": "In this section, we will introduce the details of **PROOFREAD** framework for Knowledge-based VQA. Figure 2 schematically visualizes our approach, which includes three major components: (1) Answer Prediction Module, which consists of a vision language model (VLM) for answering questions about the images based on the visual, question, and knowledge; (2) Knowledge Generation Module, which generates knowledge that may be needed to answer questions; (3) Knowledge Filter Module, which aims for filtering knowledge and giving the final answer. Consequently, these three modules will be introduced in turn."
    },
    {
      "title": "Answer Prediction Module",
      "text": "Answer Prediction Module adopts a frozen Vision Language Model(\\(\\mathcal{M}_{\\mathcal{V}}\\), VLM) as the base model for Knowledge-based VQA because VLM has a certain ability to understand instructions as well as excellent performance. Formally, define a VQA problem as \\(P=(v,q,a)\\), where \\(v,q,a\\) denotes the visual input, question about the image, and the answer to the question. The original answer for VQA can be obtained by VLM \\[\\hat{a}^{o}=\\mathcal{M}_{\\mathcal{V}}(v,q)\\in\\mathbb{R}^{V}, \\tag{1}\\] where \\(\\hat{a}^{o}=\\{\\hat{a}^{o}_{1},\\hat{a}^{o}_{2},...,\\hat{a}^{o}_{t}\\}\\) is the original answer consisting of \\(t\\) words and \\(\\mathbb{R}^{V}\\) is the answer space. And the confidence score about the original answer of \\(\\mathcal{M}_{\\mathcal{V}}\\) is computed as follows: \\[p^{o}=\\prod_{i=1}^{t}p(\\hat{a}^{o}_{i}|\\hat{a}^{o}_{1},...,\\hat{a}^{o}_{i-1}) \\in\\mathbb{R}, \\tag{2}\\] where \\(p(\\hat{a}^{o}_{i}|\\hat{a}^{o}_{1},...,\\hat{a}^{o}_{i-1})\\) denotes the prediction probability of the word \\(\\hat{a}^{o}_{i}\\). Relying on image information alone cannot solve knowledge-based VQA, and some knowledge needs to be Figure 2: The overall framework of **PROOFREAD**. Top: Knowledge Generation Module. Retrieve k similar examples from the Demo Bank according to the current problem (\\(P\\)), and generate m questions through the in-context learning of the Question Model (\\(\\mathcal{M}_{Q}\\)). Generate n pieces of knowledge through the Knowledge Answer Model (\\(\\mathcal{M}_{K}\\) ). Bottom Left: Answer Prediction Module. Answer the questions about the image through the Vision Language Model (\\(\\mathcal{M}_{\\mathcal{V}}\\)). Bottom Right: Knowledge Filter Module. According to the relationship between knowledge and questions, the final answer (e.g., Theodore Roosevelt) is given by Knowledge Perceiver (\\(\\mathcal{M}_{f}\\)). acquired at this time. We design a novel Knowledge Generation Module to obtain knowledge, which will be introduced in the next section. Knowledge Generation Module generates a series piece of knowledge \\(K=\\{K_{1},K_{2},...,K_{n}\\}\\) about the problem \\(P\\). Therefore, our framework combines every piece of Knowledge and the question into prompts through manually designed templates and inputs them into VLM to obtain knowledge-based answers, \\[\\hat{a}^{ki}=\\mathcal{M}_{\\mathcal{V}}(v,E_{1}(K_{i},q))\\in\\mathbb{R}^{V}, \\tag{3}\\] where \\(\\hat{a}^{k}i=\\{\\hat{a}^{ki}_{1},\\hat{a}^{ki}_{2},...,\\hat{a}^{ki}_{t}\\}\\) is the knowledge answer consisting of \\(t\\) words, and \\(E_{1}(\\cdot,\\cdot)\\) is the prompt function to combine question and knowledge. Similarly, the confidence score \\(p^{ki}\\) about the knowledge answer \\(a^{ki}\\) is obtained via Equation 2."
    },
    {
      "title": "Knowledge Generation Module",
      "text": "The goal of the Knowledge Generation Module is to generate knowledge for VQA efficiently and correctly. Our framework leverages ChatGPT [1], a language model with a large amount of parameters and strong interaction ability, to provide knowledge. However, directly informing ChatGPT questions and letting it generate relevant knowledge will lead to subjective inferences in the generated knowledge. The model often gives the guessed answer to the question first and gives the knowledge related to the answer it thinks, which results in limited knowledge generation. Besides, LLMs may suffer from the biasing understanding of the image, and the answer it gives will also be unreliable, which leads to misleading knowledge generation. Based on this, in order to generate knowledge more effectively, we set a Question Model (\\(\\mathcal{M}_{\\mathcal{Q}}\\)) to generate relevant knowledge questions about VQA Problem \\(P\\), and then use a Knowledge Answer Model (\\(\\mathcal{M}_{\\mathcal{K}}\\)) to answer these questions. We decouple the generation process into a question-and-answer format. When generating knowledge questions about the problems, the subjective intention of the model in the generation process will be reduced. It is very important to build high-quality demonstrations in in-context learning [1] for generating knowledge questions. Our method proposes a new way to select high-quality examples. First, we construct a Demo Bank \\(D=\\{d_{i}\\}_{i=1}^{\\tau}\\) from the training set for in-context learning, which contains \\(\\tau\\) samples. Then for new samples during training or testing, demonstrations are selected from the Demo Bank to acquire knowledge. PROOFREAD builds the Demo Bank on the training set. We first randomly select a few samples from the training set. Then ask some knowledge questions manually, and get the corresponding knowledge from Knowledge Answer Model \\(\\mathcal{M}_{\\mathcal{K}}\\). And we adopt the Vision Language Model \\(\\mathcal{M}_{\\mathcal{V}}\\) to get the original answer and the knowledge answer for these samples, using which as the initialization seed of the Demo Bank. For a testing or training sample \\(P=(v,q,a)\\), our framework leverages the encoder of Vision Language Model (\\(\\mathcal{M}_{\\mathcal{V}}\\)) to obtain the representation of the sample \\[\\mathbf{r}=Encoder_{\\mathcal{M}_{\\mathcal{V}}}(v,q)\\in\\mathbb{R}^{d}, \\tag{4}\\] where \\(d\\) stands for hidden size. According to the representation of the sample, we will find the \\(k\\) most similar samples from the Demo Bank \\(D\\) as the demonstrations \\(D_{p}\\) for generating the knowledge question, which can be formulated as \\[S_{QE}=\\underset{i\\in\\{1,2,...,\\tau\\}}{argTopK}\\;\\frac{\\mathbf{r}^{T}\\mathbf{ r}_{i}}{||\\mathbf{r}||_{2}||\\mathbf{r}||_{2}}, \\tag{5}\\] \\[D_{p}=\\{(v_{i},q_{i},a_{i})|i\\in S_{QE}\\}. \\tag{6}\\] Constructing these demonstrations and the current problem into a prompt, we can obtain the knowledge questions generated by Question Model \\(\\mathcal{M}_{\\mathcal{Q}}\\) as \\[Q =\\{Q_{i}\\}_{i=1}^{m}\\] \\[=\\mathcal{M}_{\\mathcal{Q}}(E_{2}(\\{(C(v_{i}),q_{i})|i\\in S_{QE}\\},(C(v),q))), \\tag{7}\\] where \\(m\\) is the number of generated questions, \\(E_{2}(\\cdot,\\cdot)\\) is the prompt function to combine demonstrations and the current problem, \\(C(\\cdot)\\) denotes the caption function to convert the image to caption. Finally, we feed the knowledge questions into Knowledge Answer Model \\(\\mathcal{M}_{\\mathcal{K}}\\) to get the knowledge for the problem \\(P\\) \\[K=\\{K_{i}\\}_{i=1}^{n}=\\mathcal{M}_{\\mathcal{K}}(E_{3}(Q)), \\tag{8}\\] where \\(n\\) is the number of knowledge, \\(E_{3}(\\cdot)\\) is the prompt function for ask knowledge questions to \\(\\mathcal{M}_{\\mathcal{K}}\\). It is worth noting that \\(n\\) is not equal to \\(m\\), because in the prompt, we tell the model answer in points, and each question may generate multiple pieces of knowledge. In order to evaluate the quality of automatically generated questions and knowledge on the training set, we categorize the generated knowledge into three classes: * Useful Knowledge, which makes up for the lack of knowledge that the model has to solve the problem. * Harmful Knowledge, which can mislead the model into giving wrong answers to otherwise solvable problems * Neutral Knowledge, which has no appreciable impact on the model's ability to solve the problem. Therefore, for a sample, the number of useful knowledge generated can be recorded as \\(u\\), and the number of harmful knowledge is \\(h\\) \\[u=\\sum_{i=1}^{n}\\mathbb{I}(\\hat{a}^{ki}=a)\\mathbb{I}(\\hat{a}^{o}\\neq a), \\tag{9}\\] \\[h=\\sum_{i=1}^{n}\\mathbb{I}(\\hat{a}^{ki}\\neq a)\\mathbb{I}(\\hat{a}^{o}=a), \\tag{10}\\] where \\(\\mathbb{I}(\\cdot)\\) denotes indicator function, \\(n\\) is the number of knowledge, \\(a^{ki}\\) is the i-th knowledge answer and \\(a^{o}\\) is the original answer, \\(a\\) is the ground truth answer. Traverse each sample on the training set, find similar samples from the Demo Bank to generate knowledge, get the answers to the problem, and calculate the \\(u\\) and \\(h\\) values. For each newly generated sample, the process to update the DEMO Bank is shown in the Algorithm 1. In order to ensure the diversity of samples in the Demo Bank and to retrieve them efficiently, each time a sample whose similarity with the generated sample is greater than a certain threshold \\(\\lambda\\) is found from the Bank, the two are compared, and the excellent samples are retained."
    },
    {
      "title": "Knowledge Filter Module",
      "text": "This module aims to filter out useful knowledge from the generated knowledge, which can also be defined as classifying knowledge into three classes (useful, harmful, and neutral), and give the final answer according to the classification of knowledge. Our method leverages XGBoost Chen and Guestrin (2016), a gradient-boosted decision tree (GBDT), as the Knowledge Perceiver \\(\\mathcal{M}_{\\mathcal{F}}\\) to classify the knowledge. For a knowledge-based VQA problem \\(P=(v,q,a)\\) We selected 11 relevant features to classify the knowledge, which is introduced as follows: * **Original Confident**. The confident score \\(p^{o}\\) about the original answer \\(a^{o}\\) from \\(\\mathcal{M}_{\\mathcal{V}}\\). * **Knowledge Confident**. The confidence score \\(p^{ki}\\) about the knowledge answer \\(a^{ki}\\) from \\(\\mathcal{M}_{\\mathcal{V}}\\). * **Confident Gain**. The difference between Knowledge Confident and Original Confident, which can be formally defined as \\(p^{ki}-p^{o}\\). * **Text Similarity**. The cosine similarity \\(s^{ti}\\) of the representation between the question about the image and a piece of knowledge. * **Caption Similarity**. Cosine similarity \\(s^{ci}\\) between the image caption representation and the knowledge representation. * **Image Similarity**. Cosine similarity \\(s^{vi}\\) between the image representation and the knowledge representation. * **Entailment**. The entailment score \\(e\\) between the question and knowledge. can be obtained by \\(ent=Ent(q,K_{i})\\), where \\(Ent(\\cdot,\\cdot)\\) is the entailment model. * **Contradiction**. The contradiction between the question and knowledge. can be obtained similarly by \\(con=Ent(q,K_{i})\\) * **Knowledge Confident Important**. Among all the knowledge of the problem, \\(K=\\{K_{1},K_{2},...,K_{n}\\}\\), the importance \\(I^{C}\\) of a certain piece of knowledge \\(K_{i}\\) is the softmax of \\(p^{ki}\\). from the perspective of knowledge confidence. * **Knowledge Visual Important**. Similarly, among all the knowledge of the problem, the importance \\(I^{v}\\) of a certain piece of knowledge is the softmax of \\(s^{vi}\\), from the perspective of the similarity between knowledge and image. * **Knowledge Caption Important**. Among all the knowledge of the problem, the importance \\(I^{kc}\\) of a certain piece of knowledge is the softmax of \\(s^{ci}\\), from the perspective of the similarity between knowledge and image caption. For each piece of knowledge during the test process, the knowledge will be classified by the knowledge perceiver, and finally, the answer with the highest vote of useful knowledge and natural knowledge will be adopted as the final answer."
    },
    {
      "title": "Experiment",
      "text": ""
    },
    {
      "title": "Experimental Settings",
      "text": "Datasets.We choose two public knowledge-based VQA datasets **OKVQA**Marino et al. (2019) and **A-OKVQA**Schwenk et al. (2022) to validate the effectiveness of our method. **OKVQA** is a large knowledge-based VQA dataset, which contains 14k question-answer pairs and 14k images from the MSCOCO dataset Lin et al. (2014). Questions in the dataset written by annotators during construction are required to contain knowledge. **A-OKVQA** is currently the largest knowledge-based VQA benchmark, which is an augmented successor of OK-VQA and contains a diverse set of 25k questions requiring a broad base of knowledge to answer. For the two datasets, we randomly sample 3k samples from their training sets as training samples for building Demo Bank and training Knowledge Perceiver. Implementation Details.We adopt frozen BLIP-2 (FlanT5XXL) Li et al. (2023) as Vision Language Model to generate the answer and generate the caption of the image. For multiple-choice output, we constrain the model to only generate option words (i.e., _abcd_). The demonstration number \\(k\\) used when generating knowledge questions is set to 3. When constructing the Demo Bank, our framework traverses the training set twice. The proposed method leverages MPNET Song et al. (2020) for representing the text to compute the similarity between questions and knowledge or captions and knowledge. The entailment and contradiction score adopts the entailment model used by Honovich et al. (2021). The similarity between the image and text is calculated using EVA-CLIPFang et al. (2022). All experiments are conducted with NVIDIA GeForce RTX 3090 GPUs."
    },
    {
      "title": "Comparisons With Sota Methods",
      "text": "As shown in Table 1, our method outperforms other methods on the A-OKVQA data set, whether in the setting of generating the answer directly or giving options to choose the final answer. Especially under the setting of multiple choice, our method can outperform the state-of-the-art (SOTA) methods by a large margin with 3.0% accuracy in the test set. In addition, from the Table 1, we notice: (1) Our method stands out among large language model-based methods, indicating that our method indeed combines [MISSING_PAGE_FAIL:6] Importance of different features.As illustrated in Figure 4 left, the importance of different features for classifying knowledge in the A-OKVQA validation set, the most important feature is the model confidence of for the original answer. In classification, we find that if the vision language model has low confidence in the original answer, it is often because it lacks the knowledge to solve the problem. Knowledge confidence important represents the importance of the model to obtain answers through different knowledge. The more important the knowledge, the higher the confidence that can often be obtained. Impact of different numbers of training samples.Our method leverages part of the training set to adjust the parameters, and the results obtained with different sample numbers in the A-OKVQA validation set are shown on the right side of Figure 4. Using only 1k samples, 76.7% accuracy can already be achieved, which illustrates the effectiveness of our framework for data utilization. Intuitively, as the amount of data used rises, the performance of our method also increases."
    },
    {
      "title": "Case Study",
      "text": "To illustrate the intermediate results of our method more intuitively, we pick some examples to illustrate. Figure 3 shows three cases of our method on the OKVQA and A-OKVQA datasets, as well as the knowledge questions generated and the knowledge obtained (partially). In Example (a) and (b), answering the question directly by the vision language model leads to a wrong answer, due to lacking of knowledge. In Example (a), directly answering the creation time of Pepsi-Cola Company will be incorrectly answered as the creation time of Coca-Cola, probably because of the lack of knowledge of the creation time of Pepsi-Cola. With the blessing of useful knowledge, the vision language model can answer correctly. In Example (a), after getting the useful knowledge of the creation time of Pepsi, our method can answer it correctly. In both Example (a) and Example (b) our method effectively utilizes useful knowledge to get the correct final answer. However, our method also has limitations. The introduction of knowledge may also introduce harmful information and turn the originally correct answer into a wrong one. For instance, in Example (c) our method generates some harmful knowledge and adopts the answer generated by the wrong knowledge as the final answer. At this time, it is necessary to effectively filter the knowledge. The previous experiments have verified the effectiveness of our method in filtering knowledge."
    },
    {
      "title": "Conclusion",
      "text": "In this paper, we propose a novel and efficient framework to combine the vision language model and the large language model for knowledge-based VQA. The proposed method uses a large language model to acquire knowledge and a vision language model to answer questions. To generate high-quality knowledge, we propose to use an updatable Demo Bank to pose knowledge questions. To reduce the impact of harmful knowledge, we propose to use Knowledge Perceiver to filter knowledge. In future work, we will explore the possibility of extending PROOFREAD to more knowledge-intensive tasks. Figure 4: **Left**: The importance of different features for knowledge perceiver in A-OKVQA validation set. The cover rate represents the number of samples that different features can cover in a gradient-boosted decision tree (GBDT), which represents the importance of this feature for knowledge classification. **Right**: The effect of using different numbers of training samples for the final result in the A-OKVQA val set. Figure 3: Three cases of PROOFREAD on OKQVA dataset and A-OKVQA dataset. For each example, the left side shows the image, the question, the predicted answer without using knowledge, and the predicted final answer using knowledge, while the right side shows the knowledge questions and knowledge generated according to the questions, where U, N, H represent the ground truth label for the knowledge being useful, neutral, or harmful respectively."
    },
    {
      "title": "References",
      "text": "* [1]J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan (2022) Flamingo: a visual language model for few-shot learning. In NeurIPS, Cited by: SS1. * [2]S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh (2015) VQA: visual question answering. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 2425-2433. Cited by: SS1. * [3]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020) Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, pp.. Cited by: SS1. * [4]T. Chen and C. Guestrin (2016) XGBoost: a scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pp. 785-794. Cited by: SS1. * [5]H. W. Chung, L. Hou, S. Zoph, B. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, S. Narang, G. Mishra, A. Yu, V. Y. Zhao, Y. Huang, A. M. Dai, H. Yu, S. Petrov, E. H. Dean, J. Devlin, D. Roberts, Q. V. Le, and J. Wei (2022) Scaling Instruction-Finetuned Language Models. CoRRabs/2210.11416. Cited by: SS1. * [6]Y. Ding, J. Yu, B. Liu, Y. Hu, M. Cui, and Q. Wu (2022) MuKEA: multimodal knowledge extraction and accumulation for knowledge-based visual question answering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 5079-5088. Cited by: SS1. * [7]Y. Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang, T. Huang, X. Wang, and Y. Cao (2022) EVA: exploring the limits of masked visual representation learning at scale. CoRRabs/2211.07636. Cited by: SS1. * [8]L. Gui, B. Wang, Q. Huang, A. Hauptmann, Y. Bisk, and J. Gao (2022) KAT: a knowledge augmented transformer for vision-and-language. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pp. 956-968. Cited by: SS1. * [9]J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, E. Simonyan, E. Elsen, J. W. Fiscus, and L. Sifre (2022) Training compute-optimal large language models. CoRRabs/2203.15556. Cited by: SS1. * [10]O. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor, and O. Abend (2021) SQ25: evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 7856-7870. Cited by: SS1. * [11]Y. Hu, H. Hua, Z. Yang, W. Shi, N. A. Smith, J. Luo, and J. Luo (2022) PromptCap: prompt-guided task-aware image captioning. CoRRabs/2211.09699. Cited by: SS1. * [12]J. Li, D. Li, S. Savarese, and S. C. H. Hoi (2023) BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. CoRRabs/2301.12597. Cited by: SS1. * [13]X. Li, K. Lv, H. Yan, T. Lin, W. Zhu, Y. Ni, G. Xie, X. Wang, and X. Qiu (2023) Unified demonstration retriever for in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 4644-4668. External Links: Link, Document Cited by: SS1. * ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, Vol. 8693 of Lecture Notes in Computer Science, Vol. 740-755, pp. 75. Cited by: SS1. * [15]J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen (2022) What makes good in-context examples for GPT-3?. Note: What makes good in-context examples for GPT-3? External Links: Link Cited by: SS1. * [16]J. Liu, D. Shen, B. Papik, A. Gupta, and M. Rohrbach (2021) KRISP: integrating implicit and symbolic knowledge for open-domain knowledge-based vqa. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 14111-14121. External Links: Link, Document Cited by: SS1. * [17]K. Marino, X. Chen, D. Parikh, A. Gupta, and M. Rohrbach (2021) KRISP: integrating implicit and symbolic knowledge for open-domain knowledge-based vqa. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 14111-14121. External Links: Link, Document Cited by: SS1. * [18]K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi (2019) OK-vqa: a visual question answering benchmark requiring external knowledge. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 1-1. Cited by: SS1. * [19]K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi (2019) OK-vqa: a visual question answering benchmark requiring external knowledge. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 1-1. Cited by: SS1. * ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, Vol. 8693 of Lecture Notes in Computer Science, Vol. 740-755, pp. 740-755. Cited by: SS1. * ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, Vol. 8693 of Lecture Notes in Computer Science, Vol. 740-755, pp. 755. Cited by: SS1. * ECCV 2021 - 13th European Conference, Zurich, Switzerland, September 6-12, 2021, Proceedings, Part V, Vol. 8693 of Lecture Notes in Computer Science, Vol. 740-755, pp. 740-755. Cited by: SS1. * ECCV 2021 - 13th European Conference, Zurich, Switzerland, September 6-12, 2021, Proceedings, Part V, Vol. 8693 of Lecture Notes in Computer Science, Vol. 740-755, pp. 755-755. Cited by: SS1. * ECCV 2021 - 13th European Conference, Zurich, Switzerland, September 6-12, 2021, Proceedings, Part V, Vol. 8693 of Lecture Notes in Computer Science, Vol. 740-755, pp. 75. Cited by: SS1. * ECCV 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 1-1. Cited by: SS1. * ECCV 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 1-1. Cited by: SS1. * ECCV 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 1-1. Cited by: SS1. * ECCV 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 1-1. Cited by: SS1. * ECCV 2021 Long Beach, CA, USA, June 16-20, 2019_, 3195-3204. Computer Vision Foundation / IEEE. * Ouyang et al. (2022) Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and Lowe, R. 2022. Training language models to follow instructions with human feedback. In _NeurIPS_. * Peng et al. (2023) Peng, Z.; Wang, W.; Dong, L.; Hao, Y.; Huang, S.; Ma, S.; and Wei, F. 2023. Kosmos-2: Grounding Multimodal Large Language Models to the World. _CoRR_, abs/2306.14824. * Rubin et al. (2022) Rubin, O.; Herzig, J.; and Berant, J. 2022. Learning To Retrieve Prompts for In-Context Learning. In Carpout, M.; de Marneffe, M.; and Ruiz, I. V. M., eds., _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, 2655-2671. Association for Computational Linguistics. * ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII_, volume 13668 of _Lecture Notes in Computer Science_, 146-162. Springer. * August 4, Volume 1: Long Papers_, 1073-1083. Association for Computational Linguistics. * Shao et al. (2023) Shao, Z.; Yu, Z.; Wang, M.; and Yu, J. 2023. Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, New Orleans, LA, USA, June 18-24, 2022._ * Song et al. (2020) Song, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T. 2020. MPNet: Masked and Permuted Pre-training for Language Understanding. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual._ * Speer et al. (2017) Speer, R.; Chin, J.; and Havasi, C. 2017. ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. In Singh, S.; and Markovitch, S., eds., _Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA_, 4444-4451. AAAI Press. * Talmor et al. (2019) Talmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Burstein, J.; Doran, C.; and Solorio, T., eds., _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, 4149-4158. Association for Computational Linguistics. * June 2, 2000, Athens, Greece_. European Language Resources Association. * Wang et al. (2017) Wang, P.; Wu, Q.; Shen, C.; Dick, A. R.; and van den Hengel, A. 2017. Explicit Knowledge-based Reasoning for Visual Question Answering. In Sierra, C., ed., _Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017_, 1290-1296. ijcai.org. * Wang et al. (2018) Wang, P.; Wu, Q.; Shen, C.; Dick, A. R.; and van den Hengel, A. 2018. FVQA: Fact-Based Visual Question Answering. _IEEE Trans. Pattern Anal. Mach. Intell._, 40(10): 2413-2427. * Wang et al. (2022) Wang, S.; Xu, Y.; Fang, Y.; Liu, Y.; Sun, S.; Xu, R.; Zhu, C.; and Zeng, M. 2022. Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data. In Muresan, S.; Nakov, P.; and Villavicencio, A., eds., _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, 3170-3179. Association for Computational Linguistics. * March 1, 2022_, 3081-3089. AAAI Press. * Yang et al. (2020) Yang, Z.; Cui, Y.; Yu, J.; Wang, M.; Tao, D.; and Tian, Q. 2020. Deep Multimodal Neural Architecture Search. In Chen, C. W.; Cucchiara, R.; Hua, X.; Qi, G.; Ricci, E.; Zhang, Z.; and Zimmermann, R., eds., _MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020_, 3743-3752. ACM. * Zhang et al. (2022) Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.; Diab, M. T.; Li, X.; Lin, X. V.; Mihaylov, T.; Ott, M.; Shleifer, S.; Shuster, K.; Simig, D.; Koura, P. S.; Sridhar, A.; Wang, T.; and Zettlemoyer, L. 2022. OPT: Open Pre-trained Transformer Language Models. _CoRR_, abs/2205.01068. Zhang, Y.; Feng, S.; and Tan, C. 2022. Active Example Selection for In-Context Learning. In Goldberg, Y.; Kozareva, Z.; and Zhang, Y., eds., _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, 9134-9148. Association for Computational Linguistics. * Zhu et al. (2023) Zhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M. 2023. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. _CoRR_, abs/2304.10592. * Zhu et al. (2020) Zhu, Z.; Yu, J.; Wang, Y.; Sun, Y.; Hu, Y.; and Wu, Q. 2020. Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual Question Answering. In Bessiere, C., ed., _Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020_, 1097-1103. ijcai.org."
    }
  ]
}