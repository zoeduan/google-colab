{
  "title": "Causality extraction from medical text using Large Language Models (LLMs)",
  "authors": [
    "Seethalakshmi Gopalakrishnan",
    "Luciana Garbayo"
  ],
  "abstract": "\n This study explores the potential of natural language models, including large language models, to extract causal relations from medical texts, specifically from Clinical Practice Guidelines (CPGs). The outcomes causality extraction from Clinical Practice Guidelines for gestational diabetes are presented, marking a first in the field. We report on a set of experiments using variants of BERT (BioBERT, DistilBERT, and BERT) and using Large Language Models (LLMs), namely GPT-4 and LLAMA2. Our experiments show that BioBERT performed better than other models, including the Large Language Models, with an average F1-score of 0.72. GPT-4 and LLAMA2 results show similar performance but less consistency. We also release the code and an annotated a corpus of causal statements within the Clinical Practice Guidelines for gestational diabetes. \n",
  "references": [
    {
      "id": null,
      "title": "Causality extraction from medical text using Large Language Models (LLMs)",
      "authors": [
        "Seethalakshmi Gopalakrishnan",
        "Luciana Garbayo"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "ACOG practice bulletin, Mellitus, Gestational Diabetes",
      "authors": [],
      "year": "2018",
      "venue": "ACOG practice bulletin, Mellitus, Gestational Diabetes",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Classification and diagnosis of diabetes: Standards of Medical Care in Diabetes-2020",
      "authors": [],
      "year": "",
      "venue": "Diabetes care",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Identifying predictive causal factors from news streams",
      "authors": [
        "Ananth Balashankar",
        "Sunandan Chakraborty",
        "Samuel Fraiberger",
        "Lakshminarayanan Subramanian"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Diabetes and pregnancy: An Endocrine society clinical practice guideline",
      "authors": [
        "Ian Blumer",
        "Eran Hadar",
        "Lois David R Hadden",
        "Jorge H Jovanoviƒç",
        "M Mestman",
        "Yariv Hassan Murad",
        "Yogev"
      ],
      "year": "2013",
      "venue": "The journal of clinical endocrinology & Metabolism",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Chris Tar, et al. 2018. Universal sentence encoder",
      "authors": [
        "Daniel Cer",
        "Yinfei Yang",
        "Sheng-Yi Kong",
        "Nan Hua",
        "Nicole Limtiaco",
        "Rhomni St John",
        "Noah Constant",
        "Mario Guajardo-Cespedes",
        "Steve Yuan"
      ],
      "year": "2018",
      "venue": "Chris Tar, et al. 2018. Universal sentence encoder",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations",
      "authors": [
        "Chunkit Chan",
        "Jiayang Cheng",
        "Weiqi Wang",
        "Yuxin Jiang",
        "Tianqing Fang",
        "Xin Liu",
        "Yangqiu Song"
      ],
      "year": "2023",
      "venue": "Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Beyond Surface: Probing LLaMA Across Scales and Layers",
      "authors": [
        "Nuo Chen",
        "Ning Wu",
        "Shining Liang",
        "Ming Gong",
        "Linjun Shou",
        "Dongmei Zhang",
        "Jia Li"
      ],
      "year": "2023",
      "venue": "Beyond Surface: Probing LLaMA Across Scales and Layers",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Lightner: A lightweight generative framework with prompt-guided attention for low-resource NER",
      "authors": [
        "Xiang Chen",
        "Ningyu Zhang",
        "Lei Li",
        "Xin Xie",
        "Shumin Deng",
        "Chuanqi Tan",
        "Fei Huang",
        "Luo Si",
        "Huajun Chen"
      ],
      "year": "2021",
      "venue": "Lightner: A lightweight generative framework with prompt-guided attention for low-resource NER",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Towards symbiosis in knowledge representation and natural language processing for structuring clinical practice guidelines",
      "authors": [
        "Weng Chunhua",
        "Mark Philip Ro Payne",
        "Stephen B Velez",
        "Suzanne Johnson",
        "Bakken"
      ],
      "year": "2014",
      "venue": "Studies in health technology and informatics",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Template-based named entity recognition using BART",
      "authors": [
        "Leyang Cui",
        "Yu Wu",
        "Jian Liu",
        "Sen Yang",
        "Yue Zhang"
      ],
      "year": "2021",
      "venue": "Template-based named entity recognition using BART",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Screening for gestational diabetes: US Preventive Services Task Force recommendation statement",
      "authors": [
        "Karina W Davidson",
        "Michael J Barry",
        "Carol M Mangione",
        "Michael Cabana",
        "Aaron B Caughey",
        "M Esa",
        "Katrina E Davis",
        "Chyke A Donahue",
        "Martha Doubeni",
        "Li Kubik",
        "Li"
      ],
      "year": "2021",
      "venue": "JAMA",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Ting Liu, and Bing Qin. 2022. e-CARE: a new dataset for exploring explainable causal reasoning",
      "authors": [
        "Li Du",
        "Xiao Ding",
        "Kai Xiong"
      ],
      "year": "2022",
      "venue": "Ting Liu, and Bing Qin. 2022. e-CARE: a new dataset for exploring explainable causal reasoning",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Automated application of clinical practice guidelines for asthma management",
      "authors": [
        "E M Alan R Ertle",
        "William R Campbell",
        "Hersh"
      ],
      "year": "1996",
      "venue": "Proceedings of the AMIA Annual Fall Symposium",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "A novel NLP-fuzzy system prototype for information extraction from medical guidelines",
      "authors": [
        "Lejla Begic Fazlic",
        "Ahmed Hallawa",
        "Anke Schmeink",
        "Arne Peine",
        "Lukas Martin",
        "Guido Dartmann"
      ],
      "year": "2019",
      "venue": "2019 42nd International Convention on Information and Communication Technology",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Statistical methods for rates and proportions",
      "authors": [
        "Bruce Joseph L Fleiss",
        "Myunghee Levin",
        "Paik Cho"
      ],
      "year": "2013",
      "venue": "Statistical methods for rates and proportions",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Clinical concept extraction: a methodology review",
      "authors": [
        "Sunyang Fu",
        "David Chen",
        "Huan He",
        "Sijia Liu",
        "Sungrim Moon",
        "Kevin J Peterson",
        "Feichen Shen",
        "Liwei Wang",
        "Yanshan Wang",
        "Andrew Wen"
      ],
      "year": "2020",
      "venue": "Journal of Biomedical Informatics",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Text to Causal Knowledge Graph: A Framework to Synthesize Knowledge from Unstructured Business Texts into Causal Graphs",
      "authors": [
        "Seethalakshmi Gopalakrishnan",
        "Wenwen Victor Zitian Chen",
        "Gus Dou",
        "Sreekar Hahn-Powell",
        "Wlodek Nedunuri",
        "Zadrozny"
      ],
      "year": "2023",
      "venue": "Information",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
      "authors": [
        "Andrew Gordon",
        "Zornitsa Kozareva",
        "Melissa Roemmele"
      ],
      "year": "2012",
      "venue": "SEM 2012: The First Joint Conference on Lexical and Computational Semantics",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Associations Between Aggregate NLP-extracted Conflicts of Interest and Adverse Events By Drug Product",
      "authors": [
        "Graham Scott",
        "Zoltan P Majdik",
        "Johua B Barbour",
        "Justin F Rousseau"
      ],
      "year": "2022",
      "venue": "Studies in health technology and informatics",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Language models represent space and time",
      "authors": [
        "Wes Gurnee",
        "Max Tegmark"
      ],
      "year": "2023",
      "venue": "Language models represent space and time",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "HeadlineCause: A Dataset of News Headlines for Detecting Causalities",
      "authors": [
        "Ilya Gusev",
        "Alexey Tikhonov"
      ],
      "year": "2021",
      "venue": "HeadlineCause: A Dataset of News Headlines for Detecting Causalities",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Knowledge Extraction and Analysis of Medical Text with Particular Emphasis on Medical Guidelines",
      "authors": [
        "Hossein Hematialam"
      ],
      "year": "2021",
      "venue": "Knowledge Extraction and Analysis of Medical Text with Particular Emphasis on Medical Guidelines",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Computing Conceptual Distances between Breast Cancer Screening Guidelines: An Implementation of a Near-Peer Epistemic Model of Medical Disagreement",
      "authors": [
        "Hossein Hematialam",
        "Luciana Garbayo",
        "Seethalakshmi Gopalakrishnan",
        "Wlodek Zadrozny"
      ],
      "year": "2020",
      "venue": "Computing Conceptual Distances between Breast Cancer Screening Guidelines: An Implementation of a Near-Peer Epistemic Model of Medical Disagreement",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "A Method for Computing Conceptual Distances between Medical Recommendations: Experiments in Modeling Medical Disagreement",
      "authors": [
        "Hossein Hematialam",
        "Luciana Garbayo",
        "Seethalakshmi Gopalakrishnan",
        "Wlodek W Zadrozny"
      ],
      "year": "2021",
      "venue": "Applied Sciences",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Identifying condition-action statements in medical guidelines using domain-independent features",
      "authors": [
        "Hossein Hematialam",
        "Wlodek Zadrozny"
      ],
      "year": "2017",
      "venue": "Identifying condition-action statements in medical guidelines using domain-independent features",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Agreement, the f-measure, and reliability in information retrieval",
      "authors": [
        "George Hripcsak",
        "Adam S Rothschild"
      ],
      "year": "2005",
      "venue": "Journal of the American medical informatics association",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Recommendation statements identification in clinical practice guidelines using heuristic patterns",
      "authors": [
        "Musarrat Hussain",
        "Jamil Hussain",
        "Muhammad Sadiq"
      ],
      "year": "2018",
      "venue": "2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Supporting the abstraction of clinical practice guidelines using information extraction",
      "authors": [
        "Katharina Kaiser",
        "Silvia Miksch"
      ],
      "year": "2010",
      "venue": "International Conference on Application of Natural Language to Information Systems",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Causal BERT: Language models for causality detection between events expressed in text",
      "authors": [
        "Roshni Vivek Khetan",
        "Mayuresh Ramnani",
        "Shubhashis Anand",
        "Andrew E Sengupta",
        "Fano"
      ],
      "year": "2020",
      "venue": "Causal BERT: Language models for causality detection between events expressed in text",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Information extraction from electronic medical documents: state of the art and future research directions",
      "authors": [
        "Mohamed Yassine Landolsi",
        "Lobna Hlaoua",
        "Lotfi Ben Romdhane"
      ],
      "year": "2023",
      "venue": "Knowledge and Information Systems",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Factors associated with perinatal morbidity and mortality in a sample of public and private maternity centers in the City of Rio de Janeiro",
      "authors": [
        "Maria Do",
        "Carmo Leal",
        "Silvana Granado Nogueira Da Gama",
        "M√¥nica Rodrigues Campos",
        "Luciana Tricai Cavalini",
        "Luciana Sarmento Garbayo",
        "Carla Lopes Porto",
        "C√©lia Brasil",
        "Szwarcwald Landmann"
      ],
      "year": "1999",
      "venue": "Cadernos de Sa√∫de P√∫blica",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
      "authors": [
        "Jinhyuk Lee",
        "Wonjin Yoon",
        "Sungdong Kim",
        "Donghyeon Kim",
        "Sunkyu Kim",
        "Chan Ho",
        "So",
        "Jaewoo Kang"
      ],
      "year": "2020",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Causality extraction based on self-attentive BiLSTM-CRF with transferred embeddings",
      "authors": [
        "Zhaoning Li",
        "Qi Li",
        "Xiaotian Zou",
        "Jiangtao Ren"
      ],
      "year": "2021",
      "venue": "Neurocomputing",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "KEPT: Knowledge Enhanced Prompt Tuning for event causality identification",
      "authors": [
        "Jintao Liu",
        "Zequn Zhang",
        "Zhi Guo",
        "Li Jin",
        "Xiaoyu Li",
        "Kaiwen Wei",
        "Xian Sun"
      ],
      "year": "2023",
      "venue": "Knowledge-Based Systems",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "DCU-Lorcan at FinCausal 2022: Span-based Causality Extraction from Financial Documents using Pre-trained Language Models",
      "authors": [
        "Chenyang Lyu",
        "Tianbo Ji",
        "Quanwei Sun",
        "Liting Zhou"
      ],
      "year": "2022",
      "venue": "Proceedings of the 4th Financial Narrative Processing Workshop@ LREC2022",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Event Causality Identification via Generation of Important Context Words",
      "authors": [
        "Minh Hieu Man",
        "Thien Nguyen",
        "Nguyen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 11th Joint Conference on Lexical and Computational Semantics",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "International association of diabetes and pregnancy study groups recommendations on the diagnosis and classification of hyperglycemia in pregnancy: response to Weinert",
      "authors": [
        "Steven G Boyd E Metzger",
        "Bengt Gabbe",
        "Lynn P Persson",
        "Alan R Lowe",
        "Jeremy Jn Dyer",
        "Thomas A Oats",
        "Buchanan"
      ],
      "year": "2010",
      "venue": "Diabetes care",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "BioCause: Annotating and analysing causality in the biomedical domain",
      "authors": [
        "Claudiu MihƒÉilƒÉ",
        "Tomoko Ohta",
        "Sampo Pyysalo",
        "Sophia Ananiadou"
      ],
      "year": "2013",
      "venue": "BMC bioinformatics",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? Levenshtein distance, spell checker",
      "authors": [
        "Frederic P Miller",
        "Agnes F Vandome",
        "John Mcbrewster"
      ],
      "year": "2009",
      "venue": "Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? Levenshtein distance, spell checker",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Screening for Gestational Diabetes",
      "authors": [
        "Justin Mills",
        "Sopan Mohnot"
      ],
      "year": "2021",
      "venue": "American Family Physician",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo on ten benchmarking datasets",
      "authors": [
        "Yifan Peng",
        "Shankai Yan",
        "Zhiyong Lu"
      ],
      "year": "2019",
      "venue": "Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo on ten benchmarking datasets",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Screening for gestational diabetes: updated evidence report and systematic review for the US preventive services task force",
      "authors": [
        "Jennifer Pillay",
        "Lois Donovan",
        "Samantha Guitard",
        "Bernadette Zakher",
        "Michelle Gates",
        "Allison Gates",
        "Ben Vandermeer",
        "Christina Bougatsos",
        "Roger Chou",
        "Lisa Hartling"
      ],
      "year": "2021",
      "venue": "Jama",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "The probabilistic basis of Jaccard's index of similarity",
      "authors": [
        "Raimundo Real",
        "Juan M Vargas"
      ],
      "year": "1996",
      "venue": "Systematic biology",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Medicause: Causal relation modelling and extraction from medical publications",
      "authors": [
        "Ioannis Reklos",
        "Albert Mero√±o-Pe√±uela"
      ],
      "year": "2022",
      "venue": "Proceedings of the 1st International Workshop on Knowledge Graph Generation From Text co-located with 19th Extended Semantic Conference (ESWC 2022)",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Exploiting cloze questions for few shot text classification and natural language inference",
      "authors": [
        "Timo Schick",
        "Hinrich Sch√ºtze"
      ],
      "year": "2020",
      "venue": "Exploiting cloze questions for few shot text classification and natural language inference",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "MD Anderson breaks with IBM Watson, raising questions about artificial intelligence in oncology",
      "authors": [
        "Charlie Schmidt"
      ],
      "year": "2017",
      "venue": "MD Anderson breaks with IBM Watson, raising questions about artificial intelligence in oncology",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Combining open-source natural language processing tools to parse clinical practice guidelines",
      "authors": [
        "Maria Taboada",
        "Maria Meizoso",
        "D Mart√≠nez",
        "David Riano",
        "Albert Alonso"
      ],
      "year": "2013",
      "venue": "Expert Systems",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Does synthetic data generation of llms help clinical text mining",
      "authors": [
        "Ruixiang Tang",
        "Xiaotian Han",
        "Xiaoqian Jiang",
        "Xia Hu"
      ],
      "year": "2023",
      "venue": "Does synthetic data generation of llms help clinical text mining",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Construction of an annotated corpus to support biomedical information extraction",
      "authors": [
        "Paul Thompson",
        "A Syed",
        "John Iqbal",
        "Sophia Mcnaught",
        "Ananiadou"
      ],
      "year": "2009",
      "venue": "BMC bioinformatics",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Identifying condition-action sentences using a heuristic-based information extraction method",
      "authors": [
        "Reinhardt Wenzina",
        "Katharina"
      ],
      "year": "2013",
      "venue": "Process Support and Knowledge Representation in Health Care",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "From text to tables: a local privacy preserving large language model for structured information retrieval from medical documents",
      "authors": [
        "Isabella Catharina Wiest",
        "Dyke Ferber",
        "Jiefu Zhu",
        "Marko Van Treeck",
        "Sonja Katharina Meyer",
        "Radhika Juglan",
        "I Zunamys",
        "Daniel Carrero",
        "Jens Paech",
        "Matthias P Kleesiek",
        "Ebert"
      ],
      "year": "2023",
      "venue": "medRxiv",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Pre-trained language models with domain knowledge for Biomedical extractive summarization",
      "authors": [
        "Qianqian Xie",
        "Jennifer",
        "Amy Bishop",
        "Prayag Tiwari",
        "Sophia Ananiadou"
      ],
      "year": "2022",
      "venue": "Pre-trained language models with domain knowledge for Biomedical extractive summarization",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Differentiable prompt makes pre-trained language models better few-shot learners",
      "authors": [
        "Ningyu Zhang",
        "Luoqiu Li",
        "Xiang Chen",
        "Shumin Deng",
        "Zhen Bi",
        "Chuanqi Tan",
        "Fei Huang",
        "Huajun Chen"
      ],
      "year": "2021",
      "venue": "Differentiable prompt makes pre-trained language models better few-shot learners",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Calibrate before use: Improving few-shot performance of language models",
      "authors": [
        "Zihao Zhao",
        "Eric Wallace",
        "Shi Feng",
        "Dan Klein",
        "Sameer Singh"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "2. Related Work",
      "text": "Causality extraction is the task of automatically extracting the cause/effect relationships from the text. In this section, we briefly discuss studies related to causality extraction."
    },
    {
      "title": "Work Related To Automatic Information Extraction From Clinical Practice Guidelines (Cpg)",
      "text": "This section summarizes the prior work related to information extraction on the Clinical Practice Guidelines. Extracting clinical findings from notes of outpatient progress was early done realized by (Srivastava et al., 2019). Fifteen years later (Srivastava et al., 2019) targeted the automated extraction of diagnosis and treatment procedures from clinical guidelines. A similar work (Srivastava et al., 2019) introduced a method for automatically collecting useful information using rules rooted in both syntactic and semantic information. A pattern-based approach was further used by (Srivastava et al., 2019), which contrasted a manually developed ontology for CPG eligibility criteria with a top-level ontology stemming from a semantic pattern-based approach. A more recent work (Srivastava et al., 2019) introduced an innovative system that blends together the methodologies of Natural Language Processing (NLP) and Fuzzy Logic. A supervised machine learning methodology was used by another similar work (Srivastava et al., 2019) to extract and categorize Conflicts Of Interest (COIs) from disclosure statements indexed in PubMed. Recently, Large Language Models (LLMs) have (also) been employed for a variety of NLP tasks, including those involving information extraction. LLMs can be fine-tuned to cater to a specific dataset, or a prompt-based approach can be utilized. An illustrative study (Srivastava et al., 2019) measures the efficiency of the few-shot learning performance of GPT-3 in tasks related to text classification and information extraction. A recent survey article (Srivastava et al., 2019) provides a summary of the methods and solutions employed for information extraction. It also highlights the challenges encountered when extracting information from medical documents (such as, ambiguities when the named entity belongs to more than one class, phrase boundary detection, name variations, and others)."
    },
    {
      "title": "Recent Work On Causality Extraction From Non-Medical Text",
      "text": "The study by (Srivastava et al., 2017) directly extracts cause and effect from text without separately extracting candidate pairs and their relations. A work on event extraction (Srivastava et al., 2017) focuses on identifying the causal relationship between pairs of event mentions, also known as 'Event Causality Identification' (ECI). Balashankar et al. (Balashankar et al., 2018) propose an event extraction (modality) that seeks to uncover the hidden relationships between events mentioned in news streams by creating a Predictive Causal Graph (PCG). Prompt tuning has been proposed to bridge the gap between pre-training and fine-tuning on many of the mainstream NLP tasks like text classification(Srivastava et al., 2017; Wang et al., 2018), information extraction(Chen et al., 2018; Chen et al., 2018) etc. In (Wang et al., 2018), Knowledge Enhanced Prompt Tuning (KEPT) employs external knowledge sourced from knowledge bases (KBs) to fine-tune pre-trained language models through the design of an attention mechanism. A recent article (Chen et al., 2018) describes the use of ChatGPT to extract cause/effect relationships from text on three datasets: (1) Choice of Plausible Alternatives (COPA) (Dai et al., 2018), which is a collection of premises, along with two questions related to each premise, that requires causal reasoning in order to solve the inference; (2) e-CARE (Chen et al., 2018) which is an explainable causal reasoning dataset with cause, effect and two possible explanations; (3) Headline Cause (Cang et al., 2018) dataset, which aims to identify the implicit causal relations between pair of text. On COPA, ChatGPT in-context learning got a 97% accuracy (performance); on the eCARE dataset, a 79.6% accuracy was obtained using prompt engineering, and 72.7% accuracy was recorded for on Headline Cause."
    },
    {
      "title": "Causality Extraction From The Medical Text",
      "text": "In 2013 the task of automatic detection conditional statements in medical guidelines was first introduced (Wang et al., 2018). The article used a rule-based approach, focusing on presence of connectives such as \"if\", and a collection of word-based syntactic patterns. Subsequent works on detecting condition action statements from CPGs, (Srivastava et al., 2017) and (Cang et al., 2018), apply supervised machine learning techniques to classify sentences according to whether they express conditions and actions. Another study (Srivastava et al., 2017) used heuristic patterns to identify recommendation statements in Clinical Practice Guidelines (CPG). A review article (Kang et al., 2018) documents the existing methods and tools for clinical concept extraction. The summarization of biomedical literature is addressed in (Wang et al., 2018) using pre-trained language models. A more recent study (Wang et al., 2018) explores the use of ChatGPT for clinical text mining, specifically for extracting structured data from unstructured healthcare texts and focusing on biological named entity recognition and relation extraction by identifying and extracting medical entities from text related to disease and drug, symptoms and treatment, etc."
    },
    {
      "title": "3. Data",
      "text": "We annotated seven documents of gestational diabetes (clinical practice) guidelines from various societies (and medical entities) like(such as) the American Diabetes Association (ADA) ((Aglund et al., 2018),(Srivastava et al., 2017)), US Preventive Services Task Force (USPSTF) ((Kang et al., 2018), (Srivastava et al., 2017)), American College of Obstetrics & Gynecology (ACOG) ((Aglund et al., 2018), American Academy of Family Physician (AAFP) (Kang et al., 2018), and Endocrine Society (Aglund et al., 2018). The decision to annotate gestational diabetes clinical practice guidelines was based on the opportunity to explore causality inference in the future with situated learning models with prediction (team member Dr. Garbayo worked on safety and quality database development on maternal and child in maternities, resulting in the creation of the largest maternity database in Latin America (Srivastava et al., 2017). Two annotators were recruited. Given a (medical) text document, their task was to read the document and mark the cause, effect, condition, action, modal, and degree of influence with tags. The cause was marked as C, effect as E, condition as CO, and action as A. The phrases containing any of these causal phrases should be differentiated; for example, the beginning of a cause phrase will be marked as <C>- and the end as </C>. For example: **Example 3.1**: <C>Pregnant persons with gestational diabetes</C> are at <E>increased risk for maternal and fetal complications</E> and may benefit from <A>early identification and treatment</A>."
    },
    {
      "title": "Inter-Annotator Agreement For The Medical Data",
      "text": "Due to the intricacy of causality extraction, which involves annotators labeling varying text spans as \"cause,\" effect,\" and so on, computing agreement between two annotators can be challenging as it requires comparing two spans of texts. Traditional methods of inter-annotator agreement, such as the Kappa statistic (Kappa, 1966), are inadequate due to their need for classifications to fit into mutually exclusive and discrete categories. Therefore, we decided to assess agreement using both exact match and relaxed match criteria. The F-measure is used for the exact match (Kappa, 1966; Kappa, 1966; Kappa, 1966) between the labels. In the case of the relaxed match, the average distance between phrases is computed. Initially, the annotated phrases, their corresponding labels, and the full sentence they are derived from are extracted from the entire annotated document. These annotations, originating from both annotators, are then compared and amalgamated based on the sentence. The resulting merged table thus features the sentence, the extracted phrase, and the labels as marked by Annotator 1 and Annotator 2. In total, 514 matching phrases have been identified. An overall agreement computed as a Jaccard similarity of 0.66 was obtained. Details of the inter-annotator agreement computation are given below. From the merged data table, the inter-annotator agreement was computed. This is done by computing the match between the annotations as follows. * Both annotator's phrases overlap with each other but are not necessarily an exact match. * Both annotator's phrases exactly match. Figure 1. Distribution of the labels in the corpus. The percentage of almost all the labels is around 24%. To execute the relaxed match, we employed the Levenshtein distance [(40)] and the Jaccard distance [(45)]. The Levenshtein distance quantifies the difference between two string sequences, indicating the minimum single-character edits required to transform one word into another. Jaccard similarity computes the degree of relatedness between two finite samples by dividing the intersection's size by the size of the sample sets' union. The Jaccard distance is subsequently calculated by subtracting the Jaccard similarity from 1. The Python library Levenshtein 1 is used in computing the Levenshtein distance. The Jaccard index was computed using the Python library textdistance2. The Levenshtein distance and the Jaccard distance between the annotators are summarized in Table 1 Footnote 1: https://pypiorg/project/python-Levenshtein/ Footnote 2: [https://pypi.org/project/textdistance/](https://pypi.org/project/textdistance/) From Table 1, we can understand that there is an average Levenshtein distance of 0.41 and an average Jaccard distance of 0.34. In most cases, both annotators annotated the same sentence with the same labels, but the length of the phrase was different. The exact match between the phrases is computed by finding the exact string match between phrases 1 and 2. Out of the 514 phrases, 112 phrases are exact matches. The match between the labels for the same phrase by both annotators is also computed with an average F1 score of 0.78. The match between the labels for each subcategory is given in Table 2"
    },
    {
      "title": "Data Preparation And Preprocessing",
      "text": "Seven documents on gestational diabetes guidelines provided by different societies are downloaded as PDF documents. The PDFs are converted into a document format, and the documents are given to the annotators for annotating them manually. The annotators used tags to annotate the documents. After annotating them, the NLTK sentence tokenizer is used to extract sentences from all the documents. The sentences from all the documents are appended together and converted into a data frame. Regular expressions are used to extract the causal sentence. If any of the sentences contain a tag <->, it will be extracted as a causal sentence. Again \\begin{table} \\begin{tabular}{|l|l|l|} \\hline & Levenshtein distance & Jaccard distance \\\\ \\hline Cause & 0.22 & 0.27 \\\\ \\hline Condition & 0.34 & 0.21 \\\\ \\hline Effect & 0.37 & 0.31 \\\\ \\hline Action & 0.87 & 0.48 \\\\ \\hline \\end{tabular} \\end{table} Table 1. Relaxed match between the annotated phrases. Levenshtein distance is the minimum number of edits required to transform one phrase to another, whereas Jaccard distance is the amount of non-overlap between phrases. The lower the distance, the agreement is higher. The distance is higher for action. In most of the cases where there is a mismatch, the length of the phrase by both the annotators was different. \\begin{table} \\begin{tabular}{|l|l|l|l|} \\hline & Precision & Recall & F1-score \\\\ \\hline Cause & 0.86 & 0.71 & 0.77 \\\\ \\hline Condition & 0.56 & 0.85 & 0.67 \\\\ \\hline Effect & 0.85 & 0.90 & 0.88 \\\\ \\hline Action & 0.89 & 0.70 & 0.78 \\\\ \\hline \\end{tabular} \\end{table} Table 2. For a given phrase, the labels annotated by annotators 1 and 2 are compared. An average F1 score of 0.78 was obtained. From the F1-score, we can understand that both the annotators agree on most of the categories except the signal for which the F1-score is low. regular expressions are used to extract the phrases of cause, effect, action, signal, and condition from the sentences. The extracted phrases are used for computing inter-annotator agreement."
    },
    {
      "title": "4. Methodology",
      "text": ""
    },
    {
      "title": "Causality Extraction Using Bert",
      "text": "Given the good performance of DistilBERT with organizational data (Dosov et al., 2017), this model was also applied to the medical data. Considering the limited sample size in medical data, we attempted to improve the learning process by increasing the number of epochs. This approach allows for more refined fine-tuning of the model. In order to decide on the correct number of epochs and to avoid overfitting, we tried running the model for 100 epochs and plotted the validation loss and the training loss. The graph showing the train and validation loss for our highest performing model, BioBERT, is given in Figure 2. From the graph, we can understand that with the increase in the number of epochs, the training loss is constantly increasing and approaching 0. The validation loss decreases till 18 epochs and then starts to increase. Based on this, we fine-tuned DistilBERT for 18 epochs, BERT(BERT-base-uncased) for 20 epochs, and BioBERT for 16 epochs. The data is split into train and test. DistilBERT for token classification is fine-tuned on the training data for 18 epochs. On the test data, the model obtained an average F1-score of 0.57. Similarly, we fine-tuned BioBERT for 16 epochs and BERT for 20 epochs. Out of these three models, BioBERT(Wang et al., 2018) gave us an average higher F1-score. BioBERT gave an average F1 score of 0.61, and BERT gave an average F1 score of 0.60. The detailed results of fine-tuning BioBERT on the test data are given in Table 3; and, for comparison, the summary of the results of using variants of BERT for causality extraction task is given in Table 4 Figure 2. Graph showing the train and validation loss when fine-tuning on BioBERT. Looking at the graph, we can understand that with the increase in the number of epochs, the training loss is constantly decreasing and approaching 0. The validation loss decreases till 16 epochs and then starts to increase. Based on this, we fine-tuned BioBERT for 16 epochs."
    },
    {
      "title": "Observations On Using Gpt-4 For Causality Extraction From Medical Guidelines",
      "text": "Generative Pre-trained Transformer 4 (GPT-4) (Wang et al., 2017) outperforms most of the state-of-the-art performing models on the traditional NLP benchmark datasets. In this section, we discuss our results of prompting GPT-4-0314 with a with context window of 8,192, for the causality extraction task. We explored various prompt sizes (zero, four, six, eight, ten-shot, and twenty-shot prompting). As an initial step, we tried the sentence with token-level labels for each word in the sentence as prompt examples. For the test data, the model is expected to predict a label for each word in the sentence. However, the model hallucinated by predicting a longer number of labels than in the given sentence; that is, a long sequences of non-existing \"non-causal\" labels. Since GPT-4 hallucinated for the token-level predictions, we tried extracting the phrases of cause/effect relationships in text and tried converting them into token level by assigning labels for each token. We started with a four-shot prompting. The annotated data with the tags will be given as an example in the prompt, and the model is expected to predict similarly. A sample is given in Example 4.1. **Example 4.1**: <C>Gestational diabetes</C> has also been associated with an <E>increased risk of several long-term health outcomes in pregnant persons and intermediate outcomes in their offspring</E> We tried converting the predictions with the tags into a token-level format in order to compute the F1 score. However, since the tags are placed in different places in some of the gold annotations and predictions, the number of tokens in gold and predictions doesn't match. An example is given below **Example 4.2**: **Gold:** Importance<C>Gestational diabetes</C> is diabetes that develops during pregnancy.1-3 Prevalence of gestational diabetes in the US has been estimated at 5.8% to 9.2%, based on traditional diagnostic criteria, although it may be higher if more inclusive criteria are used.4-8 <C>Pregnant persons with gestational diabetes</C> <E>increased risk for maternal and \\begin{table} \\begin{tabular}{|l|l|l|l|} \\hline & Precision & Recall & F1-score \\\\ \\hline DistilBERT & 0.69 & 0.68 & 0.68 \\\\ \\hline BERT & 0.72 & 0.72 & 0.71 \\\\ \\hline BioBERT & 0.72 & 0.73 & 0.72 \\\\ \\hline \\end{tabular} \\end{table} Table 4. Summary of the results of causality extraction on medical text using the Pre-trained Language Model (BERT) and its variants. The gestational diabetes data is split into train and test data. All the models are fine-tuned on train data and tested on test data. \\begin{table} \\begin{tabular}{|l|l|l|l|l|} \\hline & Precision & Recall & F1-score & Support \\\\ \\hline E & 0.82 & 0.75 & 0.78 & 696 \\\\ \\hline C & 0.62 & 0.69 & 0.65 & 411 \\\\ \\hline CO & 0.80 & 0.63 & 0.71 & 717 \\\\ \\hline A & 0.65 & 0.85 & 0.73 & 838 \\\\ \\hline Macro average & 0.72 & 0.73 & 0.72 & 2662 \\\\ \\hline \\end{tabular} \\end{table} Table 3. Causality extraction results on the medical data using BioBERT, the highest performing model. Each token in the text was assigned a label Effect(E), Cause(C), Condition(CO), and Action(A). The results are obtained by splitting the manually annotated data into train and test data. fetal complications, including preeclampsia, fetal macrosomia (which can cause shoulder dystcia and birth injury), and neonatal hypoglyemia</E>.3,9-11 <C> Gestational diabetes</C> has also been associated with an <E>increased risk of several long-term health outcomes in pregnant persons and intermediate outcomes in their offspring</E>.12-16Table 1. **Prediction:** Importance Gestational diabetes is diabetes that develops during pregnancy. 1-3 Prevalence of gestational diabetes in the US has been estimated at 5.8% to 9.2%, based on traditional diagnostic criteria, although it may be higher if more inclusive criteria are used.4-8 <C>Pregnant persons with gestational diabetes</C> <E>increased risk for maternal and fetal complications, including preeclampsia, fetal macrosomia (which can cause shoulder dystcia and birth injury), and neonatal hypoglycemia. 3,9-11</E> <C> Gestational diabetes</C> has also been associated with an <E>increased risk of several long-term health outcomes in pregnant persons and intermediate outcomes in their offspring.12-16Table 1.</E> In Example 4.2, the phrases marked indicate the scenario where some extra spaces can be added, leading to the indifference in the number of tokens between gold and the predictions. In the gold data, neonatal hypoglycemia</E>.3,9-11 have a space after the tag, but in the prediction, the tag is predicted after the number, which leads to no space between </E> and.3,9-11. In some scenarios, the GPT-4 omits some of the words if they do not contain a causal relation (omits the 'O' labels in some places). This mismatch between the gold and the predictions impedes the token-level comparison and reporting of the F1 score. An example is given below: **Example 4.3**.: **Gold:**Race/Ethnicity/Hemoglobinopathies<C>Hemoglobin variants </C> can <E>interfere with the measurement of A1C</E>, although most assays in use in the U.S. are unaffected by the most common variants. **Prediction:** <C>Race/Ethnicity/Hemoglobinopathies variants</C> can interfere with the measurement of A1C, although most assays in use in the U.S. are unaffected by the most common variants. In Example 4.3, in the prediction, the keyword \"Hemoglobin\" is missing, which is present in the gold data. In some places, such inconsistencies lead to token mismatch between the gold and predicted data. To compare the performance of GPT-4 with other models, the predictions are converted into the token level and manually checked to convert both the gold predictions to the same number of tokens for the four-shot prompting. In the predictions, some tokens are missed; those tokens are added to the predictions and marked as label \"O\"(as O indicates tokens that are not cause, effect, condition, action, or signal). After converting the data into a token level, we computed the F1 score. With GPT-4, we got an average F1 score of 0.39 with four-shot prompting."
    },
    {
      "title": "5. Results & Experiments",
      "text": "As the predictions of GPT-4 can be unreliable, and missing tokens in a sentence leads to a token mismatch between the gold data and the predicted data, therefore Jaccard distance is proposed as an alternative solution to the traditional F1 score as the evaluation criteria. The Jaccard similarity was computed using the textdistance3 Python library. Another alternative measure to try is the cosine similarity. The cosine similarity is obtained by computing the vectors of both the gold and the predictions using the Universal Sentence Encoder[(5)]. The computed values are used to compute the pairwise cosine similarity between two vectors using Scikit-learn 4. The cause, effect, signal, condition, and action are extracted from the predictions using regular expressions on the tags. The extracted prediction phrases and the gold annotated phrases are merged. We perform two types of matching on the gold and predicted phrases. Footnote 4: [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) * _Jaccard similarity:_ To measure the dissimilarity between the gold data and the predictions. * _Cosine similarity:_ To measure the semantic similarity between the gold data and the predictions. The results of phrase level similarity between the gold annotated data and predictions of GPT-4 using various prompt sizes are summarized in Table 5. From the results of the various prompt sizes for the causality extraction on medical data, we can understand that the result of the ten-shot prompting gives a higher similarity and F1 score. The Jaccard similarity gives the similarity score based on the overlap between the gold and the predictions. The cosine similarity gives the semantic similarity between the gold and predicted phrases. There is not much difference in the cosine similarity with various prompt sizes, indicating that it may not be the right measure for this task. The F1-scores are computed by comparing the gold labels with the predicted labels (Jaccard and cosine similarity for the predicted phrases, F1-score for the labels). The detailed F1-score for the ten-shot prompting label match between gold and predictions is given in Table 6. In particular, we can see that the F1 score for cause, effect, and action is higher compared to the other labels. (This result is comparable with a recent work [6], indicating a strong performance of ChatGPT for extracting cause/effect relationships). \\begin{table} \\begin{tabular}{l l l l} & Precision & Recall & F1 score \\\\ \\hline Action & 0.56 & 0.90 & 0.69 \\\\ \\hline Cause & 0.60 & 0.74 & 0.66 \\\\ \\hline Condition & 0.95 & 0.17 & 0.29 \\\\ \\hline Effect & 0.74 & 0.79 & 0.76 \\\\ \\hline Macro average & 0.71 & 0.65 & 0.60 \\\\ \\end{tabular} \\end{table} Table 6. Summary of the results of the GPT-4 predictions of ten-shot prompting on our medical data. Here the F1-score is computed by comparing the gold labels and the predicted labels. The F1 score for cause, effect, and action is higher compared to the condition. Many of the conditions are predicted as causes. \\begin{table} \\begin{tabular}{l l l l} & Jaccard similarity & Cosine similarity & F1 (labels) \\\\ \\hline Zero-shot & 0.42 & 0.22 & 0.27 \\\\ \\hline Four-shot & 0.44 & 0.22 & 0.35 \\\\ \\hline Six-shot & 0.57 & 0.22 & 0.52 \\\\ \\hline Eight-shot & 0.52 & 0.23 & 0.55 \\\\ \\hline Ten-shot & 0.57 & 0.20 & 0.60 \\\\ \\hline Twenty-shot & 0.46 & 0.20 & 0.28 \\\\ \\end{tabular} \\end{table} Table 5. The phrase level comparison results of few-shot prompting using GPT-4. We tried various prompt sizes (zero, four, six, eight, ten, and twenty-shot prompting). From the results, we can understand that the Jaccard similarity at ten-shot prompting is higher (higher the similarity, higher overlap between the gold and predicted spans), cosine similarity is lower (lower the similarity, higher the gold and press are related), and the F1-score between the labels is higher, after which the similarity and F1 decreases at twenty-shot. The cosine similarity, which gives the semantic similarity between gold and predictions, remains the same with all the prompt sizes. Here the F1-score is computed by comparing the gold labels and the predicted labels."
    },
    {
      "title": "Llama2 For Causality Extraction From Medical Guidelines",
      "text": "LLAMA2(LAMA2, 2018) is a pre-trained and fine-tuned Large Language Model. Three variants of LLAMA2 are available, which differ in the parameters. 7B, 13B, and 70B parameters are publicly available. LLAMA2 is trained on two trillion tokens of data. In our experiments, the LLAMA2 7B parameter is fine-tuned on the medical data. It is fine-tuned using the HuggingFace autotrain. To fine-tune LLAMA2, the first step is to prepare the data. At first, when the model was fine-tuned and tested on the token level as BERT, LLAMA2 was predicting a long number of \"O-other\" as GPT-4. So we dealt with this as a phrase-level extraction problem. The data is prepared with three parts which are instruction, input, and output. A sample training data is given in example 5.1 Example 5.1 ***Instruction: Extract the cause, condition, effect, signal, and action from the given sentence. ***Input: Pregnant persons with gestational diabetes are at increased risk for maternal and fetal complications, including preeclampsia, fetal macrosomia (which can cause shoulder dystocia and birth injury), and neonatal hypoglycemia. ***Output: ['Pregnant persons-signal', 'with gestational diabetes -cause', 'increased risk for maternal and fetal complications, including preeclampsia, fetal macrosomia (which can cause shoulder dystocia and birth injury), and neonatal hypoglycemia-effect'] The test data should be similar to the training data except for the output, which should be empty. The gestational diabetes annotated data was split into train and test data. The HuggingFace autotrain 5 for the LLM fine-tuning was used to fine-tune the model. The fine-tuned weights are pushed into the HuggingFace dataset for inference. This experiment was done using Google Colab Pro+ with a High-RAM A100 GPU. Similar to the GPT-4, the predictions of LLAMA-\\(2\\) were also at phrase level. So a similar evaluation strategy is followed for LLAMA2. We present the results with three types of distance. Footnote 5: [https://huggingface.co/docs/autotrain/llm_finetuning](https://huggingface.co/docs/autotrain/llm_finetuning) The predictions are split into phrase levels and then compared with gold data. The Jaccard similarity was computed using the textdistance6 Python library. The cosine similarity is obtained by computing the vectors of both the gold and the predictions using the Universal sentence encoder(Cheng et al., 2018). The computed values are used to compute the pairwise cosine similarity between two vectors using Scikit-learn 7. Footnote 6: [https://pypi.org/project/textdistance/](https://pypi.org/project/textdistance/) Footnote 7: [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) Initially, we split the data into train and test using the Scikit learn train_test_split(). We have converted the phrase-level predictions into token-level. In the test data, there were a total of 59 samples. Out of the 59 samples, only 29 samples, LLAMA2 predicted the labels, so the evaluation is only for those sentences. With LLAMA2, we got an average F1-score of 0.36, which is lower than that of all the other models. Since the test data size is very small, we have also tried a four-fold cross-validation on this data. The results of fine-tuning LLAMA2 using four-fold cross-validation with 3,5, and 10 epochs are given in Table 7. With the increase in the number of epochs, both the Jaccard similarity and F1-score increase. Also, the predictions of LLAMA2 missed labels in many of the predictions. It extracted the phrases with no label. With three epochs, LLAMA2 missed 38% of the labels; with five epochs, 21% of the labels; and with ten epochs, it missed 26% of the labels. We omitted the predictions with no labels (108 predictions, 60 predictions, 76 predictions). The results of causality extraction presented in Table 7 are after omitting the predictions with no labels. From the results, we can understand that Jaccard similarity, cosine similarity, and F1 score increase with the increase in the number of epochs. However, the number of missed labels started increasing after 5 epochs."
    },
    {
      "title": "6. Discussion",
      "text": "Above we presented results on causality extraction from medical guidelines using recently introduced large language models such as LLAMA2 and GPT-4, and compared them with the performance of BERT, an older, and smaller LLM. The annotated data and the code are all publicly available on GitHub: [https://github.com/gseetha04/LLMs-Medicaldata.git](https://github.com/gseetha04/LLMs-Medicaldata.git). We observed that GPT-4 expresses strong performance for the cause-effect relationships with medical data, and generally has a good understanding of medical text without fine-tuning. However, in contrast with GPT-3.5 GPT-4 cannot deal with token classification, which limits the traditional way of finding cause and effect phrases, as discussed e.g. in our previous work (Han et al., 2017). Even though LLAMA2 seems to perform well for causality extraction, the predictions of the LLAMA2 do not predict labels for many cases, which limits its practical application. This perhaps was caused by fine-tuning LLAMA2 on our small dataset. Therefore, increasing the size of the dataset before the fine-tuning may improve the performance. However, large annotated datasets for CPGs are not available, and further experiments would require annotating more data. Since we focused on the accuracy of actual predictions, we omitted 38% of labels with three epochs, 21% of labels with five epochs, and 26% of labels with ten epochs. Given its relatively high performance and ease of use, BERT-based models continue to be a state-of-the-art for causality extraction tasks, even in the age of LLM."
    },
    {
      "title": "7. Conclusion",
      "text": "We developed an automated technique for extracting causalities from annotated corpora of medical guidelines. Additionally, we exhibited the practicality of employing new Large Language Models for causality extraction tasks. With BioBERT, we got an average F1-score of 0.72, whereas with LLAMA2, an average Jaccard distance of 0.40 was obtained. We demonstrated the potential for extracting causalities from medical guidelines using a small annotated corpus. The next logical step could involve expanding the corpus through the annotation of more data and creating a benchmark dataset for causality extraction from medical guidelines. The potential of this research opens up novel dimensions for the health domain, as causality extraction from medical guidelines can enhance clinical decision-making and patient care. This work explored both machine learning and natural language processing techniques for causality extraction. Despite the abundance of causal sentences within these guidelines, automatic extraction is an unexplored field of research. Also, machine learning models often fail in clinical applications (Zhu et al., 2017) due to the gap between data (both training and testing). In order to avoid this gap, more realistic tests need to be done so that they can be employed for real-world data. \\begin{table} \\begin{tabular}{l l l l} & Jaccard similarity & Cosine similarity & F1-score \\\\ \\hline LLAMA2 (3epochs) & 0.73 & 0.19 & 0.70 \\\\ \\hline LLAMA2 (5epochs) & 0.888 & 0.20 & 0.75 \\\\ \\hline LLAMA2 (10epochs) & 0.90 & 0.21 & 0.76 \\\\ \\end{tabular} \\end{table} Table 7. The phrase level comparison results of LLAMA2 using 4 fold cross validation. Jaccard similarity and cosine similarity indicate the average similarity between the gold and the predictions. The F1 score is the comparison between the gold labels and predicted labels."
    },
    {
      "title": "Credit Authorship Contribution Statement",
      "text": "S.G. performed the majority of the experiments and writing. She also supervised the annotation process. L.G. chose the clinical guidelines data for annotations and participated in discussions and writing. W.Z. designed some of the experiments, provided feedback, and contributed to writing."
    },
    {
      "title": "Acknowledgments",
      "text": "This research was partly funded by the National Science Foundation (NSF) grant number 2141124. We would like to thank Nikhil Vundela for his contributions to data annotation. We would like to thank Dr. Wenwen Dou and Dr. Victor Zitian Chen for their feedback on the causality extraction tasks."
    },
    {
      "title": "References",
      "text": "* (1) * A. Balashankar, S. Chakraborty, S. Fraiberger, and L. Subramanian (2019)Identifying predictive causal factors from news streams. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2338-2348. Cited by: SS1. * I. Blumer, E. Hader, D. R. Hadden, L. Jovanovic, J. H. Mestman, M. Murad, and Y. Yogev (2013)Diabetes and pregnancy: an Endocrine society clinical practice guideline. The journal of clinical endociology & Metabolism98 (1), pp. 4227-4249. Cited by: SS1. * D. Cer, Y. Yang, S. Kong, N. Hua, N. Limtiaco, R. S. John, N. Constant, M. Guajardo-Cespedes, S. Yuan, C. Tar, et al. (2018)Universal sentence encoder. arXiv preprint arXiv:1803.11175. Cited by: SS1. * C. Chan, J. Cheng, W. Wang, Y. Jiang, T. Fang, X. Liu, and Y. Song (2023)Chatgpt evaluation on sentence level relations: a focus on temporal, causal, and discourse relations. arXiv preprint arXiv:2304.14827. Cited by: SS1. * N. Chen, N. Wu, S. Liang, M. Gong, L. Shou, D. Zhang, and J. Li (2023)Beyond surface: probing lLaMA Across Scales and Layers. arXiv preprint arXiv:2312.04338. Cited by: SS1. * X. Chen, N. Zhang, L. Li, X. Xie, S. Deng, C. Tan, F. Huang, L. Si, and H. Chen (2021)Lightnter: a lightweight generative framework with prompt-guided attention for low-resource NER. arXiv preprint arXiv:2109.00720. Cited by: SS1. * W. Chunhua, P. R. Payne, M. Velez, S. B. Johnson, and S. Bakken (2014)Towards symbiosis in knowledge representation and natural language processing for structuring clinical practice guidelines. Studies in health technology and informatics201, pp. 461. Cited by: SS1. * L. Cui, Y. Wu, J. Liu, S. Yang, and Y. Zhang (2021)Template-based named entity recognition using bart. arXiv preprint arXiv:2106.01760. Cited by: SS1. * K. W. Davidson, M. J. Barry, C. M. Mangione, M. Cabana, A. B. Caughey, E. M. Davis, K. E. Donahueh, C. A. Doubeni, M. Kubik, L. Li, et al. (2021)Screening for gestational diabetes: us preventive services task force recommendation statement. JAMA326, pp. 531-538. Cited by: SS1. * J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018)Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Cited by: SS1. * L. Du, X. Ding, K. Xiong, T. Liu, and B. Qin (2022)e-care: a new dataset for exploring explainable causal reasoning. arXiv preprint arXiv:2205.05849. Cited by: SS1. * A. R. Ertle, E. Campbell, and W. R. Hersh (1996)Automated application of clinical practice guidelines for asthma management.. In Proceedings of the AMIA Annual Fall Symposium, American Medical Informatics Association, pp. 552. Cited by: SS1. * L. Begic Fazlic, A. Halllawa, A. Schmeink, A. Peine, L. Martin, and G. Dartmann (2019)A novel nlp-fuzzy system prototype for information extraction from medical guidelines. In 2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MPRO), pp. 1025-1030. Cited by: SS1. * J. L. Fleiss, B. Levin, and M. Cho Paik (2013)Statistical methods for rates and proportions. John wiley & sons. Cited by: SS1. * S. Fu, D. Chen, H. He, S. Liu, S. Moon, K. J. Peterson, F. Shen, L. Wang, Y. Wang, A. Wen, et al. (2020)Clinical concept extraction: a methodology review. Journal of Biomedical Informatics109, pp. 103526. Cited by: SS1. * S. Gopalakrishnan, V. Z. Chen, W. Dou, G. Hahn-Powell, S. Nedunuri, and W. Zadromy (2023)Text to causal knowledge graph: a framework to synthesize knowledge from unstructured business texts into causal graphs. Information14 (7), pp. 367. Cited by: SS1. * A. Gordon, Z. Kozareva, and M. Roemmele (2012)SemEval-2012 task 7: choice of plausible alternatives: an evaluation of commonsense causal reasoning. In SEM 2012: The First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pp. 394-398. * Graham et al. (2022) S Scott Graham, Zoltan P Majdik, Johua B Barbour, and Justin F Rousseau. 2022. Associations Between Aggregate NLP-extracted Conflicts of Interest and Adverse Events By Drug Product. _Studies in health technology and informatics_ 290 (2022), 405. * Gurnee and Tegmark (2023) Wes Gurnee and Max Tegmark. 2023. Language models represent space and time. _arXiv preprint arXiv:2310.02207_ (2023). * Gusev and Tikhonov (2021) Ilya Gusev and Alexey Tikhonov. 2021. HeadlineCause: A Dataset of News Headlines for Detecting Causalities. _arXiv preprint arXiv:2108.12626_ (2021). * Hematialm (2021) Hossein Hematialm. 2021. _Knowledge Extraction and Analysis of Medical Text with Particular Emphasis on Medical Guidelines_. Ph. D. Dissertation. The University of North Carolina at Charlotte. * Hematialm et al. (2020) Hossein Hematialm, Luciana Garbayo, Seethalakshmi Gopalakrishnan, and Wlodek Zadrozny. 2020. Computing Conceptual Distances between Breast Cancer Screening Guidelines: An Implementation of a Near-Peer Epistemic Model of Medical Disagreement. _arXiv preprint arXiv:2007.00709_ (2020). * Hematialm et al. (2021) Hossein Hematialm, Luciana Garbayo, Seethalakshmi Gopalakrishnan, and Wlodek W Zadrozny. 2021. A Method for Computing Conceptual Distances between Medical Recommendations: Experiments in Modeling Medical Disagreement. _Applied Sciences_ 11, 5 (2021), 2045. * Hematialm and Zadrozny (2017) Hossein Hematialm and Wlodek Zadrozny. 2017. Identifying condition-action statements in medical guidelines using domain-independent features. _arXiv preprint arXiv:1706.04206_ (2017). * Hripcsak and Rothschild (2005) George Hripcsak and Adam S Rothschild. 2005. Agreement, the f-measure, and reliability in information retrieval. _Journal of the American medical informatics association_ 12, 3 (2005), 296-298. * Hussain et al. (2018) Musarrat Hussain, Jamil Hussain, Muhammad Sadiq, Anees Ul Hassan, and Sungyoung Lee. 2018. Recommendation statements identification in clinical practice guidelines using heuristic patterns. In _2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)_. IEEE, 152-156. * Kaiser and Miksch (2010) Katharina Kaiser and Silvia Miksch. 2010. Supporting the abstraction of clinical practice guidelines using information extraction. In _International Conference on Application of Natural Language to Information Systems_. Springer, 304-311. * Khetan et al. (2020) Vivek Khetan, Roshni Rammani, Mayuresh Anand, Shubhashis Sengupta, and Andrew E Fano. 2020. Causal BERT: Language models for causality detection between events expressed in text. _arXiv preprint arXiv:2012.05453_ (2020). * Landolsi et al. (2023) Mohamed Yassine Landolsi, Lohna Hlaoua, and Lotfi Ben Romdhane. 2023. Information extraction from electronic medical documents: state of the art and future research directions. _Knowledge and Information Systems_ 65, 2 (2023), 463-516. * Leal et al. (2004) Maria do Carmo Leal, Silvana Granado Nogueira da Gama, Monica Rodrigues Campos, Luciana Tricai Cavalini, Luciana Sarmento Garbayo, Carla Lopes Porto Brasil, and Celia Landmann Szwarcwald. 2004. Factors associated with perinatal morbidity and mortality in a sample of public and private maternity centers in the City of Rio de Janeiro, 1999-2001. _Cadernos de Saude Publica_ 20 (2004), S20-S33. * Lee et al. (2020) Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. _Bioinformatics_ 36, 4 (2020), 1234-1240. * Li et al. (2021) Zhaoning Li, Qi Li, Xiaotian Zou, and Jiangtao Ren. 2021. Causality extraction based on self-attentive BiLSTM-CRF with transferred embeddings. _Neurocomputing_ 423 (2021), 207-219. * Liu et al. (2023) Jintao Liu, Zequn Zhang, Zhi Guo, Li Jin, Xiaoyu Li, Kaiwen Wei, and Xian Sun. 2023. KEPT: Knowledge Enhanced Prompt Tuning for event causality identification. _Knowledge-Based Systems_ 259 (2023), 110064. * Lyu et al. (2022) Chenyang Lyu, Tianbo Ji, Quanwei Sun, and Liting Zhou. 2022. DCU-Lorcan at FinCausal 2022: Span-based Causality Extraction from Financial Documents using Pre-trained Language Models. In _Proceedings of the 4th Financial Narrative Processing Workshop@ LREC2022_. 116-120. * Man et al. (2022) Hieu Man, Minh Nguyen, and Thien Nguyen. 2022. Event Causality Identification via Generation of Important Context Words. In _Proceedings of the 11th Joint Conference on Lexical and Computational Semantics_. 323-330. * Metzger et al. (2010) Boyd E Metzger, Steven G Gabbe, Bengt Persson, Lynn P Lowe, Alan R Dyer, Jeremy JN Oats, and Thomas A Buchanan. 2010. International association of diabetes and pregnancy study groups recommendations on the diagnosis and classification of hyperglycemia in pregnancy: response to Weinert. _Diabetes care_ 33, 7 (2010), e98-e98. * Mihaila et al. (2013) Claudiu Mihaila, Tomoko Ohta, Sampo Pyysalo, and Sophia Ananiadou. 2013. BioCause: Annotating and analysing causality in the biomedical domain. _BMC bioinformatics_ 14 (2013), 1-18. * Miller et al. (2009) Frederic P Miller, Agnes F Vandome, and John McBrewster. 2009. Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? Levenshtein distance, spell checker, hamming distance. * Mills and Mohhot (2021) Justin Mills and Sopan Mohhot. 2021. Screening for Gestational Diabetes. _American Family Physician_ 104, 6 (2021), 641-642. * OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] * Peng et al. (2019) Yifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo on ten benchmarking datasets. _arXiv preprint arXiv:1906.05474_ (2019). * Pillay et al. (2021) Jennifer Pillay, Lois Donovan, Samantha Guitard, Bernadette Zakhter, Michelle Gates, Allison Gates, Ben Vandermeer, Christina Bougatsoos, Roger Chou, and Lisa Hartling. 2021. Screening for gestational diabetes: updated evidence report and systematic review for the US preventive services task force. _Jama_ 326, 6 (2021), 539-562. * Real and Vargas (1996) Raimundo Real and Juan M Vargas. 1996. The probabilistic basis of Jaccard's index of similarity. _Systematic biology_ 45, 3 (1996), 380-385. * Reklos and Merono-Fquieta (2022) Ioannis Reklos and Albert Merono-Fquieta. 2022. Medicause: Causal relation modelling and extraction from medical publications. In _Proceedings of the 1st International Workshop on Knowledge Graph Generation From Text co-located with 19th Extended Semantic Conference (ESWC 2022), Hersonisos, Greece_, Vol. 3184. 1-18. * Schick and Schutze [2020] Timo Schick and Hinrich Schutze. 2020. Exploiting clone questions for few shot text classification and natural language inference. _arXiv preprint arXiv:2001.07676_ (2020). * Schmidt [2017] Charlie Schmidt. 2017. MD Anderson breaks with IBM Watson, raising questions about artificial intelligence in oncology. * Taboada et al. [2013] Maria Taboada, Maria Meizoso, D Martinez, David Riano, and Albert Alonso. 2013. Combining open-source natural language processing tools to parse clinical practice guidelines. _Expert Systems_ 30, 1 (2013), 3-11. * Tang et al. [2023] Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. 2023. Does synthetic data generation of llms help clinical text mining? _arXiv preprint arXiv:2303.04360_ (2023). * Thompson et al. [2009] Paul Thompson, Syed A Iqbal, John McNaught, and Sophia Ananiadou. 2009. Construction of an annotated corpus to support biomedical information extraction. _BMC bioinformatics_ 10 (2009), 1-19. * Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_ (2023). * Wenzima and Kaiser [2013] Reinhardt Wenzima and Katharina Kaiser. 2013. Identifying condition-action sentences using a heuristic-based information extraction method. In _Process Support and Knowledge Representation in Health Care_. Springer, 26-38. * Wiest et al. [2023] Isabella Catharina Wiest, Dyke Ferber, Jiefi Zhu, Marko Van Treeck, Sonja Katharina Meyer, Radhika Juglan, Zunamys I Carrero, Daniel Paech, Jens Kleesiek, Matthias P Ebert, et al. 2023. From text to tables: a local privacy preserving large language model for structured information retrieval from medical documents. _medRxiv_ (2023), 2023-21. * Xie et al. [2022] Qianqian Xie, Jennifer Amy Bishop, Prayag Tiwari, and Sophia Ananiadou. 2022. Pre-trained language models with domain knowledge for Biomedical extractive summarization. _Knowledge-Based Systems_ (2022), 109460. * Zhang et al. [2021] Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. 2021. Differentiable prompt makes pre-trained language models better few-shot learners. _arXiv preprint arXiv:2108.13161_ (2021). * Zhao et al. [2021] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In _International Conference on Machine Learning_. PMLR, 12697-12706."
    }
  ]
}