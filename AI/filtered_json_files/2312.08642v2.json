{
  "title": "METACOGNITION-ENHANCED FEW-SHOT PROMPTING WITH POSITIVE REINFORCEMENT",
  "authors": [
    "Yu Ji",
    "Wen Wu",
    "Yi Hu",
    "Hong Zheng",
    "Liang He"
  ],
  "abstract": "\n Few-shot prompting elicits the remarkable abilities of large language models by equipping them with a few demonstration examples in the input. However, the traditional method of providing large language models with all demonstration input-output pairs at once may not effectively guide large language models to learn the specific input-output mapping relationship. In this paper, inspired by the regulatory and supportive role of metacognition in students' learning, we propose a novel metacognition-enhanced few-shot prompting, which guides large language models to reflect on their thought processes to comprehensively learn the given demonstration examples. Furthermore, considering that positive reinforcement can improve students' learning motivation, we introduce positive reinforcement into our metacognition-enhanced few-shot prompting to promote the few-shot learning of large language models by providing response-based positive feedback. The experimental results on two real-world datasets show that our metacognition-enhanced few-shot prompting with positive reinforcement surpasses traditional few-shot prompting in classification accuracy and macro F1. \n",
  "references": [
    {
      "id": null,
      "title": "METACOGNITION-ENHANCED FEW-SHOT PROMPTING WITH POSITIVE REINFORCEMENT",
      "authors": [
        "Yu Ji",
        "Wen Wu",
        "Yi Hu",
        "Hong Zheng",
        "Liang He"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "",
      "authors": [
        "References"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Is chatgpt a good sentiment analyzer? a preliminary study",
      "authors": [
        "Zengzhi Wang",
        "Qiming Xie",
        "Zixiang Ding",
        "Yi Feng",
        "Rui Xia"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good sentiment analyzer? a preliminary study",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Is chatgpt a good personality recognizer? a preliminary study",
      "authors": [
        "Yu Ji",
        "Wen Wu",
        "Hong Zheng",
        "Yi Hu",
        "Xi Chen",
        "Liang He"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good personality recognizer? a preliminary study",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Rethinking the role of demonstrations: What makes in-context learning work?",
      "authors": [
        "Sewon Min",
        "Xinxi Lyu",
        "Ari Holtzman",
        "Mikel Artetxe",
        "Mike Lewis",
        "Hannaneh Hajishirzi",
        "Luke Zettlemoyer"
      ],
      "year": "2022",
      "venue": "EMNLP",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "NIPS",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Against spoonfeeding. for learning. reflections on students' claims to knowledge",
      "authors": [
        "E Gordon",
        "M Ann Dehler",
        "Welsh"
      ],
      "year": "2014",
      "venue": "JME",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Tell model where to attend: Improving interpretability of aspect-based sentiment classification via small explanation annotations",
      "authors": [
        "Zhenxiao Cheng",
        "Jie Zhou",
        "Wen Wu",
        "Qin Chen",
        "Liang He"
      ],
      "year": "2023",
      "venue": "ICASSP",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Fostering metacognition to support student learning and performance",
      "authors": [
        "Julie Dangremond Stanton",
        "Amanda J Sebesta",
        "John Dunlosky"
      ],
      "year": "2021",
      "venue": "CBE-LSE",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Using metacognitive prompts to enhance self-regulated learning and learning outcomes: A metaanalysis of experimental studies in computer-based learning environments",
      "authors": [
        "Lin Guo"
      ],
      "year": "2022",
      "venue": "JCAL",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Simas eric learning model (selm): Enhance student metacognitive skill based on the academic level",
      "authors": [
        "Ericka Darmawan",
        "Siti Zubaidah"
      ],
      "year": "2020",
      "venue": "IJI",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Building children's learning motivation through positive reinforcement in science and math classroom",
      "authors": [
        "Sumiati",
        "S Septiani",
        "Widodo",
        "Caturiasari"
      ],
      "year": "2019",
      "venue": "JPCS",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Large language models are human-level prompt engineers",
      "authors": [
        "Yongchao Zhou",
        "Andrei Ioan Muresanu",
        "Ziwen Han",
        "Keiran Paster",
        "Silviu Pitis",
        "Harris Chan",
        "Jimmy Ba"
      ],
      "year": "2022",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
      "authors": [
        "Yao Lu",
        "Max Bartolo",
        "Alastair Moore",
        "Sebastian Riedel",
        "Pontus Stenetorp"
      ],
      "year": "2022",
      "venue": "ACL",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "What makes good in-context examples for gpt-3?",
      "authors": [
        "Jiachang Liu",
        "Dinghan Shen",
        "Yizhe Zhang",
        "William B Dolan",
        "Lawrence Carin",
        "Weizhu Chen"
      ],
      "year": "2022",
      "venue": "What makes good in-context examples for gpt-3?",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Fairness-guided fewshot prompting for large language models",
      "authors": [
        "Huan Ma",
        "Changqing Zhang",
        "Yatao Bian",
        "Lemao Liu",
        "Zhirui Zhang",
        "Peilin Zhao",
        "Shu Zhang",
        "Huazhu Fu",
        "Qinghua Hu",
        "Bingzhe Wu"
      ],
      "year": "2023",
      "venue": "Fairness-guided fewshot prompting for large language models",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "An overview: Metacognition in education",
      "authors": [
        "Mohsen Mahdavi"
      ],
      "year": "2014",
      "venue": "IJMCER",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Enhancing the metacognition of nursing students using eye tracking glasses",
      "authors": [
        "Quentin Meteier",
        "Elena Mugellini",
        "Leonardo Angelini",
        "Alain",
        "Adrian Verdon",
        "Catherine Senn-Dubey",
        "Jean-Michel Vasse"
      ],
      "year": "2023",
      "venue": "ETRA",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Orgbox: Supporting cognitive and metacognitive activities during exploratory search",
      "authors": [
        "R Austin",
        "Robert Ward",
        "Capra"
      ],
      "year": "2021",
      "venue": "SIGIR",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Using positive reinforcement with young children",
      "authors": [
        "K Jessica",
        "Ragan H Hardy",
        "Mcleod"
      ],
      "year": "2020",
      "venue": "Beyond Behavior",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Understanding positive reinforcement and replacement behaviors within the classroom",
      "authors": [
        "K Reesha M Adamson",
        "Paige Kilpatrick",
        "Paris Smith",
        "Depaepe"
      ],
      "year": "2015",
      "venue": "Understanding positive reinforcement and replacement behaviors within the classroom",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Teachers' classroom instruction reinforcement strategies in english language class",
      "authors": [
        "Wuli Sri",
        "Devi Fitriati",
        "Agung Fatmala",
        "Anjaniputra Ginanjar"
      ],
      "year": "2020",
      "venue": "EduLearn",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Metrics for multi-class classification: An overview",
      "authors": [
        "Margherita Grandini",
        "Enrico Spa",
        "Giorgio Bagli",
        "Visani"
      ],
      "year": "2020",
      "venue": "stat",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Modeling aspect correlation for aspect-based sentiment analysis via recurrent inverse learning guidance",
      "authors": [
        "Longfeng Li",
        "Haifeng Sun",
        "Qi Qi",
        "Jingyu Wang",
        "Jing Wang",
        "Jianxin Liao"
      ],
      "year": "2022",
      "venue": "COLING",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Towards unifying the label space for aspect-and sentence-based sentiment analysis",
      "authors": [
        "Yiming Zhang",
        "Min Zhang",
        "Sai Wu",
        "Junbo Zhao"
      ],
      "year": "2022",
      "venue": "ACL",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Metacognition-Enhanced Few-Shot Prompting With Positive Reinforcement",
      "text": ""
    },
    {
      "title": "Abstract",
      "text": "Few-shot prompting elicits the remarkable abilities of large language models by equipping them with a few demonstration examples in the input. However, the traditional method of providing large language models with all demonstration input-output pairs at once may not effectively guide large language models to learn the specific input-output mapping relationship. In this paper, inspired by the regulatory and supportive role of metacognition in students' learning, we propose a novel metacognition-enhanced few-shot prompting, which guides large language models to reflect on their thought processes to comprehensively learn the given demonstration examples. Furthermore, considering that positive reinforcement can improve students' learning motivation, we introduce positive reinforcement into our metacognition-enhanced few-shot prompting to promote the few-shot learning of large language models by providing response-based positive feedback. The experimental results on two real-world datasets show that our metacognition-enhanced few-shot prompting with positive reinforcement surpasses traditional few-shot prompting in classification accuracy and macro F1. Yu Ji\\({}^{1,2}\\), Wen Wu\\({}^{2,3,*}\\), Yi Hu\\({}^{3}\\), Hong Zheng\\({}^{4,\\dagger}\\), Liang He\\({}^{1,2}\\)\\({}^{1}\\)Institute of AI for Education, East China Normal University, Shanghai, China \\({}^{2}\\)School of Computer Science and Technology, East China Normal University, Shanghai, China \\({}^{3}\\)Shanghai Key Laboratory of Mental Health and Psychological Crisis Intervention, School of Psychology and Cognitive Science, East China Normal University, Shanghai, China \\({}^{4}\\)Shanghai Changing Mental Health Center, Shanghai, China Few-Shot Prompting, Metacognition, Positive Reinforcement, Large Language Models"
    },
    {
      "title": "1 Introduction",
      "text": "Recently, Large Language Models (LLMs) (e.g., ChatGPT, GPT-4, LLaMA 2, and Claude 2) have exhibited impressive capabilities in various downstream tasks (e.g., sentiment analysis [1] and personality prediction [2]) with the assistance of different prompting strategies (e.g, few-shot prompting [3]). As one of the commonly employed prompting strategies, few-shot prompting provides a few demonstration examples of desired input-output pairs in the input of LLMs, which enables LLMs to quickly learn the specific input-output mapping relationship corresponding to the downstream task [4]. However, this passive learning of the given demonstration examples (similar to spoon-feeding in education [5]) makes LLMs lack the autonomous reflection of their thought processes, which may limit their cognitive development and consequently affect their performance in downstream tasks. Take Apsect-Based Sentiment Classification (ABSC) task [6] as an example, we select two similar samples from 14-Restaurant dataset, with one serving as a demonstration example and the other as a test sample. As shown in Fig. 1, even if few-shot prompting provides the demonstration input-output pair for ChatGPT to learn, ChatGPT still makes wrong prediction for the test sample which is similar to the given demonstration example. This phenomenon indicates that only providing demonstration input-output pairs directly to LLMs may not necessarily help LLMs learn the specific mapping relationship behind the given demonstration examples. In this paper, we are motivated to propose a novel MetaCognition-enhanced Few-Shot (MCeFS) prompting to improve the performance of traditional few-shot prompting, where the idea is inspired by the regulatory and supportive role of learners' metacognition in their learning [7]. Concretely, metacognition refers to thinking about thinking, which is the individual's ability to plan, monitor, evaluate, and reflect on her/his own learning [8]. Recently, some educational psychologists have attempted to improve students' learning performance in classroom by enhancing their metacognition [9]. Similarly, we design MCeFS prompting Figure 1: Example of the weakness of few-shot prompting. to enhance the metacognition of LLMs to better accomplish downstream tasks. Concretely, MCeFS prompting requires LLMs to analyze the given demonstration examples one by one and reflect on their thought processes about the analysis of demonstration examples, thus enabling LLMs to better understand the specific mapping relationship behind the given demonstration examples. Furthermore, considering that teachers normally use positive reinforcement (i.e., providing rewards to increase the frequency of good learning behaviors) to enhance students' learning motivation [10], we introduce positive reinforcement into our MCeFS prompting. To be specific, we offer appropriate positive feedback to LLMs based on their analysis results of the given demonstration examples, which could guide LLMs to develop their thinking towards accurately completing specific downstream task. The main contributions of our work are as follows: (1) We have proposed a novel MCeFS prompting to better elicit the abilities of LLMs with a few demonstration examples. Compared with traditional few-shot prompting, our MCeFS prompting could guide LLMs to learn the given demonstration examples more comprehensively. (2) We have introduced positive reinforcement into the few-shot learning of LLMs. By providing positive feedback corresponding to the responses of LLMs, LLMs are promoted to accomplish downstream tasks more precisely. (3) We have conducted experiments on two real-world datasets to verify the performance of our MCeFS prompting with positive reinforcement. The experimental results illustrate that our MCeFS prompting with positive reinforcement outperforms traditional few-shot prompting in terms of classification accuracy and macro F1."
    },
    {
      "title": "2 Related Work",
      "text": "**Few-shot Prompting.** With the rapid development of LLMs, more and more researchers attempted to develop and optimize different prompting strategies to effectively employ LLMs in various downstream tasks (e.g., sentiment analysis [1] and personality prediction [2]) [11]. Among the various prompting strategies, few-shot prompting is one of the simple and effective prompt strategies, which allows LLMs to capture the specific mapping relationship corresponding to downstream task by providing a few demonstration examples in the input of LLMs [3, 4]. Therefore, some researchers attempted to optimize few-shot prompting for better eliciting the abilities of LLMs [12, 13, 14]. For example, Lu et al. [12] adopted the generative nature of language models to construct demonstration examples and utilized entropy-based statistics to sort the demonstration examples. Liu et al. [13] proposed to retrieve demonstration examples that are semantically-similar to a test sample to construct its corresponding prompting. Ma et al. [14] introduced a metric to evaluate the predictive bias of a fixed prompting against labels and designed greedy search based strategy to select prompting. However, most of them normally provide all demonstration input-output pairs for LLMs to learn. In fact, in this passive learning approach, LLMs may only learn the specific output format rather than the deeper input-output mapping relationship. **Metacognition.** Metacognition refers to the ability to think about and reflect on one's own cognitive process [8]. Previous studies have shown that metacognition plays a critical role in successful learning [15]. Therefore, an increasing number of researchers tried to enhance individuals' metacognition to assist them in learning more effectively. For example, Meteier et al. [16] improved the learning of nursing students by enhancing their metacognition with eye-tracking glasses. Ward et al. [17] designed a search-assistance tool to encourage users to engage in metacognitive activities during their search processes (e.g., reflect on the information they found). However, most researchers focus on improving individuals' metacognition while overlooking the impact of metacognition on the learning of LLMs. **Positive Reinforcement.** Positive reinforcement involves the usage of desirable or pleasant stimuli after the performance of certain behaviors to increase the likelihood that the behaviors will occur [18]. The field of education is one of the common fields where positive reinforcement is widely employed. Concretely, teachers often use positive reinforcement (e.g., praise or other verbal reinforcement, tangible rewards, and token rewards [19, 20]) to help boost students' learning motivation, thus guiding them to learn more effectively [10]. Hence, we would like to enhance the metacognition of LLMs to guide them to better learn the given demonstration examples. Besides, we are interested in introducing positive reinforcement into the few-shot learning of LLMs to promote them to better complete downstream tasks."
    },
    {
      "title": "3 Methodology",
      "text": "In this section, we will introduce the details of our proposed MCeFS prompting with positive reinforcement. Fig. 2 shows the learning process of our MCeFS prompting with positive reinforcement. To be specific, for each demonstration example, we no longer provide the corresponding input-output pair in the input of LLMs. Instead, we ask LLMs to complete the specific downstream task according to the given demonstration example, while the corresponding ground truth is not provided. We then assess the prediction result of LLMs. If the prediction result of LLMs is consistent with the corresponding ground truth, we will praise them (i.e., one type of positive reinforcement) and ask them to reflect on their thought processes. Otherwise, we will require them to reflect on their thought processes as well and urge them to avoid making comparable errors again. Note that, for the employment of positive reinforcement, we request LLMs to provide several common praises (e.g., you're really good) and use them to simulate the learning motivation of LLMs. Finally, we select the best one according to the results."
    },
    {
      "title": "4 Experiments",
      "text": "In this section, we conduct experiments on two real-world datasets to validate the effect of MCeFS prompting and positive reinforcement1. Specifically, we aim to answer the following Research Questions (RQs): Footnote 1: Code is available at [https://github.com/jiyu0201/MCeFSL](https://github.com/jiyu0201/MCeFSL) **RQ1**: Whether our MCeFS prompting could guide LLMs to better learn the given demonstration examples than traditional few-shot prompting? (see Section 4.3) **RQ2**: Whether positive reinforcement is effective for promoting the few-shot learning of LLMs? (see Section 4.4)"
    },
    {
      "title": "Datasets",
      "text": "For a fair comparison, we choose the same downstream task (i.e., ABSC task) as Wang et al. [1]. Similarly, we conduct experiments on SemEval-2014 Datasets2 (i.e., 14-Laptop and 14-Restaurant). The statistics of the two datasets are shown in Table 1. Besides, we use _Accuracy_ and _Macro F1_ as evaluation metrics [21]. Footnote 2: [https://alt.qcri.org/semeval2014/task4/](https://alt.qcri.org/semeval2014/task4/)"
    },
    {
      "title": "Implementation Details",
      "text": "For the usage of LLMs, we adopt the representative version of ChatGPT (i.e., gpt-3.5-turbo). In addition, we set the temperature to 0 to produce more deterministic and focused responses. To minimize the variance resulting from the sampling of demonstration examples, we adopt three random seeds (i.e., 13, 42, 550) for sampling to conduct experiments and report the average performance. Considering the limitations of ChatGPT on the input length, the shot number is selected from [1, 3, 9]. Moreover, we utilize the zer-shot prompting3 designed by Wang et al. [1] to request LLMs to complete ABSC task. Footnote 3: Zer-shot prompting: _Sentence: {sentence} What is the sentiment polarity of the aspect {aspect} in this sentence?_"
    },
    {
      "title": "Effectiveness Of Mcefs Prompting (Rq1)",
      "text": "The related experimental results are shown in Table 2, where the performance of the State-Of-The-Art (SOTA) models on 14-Laptop and 14-Restaurant datasets are quoted from [22] and [23]. In addition, Few-Shot (k) and MCeFS (k) represent ChatGPT with traditional few-shot prompting and our MCeFS prompting respectively, while \\(k\\) stands for the shot number. It can be seen from Table 2 that compared with few-shot prompting, our MCeFS prompting better elicits the sentiment analysis ability of ChatGPT and largely reduces the performance gap between ChatGPT and SOTA. Concretely, under the same shot number, our MCeFS prompting has better performance than traditional few-shot prompting w.r.t. classification accuracy and macro F1 on the two datasets. For example, relative to Few-Shot (3), MCeFS (3) increases its macro F1 from 0.671 to 0.775 on 14-Restaurant dataset. Furthermore, even if our MCeFS prompting uses fewer demonstration examples than traditional few-shot prompting, the performance of our MCeFS prompting still surpasses that of traditional few-shot prompting. For instance, the classification accuracy of MCeFS (3) has been improved from 74.4% to 80.3% on 14-Laptop when compared to Few-Shot (9). As for the reason, traditional few-shot prompting allows LLMs to learn the given demonstration input-output pairs by the way of passively receiving information, which may limit the autonomous reflection and thinking development of LLMs. In contrast, by guiding LLMs to reflect on their own thought processes regarding the completion of the downstream task, \\begin{table} \\begin{tabular}{l c c} \\hline Dataset & \\#Train & \\#Test \\\\ \\hline 14-Laptop & 2,282 & 632 \\\\ 14-Restaurant & 3,608 & 1,119 \\\\ \\hline \\end{tabular} \\end{table} Table 1: Statistics of 14-Laptop and 14-Restaurant datasets \\begin{table} \\begin{tabular}{l c c c c} \\hline \\multirow{2}{*}{Model} & \\multicolumn{2}{c}{14-Laptop} & \\multicolumn{2}{c}{14-Restaurant} \\\\ \\cline{2-5} & Accuracy & Macro F1 & Accuracy & Macro F1 \\\\ \\hline \\multicolumn{5}{l}{_Fully-supervised models_} \\\\ \\hline SOTA & **83.7\\%** & **0.801** & **89.5\\%** & **0.849** \\\\ \\hline \\multicolumn{5}{l}{_ChatGPT with few-shot prompting_} \\\\ \\hline Few-Shot (1) & 74.5\\% & 0.622 & 82.3\\% & 0.640 \\\\ Few-Shot (3) & 75.5\\% & 0.641 & 83.2\\% & 0.671 \\\\ Few-Shot (9) & 74.4\\% & 0.613 & 83.2\\% & 0.667 \\\\ \\hline \\multicolumn{5}{l}{_ChatGPT with our MCeFS prompting_} \\\\ \\hline MCeFS (1) & 79.5\\% & 0.746 & 84.7\\% & 0.743 \\\\ MCeFS (3) & 80.3\\% & 0.745 & 84.2\\% & 0.775 \\\\ MCeFS (9) & 79.0\\% & 0.722 & 86.0\\% & 0.773 \\\\ \\hline \\end{tabular} \\end{table} Table 2: The experiment results about the effectiveness of MCeFS prompting. The boldface indicates the best model results of the dataset, and the underline indicates the second best model result of the dataset Figure 2: The learning process of our MCeFS prompting with positive reinforcement. The blue and orange parts are the core of MCeFS prompting and positive reinforcement respectively. our MCeFS prompting enhances the metacognition of LLMs, which enables LLMs to understand the mapping relationship corresponding to the downstream task more precisely. Hence, LLMs could possess a more targeted ability to solve the downstream task with a few demonstration examples."
    },
    {
      "title": "Effectiveness Of Positive Reinforcement (Rq2)",
      "text": "The related experimental results are shown in Fig. 3, where MCeFS+PR denotes MCeFS prompting with Positive Reinforcement. Concretely, we observe that under the same shot number, the performance of our MCeFS prompting has been further enhanced after incorporating positive reinforcement in most cases. For instance, compared with MCeFS (1), MCeFS+PR (1) increases its macro F1 from 0.746 to 0.751 on 14-Laptop dataset. The possible reason is that positive reinforcement offers response-based positive feedback (i.e., praise) to simulate the learning motivation of LLMs, enabling them to learn in the direction of accurately analyzing given demonstration examples, thereby gaining a deeper understanding of the specific mapping relationship behind the given demonstration examples and ultimately completing the downstream task precisely. This discovery offers new insights into guiding LLMs learning from a psychological perspective."
    },
    {
      "title": "Case Study",
      "text": "To illustrate the effectiveness of our MCeFS prompting, we take the samples used in Fig. 1 to display the learning process of ChatGPT with our MCeFS prompting. It can be seen from Fig. 4, ChatGPT does not analyze the given demonstration example correctly. Subsequently, with the guidance of our MCeFS prompting, ChatGPT reflects on its thought process and recognizes its mistake. Finally, when encountering the test sample that is similar to the demonstration example, ChatGPT successfully makes correct prediction for the test sample. This case study shows that our MCeFS prompting could enhance the performance of LLMs on downstream tasks by guiding LLMs to autonomous reflection. In addition, the reflective process presented by LLMs also benefits researchers in uncovering cognitive errors in LLMs."
    },
    {
      "title": "5 Conclusions And Future Work",
      "text": "To address the problem of insufficient learning from demonstration examples in traditional few-shot prompting, we propose a novel metacognition-enhanced few-shot prompting to guide LLMs in better learning the given demonstration examples. Furthermore, we introduce positive reinforcement into our MCeFS prompting to promote LLMs learning towards accurate completion of downstream tasks. The experimental results on two real-world datasets show that our MCeFS prompting with positive reinforcement could better elicit the abilities of LLMs than traditional few-shot prompting. As for future work, we are interested in designing a Human-In-The-Loop (HTIL) learning framework and introducing expert feedback corresponding to the reflective process of LLMs to further develop their thinking."
    },
    {
      "title": "Acknowledgment",
      "text": "This work is funded by National Natural Science Foundation of China (under project No. 62377013), Science and Technology Commission of Shanghai Municipality, China (under project No. 21511100302), Natural Science Foundation of Shanghai (under project No. 22ZR1419000), the Research Project of Changing District Science and Technology Committee (under project No. CNKW2022Y37), and the Medical Master's and Doctoral Innovation Talent Base Project of Changing District (under project No.RCJD2022S07). Figure 4: Example of our MCeFS prompting. Figure 3: The experiment results about the effectiveness of positive reinforcement."
    },
    {
      "title": "References",
      "text": "* [1]Z. Chang, J. Zhou, W. Wu, Q. Chen, and L. He (2023) Tell model where to attend: improving interpretability of aspect-based sentiment classification via small explanation annotations. In ICASSP, pp. 1-5. Cited by: SS1. * [2]J. Liu, D. Shen, Y. Zhang, W. B. Dolan, L. Carin, and W. Chen (2022) What makes good in-context examples for gpt-3?. In Dee-LIO, pp. 100-114. Cited by: SS1. * [3]J. Liu, D. Shen, Y. Zhang, W. B. Dolan, L. Carin, and W. Chen (2022) What makes good in-context examples for gpt-3?. In Dee-LIO, pp. 100-114. Cited by: SS1. * [4]J. K. Hardy and R. H. McLeod (2020) Using positive reinforcement with young children. Beyond Behavior29 (2), pp. 95-107. Cited by: SS1. * [5]J. K. Hardy and R. H. McLeod (2020) Using positive reinforcement with young children. Beyond Behavior29 (2), pp. 95-107. Cited by: SS1. * [6]J. L. G. Stanton, A. J. Sebesta, and J. D. Dunlosky (2021) Fostering metacognition to support student learning and performance. CBE-LSE20 (2), pp. fe3. Cited by: SS1. [MISSING_PAGE_POST]"
    }
  ]
}