{
  "title": "Automatic deductive coding in discourse analysis: an application of large language models in learning analytics",
  "authors": [
    "Lishan Zhang",
    "Han Wu",
    "Xiaoshan Huang",
    "Tengfei Duan",
    "Hanxiang Du"
  ],
  "abstract": "\n Deductive coding is a common discourse analysis method widely used by learning science and learning analytics researchers for understanding teaching and learning interactions. It often requires researchers to manually label all discourses to be analyzed according to a theoretically guided coding scheme, which is timeconsuming and labor-intensive. The emergence of large language models such as GPT has opened a new avenue for automatic deductive coding to overcome the limitations of traditional deductive coding. To evaluate the usefulness of large language models in automatic deductive coding, we employed three different classification methods driven by different artificial intelligence technologies, including the traditional text classification method with text feature engineering, BERT-like pretrained language model and GPT-like pretrained large language model (LLM). We applied these methods to two different datasets and explored the potential of GPT and prompt engineering in automatic deductive coding. By analyzing and comparing the accuracy and Kappa values of these three classification methods, we found that GPT with prompt engineering outperformed the other two methods on both datasets with limited number of training samples. By providing detailed prompt structures, the reported work demonstrated how large language models can be used in the implementation of automatic deductive coding. \n",
  "references": [
    {
      "id": null,
      "title": "Automatic deductive coding in discourse analysis: an application of large language models in learning analytics",
      "authors": [
        "Lishan Zhang",
        "Han Wu",
        "Xiaoshan Huang",
        "Tengfei Duan",
        "Hanxiang Du"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Language models are few-shot learners",
      "authors": [
        "T B Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D M Ziegler",
        "J Wu",
        "C Winter",
        ". . Amodei",
        "D"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Emotional tones of voice affect the acoustics and perception of Mandarin tones",
      "authors": [
        "H S Chang",
        "C Y Lee",
        "X Wang",
        "S T Young",
        "C H Li",
        "W C Chu"
      ],
      "year": "2023",
      "venue": "Plos One",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "LLM-assisted content analysis: Using large language models to support deductive coding",
      "authors": [
        "R Chew",
        "J Bollenbacher",
        "M Wenger",
        "J Speer",
        "A Kim"
      ],
      "year": "2023",
      "venue": "LLM-assisted content analysis: Using large language models to support deductive coding",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "The ICAP Framework: Linking Cognitive Engagement to Active Learning Outcomes",
      "authors": [
        "M T H Chi",
        "R Wylie"
      ],
      "year": "2014",
      "venue": "Educational Psychologist",
      "doi": "10.1080/00461520.2014.965823"
    },
    {
      "id": "b4",
      "title": "Text classification based behavioural analysis of whatsapp chats",
      "authors": [
        "S Dahiya",
        "A Mohta",
        "A Jain"
      ],
      "year": "2020",
      "venue": "Paper presented at the 5th International Conference on Communication and Electronics Systems (ICCES)",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "An investigation of influentials and the role of sentiment in political communication on Twitter during election periods. Paper presented at Social Media and Election Campaigns",
      "authors": [
        "L Dang-Xuan",
        "S Stieglitz",
        "J Wladarsch",
        "C Neuberger"
      ],
      "year": "2017",
      "venue": "An investigation of influentials and the role of sentiment in political communication on Twitter during election periods. Paper presented at Social Media and Election Campaigns",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Automatic Prompt Selection for Large Language Models",
      "authors": [
        "V T Do",
        "V K Hoang",
        "D H Nguyen",
        "S Sabahi",
        "J Yang",
        "H Hotta",
        "M T Nguyen",
        "H Le"
      ],
      "year": "2024",
      "venue": "Automatic Prompt Selection for Large Language Models",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "International Conference on Educational Data Mining",
      "authors": [
        "J A Erickson",
        "A Botelho"
      ],
      "year": "2021",
      "venue": "International Conference on Educational Data Mining",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "The automated grading of student open responses in mathematics",
      "authors": [
        "J A Erickson",
        "A F Botelho",
        "S Mcateer",
        "A Varatharaj",
        "N T Heffernan"
      ],
      "year": "2020",
      "venue": "Proceedings of the Tenth International Conference on Learning Analytics & Knowledge",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Retrieval-augmented generation for large language models: A survey",
      "authors": [
        "Y Gao",
        "Y Xiong",
        "X Gao",
        "K Jia",
        "J Pan",
        "Y Bi",
        "Y Dai",
        "J Sun",
        "M Wang",
        "H Wang"
      ],
      "year": "2023",
      "venue": "Retrieval-augmented generation for large language models: A survey",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Discourse analysis. Paper presented at the Routledge Handbook of Research Methods in the Study of Religion",
      "authors": [
        "T Hjelm"
      ],
      "year": "2021",
      "venue": "Discourse analysis. Paper presented at the Routledge Handbook of Research Methods in the Study of Religion",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Prompt-based and Fine-tuned GPT Models for Context-Dependent and-Independent Deductive Coding in Social Annotation",
      "authors": [
        "C Hou",
        "G Zhu",
        "J Zheng",
        "L Zhang",
        "X Huang",
        "T Zhong",
        ". . Ker",
        "C L"
      ],
      "year": "2024",
      "venue": "Proceedings of the 14th Learning Analytics and Knowledge Conference",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Exploring teachers' emotional experience in a TPACK development task",
      "authors": [
        "X Huang",
        "L Huang",
        "S P Lajoie"
      ],
      "year": "2022",
      "venue": "Educational Technology Research and Development",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Spontaneous attention to word content versus emotional tone: Differences among three cultures",
      "authors": [
        "K Ishii",
        "J A Reyes",
        "S Kitayama"
      ],
      "year": "2003",
      "venue": "Psychological Science",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Weakly supervised framework for aspect-based sentiment analysis on students' reviews of MOOCs",
      "authors": [
        "Z Kastrati",
        "A S Imran",
        "A Kurti"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Identifying the mood of a software development team by analyzing text-based communication in chats with machine learning",
      "authors": [
        "J Klünder",
        "J Horstmann",
        "O Karras"
      ],
      "year": "2020",
      "venue": "Human-Centered Software Engineering: 8th IFIP WG 13.2 International Working Conference, HCSE 2020",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Assessing experiential learning styles: A methodological reconstruction and validation of the Kolb Learning Style Inventory",
      "authors": [
        "C Manolis",
        "D J Burns",
        "R Assudani",
        "R Chinta"
      ],
      "year": "2013",
      "venue": "Learning and Individual Differences",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Using instructional discourse analysis to study the scaffolding of student self-regulation. Paper presented at Using Qualitative Methods to Enrich Understandings of Self-regulated Learning",
      "authors": [
        "D K Meyer"
      ],
      "year": "2023",
      "venue": "Using instructional discourse analysis to study the scaffolding of student self-regulation. Paper presented at Using Qualitative Methods to Enrich Understandings of Self-regulated Learning",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Socio-emotional conflict in collaborative learning-A processoriented case study in a higher education context",
      "authors": [
        "P Näykki",
        "S Järvelä",
        "P A Kirschner",
        "H Järvenoja"
      ],
      "year": "2014",
      "venue": "International Journal of Educational Research",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "The control-value theory of achievement emotions: Assumptions, corollaries, and implications for educational research and practice",
      "authors": [
        "R Pekrun"
      ],
      "year": "2006",
      "venue": "Educational Psychology Review",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "An overview of bag of words; importance, implementation, applications, and challenges",
      "authors": [
        "W A Qader",
        "M M Ameen",
        "B I Ahmed"
      ],
      "year": "2019",
      "venue": "International Engineering Conference (IEC)",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "A Fine-Tuned BERT-Based Transfer Learning Approach for Text Classification",
      "authors": [
        "R Qasim",
        "W H Bangyal",
        "M A Alqarni",
        "A Ali Almazroi"
      ],
      "year": "2022",
      "venue": "Journal of Healthcare Engineering",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "The impact of emotional intelligence on work team cohesiveness and performance",
      "authors": [
        "B A Rapisarda"
      ],
      "year": "2002",
      "venue": "The International Journal of Organizational Analysis",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Analyzing collaborative learning processes automatically: Exploiting the advances of computational linguistics in computer-supported collaborative learning",
      "authors": [
        "C Rosé",
        "Y C Wang",
        "Y Cui",
        "J Arguello",
        "K Stegmann",
        "A Weinberger",
        "F Fischer"
      ],
      "year": "2008",
      "venue": "International Journal of Computer-supported Collaborative Learning",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "A social spin on language analysis",
      "authors": [
        "C P Rosé"
      ],
      "year": "2017",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Technology support for discussion based learning: From computer supported collaborative learning to the future of massive open online courses",
      "authors": [
        "C P Rosé",
        "O Ferschke"
      ],
      "year": "2016",
      "venue": "International Journal of Artificial Intelligence in Education",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "The random forest algorithm for statistical learning",
      "authors": [
        "M Schonlau",
        "R Y Zou"
      ],
      "year": "2020",
      "venue": "The Stata Journal",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Large scale legal text classification using transformer models",
      "authors": [
        "Z Shaheen",
        "G Wohlgenannt",
        "E Filtz"
      ],
      "year": "2020",
      "venue": "Large scale legal text classification using transformer models",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Use of large language models to aid analysis of textual data",
      "authors": [
        "R H Tai",
        "L R Bentley",
        "X Xia",
        "J M Sitt",
        "S C Fankhauser",
        "A M Chicas-Mosier",
        "B M Monteith"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "B Nikolay",
        "B Soumya",
        "B Prajjwal",
        "B Shruti",
        "D Bikel",
        "L Blecher",
        "C C Ferrer",
        "M Chen",
        "G Cucurull",
        "D Esiobu",
        "J Fernandes",
        "J Fu",
        "W Fu",
        ". . Scialom",
        "T"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Learning machine learning with very young children: Who is teaching whom?",
      "authors": [
        "H Vartiainen",
        "M Tedre",
        "T Valtonen"
      ],
      "year": "2020",
      "venue": "International Journal of Child-computer Interaction",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Investigating How Student's Cognitive Behavior in MOOC Discussion Forums Affect Learning Gains",
      "authors": [
        "X Wang",
        "D Yang",
        "M Wen",
        "K Koedinger",
        "C P Rosé"
      ],
      "year": "2015",
      "venue": "Investigating How Student's Cognitive Behavior in MOOC Discussion Forums Affect Learning Gains",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "V Q Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "authors": [
        "J White",
        "Q Fu",
        "S Hays",
        "M Sandborn",
        "C Olea",
        "H Gilbert",
        "A Elnashar",
        "J Spencher-Smith",
        "D C Schmidt"
      ],
      "year": "2023",
      "venue": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Classification of open-ended responses to a research-based assessment using natural language processing",
      "authors": [
        "J Wilson",
        "B Pollard",
        "J M Aiken",
        "M D Caballero",
        "H J Lewandowski"
      ],
      "year": "2022",
      "venue": "Physical Review Physics Education Research",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding",
      "authors": [
        "Z Xiao",
        "X Yuan",
        "Q V Liao",
        "R Abdelghani",
        "P Y Oudeyer"
      ],
      "year": "2023",
      "venue": "Companion proceedings of the 28th international conference on intelligent user interfaces",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "An automatic short-answer grading model for semi-open-ended questions",
      "authors": [
        "L Zhang",
        "Y Huang",
        "X Yang",
        "S Yu",
        "F Zhuang"
      ],
      "year": "2022",
      "venue": "Interactive Learning Environments",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Abstract",
      "text": "Deductive coding is a common discourse analysis method widely used by learning science and learning analytics researchers for understanding teaching and learning interactions. It often requires researchers to manually label all discourses to be analyzed according to a theoretically guided coding scheme, which is time-consuming and labor-intensive. The emergence of large language models such as GPT has opened a new avenue for automatic deductive coding to overcome the limitations of traditional deductive coding. To evaluate the usefulness of large language models in automatic deductive coding, we employed three different classification methods driven by different artificial intelligence technologies, including the traditional text classification method with text feature engineering, BERT-like pretrained language model and GPT-like pretrained large language model (LLM). We applied these methods to two different datasets and explored the potential of GPT and prompt engineering in automatic deductive coding. By analyzing and comparing the accuracy and Kappa values of these three classification methods, we found that GPT with prompt engineering outperformed the other two methods on both datasets with limited number of training samples. By providing detailed prompt structures, the reported work demonstrated how large language models can be used in the implementation of automatic deductive coding. Keywords:large language model, discourse analysis, deductive coding, AI in education, human-AI collaboration"
    },
    {
      "title": "1 Introduction",
      "text": "Discourse analysis investigates the functional use of language to perform actions and construct identities, focusing on the meaning conveyed rather than the structural aspects or surface features of the language (Hjelm, 2021). Discourse analysis can be accomplished based on text, spoken language and recordings. In learning sciences, it is used to understand teaching and learning interactions in class or after-class tutoring (Rose, 2017), which provides valuable insights for scaffolding and facilitating learning process(Wang, Yang, Wen, Koedinger, & Rose, 2015; Meyer, 2023). One common method for discourse analysis is deductive coding, which can transform qualitative analysis into quantitative analysis. In deductive coding, researchers first code each segment of discourses according to a predefined coding schema and then apply any statistical methods to analyze the coded results. This made discourse analysis easy to follow and more reproducible. However, the conduction of deductive coding is quite time-consuming because researchers need to human label all discourses to be analyzed according to theory-informed coding schema. To overcome the disadvantage of deductive coding, researchers started using natural language processing technologies to automate the coding process, thereby saving precious research time (Rose et al., 2008). This shift towards automated deductive coding not only addresses the time constraints but also enhances efficiency in data analysis. We consider such methods of using natural language processing to code discourse as automatic deductive coding. This analysis technique facilitated mining the sentiments embedded in the discussion discourses. Researchers are able track the learning sentiments embedded in huge discussion data with the help of natural language processing technology. Additionally, beyond the immediate benefit of saving research time, developing and refining such technology serves as a foundational step towards the creation of dialog-based intelligent tutoring agents (Rose & Ferschke, 2016). From a technical perspective, automatic deductive coding shares similarities with automatic grading student's answers to open-response questions, which have been studied extensively (Zhang, Huang, Yang, Yu, & Zhuang, 2022). They both aim to classify students' plain text into several predefined classes. This is a classical text classification task in natural language processing. The classes can be either coding labels for deductive coding tasks or grades for auto-grading tasks. Automatic grading has been studied for decades. With the advent of advanced natural language processing methods such as LSTM and BERT, auto-grading accuracy has significantly improved. In contrast, while learning analytics researchers use text classification techniques in emotion and sentiment analysis, these techniques do not seem to have a widespread impact on traditional discourse analysis. Researchers still often need to manually code all text instead of referring to automatic coding or text classification techniques. This is probably because such text coding tasks often cannot provide enough training data. The recent achievement in large language models, particularly GPT made by OpenAI, has opened up a new way for automatic deductive coding. GPT demonstrates remarkable performance on general classification tasks with few examples, known as few-shot learning, or even without any samples, which are called as zero-shot learning. Some recent studies have begun to explore the use of LLM like GPT for tasks related to qualitative discourse analysis, specifically focusing on deductive coding. However, while these studies have illustrated LLM's potential, limitations remain, especially concerning the LLM's advantages comparing to other machine learning methods and the different design strategies of prompts. Unlike previous research that relied primarily on expert-developed codebooks, we explore the integration of fine-tuning techniques and retrieval-augmented generation to enhance the model's performance in deductive coding. By introducing these advanced methods, we aim to improve the adaptability of LLM in qualitative research, reducing reliance on rigid codebooks and providing more flexibility in coding diverse data sets. Moreover, our comparison across traditional text classification, BERT-like models, and GPT-based LLM offers new insights into the relative performance of these models in different qualitative data environments, highlighting areas where LLM can outperform of complement existing methods. By conducting the exploration and classification methods comparison, the study aims for answering the following two research questions: (1) How do the three classification methods perform with the given two data sets? (2) How much improvements can we make by integrating the LLMs related techniques such as fine tuning and RAG? The rest of the paper first briefly reviewed the existing works regarding sentiment analysis and auto-grading. These are the two fields where text classification algorithms have well proved their successes. Then we described two different data sets of our experiments. In the third, we introduced the three different text classification approaches with the emphasis in GPT approach. In the fourth, we reported the results with all the different settings. In the last, we concluded with remarks."
    },
    {
      "title": "2 Related Work",
      "text": ""
    },
    {
      "title": "Emotion And Sentiment Analysis",
      "text": "In the field of learning analytics, text classification technology is often employed for the analysis of students' emotions in participatory learning processes. Participatory learning, different from rigid teacher-controlled instruction, underscores the active involvement of students in collaborative learning processes. The key to successful participatory learning is to create positive experiences that enhance children's cognitive engagement and awareness (Vartiainen, Tedre, & Valtonen, 2020). Emotions refers to a multi-componential construct of psychological subsystems, including affective, cognitive, motivational, expressive, and peripheral physiological processes (Pekrun, 2006). With the support of advanced technology, research on emotions and their role in cognitive processes within technology-rich learning environments has been gaining more attention (e.g., Huang, Huang, & Lajoie, 2022). In technology-supported learning contexts, emotions can be expressed through emotional tones in the form of verbal or textual output, which refers to the vocal expression or dialogue of emotion that conveys a student's affective states (Chang et al., 2023; Ishii, Reyes, & Kitayama, 2003). Emotions and sentiments interpreted from expressed emotional tones are associated with students' engagement, social interactions, and knowledge-sharing behaviors (Dang-Xuan et al., 2017; Nayakki et al., 2014; Rapisarda, 2002), thereby shaping learners' overall learning experience. Automatically detecting learners' emotions form the foundation for understanding what keeps their learning experience positive and how to maintain such experiences. Therefore, it is important to automatically assess students' emotions and sentiments in real-time learning procedure with the assistance of machine learning and large language models. Kastrati et al. (2020) devised a weakly supervised aspect-based sentiment analysis framework. Given student comments, they employed Convolutional Neural Network (CNN) for sentiment classification and Long Short Term Memory (LSTM) for aspect category recognition. Experiments were conducted using a substantial real-world education dataset, encompassing approximately 105K students' reviews gathered from Coursera, along with a dataset comprising 5,989 students' feedback in traditional classroom settings. CNN sentiment classification was applied for the binary classification of aspect sentiment, achieving 82.1% F1 score, while LSTM aspect category recognition was employed for identifying four aspect categories, achieving 86.3% F1 score. Dahiya, Mohta and Jain (2020) performed sentiment and emotion analysis on textual messages with emoticons. Employing a CNN model, they trained a classifier to categorize 29,939 unique statements which are acquired from Kaggle into six distinct emotions and gave an average accuracy of 72.9%. Klunder, Horstmann and Karras (2020) integrate sentiment analysis with tradition natural language processing techniques to automate sentiment classification in text-based communication. They utilized three machine learning methods--random forest, Support Vector Machine (SVM), and Naive Bayes--to categorize each text segment as positive, neutral, or negative. The efficacy of this approach is substantiated through an industrial case study in software development. The case study comprises 1,947 messages extracted from a group chat within the Zulip communication tool, encompassing a total of 7,070 sentences. The ultimate classification results, reaching an accuracy of 62.97%, exhibit a level of effectiveness comparable to human ratings (Klunder et al., 2020). Generally, datasets used for sentiment analysis are large and need a lot of human labeling for training. Even traditional methods such as random forest and SVM can sometimes satisfy the requirements. However, certain limitations may exist, such as the inability to integrate contextual information from the dialogue for assessment."
    },
    {
      "title": "Automatic Grading For Open-Response Questions",
      "text": "The grading of open-response questions is a critical aspect of educational assessments, requiring a balance between subjective evaluation and the need for efficient, scalable, and consistent grading methods. Grading for open-response questions involves the assessment of students' answers to questions that require free-form responses, as opposed to multiple-choice or other closed-ended formats. This context often demands more flexible and subjective grading methods in educational and examination settings. So traditional approach involves manual grading by educators or domain experts. To save repetitive human works and support personalized learning (Erickson & Botelho, 2021), recent researches have focused on the development of automatic grading methods with machine learning and natural language processing technologies. Erickson et al. (2020) employed tree-based machine learning approaches, including random forest and XGBoost, as well as deep learning methodologies such as LSTM and the Rasch Model, to assess and analyze open-response questions in mathematics. The dataset utilized in their study comprised a total of 141,612 student responses to 2,042 unique problems from 25,069 students. Wilson et al (2022) conducted a comparative analysis employing three machine learning models including logistic regression, random forest, and K nearest neighbors. These algorithms are used to classify 2,450 student responses to open-ended questions in the Physical Measurement Questionnaire into four classes. Zhang et al (2022) utilized the continuous bag-of-words model (CBOW) to integrate the domain-general and domain-specific information in the process of feature engineering. Then they built the classifier model using LSTM and evaluated it with 7 reading comprehension questions with over 16,000 labeled student answers. Compared with other traditional automatic grading models, their proposed model significantly improved the automatic grading performance on semi-open-ended questions."
    },
    {
      "title": "Llm-Based Models For Deductive Coding",
      "text": "Building on the advancements in emotion and sentiment analysis, as well as the development of automatic grading systems for open-response questions, there has been growing interest in applying LLM to other complex tasks such as deductive coding in qualitative research. To offer new possibilities for handling large-scale qualitative data sets more efficiently, researchers have begun exploring how these models can assist or automate parts of this process. Xiao et al (2023) find that combing LLM with expert-drafted codebooks achieves fair to substantial agreements with expert-coded results in deductive coding. Their study utilized an expert codebook to construct prompts, and although it provided transparency and explicit control, it limited the performance of the model. Tai et al (2023) proposed a methodology using LLM to support traditional deductive coding in qualitative research. They compared the performance of a large language model with traditional human coding in identifying five conceptual codes in three narrative texts, providing a systematic and reliable platform for code identification and offering a means of avoiding analysis misalignment. They also pointed that there is a lack of validated research examining the use of LLM in qualitative analysis. Chew et al (2023) provide a holistic approach for performing deductive coding with LLM, and aims to assess the effectiveness of GPT-3.5 across a range of deductive coding tasks through an in-depth case study and empirical evaluation on four publicly available datasets. The study noted several limitations, including the need for extensive prompt engineering and the assessment of coding performance across a wide variety of LLM. Hou et al (2024) explored the use of LLM to assist in deductive coding of social annotation data, achieving fair to substantial agreement with human raters in context-independent dimensions and moderate agreement in context-dependent dimensions. They claimed that there were still some challenges of including original text in the prompt engineering or fine-tuning models for context-dependent dimensions. In summary, existing works regarding text classification adoption in education mainly focus on sentiment analysis in participatory learning and automatic grading on student answers. Both of these works required considerable large data set for training the supervised machine learning models, so that satisfactory results can be achieved. Only a few studies have attempted to use LLM for deductive coding in content analysis. Our study aims to further verify that generalist foundation models such as GPT and their combination with other technologies can be helpful in deductive coding tasks with few labeled items. In particular, we took two different datasets to conduct our experiments."
    },
    {
      "title": "3 Datasets",
      "text": "To conduct our study and evaluate the three automatic coding techniques, we used two datasets that were obtained from a Chinese poetry appreciation literary course at a university in central China. The course was open for science and engineering students, aiming to improve their literacy ability and cultivate their aesthetic ability. The course adopted a face-to-face collaborative teaching model and the classroom students were divided into groups. Some pre-class or in-class tasks require students to complete through a collaborative annotation platform developed by our research team. Before class, the teacher assigned reading tasks, requiring students to read learning materials on the platform and highlight and annotate the materials. This formed the annotation dataset. During class, students used the chat section of the platform to interact within groups based on in-class tasks and collaborate to solve problems. This formed the discussion dataset. We described the two datasets and the coding schema in detail below. Each dataset was split into training and testing sets using an 8:2 ratio. The training set was to train the machine learning models or tune the prompts. The testing set was to evaluate the performance of different automatic coding techniques. In this way, all the automatic coding models used the exact same dataset for model calibration and evaluation. Note that the GPT approach only used several items in the training set and the other machine learning approaches used the entire training set. However, such settings formed fair comparisons for all the models. In the fall semester of 2021, we collected a dataset comprising 607 student annotations. This dataset was divided into a training set containing 484 pieces of data and a testing set containing 123 pieces. Seventy-three students participated in the course, representing diverse majors such as artificial intelligence, history, psychology, physics, etc. Students were asked to read and annotate a Chinese article titled \"The Image of Plum in Ancient Chinese Literature\" on a reading annotation system developed by our research team. The annotation reading interface of the system is shown in Figure 1. **Figure 1** _The annotation reading interface of the platform_ After collecting the data, two researchers manually analyzed and coded the annotation data according to the cognitive engagement classification scheme adapted from Chi and Wylie (2014). After randomly selecting 200 items from the annotation data, the level of agreement between the two researchers was measured at 0.752 Kappa, indicating good reliability. Cognitive engagement of the annotation data was analyzed manually based on the coding scheme of Table 1. _The coding scheme of annotation data_ \\begin{tabular}{p{42.7pt} p{113.8pt} p{113.8pt} p{113.8pt}} \\hline Code & Behaviors & Descriptions & Exemplar \\\\ \\hline A & Copy & Highlight and directly/selectively copy ideas from material. & \"During the Six Dynasties period, plum blossoms served as symbols of friendship, love, and hometown sentiments, yet they had not yet freed themselves from the metaphorical expressions of comparison.\" \"I believe this is inseparable from the harsh winter, preceding the myriad flowers, they stand alone in ushering in spring. This resilience in blooming independently in the cold resonates with many poets who have faced adversity in their careers and lives, yet refuse to conform to worldly impurities.\" \"In the ancient poems I've studied before, there were \"red beans\" symbolizing love and \"broken willows\" conveying homesickness. Now, I've come to realize that \"plum blossoms\" can also represent friendship, love, and hometown sentiments.\" \\begin{table} \\begin{tabular}{p{42.7pt} p{113.8pt} p{113.8pt}} \\hline Code & Behaviors & Descriptions & Exemplar \\\\ \\hline A & Copy & Highlight and directly/selectively copy ideas from material. & \"During the Six Dynasties period, plum blossoms served as symbols of friendship, love, and hometown sentiments, yet they had not yet freed themselves from the metaphorical expressions of comparison.\" \"I believe this is inseparable from the harsh winter, preceding the myriad flowers, they stand alone in ushering in spring. This resilience in blooming independently in the cold resonates with many poets who have faced adversity in their careers and lives, yet refuse to conform to worldly impurities.\" \"In the ancient poems I've studied before, there were \"red beans\" symbolizing love and \"broken willows\" conveying homesickness. Now, I've come to realize that \"plum blossoms\" can also represent friendship, love, and hometown sentiments.\" \\\\ \\hline C2 & Integration & Highlight, and integrate other information in the material or other materials for comparison and connection, etc. & \"In the ancient poems I've studied before, there were \"red beans\" symbolizing love and \"broken willows\" conveying homesickness. Now, I've come to realize that \"plum blossoms\" can also represent friendship, love, and hometown sentiments.\" \\\\ \\hline \\end{tabular} \\end{table} Table 1: The coding scheme of annotation data"
    },
    {
      "title": "The Discussion Dataset",
      "text": "The discussion dataset, comprising a total of 404 pieces of discussion data, was gathered in the fall semester of 2022. This dataset was subsequently divided into a training set containing 320 pieces of data and a testing set containing 84 pieces. Seventy-two students, voluntarily enrolled in the course, participated in this dataset. The students had an average age of 20 and came from various non-literary majors. The students were grouped based on the Kolb Learning Style Questionnaire(Manolis, Burns, Assudani, & Chinta, 2013), resulting in a total of 10 groups with 6-7 students in each. Students worked in groups to select one of the five poets and analyzed their representative works of chrysanthemum poems through text communication using the same system. The discussion interface of the system is shown in Figure 2. Following data collection, two researchers conducted manual coding and analysis of the discussion data. The coding scheme, a modified iteration of Chi's cognitive engagement framework (Chi & Wylie, 2014), was tailored to students' discussion tasks and learning characteristics. In the initial phase, two researchers independently selected data from two groups for coding, amounting to a total of 84 instances, achieving an inter-rater reliability of 0.69. Following this, the researchers refined the coding framework and deliberated on the initially coded data. Through negotiation and consensus-building, they proceeded with a second round of pre-coding. In this subsequent round, two groups, comprising a total of 69 instances, were once again randomly chosen, resulting in an enhanced inter-rater reliability of 0.80. The cognitive engagement coding framework employed by researchers during the manual analysis of the discussion data is detailed in Table 2. \\begin{table} \\begin{tabular}{l l l} \\hline Code & Descriptions & Exemplar \\\\ \\hline M & Assigning, coordinating, and & “Sure, let’s start with the translation, shall we?” \\\\ & supervising tasks. & \\\\ \\hline \\end{tabular} \\end{table} Table 2: The coding scheme of discussion data [MISSING_PAGE_EMPTY:7]"
    },
    {
      "title": "4 Methods",
      "text": "This section briefly introduced the three different approaches of text classification for automatic deductive coding. In this study, we considered two different types of discourses. One is students reading annotation, the other is student discussion dialog. Since that the purpose of the comparison is to explore the power of large language models, we respectively selected one representative model for traditional machine learning and BERT-like methods. The rest of the section described these two models and how we used GPT on deductive discourse coding."
    },
    {
      "title": "Traditional Machine Learning Method",
      "text": "We selected Random Forest (RF) as the representative model for traditional machine learning method, because RF has proved to be well performed in many related tasks (Schonlau and Zou, 2020). Figure 4 illustrates the process of using traditional machine learning method for classification. The very first step is to conduct word segmentation to transform each student sentence into a set of words. We used jieba library to perform this step. Note that such a transformation would lose the sequential information of the original sentence as well as the grammatical structures (Qader, Ameen, and Ahmed, 2019). After the sentences were split into sets of words, the frequency of each word can be calculated and the corresponding frequencies of the words used as the inputs of Random Forest. This kind of feature engineering approach is called as Bag-of-Words (BoW). Random Forest is essentially a group of decision trees. Each decision node of the decision trees was comprised of word frequency and a threshold. We used RandomForestClassifier in scikit-learn to perform the implementation. There were two hyper-parameters of this algorithm, which were estimator and random feed. Estimator defines the upper bound of the number of decision trees in the forest and the random feed decides initial values of the parameters. The estimators was set to 100 and the random feed was set to 42."
    },
    {
      "title": "Bert-Like Pretrained Model For Classification",
      "text": "For BERT-like pretrained models, we used RoBERTa, an advanced version of BERT, enhanced through training on a larger corpus (Shaheen, Wohlgenannt, & Fitz, 2020). Notably, Qasim et al (2022) demonstrated the superior performance of the RoBERTa model in various classification tasks compared to other pre-trained language models. However, they also highlighted that RoBERTa exhibits stronger linguistic bias. Figure 5 shows the process of using RoBERTa for classification. The initial step involves loading the dataset and tokenizing it using RoBERTa's tokenizer. This process converts the text into numerical representations suitable for machine learning. In specific, we utilized the chinese_roberta_wwm_ext_pytorch pre-trained model. Different pooling strategies, including CLS pooling, mean pooling, and max pooling, are implemented to extract features from the hidden states of the RoBERTa model. These strategies contribute to capturing essential information from the input sequences. A linear classifier head is incorporated into the model to translate the extracted features into target categories. The model is trained using the Adam optimizer with a learning rate of 2e-5 and a weight decay of 1e-4. The training is conducted over 5 epochs. A batch size of 16 is employed during the training process. The maximum sequence length for tokenization is set to 200. Sentences exceeding this length are truncated, while shorter sentences are padded with spaces to ensure uniform input dimensions."
    },
    {
      "title": "Figure 5",
      "text": "_Process of RoBERTa classification method_ [MISSING_PAGE_FAIL:10] prompt, we gave the input data that needs to be coded. This method was only used for the annotation dataset. So, the input data comprised the annotation along with its highlighting text. Regarding GPT settings, we used the gpt-4-1106-preview version with a temperature value of 0, a maximum text length of 4096, frequency_penalty set to 0, and presence_penalty set to 0."
    },
    {
      "title": "4.3.2 Finetuning",
      "text": "We performed fine-tuning based on the GPT-3.5-Turbo model. The basic idea of fine-tuning is to use many labeled examples as the training data to calibrate the foundation model, so that the fine-tuned model can adapt to the specific downstream task, which is deductive coding in our case. As the documents of GPT suggested, around 100 trainning samples would be sufficient for the fine-tuning task of GPT. This method was applied to both annotation and discussion datasets. For the annotation dataset, we used the same prompts as those in the previous setting (i.e. prompt only). We used 90 entries to build fine-tuned model and evaluated its performance. The training epoch was set to 3. For the discussion dataset, because we had 10 sets of dialogs generated by 10 groups of students with 404 dialog turns in total, we asked GPT to consider the dialog turns independently without the context and did fine-tuning. We used 100 entries to build fine-tuned model. The training epoch was set to 3 as well."
    },
    {
      "title": "4.3.3 Prompt + Traditional Nlp",
      "text": "As Do et al (2024) suggested, the overall performance can be improved by integrating prompt based LLMs and traditional NLP technologies. So, we did such combinations in our automatic deductive coding as well. In specific, we built a reference database including the original reading material and the corresponding online reference information. During the process of automatic coding, instead of handling over the entire task to GPT, we did similar sentence checking at first. It means that for each input sentence of students' annotations, we wrote program to find out the most similar sentence in the reference database. The similarity between two sentences is defined as the follows: \\[similarity_{cos}(A,B)=\\frac{\\sum_{i=1}^{n}A_{i}\\times B_{i}}{\\sqrt{\\sum_{i=1} ^{n}(A_{i})^{2}}\\times\\sqrt{\\sum_{i=1}^{n}(B_{i})^{2}}}\\,\\#(1)\\] If the similarity score between a student's annotation and the most similar sentence in the reference database exceeded a threshold, the program coded the annotation as \"A\", otherwise, the program asked GPT to make further judgement. The structure of the prompt for the further judgement remained the same, except for the absence of the introduction to the code \"A\"."
    },
    {
      "title": "4.3.4 Prompt With Context Knowledge + Traditional Nlp",
      "text": "At last, we further included context knowledge in the prompt. We used different strategies for the two different datasets. We used RAG with the reference database mentioned in the previous section for the annotation dataset. We injected the full dialog context for the discussion dataset. We respectively described the two strategies in the following. For the annotation dataset, we retrieved two relevant sentences of the input annotation from the reference database based on their sentence similarities. We used the retrieved information to augment the generated results of GPT, in which retrieval augmented generation (RAG) was implemented. Note that the traditional NLP technique was used again in the calculation of sentence similarity. Different from the prompts introduced previously, prompts here were constructed dynamically based on the reference sentences. The contents of the prompts were also slightly different because we asked GPT to break the deductive coding process into two steps inspired by CoT. In specific, GPTneeded to first consider the relation of the student's annotation to the two reference sentences, then make the coding decision. Figure 6 illustrated how traditional NLP and context knowledge were integrated in the workflow of the automatic deductive coding with GPT. Traditional NLP was integrated mainly through similarity calculation and context knowledge was integrated through RAG. The resulting framework of the classification prompt for this experiment is outlined below: * **[**I**ntroduction to the Course Background **]** * **[**I**ssuance of Instructions **]** * **[**I**Detailed Introduction to Encoding Rules **]** * **[**I**Output Structure and Examples **]** * Comment * Highlight * Reference1 * Reference2 **Figure 6** _The workflow of automatic deductive coding for annotation data_ For the discussion dataset, we included the entire dialog of the discussion group as the context knowledge and asked GPT to code each dialog turn. GPT was instructed to consider the surrounding dialog turns while determining the encoding categories. The framework of the classification prompt for this dataset is as follows: * **[**I**ntroduction to the Course Background **]** * **[**I**ssuance of Instructions **]** * **[**I**Detailed Introduction to Encoding Rules **]** * **[**I**Output Structure and Examples **]** * Student dialogs"
    },
    {
      "title": "Evaluation Metrics",
      "text": "In this section, we described the evaluation metrics employed to assess the performance of the automatic deductive coding approaches. We used accuracy, precision, recall, F1 score, and Cohen's kappa to evaluate and compare the approaches. The first four metrics are widely used in computational-related journals and the last one is used widely in psychology journals. Accuracy represents the ratio of correctly predicted instances to the total instances in the dataset. It provides an overall measure of the model's correctness. Precision is the ratio of correctly predicted positive observations to the total predicted positives. It measures the accuracy of positive predictions. Recall, also known as sensitivity or true positive rate, is the ratio of correctly predicted positive observations to the actual positives in the dataset. It assesses the model's ability to capture all relevant instances. The F1 score is the harmonic mean of precision and recall. It provides a balanced measure that considers both false positives and false negatives. Cohen's Kappa is a statistic that measures the agreement between the predicted and actual classifications, considering the possibility of the agreement occurring by chance. It corrects for the chance agreement inherent in accuracy. High accuracy, precision, recall, and F1 score values all indicate effective classification performance. Kappa values indicate the amount of agreement between the predicted results and the ground truth. Values close to 1 indicate substantial agreement, while 0 suggests no agreement. This comprehensive set of evaluation metrics enables a thorough analysis of the classification methods, shedding light on their efficiency in handling annotation and discussion datasets."
    },
    {
      "title": "5 Results And Discussion",
      "text": "In this section, we reported the results of the three different approaches for automatic deductive coding and make a discussion on the results. For each approach, we reported the coding results of annotation and discussion datasets respectively. We made a summary of the three approaches by the end of this section. Among the three approaches, we focused on the GPT-based one."
    },
    {
      "title": "Traditional Machine Learning (Random Forest)",
      "text": "For the annotation dataset, the random forest classification method achieved an overall accuracy of 0.56. This accuracy underscores a moderate yet promising success in accurately assigning coded annotations to their respective categories. The Kappa value was found to be 0.28. We calculated precision, recall, and F1-score for each code. The results were reported in Table 3. The results clearly showed that the random forest classifier did not provide good performance in general and had quite different performance on different coding categories. This approach exhibited an especially bad performance in the Interaction (I) coding category in the discussion dataset. This was too surprising because this approach simply treated all discourses as bags of words and made deductive coding decisions solely based on the frequencies of occurrences of these words. \\begin{table} \\begin{tabular}{l l l l l} \\hline & precision & recall & f1-score & support \\\\ \\hline M & 1.00 & 0.62 & 0.76 & 13 \\\\ P & 1.00 & 0.25 & 0.40 & 20 \\\\ A & 0.32 & 1.00 & 0.49 & 12 \\\\ C & 0.45 & 0.54 & 0.49 & 28 \\\\ I & 0.00 & 0.00 & 0.00 & 11 \\\\ Macro avg & 0.56 & 0.48 & 0.43 & 84 \\\\ Weighted avg & 0.59 & 0.48 & 0.45 & 84 \\\\ \\hline \\end{tabular} \\end{table} Table 4: Performance Metrics for Discussion Data \\begin{table} \\begin{tabular}{l l l l l} \\hline & precision & recall & f1-score & support \\\\ \\hline A & 0.56 & 0.33 & 0.42 & 30 \\\\ C1 & 0.55 & 0.89 & 0.68 & 53 \\\\ C2 & 0.63 & 0.30 & 0.41 & 40 \\\\ Macro avg & 0.58 & 0.51 & 0.50 & 123 \\\\ Weighted avg & 0.58 & 0.56 & 0.53 & 123 \\\\ \\hline \\end{tabular} \\end{table} Table 3: Performance Metrics for Annotation Data"
    },
    {
      "title": "Bert-Based (Roberta)",
      "text": "For the annotation dataset, the overall accuracy was determined to be 0.59, with a Kappa value of 0.36. Precision, recall, F1-score for each coding category was reported in Table 5. As a more advanced NLP technique, RoBERTa demonstrated improved performance in both annotation and discussion datasets. However, this approach seemed to produce biased results more easily. For example, the trained automatic coder did not identify any M or I code. Probably because its computational model was complex and easy to be overfitted by a small amount of data."
    },
    {
      "title": "Gpt-Based",
      "text": "For the annotation dataset, we ran four experiments with the automatic coding methods including prompt-only, fine-tuning, prompt +NLP, and prompt with context + NLP. We reported the four results in the following. **Results of prompt-only method.** The overall accuracy was quite low, at 0.37. Given that it was essentially a three-fold classification problem, the performance was just a little bit better than chance. The Kappa value was only 0.005. So, using prompt alone was not enough for accomplishing our deductive coding task. Table 7 provides more details of the results for each coding category. \\begin{table} \\begin{tabular}{l l l l l} \\hline & precision & recall & f1-score & support \\\\ \\hline M & 0.00 & 0.00 & 0.00 & 13 \\\\ P & 0.90 & 0.95 & 0.93 & 20 \\\\ A & 0.92 & 0.92 & 0.92 & 12 \\\\ C & 0.51 & 0.93 & 0.66 & 28 \\\\ I & 0.00 & 0.00 & 0.00 & 11 \\\\ Macro avg & 0.47 & 0.56 & 0.50 & 84 \\\\ Weighted avg & 0.52 & 0.67 & 0.57 & 84 \\\\ \\hline \\end{tabular} \\end{table} Table 6: Discussion classification metrics using RoBERTa \\begin{table} \\begin{tabular}{l l l l l} \\hline & precision & recall & f1-score & support \\\\ \\hline A & 0.29 & 0.17 & 0.21 & 30 \\\\ C1 & 0.42 & 0.45 & 0.44 & 53 \\\\ C2 & 0.33 & 0.40 & 0.36 & 40 \\\\ Macro avg & 0.35 & 0.34 & 0.34 & 123 \\\\ Weighted avg & 0.36 & 0.37 & 0.36 & 123 \\\\ \\hline \\end{tabular} \\end{table} Table 7: Annotation classification metrics of prompt only experiment \\begin{table} \\begin{tabular}{l l l l l} \\hline & precision & recall & f1-score & support \\\\ \\hline A & 0.64 & 0.47 & 0.54 & 30 \\\\ C1 & 0.66 & 0.75 & 0.70 & 53 \\\\ C2 & 0.47 & 0.47 & 0.48 & 40 \\\\ Macro avg & 0.59 & 0.57 & 0.57 & 123 \\\\ Weighted avg & 0.59 & 0.59 & 0.59 & 123 \\\\ \\hline \\end{tabular} \\end{table} Table 5: Annotation classification metrics using RoBERTa [MISSING_PAGE_FAIL:15] [MISSING_PAGE_FAIL:16] [MISSING_PAGE_EMPTY:17] **RQ2:** The power of prompt techniques for automatic deductive coding We need to note that the GPT-based approach is not just plug-and-play for automatic deductive coding. Indeed, ChatGPT provides an easy-to-use interface for all. Even people without any programming experience can use ChatGPT to accomplish some sophisticated tasks like article summarization and data analysis. However, as we showed in the results of the different methods of the GPT-based approach, the performance can be significantly improved when techniques such as RAG and CoT are included because GPT needs contextual information to make more accurate decisions. We also showed that we may need to figure out some rules with traditional NLP techniques like discourse similarity calculation to identify some discourse code, instead of having GPT taking over all the tasks. The reasons behind such integration are probably because we as human experts know more contextual information and guidance than LLMs, and we should describe them as much as possible by using all kinds of LLMs-related techniques such as RAG, CoT, few-shots and so forth."
    },
    {
      "title": "6 Conclusion",
      "text": "In conclusion, this study aimed to assess and compare three distinct text classification methods applied to automatic deductive coding. The methods we adopted encompassed traditional text classification with feature engineering, BERT-like pre-trained language models, and GPT-like pretrained language models, representing generative language models. The traditional text classification method, employing feature engineering struggled to adapt to the case of student-generated content in both annotation and discussion datasets. On the other hand, the BERT-like model exhibited improved accuracy, leveraging its contextual understanding of language. However, its reliance on tokenized input and the need for substantial data size and computational resources limit its practicality. The standout performer in our study was the GPT-based approach. This approach showcased remarkable adaptability and effectiveness in classifying both annotation and discussion data, outperforming other methods in terms of accuracy and Kappa values. The generative language model, with its inherent ability to consider word order and context, demonstrated promising results even with a limited dataset. The comparison highlighted the potential of generative language models. The GPT-like model, in particular, presents a compelling avenue for further exploration in educational technology, showcasing promising results in the context of student participatory learning. As the field evolves, leveraging such models could bring more efficient and accurate ways to assess and engage with student-generated content, ultimately enhancing the quality of educational processes."
    },
    {
      "title": "References",
      "text": "* Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C.,... Amodei, D. (2020). Language models are few-shot learners. _Advances in Neural Information Processing Systems_, 33, 1877-1901. * Chang et al. (2023) Chang, H. S., Lee, C. Y., Wang, X., Young, S. T., Li, C. H., & Chu, W. C. (2023). Emotional tones of voice affect the acoustics and perception of Mandarin tones. _Plos One_, 18(4), e0283635. * Chew et al. (2023) Chew, R., Bollenbacher, J., Wenger, M., Speer, J., & Kim, A. (2023). LLM-assisted content analysis: Using large language models to support deductive coding. _arXiv preprint arXiv:2306.14924_. * Chi & Wylie (2014) Chi, M. T. H., & Wylie, R. (2014). The ICAP Framework: Linking Cognitive Engagement to Active Learning Outcomes. _Educational Psychologist_, 49(4), 219-243. doi:10.1080/00461520.2014.965823 * Dahiya et al. (2020) Dahiya, S., Mohta, A., & Jain, A. (2020, June). _Text classification based behavioural analysis of whatsapp chats._ Paper presented at the 5th International Conference on Communication and Electronics Systems (ICCES) (pp. 717-724). IEEE. * Dang-Xuan et al. (2017) Dang-Xuan, L., Stieglitz, S., Wladarsch, J., & Neuberger, C. (2017). _An investigation of influentials and the role of sentiment in political communication on Twitter during election periods._ Paper presented at Social Media and Election Campaigns (pp. 168-198): Routledge. * Do et al. (2024) Do, V. T., Hoang, V. K., Nguyen, D. H., Sabahi, S., Yang, J., Hotta, H., Nguyen, M. T., & Le, H. (2024). Automatic Prompt Selection for Large Language Models. _arXiv preprint arXiv:2404.02717_. * Dang et al. (2020)Erickson, J. A., & Botelho, A. (2021, January). _Is it fair? Automated open response grading_. Paper presented at International Conference on Educational Data Mining. * Erickson et al. (2020) Erickson, J. A., Botelho, A. F., McAteer, S., Varatharaj, A., & Hefferman, N. T. (2020, March). _The automated grading of student open responses in mathematics_. Paper presented at Proceedings of the Tenth International Conference on Learning Analytics & Knowledge (pp. 615-624). * Gao et al. (2023) Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang, M., & Wang, H. (2023). Retrieval-augmented generation for large language models: A survey. _arXiv preprint arXiv:2312.10997_. * Hjelm (2021) Hjelm, T. (2021). _Discourse analysis_. Paper presented at the Routledge Handbook of Research Methods in the Study of Religion (pp. 229-244). Routledge. * Hou et al. (2024) Hou, C., Zhu, G., Zheng, J., Zhang, L., Huang, X., Zhong, T.,... & Ker, C. L. (2024, March). Prompt-based and Fine-tuned GPT Models for Context-Dependent and-Independent Deductive Coding in Social Annotation. In _Proceedings of the 14th Learning Analytics and Knowledge Conference_ (pp. 518-528). * Huang et al. (2022) Huang, X., Huang, L., & Lajoie, S. P. (2022). Exploring teachers' emotional experience in a TPACK development task. _Educational Technology Research and Development, 70_(4), 1283-1303. * Ishii et al. (2003) Ishii, K., Reyes, J. A., & Kitayama, S. (2003). Spontaneous attention to word content versus emotional tone: Differences among three cultures. _Psychological Science_, 14(1), 39-46. * Kastrati et al. (2020) Kastrati, Z., Imran, A. S., & Kurti, A. (2020). Weakly supervised framework for aspect-based sentiment analysis on students' reviews of MOOCs. _IEEE Access, 8_, 106799-106810. * Klunder et al. (2020) Klunder, J., Horstmann, J., & Karras, O. (2020). _Identifying the mood of a software development team by analyzing text-based communication in chats with machine learning._ Paper presented at Human-Centered Software Engineering: 8th IFIP WG 13.2 International Working Conference, HCSE 2020, Eindhoven, The Netherlands, November 30-December 2, 2020, Proceedings 8 (pp. 133-151). Springer International Publishing. * Manolis et al. (2013) Manolis, C., Burns, D. J., Assudani, R., & Chinta, R. (2013). Assessing experiential learning styles: A methodological reconstruction and validation of the Kolb Learning Style Inventory. _Learning and Individual Differences, 23_, 44-52. * Meyer (2023) Meyer, D. K. (2023). _Using instructional discourse analysis to study the scaffolding of student self-regulation_. Paper presented at Using Qualitative Methods to Enrich Understandings of Self-regulated Learning (pp. 17-25). Routledge. * Nayakki et al. (2014) Nayakki, P., Jarvela, S., Kirschner, P. A., & Jarvenoja, H. (2014). Socio-emotional conflict in collaborative learning--A process-oriented case study in a higher education context. _International Journal of Educational Research_, 68, 1-14. * Pekrun (2006) Pekrun, R. (2006). The control-value theory of achievement emotions: Assumptions, corollaries, and implications for educational research and practice. _Educational Psychology Review_, 18, 315-341. * Qader et al. (2019) Qader, W. A., Ameen, M. M., & Ahmed, B. I. (2019, June). _An overview of bag of words; importance, implementation, applications, and challenges._ Paper presented at 2019 International Engineering Conference (IEC) (pp. 200-204). IEEE. * Qasim et al. (2022) Qasim, R., Bangyal, W. H., Alqarni, M. A., & Ali Almazroi, A. (2022). A Fine-Tuned BERT-Based Transfer Learning Approach for Text Classification. _Journal of Healthcare Engineering, 2022_(1), 3498-123. * Rapisarda (2002) Rapisarda, B. A. (2002). The impact of emotional intelligence on work team cohesiveness and performance. _The International Journal of Organizational Analysis, 10_(4), 363-379. * Rose et al. (2008) Rose, C., Wang, Y. C., Cui, Y., Arguello, J., Stegmann, K., Weinberger, A., & Fischer, F. (2008). Analyzing collaborative learning processes automatically: Exploiting the advances of computational linguistics in computer-supported collaborative learning. _International Journal of Computer-supported Collaborative Learning, 3_, 237-271. * Rose (2017) Rose, C. P. (2017). A social spin on language analysis. _Nature, 545_(7653), 166-167. * Rose & Ferschke (2016) Rose, C. P., & Ferschke, O. (2016). Technology support for discussion based learning: From computer supported collaborative learning to the future of massive open online courses. _International Journal of Artificial Intelligence in Education, 26_, 660-678. * Schonlau & Zou (2020) Schonlau, M., & Zou, R. Y. (2020). The random forest algorithm for statistical learning. _The Stata Journal, 20_(1), 3-29. * Shaheen et al. (2020) Shaheen, Z., Wohlgenannt, G., & Fitz, E. (2020). Large scale legal text classification using transformer models. _arXiv preprint arXiv:2010.12871_. * Tai et al. (2023) Tai, R. H., Bentley, L. R., Xia, X., Sitt, J. M., Fankhauser, S. C., Chicas-Mosier, A. M., & Monteith, B. M. (2023). Use of large language models to aid analysis of textual data. _bioRxiv_. * Tai et al. (2020) Tai, R. H., Bentley, L. R., Xia, X., Sitt, J. M., Fankhauser, S. C., Chicas-Mosier, A. M., & Monteith, B. M. (2020). Use of large language models to aid analysis of textual data. _bioRxiv_. * Tai et al. (2020) Tai, R. H., Bentley, L. R., Xia, X., Sitt, J. M., Fankhauser, S. C., Chicas-Mosier, A. M., & Monteith, B. M. (2020). Use of large language models to aid analysis of textual data. _bioRxiv_. * Tai et al. (2020) Tai, R. H., Bentley, L. R., Xia, X., Sitt, J. M., Fankhauser, S. C., Chicas-Mosier, A. M., & Monteith, B. M. (2020). Use of large language models to aid analysis of textual data. _bioRxiv_. * Tai et al. (2020) Tai, R. H., Bentley, L. R., Xia, X., Sitt, J. M., Fankhauser, S. C., Chicas-Mosier, A. M., & Monteith, B. M. (2020). Use of large language models to aid analysis of textual data. _bioRxiv_. * Tai et al. (2020) Tai, R. H., Bentley, L. R., Xia, X., Sitt, J. M., Fankhauser, S. C., Chicas-Mosier, A. M., & Monteith, B. M. (2020). Use of large language models to aid analysis of textual data. _bioRxiv_. * Tai et al. (2020) Tai, R. H., Bentley, L. R., Xia, X., Sitt, J. M., Fankhauser, S. C., Chicas-Mosier, A. M., & Monteith, B. M. (2020). Use of large language models to aid analysis of textual data. _bioRxiv_. * Tai et al. (2020) Tai, R. H., Bentley, L. R., Xia, X., Sitt, J. M., Fankhauser, S. C., Chicas-Mosier, A. M., & Monteith, B. M. (2020). Use of large language models to aid analysis of textual data. _bioRxiv_. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Nikolay, B., Soumya, B., Prajjwal, B., Shrutti, B., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,... Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_. * Variainen _et al._ (2020) Variainen, H., Tedre, M., & Valtonen, T. (2020). Learning machine learning with very young children: Who is teaching whom?. _International Journal of Child-computer Interaction, 25_, 100182. * Wang _et al._ (2015) Wang, X., Yang, D., Wen, M., Koedinger, K., & Rose, C. P. (2015). _Investigating How Student's Cognitive Behavior in MOOC Discussion Forums Affect Learning Gains_. Paper presented at International Educational Data Mining Society. * Wei _et al._ (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, V. Q., & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems, 35_, 24824-24837. * White _et al._ (2023) White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J., & Schmidt, D. C. (2023). A prompt pattern catalog to enhance prompt engineering with chatgpt. _arXiv preprint arXiv:2302.11382_. * Wilson _et al._ (2022) Wilson, J., Pollard, B., Aiken, J. M., Caballero, M. D., & Lewandowski, H. J. (2022). Classification of open-ended responses to a research-based assessment using natural language processing. _Physical Review Physics Education Research, 18_(1), 010141. * Xiao _et al._ (2023) Xiao, Z., Yuan, X., Liao, Q. V., Abdelghani, R., & Oudeyer, P. Y. (2023, March). Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding. In _Companion proceedings of the 28th international conference on intelligent user interfaces_ (pp. 75-78). * Zhang _et al._ (2022) Zhang, L., Huang, Y., Yang, X., Yu, S., & Zhuang, F. (2022). An automatic short-answer grading model for semi-open-ended questions. _Interactive Learning Environments, 30_(1), 177-190."
    }
  ]
}