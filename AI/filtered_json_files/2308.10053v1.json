{
  "title": "Large Language Models as Zero-Shot Conversational Recommenders",
  "authors": [
    "Zhankui He",
    "Zhouhang Xie",
    "Rahul Jha",
    "Harald Steck",
    "Dawen Liang",
    "Bodhisattwa Prasad Majumder",
    "Nathan Kallus",
    "Julian Mcauley"
  ],
  "abstract": "\n In this paper, we present empirical studies on conversational recommendation tasks using representative large language models in a zero-shot setting with three primary contributions. (1) Data: To gain insights into model behavior in \"in-the-wild\" conversational recommendation scenarios, we construct a new dataset of recommendation-related conversations by scraping a popular discussion website. This is the largest public real-world conversational recommendation dataset to date. (2) Evaluation: On the new dataset and two existing conversational recommendation datasets, we observe that even without fine-tuning, large language models can outperform existing fine-tuned conversational recommendation models. (3) Analysis: We propose various probing tasks to investigate the mechanisms behind the remarkable performance of large language models in conversational recommendation. We analyze both the large language models' behaviors and the characteristics of the datasets, providing a holistic understanding of the models' effectiveness, limitations and suggesting directions for the design of future conversational recommenders. \n CCS CONCEPTS • Information systems → Personalization; • Computing methodologies → Natural language generation. \n",
  "references": [
    {
      "id": null,
      "title": "Large Language Models as Zero-Shot Conversational Recommenders",
      "authors": [
        "Zhankui He",
        "Zhouhang Xie",
        "Rahul Jha",
        "Harald Steck",
        "Dawen Liang",
        "Bodhisattwa Prasad Majumder",
        "Nathan Kallus",
        "Julian Mcauley"
      ],
      "year": "2023",
      "venue": "",
      "doi": "10.1145/3583780.3614949"
    },
    {
      "id": "b0",
      "title": "Dbpedia: A nucleus for a web of open data",
      "authors": [
        "Sören Auer",
        "Christian Bizer",
        "Georgi Kobilarov",
        "Jens Lehmann",
        "Richard Cyganiak",
        "Zachary Ives"
      ],
      "year": "2007",
      "venue": "The Semantic Web: 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007+ ASWC 2007",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
      "authors": [
        "Payal Bajaj",
        "Daniel Campos",
        "Nick Craswell",
        "Li Deng",
        "Jianfeng Gao",
        "Xiaodong Liu",
        "Rangan Majumder",
        "Andrew Mcnamara",
        "Bhaskar Mitra",
        "Tri Nguyen",
        "Mir Rosenberg",
        "Xia Song",
        "Alina Stoica",
        "Saurabh Tiwary",
        "Tong Wang"
      ],
      "year": "2018",
      "venue": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation",
      "authors": [
        "Keqin Bao",
        "Jizhi Zhang",
        "Yang Zhang",
        "Wenjie Wang",
        "Fuli Feng",
        "Xiangnan He"
      ],
      "year": "2023",
      "venue": "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell",
        "Sandhini Agarwal",
        "Ariel Herbert-Voss",
        "Gretchen Krueger",
        "Tom Henighan",
        "Rewon Child",
        "Aditya Ramesh",
        "Daniel Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Chris Hesse",
        "Mark Chen",
        "Eric Sigler",
        "Mateusz Litwin",
        "Scott Gray",
        "Benjamin Chess",
        "Jack Clark",
        "Christopher Berner",
        "Sam Mccandlish",
        "Alec Radford",
        "Ilya Sutskever",
        "Dario Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "authors": [
        "Sébastien Bubeck",
        "Varun Chandrasekaran",
        "Ronen Eldan",
        "Johannes Gehrke",
        "Eric Horvitz",
        "Ece Kamar",
        "Peter Lee",
        "Yin Tat Lee",
        "Yuanzhi Li",
        "Scott Lundberg"
      ],
      "year": "2023",
      "venue": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Autoregressive Entity Retrieval",
      "authors": [
        "Nicola De Cao",
        "Gautier Izacard",
        "Sebastian Riedel",
        "Fabio Petroni"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Bias and debias in recommender system: A survey and future directions",
      "authors": [
        "Jiawei Chen",
        "Hande Dong",
        "Xiang Wang",
        "Fuli Feng",
        "Meng Wang",
        "Xiangnan He"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Information Systems",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Critiquing-based recommenders: survey and emerging trends",
      "authors": [
        "Li Chen",
        "Pearl Pu"
      ],
      "year": "2012",
      "venue": "User Modeling and User-Adapted Interaction",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Towards Knowledge-Based Recommender Dialog System",
      "authors": [
        "Qibin Chen",
        "Junyang Lin",
        "Yichang Zhang",
        "Ming Ding",
        "Yukuo Cen",
        "Hongxia Yang",
        "Jie Tang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph E Gonzalez",
        "Ion Stoica",
        "Eric P Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann",
        "Parker Schuh",
        "Kensen Shi",
        "Sasha Tsvyashchenko",
        "Joshua Maynez",
        "Abhishek Rao",
        "Parker Barnes",
        "Yi Tay",
        "Noam M Shazeer",
        "Emily Vinodkumar Prabhakaran",
        "Nan Reif",
        "Benton C Du",
        "Reiner Hutchinson",
        "James Pope",
        "Jacob Bradbury",
        "Michael Austin",
        "Guy Isard",
        "Pengcheng Gur-Ari",
        "Toju Yin",
        "Anselm Duke",
        "Sanjay Levskaya",
        "Sunipa Ghemawat",
        "Henryk Dev",
        "Xavier Michalewski",
        "Vedant García",
        "Kevin Misra",
        "Liam Robinson",
        "Denny Fedus",
        "Daphne Zhou",
        "David Ippolito",
        "Hyeontaek Luan",
        "Barret Lim",
        "Alexander Zoph",
        "Ryan Spiridonov",
        "David Sepassi",
        "Shivani Dohan",
        "Mark Agrawal",
        "Andrew M Omernick",
        "Thanumalayan Dai",
        "Marie Sankaranarayana Pillai",
        "Aitor Pellat",
        "Erica Lewkowycz",
        "Rewon Moreira",
        "Oleksandr Child",
        "Katherine Polozov",
        "Zongwei Lee",
        "Xuezhi Zhou",
        "Brennan Wang",
        "Mark Saeta",
        "Orhan Díaz",
        "Michele Firat",
        "Jason Catasta",
        "Kathleen S Wei",
        "Douglas Meier-Hellstern",
        "Eck"
      ],
      "year": "2022",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Towards conversational recommender systems",
      "authors": [
        "Konstantina Christakopoulou",
        "Filip Radlinski",
        "Katja Hofmann"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems",
      "authors": [
        "Zeyu Cui",
        "Jianxin Ma",
        "Chang Zhou",
        "Jingren Zhou",
        "Hongxia Yang"
      ],
      "year": "2022",
      "venue": "M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Leveraging Large Language Models in Conversational Recommender Systems",
      "authors": [
        "Luke Friedman",
        "Sameer Ahuja",
        "David Allen",
        "Terry Tan",
        "Hakim Sidahmed",
        "Changbo Long",
        "Jun Xie",
        "Gabriel Schubiner",
        "Ajay Patel",
        "Harsh Lara"
      ],
      "year": "2023",
      "venue": "Leveraging Large Language Models in Conversational Recommender Systems",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "A survey on trustworthy recommender systems",
      "authors": [
        "Yingqiang Ge",
        "Shuchang Liu",
        "Zuohui Fu",
        "Juntao Tan",
        "Zelong Li",
        "Shuyuan Xu",
        "Yunqi Li",
        "Yikun Xian",
        "Yongfeng Zhang"
      ],
      "year": "2022",
      "venue": "A survey on trustworthy recommender systems",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Shortcut learning in deep neural networks",
      "authors": [
        "Robert Geirhos",
        "Jörn-Henrik Jacobsen",
        "Claudio Michaelis",
        "Richard S Zemel",
        "Wieland Brendel",
        "Matthias Bethge",
        "Felix Wichmann"
      ],
      "year": "2020",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)",
      "authors": [
        "Shijie Geng",
        "Shuchang Liu",
        "Zuohui Fu",
        "Yingqiang Ge",
        "Yongfeng Zhang"
      ],
      "year": "2022",
      "venue": "In RecSys '22: Sixteenth ACM Conference on Recommender Systems",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "The False Promise of Imitating Proprietary LLMs",
      "authors": [
        "Arnav Gudibande",
        "Eric Wallace",
        "Charlie Snell",
        "Xinyang Geng",
        "Hao Liu",
        "Pieter Abbeel",
        "Sergey Levine",
        "Dawn Song"
      ],
      "year": "2023",
      "venue": "The False Promise of Imitating Proprietary LLMs",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "",
      "authors": [
        "F",
        "Maxwell Harper",
        "Joseph A Konstan"
      ],
      "year": "2016",
      "venue": "The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "INSPIRED: Toward Sociable Recommendation Dialog Systems",
      "authors": [
        "Dongyeop Shirley Anugrah Hayati",
        "Qingxiaoyang Kang",
        "Weiyan Zhu",
        "Zhou Shi",
        "Yu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Adversarial personalized ranking for recommendation",
      "authors": [
        "Xiangnan He",
        "Zhankui He"
      ],
      "year": "2018",
      "venue": "The 41st International ACM SIGIR conference on research & development in information retrieval",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Neural collaborative filtering",
      "authors": [
        "Xiangnan He",
        "Lizi Liao",
        "Hanwang Zhang",
        "Liqiang Nie",
        "Xia Hu",
        "Tat-Seng Chua"
      ],
      "year": "2017",
      "venue": "Proceedings of the 26th international conference on world wide web",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Locker: Locally constrained self-attentive sequential recommendation",
      "authors": [
        "Zhankui He",
        "Handong Zhao",
        "Zhe Lin",
        "Zhaowen Wang",
        "Ajinkya Kale",
        "Julian Mcauley"
      ],
      "year": "2021",
      "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Bundle MCR: Towards Conversational Bundle Recommendation",
      "authors": [
        "Zhankui He",
        "Handong Zhao",
        "Tong Yu",
        "Sungchul Kim",
        "Fan Du",
        "Julian Mcauley"
      ],
      "year": "2022",
      "venue": "Proceedings of the 16th ACM Conference on Recommender Systems",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems",
      "authors": [
        "Yupeng Hou",
        "Junjie Zhang",
        "Zihan Lin",
        "Hongyu Lu",
        "Ruobing Xie",
        "Julian Mcauley",
        "Wayne Xin Zhao"
      ],
      "year": "2023",
      "venue": "Large Language Models are Zero-Shot Rankers for Recommender Systems",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data",
      "authors": [
        "Harsh Jhamtani",
        "Varun Gangal",
        "Eduard Hovy",
        "Graham Neubig",
        "Taylor Berg-Kirkpatrick"
      ],
      "year": "2018",
      "venue": "The 56th Annual Meeting of the Association for Computational Linguistics (ACL)",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Chatgpt: Optimizing language models for dialogue",
      "authors": [
        "C Kim",
        "Jacob Hilton",
        "Jacob Menick",
        "Jiayi Weng",
        "Juan Felipe",
        "Ceron Uribe",
        "Liam Fedus",
        "Luke Metz Michael",
        "Pokorny Rapha",
        "Gontijo Lopes",
        "Sengjia Zhao",
        "John Schulman",
        "Barret Zoph"
      ],
      "year": "2022",
      "venue": "OpenAI",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Fism: factored item similarity models for top-n recommender systems",
      "authors": [
        "Santosh Kabbur",
        "Xia Ning",
        "George Karypis"
      ],
      "year": "2013",
      "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue",
      "authors": [
        "Dongyeop Kang",
        "Anusha Balakrishnan",
        "Pararth Shah",
        "Paul A Crook",
        "Y-Lan Boureau",
        "Jason Weston"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Self-attentive sequential recommendation",
      "authors": [
        "Wang-Cheng Kang",
        "Julian Mcauley"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on data mining (ICDM)",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
      "authors": [
        "Wang-Cheng Kang",
        "Jianmo Ni",
        "Nikhil Mehta",
        "Maheswaran Sathiamoorthy",
        "Lichan Hong",
        "Ed Chi",
        "Derek Zhiyuan Cheng"
      ],
      "year": "2023",
      "venue": "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Scaling Laws for Neural Language Models",
      "authors": [
        "Jared Kaplan",
        "Sam Mccandlish",
        "T J Henighan",
        "Tom B Brown",
        "Benjamin Chess",
        "Rewon Child",
        "Scott Gray",
        "Alec Radford",
        "Jeff Wu",
        "Dario Amodei"
      ],
      "year": "2020",
      "venue": "Scaling Laws for Neural Language Models",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Matrix factorization techniques for recommender systems",
      "authors": [
        "Yehuda Koren",
        "Robert Bell",
        "Chris Volinsky"
      ],
      "year": "2009",
      "venue": "Computer",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Estimation-action-reflection: Towards deep interaction between conversational and recommender systems",
      "authors": [
        "Wenqiang Lei",
        "Xiangnan He",
        "Yisong Miao",
        "Qingyun Wu",
        "Richang Hong",
        "Min-Yen Kan",
        "Tat-Seng Chua"
      ],
      "year": "2020",
      "venue": "Proceedings of the 13th International Conference on Web Search and Data Mining",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Interactive path reasoning on graph for conversational recommendation",
      "authors": [
        "Wenqiang Lei",
        "Gangyi Zhang",
        "Xiangnan He",
        "Yisong Miao",
        "Xiang Wang",
        "Liang Chen",
        "Tat-Seng Chua"
      ],
      "year": "2020",
      "venue": "Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation",
      "authors": [
        "Jinming Li",
        "Wentao Zhang",
        "Tian Wang",
        "Guanglei Xiong",
        "Alan Lu",
        "Gerard Medioni"
      ],
      "year": "2023",
      "venue": "GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "A next basket recommendation reality check",
      "authors": [
        "Ming Li",
        "Sami Jullien",
        "Mozhdeh Ariannezhad",
        "Maarten De Rijke"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Information Systems",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Towards deep conversational recommendations",
      "authors": [
        "Raymond Li",
        "Samira Ebrahimi Kahou",
        "Hannes Schulz",
        "Vincent Michalski",
        "Laurent Charlin",
        "Chris Pal"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Self-Supervised Bot Play for Conversational Recommendation with Justifications",
      "authors": [
        "Shuyang Li",
        "Bodhisattwa Prasad Majumder",
        "Julian Mcauley"
      ],
      "year": "2021",
      "venue": "Self-Supervised Bot Play for Conversational Recommendation with Justifications",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Xiang Ao, Fuzhen Zhuang, and Qing He. 2022. User-centric conversational recommendation with multi-aspect user modeling",
      "authors": [
        "Shuokai Li",
        "Ruobing Xie",
        "Yongchun Zhu"
      ],
      "year": "",
      "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Competition-level code generation with AlphaCode",
      "authors": [
        "Yujia Li",
        "David H Choi",
        "Junyoung Chung",
        "Nate Kushman",
        "Julian Schrittwieser",
        "Rémi Leblond",
        "Tom",
        "James Eccles",
        "Felix Keeling",
        "Agustin Dal Gimeno",
        "Thomas Lago",
        "Peter Hubert",
        "Cyprien Choy",
        "De",
        "Igor Masson D'autume",
        "Xinyun Babuschkin",
        "Po-Sen Chen",
        "Johannes Huang",
        "Sven Welbl",
        "Gowal",
        "Alexey",
        "James Cherepanov",
        "Daniel Jaymin Molloy",
        "Esme Mankowitz",
        "Sutherland Robson"
      ],
      "year": "2022",
      "venue": "Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Variational autoencoders for collaborative filtering",
      "authors": [
        "Dawen Liang",
        "G Rahul",
        "Matthew D Krishnan",
        "Tony Hoffman",
        "Jebara"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 world wide web conference",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "ConceptNet-a practical commonsense reasoning tool-kit",
      "authors": [
        "Hugo Liu",
        "Push Singh"
      ],
      "year": "2004",
      "venue": "BT technology journal",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Is ChatGPT a Good Recommender? A Preliminary Study",
      "authors": [
        "Junling Liu",
        "Chao Liu",
        "Renjie Lv",
        "Kang Zhou",
        "Yan Zhang"
      ],
      "year": "2023",
      "venue": "Is ChatGPT a Good Recommender? A Preliminary Study",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "RevCore: Review-Augmented Conversational Recommendation",
      "authors": [
        "Yu Lu",
        "Junwei Bao",
        "Yan Song",
        "Zichen Ma",
        "Shuguang Cui",
        "Youzheng Wu",
        "Xiaodong He"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation",
      "authors": [
        "Wenchang Ma",
        "Ryuichi Takanobu",
        "Minlie Huang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll L Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul F Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "year": "2022",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "What does bert know about books, movies and music? probing bert for conversational recommendation",
      "authors": [
        "Gustavo Penha",
        "Claudia Hauff"
      ],
      "year": "2020",
      "venue": "Proceedings of the 14th ACM Conference on Recommender Systems",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Variational Reasoning about User Preferences for Conversational Recommendation",
      "authors": [
        "Zhaochun Ren",
        "Zhi Tian",
        "Dongdong Li",
        "Pengjie Ren",
        "Liu Yang",
        "Xin Xin",
        "Huasheng Liang",
        "Maarten De Rijke",
        "Zhumin Chen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Factorization machines",
      "authors": [
        "Steffen Rendle"
      ],
      "year": "2010",
      "venue": "2010 IEEE International conference on data mining",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "LaMP: When Large Language Models Meet Personalization",
      "authors": [
        "Alireza Salemi",
        "Sheshera Mysore",
        "Michael Bendersky",
        "Hamed Zamani"
      ],
      "year": "2023",
      "venue": "LaMP: When Large Language Models Meet Personalization",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
      "authors": [
        "Victor Sanh",
        "Albert Webson",
        "Colin Raffel",
        "Stephen H Bach",
        "Lintang Sutawika",
        "Zaid Alyafeai",
        "Antoine Chaffin",
        "Arnaud Stiegler",
        "Arun Raja",
        "Manan Dey",
        "M Saiful Bari",
        "Canwen Xu",
        "Urmish Thakker",
        "Shanya Sharma Sharma",
        "Eliza Szczechla",
        "Taewoon Kim",
        "Gunjan Chhablani",
        "V Nihal",
        "Debajyoti Nayak",
        "Jonathan Datta",
        "Mike Chang",
        "Tian-Jian",
        "Han Jiang",
        "Matteo Wang",
        "Sheng Manica",
        "Zheng Xin Shen",
        "Harshit Yong",
        "Rachel Pandey",
        "Thomas Bawden",
        "Trishala Wang",
        "Jos Neeraj",
        "Abheesht Rozen",
        "Andrea Sharma",
        "Thibault Santilli",
        "Jason Févry",
        "Alan Fries",
        "Ryan Teehan",
        "Le Teven",
        "Stella Scao",
        "Leo Biderman",
        "Thomas Gao",
        "Alexander M Wolf",
        "Rush"
      ],
      "year": "2022",
      "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Autorec: Autoencoders meet collaborative filtering",
      "authors": [
        "Suvash Sedhain",
        "Aditya Krishna Menon",
        "Scott Sanner",
        "Lexing Xie"
      ],
      "year": "2015",
      "venue": "Proceedings of the 24th international conference on World Wide Web",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Rec: Sequential recommendation with bidirectional encoder representations from transformer",
      "authors": [
        "Fei Sun",
        "Jun Liu",
        "Jian Wu",
        "Changhua Pei",
        "Xiao Lin",
        "Wenwu Ou",
        "Peng Jiang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th ACM international conference on information and knowledge management",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Transformer memory as a differentiable search index",
      "authors": [
        "Yi Tay",
        "Vinh Tran",
        "Mostafa Dehghani",
        "Jianmo Ni",
        "Dara Bahri",
        "Harsh Mehta",
        "Zhen Qin",
        "Kai Hui",
        "Zhe Zhao",
        "Jai Gupta"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "year": "2023",
      "venue": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Xiangnan He, and Tat-Seng Chua",
      "authors": [
        "Wenjie Wang",
        "Xinyu Lin",
        "Fuli Feng"
      ],
      "year": "2023",
      "venue": "Generative Recommendation: Towards Next-generation Recommender Paradigm",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models",
      "authors": [
        "Xiaolei Wang",
        "Xinyu Tang",
        "Wayne Xin Zhao",
        "Jingyuan Wang",
        "Ji-Rong Wen"
      ],
      "year": "2023",
      "venue": "Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning",
      "authors": [
        "Xiaolei Wang",
        "Kun Zhou",
        "Ji-Rong Wen",
        "Wayne Xin Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "Denny Quoc V Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "V Quoc",
        "Denny Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "Deep language-based critiquing for recommender systems",
      "authors": [
        "Ga Wu",
        "Kai Luo",
        "Scott Sanner",
        "Harold Soh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th ACM Conference on Recommender Systems",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "Baize: An opensource chat model with parameter-efficient tuning on self-chat data",
      "authors": [
        "Canwen Xu",
        "Daya Guo",
        "Nan Duan",
        "Julian Mcauley"
      ],
      "year": "2023",
      "venue": "Baize: An opensource chat model with parameter-efficient tuning on self-chat data",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "DIALOGPT: Large-Scale Generative Pre-training for Conversational Response Generation",
      "authors": [
        "Yizhe Zhang",
        "Siqi Sun",
        "Michel Galley",
        "Yen-Chun Chen",
        "Chris Brockett",
        "Xiang Gao",
        "Jianfeng Gao",
        "Jingjing Liu",
        "William B Dolan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "Multiple Choice Questions based Multi-Interest Policy Learning for Conversational Recommendation",
      "authors": [
        "Yiming Zhang",
        "Lingfei Wu",
        "Qi Shen",
        "Yitong Pang",
        "Zhihua Wei",
        "Fangli Xu",
        "Bo Long",
        "Jian Pei"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM Web Conference 2022",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "A survey of large language models",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X",
      "authors": [
        "Qinkai Zheng",
        "Xiao Xia",
        "Xu Zou",
        "Yuxiao Dong",
        "Shan Wang",
        "Yufei Xue",
        "Zihan Wang",
        "Lei Shen",
        "Andi Wang",
        "Yang Li",
        "Teng Su",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2023",
      "venue": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization",
      "authors": [
        "Kun Zhou",
        "Hui Wang",
        "Wayne Xin Zhao",
        "Yutao Zhu",
        "Sirui Wang",
        "Fuzheng Zhang",
        "Zhongyuan Wang",
        "Ji-Rong Wen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 29th ACM international conference on information & knowledge management",
      "doi": ""
    },
    {
      "id": "b73",
      "title": "Improving conversational recommender systems via knowledge graph based semantic fusion",
      "authors": [
        "Kun Zhou",
        "Wayne Xin Zhao",
        "Shuqing Bian",
        "Yuanhang Zhou",
        "Ji-Rong Wen",
        "Jingsong Yu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "Towards Topic-Guided Conversational Recommender System",
      "authors": [
        "Kun Zhou",
        "Yuanhang Zhou",
        "Wayne Xin Zhao",
        "Xiaoke Wang",
        "Ji-Rong Wen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "Improving conversational recommender systems via transformerbased sequential modelling",
      "authors": [
        "Jie Zou",
        "Evangelos Kanoulas",
        "Pengjie Ren",
        "Zhaochun Ren",
        "Aixin Sun",
        "Cheng Long"
      ],
      "year": "2022",
      "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "2. Llms As Zero-Shot Crs",
      "text": ""
    },
    {
      "title": "Task Formation",
      "text": "Given a user set \\(\\mathcal{U}\\), an item set \\(\\mathcal{I}\\) and a vocabulary \\(\\mathcal{V}\\), a conversation can be denoted as \\(C=(u_{t},s_{t},\\mathcal{I}_{t})_{t=1}^{T}\\). That means during the \\(t^{\\text{th}}\\) turn of the conversation, a speaker \\(u_{t}\\in\\mathcal{U}\\) generates an utterance \\(s_{t}=(w_{t})_{t=1}^{m}\\), which is a sequence of words \\(w_{i}\\in\\mathcal{V}\\). This utterance \\(s_{t}\\) also contains a set of mentioned items \\(\\mathcal{I}_{t}\\subset\\mathcal{I}\\) (\\(\\mathcal{I}_{t}\\) can be an empty set if no items mentioned). Typically, there are two users in the conversation \\(C\\) playing the role of _seeker_ and _recommender_ respectively. Let us use the \\(2^{\\text{nd}}\\) conversation turn in Figure 1 as an example. Here \\(t=2\\), \\(u_{t}\\) is [System], \\(s_{t}\\) is \"You would love Terminator!\" and \\(\\mathcal{I}_{2}\\) is a set containing the movie Terminator. Figure 1. Large Language Models (LLMs) as Zero-Shot Conversational Recommenders (CRS).We introduce a simple prompting strategy to define the task description \\(T\\), format requirement \\(F\\) and conversation context \\(S\\) for a LLM, denoted as \\(\\mathcal{F}\\), we then post-process the generative results into ranked item lists with processor \\(\\Phi\\). Following many CRS papers (Hung et al., 2018; Zhang et al., 2019; Zhang et al., 2019; Zhang et al., 2019), the _recommender_ component of a CRS is specifically designed to optimize the following objective: during the \\(k^{\\text{th}}\\) turn of a conversation, where \\(u_{k}\\) is the _recommender_, the recommender takes the conversational context \\((u_{t},s_{t},I_{t})_{t=1}^{k-1}\\) as its input, and generate a ranked list of items \\(\\tilde{I}_{k}\\) that best matches the ground-truth items in \\(\\tilde{I}_{k}\\)."
    },
    {
      "title": "Framework",
      "text": "**Prompting.** Our goal is to utilize LLMs as zero-shot conversational recommenders. Specifically, without the need for fine-tuning, we intend to prompt an LLM, denoted as \\(\\mathcal{F}\\), using a task description template \\(T\\), format requirement \\(F\\), and conversational context \\(S\\) before the \\(k^{\\text{th}}\\) turn. This process can be formally represented as: \\[\\hat{I}_{k}=\\Phi\\left(\\mathcal{F}(T,F,S)\\right). \\tag{1}\\] To better understand this zero-shot recommender, we present an example in Figure 1 with the prompt setup in our experiments.3 Footnote 3: We leave more prompting techniques such as CoT (Zhou et al., 2019) in future work. **Models.** We consider several popular LLMs \\(\\mathcal{F}\\) that exhibit zero-shot prompting abilities in two groups. To try to ensure deterministic results, we set the decoding temperature to 0 for all models. * **GPT-3.5-turbo**(Zhou et al., 2019)4 and **GPT-4**(Zhou et al., 2019) from OPENAI with abilities of solving many complex tasks in zero-shot setting (Zhou et al., 2019; Zhang et al., 2019) but are closed-sourced. Footnote 4: We use **BAIZE**(Zhou et al., 2019)5 and **Vicuna**(Zhou et al., 2019), which are representative open-sourced LLMs fine-tuned based on LLAMA-13B (Zhou et al., 2019). Footnote 5: We use **BAIZE-V2** in [https://huggingface.co/project-baize/baire-v2-13b](https://huggingface.co/project-baize/baire-v2-13b) **Processing.** We do not assess model weights or output logits from LLMs. Therefore, we apply a post-processor \\(\\Phi\\) (e.g., fuzzy matching) to convert a recommendation list in natural language to a ranked list \\(\\hat{I}_{k}\\). The approach of generating item titles instead of ranking item IDs is referred to as a _generative retrieval_(Zhou et al., 2019; Zhang et al., 2019) paradigm."
    },
    {
      "title": "3. Dataset",
      "text": "Ideally, a large-scale dataset with diverse interactions and real-world conversations is needed to evaluate models' ability in conversational recommendation. Existing conversational recommendation datasets are usually crowd-sourced (Zhou et al., 2019; Zhang et al., 2019; Zhang et al., 2019; Zhang et al., 2019) and thus only partially capture realistic conversation dynamics. For example, a crowd worker responded with \"whatever Whatever I'm open to any suggestion.\" when asked about movie preferences in ReDIAL; this happens since crowd workers often do not have a particular preference at the time of completing a task. In contrast, a real user could have a very particular need, as shown in Figure 2. To complement crowd-sourced CRS datasets, we present the _Reddit-Movie_ dataset, the largest-scale conversational movie recommendation dataset to date, with naturally occurring movie recommendation conversations that can be used along with existing crowd-sourced datasets to provide richer perspectives for training and evaluating CRS models. In this work, we conduct our model evaluation and analysis on two commonly used crowd-sourcing datasets: ReDIAL (Zhou et al., 2019) and INSPIRED (Zhou et al., 2019), as well as our newly collected Reddit dataset. We show qualitative examples from the Reddit dataset as in Figure 2 and quantitative analysis in Section 5.2. **Dataset Construction** To construct a CRS dataset from Reddit, we process all Reddit posts from 2012 Jan to 2022 Dec from _pushshift_.io6. We consider movie recommendation scenarios7 and extract related posts from five related subreddits: _r/movies, r/bestoftetflix, r/moviesuggents, r/netflixbestf_ and _r/trueflim_. We process the raw data with the pipeline of _consversational recommendation identification, movie mention recognition_ and _movie entity linking8_. In our following evaluation, we use the most recent 9k conversations in _Reddit-Movie\\({}^{\\text{base}}\\)_ from December 2022 as the testing set since these samples occur _after_ GPT-3.5-t's release. Meanwhile, GPT-4 (Zhou et al., 2019) also mentioned its pre-training data cut off in Sept. 20219. For other compared models, we use the remaining 76k conversations in _Reddit-Movie\\({}^{\\text{base}}\\)_ dataset for training and validation. \\begin{table} \\begin{tabular}{l r r r r} \\hline \\hline **Dataset** & **\\#Conv.** & **\\#Turns** & **\\#Users** & **\\#Items** \\\\ \\hline **INSPIRED**(Zhou et al., 2019) & 999 & 35,686 & 999 & 1,967 \\\\ **ReDIAL**(Zhou et al., 2019) & 11,348 & 139,557 & 764 & 6,281 \\\\ **_Reddit-Movie\\({}^{\\text{base}}\\)_** & 85,052 & 133,005 & 10,946 & 24,326 \\\\ **_Reddit-Movie\\({}^{\\text{large}}\\)_** & 634,392 & 1,669,720 & 36,247 & 51,203 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1. Dataset Statistics. We denote a subset of _Reddit-Movie_ in 2022 as base, and the entire ten-year dataset as large. Figure 2. Typical model inputs from a traditional recommendation dataset (MovieLens (Zhou et al., 2019)), an existing CRS dataset (**RCDIAL**(Zhou et al., 2019)), and our _Reddit-Movie_ dataset. The _Reddit-Movie_ dataset contains more information in its textual content compared to existing datasets where users often explicitly specify their preference. See Section 5.2 for quantitative analysis. **Discussion.** From the statistics in Table 1, we observe: (1) The dataset _Reddit-Movie_ stands out as the largest conversational recommendation dataset, encompassing 634,392 conversations and covering 51,203 movies. (2) In comparison to ReDIAL (Zhu et al., 2017) and INSPIRED (Zhu et al., 2018), _Reddit-Movie_ contains fewer multi-turn conversations, mainly due to the inherent characteristics of Reddit posts. (3) By examining representative examples depicted in Figure 2, we find that _Reddit-Movie_ conversations tend to include more complex and detailed user preference in contrast to ReDIAL, as they originate from real-world conversations on Reddit, enriching the conversational recommendation datasets with a diverse range of discussions."
    },
    {
      "title": "4. Evaluation",
      "text": "In this section, we evaluate the proposed LLMs-based framework on ReDIAL (Zhu et al., 2017), INSPIRED (Zhu et al., 2018) and our Reddit datasets. We first explain the evaluation setup and a _repeated item shortcut_ of the previous evaluation in Sections 4.1 and 4.2. Then, we re-train models and discuss LLM performance in Section 4.3."
    },
    {
      "title": "Evaluation Setup",
      "text": "**Repeated _vs._ New Items.** Given a conversation \\(C=(u_{I},s_{I},I_{I})^{T}_{I=1}\\), it is challenging to identify the ground-truth recommended items, i.e., whether the mentioned items \\(I_{k}\\) at the \\(k^{\\text{th}}(k\\leq T)\\) turn are used for recommendation purposes. A common evaluation setup assumes that when \\(u_{k}\\) is the _recommender_, all items \\(i\\in I_{k}\\) serve as ground-truth recommended items. In this work, we further split the items \\(i\\in I_{k}\\) into two categories: _repeated items_ or _new items_. Repeated items are items that have appeared in previous conversation turns, i.e., \\(\\{i\\mid\\exists t\\in[1,k),i\\in I_{I}\\}\\); and new items are items not mentioned in previous conversation turns. We explain the details of this categorization in Section 4.2. **Evaluation Protocol.** On those three datasets, we evaluate several representative CRS models and several LLMs on their recommendation abilities. For baselines, after re-running the training code provided by the authors, we report the prediction performance using Recall@K (Kohn et al., 2017; Zhu et al., 2017; Zhu et al., 2017) (i.e., HIT@K). We consider the means and the standard errors10 of the metric with \\(K=\\{1,5\\}\\). Footnote 10: We show standard errors as error bars in our figures and gray numbers in our tables. **Compared CRS Models.** We consider several representative CRS models. For baselines which rely on structured knowledge, we use the entity linking results of ReDIAL and INSPIRED datasets provided by UniCRS (Zhu et al., 2017). Note that we do not include more works (Zhu et al., 2017; Zhu et al., 2017; Zhu et al., 2017) because UniCRS (Zhu et al., 2017) is representative with similar results. * **ReDIAL (Zhu et al., 2017)**: This model is released along with the ReDIAL dataset with an auto-encoder (Zhu et al., 2017)-based recommender. * **KBRD (Kohn et al., 2017)**: This model proposes to use the DBPedia (Beng et al., 2016) to enhance the semantic knowledge of items or entities. * **KCSF (Zhu et al., 2017)**: This model incorporates two knowledge graphs to enhance the representations of words and entities, and uses the Mutual Information Maximization method to align the semantic spaces of those two knowledge graphs. * **UniCRS (Zhu et al., 2017)**: This model uses pre-trained language model, DialoGPT (Zhu et al., 2017), with prompt tuning to conduct recommendation and conversation generation tasks respectively."
    },
    {
      "title": "Repeated Items Can Be Shortcuts",
      "text": "Current evaluation for conversational recommendation systems does not differentiate between repeated and new items in a conversation. We observed that this evaluation scheme favors systems that optimize for mentioning repeated items. As shown in Figure 3, a trivial baseline that always copies seen items from the conversation history has better performance than most previous models under the standard evaluation scheme. This phenomenon highlights the risk of shortcut learning (Zhu et al., 2017), where a decision rule performs well against certain benchmarks and evaluations but fails to capture the true intent of the system designer. Indeed, the #HIT@1 for the models tested dropped by more than 60% on average when we focus on new item recommendation only, which is unclear from the overall recommendation performance. After manually checking, we observe a typical pattern of repeated items, which is shown in the example conversation in Figure 1. In this conversation, Terminator at the \\(6^{\\text{th}}\\) turn is used as the ground-truth item. The system repeated this Terminator because the system quoted this movie for a content-based discussion during the conversation rather than making recommendations. Given the nature of recommendation conversations between two users, it is more probable that items repeated during a conversation are intended for discussion rather Figure 3. To show the _repeated item shortcut_, we count CRS recommendation hits using the Top-K ranked list \\(K=\\{1,5\\}\\). We group the ground-truth hits by repeated items (shaded bars) and new items (not shaded bars). The trivial baseline copies existing items from the current conversation history in chronological order, from the most recent and does not recommend new items. than serving as recommendations. We argue that considering the large portion of repeated items (e.g., more than 15% ground-truth items are repeated items in INSPIRED), it is beneficial to remove repeated items and re-evaluate CRS models to better understand models' recommendation ability. It is worth noting that the repetition patterns have also been investigated in evaluating other recommender systems such as _next-basket_ recommendation (Wang et al., 2018)."
    },
    {
      "title": "Llms Performance",
      "text": "**Finding 1 - LLMs outperform fine-tuned CRS models in a zero-shot setting.** For a comparison between models' abilities to recommend new items to the user in conversation, we re-train existing CRS models on all datasets for new item recommendation only. The evaluation results are as shown in Figure 4. Large language models, although not fine-tuned, have the best performance on all datasets. Meanwhile, the performance of all models is uniformly lower on Reddit compared to the other datasets, potentially due to the large number of items and fewer conversation turns, making recommendation more challenging. **Finding 2 - GPT-based models achieve superior performance than open-sourced LLMs.** As shown in Figure 4, large language models consistently outperform other models across all three datasets, while GPT-4 is generally better than GPT-3.5-t. We hypothesize this is due to GPT-4's larger parameter size enables it to retain more correlation information between movie names and user preferences that naturally occurs in the language models' pre-training data. Vicuna and BAIZE, while having comparable performance to prior models on most datasets, have significantly lower performance than its teacher, GPT-3.5-t. This is consistent with previous works' finding that smaller distilled models via imitation learning cannot fully inherit larger models ability on downstream tasks (Kang et al., 2019). **Finding 3 - LLMs may generate out-of-dataset item titles, but few hallucinated recommendations.** We note that language models trained on open-domain data naturally produce items out of the allowed item set during generation. In practice, removing these items improves the models' recommendation performance. Large language models outperform other models (with GPT-4 being the best) consistently regardless of whether these unknown items are removed or not, as shown in Table 2. Meanwhile, Table 3 shows that around 95% generated recommendations from GPT-based models (around 81% from BAIZE and 87% from Vicuna) can be found in IMDB 11 by string matching. Those lower bounds of these matching rates indicate that there are only a few hallucinated item titles in the LLM recommendations in the movie domain. Footnote 11: Movie titles in [https://datasets.imdbws.com/](https://datasets.imdbws.com/)."
    },
    {
      "title": "5. Detailed Analysis",
      "text": "Observing LLMs' remarkable conversational recommendation performance for zero-shot recommendation, we are interested in _what accounts for their effectiveness_ and _what their limitations are_. We aim to answer these questions from both a model and data perspective."
    },
    {
      "title": "Knowledge In Llms",
      "text": "**Experiment Setup.** Motivated by the probing work of (Zhu et al., 2017), we posit that two types of knowledge in LLMs can be used in CRS: * **Collaborative knowledge**, which requires the model to match items with similar ones, according to community interactions like \"users who like A typically also like B\". In \\begin{table} \\begin{tabular}{l c c c c c c} \\hline \\hline & \\multicolumn{2}{c}{**INSPIRED**} & \\multicolumn{2}{c}{**ReDIAL**} & \\multicolumn{2}{c}{**Reddit**} \\\\ \\cline{2-7} **Model** & \\(\\Phi_{0}\\) & \\(\\Phi_{1}\\) & \\(\\Phi_{0}\\) & \\(\\Phi_{1}\\) & \\(\\Phi_{0}\\) & \\(\\Phi_{1}\\) \\\\ \\hline **BAIZE** & 0.019 0.019 & **.028** 0.011 & **.021** 0.002 & **.021** 0.002 & **.012** 0.001 & **.013** 008 \\\\ **Vicuna** &.028 0.011 & **.033** 0.012 & **.020** 0.002 & **.020** 0.002 & **.012** 0.001 & **.012** 0.001 \\\\ **GPT-3.5-t** &.047 0.015 & **.052** 0.015 & **.041** 0.003 & **.043** 0.003 & **.022** 0.001 & **.023** 0.001 \\\\ **GPT-4** &.062 0.017 & **.066** 0.017 & **.043** 0.003 & **.046** 0.001 & **.022** 0.001 & **.023** 0.001 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2. Recall@1 results of considering all generated item titles (\\(\\Phi_{0}\\)) and only considering in-dataset item titles (\\(\\Phi_{1}\\)). Figure 4. CRS recommendation performance on New Items in terms of Recall@K, with \\(K=\\{1,5\\}\\). To exclude the influence of repeated items in CRS evaluation, we remove all repeated items in training and testing datasets and re-train all baselines. \\begin{table} \\begin{tabular}{c c c c c c c} \\hline \\hline \\multicolumn{2}{c}{**BAIZE**} & \\multicolumn{2}{c}{**Vicuna**} & \\multicolumn{2}{c}{**GPT-3.5-t**} & \\multicolumn{2}{c}{**GPT-4**} \\\\ \\hline \\#rec & \\%imdb & \\#rec & \\%imdb & \\#rec & \\%imdb & \\#rec & \\%imdb \\\\ 259,333 & 81.56\\% & 258,984 & 86.98\\% & 321,048 & 95.51\\% & 322,323 & 94.86\\% \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3. Fraction of Top-K (\\(K=20\\) in our prompt setup) recommendations (#rec) that can be string matched in the IMDB movie database (%imdb) for the different models, which shows a lower bound of non-hallucinated movie titles. [MISSING_PAGE_FAIL:6] denoted as \\(\\Phi_{2}\\) (describe in the caption of Figure 5). By employing \\(\\Phi_{2}\\), the performance gaps between _Original_ and _ItemRemoved_ (or _ItemRandom_) are further reduced. Furthermore, Figure 6 demonstrates the consistent and close performance gap between _Original_ and _ItemRemoved_ (or _ItemRandom_) across different testing samples, which vary in size and the number of item mentions in _Original_. These results suggest that given a conversation context, LLMs primarily rely on content/context knowledge rather than collaborative knowledge to make recommendations. This behavior interestingly diverges from many traditional recommenders like collaborative filtering [23, 24, 36, 46, 55, 58] or sequential recommenders [25, 33, 59, 73], where user-interacted items are essential. **Finding 5 - GPT-based LLMs possess better content/context knowledge than existing CRS.** From Table 4, we observe the superior recommendation performance of GPT-based LLMs against representative conversational recommendation or text-only models on all datasets, showing the remarkable zero-shot abilities in understanding user preference with the textual inputs and generating correct item titles. We conclude that GPT-based LLMs can provide more accurate recommendations than existing trained CRS models in an _ItemRemoved_ (\\(S_{2}\\)) setting, demonstrating better content/context knowledge. **Finding 6 - LLMs generally possess weaker collaborative knowledge than existing CRS.** In Table 5, the results from INSPIRED and ReDIAL indicate that LLMs underperform existing representative CRS or ItemCF models by 30% when using only the item-based conversation context _ItemOnly_ (\\(S_{1}\\)). It indicates that LLMs, trained on a general corpus, typically lack the collaborative knowledge exhibited by representative models trained on the target dataset. There are several possible reasons for this weak collaborative knowledge in LLMs. First, the training corpus may not contain sufficient information for LLMs to learn the underlying item similarities. Second, although LLMs may possess some collaborative knowledge, they might not align with the interactions in the target datasets, possibly because the underlying item similarities can be highly dataset- or platform-dependent. However, in the case of the Reddit dataset, LLMs outperform baselines in both Recall@1 and Recall@5, as shown in Table 5. This outcome could be attributed to the dataset's large number of rarely interacted items, resulting in limited collaborative information. The Reddit dataset contains 12,982 items with no more than 3 mentions as responses. This poses a challenge in correctly ranking these items within the Top-5 or even Top-1 positions. LLMs, which possess at least some understanding of the semantics in item titles, have the chance to outperform baselines trained on datasets containing a large number of cold-start items. Recent research on LLMs in traditional recommendation systems [27, 34, 48] also observes the challenge of effectively leveraging collaborative information without knowing the target interaction data distribution. Additionally, another study [3] on traditional recommendation systems suggests that LLMs are beneficial in a setting with many cold-start items. Our experimental results support these findings within the context of conversational recommendations."
    },
    {
      "title": "Information From Crs Data",
      "text": "**Experimental Setup for Finding 7**. To understand LLMs in CRS tasks from the data perspective, we first measure the _content/context information_ in CRS datasets. Content/context information refers to the amount of information contained in conversations, excluding the item titles, which reasonably challenges existing CRS and favors LLMs according to the findings in Section 5.1. Specifically, we conduct an entropy-based evaluation for each CRS dataset and compare the conversational datasets with several popular conversation and question-answering datasets, namely DailyDialog (chit chat) [45], MsMarco (conversational search) [2], and HotpotQA (question answering). We use _ItemRemoved_ (\\(S_{2}\\)) conversation texts like Section 5.1, and adopt the geometric mean of the entropy distribution of 1,2,3-grams as a surrogate for the amount of information contained in the datasets, following previous work on evaluating information content in text [29]. However, entropy naturally grows with the size of a corpus, and each CRS dataset has a different distribution of words per sentence, sentences per dialog, and corpus size. Thus, it would be unfair to compare entropy between corpus on a Figure 7. The left subfigure shows the entropy of the frequency distribution of 1,2,3-grams with respect to number of words drawn from each dataset (item names excluded) to measure the content/context information across datasets. The right subfigure shows the results of processed Reddit collaborative dataset aligned to ML-25M [21]. RAND denotes random baseline, FT denotes fine tuning on Reddit, PT denotes pre-training on ML-25M, PT+FT means FT after PT. \\begin{table} \\begin{tabular}{l c c c c c c} \\hline \\hline & \\multicolumn{2}{c}{**INSPIRED**} & \\multicolumn{2}{c}{**ReDIAL**} & \\multicolumn{2}{c}{**Reddit**} \\\\ \\cline{2-7} **Model** & R@P1 & R@S5 & R@1 & R@S5 & R@1 & R@S5 \\\\ \\hline **Vicuna** &.005.005 &.024.010 &.011.002 &.039.003 &.005.000 &.015.001 \\\\ **GPT-3.5-t** &.024.010 &.052.015 &.021.002 &.063.004 &.007.001 &.026.001 \\\\ **GPT-4** &.014.008 &.052.015 &.025.002 &.069.004 & **.007** &.001 & **.028**.001 \\\\ \\hline **CRS\\({}^{*}\\)** &.038.013 &.085.019 &.025.002 &.072.004 &.003.000 &.015.001 \\\\ **ItemCF\\({}^{*}\\)** & **.042** &.012 & **.087** &.016 & **.029** &.003 & **.088**.004 &.004.001 &.018.001 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5. To understand the collaborative knowledge in LLMs and existing CRS models, we re-train the existing CRS models using the same perturbed conversation context _ItemOnly_ (\\(S_{1}\\)). We include the results of the representative CRS model Uni-CRS (denoted as CRS’) as well as a representative item-based collaborative model FISM [31] (denoted as ItemCF\\({}^{*}\\)). per-dialog, per-turn, or per-dataset basis. To ensure a fair comparison, we repeatedly draw increasingly large subsets of texts from each of the datasets, compute the entropy of these subsets, and report the trend of entropy growth with respect to the size of the subsampled text for each CRS dataset. **Finding 7** - **Reddit provides more content/context information than the other two CRS datasets.** Based on the results in Figure 6(a), we observe that the Reddit dataset has the most content/context information among the three conversational recommendation datasets. Those observations are also aligned with the results in Figure 5 and table 4, where LLMs - which possess better content/context knowledge than baselines - can achieve higher relative improvements compared to the other two datasets. Meanwhile, the content/context information in Reddit is close to question answering and conversational search, which is higher than existing conversational recommendation and chit-chat datasets. **Finding 8** - **Collaborative information is insufficient for satisfactory recommendations, given the current models.** Quantifying the collaborative information in datasets is challenging. Instead of proposing methods to measure collaborative information, we aim to make new observations based on general performance results presented in Figure 4 and recommendation results using only collaborative information in Table 5. Comparing the performance of the best models in Table 5 under an _ItemOnly_ (\\(S_{1}\\)) setting with the performance of the best models in Figure 4 under an _Original_ (\\(S_{0}\\)) setting reveals a significant disparity. For instance, on ReDIAL, the Recall@1 performance is 0.029 for ItemCF* compared to 0.046 for GPT-4, representing a 39.96% decrease. Similarly, for Reddit, the Recall@1 performance is 0.007 compared to 0.023 for GPT-4 both, which is 69.57% lower. We also experimented with other recommender systems, such as transformer-based models (Shi et al., 2019; Wang et al., 2020) to encode the item-only inputs and found similar results. Based on the current performance gap, we find that using the existing models, relying solely on collaborative information, is insufficient to provide satisfactory recommendations. We speculate that either (1) more advanced models or training methods are required to better comprehend the collaborative information in CRS datasets, or (2) the collaborative information in CRS datasets is too limited to support satisfactory recommendations. **Experimental Setup for Finding 9.** To understand whether the collaborative information from CRS datasets are aligned with pure interaction datasets, we conduct an experiment on the Reddit dataset. In this experiment, we first process the dataset to link the items to a popular interaction dataset ML-25M (Krishnan et al., 2019)12. We then experiment with two representative encoders for item-based collaborative filtering based on FISM (Krishnan et al., 2019) and Transformer (Wang et al., 2020) (TRM), respectively. We report the testing results on Reddit, with fine-tuning on Reddit (FT), pre-training on ML-25M (PT), and pre-training on ML-25M then fine-tuning Reddit (PT+FT). Note that since it is a linked dataset with additional processing, the results are not comparable with beforementioned results on Reddit. Footnote 12: We only use items that can be linked to ML-25M in this experiment. Here 63.32% items are linked using the links. csv file from ML-25M. **Finding 9** - **Collaborative information can be dataset- or platform-dependent.** From Figure 6(b) shows that the models solely pre-trained on ML-25M (PT) outperform a random baseline, indicating that the data in CRS may share item similarities with pure interaction data from another platform to some extent. However, Figure 6(b) also shows a notable performance gap between PT and fine-tuning on Reddit (FT). Additionally, we do not observe further performance improvement when pre-training on ML-25M then fine-tuning on Reddit (PT+FT). These observations indicate that the collaborative information and underlying item similarities, even when utilizing the same items, can be largely influenced by the specific dataset or platform. The finding also may partially explain the inferior zero-shot recommendation performance of LLMs in Table 5 and suggest the necessity of further checking the alignment of collaborative knowledge in LLMs with the target datasets."
    },
    {
      "title": "Limitations Of Llms As Zero-Shot Crs",
      "text": "**Finding 10** - **LLM recommendations suffer from popularity bias in CRS.** Popularity bias refers to a phenomenon that popular items are recommended even more frequently than their popularity would warrant (Krishnan et al., 2019). Figure 8 shows the popularity bias in LLM recommendations, though it may not be biased to the popular items in the target datasets. On ReDIAL, the most popular movies such as Avengers: Infinity War appear around 2% of the time over all ground-truth items; On Reddit, the most popular movies such as Everything Everywhere A11 at Once appears less than 0.3% of the time over ground-truth items. But for the _generated_ recommendations from GPT-4 (other LLMs share a similar trend), Figure 8. Scatter plots of the frequency of LLMs (GPT-4) generated recommendations and ground-truth items. Figure 9. Ground-truth item counts in Reddit by country (in log scale) and the corresponding Recall@1 by country. the most popular items such as The Shawshank Redemption appear around 5% times on ReDIAL and around 1.5% times on Reddit. Compared to the target datasets, LLMs recommendations are more concentrated on popular items, which may cause further issues like the bias amplification loop (Beng et al., 2017). Moreover, the recommended popular items are similar across different datasets, which may reflect the item popularity in the pre-training corpus of LLMs. **Finding 11 - Recommendation performance of LLMs is sensitive to geographical regions.** Despite the effectiveness in general, it is unclear whether LLMs can be good recommenders across various cultures and regions. Specifically, pre-trained language models' strong open-domain ability can be attributed to pre-training from massive data (Beng et al., 2017). But it also leads to LLMs' sensitivity to data distribution. To investigate LLMs recommendation abilities for various regions, we take test instances from the Reddit dataset and obtain the production region of 7,476 movies from a publicly available movie dataset 13 by exact title matching, then report the Recall@1 for the linked movies grouped by region. We only report regions with more than 300 data points available to ensure enough data to support the result. As shown in Figure 9 the current best model, GPT-4's performance on recommendation is higher for movies produced in English-speaking regions. This could be due to bias in the training data - the left of Figure 9 show item on Reddit forums are dominated by movies from English-speaking regions. Such a result highlights large language model's recommendation performance varies by region and culture and demonstrates the importance of cross-regional analysis and evaluation for language model-based conversational recommendation models. Footnote 13: [https://www.kaggle.com/datasets/roumskhanik/the-movies-dataset](https://www.kaggle.com/datasets/roumskhanik/the-movies-dataset)"
    },
    {
      "title": "6. Related Work",
      "text": "**Conversational Recommendation.** Conversational recommender systems (CRS) aim to understand user preferences and provide personalized recommendations through conversations. Typical traditional CRS setups include template-based CRS (Kang et al., 2016; Liu et al., 2017; Wang et al., 2018; Wang et al., 2019) and critiquing-based CRS (Beng et al., 2017; Wang et al., 2018; Wang et al., 2019). More recently, as natural language processing has advanced, the community developed \"deep\" CRS (Kang et al., 2016; Wang et al., 2018; Wang et al., 2018) that support interactions in natural language. Aside from collaborative filtering signals, prior work shows that CRS models benefit from various additional information. Examples include knowledge-enhanced models (Kang et al., 2016; Wang et al., 2018) that make use of external knowledge bases (Kang et al., 2016; Wang et al., 2018), review-aware models (Wang et al., 2018), and session/sequence-based models (Wang et al., 2018; Wang et al., 2018). Presently, UniCRS (Wang et al., 2018), a model built on DialoGPT (Wang et al., 2018) with prompt tuning (Beng et al., 2017), stands as the state-of-the-art approach on CRS datasets such as ReDIAL (Wang et al., 2018) and INSPIRED (Wang et al., 2018). Currently, by leveraging LLMs, (Wang et al., 2018) proposes a new CRS pipeline but does not provide quantitative results, and (Wang et al., 2018) proposes better user simulators to improve evaluation strategies in LLMs. Unlike those papers, we uncover a _repeated item shortcut_ in the previous evaluation protocol, and propose a framework where LLMs serve as zero-shot CRS with detailed analyses to support our findings from both model and data perspectives. **Large Language Models.** Advances in natural language processing (NLP) show that large language models (LLMs) exhibit strong generalization ability towards unseen tasks and domains (Beng et al., 2017; Wang et al., 2018; Wang et al., 2018). In particular, existing work reveals language models' performance and sample efficiency on downstream tasks can be improved simply through scaling up their parameter sizes (Wang et al., 2018). Meanwhile, language models could further generalize to a wide range of unseen tasks by instruction tuning, learning to follow task instructions in natural language (Wang et al., 2018; Wang et al., 2018). Following these advances, many works successfully deploy large language models to a wide range of downstream tasks such as question answering, numerical reasoning, code generation, and commonsense reasoning without any gradient updates (Beng et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Recently, there have been various attempts by the recommendation community to leverage large language models for recommendation, this includes both adapting architectures used by large language models (Kang et al., 2016; Wang et al., 2018) and repurposing existing LLMs for recommendation (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). However, to our best knowledge, we are the first work that provides a systematic quantitative analysis of LLMs' ability on _conversational_ recommendation."
    },
    {
      "title": "7. Conclusion And Discussion",
      "text": "We investigate Large Language Models (LLMs) as zero-shot Conversational Recommendation Systems (CRS). Through our empirical investigation, we initially address a repetition shortcut in previous standard CRS evaluations, which can potentially lead to unreliable conclusions regarding model design. Subsequently, we demonstrate that LLMs as zero-shot CRS surpass all fine-tuned existing CRS models in our experiments. Inspired by their effectiveness, we conduct a comprehensive analysis from both the model and data perspectives to gain insights into the working mechanisms of LLMs, the characteristics of typical CRS tasks, and the limitations of using LLMs as CRS directly. Our experimental evaluations encompass two publicly available datasets, supplemented by our newly-created dataset on movie recommendations collected by scraping a popular discussion website. This dataset is the largest public CRS dataset and ensures more diverse and realistic conversations for CRS research. We also discuss the future directions based on our findings in this section. **On LLMs.** Given the remarkable performance even without fine-tuning, LLMs hold great promise as an effective approach for CRS tasks by offering superior content/contextual knowledge. The encouraging performance from the open-sourced LLMs (Kang et al., 2016; Wang et al., 2018) also opens up the opportunities to further improve CRS performance via efficient tuning (Beng et al., 2017; Wang et al., 2018) and collaborative filtering (Wang et al., 2018) ensembling. Meanwhile, many conventional tasks, such as debiasing (Beng et al., 2017) and trustworthy (Kang et al., 2018) need to be revisited in the context of LLMs. **On CRS.** Our findings suggest the systematic re-benchmarking of more CRS models to understand their recommendation abilities and the characteristics of CRS tasks comprehensively. Gaining a deeper understanding of CRS tasks also requires new datasets from diverse sources e.g., crowd-sourcing platforms (Wang et al., 2018; Wang et al., 2018), discussion forums, and realistic CRS applications with various domains, languages, and cultures. Meanwhile, our analysis of the information types uncovers the unique importance of the superior content/context knowledge in LLMs for CRS tasks; this distinction also sets CRS tasks apart from traditional recommendation settings and urges us to explore the interconnections between CRS tasks and traditional _recommendation_(Wang et al., 2018) or _conversational search_(Beng et al., 2017) tasks."
    },
    {
      "title": "References",
      "text": "* (1) * 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007, ASWC 2007, Busan, Korea, November 11-15, 2007. Proceedings._ Springer, 722-735. * Bajaj et al. (2018) Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Ming Stoica, Saurabru Tiwary, and Tong Wang. 2018. MS MARC0: A Human Generated Machine Reading Comprehension Dataset. arXiv:1611.09268 [cs.CL]. * Bao et al. (2023) Keqin Bao, Jizhi Zhang, Tang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TAILREC: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. _arXiv preprint arXiv:2305.00447_ (2023). * Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared L Kaplan, Prafulla Dhariwal, Arvind Neelakant, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_ 33 (2020), 1877-1901. * Brown et al. (2021) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakant, Pranav Shyam, Girish Sastry, Amanda Askell, Sandham Agarwal, Ariel Herbert-Voss, Gretchen Kruger, Tom Hengiban, Ehseth Childs Barnes, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Liwinc, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sum McCandish, Alex Radford, Ilya Sutskever, and Dario Amodei. 2021. Language Models: Few-shot learners. In _Advances in Neural Information Processing Systems_, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877-1901. [https://proceedings.ne/en/en/en/en/en/1877-1901.pdf/145706dbdb64964718db8ac142648Paper.pdf](https://proceedings.ne/en/en/en/en/en/1877-1901.pdf/145706dbdb64964718db8ac142648Paper.pdf) * Bubek et al. (2023) Sebastian Bubek, Varam Chandrasekaran, Ronen Eldan, Johannes Gehrle, Eric Horne, Kean Petter, Lee Tari Lee, Tianxhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.2172_ (2023). * Cao et al. (2021) Nicola De Cao, Gauttier Lacard, Sebastian Rierdel, and Fabio Petroni. 2021. Autoregressive Entity Retrieval. In _International Conference on Learning Representations_. [https://openreview.net/forum1-5&SRFUSWY](https://openreview.net/forum1-5&SRFUSWY) * Chen et al. (2023) Jiawei Chen, Handle Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023. Bias and debias in recommender system: A survey and future directions. _ACM Transactions on Information Systems_ 41, 3 (2023), 1-39. * Chen and Pu (2012) Li Chen and Pearl Pu. 2012. Critiguing-based recommenders: survey and emerging trends. _User Modeling and User-Adaplated Interaction_ 22 (2012), 125-150. * Chen et al. (2019) Qihu Chen, Junying Liu, Yichang Zhang, Ming Ding, Yuliux Chen, Hongxia Yang, and Jie Tang. 2019. Towards Knowledge-Based Recommender Dialog System. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 10th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_. 1803-1813. * Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Jiangxhang Wu, Hao Zhang, Linning Zheng, Syuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Imressing GPT-4 with 90% CNTe-MTP Quality. [https://mys.org/blog/2023-03-vicuna](https://mys.org/blog/2023-03-vicuna) * Chowberty et al. (2022) Aakanks Chowberty, Sharan Narara, Jacob Devlin, Maarten Bosma, Gurauro Shishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Singh, Sasha Tsuyeshchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodlamar Prabhakaran, Emily Reif, Nan Du, Kenton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Arf, Pengpeng Yin, Tfuj Duc, Anais Lemkevak, Sanjay Ghemawat, Sunjie Dev, Henryk Michaelski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Demy Zhou, Daphne Ipolpolito, David Lian, Hyonnetko Lim, Barret Pello, Alexander Spiriotto, Ryan Seppanski, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thunnalyan Sankankaranayana Filia, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Childs, Oleksandr Polov, Arvind Nezie, L'Aureli Wang, Brennan Satch, Matev Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Pal:Asl: Scaling Language Modeling with Pathways. _ArXiv preprint arXiv:2204.0231_ (2022). * Christakopoulou et al. (2016) Konstantina Christakopoulou, Filip Radminski, and Kaja Hoffman. 2016. Towards conversational recommender systems. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_. 315-324. * Cui et al. (2022) Zey Cui, Jianxin Ma, Chang Zhou, Jingwen Zhou, and Hongxi Yang. 2022. McRec: Generative Pretrained Language Models are Open-Ended Recommender Systems. arXiv:2205.08084 [cs.R]. * Dvini et al. (2019) Jacob Dvini, Ming-Wie Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, Volume 1 (Long and Short Papers)_. 4711-4186. * Friedman et al. (2023) Luke Friedman, Sameer Ahuja, David Allen, Terry Tan, Hakim Sidahmed, Changbo Long, Jun Xie, Gabriel Schubinger, Ajay Patel, Harsh Lara, et al. 2023. Leveraging Large Language Models in Conversational Recommender Systems. _arXiv preprint arXiv:2305.07961_ (2023). * Gao et al. (2022) Yingqiang Gao, Shuchao Liu, Yanhua Fu, Juntao Tan, Ediong Li, Shuyuan Xu, Yunyu Li, Yixun Xia, and Yongfeng Zhang. 2022. A survey on trustworthy recommender systems. _arXiv preprint arXiv:2207.12515_ (2022). * Geng et al. (2022) Shihie Geng, Shuchua Liu, Zuohan Fu, Yingqiang He, and Yongfeng Zhang. 2022. Recommendation as Language Processing (RIP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). In _Recsys '22 Sixteenth ACM Conference on Recommender Systems, Seattle, WA, USA, September 18-2, 2022_, Jennifer Goblec, F. Maxwell Harper, J. Rossa Murdock, Michael E. Elstam, Bracha Shapira, Justin Basilico, Keld T. Lundgaard, and Even Oldridge (Eds.). ACM, 299-315. * Guidbande et al. (2020) Armao Guidbande, Eric Wallace, Charlie Shell, Xiangyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2020. The False Promise of Unfitting Proprietary LLMs. arXiv:2305.15717 [cs.CL]. * Harper and Konstan (2016) F. Maxwell Harper and Joseph A. Konstan. 2016. The MovieLens Datasets: History and Context. _ACM Trans. Interact. Intell. Syt._ 5 (2016), 19:1-19:19. * Hayati et al. (2020) Shirley Anupash Hayati, Dongyeop Kang, Qingioanoyang Zhu, Weiyan Shi, and Zhou Yu. 2020. INSIPRED: Toward Socialing Recommendation Dialog Systems. In _Proceedings of the 22020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_. 8142-8152. * He et al. (2018) Xiangnan He, Zhankui He, Xiaoyu Du, and Tat-Seng Chua. 2018. Adversarial personalized ranking for recommendation. In _The 41st International ACM SIGIR conference on research & development in information retrieval_. 355-364. * Li et al. (2017) Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xiia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In _Proceedings of the 26th international conference on world wide web_. 173-182. * He et al. (2021) Zhanhui He, Handong Zhao, Zhe Li, Zhaowan Wang, Ajinkya Kale, and Julian McAuley. 2021. Locker: Locally constrained self-attentive sequential recommendation. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_. 3088-309. * He et al. (2022) Zhanhui He, Handong Zhao, Tong Yu, Sungchul Kim, Fan Du, and Julian McAuley. 2022. Bundle MCR: Towards Conversational Bundle Recommendation. In _Proceedings of the 16th ACM Conference on Recommender Systems_. 288-298. * Hou et al. (2023) Yuyeng Hou, Junjie Zhang, Zhan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large Language Models are Zero-Shot Rankers for Recommender Systems. _arXiv preprint arXiv:2305.08848_ (2023). * Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanhi Li, Shean Wang, Lu Wang, and Weihur Chen. 2021. Lorie: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.0985_ (2021). * Jhamtani et al. (2018) Harsh Jhamtani, Varam Gangl, Eduard Hovy, Graham Neubig, and Taylor Berg-Kirkpatrick. 2018. Learning to Generate Move-by-Move Commenters for Games Games from Large-Scale Social Forum Data. In _The 56th Annual Meeting of the Association for Computational Linguistics (ACL)_. Melbourne, Australia. * Jacob Hillon et al. (2022) C Kim Jacob Hillon Jacob Menick Jiayi Weng Juan Felipe Ceron Uribe Lian Fedus Luke Metichel Tolokery Rapba Kontilfo Lopes Sergia Zhao Doha Schlamm, Barret Zophi. 2022. Catapfit Optimizing Images models for dialogue. _OpenAI_ (2022). * Kabbur et al. (2022) Santosh Kabbur, Xia Ning, and George Karypis. 2022. Fism: factored similarity models for top-n recommender systems. In _Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining_. 659-667. * Kang et* [38] Wenqiang Lei, Gangyi Zhang, Xiangnan He, Tsiong Miao, Xiang Wang, Liang Chen, and Tat-Seng Chu. 2020. Interactive path reasoning on graph for conversational recommendation. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_. 2073-2083. * [39] Jinming Li, Wentzhang Zhang, Tian Wang, Guangzie Xiong, Alan Lu, and Gerard Medioni. 2023. GPT-face: A Generative Framework for Personalized Recommendation and User Interests Interpretation. arXiv:2304.03879 [cs.IR] * [40] Ming Li, Samilen, Johnrich Ariannezhad, and Maarten de Rijke. 2023. A next basket recommendation reality check. _ACM Transactions on Information Systems_ 41, 4 (2023), 1-29. * [41] Raymond L, Samira Phamhi Kahou, Hannes Schulz, Vincent Michalski, Laurent Charlin, and Chris Pali. 2018. Towards deep conversational recommendations. _Advances in neural information processing systems_ 31 (2018). * [42] Shuyang Li, Bodishiroa Prasad Majumder, and Julian McAuley. 2021. Self-Supervised R1q Play for Conversational Recommendation with Justifications. _arXiv preprint arXiv:2112.05197_ (2021). * [43] Shukai Li, Ruong Xie, Yongchuan Zhu, Xiang Ao, Furken Zhang, and Qing He. 2022. User-centric conversation recommendation with multi-aspect user modeling. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_ 223-233. * [45] Tannan Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset. In _Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1:Long Papers)_. Asian Federation of Natural Language Processing, Taipei, Taiwan, 986-995. [https://aclanthology.org/1717-1099](https://aclanthology.org/1717-1099). * [46] Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018. Variational autoencoders for collaborative filtering. In _Proceedings of the 2018 world wide web conference_. 689-698. * [47] Hugo Liu and Push Singh. 2004. Concept-a-practical commonsense reasoning tool-kit. _BT technology journal_ 22, 4 (2004), 211-226. * [48] Junling Liu, Chao Liu, Renzio Lv, Nang Zhou, and Yan Zhang. 2023. Is ChattGPT a Good Recommender? A Preliminary Study. arXiv:2304.10149 [cs.IR] * [49] Yu Lu, Junwei Bao, Yan Song, Zichen Ma, Shuguugu Cui, Youzheng Wu, and Xiaodong He. 2021. Recface: Review-Augmented Conversational Recommendation. In _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_. 1161-1173. * [50] Wenching Ma, Ryuichi Takanobu, and Minle Huang. 2021. CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing Association for Computational Linguistics_. [https://aclanthology.org/2021-emlp-main.139](https://aclanthology.org/2021-emlp-main.139) * [51] OpenAI 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] * [52] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamels Mishkin, Chong Zhang, Sandhim Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Killion, Fraser Kelton, Luke Miller, Madeie Simers, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In _Twenty-ppgapets_.npgecopper_.flags/paper/2022/hash/biele36e36a47391458805001731-Abstract-Conference.html * [53] Gustavo Penha and Claudio Hawll. 2020. What does bert know about books, movies and music? probing bert for conversational recommendation. In _Proceedings of the 14th ACM Conference on Recommender Systems_. 388-397. * [54] Zhaochun Ren, Zhi Tian, Donglong Li, Pengjie Ren, Lin Yang, Xin Hu, Huang Liang, Maarten de Rijke, and Zhumin Chen. 2022. Variational Reasoning about User Preferences for Conversational Recommendation. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 165-175. * [55] Steffen Rendle. 2010. Factorization machines. In _2010 IEEE International conference on data mining_. IEEE, 995-1000. * [56] Alireza Salemi, Shengkus Mywore, Michael Bendersky, and Hamed Zamani. 2023. LaMP: When Large Language Models Meet Personalization. _arXiv preprint arXiv:2304.11406_ (2023). * [57] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Satawika, Zaid Ajyafaei, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Mannen Dey, M Saiful Bari, Cauvenven Nurnis Thakker, Shanya Sharma Sharma, Elia Saccrechia, Thewon Kim, Gunjan Chikhani, Niall V.ayak, Deholyu Gupta, Jonathan Chang, Mike Tian-Jin Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Marshall Pandre, Rachel Bawden, Thomas Wang, Trishahi Neerja, Jos Rozen, Abheest Sharma, Andrea Satilli, Thhah Castro Fey, Jason An Fries, Ryan Teken, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask Prompted Training Enables Zero-Shot Task Generalization. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReviewen. [https://openreview.net/forum?id=9vh9D0W14](https://openreview.net/forum?id=9vh9D0W14) * [58] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015. Autorec: Autoencoders meet collaborative filtering. In _Proceedings of the 24th international conference on World Wide Web_. 111-112. * [59] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenruu Ou, and Peng Jiang. 2019. BERT42e: Sequential recommendation with bidirectional encoder representations from transformer. In _Proceedings of the 28th ACM international conference on information and knowledge management_. 1441-1450. * [60] Yi Tay, Vinh Tran, Mostafa Dehghani, Jiannn Niu, Darah Bahri, Harish Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jia Gupta, et al. 2022. Transformer memory as a differentiable search index. _Advances in Neural Information Processing Systems_ 35 (2022), 21831-21843. * [61] Hugo Touvron, Thibaut Lavrili, Gautier Incaard, Xavier Martinet, Marie-Anne Lachaus, Timothero Lacroix, Baptsen Rozice, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.1971_ (2023). * [62] Wenjie Wang, Xinyu Lin, Paul Feng, Xiangnan He, and Tat-Seng Chua. 2023. Generative Recommendation: Towards Next-generation Recommender Paradigm. arXiv:2304.03516 [cs.IR] * [63] Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, and Ji-Rong Wen. 2023. Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models. _arXiv preprint arXiv:2305.13121_ (2023). * [64] Xiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao. 2022. Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_. 1929-1937. * [65] Jason Wei, Xuedhi Wang, Dale Schuurmans, Maarten Bosma, Brian icilter, Fei Xia, Ed Chi, Quoc V Le, and Demmy Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Image Language Models. In _Advances in Neural Information Processing Systems_. S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 42842-42837. [https://proceedings.neurips.cc/paper.files/paper/2022/file/9456091532461524671031384-Paper-Conference.pdf](https://proceedings.neurips.cc/paper.files/paper/2022/file/9456091532461524671031384-Paper-Conference.pdf) * [66] Jason Wei, Xuedhi Wang, Dale Schuurmans, Mastern Bosma, Fei Xia, Ed Chi, Quoc V Le, Demmy Zhou, et al. 2022. Chain-of-thopath prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_ 35 (2022), 24824-24837. * [67] Gu Wu, Kai Luo, Scott Sanner, and Harold Soh. 2019. Deep language-based critiquing for recommender systems. In _Proceedings of the 13th ACM Conference on Recommender Systems_. 137-145. * [68] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baire: An open-source chat model with parameter-efficient tuning on self-chat data. _arXiv preprint arXiv:2304.01196_ (2023). * [69] Yizhe Zhang, Sui Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Qiao Yang, Jingjing Liu, and William B Dolan. 2020. DIALOGPT: Large-Scale Generative Pre-training for Conversational Response Generation. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_. 270-278. * [70] Ying Zhang, Lingfei Wu, Qi Shen, Yitong Pang, Zhihua Wei, Fangli Xu, Bo Long, and Jian Pei. 2022. Multiple Choice Questions based Multi-Interested Policy Learning for Conversational Recommendation. In _Proceedings of the ACM Web Conference_. 2022 2153-2162. * [71] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beicham Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. _arXiv preprint arXiv:2303.18223_ (2023). * [72] Qihank Zheng, Xiao Xia, You, Zuxiong Wang, Shuang Wang, Yufei Xue, Zihang Wang, Lei Shen, Anui Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. 2023. Codefee: A Pre-Trained Model for Code Generation with Multilingual Evaluations on Humanval-X-2303.17568 [cs.LG] * [73] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yitao Zhu, Siwei Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In _Proceedings of the 29th ACM international conference on information & knowledge management_. 1893-1902. * [74] Kun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang Zhou, Ji-Rong Wen, and Jingsong Yu. 2020. Improving conversational recommender systems via knowledge graph based semantic fusion. In _Proceedings of the 26th ACM SIGK"
    }
  ]
}