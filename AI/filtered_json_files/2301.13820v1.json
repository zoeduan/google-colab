{
  "title": "Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)",
  "authors": [
    "Daking Rai",
    "Yilun Zhou",
    "Bailin Wang",
    "Ziyu Yao"
  ],
  "abstract": "\n While large language models (LLMs) have demonstrated strong capability in structured prediction tasks such as semantic parsing, few amounts of research have explored the underlying mechanisms of their success. Our work studies different methods for explaining an LLM-based semantic parser and qualitatively discusses the explained model behaviors, hoping to inspire future research toward better understanding them. \n",
  "references": [
    {
      "id": null,
      "title": "Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)",
      "authors": [
        "Daking Rai",
        "Yilun Zhou",
        "Bailin Wang",
        "Ziyu Yao"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S M Lundberg",
        "S.-I Lee"
      ],
      "year": "2017",
      "venue": "A unified approach to interpreting model predictions",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Why should I trust you?\": Explaining the predictions of any classifier",
      "authors": [
        "M T Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "SIGKDD",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "PICARD: Parsing incrementally for constrained auto-regressive decoding from language models",
      "authors": [
        "T Scholak",
        "N Schucher",
        "D Bahdanau"
      ],
      "year": "2021",
      "venue": "EMNLP",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "A Value for N-Person Games",
      "authors": [
        "L S Shapley"
      ],
      "year": "1952",
      "venue": "RAND Corporation",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Local explanation of dialogue response generation",
      "authors": [
        "Y Tuan",
        "C Pryor",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Local explanation of dialogue response generation",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "UnifiedSKG: Unifying and multi-tasking structured knowledge grounding with text-to-text language models",
      "authors": [
        "T; Xie",
        "C H Wu",
        "P Shi"
      ],
      "year": "2022",
      "venue": "EMNLP",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task",
      "authors": [
        "T Yu",
        "R Zhang",
        "K Yang"
      ],
      "year": "2018",
      "venue": "EMNLP",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)",
      "text": "Daking Rai,1 Yilun Zhou,2 Bailin Wang,2 Ziyu Yao1 1 George Mason University, 2 Massachusetts Institute of Technology drai2@gmu.edu, yilun@mit.edu, bailinw@mit.edu, ziyuyao@gmu.edu"
    },
    {
      "title": "Abstract",
      "text": "While large language models (LLMs) have demonstrated strong capability in structured prediction tasks such as semantic parsing, few amounts of research have explored the underlying mechanisms of their success. Our work studies different methods for explaining an LLM-based semantic parser and qualitatively discusses the explained model behaviors, hoping to inspire future research toward better understanding them."
    },
    {
      "title": "Introduction",
      "text": "Semantic parsing is a task of mapping natural language utterances to their logical forms like SQL queries or lambda expressions for database or knowledge base querying. Despite its structured prediction nature, recent work has shown that a large language model (LLM) which generates output sequentially could achieve comparable or even better performance than the traditional structured decoders [15]. However, why these LLMs could do well in semantic parsing is still unclear. In this paper, we seek to provide one of the first studies toward explaining LLM-based neural semantic parsers. We use the text-to-SQL semantic parsing task [23] and the UnifiedSKG model [24] for a case study. We empirically explore a set of local explanation methods and quantitatively discussed the explanation results."
    },
    {
      "title": "Method",
      "text": "(1) LIME [16] generates an explanation by training locally-faithful interpretable models with the dataset obtained by perturbing the prediction instance. (2) Shapley value measures the importance of a feature by its average marginal contribution to the prediction score. (3) Kernel SHAP [14] is another efficient way of estimating Shapley values by training a linear classifier. (4) LERG [21] is a set of two approaches, LERG_L and LERG_S, recently adapted from LIME and Shapley value to conditioned sequence generation tasks. When applying these methods to explain an LLM-based semantic parser, we consider each output token as one prediction and attribute it to the input features. (5) Attention: Prior work has revealed that attention may be interpreted as feature importance. Therefore, we also introduce an attention-based local explanation method, where the feature attribution is calculated by averaging the last layer of the multi-headed cross-attention weights."
    },
    {
      "title": "Experimental Setup",
      "text": "In our experiments, we consider the task of text-to-SQL semantic parsing where the goal is to generate a SQL query given a natural language question and the database schema (i.e., tables and columns included in the database) as input. We experiment with UnifiedSKG [24], one of the state-of-the-art models, which adopts a T5 encoder-decoder structure.1 Following Xie et al. [2], we train and evaluate the parser on the Spider dataset [23]. Footnote 1: We used the “T5_base_prefix_spider_with_cell_value” version from [https://github.com/HKUNLP/UnifiedSKG](https://github.com/HKUNLP/UnifiedSKG). We did not use the T5-3B version because of the large computational demand, which we will discuss in Section. Through the experiments, we seek to answer two _Research Questions (RQs)_: (1) _Which local explanation method is the most faithful to explaining the LLM-based UnifiedSKG parser?_ (2) _How well does the explanation align with human intuitions?_ To answer RQ1, we follow Tuan et al. [21] and compare different explanation methods on two metrics: (a) Sufficiency measures the perplexity when keeping only the top-K% most important features by each explanation method; the lower the better/faithful. (b) Necessity measures the perplexity change when the top-K% Figure 1: Necessity (left) and Sufficiency (right) scores when removing or keeping the top-K% important features. most important features are removed; the higher the better/faithful. To answer RQ2, we qualitatively discuss the most faithful explanation results."
    },
    {
      "title": "Experimental Results",
      "text": "**Faithfulness.** The results in Figure 1 show LERG_S has the best performance as per both sufficiency and necessity metrics with Kernel SHAP having comparable performance as well. In general, we observe that Shapley value-based explanation methods have more faithful explanations than other methods. In addition, we also found that attention-based explanations are more faithful than LIME and LERG_L. **Plausibility.** Using LERG_S as a lens, we qualitatively study how UnifiedSKG works. We define _plausible_ explanations as those which align well with human intuition. In our study, we classify each explanation into plausible or partially plausible ones. Interestingly, we didn't find any explanation that is completely implausible. Under this setup, we investigate the four aspects listed below (Figure 2): **(1) Feature Attribution for (In)correct Predictions**: We randomly sample 20 examples where the model makes correct and incorrect predictions, respectively. We find out that in most cases (85% for correct and 70% for incorrect), LERG_S generates a plausible explanation for both types. **(2) Different Hardness Levels**: We randomly sample 20 examples for each hardness level - easy, medium, hard, and extra hard, as defined by the Spider benchmark based on the SQL complexity. We observed that in most cases the model behaviors are in line with human intuitions even at the extra hard level (80%; \\(>\\)90% for other levels). **(3) Compositional Generalization**: We seek to understand whether the model attributes the output fragments to correct features compositionally when it makes correct predictions. We conducted a similar manual examination as before and observed that in most (80%) cases our model shows compositionally generalizable feature attribution. **(4) In-domain vs. Out-of-domain**: As the Spider training and dev sets are split by databases (which could be seen as different domains), we also manually compare the model explanations in in-domain and out-of-domain cases. We observe that for both cases (75% and 80% respectively), the generated explanations were mostly plausible."
    },
    {
      "title": "Discussion And Future Directions",
      "text": "Our study has revealed several challenges and opportunities in explaining an LLM-based semantic parser: **(1) Computational costs**: Most local explanation methods require model inference over a large set of input perturbations, which is computationally inefficient. Future work may look into improving the attention-based explanation method, which does not rely on perturbations and hence could save much computation. **(2) Feature interaction**: Traditional feature attribution does not provide information about how features (e.g., question tokens and contextual database schema items) interact with each other. Future work may uncover these interactions to gain deeper insights into how the model works. **(3) Explanation for user understanding**: Current saliency maps encompass a lot of information. Future work could examine how to present the information in a concise and friendly way such that users could easily grasp the intuition of the model prediction and verify its correctness. **(4) Explanation for debugging**: Future work should also investigate how the local explanation results could be used to probe and debug a semantic parser, such as to improve their capability in compositional generalization."
    },
    {
      "title": "References",
      "text": "* Lundberg and Lee (2017) Lundberg, S.M.; and Lee, S.-L. 2017. A unified approach to interpreting model predictions. In _NeurIPS_. * Ribeiro et al. (2016) Ribeiro, M.T.; Singh, S.; Guestrin, C.. 2016. \"Why should I trust you?\": Explaining the predictions of any classifier. In _SIGKDD_. * Scholak et al. (2021) Scholak T.; Schucher N.; Bahdanau D.. 2021. PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. In _EMNLP_. * Shapley (1952) Shapley, L.S.. 1952. A Value for N-Person Games. In _RAND Corporation_. * Tuan et al. (2021) Tuan, Y.; Pryor, C.; Chen, W.; et al. 2021. Local explanation of dialogue response generation. In _NeurIPS_. * Xie et al. (2022) Xie, T; Wu, C.H.; Shi, P.; et al. 2022. UnifiedSKG: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. In _EMNLP_. * Yu et al. (2018) Yu, T.; Zhang, R.; Yang, K.; et al. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In _EMNLP_. Figure 2: UnifiedSKG generally shows plausible explanations. In (a), “concert_singer” is the database name; “singer :...” shows the table along with its columns (omitted with ellipsis); similarly for other examples. For (b), the selected column “hp” is incorrect and should be “horsepower”. Blue/red indicates positive/negative importance. Darkness indicates strength."
    }
  ]
}