{
  "title": "Automated Repair of AI Code with Large Language Models and Formal Verification",
  "authors": [
    "Yiannis Charalambous",
    "Edoardo Manino",
    "Lucas C Cordeiro"
  ],
  "abstract": "\n The next generation of AI systems requires strong safety guarantees. This report looks at the software implementation of neural networks and related memory safety properties, including NULL pointer deference, out-of-bound access, double-free, and memory leaks. Our goal is to detect these vulnerabilities, and automatically repair them with the help of large language models. To this end, we first expand the size of NeuroCodeBench, an existing dataset of neural network code, to about 81k programs via an automated process of program mutation. Then, we verify the memory safety of the mutated neural network implementations with ESBMC, a stateof-the-art software verifier. Whenever ESBMC spots a vulnerability, we invoke a large language model to repair the source code. For the latest task, we compare the performance of various state-of-the-art prompt engineering techniques, and an iterative approach that repeatedly calls the large language model. \n",
  "references": [
    {
      "id": null,
      "title": "Automated Repair of AI Code with Large Language Models and Formal Verification",
      "authors": [
        "Yiannis Charalambous",
        "Edoardo Manino",
        "Lucas C Cordeiro"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Information capacity of the hopfield model",
      "authors": [
        "Y Abu-Mostafa",
        "J St",
        "Jacques"
      ],
      "year": "1985",
      "venue": "IEEE Transactions on Information Theory",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "",
      "authors": [
        "M",
        "L Foundation",
        "Pytorch"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Pitfalls in machine learning research: Reexamining the development cycle",
      "authors": [
        "S Biderman",
        "W J Scheirer"
      ],
      "year": "2020",
      "venue": "Proceedings on \"I Can't Believe It's Not Better!\" at NeurIPS Workshops",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "",
      "authors": [
        "G Brain",
        "Tensorflow"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "The Fourth International Verification of Neural Networks Competition (VNN-COMP 2023): Summary and Results",
      "authors": [
        "C Brix",
        "S Bak",
        "C Liu",
        "T T Johnson"
      ],
      "year": "2023",
      "venue": "The Fourth International Verification of Neural Networks Competition (VNN-COMP 2023): Summary and Results",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J D Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "A new era in software security: Towards self-healing software via large language models and formal verification",
      "authors": [
        "Y Charalambous",
        "N Tihanyi",
        "R Jain",
        "Y Sun",
        "M A Ferrag",
        "L C Cordeiro"
      ],
      "year": "2023",
      "venue": "A new era in software security: Towards self-healing software via large language models and formal verification",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Bipartite expander hopfield networks as self-decoding high-capacity error correcting codes",
      "authors": [
        "R Chaudhuri",
        "I Fiete"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "How is chatgpt's behavior changing over time",
      "authors": [
        "L Chen",
        "M Zaharia",
        "J Zou"
      ],
      "year": "2023",
      "venue": "How is chatgpt's behavior changing over time",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "cwe top 25 most dangerous software weaknesses",
      "authors": [
        "C Community"
      ],
      "year": "2023",
      "venue": "cwe top 25 most dangerous software weaknesses",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Open neural network exchange: The open standard for machine learning interoperability",
      "authors": [
        "O Community"
      ],
      "year": "2023",
      "venue": "Open neural network exchange: The open standard for machine learning interoperability",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "keras2c github repository",
      "authors": [
        "R Conlin"
      ],
      "year": "2023",
      "venue": "keras2c github repository",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Keras2c: A library for converting keras neural networks to real-time compatible c",
      "authors": [
        "R Conlin",
        "K Erickson",
        "J Abbate",
        "E Kolemen"
      ],
      "year": "2021",
      "venue": "Engineering Applications of Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Software security",
      "authors": [
        "L Cordeiro"
      ],
      "year": "",
      "venue": "Software security",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "SMT-Based Bounded Model Checking for Embedded ANSI-C Software",
      "authors": [
        "L Cordeiro",
        "B Fischer",
        "J Marques-Silva"
      ],
      "year": "2009",
      "venue": "SMT-Based Bounded Model Checking for Embedded ANSI-C Software",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Model checking of real-time reachability properties using abstractions",
      "authors": [
        "C Daws",
        "S Tripakis"
      ],
      "year": "1998",
      "venue": "Model checking of real-time reachability properties using abstractions",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Differential testing of cross deep learning framework APIs: Revealing inconsistencies and vulnerabilities",
      "authors": [
        "Z Deng",
        "G Meng",
        "K Chen",
        "T Liu",
        "L Xiang",
        "C Chen"
      ],
      "year": "2023",
      "venue": "32nd USENIX Security Symposium (USENIX Security 23)",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Mull it over: Mutation testing based on llvm",
      "authors": [
        "A Denisov",
        "S Pankevich"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Automated repair of programs from large language models",
      "authors": [
        "Z Fan",
        "X Gao",
        "M Mirchev",
        "A Roychoudhury",
        "S H Tan"
      ],
      "year": "2023",
      "venue": "2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Exploring the limits of out-of-distribution detection",
      "authors": [
        "S Fort",
        "J Ren",
        "B Lakshminarayanan"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Audee: Automated testing for deep learning frameworks",
      "authors": [
        "Q Guo",
        "X Xie",
        "Y Li",
        "X Zhang",
        "Y Liu",
        "X Li",
        "C Shen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, ASE '20",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability",
      "authors": [
        "X Huang",
        "D Kroening",
        "W Ruan",
        "J Sharp",
        "Y Sun",
        "E Thamo",
        "M Wu",
        "X Yi"
      ],
      "year": "2020",
      "venue": "Computer Science Review",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Taxonomy of real faults in deep learning systems",
      "authors": [
        "N Humbatova",
        "G Jahangirova",
        "G Bavota",
        "V Riccio",
        "A Stocco",
        "P Tonella"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, ICSE '20",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Neural code completion",
      "authors": [
        "C Liu",
        "X Wang",
        "R Shin",
        "J E Gonzalez",
        "D Song"
      ],
      "year": "2016",
      "venue": "Neural code completion",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "NeuroCodeBench: a Plain C Neural Network Benchmark for Software Verification",
      "authors": [
        "E Manino",
        "R S Menezes",
        "F Shmarov",
        "L C Cordeiro"
      ],
      "year": "2023",
      "venue": "Workshop on Automated Formal Reasoning for Trustworthy AI Systems",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Deep learning-based wave digital modeling of rate-dependent hysteretic nonlinearities for virtual analog applications",
      "authors": [
        "O Massi",
        "A I Mezza",
        "R Giampiccolo",
        "A Bernardini"
      ],
      "year": "2023",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Learning density distribution of reachable states for autonomous systems",
      "authors": [
        "Y Meng",
        "D Sun",
        "Z Qiu",
        "M T B Waez",
        "C Fan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 5th Conference on Robot Learning",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Trends and challenges in the vulnerability mitigation landscape",
      "authors": [
        "M Miller"
      ],
      "year": "2019",
      "venue": "Trends and challenges in the vulnerability mitigation landscape",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Bugs in machine learning-based systems: a faultload benchmark",
      "authors": [
        "M M Morovati",
        "A Nikanjam",
        "F Khomh",
        "Z M J Jiang"
      ],
      "year": "2023",
      "venue": "Empirical Software Engineering",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "The third international verification of neural networks competition (vnn-comp 2022): Summary and results",
      "authors": [
        "M N Müller",
        "C Brix",
        "S Bak",
        "C Liu",
        "T T Johnson"
      ],
      "year": "2023",
      "venue": "The third international verification of neural networks competition (vnn-comp 2022): Summary and results",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Simple black-box adversarial attacks on deep neural networks",
      "authors": [
        "N Narodytska",
        "S Kasiviswanathan"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "TensorFuzz: Debugging neural networks with coverage-guided fuzzing",
      "authors": [
        "A Odena",
        "C Olsson",
        "D Andersen",
        "I Goodfellow"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI Blog",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Safe reinforcement learning benchmark environments for aerospace control systems",
      "authors": [
        "U J Ravaioli",
        "J Cunningham",
        "J Mccarroll",
        "V Gangal",
        "K Dunlap",
        "K L Hobbs"
      ],
      "year": "2022",
      "venue": "2022 IEEE Aerospace Conference",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "A framework for understanding sources of harm throughout the machine learning life cycle",
      "authors": [
        "H Suresh",
        "J Guttag"
      ],
      "year": "2021",
      "venue": "Equity and Access in Algorithms, Mechanisms, and Optimization, EAAMO '21",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "onnx2c github repository",
      "authors": [
        "K User"
      ],
      "year": "2023",
      "venue": "onnx2c github repository",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A N Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "authors": [
        "J White",
        "Q Fu",
        "S Hays",
        "M Sandborn",
        "C Olea",
        "H Gilbert",
        "A Elnashar",
        "J Spencer-Smith",
        "D C Schmidt"
      ],
      "year": "2023",
      "venue": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Neural-based dynamic modeling of nonlinear microwave circuits",
      "authors": [
        "J Xu",
        "M Yagoub",
        "R Ding",
        "Q.-J Zhang"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on Microwave Theory and Techniques",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Automated Repair Of Ai Code With Large Language Models And Formal Verification",
      "text": "Yiannis Charalambous The University of Manchester, Department of Computer Science, Manchester, United Kingdom Edoardo Manino The University of Manchester, Department of Computer Science, Manchester, United Kingdom Lucas C. Cordeiro"
    },
    {
      "title": "Abstract",
      "text": "The next generation of AI systems requires strong safety guarantees. This report looks at the software implementation of neural networks and related memory safety properties, including NULL pointer reference, out-of-bound access, double-free, and memory leaks. Our goal is to detect these vulnerabilities, and automatically repair them with the help of large language models. To this end, we first expand the size of _NeuroCodeBench_, an existing dataset of neural network code, to about 81k programs via an automated process of program mutation. Then, we verify the memory safety of the mutated neural network implementations with ESBMC, a state-of-the-art software verifier. Whenever ESBMC spots a vulnerability, we invoke a large language model to repair the source code. For the latest task, we compare the performance of various state-of-the-art prompt engineering techniques, and an iterative approach that repeatedly calls the large language model."
    },
    {
      "title": "1 Introduction",
      "text": "In contrast to classic software development, neural networks are crafted via a long process of trial and error that terminates when their predictive performance reaches a satisfactory level [4, 36]. The iterative and performance-driven nature of this process leaves neural networks vulnerable on many fronts [23]: poor performance on out-of-distribution [21] and adversarial inputs [32], misspecification of the neural architecture and training process [24], invocation of broken and deprecated libraries [30], outright software bugs [22]. Unfortunately, many of these vulnerabilities are not easy to catch early in the development process and may remain hidden until after deployment. Although efforts to debug the actual implementation of neural networks exist, they are based on automatic testing and thus cannot prove correctness for all inputs [33, 22, 18]. This lack of guarantees is especially concerning for safety-critical systems since common software vulnerabilities [11] (e.g., arithmetic overflows, invalid memory accesses) can make the networks produce wrong results, expose sensitive data or corrupt the system they are executed on. In this report, we tackle the challenge of producing bug-free implementations of neural networks in the following way. First, we employ software verifiers to ensure full coverage of the state space. In the past, it has been claimed that software verifiers struggle to cope with neural network code due to its size, complexity and the presence of numerous calls to the standard mathematical library [26]. To verify this claim, we generate a large dataset of neural network code, by injecting memory vulnerabilities via program mutations. Our results show that software verifiers can find memory safety vulnerabilities in neural network code relatively easily, thus allowing us to use them as oracles for the correctness of neural network implementations. Second, we adopt large language model as a powerful tool for program repair. In the past years, large language models have shown great promise in a wide variety of code processing tasks including program translation [39], code completion [25] and automated repair [8]. At the same time, the related literature always show a drop in performance on out-of-distribution samples, which are different from those seen during training. Here, we posit that neural network implementations, or AI code in short, fits in the latter category and constitutes an excellent benchmark for out-of-distribution performance. Indeed, our results show that off-the-shelf large language models can repair AI code, but require very specific prompting techniques to do so at an acceptable level. More specifically, this report covers the following topics: * Our methodology to generate a large dataset of AI code examples. This is done by automatically increasing the size of the _NeuroCodeBench_ dataset [26] via code-specific data augmentation techniques. * Experimental results on running the ESBMC software verifiers on the augmented dataset to classify benchmarks that contain memory safety vulnerabilities. * A comparison of different prompt engineering techniques to optimise the repair performance of large language models. In this respect, we propose a few solutions to the issues of limited context windows, code formatting and including feedback from compilers and software verifiers. * Preliminary results on the repair performance of large language model. Specifically, we analyse their output along four axes: correct syntax, relevance to task, compilability and successful repairs. * Comparing and finding the best format of the history in iterative code repair affects iterative code repair performance. * Impact of temperature on iterative code repair. The structure of this report is the following. In Section 2, we give some background on _NeuroCodeBench_, common safety vulnerabilities found in neural networks and the program mutation techniques we employ. In Section 3, we detail our methodology towards creating an AI code dataset, we present our empirical results in using ESBMC to label the dataset. In Section 4 we introduce a wide variety of state-of-the-art prompting techniques, we show how to employ them for code repair, and compare their code repair performance with ChatGPT. This report is to be considered as the official documentation of the public software repository at [https://github.com/emanino/plain_c_nn_benchmark](https://github.com/emanino/plain_c_nn_benchmark), the staging repository can be found at [https://github.com/Yiannis128/plain_c_nn_benchmark](https://github.com/Yiannis128/plain_c_nn_benchmark)."
    },
    {
      "title": "2 Background",
      "text": ""
    },
    {
      "title": "Neurocodebench",
      "text": "_NeuroCodeBench_ is a plain C benchmark of neural network implementations designed for formal software verification. In general, mainstream machine learning libraries (e.g., PyTorch [3] and Tensorflow [5]) have an opaque multi-language interpreted structure that can be easily tested [22, 18],but does not lend itself to automated software verification. For this reason, _NeuroCodeBench_ opts for micro-controller frameworks, where the network's source code is fully available. More specifically, it uses two existing tools for converting high-level neural network specifications to standalone C code: onnx2c[37] and keras2c[13, 14]. Since November 2023, _NeuroCodeBench_ is part of the official benchmark for the international software verification competition (SV-COMP).1 Footnote 1: [https://gitlab.com/sosy-lab/benchmarking/sv-benchmarks/-/merge_requests/1456](https://gitlab.com/sosy-lab/benchmarking/sv-benchmarks/-/merge_requests/1456) In Table 1, we give an overview of the neural architectures of _NeuroCodeBench_. These have been designed to cover several use cases in machine learning and engineering, as detailed below: Hopfield Networks.For a long time, it has been known that certain types of recurrent neural networks can act as error-correcting decoders [2, 9]. The main idea is encoding a sequence of \\(d\\) bits into a vector \\(x\\in\\{\\pm 1\\}^{d}\\), and letting the neural network flip the sign of the incorrect entries. Here, we choose Hopfield networks with Hebbian weights since their properties are well understood [2]. Specifically, we build networks reconstructing a single pattern \\(x=(1,\\ldots,1)\\). We vary the pattern length in \\(d\\in\\{4,8,16,32,64\\}\\) and the number of recursions in \\(t\\in[1,4]\\). For compatibility with keras2c[13, 14], we use Softsign and TanH activations (see Table 1) rather than traditional sign activations [2]. Polynomial Approximation Networks.In several engineering areas, neural networks are used to approximate the transfer function of electrical components [40, 27]. We emulate this process by defining a hypothetical polynomial component \\(f(x)=0.125x^{4}-0.25x^{3}-0.75x^{2}+x+0.5\\) with an oscillating transfer function. Then, we create a training set by uniformly sampling \\(f(x)\\) in \\(x\\in[-2,3]\\) and train 16 different feedforward ReLU networks \\(\\hat{f}(x)\\). The smallest has four layers with four neurons each, and the largest has a single hidden layer with 1024 neurons (see poly_approx category in Table 1). VNN-COMP Networks.Since its first edition in 2020, the International Verification of Neural Networks Competition (VNN-COMP) has published all its benchmarks [31]. These benchmarks do not contain implementation details since they target a higher level of abstraction (real number arithmetic, no memory model). To provide a reference implementation, we translate the networks from ONNX format [12] to C with onnx2c[37]. From the 2022 edition [31], we select two categories with small networks (see Table 1): reach_prob_density[28] and reinforcement_learning[35]."
    },
    {
      "title": "Safety Properties",
      "text": "Originally, _NeuroCodeBench_[26] was designed with the purpose of testing software verifiers. As such, it features very challenging verification problems centered around reachability properties [17]. In contrast, our AI code dataset is meant to test the ability of large language models to repair code. \\begin{table} \\begin{tabular}{|r|c|c|c|c|c|c|c|} \\hline Neural Network Category & Inputs & Outputs & Layers & Neurons & Activations & Architecture & Conversion \\\\ \\hline hopfield\\_nets & 4–64 & 4–64 & 1 & 4–64 & Softsign/TanH & Recurrent & keras2c \\\\ poly\\_approx & 1 & 1 & 1–4 & 16–1024 & ReLU & Feedforward & keras2c \\\\ reach\\_prob\\_density & 3–14 & 3–14 & 2–3 & 64–192 & ReLU & Feedforward & onnx2c \\\\ reinforcement\\_learning & 4–8 & 2–8 & 2 & 128–512 & ReLU & Feedforward & onnx2c \\\\ \\hline \\end{tabular} \\end{table} Table 1: Neural networks in _NeuroCodeBench_. The “Layers” and “Neurons” columns refer to the hidden layers only. The networks in hopfield_nets have a number of iterations between 1 and 4. As the majority of real-world bugs [24] are the result of programming mistakes, in this report, we focus on memory safety properties instead. Indeed, memory safety is the leading cause of software vulnerabilities [29]. Here, we include a brief description of the different safety properties. Reachability.For neural networks, we define reachability properties as necessary conditions over a set of inputs \\(x\\in\\mathcal{X}\\) and outputs \\(y\\in\\mathcal{Y}\\), where \\(y=f(x)\\) and \\(f\\) is the neural network [6]. In general, a reachability property would take the following form: \\[\\forall x\\in\\mathcal{X},y=f(x)\\implies y\\in\\mathcal{Y} \\tag{1}\\] As a concrete example, consider the following. Take an error-correcting Hopfield network, which is trained to reconstruct the code \\(y=(1,1,\\ldots,1)\\) in the presence of input noise. Assume that all input vectors \\(x\\) of dimension \\(d\\) have at most three flipped inputs, i.e., \\(\\mathcal{X}=\\{x|x_{i}\\in\\{\\pm 1\\}\\wedge\\sum_{i}^{d}x_{i}\\geq d-6\\}\\). The neural network is always able to reconstruct the correct code if Equation 1 holds for \\(\\mathcal{Y}=\\{y=(1,1\\ldots 1)\\}\\). Memory safety.Properties encompass checking vulnerabilities regarding NULL pointer dereferences, out-of-bounds accesses, double-free, and memory leaks. In more broad terms, we check for situations where an illegal memory read or write can occur. ESBMC can check these properties by default by enabling the correct flags. For running the experiments, the following flags were enabled: --interval-analysis --goto-unwind --unlimited-goto-unwind --incremental-bmc --state-hashing --add-symex-value-sets --k-step 2 --floatbv --unlimited-k-steps --memory-leak-check --context-bound 2 --timeout 300 --includes --Inetworks. These properties instruct ESBMC to encode memory safety criteria within the symbolic execution engine, ensuring that (1) all memory deallocations are valid, (2) all pointer dereferences are valid, and (3) all allocated memory is accurately tracked, encompassing pointed to or deallocated."
    },
    {
      "title": "Generative Language Models",
      "text": "AI has been used as mutation generators for automatically repairing code. In recent years, Large Language Models (LLMs) have taken precedence [20], however, the code that they generate is not secure and is filled with vulnerabilities [8]. LLMs are a type of Recurrent Neural Network (RNN), an AI architecture that uses an encoder/decoder architecture, typically along with an attention mechanism [38]. Opposite to conventional Recurrent Neural Networks (RNNs) that compute their values over a sequence of hidden states, LLMs with the encoder/decoder architecture can be heavily parallelized, allowing for more training in less time [38]. Figure 1 shows a diagram of the layout of LLMs. Furthermore, these developments have paved the way for OpenAI's _GPT-3.5-Turbo_, a closed source LLM. GPT-3.5-Turbo is a 175 billion parameter autoregressive language model [7] that will be used in the experiments in this report."
    },
    {
      "title": "Software Verification",
      "text": "There are many methods of verifying software, in this report we use Bounded Model Checking (BMC) to detect if the code in the experiments contains any memory vulnerabilities [16]. BMC is a technique used where a given property is checked at a specified depth in a system [16]. To elaborate, given a state transition system, and a property, BMC unrolls the system and translates it to a verification condition, if the verification condition is true, then the system is satisfiable up to that specified bound [16]. Figure 2 visualizes how BMC works. The Efficient SMT-based Bounded Model Checker (ESBMC) is such a software verifier that uses bounded model checking to prove mathematically up to a certain depth the presence of software vulnerabilities [16]. This report will use ESBMC to test samples of AI C code for the presence of vulnerabilities."
    },
    {
      "title": "3 Creating An Ai Code Dataset",
      "text": "In order to fine-tune and evaluate the performance of large language models in repairing AI code, we need a dataset of neural network implementations. Since existing ones are fairly small, amounting to only a few hundred samples [26], we turn to data augmentation techniques to greatly expand their size. This is needed to obtain statistically significant evaluation metrics."
    },
    {
      "title": "Program Mutation",
      "text": "The test suite chosen for this task is the \"practical mutation testing tool for C and C++\" Mull [19]. Mull comprises two key components, the compiler plugin and Mull Runner, an entirely separate program [19]. The compiler plugin generates program mutations at the compilation stage and injects them into the LLVM bit code [19]. The mutations enabled in Mull's configuration files are Figure 1: Diagrammatic layout of a transformer model architecture [38]. injected into the IR of the program but are hidden behind conditional flags that allow them to be individually enabled during runtime [19]. Mull Runner can run the program repeatedly with each mutation condition enabled [19]. Mull runner will then check how the injection of the mutation affects the program's execution [19]. Each mutation can be saved into a patch file and stored on disk [19]."
    },
    {
      "title": "Data Augmentation",
      "text": "The _NeuroCodeBench_ dataset [26] was expanded by a pipeline that aimed to automate the dataset generation as much as possible. The sheer size of the dataset is the reason for this, as it is not realistic to create a dataset with 81k samples in such a short time span using manual labor methods. The pipeline can be divided into three distinct sections that process and transform the base dataset from one form to the next. The process begins by building the base sample source code, followed by generating mutation patches using a mutation test suite; finally, the last step consists of using ESBMC to classify the samples. Figure 3 illustrates an overview of this pipeline process. Building the Base Samples.We follow the same procedure outlined by the _NeuroCodeBench_ instructions for building the base dataset. The base dataset contains 505 samples. There are 79 Hopfield networks, 97 polynomial approximation networks, 34 probability density networks, and 295 reinforcement learning networks. Generating Mutation Patches.Having obtained the base dataset, we use a mutation testing suite to generate patches for each sample that modify the code in a small way (see Section 3.1). While Mull is designed to evaluate the quality of test suites for C programs [19], we have managed to use it to multiply the size of the base dataset by around 80x, reaching 81129 total samples. This is because we use the diff tool \\(d\\) to apply each patch generated \\(y_{x}\\) to its original file \\(f\\), yielding a new transformed file \\(f^{\\prime}\\) in the following process \\(f^{\\prime}=d(f,y_{x})\\). Patches are produced through the Mull runner program $m$ via the following procedure: $y=m(f, c)$, where $c$ denotes Mull's configuration containing the specified mutations allowed for patch creation, and $y$ represents the collection of patches generated by the Mull compiler plugin. Creating a new sample for every patch Figure 3: Overview of the _NeuroCodeBench_ expansion pipeline. It consists of three key stages. Figure 2: A diagram visualization of how bounded model checking works. The visualization represents a state transition system \\(M\\), a given property \\(\\phi\\), and a verification condition \\(\\psi\\)[15]. expands the size of the samples from 505 to 81129 samples; this increase depends on the amount of mutations that are enabled in the Mull configuration. _NeuroCodeBench_ samples also include calls __VERIFIER_assume(expression) and __VERIFIER_assert(expression) which instruct the verifiers to verify certain safety properties, as discussed in Section 2.2. As discussed previously, removing those statements is necessary as this dataset focuses on memory safety only2. Footnote 2: Note that the expanded sample dataset may contain some duplicates after the assert declarations are removed. Classifying the new Dataset.With the expanded dataset generated, ESBMC can then be used to verify the safety of each sample. As we are verifying only memory safety properties, the number of samples ESBMC can classify should be higher than when verifying the additional safety properties of the base dataset."
    },
    {
      "title": "Empirical Results",
      "text": "This section explores the experimental setup and results of processing this extended _NeuroCodeBench_ dataset using the ESBMC verifier. The experiments were conducted on 6 machines with Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz, 198 GB RAM. Classification ResultsFigure 3(a) shows classifications assigned by ESBMC to each sample in each category. Note that each category exhibits different ratios of safe, unsafe, and unknown verdicts. Hopfield networks have the smallest amount of safe cases, compared to the amount of unsafe and unknown cases that they have, this is possibly due to the complexity of Hopfield networks, as they are recurrent neural networks. Probability density and reinforcement learning networks have a much higher portion of safe and unsafe cases compared to unknowns. These observations suggest that ESBMC is more capable at processing the latter two network types, possibly due to their simpler structure, as the symbolic execution would be less demanding due to possibly having less loop unrolling. Verification TimeFigure 3(b) shows a box-plot of each category, illustrating various information about the verification time for safe or unsafe cases. As can be observed, Hopfield and polynomial approximation neural networks are much faster to complete on average than probability density and reinforcement learning graphs. One possible explanation is that these two categories of benchmarks use a different neural network library (keras2c) than the others (onnx2c). However, more data is Figure 4: Performance of ESBMC on 25952/81129 mutated programs, by neural network category. needed to get a conlusive answer. Finally, note that while ESBMC could not successfully classify various samples, a sizable portion was processed successfully within a range of a few minutes."
    },
    {
      "title": "Lessons Learned",
      "text": "Overall, ESBMC is relatively efficient in verifying neural network code containing memory safety vulnerabilities. This contrasts with previous work on _NeuroCodeBench_, which showed that software verifiers such as ESBMC struggle to reason over reachability properties of neural networks. Still, the moderately large ratio of unknown verdicts, together with the long timeout of 300 seconds (or 5 minutes), leave room for improvements in terms of verification time. In the future, we might consider a portfolio approach combining falsification tools (e.g., fuzzers) with verification tools such as ESBMC. For the time being, the sizeable number of conclusive verdicts (safe or unsafe) still allows us to build a large AI code dataset. With this dataset, we can explore the effectiveness of LLMs in recognizing potential vulnerabilities and repairing unsafe AI programs."
    },
    {
      "title": "4 Repairing Ai Code With Large Language Models",
      "text": "The AI code dataset we generate in Section 3 is a representative instance of _out-of-distribution_ data. Indeed, while the original _NeuroCodeBench_ is in the public domain, it was released in late 2023 [26], and it is thus unlikely to have been included in the training set of most state-of-the-art large language models [34]. In addition, our automated mutation technique further ensures that our AI code dataset looks very different from any piece of software the current large language models have been trained on. Against this background, we pose the following research question: _can off-the-shelf large language models spot the memory vulnerabilities we introduced in Section 3 and eliminate them by repairing the code?_ Figure 5 displays a visualization of the question."
    },
    {
      "title": "Prompt Templates",
      "text": "The consensus in the natural language processing community is that the performance of large language models depends on our ability to prompt them effectively [39]. This has sparked much interest in _prompt engineering_ techniques, which have sometimes entirely replaced the need to finetune a large language model [39]. Here, we list a few state-of-the-art prompt engineering strategies relevant Figure 5: Flowchart of a single attempt at repairing AI C Code with the LLM. [MISSING_PAGE_FAIL:9] The following source code contains a memory vulnerability {source} The following is the output of ESBMC describing the vulnerability {esbmc}. Fix the source code. Fix the source code: {source} {esbmc} ``` Listing 4: Simple Prompt Template with ESBMC Output before Source Code ESBMC output describes a memory vulnerability in the source code; the following is ESBMC output: {esbmc} The following is the vulnerable source code: {source} Fix the source code. Fix the source code: {esbmc} {source}"
    },
    {
      "title": "4.1.3 Persona Prompts",
      "text": "Recent research has found that large language models produce better output when performing according to a specific role [39]. This prompt engineering technique is usually called _persona_ and creates many optimization opportunities. In our case, we ask the following research question: * _Does the role we assign to the large language model make a difference_. Here are the six roles we compare: 1. Programmer with 1 million years of experience; 2. Senior software engineer; 3. Automated code repair tool; 4. Artificial intelligence that specializes in repairing C programs; 5. The smartest human in the universe; 6. Dog. Note that roles 1 and 2 are humanoid expert roles. Roles 3 and 4 are robotic expert roles. Roles 5 and 6 are wildcards, which were added as a control. These roles are inserted in the following prompt templates instead of the placeholder {role}. ``` Listing 5: Persona Prompt Template no ESBMC Output: You're a {role}. You'll be shown some C code. Repair the code and display it. The code is {source} From now on, act as an {role} that repairs AI C code. You will be shown AI C code. Provide the repaired C code as output, as would an {role}. Aside from the corrected source code, do not output any other text. The code is {source} You're a {role}. You'll be shown some C code, along with ESBMC output. Repair the code and display it. The code is {source} The ESBMC output is {esbmc} From now on, act as an {role} that repairs AI C code. You will be shown AI C code, along with ESBMC output. Pay close attention to the ESBMC output, which contains a stack trace along with the type of error that occurred and its location. Provide the repaired C code as output, as would an {role}. Aside from the corrected source code, do not output any other text. The code is {source} The ESBMC output is {esbmc} ``` \\begin{tabular}{} Listing 6: Person Bromp Template with ESBMC Output after Sentence Code You're a {role}. You'll be shown some C code, along with ESBMC output. Repair the code and display it. The ESBMC output is {esbmc} The source code is {source} From now on, act as an {role} that repairs AI C code. You will be shown AI C code, along with ESBMC output. Pay close attention to the ESBMC output, which contains a stack trace along with the type of error that occurred and its location. Provide the repaired C code as output, as would an {role}. Aside from the corrected source code, do not output any other text. The ESBMC output is {esbmc} The source code is {source}"
    },
    {
      "title": "4.1.4 Source Code",
      "text": "Large language models can only process a limited input text length. In the case of ChatGPT [34], which we use in our experiment of Section 4.2, the maximum input length is 16K tokens. As such, it is often impossible to feed the large language model the whole code to be repaired. To overcome this limitation, we employ the following two strategies. Contextual.Most software verifiers, including ESBMC, do not just report the presence of a vulnerability, but they also include the line of code where it triggered an unwanted behavior. We cut the largest code window around the reported vulnerability to fit as much code as possible in the available space. More specifically, we select a window that contains around 90% lines of code before the vulnerability and 10% after. One line.For memory safety vulnerabilities, such as array-out-of-bounds, it is likely that modifying the very same line of code that triggered the verifier is sufficient to repair it. For this reason, we run a full set of experiments showing the large language model with only one line of code."
    },
    {
      "title": "4.1.5 Verifier Feedback",
      "text": "The output of most software verifiers contains a bug report with the violated property and a full bug trace with a concrete value for each state (counterexample). Our question is whether these pieces of information are useful to the large language model. For this reason, we run two sets of experiments that include either the full counterexample or just the short bug report. We observe no significant difference between these."
    },
    {
      "title": "4.1.6 Prompt Combinations",
      "text": "In summary, we have the following combinations of prompts: * **Without Verifier Feedback.* * (2 simple + 2 persona * 6 role) * 2 source * **With Verifier Feedback.* * (1 old + 4 simple + 4 persona * 6 role) * 2 feedback * 2 source for a total of \\((2+2*6)*2+(1+4+4*6)*2*2=144\\) prompts."
    },
    {
      "title": "Comparing Templates",
      "text": "This section compares the repair performance of LLMs using the different prompting strategies listed above. Overall, we measure the LLM performance on four metrics of increasing difficulty: * **Syntax.** Do the repair patches contain C code? * **Relevance.** Do the repair patches match the input source code? * **Compilation.** Do the repair patches compile? * **Verification.** Do the repair patches solve the memory safety vulnerability? Experimental SetupThe experiments were run on a distributed computational infrastructure. For the duration of the experiments, GPT-3.5-Turbo was used as the LLM of choice, specifically gpt-3.5-turbo-0125, with a temperature of 1.0, which is the default set by the owners of the LLM API; no defaults were changed. ESBMC v7.4.0 was run on a server with Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz, 198 GB RAM. For each of the 144 prompt combinations listed in Section 4.1, we run 100 random samples from the AI code dataset we generate in Section 3, for a total of 14400 runs. Syntax of the LLM PatchesAs a first sanity check, we want to confirm that the LLM produces C code rather than a mixture of textual instructions and small code snippets. We do so by running the automated code detector used in Visual Studio3 and extracting the score associated with C or C++. The results in Figure 6 illustrate markedly higher scores for a prompt containing a contextual window of source code rather than a single line. This is expected as the detector struggles to detect the language of short code snippets. Still, we can see that persona prompts yield a higher percentage of C-like LLM outputs for contextual strategy than other prompts (see Figure 5(a). Also, adding the ESBMC output to the prompt makes things worse for contextual strategy but better for single lines of code (Figure 5(b)). Footnote 3: [https://github.com/yoeo/guesslang](https://github.com/yoeo/guesslang) Syntax TakeawayLarge language models produce C-like code fairly consistently. Relevance of the LLM PatchesWe know that the vulnerabilities introduced by Mull [19] are due to a few character changes only, e.g., replacing < with <=. Thus, we expect a successful patch to copy most of the input code verbatim, except for a small difference. To see whether the LLM repair matches our expectations, we measure how many characters the input and output code have in common (not counting whitespaces) and report the results in Figure 7. The contextual strategy \\begin{table} \\begin{tabular}{|c|c c c c c c c c|c c c|} \\hline & old & simple & pers-0 & pers-1 & pers-2 & pers-3 & pers-4 & pers-5 & none & before & after \\\\ \\hline contextual & 1.50\\% & 1.25\\% & 0.25\\% & 0\\% & 1.58\\% & 2.16\\% & 1.16\\% & 1.25\\% & 0.14\\% & 1.32\\% & 1.82\\% \\\\ \\hline one line & 63.0\\% & 16.91\\% & 61.25\\% & 61.41\\% & 31.8\\% & 56.3\\% & 38.9\\% & 52.0\\% & 54.9\\% & 41.7\\% & 40.0\\% \\\\ \\hline \\end{tabular} \\end{table} Table 2: Percentage of repair patches that make the code compile. Asking the LLM to repair only one line of code yields patches that compile more consistently. \\begin{table} \\begin{tabular}{|c|c c c c c c c|c c c|} \\hline & old & simple & pers-0 & pers-1 & pers-2 & pers-3 & pers-4 & pers-5 & none & before & after \\\\ \\hline contextual & 1.50\\% & 1.25\\% & 0.25\\% & 0\\% & 1.58\\% & 2.16\\% & 1.16\\% & 1.25\\% & 0.14\\% & 1.32\\% & 1.82\\% \\\\ \\hline one line & 0.00\\% & 4.25\\% & 5.50\\% & 5.83\\% & 6.75\\% & 8.25\\% & 10.3\\% & 10.5\\% & 5.32\\% & 9.57\\% & 7.10\\% \\\\ \\hline \\end{tabular} \\end{table} Table 3: Percentage of repair patches that are successfully verified. All the patches that compile (see Table 2) are also successfully verified when using the contextual strategy. Figure 6: Distribution of probability scores from the C/C++ detector used in Visual Studio on the LLM repair patches. Persona prompts cause the LLM to produce C/C++ code more consistently. Figure 7: String match between source code in the prompt and LLM output patch (ignoring whitespaces). When using the contextual strategy, the LLM tends to omit large swathes of the original code. has a low match, as the LLM sometimes reports only a few lines of code around the bug rather than copying the whole input code. One-liners (single) have a better match as it is easier for the LLM to repeat what we feed as input. The anomaly for single and simple prompts in Figure 6(a) may be due to temporary changes in the ChatGPT backend that are out of our control rather than a real effect of our prompting technique [10]. Again, adding feedback from ESBMC to our prompt seems to lower the performance of the LLM (see Figure 6(b)). **Relevance Takeaway.** Large language models skip over crucial portions of the input code. Compiling the Repaired CodeEven though we explicitly ask the large language model to produce valid C code as its output (see Section 4.1), there is no mechanism to guarantee that it does so. For this reason, we check whether introducing the repaired patch back into the original code yields a piece of code that compiles. The results in Table 2 show that most of the runs did not produce compile code. However, there is a marked difference between how much source code is shown to the large language model. Indeed, letting the large language model repair only one line of code yields around 50% compilable patches across all settings. **Compilation Takeaway.** Multi-line patches hardly compile, one-liners have a 50% chance. Verifying the Repaired CodeFinally, we evaluate whether the repaired code passes all checks for memory safety vulnerabilities. We do so by invoking ESBMC on the repaired code and counting how many programs are successfully verified (see Table 3). All the repair patches generated using the contextual strategy display an interesting property: if the code compiles, it is also successfully verified. However, while one-line repair patches might cause a verification failure or a time-out, they yield more successful repairs overall. In this respect, persona prompt techniques (no matter the specific role chosen) are the best. **Verification Takeaway.** The best setting achieves less than 10% successful repairs."
    },
    {
      "title": "Comparing Individual Prompts",
      "text": "The following section will compare all the individual prompts executed in the experiments and outline the best prompt for the Contextual and Single-line experiments. The grouping of the experiments and the use of \"prompt\" in this section refers to substituting the role and the ESBMC output type into a prompt template. The notation of a prompt is \\(x.y.z\\) where \\(x\\) describes the prompt index, \\(y\\) describes the role index used4, and \\(z\\) describes the ESBMC output type. Footnote 4: Incidentally, non Persona prompts are always index 0 for role. ContextualFigure 7(a) shows the performance of the best prompts for the Contextual experiments. As evaluated in Section 4.2, it was discovered that Persona prompts performed overall better than the simple prompts; however, surprisingly, the Persona prompt assigned roles did not seem to affect the performance in an expected way. One note is that many prompts in the Contextual experiments did not successfully repair even one sample, so they have been omitted and not shown in Figure 7(a). Two prompts perform the best, they both use prompt template 9. The roles are \"Automated CodeRepair Tool\" and \"Dog\". The verifier output can both be violated property (VP) or counterexample (CE), as they both successfully repaired 4% of the samples. Single LineRegarding the best prompt in Figure 7(b), the flipped Persona prompt template, index 11, was the most successful, at around 18%. The role that was used by the prompt was \"Automated Code Repair Tool\". The ESBMC output type used in the prompt was Counterexample (CE). The implications of these results suggest that, while assigning a specific role to the LLM is important, the role assigned may not impact performance consistently. However, it is worth noting that the \"Automated Code Repair Tool\" role seemed to have been among the best prompts in both Contextual and Single Line experiments. Also, longer prompts seemed to perform better in both Contextual and Single Line experiments. **Best Prompt Takeaway.** The best prompt is the second prompt in Listing 7 in Section 4.1.3. Using the role \"Automated Code Repair Tool\"."
    },
    {
      "title": "Further Fine-Tuning Of The Best Prompts",
      "text": "Prompts can be further refined to achieve better results. Here, we focus on the best two prompts found in Section 4.3: 9 and 11. Specifically, we consider two potential improvements. First, in Appendix A, we explore the role of backticks, which are usually added as a separator around code snippets. Second, in the current Section, we present two prompts with longer and more detailed instructions, which in previous experiments have yielded better performance (see Section 4). More specifically, we add further instruction to prompt templates 9 and 11 to do an additional 3 things. Firstly, we explicitly state to use the verifier output to find out what the fault of the sample is. Secondly, we instruct the LLM to repair only one line of code. Lastly, we instruct the LLM to minimize the size of the changes. The modified prompt templates, referred to as 9-2 and 11-2 can be seen below: Figure 8: Prompt comparison for the best prompts. The Contextual Figure presents results greater than or equal to 3. The Single Line Figure presents results greater than or equal to 10. From now on, act as an {role} that repairs AI C code. You will be shown a line of AI C code, along with ESBMC output. Pay close attention to the ESBMC output, which contains what type of error has occurred and its location. Provide the repaired C code as output, as would an {role}. Aside from the corrected line of source code, do not output any other text. The code is {source_code} The ESBMC output is {esbmc_output} Guideline: Always prefer to repair using a single line of C code, unless necessary. Guideline: Read the error in the ESBMC output and try to repair the fault. From now on, act as an {role} that repairs AI C code. You will be shown a line of AI C code, along with ESBMC output. Pay close attention to the ESBMC output, which contains what type of error has occurred and its location. Provide the repaired C code as output, as would an {role}. Aside from the corrected line of source code, do not output any other text. The ESBMC output is {esbmc_output} The source code is {source_code} Guideline: Always prefer to repair using a single line of C code, unless necessary. Guideline: Read the error in the ESBMC output and try to repair the fault. The impact of the new prompts will be presented in the following Section."
    },
    {
      "title": "Lessons Learned",
      "text": "As shown in Section 4.2, repairing out-of-distribution code with large language models appears to be a delicate endeavour. On the one hand, some advanced prompt engineering techniques, e.g., assigning the model a persona role, seem to improve the success rate overall. However, the specific role does not seem to matter: the performance of \"Dog\" is more or less the same as that of a more credible role such as \"Automated code repair tool\". On the other hand, the choice of which information we show to the model seems crucial for obtaining good-quality patches. Perhaps surprisingly, providing feedback from a verifier about the nature of the vulnerability makes the performance worse. At the same time, choosing how many lines of code we include in the prompt is crucial for generating patches that compile and remove the vulnerabilities. Conversely, a cleverer choice of which lines of code to include in the prompt may be one of the most promising avenues for improvement. In this respect, formal methods such as static analysis can greatly help in identifying such program subsets. Finally, the overall percentage of successfully repaired programs is just below 18%, even under the best settings. This suggests that it would take a large language model several attempts to propose a correct patch, thus greatly increasing the computational cost of automated program repair. We plan to minimize the cost of such an iterative approach in Work Package 3."
    },
    {
      "title": "5 Iterative Automated Program Repair",
      "text": "Allowing the LLM to have multiple attempts at repairing the faulty sample may yield an increase in automated code repair (APR) performance. Figure 9 shows a diagram of how the iterative repair would function. It is an extended version of Figure 5 that adds an iterative loop element with the tracking of attempts. If the total attempts exceed the limit, in this case 5, then the verification fails. With that in mind, we propose the following research questions:1. _Do large language models combined with formal verification increase automated program repair performance of AI C code when given multiple attempts to do so?_ 2. _Does showing the history of patches to the LLM improve the performance of iterative APR?_ 3. _What is the optimal way to show the history to the LLM?_ 4. _What is the optimal temperature to conduct APR of AI C Code?_ Section 5.1 will describe the implementation details of the experiments. Section 5.2 will describe the experimental setup used to conduct the experiments. Section 5.3 will analyze and interpret the results."
    },
    {
      "title": "Methodology And Workflow",
      "text": "By adding the iterative loop element to the experimental setup, we expect an increase in the repair performance since the LLM has multiple attempts to repair the artifact successfully. ESBMC-AIThe automated program repair functionality in ESBMC-AI builds a list of pairs of previously attempted repairs consisting of the source code and verifier output on top of the initial source code and verifier output [8]. The iterative loop is described in Algorithm 1[8]: 1. The message list initially consists of the initial prompt and the source code, along with the verifier output, being substituted in. In the algorithm, Line 2 initializes an empty message prompt, and on Line 8 gets assigned to as described. 2. The LLM is sent the prompt instructing it to repair the source code, using the verifier output as a guide, as seen in Line 9. 3. The LLM returns the patched source code, and ESBMC is then tasked with verifying it is free of memory violations. If it is correct, then the APR process has successfully concluded. Figure 9: Diagram of the iterative repair algorithm of ESBMC-AI. 4. If the verifier confirms that the patched code is still wrong, a new message is added to the message list consisting of a new prompt template with the latest source code, and new verifier output is also included. 5. The process repeats (from step 2)."
    },
    {
      "title": "5.1.1 Prompt Settings",
      "text": "Prompt TemplatesPrompts 9 and 11 will be used for the iterative repair experiments, along with prompts 9-2 and 11-2 from Section 4.4. Additionally, the old ESBMC-AI prompt will also be used in the experiments discussed in Section 4.1.1; the aim of including such a prompt is to observe the improvement in performance that the new prompts provide in an iterative environment, as it allows for an additional baseline measurement. Source CodeDue to the introduction of the iterative APR loop, the prompt structure becomes incompatible with the contextual experiments conducted in Section 4. The reason for this is that the contextual experiments were conducted to maximize the amount of source code and verifier output placed into the prompt. By its very nature, the creation of a contextual source code system with iterative loop mechanics would not work, as each iteration would require the space that the initial prompt had already taken. For the experiments in this section, the one-line format of displaying the source code has been chosen, as described in Section 4.1.4. The one-line experiments were also the group of experiments that performed better than contextual in every metric, as seen in Section 4.2. Verifier OutputIn the previous experiments, the output of the verifier was either to show the counter-example stack trace and violated property, denoted as CE, or show only the violated property section of the verifier output, denoted as VP, as discussed in Section 4.1.5. Due to the structure of the AI C Code being repaired, the stack-trace generated is very large. Thus, the context window of the LLM fills up without going through all the cycles of the iterative APR loop, making running the AI C code experiments infeasible. For the iterative experiments, the VP output type was chosen,as it discards the large stack-trace produced by the verifier, instead keeping the violated property that contains a copy of the statement that the error occurs in, along with the type of error that the verifier had detected during the verification process."
    },
    {
      "title": "5.1.2 Message History",
      "text": "The iterative APR process introduces the concept of a message history: the collection of previous messages sent to the LLM. The three methods of representing current and previous messages are denoted as _Latest State Only_, _Forward History_, and _Reverse History_. The next paragraphs provide detailed explanations of each. Figure 10 visually represents each format. Latest State Only Experiments (LSO)The LSO experiments are conducted to establish a baseline performance for the iterative APR approach. The iterative repair cycle of ESBMC-AI is modified to discard the history at each process of the APR loop. Effectively making it so that any previous repairs, aside from the last patch and verifier output, are discarded and not visible to the LLM. Forward History ExperimentsThe Forward History experiments construct the prompt sent to the LLM such that the messages are in chronological order. In addition, the experiments store information from previous messages. The LLM gets a complete history of prior messages from the start of the repair process until the end. Reverse History ExperimentsMuch like the Normal History experiments described previously, the Reverse History experiments use the same principle. However, the main difference is that the messages are reversed. The idea behind this is that the original state of the code would be displayed last in the message list. The last message read by the LLM would be the original state, which can potentially stop the LLM from drifting too far from the original code state."
    },
    {
      "title": "Experimental Setup",
      "text": "This section will explore the results of the iterative code repair experiments. HardwareThe iterative loop experiments will be conducted using the following environment: * The experiments were conducted on a server running both Intel Ivybridge and Haswell CPUs with 32 GB of RAM. * ESBMC-AI version 0.5.0rc4 * ESBMC version 7.4.0 64-bit x86_64 linux * The LLM chosen is GPT-3.5-Turbo * Each experiment was conducted over the 100 samples that the previous experiments used, in Section 4. Each experiment will be carried out over the following temperatures: _0.0, 0.4, 0.7, 1.0, 1.3_. Each of the temperatures aims to determine which temperature constitutes the best performance for AI C code APR."
    },
    {
      "title": "Experimental Evaluation",
      "text": ""
    },
    {
      "title": "5.3.1 Successful Verifications Per Attempt",
      "text": "Figure 11 illustrates the percentage of successful repairs at each attempt after the first, the total being the overall amount of samples repaired. The Forward History experiment performed best, followed by the LSO experiment and the Reverse History. The Forward History experiments had the most successful attempt at repairing the AI C code on the first try; from the 1st retry and onwards, the number of successful code repairs is significantly lowered. LSO had a similar number of 1st and 2nd attempt successful repairs of samples, dropping sharply on the 3rd attempt. This could be caused by the LLM changing the state of the line where the fault occurs too much by the 2nd attempt. Thus making it extremely unlikely that a correct solution will be found. Reverse History had the lowest performance and had no successful repairs by the 2nd retry. Figure 10: The 3 message history formats used in the experiments visualized. **Optimal Attempts Takeaway.** There is a sharp drop in successfully repairing a sample after the initial attempt."
    },
    {
      "title": "5.3.2 Successful Repairs Per Temperature",
      "text": "Figure 12 illustrates each experiment set's successful repairs per temperature. The main observation of each experiment is that temperature affects the number of successful repairs differently. Varying the temperature does not increase the repair performance for LSO. In contrast, the Forward History experiments have an inverse correlation between increasing the temperature and getting more repairs. In other words, the lower temperature values, which make the LLM behave more deterministically, yield higher repair performance. Interestingly, the opposite is true for Reverse History. While the performance of Reverse History is the lowest of the three experiments, there is a direct correlation between the number of successful repairs and a higher temperature. Figure 11: Successful verifications by attempt. Figure 12: Successful verifications by temperature. **Temperature Takeaway.** A lower temperature yields a higher repair accuracy for AI C Code APR using LSO or Forward History."
    },
    {
      "title": "5.3.3 Successful Verifications Per Prompt",
      "text": "Figure 13 illustrates the percentage of successful repairs per prompt for the three experiments. These percentages represent the entire range of temperatures. The LSO experiments show prompts 11 and 11-2 performing the best, possibly due to the added context of the prompt. As the LSO experiments do not contain historical patches, they help repair performance by providing additional instruction. Interestingly, the Forward History experiment shows the opposite; prompts 9 and 11 perform best. Prompts 9 and 11 were the best in Section 4 experiments. The reverse history shows no significantly better performance between any of the prompts; in general, it performs the worst of all the experiments. In all three experiments, the Old prompt failed to repair many prompts successfully; however, the reverse one was the most successful. Furthermore, we can observe the following if the new prompts are grouped into two types: modified and unmodified. The unmodified prompts have the highest amount of successful repairs in the Forward History experiments. However, the modified prompts perform better in the LSO experiments. In the end, the best prompts are, by far, prompt 9 and prompt 11 in the Forward History experiments. The most stable prompts are prompts 9-2 and 11-2, due to the added instructions, as they perform comparatively between the LSO and Forward History experiments. **Best Prompts Takeaway.** The best prompts are prompt 9 and prompt 11."
    },
    {
      "title": "Lessons Learned",
      "text": "Iterative APR allows the LLM to make multiple attempts to fix a source file. As seen from the experiments, the best results of non-iterative APR were \\(\\sim\\)18%, while the iterative approach explored in Section 5 increased the successful repairs to \\(\\sim\\)25%. Throughout all the experiments conducted, the option to produce more repaired samples is to use a lower temperature, with 0.0 being the best for prompts 9 and 11. The best number of retries is 2, which means that the LLM has three attempts Figure 13: Successful verifications by prompt. at resolving a single fault. The best type of history is the Forward History, where the LLM gains an advantage from past patches. LSO performed second best, but missing the history of repairs in the prompt leaves the LLM directionless in repairing the code. In summary, we can answer our research questions as follows: 1. _Do large language models combined with formal verification increase automated program repair performance of AI C code when given multiple attempts to do so?_ LLMs with formal verification increase the automated program repair performance when given multiple attempts. The best number of attempts is 3 for prompts 9 and 11, as a successful repair becomes unlikely after the 2nd retry. 2. _Does showing the history of patches to the LLM improve the performance of iterative APR?_ Yes, when comparing the number of successful repairs between the LSO experiments and the Forward History experiments, the latter has a much higher number of successful repairs. 3. _What is the optimal way to show the history to the LLM?_ The best format to show the history is to display the oldest messages first and the last message being the latest. This has been observed when comparing Forward History and Reverse History. 4. _What is the optimal temperature to conduct APR of AI C Code?_ The optimal temperature to achieve the highest number of repaired samples is 0.0 for prompts 9 and 11 for Forward History. A higher temperature is necessary for less conventional prompts to allow the LLM to parse it correctly."
    },
    {
      "title": "6 Conclusions And Future Work",
      "text": "In this report, we expanded NeuroCodeBench to create a large dataset of memory-vulnerable AI C code using mutations. We used ESBMC to classify which samples contained memory vulnerabilities and which samples were secure. In addition, we used GPT-3.5-Turbo and various prompt engineering techniques to explore how well an LLM could repair the mutated code. In the process, we discovered that a long persona prompt with the role Automated Code Repair Tool is the most optimized at repairing AI C Code. Furthermore, we proposed methods for extracting the faulty source code from the large volume of AI C code to circumvent the issues that arise due to LLMs' relatively small context window. Lastly, we used ESBMC-AI to test how the iterative APR process improves repair performance. We have showed that the iterative repair process provides a substantial increase of 7% in repair performance. We also found that after attempt 3 the chances of successfully repairing a sample decrease significantly by 23%. In the future, we plan to conduct more experiments using a diverse set of LLMs to discover whether our findings generalize beyond GPT-3.5-Turbo. In this respect, open-source LLMs would benefit our research, as they can be fine-tuned for program repair."
    },
    {
      "title": "References",
      "text": "* [1] Prompt engineering. [https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-code-execution-to-perform-more-accurate-calculations-or-call-external-apis](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-code-execution-to-perform-more-accurate-calculations-or-call-external-apis). Online; Accessed: 30th April 2024. * [2] Y. Abu-Mostafa and J. St. Jacques. Information capacity of the hopfield model. _IEEE Transactions on Information Theory_, 31(4):461-464, 1985. * [3] M. AI and L. Foundation. Pytorch, 2023. [https://pytorch.org/](https://pytorch.org/) [Accessed: 31 August 2023]. * [4] S. Biderman and W. J. Scheirer. Pitfalls in machine learning research: Reexamining the development cycle. In J. Zosa Forde, F. Ruiz, M. F. Pradier, and A. Schein, editors, _Proceedings on \"I Can't Believe It's Not Better!\" at NeurIPS Workshops_, volume 137 of _Proceedings of Machine Learning Research_, pages 106-117. PMLR, 12 Dec 2020. * [5] G. Brain. Tensorflow, 2023. [https://www.tensorflow.org](https://www.tensorflow.org) [Accessed: 31 August 2023]. * [6] C. Brix, S. Bak, C. Liu, and T. T. Johnson. The Fourth International Verification of Neural Networks Competition (VNN-COMP 2023): Summary and Results, 2023. * [7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020. * [8] Y. Charalambous, N. Tihanyi, R. Jain, Y. Sun, M. A. Ferrag, and L. C. Cordeiro. A new era in software security: Towards self-healing software via large language models and formal verification. _arXiv preprint arXiv:2305.14752_, 2023. * [9] R. Chaudhuri and I. Fiete. Bipartite expander hopfield networks as self-decoding high-capacity error correcting codes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. * [10] L. Chen, M. Zaharia, and J. Zou. How is chatgpt's behavior changing over time? _arXiv preprint arXiv:2307.09009_, 2023. * [11] C. Community. 2023 cwe top 25 most dangerous software weaknesses, 2023. [https://cwe.mi](https://cwe.mi) tre.org/top25/archive/2023/2023_top25_list.html [Accessed: 25 August 2023]. * [12] O. Community. Open neural network exchange: The open standard for machine learning interoperability, 2023. [https://onnx.ai/](https://onnx.ai/) [Accessed: 31 August 2023]. * [13] R. Conlin. keras2c github repository, 2023. [https://github.com/fouriest/keras2c](https://github.com/fouriest/keras2c) [Accessed: 25 August 2023]. * [14] R. Conlin, K. Erickson, J. Abbate, and E. Kolemen. Keras2c: A library for converting keras neural networks to real-time compatible c. _Engineering Applications of Artificial Intelligence_, 100:104182, 2021. * [15] L. Cordeiro. Software security. * [16] L. Cordeiro, B. Fischer, and J. Marques-Silva. SMT-Based Bounded Model Checking for Embedded ANSI-C Software, July 2009. arXiv:0907.2072 [cs]. * [17] C. Daws and S. Tripakis. Model checking of real-time reachability properties using abstractions. In B. Steffen, editor, _Tools and Algorithms for the Construction and Analysis of Systems_, pages 313-329, Berlin, Heidelberg, 1998. Springer Berlin Heidelberg. * [18] Z. Deng, G. Meng, K. Chen, T. Liu, L. Xiang, and C. Chen. Differential testing of cross deep learning framework APIs: Revealing inconsistencies and vulnerabilities. In _32nd USENIX Security Symposium (USENIX Security 23)_, pages 7393-7410, Anaheim, CA, Aug. 2023. USENIX Association. * [19] A. Denisov and S. Pankevich. Mull it over: Mutation testing based on llvm. In _2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)_, pages 25-31, April 2018. * [20] Z. Fan, X. Gao, M. Mirchev, A. Roychoudhury, and S. H. Tan. Automated repair of programs from large language models. In _2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)_, pages 1469-1481. IEEE, 2023. * [21] S. Fort, J. Ren, and B. Lakshminarayanan. Exploring the limits of out-of-distribution detection. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 7068-7081. Curran Associates, Inc., 2021. * [22] Q. Guo, X. Xie, Y. Li, X. Zhang, Y. Liu, X. Li, and C. Shen. Audee: Automated testing for deep learning frameworks. In _Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering_, ASE '20, page 486-498, New York, NY, USA, 2021. Association for Computing Machinery. * [23] X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo, M. Wu, and X. Yi. A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability. _Computer Science Review_, 37:100270, 2020. * [24] N. Humbatova, G. Jahangirova, G. Bavota, V. Riccio, A. Stocco, and P. Tonella. Taxonomy of real faults in deep learning systems. In _Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering_, ICSE '20, page 1110-1121, New York, NY, USA, 2020. Association for Computing Machinery. * [25] C. Liu, X. Wang, R. Shin, J. E. Gonzalez, and D. Song. Neural code completion. 2016. * [26] E. Manino, R. S. Menezes, F. Shmarov, and L. C. Cordeiro. NeuroCodeBench: a Plain C Neural Network Benchmark for Software Verification. In _Workshop on Automated Formal Reasoning for Trustworthy AI Systems_, 2023. * [27] O. Massi, A. I. Mezza, R. Giampiccolo, and A. Bernardini. Deep learning-based wave digital modeling of rate-dependent hysteretic nonlinearities for virtual analog applications. _EURASIP Journal on Audio, Speech, and Music Processing_, 2023(1):12, Mar 2023. * [28] Y. Meng, D. Sun, Z. Qiu, M. T. B. Waez, and C. Fan. Learning density distribution of reachable states for autonomous systems. In A. Faust, D. Hsu, and G. Neumann, editors, _Proceedings of the 5th Conference on Robot Learning_, volume 164 of _Proceedings of Machine Learning Research_, pages 124-136. PMLR, 08-11 Nov 2022. * [29] M. Miller. Trends and challenges in the vulnerability mitigation landscape. _USENIX Association_, 2019. * [30] M. M. Morovati, A. Nikanjam, F. Khomh, and Z. M. J. Jiang. Bugs in machine learning-based systems: a faultload benchmark. _Empirical Software Engineering_, 28(3):62, Apr 2023. * [31] M. N. Muller, C. Brix, S. Bak, C. Liu, and T. T. Johnson. The third international verification of neural networks competition (vnn-comp 2022): Summary and results, 2023. * [32] N. Narodytska and S. Kasiviswanathan. Simple black-box adversarial attacks on deep neural networks. In _2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 1310-1318, 2017. * [33] A. Odena, C. Olsson, D. Andersen, and I. Goodfellow. TensorFlow-EUR: Debugging neural networks with coverage-guided fuzzing. In K. Chaudhuri and R. Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 4901-4911. PMLR, 09-15 Jun 2019. * [34] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. _OpenAI Blog_, 1(8):9, 2019. * [35] U. J. Ravaioli, J. Cunningham, J. McCarroll, V. Gangal, K. Dunlap, and K. L. Hobbs. Safe reinforcement learning benchmark environments for aerospace control systems. In _2022 IEEE Aerospace Conference (AERO)_, pages 1-20, 2022. * [36] H. Suresh and J. Guttag. A framework for understanding sources of harm throughout the machine learning life cycle. In _Equity and Access in Algorithms, Mechanisms, and Optimization_, EAAMO '21, New York, NY, USA, 2021. Association for Computing Machinery. * [37] K. User. onnx2c github repository, 2023. [https://github.com/kraiskil/onnx2c](https://github.com/kraiskil/onnx2c) [Accessed: 25 August 2023]. * [38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017. * [39] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C. Schmidt. A prompt pattern catalog to enhance prompt engineering with chatgpt. _arXiv preprint arXiv:2302.11382_, 2023. * [40] J. Xu, M. Yagoub, R. Ding, and Q.-J. Zhang. Neural-based dynamic modeling of nonlinear microwave circuits. _IEEE Transactions on Microwave Theory and Techniques_, 50(12):2769-2780, 2002."
    },
    {
      "title": "Appendix A Testing Backtick Performance Effectiveness",
      "text": "The annotation of source code using Markdown code block syntax has always been an implicit feature in ChatGPT and LLMs [1]. It is widely accepted that code should be surrounded by '' to mark where it begins and ends. In Section 4, the single iteration experiments provided an understanding of how effective LLMs are at repairing AI C code vulnerabilities. To understand the effectiveness of backticks in the source code provided, the experiments will be executed again, with the only difference being the exclusion of backticks in the prompt. The goal of these experiments is to influence future prompt engineering design. If the results of the prompts without backticks are better than with backticks, then this could mean excluding them would yield better APR accuracy in future experiments, such as those conducted in Section 5. Each of the aforementioned prompts in Section 4.1 has been modified to exclude the Markdown backticks. This is done in order to be able to directly compare the performance with the previous experiments and understand the true impact that backticks have on the performance of prompts. The results are going to be explored for the remainder of this section. C/C++ Detector ResultsFigure 14 illustrates the distribution of probability scores of the LLM output that the C/C++ detector assigned for each prompt, as described in Figure 6. In each prompt shown in Figure (a)a, the main differences observed are that the experiments without backticks perform overall worse, aside from the simple prompts that did not display a noticeable difference. The old prompt contains a higher median value without backticks. And, the results for the persona prompts show a reduced confidence of the C/C++ detector, as observed by the Q1 and Q3 being lower, the median value was higher overall. Figure (b)b shows the detection results when the verifier is excluded from the prompt, along with the results for when the verifier is included before and after the source code. The syntax detector assigned the contextual experiments with no verifier output in the prompt less confidence in it being valid C/C++ source code, as can be observed by a lower Q1 and median score. When verifier output is included, the experiments with no backticks perform slightly better than the experiments with backticks. The results, for verifier output before the source code, exhibit a higher Q3. The results for verifier output after the source code exhibit a slightly higher median and Q3. The results for the contextual experiments signify the need to clearly mark the boundaries of code and raw output with Markdown syntax code blocks, as it shows that the LLM could output more consistent C/C++ code as evidenced by the detector confidence results. The experiments that were conducted using one line source code prompts show no major difference in results for each of the prompts. The results for the verifier output show a slightly higher max value for the experiments that include verifier output. This can be explained as experimental noise, as the difference is not significant enough. Relevance ResultsThe measurements that show how similar the proposed patches by the LLM were for the experiments without backticks can be observed in Figure 15. As stated in the experiments with backticks, it is expected that the LLM outputs the code verbatim, aside from the single line of code to be patched. Figure (a)a illustrates a similarity box plot with the original code for Figure 14: Distribution of probability scores from the C/C++ detector used in Visual Studio on the LLM backtick-less repair patches. It exhibits a lower first quartile than the results with backticks included in the prompt, as illustrated in Figure 6. each prompt. For the contextual experiments, the backtick-less match to the original prompt less in every prompt, as the max match values and Q3 values, and median values assigned in each prompt is lower for the backtick-less experiments. The results for one line mirror the results for contextual, aside from the fact that the overall difference is much more noticeable, and that some prompts have a higher max match value for backtick-less experiments. Figure 14(b) illustrates a box plot of how the results match the original for each type of verifier output experiment. As is observed, the performance seems lower in every metric. Contextual results have lower first5 quartiles, along with a lower median. For the verifier output experiments where no verifier output is included, there is a significantly less Q3 matching to the original prompt. For the one line experiments, experiments where no verifier output is included, and experiments where the verifier output is placed before source code, the max value of those experiments is higher for both, however, every other metric minimum value and Q1-3 are all lower. Footnote 5: On plots where the first quartile is not zero. **Relevance Takeaway.** The LLM outputs code that more closely matches the original input when the backticks are included in the input. Compilation ResultsThe experiments conducted to find out the difference in the amount of samples that successfully compile, as the LLM produces valid C code as its output without Markdown backticks, is shown in Table 4. The results for the contextual experiments are the exact same as with the Markdown backtick experiments, shown in Table 2. For the one line experiments, the experiments with backticks seem to perform worse, the prompts and the verifier output sections of the table both have a higher number of samples that successfully compile for the experiments that exclude the backticks from the prompts. \\begin{table} \\begin{tabular}{|c|c c c c c c c c|c c c|} \\hline & old & simple & pers-0 & pers-1 & pers-2 & pers-3 & pers-4 & pers-5 & none & before & after \\\\ \\hline contextual & 1.50\\% & 1.25\\% & 0.25\\% & 0\\% & 1.58\\% & 2.16\\% & 1.16\\% & 1.25\\% & 0.14\\% & 1.32\\% & 1.82\\% \\\\ \\hline one line & 42.5\\% & 19.91\\% & 62.7\\% & 71.5\\% & 36.7\\% & 57.5\\% & 32.25\\% & 58.5\\% & 59.9\\% & 42.1\\% & 43.3\\% \\\\ \\hline \\end{tabular} \\end{table} Table 4: Percentage of repair patches that make the code compile. Asking the LLM to repair only one line of code yields patches that compile more consistently. Much like with Markdown backticks. Figure 15: String match between source code in the prompt and LLM output patch (ignoring whitespaces). This figure is the equivalent of Figure 7 for the backtick-less experiment. **Compilation Takeaway.** With large samples of code, as seen in the contextual experiments, the LLM does not exhibit an increase in compilable code output, the amount is the exact same as with backticks. For smaller samples, compilable code is produced more often when backticks are excluded. Verifying the Code ResultsTable 5 shows the verification success rate of each prompt and verifier feedback. For the contextual experiments, the results shown reflect the same pattern where the percentage of samples that successfully compiled is exactly the same as Tables 2, 3, and 4. This implies that when the contextual experiments compile, they will also have a successful verification. For one line backtick-less experiments, the amount of samples that have been verified successfully is lower in all the metrics. **Verification Takeaway.** With large samples of code input, the code will compile and verify the same regardless of if code is surrounded in backticks or not. For smaller samples, the successful verification of code is slightly higher when backticks are included."
    },
    {
      "title": "Appendix B Iterative Repair Experiment Details",
      "text": ""
    },
    {
      "title": "Successful Verifications By Prompt Per Temperature",
      "text": "The following figures are conducted over a range of temperatures, in order to further understand how the number of repairs are affected. Figures 16 to 20 shows the successful verifications by prompt for temperatures 0.0, 0.4, 0.7, 1.0, and 1.3 respectively. The general pattern that can be observed, is that as the temperature is increased, the LSO and Forward History performance increases, while the opposite is true for Reverse History. For LSO, prompts 9-2, 11 and 11-2 perform the best, as temperature increases, so does the number of successful repairs. Prompts 9 and Old do not manage to repair any samples. The only exception is that in temperature 1.3, prompt 9 performed as well as prompt 11, however, still lower than prompts 9-2 and 11-2. The best Forward History prompts are prompt 9 and prompt 11, they have the same number of successful repairs in temperature 0.0, which is the highest. Prompt 9 is only surpassed by the other prompts in Figures 18 and 19. Aside from that, prompt 9 and 11 are the best performing prompts. Prompts 9 and 11 solve the exact same samples, however, the patches produced are different. As seen in the previous experiments, the Reverse History experiments score the lowest in successfully repairing AI C code. In Figure 16, Reverse History was unable to repair any samples. The reason for this is probably due to the indirect nature of displaying the history in reverse order, making the LLM perform worse as the instructions are not as clear for instruct models. \\begin{table} \\begin{tabular}{|c|c c c c c c c c|c c c|} \\hline & old & simple & pers-0 & pers-1 & pers-2 & pers-3 & pers-4 & pers-5 & none & before & after \\\\ \\hline contextual & 1.50\\% & 1.25\\% & 0.25\\% & 0\\% & 1.58\\% & 2.16\\% & 1.16\\% & 1.25\\% & 0.14\\% & 1.32\\% & 1.82\\% \\\\ \\hline one line & 0.00\\% & 4.16\\% & 5.00\\% & 5.83\\% & 4.25\\% & 6.50\\% & 9.25\\% & 7.00\\% & 5.21\\% & 7.67\\% & 5.10\\% \\\\ \\hline \\end{tabular} \\end{table} Table 5: Percentage of repair patches that are successfully verified. All the patches that compile (see Table 2) are also successfully verified when using the contextual strategy. Figure 16: Successful verifications by prompt for temperature \\(0.0\\). Reverse History is not included as it did not yield any successful verifications. Figure 17: Successful verifications by prompt for temperature \\(0.4\\). Figure 19: Successful verifications by prompt for temperature 1.0. Figure 20: Successful verifications by prompt for temperature 1.3. Figure 18: Successful verifications by prompt for temperature 0.7. [MISSING_PAGE_FAIL:32] Figure 23: Successful verifications by attempt for temperature 0.7. Figure 24: Successful verifications by attempt for temperature 1.0. Figure 22: Successful verifications by attempt for temperature 0.4. **Changing the Temperature Takeaway.** Increasing the temperature for Forward History experiments only affects the 1st attempt. The LSO experiments did not noticeably change in the amount of successful repairs as the temperature is varied."
    },
    {
      "title": "Successful Verifications By Attempt Per Prompt (Temperature 0.0)",
      "text": "At this stage, it is obvious that the best performing configuration for repairing AI C code is using temperature 0.0, and Forward History. Figures 26 to 29 show the percentage of successful repairs at each attempt for each prompt. As seen from the previous figures, the Reverse History and LSO experiments did not have any successful repairs for the prompt 9, so there is no figure shown for those experiments. Also, the old ESBMC-AI prompt did not successfully repair a single sample for temperature 0.0, so it is not shown either. The remaining bar graphs for the LSO experiments show that the most successful repairs occurred in prompt 9-2 and prompt 11-2, where the same amount of samples were repaired in the first two attempts. For the Forward History experiments, prompt 9 has the highest performance, showing that samples were repaired in the first 3 attempts, after attempt 2, no further samples were repaired. Prompt 9-2 had a much lower number of successful repairs per attempt, however, there were some successful repairs on attempt 3. Prompt 11 had no successful repairs of samples during any retries, and only repaired samples successfully on the first attempt. Figure 25: Successful verifications by attempt for temperature 1.3. Figure 28: Successful verifications by attempt for prompt 11 (temperature 0.0). Figure 27: Successful verifications by attempt for prompt 9-2 (temperature 0.0). Figure 26: Successful verifications by attempt for prompt 9 (temperature 0.0). **Takeaway.** For AI C code one line repair, the optimal total number of attempts is 3. Figure 29: Successful verifications by attempt for prompt 11-2 (temperature 0.0)."
    }
  ]
}