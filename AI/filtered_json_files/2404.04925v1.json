{
  "title": "Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers",
  "authors": [
    "Libo Qin",
    "Qiguang Chen",
    "Yuhang Zhou",
    "Zhi Chen",
    "Yinghui Li",
    "♮ Lizi Liao",
    "Min Li",
    "♣ Wanxiang",
    "Che ♠ Philip",
    "S Yu"
  ],
  "abstract": "\n Multilingual Large Language Models are capable of using powerful Large Language Models to handle and respond to queries in multiple languages, which achieves remarkable success in multilingual natural language processing tasks. Despite these breakthroughs, there still remains a lack of a comprehensive survey to summarize existing approaches and recent developments in this field. To this end, in this paper, we present a thorough review and provide a unified perspective to summarize the recent progress as well as emerging trends in multilingual large language models (MLLMs) literature. The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step and present a thorough review in MLLMs research field according to multi-lingual alignment; (2) New taxonomy: we offer a new and unified perspective to summarize the current progress of MLLMs; (3) New frontiers: we highlight several emerging frontiers and discuss the corresponding challenges; (4) Abundant resources: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards. We hope our work can provide the community with quick access and spur breakthrough research in MLLMs. \n",
  "references": [
    {
      "id": null,
      "title": "Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers",
      "authors": [
        "Libo Qin",
        "Qiguang Chen",
        "Yuhang Zhou",
        "Zhi Chen",
        "Yinghui Li",
        "♮ Lizi Liao",
        "Min Li",
        "♣ Wanxiang",
        "Che ♠ Philip",
        "S Yu"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Benchmarking arabic ai with large language models",
      "authors": [
        "Ahmed Abdelali",
        "Hamdy Mubarak",
        "Absar Shammur",
        "Maram Chowdhury",
        "Basel Hasanain",
        "Sabri Mousi",
        "Yassine El Boughorbel",
        "Daniel Kheir",
        "Fahim Izham",
        "Majd Dalvi",
        "Hawasly"
      ],
      "year": "2023",
      "venue": "Benchmarking arabic ai with large language models",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Jasmine: Arabic gpt models for few-shot learning",
      "authors": [
        "Muhammad Abdul-Mageed",
        "Abdelrahim Elmadany",
        "Alcides Inciarte",
        "Md Tawkat Islam",
        "Khondaker"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Masakhaner: Named entity recognition for african languages",
      "authors": [
        "David Ifeoluwa Adelani",
        "Jade Abbott",
        "Graham Neubig",
        "D' Daniel",
        "Julia Souza",
        "Constantine Kreutzer",
        "Chester Lignos",
        "Happy Palen-Michel",
        "Shruti Buzaaba",
        "Sebastian Rijhwani",
        "Ruder"
      ],
      "year": "2021",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "A few thousand translations go a long way! leveraging pre-trained models for african news translation",
      "authors": [
        "David Ifeoluwa Adelani",
        "Jesujoba Oluwadara Alabi",
        "Angela Fan",
        "Julia Kreutzer",
        "Xiaoyu Shen",
        "Machel Reid",
        "Dana Ruiter",
        "Dietrich Klakow",
        "Peter Nabende",
        "Ernie Chang"
      ],
      "year": "2022",
      "venue": "A few thousand translations go a long way! leveraging pre-trained models for african news translation",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Masakhanews: News topic classification for african languages",
      "authors": [
        "David Ifeoluwa Adelani",
        "Marek Masiak",
        "Jesujoba Israel Abebe Azime",
        "Atnafu Oluwadara Alabi",
        "Christine Lambebo Tonja",
        "Odunayo Mwase",
        "Ogundepo",
        "F P Bonaventure",
        "Akintunde Dossou",
        "Doreen Oladipo",
        "Nixdorf"
      ],
      "year": "2023",
      "venue": "Masakhanews: News topic classification for african languages",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Zero-shot cross-lingual reranking with large language models for low-resource languages",
      "authors": [
        "Mofetoluwa Adeyemi",
        "Akintunde Oladipo",
        "Ronak Pradeep",
        "Jimmy Lin"
      ],
      "year": "2023",
      "venue": "Zero-shot cross-lingual reranking with large language models for low-resource languages",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Findings of the iwslt 2023 evaluation campaign",
      "authors": [
        "Milind Agarwal",
        "Sweta Agarwal",
        "Antonios Anastasopoulos",
        "Luisa Bentivogli",
        "Ondřej Bojar",
        "Claudia Borg",
        "Marine Carpuat",
        "Roldano Cattoni",
        "Mauro Cettolo",
        "Mingda Chen"
      ],
      "year": "2023",
      "venue": "Findings of the iwslt 2023 evaluation campaign",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Maple: Multilingual evaluation of parameter efficient finetuning of large language models",
      "authors": [
        "Divyanshu Aggarwal",
        "Ashutosh Sathe",
        "Sunayana Sitaram"
      ],
      "year": "2024",
      "venue": "Maple: Multilingual evaluation of parameter efficient finetuning of large language models",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Jw300: A widecoverage parallel corpus for low-resource languages",
      "authors": [
        "Željko Agic",
        "Ivan Vulic"
      ],
      "year": "2019",
      "venue": "Jw300: A widecoverage parallel corpus for low-resource languages",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Multilingual qa with only 5 examples",
      "authors": [
        "Priyanka Agrawal",
        "Chris Alberti",
        "Fantine Huot",
        "Joshua Maynez",
        "Ji Ma",
        "Sebastian Ruder",
        "Kuzman Ganchev",
        "Dipanjan Das",
        "Mirella Lapata"
      ],
      "year": "2022",
      "venue": "Multilingual qa with only 5 examples",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "2022b. Incontext examples selection for machine translation",
      "authors": [
        "Sweta Agrawal",
        "Chunting Zhou",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Marjan Ghazvininejad"
      ],
      "year": "",
      "venue": "2022b. Incontext examples selection for machine translation",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Multilingual summarization with factual consistency evaluation",
      "authors": [
        "Roee Aharoni",
        "Shashi Narayan",
        "Joshua Maynez",
        "Jonathan Herzig",
        "Elizabeth Clark",
        "Mirella Lapata"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Do all languages cost the same? tokenization in the era of commercial language models",
      "authors": [
        "Orevaoghene Ahia",
        "Sachin Kumar",
        "Hila Gonen",
        "Jungo Kasai",
        "Noah A David R Mortensen",
        "Yulia Smith",
        "Tsvetkov"
      ],
      "year": "2023",
      "venue": "Do all languages cost the same? tokenization in the era of commercial language models",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Enhancing multilingual information retrieval in mixed human resources environments: A rag model implementation for multicultural enterprise",
      "authors": [
        "Ahmad Syed Rameel"
      ],
      "year": "2024",
      "venue": "Enhancing multilingual information retrieval in mixed human resources environments: A rag model implementation for multicultural enterprise",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Mega: Multilingual evaluation of generative ai",
      "authors": [
        "Kabir Ahuja",
        "Rishav Hada",
        "Millicent Ochieng",
        "Prachi Jain",
        "Harshita Diddee",
        "Samuel Maina",
        "Tanuja Ganu",
        "Sameer Segal",
        "Maxamed Axmed",
        "Kalika Bali"
      ],
      "year": "",
      "venue": "Mega: Multilingual evaluation of generative ai",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "On the calibration of massively multilingual language models",
      "authors": [
        "Kabir Ahuja",
        "Sunayana Sitaram",
        "Sandipan Dandapat",
        "Monojit Choudhury"
      ],
      "year": "2022",
      "venue": "On the calibration of massively multilingual language models",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Megaverse: Benchmarking large language models across languages, modalities, models and tasks",
      "authors": [
        "Sanchit Ahuja",
        "Divyanshu Aggarwal",
        "Varun Gumma",
        "Ishaan Watts",
        "Ashutosh Sathe",
        "Millicent Ochieng",
        "Rishav Hada",
        "Prachi Jain",
        "Maxamed Axmed",
        "Kalika Bali"
      ],
      "year": "2023",
      "venue": "Megaverse: Benchmarking large language models across languages, modalities, models and tasks",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Advancements in Arabic grammatical error detection and correction: An empirical investigation",
      "authors": [
        "Bashar Alhafni",
        "Go Inoue",
        "Christian Khairallah",
        "Nizar Habash"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Tokenizer choice for llm training: Negligible or crucial? arXiv preprint",
      "authors": [
        "Mehdi Ali",
        "Michael Fromm",
        "Klaudia Thellmann",
        "Richard Rutmann",
        "Max Lübbering",
        "Johannes Leveling",
        "Katrin Klug",
        "Jan Ebert",
        "Niclas Doll",
        "Jasper Schulze Buschhoff"
      ],
      "year": "2023",
      "venue": "Tokenizer choice for llm training: Negligible or crucial? arXiv preprint",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Marathi-english code-mixed text generation",
      "authors": [
        "Dhiraj Amin",
        "Sharvari Govilkar",
        "Sagar Kulkarni",
        "Yash Shashikant Lalit",
        "Arshi Ajaz Khwaja",
        "Daries Xavier",
        "Sahil Girijashankar Gupta"
      ],
      "year": "2023",
      "venue": "Marathi-english code-mixed text generation",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Palm 2 technical report",
      "authors": [
        "Rohan Anil",
        "Andrew M Dai",
        "Orhan Firat",
        "Melvin Johnson",
        "Dmitry Lepikhin",
        "Alexandre Passos",
        "Siamak Shakeri",
        "Emanuel Taropa",
        "Paige Bailey",
        "Zhifeng Chen"
      ],
      "year": "2023",
      "venue": "Palm 2 technical report",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Crosssum: Beyond englishcentric cross-lingual abstractive text summarization for 1500+ language pairs",
      "authors": [
        "Abhik Bhattacharjee",
        "Tahmid Hasan",
        "Uddin Wasi",
        "Yuan-Fang Ahmad",
        "Yong-Bin Li",
        "Rifat Kang",
        "Shahriyar"
      ],
      "year": "2021",
      "venue": "Crosssum: Beyond englishcentric cross-lingual abstractive text summarization for 1500+ language pairs",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Gpt-neox-20b: An open-source autoregressive language model. Challenges & Perspectives in Creating Large Language Models",
      "authors": [
        "Sid Black",
        "Stella Biderman",
        "Eric Hallahan",
        "Quentin Anthony",
        "Leo Gao",
        "Laurence Golding",
        "Horace He",
        "Connor Leahy",
        "Kyle Mcdonell",
        "Jason Phang"
      ],
      "year": "2022",
      "venue": "Gpt-neox-20b: An open-source autoregressive language model. Challenges & Perspectives in Creating Large Language Models",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Breaking the curse of multilinguality with cross-lingual expert language models",
      "authors": [
        "Terra Blevins",
        "Tomasz Limisiewicz",
        "Suchin Gururangan",
        "Margaret Li",
        "Hila Gonen",
        "Noah A Smith",
        "Luke Zettlemoyer"
      ],
      "year": "2024",
      "venue": "Breaking the curse of multilinguality with cross-lingual expert language models",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Language contamination helps explain the cross-lingual capabilities of english pretrained models",
      "authors": [
        "Terra Blevins",
        "Luke Zettlemoyer"
      ],
      "year": "2022",
      "venue": "Language contamination helps explain the cross-lingual capabilities of english pretrained models",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Tudor Nicolae Mateiu, Jindřich Helcl, and Mikko Aulamo. 2023. Opuscleaner and opustrainer, open source toolkits for training machine translation and large language models",
      "authors": [
        "Nikolay Bogoychev",
        "Jelmer Van Der Linde",
        "Graeme Nail",
        "Barry Haddow",
        "Jaume Zaragoza-Bernabeu",
        "Gema Ramírez-Sánchez",
        "Lukas Weymann"
      ],
      "year": "",
      "venue": "Tudor Nicolae Mateiu, Jindřich Helcl, and Mikko Aulamo. 2023. Opuscleaner and opustrainer, open source toolkits for training machine translation and large language models",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Analyzing multilingual competency of llms in multi-turn instruction following: A case study of arabic",
      "authors": [
        "Sabri Boughorbel",
        "Majd Hawasly"
      ],
      "year": "2023",
      "venue": "Analyzing multilingual competency of llms in multi-turn instruction following: A case study of arabic",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Searching for needles in a haystack: On the role of incidental bilingualism in palm's translation capability",
      "authors": [
        "Eleftheria Briakou",
        "Colin Cherry",
        "George Foster"
      ],
      "year": "2023",
      "venue": "Searching for needles in a haystack: On the role of incidental bilingualism in palm's translation capability",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "2023a. Nusacrowd: Open source initiative for indonesian nlp resources",
      "authors": [
        "Samuel Cahyawijaya",
        "Holy Lovenia",
        "Alham Fikri Aji",
        "Genta Winata",
        "Bryan Wilie",
        "Fajri Koto",
        "Rahmad Mahendra",
        "Christian Wibisono",
        "Ade Romadhony",
        "Karissa Vincentio"
      ],
      "year": "",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Teaching novel languages with to llms through alignment-based cross-lingual instruction",
      "authors": [
        "Samuel Cahyawijaya",
        "Holy Lovenia",
        "Tiezheng Yu",
        "Willy Chung",
        "Pascale Fung"
      ],
      "year": "2023",
      "venue": "Teaching novel languages with to llms through alignment-based cross-lingual instruction",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Multilingual large language models leak human stereotypes across language boundaries",
      "authors": [
        "Trista Yang",
        "Anna Cao",
        "Jieyu Sotnikova",
        "Linda X Zhao",
        "Rachel Zou",
        "Hal Rudinger",
        "Iii Daume"
      ],
      "year": "2023",
      "venue": "Multilingual large language models leak human stereotypes across language boundaries",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "xcot: Crosslingual instruction tuning for cross-lingual chain-ofthought reasoning",
      "authors": [
        "Linzheng Chai",
        "Jian Yang",
        "Tao Sun",
        "Hongcheng Guo",
        "Jiaheng Liu",
        "Bing Wang",
        "Xiannian Liang",
        "Jiaqi Bai",
        "Tongliang Li",
        "Qiyao Peng"
      ],
      "year": "2024",
      "venue": "xcot: Crosslingual instruction tuning for cross-lingual chain-ofthought reasoning",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Ernie-code: Beyond english-centric cross-lingual pretraining for programming languages",
      "authors": [
        "Yekun Chai",
        "Shuohuan Wang",
        "Chao Pang",
        "Yu Sun",
        "Hua Hao Tian",
        "Wu"
      ],
      "year": "2022",
      "venue": "Ernie-code: Beyond english-centric cross-lingual pretraining for programming languages",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Towards multi-lingual visual question answering",
      "authors": [
        "Soravit Changpinyo",
        "Linting Xue",
        "Idan Szpektor",
        "Ashish V Thapliyal",
        "Julien Amelot",
        "Xi Chen",
        "Radu Soricut"
      ],
      "year": "2022",
      "venue": "Towards multi-lingual visual question answering",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Orion-14b: Open-source multilingual large language models",
      "authors": [
        "Du Chen",
        "Yi Huang",
        "Xiaopu Li",
        "Yongqiang Li",
        "Yongqiang Liu",
        "Haihui Pan",
        "Leichao Xu",
        "Dacheng Zhang",
        "Zhipeng Zhang",
        "Kun Han"
      ],
      "year": "2024",
      "venue": "Orion-14b: Open-source multilingual large language models",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "2023a. Large language models meet harry potter: A dataset for aligning dialogue agents with characters",
      "authors": [
        "Nuo Chen",
        "Yan Wang",
        "Haiyun Jiang",
        "Deng Cai",
        "Yuhan Li",
        "Ziyang Chen",
        "Longyue Wang",
        "Jia Li"
      ],
      "year": "",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Breaking language barriers in multilingual mathematical reasoning: Insights and observations",
      "authors": [
        "Nuo Chen",
        "Zinan Zheng",
        "Ning Wu",
        "Linjun Shou",
        "Ming Gong",
        "Yangqiu Song",
        "Dongmei Zhang",
        "Jia Li"
      ],
      "year": "2023",
      "venue": "Breaking language barriers in multilingual mathematical reasoning: Insights and observations",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Monolingual or multilingual instruction tuning: Which makes a better alpaca",
      "authors": [
        "Pinzhen Chen",
        "Shaoxiong Ji",
        "Nikolay Bogoychev",
        "Barry Haddow",
        "Kenneth Heafield"
      ],
      "year": "2023",
      "venue": "Monolingual or multilingual instruction tuning: Which makes a better alpaca",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "On scaling up a multilingual vision and language model",
      "authors": [
        "Xi Chen",
        "Josip Djolonga",
        "Piotr Padlewski",
        "Basil Mustafa",
        "Soravit Changpinyo",
        "Jialin Wu",
        "Carlos Riquelme Ruiz",
        "Sebastian Goodman",
        "Xiao Wang",
        "Yi Tay"
      ],
      "year": "2023",
      "venue": "On scaling up a multilingual vision and language model",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Piotr Padlewski, et al. 2023e. Pali-3 vision language models: Smaller, faster, stronger",
      "authors": [
        "Xi Chen",
        "Xiao Wang",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Jialin Wu",
        "Paul Voigtlaender",
        "Basil Mustafa",
        "Sebastian Goodman"
      ],
      "year": "",
      "venue": "Piotr Padlewski, et al. 2023e. Pali-3 vision language models: Smaller, faster, stronger",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Pali: A jointly-scaled multilingual language-image model",
      "authors": [
        "Xi Chen",
        "Xiao Wang",
        "Soravit Changpinyo",
        "Piotr Piergiovanni",
        "Daniel Padlewski",
        "Sebastian Salz",
        "Adam Goodman",
        "Basil Grycner",
        "Lucas Mustafa",
        "Beyer"
      ],
      "year": "2022",
      "venue": "Pali: A jointly-scaled multilingual language-image model",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Tigerbot: An open multilingual multitask llm",
      "authors": [
        "Ye Chen",
        "Wei Cai",
        "Liangmin Wu",
        "Xiaowei Li",
        "Zhanxuan Xin",
        "Cong Fu"
      ],
      "year": "2023",
      "venue": "Tigerbot: An open multilingual multitask llm",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Improving translation faithfulness of large language models via augmenting instructions",
      "authors": [
        "Yijie Chen",
        "Yijin Liu",
        "Fandong Meng",
        "Yufeng Chen",
        "Jinan Xu",
        "Jie Zhou"
      ],
      "year": "2023",
      "venue": "Improving translation faithfulness of large language models via augmenting instructions",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Phoenix: Democratizing chatgpt across languages",
      "authors": [
        "Zhihong Chen",
        "Feng Jiang",
        "Junying Chen",
        "Tiannan Wang",
        "Fei Yu",
        "Guiming Chen",
        "Hongbo Zhang",
        "Juhao Liang",
        "Chen Zhang",
        "Zhiyi Zhang"
      ],
      "year": "2023",
      "venue": "Phoenix: Democratizing chatgpt across languages",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Wan Xiang, and Benyou Wang. 2023i. MultilingualSIFT: Multilingual Supervised Instruction Fine-tuning",
      "authors": [
        "Zhihong Chen",
        "Shuo Yan",
        "Juhao Liang",
        "Feng Jiang",
        "Xiangbo Wu",
        "Fei Yu",
        "Guiming Hardy Chen",
        "Junying Chen",
        "Hongbo Zhang",
        "Li Jianquan"
      ],
      "year": "",
      "venue": "Wan Xiang, and Benyou Wang. 2023i. MultilingualSIFT: Multilingual Supervised Instruction Fine-tuning",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "2023a. Scale: Synergized collaboration of asymmetric language translation engines",
      "authors": [
        "Xin Cheng",
        "Xun Wang",
        "Tao Ge",
        "Si-Qing Chen",
        "Furu Wei",
        "Dongyan Zhao",
        "Rui Yan"
      ],
      "year": "",
      "venue": "2023a. Scale: Synergized collaboration of asymmetric language translation engines",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "2023b. Mu 2 slam: Multitask, multilingual speech and language models",
      "authors": [
        "Yong Cheng",
        "Yu Zhang",
        "Melvin Johnson",
        "Wolfgang Macherey",
        "Ankur Bapna"
      ],
      "year": "",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Vicuna: An opensource chatbot impressing gpt-4",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph E Gonzalez",
        "Ion Stoica",
        "Eric P Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An opensource chatbot impressing gpt-4",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Ask me in english instead: Cross-lingual evaluation of large language models for healthcare queries",
      "authors": [
        "De Choudhury"
      ],
      "year": "2023",
      "venue": "Ask me in english instead: Cross-lingual evaluation of large language models for healthcare queries",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2022",
      "venue": "Palm: Scaling language modeling with pathways",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Sharan Narang, and Noah Constant. 2022a. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining",
      "authors": [
        "Chung Hyung Won",
        "Xavier Garcia",
        "Adam Roberts",
        "Yi Tay",
        "Orhan Firat"
      ],
      "year": "",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Siddhartha Brahma, et al. 2022b. Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li",
        "Xuezhi Wang",
        "Mostafa Dehghani"
      ],
      "year": "",
      "venue": "Siddhartha Brahma, et al. 2022b. Scaling instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Seahorse: A multilingual, multifaceted dataset for summarization evaluation",
      "authors": [
        "Elizabeth Clark",
        "Shruti Rijhwani",
        "Sebastian Gehrmann",
        "Joshua Maynez",
        "Roee Aharoni",
        "Vitaly Nikolaev",
        "Thibault Sellam",
        "Aditya Siddhant",
        "Dipanjan Das",
        "Ankur P Parikh"
      ],
      "year": "2023",
      "venue": "Seahorse: A multilingual, multifaceted dataset for summarization evaluation",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Tydi qa: A benchmark for information-seeking question answering in ty pologically di verse languages",
      "authors": [
        "Jonathan H Clark",
        "Eunsol Choi",
        "Michael Collins",
        "Dan Garrette",
        "Tom Kwiatkowski",
        "Vitaly Nikolaev",
        "Jennimaria Palomaki"
      ],
      "year": "2020",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Redpajama: an open dataset for training large language models",
      "authors": [],
      "year": "2023",
      "venue": "Redpajama: an open dataset for training large language models",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Increasing coverage and precision of textual information in multilingual knowledge graphs",
      "authors": [
        "Simone Conia",
        "Min Li",
        "Daniel Lee",
        "Umar Minhas",
        "Ihab Ilyas",
        "Yunyao Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "Alexis Conneau",
        "Kartikay Khandelwal",
        "Naman Goyal",
        "Vishrav Chaudhary",
        "Guillaume Wenzek",
        "Francisco Guzmán",
        "Édouard Grave",
        "Myle Ott",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Xnli: Evaluating crosslingual sentence representations",
      "authors": [
        "Alexis Conneau",
        "Guillaume Lample",
        "Ruty Rinott",
        "Adina Williams",
        "Holger Samuel R Bowman",
        "Veselin Schwenk",
        "Stoyanov"
      ],
      "year": "2018",
      "venue": "Xnli: Evaluating crosslingual sentence representations",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Multilingual holistic bias: Extending descriptors and patterns to unveil demographic biases in languages at scale",
      "authors": [
        "Pierre Marta R Costa-Jussà",
        "Eric Andrews",
        "Prangthip Smith",
        "Christophe Hansanti",
        "Elahe Ropers",
        "Cynthia Kalbassi",
        "Daniel Gao",
        "Carleigh Licht",
        "Wood"
      ],
      "year": "2023",
      "venue": "Multilingual holistic bias: Extending descriptors and patterns to unveil demographic biases in languages at scale",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "No language left behind: Scaling human-centered machine translation",
      "authors": [
        "James Marta R Costa-Jussà",
        "Onur Cross",
        "Maha Çelebi",
        "Kenneth Elbayad",
        "Kevin Heafield",
        "Elahe Heffernan",
        "Janice Kalbassi",
        "Daniel Lam",
        "Jean Licht",
        "Maillard"
      ],
      "year": "",
      "venue": "No language left behind: Scaling human-centered machine translation",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Toxicity in multilingual machine translation at scale",
      "authors": [
        "Eric Marta R Costa-Jussà",
        "Christophe Smith",
        "Daniel Ropers",
        "Jean Licht",
        "Javier Maillard",
        "Carlos Ferrando",
        "Escolano"
      ],
      "year": "2022",
      "venue": "Toxicity in multilingual machine translation at scale",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Efficient and effective text encoding for chinese llama and alpaca",
      "authors": [
        "Yiming Cui",
        "Ziqing Yang",
        "Xin Yao"
      ],
      "year": "2023",
      "venue": "Efficient and effective text encoding for chinese llama and alpaca",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "A survey of multilingual neural machine translation",
      "authors": [
        "Raj Dabre",
        "Chenhui Chu",
        "Anoop Kunchukuttan"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Halomi: A manually annotated benchmark for multilingual hallucination and omission detection in machine translation",
      "authors": [
        "David Dale",
        "Elena Voita",
        "Janice Lam",
        "Prangthip Hansanti",
        "Christophe Ropers",
        "Elahe Kalbassi",
        "Cynthia Gao",
        "Loïc Barrault",
        "Marta R Costa-Jussà"
      ],
      "year": "2023",
      "venue": "Halomi: A manually annotated benchmark for multilingual hallucination and omission detection in machine translation",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "Mildsum: A novel benchmark dataset for multilingual summarization of indian legal case judgments",
      "authors": [
        "Debtanu Datta",
        "Shubham Soni",
        "Rajdeep Mukherjee",
        "Saptarshi Ghosh"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "Automated hate speech detection and the problem of offensive language",
      "authors": [
        "Thomas Davidson",
        "Dana Warmsley",
        "Michael Macy",
        "Ingmar Weber"
      ],
      "year": "2017",
      "venue": "Proceedings of the international AAAI conference on web and social media",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "Zeroshot cross-lingual sentiment classification under distribution shift: an exploratory study",
      "authors": [
        "Maarten De Raedt",
        "Semere Kiros Bitew",
        "Fréderic Godin",
        "Thomas Demeester",
        "Chris Develder"
      ],
      "year": "2023",
      "venue": "Zeroshot cross-lingual sentiment classification under distribution shift: an exploratory study",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "Scaling in cognitive modelling: a multilingual approach to human reading times",
      "authors": [
        "Andrea De",
        "Varda",
        "Marco Marelli"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-short.14"
    },
    {
      "id": "b69",
      "title": "Knowledge extraction in low-resource scenarios: survey and perspective",
      "authors": [
        "Shumin Deng",
        "Ningyu Zhang",
        "Feiyu Xiong",
        "Jeff Z Pan",
        "Huajun Chen"
      ],
      "year": "2022",
      "venue": "Knowledge extraction in low-resource scenarios: survey and perspective",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "Qlora: Efficient finetuning of quantized llms",
      "authors": [
        "Tim Dettmers",
        "Artidoro Pagnoni",
        "Ari Holtzman",
        "Luke Zettlemoyer"
      ],
      "year": "2023",
      "venue": "Qlora: Efficient finetuning of quantized llms",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "A survey of code-switching: Linguistic and social perspectives for language technologies",
      "authors": [
        "Sunayana Seza Dogruöz",
        "Barbara E Sitaram",
        "Bullock",
        "Jacqueline Almeida",
        "Toribio"
      ],
      "year": "2023",
      "venue": "A survey of code-switching: Linguistic and social perspectives for language technologies",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "Representativeness as a forgotten lesson for multilingual and code-switched data collection and preparation",
      "authors": [
        "A Seza Dogruöz",
        "Sunayana Sitaram",
        "Zheng Xin",
        "Yong"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": "10.18653/v1/2023.findings-emnlp.382"
    },
    {
      "id": "b73",
      "title": "A survey for in-context learning",
      "authors": [
        "Qingxiu Dong",
        "Lei Li",
        "Damai Dai",
        "Ce Zheng",
        "Zhiyong Wu",
        "Baobao Chang",
        "Xu Sun",
        "Jingjing Xu",
        "Zhifang Sui"
      ],
      "year": "2022",
      "venue": "A survey for in-context learning",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf",
      "authors": [
        "Yi Dong",
        "Zhilin Wang",
        "Makesh Sreedhar",
        "Xianchao Wu",
        "Oleksii Kuchaiev"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "Palm-e: An embodied multimodal language model",
      "authors": [
        "Danny Driess",
        "Fei Xia",
        "S M Mehdi",
        "Corey Sajjadi",
        "Aakanksha Lynch",
        "Brian Chowdhery",
        "Ayzaan Ichter",
        "Jonathan Wahid",
        "Quan Tompson",
        "Tianhe Vuong",
        "Yu"
      ],
      "year": "2023",
      "venue": "Palm-e: An embodied multimodal language model",
      "doi": ""
    },
    {
      "id": "b76",
      "title": "Multilingual coarse political stance classification of media. the editorial line of a chatgpt and bard newspaper",
      "authors": [],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": ""
    },
    {
      "id": "b77",
      "title": "Do multilingual language models think better in english? arXiv preprint",
      "authors": [
        "Julen Etxaniz",
        "Gorka Azkune",
        "Aitor Soroa",
        "Oier Lopez De Lacalle",
        "Mikel Artetxe"
      ],
      "year": "2023",
      "venue": "Do multilingual language models think better in english? arXiv preprint",
      "doi": ""
    },
    {
      "id": "b78",
      "title": "Beyond english-centric multilingual machine translation",
      "authors": [
        "Angela Fan",
        "Shruti Bhosale",
        "Holger Schwenk",
        "Zhiyi Ma",
        "Ahmed El-Kishky",
        "Siddharth Goyal",
        "Mandeep Baines",
        "Onur Celebi",
        "Guillaume Wenzek",
        "Vishrav Chaudhary"
      ],
      "year": "2021",
      "venue": "The Journal of Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b79",
      "title": "Meep: Is this engaging? prompting large language models for dialogue evaluation in multilingual settings",
      "authors": [
        "Amila Ferron",
        "Amber Shore",
        "Ekata Mitra",
        "Ameeta Agrawal"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": ""
    },
    {
      "id": "b80",
      "title": "Multiconer v2: a large multilingual dataset for fine-grained and noisy named entity recognition",
      "authors": [
        "Besnik Fetahu",
        "Zhiyu Chen",
        "Sudipta Kar",
        "Oleg Rokhlenko",
        "Shervin Malmasi"
      ],
      "year": "2023",
      "venue": "Multiconer v2: a large multilingual dataset for fine-grained and noisy named entity recognition",
      "doi": ""
    },
    {
      "id": "b81",
      "title": "Massive: A 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages",
      "authors": [
        "Jack Fitzgerald",
        "Christopher Hench",
        "Charith Peris",
        "Scott Mackie",
        "Kay Rottmann",
        "Ana Sanchez",
        "Aaron Nash",
        "Liam Urbach",
        "Vishesh Kakarala",
        "Richa Singh"
      ],
      "year": "2022",
      "venue": "Massive: A 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages",
      "doi": ""
    },
    {
      "id": "b82",
      "title": "Wikimedia Foundation. Wikimedia downloads",
      "authors": [],
      "year": "",
      "venue": "Wikimedia Foundation. Wikimedia downloads",
      "doi": ""
    },
    {
      "id": "b83",
      "title": "Polyglot prompt: Multilingual multitask promptraining",
      "authors": [
        "Jinlan Fu",
        "See-Kiong Ng",
        "Pengfei Liu"
      ],
      "year": "2022",
      "venue": "Polyglot prompt: Multilingual multitask promptraining",
      "doi": ""
    },
    {
      "id": "b84",
      "title": "Terufumi Morishita, and Yasuhiro Sogawa. 2023. How do different tokenizers perform on downstream tasks in scriptio continua languages?",
      "authors": [
        "Takuro Fujii",
        "Koki Shibata",
        "Atsuki Yamaguchi"
      ],
      "year": "",
      "venue": "A case study in japanese",
      "doi": ""
    },
    {
      "id": "b85",
      "title": "A multi-modal multilingual benchmark for document image classification",
      "authors": [
        "Yoshinari Fujinuma",
        "Siddharth Varia",
        "Nishant Sankaran",
        "Srikar Appalaraju",
        "Bonan Min",
        "Yogarshi Vyas"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": ""
    },
    {
      "id": "b86",
      "title": "Normsage: Multi-lingual multi-cultural norm discovery from conversations on-the-fly",
      "authors": [
        "Tuhin Yi R Fung",
        "Hao Chakraborty",
        "Owen Guo",
        "Smaranda Rambow",
        "Heng Muresan",
        "Ji"
      ],
      "year": "2022",
      "venue": "Normsage: Multi-lingual multi-cultural norm discovery from conversations on-the-fly",
      "doi": ""
    },
    {
      "id": "b87",
      "title": "Towards boosting many-to-many multilingual machine translation with large language models",
      "authors": [
        "Pengzhi Gao",
        "Zhongjun He",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "year": "2024",
      "venue": "Towards boosting many-to-many multilingual machine translation with large language models",
      "doi": ""
    },
    {
      "id": "b88",
      "title": "Introducing bode: A finetuned large language model for portuguese promptbased task",
      "authors": [
        "Lino Gabriel",
        "Pedro Henrique Garcia",
        "Luis Paiola",
        "Henrique Morelli",
        "Giovani Candido",
        "Cândido Arnaldo",
        "Danilo Júnior",
        "Luis Samuel Jodas",
        "Ivan Afonso",
        "Bruno Elias Rizzo Guilherme",
        "João Penteado",
        "Papa Paulo"
      ],
      "year": "2024",
      "venue": "Introducing bode: A finetuned large language model for portuguese promptbased task",
      "doi": ""
    },
    {
      "id": "b89",
      "title": "The unreasonable effectiveness of fewshot learning for machine translation",
      "authors": [
        "Xavier Garcia",
        "Yamini Bansal",
        "Colin Cherry",
        "George Foster",
        "Maxim Krikun",
        "Melvin Johnson",
        "Orhan Firat"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b90",
      "title": "mblip: Efficient bootstrapping of multilingual vision-llms",
      "authors": [
        "Gregor Geigle",
        "Abhay Jain",
        "Radu Timofte",
        "Goran Glavaš"
      ],
      "year": "2023",
      "venue": "mblip: Efficient bootstrapping of multilingual vision-llms",
      "doi": ""
    },
    {
      "id": "b91",
      "title": "Trueteacher: Learning factual consistency evaluation with large language models",
      "authors": [
        "Zorik Gekhman",
        "Jonathan Herzig",
        "Roee Aharoni",
        "Chen Elkind",
        "Idan Szpektor"
      ],
      "year": "2023",
      "venue": "Trueteacher: Learning factual consistency evaluation with large language models",
      "doi": ""
    },
    {
      "id": "b92",
      "title": "Presto: A multilingual dataset for parsing realistic task-oriented dialogs",
      "authors": [
        "Rahul Goel",
        "Waleed Ammar",
        "Aditya Gupta",
        "Siddharth Vashishtha",
        "Motoki Sano",
        "Faiz Surani",
        "Max Chang",
        "Hyunjeong Choe",
        "David Greene",
        "Kyle He"
      ],
      "year": "2023",
      "venue": "Presto: A multilingual dataset for parsing realistic task-oriented dialogs",
      "doi": ""
    },
    {
      "id": "b93",
      "title": "Explanatory argument extraction of correct answers in resident medical exams",
      "authors": [
        "Iakes Goenaga",
        "Aitziber Atutxa",
        "Koldo Gojenola",
        "Maite Oronoz",
        "Rodrigo Agerri"
      ],
      "year": "2023",
      "venue": "Explanatory argument extraction of correct answers in resident medical exams",
      "doi": ""
    },
    {
      "id": "b94",
      "title": "Roscoe: A suite of metrics for scoring step-by-step reasoning",
      "authors": [
        "O Yu",
        "Moya Golovneva",
        "Spencer Chen",
        "Martin Poff",
        "Luke Corredor",
        "Maryam Zettlemoyer",
        "Asli Fazel-Zarandi",
        "Celikyilmaz"
      ],
      "year": "2022",
      "venue": "Roscoe: A suite of metrics for scoring step-by-step reasoning",
      "doi": ""
    },
    {
      "id": "b95",
      "title": "The flores-101 evaluation benchmark for low-resource and multilingual machine translation",
      "authors": [
        "Naman Goyal",
        "Cynthia Gao",
        "Vishrav Chaudhary",
        "Peng-Jen Chen",
        "Guillaume Wenzek",
        "Da Ju",
        "Sanjana Krishnan",
        "Marc'aurelio Ranzato",
        "Francisco Guzmán",
        "Angela Fan"
      ],
      "year": "2022",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b96",
      "title": "Hallucinations in large multilingual translation models",
      "authors": [
        "Duarte Nuno M Guerreiro",
        "Jonas Alves",
        "Barry Waldendorf",
        "Alexandra Haddow",
        "Pierre Birch",
        "André Ft Colombo",
        "Martins"
      ],
      "year": "2023",
      "venue": "Hallucinations in large multilingual translation models",
      "doi": ""
    },
    {
      "id": "b97",
      "title": "Pierre Colombo, and André FT Martins. 2023b. xcomet: Transparent machine translation evaluation through fine-grained error detection",
      "authors": [
        "Ricardo Nuno M Guerreiro",
        "Daan Rei",
        "Luisa Van Stigt",
        "Coheur"
      ],
      "year": "",
      "venue": "Pierre Colombo, and André FT Martins. 2023b. xcomet: Transparent machine translation evaluation through fine-grained error detection",
      "doi": ""
    },
    {
      "id": "b98",
      "title": "Jwsign: A highly multilingual corpus of bible translations for more diversity in sign language processing",
      "authors": [
        "Shester Gueuwou",
        "Sophie Siake",
        "Colin Leong",
        "Mathias Müller"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": ""
    },
    {
      "id": "b99",
      "title": "Bridging the gap between synthetic and authentic images for multimodal machine translation",
      "authors": [
        "Wenyu Guo",
        "Qingkai Fang",
        "Dong Yu",
        "Yang Feng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b100",
      "title": "Are large language model-based evaluators the solution to scaling up multilingual evaluation",
      "authors": [
        "Rishav Hada",
        "Varun Gumma",
        "Adrian De Wynter",
        "Harshita Diddee",
        "Mohamed Ahmed",
        "Monojit Choudhury",
        "Kalika Bali",
        "Sunayana Sitaram"
      ],
      "year": "2023",
      "venue": "Are large language model-based evaluators the solution to scaling up multilingual evaluation",
      "doi": ""
    },
    {
      "id": "b101",
      "title": "Do multilingual language models capture differing moral norms? arXiv preprint",
      "authors": [
        "Katharina Hämmerl",
        "Björn Deiseroth",
        "Patrick Schramowski",
        "Jindřich Libovickỳ",
        "Alexander Fraser",
        "Kristian Kersting"
      ],
      "year": "2022",
      "venue": "Do multilingual language models capture differing moral norms? arXiv preprint",
      "doi": ""
    },
    {
      "id": "b102",
      "title": "Bridging background knowledge gaps in translation with automatic explicitation",
      "authors": [
        "Hyojung Han",
        "Jordan Boyd-Graber",
        "Marine Carpuat"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b103",
      "title": "Exams: A multi-subject high school examinations dataset for cross-lingual and multilingual question answering",
      "authors": [
        "Momchil Hardalov",
        "Todor Mihaylov",
        "Dimitrina Zlatkova",
        "Yoan Dinkov",
        "Ivan Koychev",
        "Preslav Nakov"
      ],
      "year": "2020",
      "venue": "Exams: A multi-subject high school examinations dataset for cross-lingual and multilingual question answering",
      "doi": ""
    },
    {
      "id": "b104",
      "title": "Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models",
      "authors": [
        "Conghui He",
        "Zhenjiang Jin",
        "Chao Xu",
        "Jiantao Qiu",
        "Bin Wang",
        "Wei Li",
        "Hang Yan",
        "Jiaqi Wang",
        "Dahua Lin"
      ],
      "year": "2023",
      "venue": "Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models",
      "doi": ""
    },
    {
      "id": "b105",
      "title": "Shuming Shi, and Xing Wang. 2023b. Exploring humanlike translation strategy with large language models",
      "authors": [
        "Zhiwei He",
        "Tian Liang",
        "Wenxiang Jiao",
        "Zhuosheng Zhang",
        "Yujiu Yang",
        "Rui Wang",
        "Zhaopeng Tu"
      ],
      "year": "",
      "venue": "Shuming Shi, and Xing Wang. 2023b. Exploring humanlike translation strategy with large language models",
      "doi": ""
    },
    {
      "id": "b106",
      "title": "A material lens on coloniality in nlp",
      "authors": [
        "William Held",
        "Camille Harris",
        "Michael Best",
        "Diyi Yang"
      ],
      "year": "2023",
      "venue": "A material lens on coloniality in nlp",
      "doi": ""
    },
    {
      "id": "b107",
      "title": "",
      "authors": [
        "Daniel Hershcovich",
        "Stella Frank",
        "Heather Lent",
        "Mostafa Miryam De Lhoneux",
        "Stephanie Abdou"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b108",
      "title": "Challenges and strategies in cross-cultural nlp",
      "authors": [
        "Emanuele Brandl",
        "Laura Cabello Bugliarello",
        "Ilias Piqueras",
        "Ruixiang Chalkidis",
        "Cui"
      ],
      "year": "2022",
      "venue": "Challenges and strategies in cross-cultural nlp",
      "doi": ""
    },
    {
      "id": "b109",
      "title": "Chinese-mixtral-8x7b: An opensource mixture-of-experts llm",
      "authors": [
        "Hit-Scir"
      ],
      "year": "2024",
      "venue": "Chinese-mixtral-8x7b: An opensource mixture-of-experts llm",
      "doi": ""
    },
    {
      "id": "b110",
      "title": "Empowering cross-lingual behavioral testing of nlp models with typological features",
      "authors": [
        "Ester Hlavnova",
        "Sebastian Ruder"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b111",
      "title": "On-the-fly fusion of large language models and machine translation",
      "authors": [
        "Hieu Hoang",
        "Huda Khayrallah",
        "Marcin Junczys-Dowmunt"
      ],
      "year": "2023",
      "venue": "On-the-fly fusion of large language models and machine translation",
      "doi": ""
    },
    {
      "id": "b112",
      "title": "Making instruction finetuning accessible to nonenglish languages: A case study on swedish models",
      "authors": [
        "Oskar Holmström",
        "Ehsan Doostmohammadi"
      ],
      "year": "2023",
      "venue": "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)",
      "doi": ""
    },
    {
      "id": "b113",
      "title": "Bridging the resource gap: Exploring the efficacy of english and multilingual llms for swedish",
      "authors": [
        "Oskar Holmström",
        "Jenny Kunz",
        "Marco Kuhlmann"
      ],
      "year": "2023",
      "venue": "Proceedings of the Second Workshop on Resources and Representations for Under-Resourced Languages and Domains (RESOURCEFUL-2023)",
      "doi": ""
    },
    {
      "id": "b114",
      "title": "2023a. Large multilingual models pivot zero-shot multimodal learning across languages",
      "authors": [
        "Jinyi Hu",
        "Yuan Yao",
        "Chongyi Wang",
        "Shan Wang",
        "Yinxu Pan",
        "Qianyu Chen",
        "Tianyu Yu",
        "Hanghao Wu",
        "Yue Zhao",
        "Haoye Zhang"
      ],
      "year": "",
      "venue": "2023a. Large multilingual models pivot zero-shot multimodal learning across languages",
      "doi": ""
    },
    {
      "id": "b115",
      "title": "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
      "authors": [
        "Junjie Hu",
        "Sebastian Ruder",
        "Aditya Siddhant",
        "Graham Neubig",
        "Orhan Firat",
        "Melvin Johnson"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b116",
      "title": "Yu Qiao, and Ping Luo. 2023b. Tree-planner: Efficient close-loop task planning with large language models",
      "authors": [
        "Mengkang Hu",
        "Yao Mu",
        "Xinmiao Yu",
        "Mingyu Ding",
        "Shiguang Wu",
        "Wenqi Shao",
        "Qiguang Chen",
        "Bin Wang"
      ],
      "year": "",
      "venue": "Yu Qiao, and Ping Luo. 2023b. Tree-planner: Efficient close-loop task planning with large language models",
      "doi": ""
    },
    {
      "id": "b117",
      "title": "Dialight: Lightweight multilingual development and evaluation of taskoriented dialogue systems with large language models",
      "authors": [
        "Songbo Hu",
        "Xiaobin Wang",
        "Zhangdie Yuan",
        "Anna Korhonen",
        "Ivan Vulić"
      ],
      "year": "2024",
      "venue": "Dialight: Lightweight multilingual development and evaluation of taskoriented dialogue systems with large language models",
      "doi": ""
    },
    {
      "id": "b118",
      "title": "Multi 3 woz: A multilingual, multi-domain, multi-parallel dataset for training and evaluating culturally adapted task-oriented dialog systems",
      "authors": [
        "Songbo Hu",
        "Han Zhou",
        "Mete Hergul",
        "Milan Gritta",
        "Guchun Zhang",
        "Ignacio Iacobacci",
        "Ivan Vulić",
        "Anna Korhonen"
      ],
      "year": "2023",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b119",
      "title": "Lacos-bloom: Low-rank adaptation with contrastive objective on 8 bits siamese-bloom",
      "authors": [
        "Wen-Yu Hua",
        "Brian Williams",
        "Davood Shamsi"
      ],
      "year": "2023",
      "venue": "Lacos-bloom: Low-rank adaptation with contrastive objective on 8 bits siamese-bloom",
      "doi": ""
    },
    {
      "id": "b120",
      "title": "2023a. Not all languages are created equal in llms: Improving multilingual capability by cross-lingual-thought prompting",
      "authors": [
        "Haoyang Huang",
        "Tianyi Tang",
        "Dongdong Zhang",
        "Wayne Xin Zhao",
        "Ting Song",
        "Yan Xia",
        "Furu Wei"
      ],
      "year": "",
      "venue": "2023a. Not all languages are created equal in llms: Improving multilingual capability by cross-lingual-thought prompting",
      "doi": ""
    },
    {
      "id": "b121",
      "title": "Multilingual and fully non-autoregressive asr with large language model fusion: A comprehensive study",
      "authors": [
        "Ronny Huang",
        "Cyril Allauzen",
        "Tongzhou Chen",
        "Kilol Gupta",
        "Ke Hu",
        "James Qin",
        "Yu Zhang",
        "Yongqiang Wang",
        "Shuo-Yiin Chang",
        "Tara N Sainath"
      ],
      "year": "2024",
      "venue": "Multilingual and fully non-autoregressive asr with large language model fusion: A comprehensive study",
      "doi": ""
    },
    {
      "id": "b122",
      "title": "Speech translation with large language models: An industrial practice",
      "authors": [
        "Zhichao Huang",
        "Rong Ye",
        "Tom Ko",
        "Qianqian Dong",
        "Shanbo Cheng",
        "Mingxuan Wang",
        "Hang Li"
      ],
      "year": "2023",
      "venue": "Speech translation with large language models: An industrial practice",
      "doi": ""
    },
    {
      "id": "b123",
      "title": "Constanza Fierro, Shashi Narayan, and Mirella Lapata. 2023. µplan: Summarizing using a content plan as cross-lingual bridge",
      "authors": [
        "Fantine Huot",
        "Joshua Maynez",
        "Chris Alberti",
        "Reinald Kim Amplayo",
        "Priyanka Agrawal"
      ],
      "year": "",
      "venue": "Constanza Fierro, Shashi Narayan, and Mirella Lapata. 2023. µplan: Summarizing using a content plan as cross-lingual bridge",
      "doi": ""
    },
    {
      "id": "b124",
      "title": "Scaling multilingual corpora and language models to 500 languages",
      "authors": [
        "Ayyoob Imanigooghari",
        "Peiqin Lin",
        "Amir Hossein Kargaran",
        "Silvia Severini",
        "Jalili Masoud",
        "Nora Sabet",
        "Chunlan Kassner",
        "Helmut Ma",
        "Schmid",
        "F T André",
        "François Martins",
        "Yvon"
      ],
      "year": "2023",
      "venue": "Scaling multilingual corpora and language models to 500 languages",
      "doi": ""
    },
    {
      "id": "b125",
      "title": "Towards effective disambiguation for machine translation with large language models",
      "authors": [
        "Vivek Iyer",
        "Pinzhen Chen",
        "Alexandra Birch"
      ],
      "year": "2023",
      "venue": "Proceedings of the Eighth Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b126",
      "title": "Cross-lingual offensive language detection: A systematic review of datasets, transfer approaches and challenges",
      "authors": [
        "Aiqi Jiang",
        "Arkaitz Zubiaga"
      ],
      "year": "2024",
      "venue": "Cross-lingual offensive language detection: A systematic review of datasets, transfer approaches and challenges",
      "doi": ""
    },
    {
      "id": "b127",
      "title": "Mistral 7b",
      "authors": [
        "Alexandre Albert Q Jiang",
        "Arthur Sablayrolles",
        "Chris Mensch",
        "Devendra Bamford",
        "Diego Singh Chaplot",
        "Florian De Las Casas",
        "Gianna Bressand",
        "Guillaume Lengyel",
        "Lucile Lample",
        "Saulnier"
      ],
      "year": "2023",
      "venue": "Mistral 7b",
      "doi": ""
    },
    {
      "id": "b128",
      "title": "Mixtral of experts",
      "authors": [
        "Alexandre Albert Q Jiang",
        "Antoine Sablayrolles",
        "Arthur Roux",
        "Blanche Mensch",
        "Chris Savary",
        "Devendra Bamford",
        "Diego Singh Chaplot",
        "Emma Bou De Las Casas",
        "Florian Hanna",
        "Bressand"
      ],
      "year": "2024",
      "venue": "Mixtral of experts",
      "doi": ""
    },
    {
      "id": "b129",
      "title": "Cpopqa: Ranking cultural concept popularity by llms",
      "authors": [
        "Ming Jiang",
        "Mansi Joshi"
      ],
      "year": "2023",
      "venue": "Cpopqa: Ranking cultural concept popularity by llms",
      "doi": ""
    },
    {
      "id": "b130",
      "title": "Parrot: Translating during chat using large language models tuned with human translation and feedback",
      "authors": [
        "Wenxiang Jiao",
        "Jen-Tse Huang",
        "Wenxuan Wang",
        "Zhiwei He",
        "Tian Liang",
        "Xing Wang",
        "Shuming Shi",
        "Zhaopeng Tu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": ""
    },
    {
      "id": "b131",
      "title": "Multilingual simplification of medical texts",
      "authors": [
        "Sebastian Joseph",
        "Kathryn Kazanas",
        "Keziah Reina",
        "J Vishnesh",
        "Wei Ramanathan",
        "Byron C Xu",
        "Junyi Jessy Wallace",
        "Li"
      ],
      "year": "2023",
      "venue": "Multilingual simplification of medical texts",
      "doi": ""
    },
    {
      "id": "b132",
      "title": "Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, and Graham Neubig. 2023. Multi-lingual and multi-cultural figurative language understanding",
      "authors": [
        "Anubha Kabra",
        "Emmy Liu",
        "Simran Khanuja"
      ],
      "year": "",
      "venue": "Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, and Graham Neubig. 2023. Multi-lingual and multi-cultural figurative language understanding",
      "doi": ""
    },
    {
      "id": "b133",
      "title": "2021. nmt5-is parallel data still relevant for pre-training massively multilingual language models? arXiv preprint",
      "authors": [
        "Mihir Kale",
        "Aditya Siddhant",
        "Noah Constant",
        "Melvin Johnson",
        "Rami Al-Rfou",
        "Linting Xue"
      ],
      "year": "",
      "venue": "2021. nmt5-is parallel data still relevant for pre-training massively multilingual language models? arXiv preprint",
      "doi": ""
    },
    {
      "id": "b134",
      "title": "The multilingual amazon reviews corpus",
      "authors": [
        "Phillip Keung",
        "Yichao Lu",
        "György Szarvas",
        "Noah A Smith"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": ""
    },
    {
      "id": "b135",
      "title": "Turning english-centric llms into polyglots: How much multilinguality is needed",
      "authors": [
        "Tannon Kew",
        "Florian Schottmann",
        "Rico Sennrich"
      ],
      "year": "2023",
      "venue": "Turning english-centric llms into polyglots: How much multilinguality is needed",
      "doi": ""
    },
    {
      "id": "b136",
      "title": "A study of multilingual versus meta-learning for language model pre-training for adaptation to unseen low resource languages",
      "authors": [
        "Jyotsana Khatri",
        "Rudra Murthy",
        "Amar Prakash Azad",
        "Pushpak Bhattacharyya"
      ],
      "year": "2023",
      "venue": "Proceedings of Machine Translation Summit XIX",
      "doi": ""
    },
    {
      "id": "b137",
      "title": "Gptaraeval: A comprehensive evaluation of chatgpt on arabic nlp",
      "authors": [],
      "year": "2023",
      "venue": "Gptaraeval: A comprehensive evaluation of chatgpt on arabic nlp",
      "doi": ""
    },
    {
      "id": "b138",
      "title": "What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers",
      "authors": [
        "Boseop Kim",
        "Hyoungseok Kim",
        "Sang-Woo Lee",
        "Gichang Lee",
        "Donghyun Kwak",
        "Jeon Dong Hyeon",
        "Sunghyun Park",
        "Sungju Kim",
        "Seonhoon Kim",
        "Dongpil Seo"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b139",
      "title": "Boosting cross-lingual transferability in multilingual models via in-context learning",
      "authors": [
        "Sunkyoung Kim",
        "Dayeon Ki",
        "Yireun Kim",
        "Jinsik Lee"
      ],
      "year": "2023",
      "venue": "Boosting cross-lingual transferability in multilingual models via in-context learning",
      "doi": ""
    },
    {
      "id": "b140",
      "title": "2023b. Pr-mcs: Perturbation robust metric for multilingual image captioning",
      "authors": [
        "Yongil Kim",
        "Yerin Hwang",
        "Hyeongu Yun",
        "Seunghyun Yoon",
        "Trung Bui",
        "Kyomin Jung"
      ],
      "year": "",
      "venue": "2023b. Pr-mcs: Perturbation robust metric for multilingual image captioning",
      "doi": ""
    },
    {
      "id": "b141",
      "title": "Findings of the 2023 conference on machine translation (wmt23): Llms are here but not quite there yet",
      "authors": [
        "Tom Kocmi",
        "Eleftherios Avramidis",
        "Rachel Bawden",
        "Ondřej Bojar",
        "Anton Dvorkovich",
        "Christian Federmann",
        "Mark Fishel",
        "Markus Freitag",
        "Thamme Gowda",
        "Roman Grundkiewicz"
      ],
      "year": "2023",
      "venue": "Proceedings of the Eighth Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b142",
      "title": "Europarl: A parallel corpus for statistical machine translation",
      "authors": [
        "Philipp Koehn"
      ],
      "year": "2005",
      "venue": "Proceedings of machine translation summit x: papers",
      "doi": ""
    },
    {
      "id": "b143",
      "title": "Building a llama2-finetuned llm for odia language utilizing domain knowledge instruction set",
      "authors": [
        "Guneet Singh Kohli",
        "Shantipriya Parida",
        "Sambit Sekhar",
        "Samirit Saha",
        "Nipun B Nair",
        "Parul Agarwal",
        "Sonal Khosla",
        "Kusumlata Patiyal",
        "Debasish Dhal"
      ],
      "year": "2023",
      "venue": "Building a llama2-finetuned llm for odia language utilizing domain knowledge instruction set",
      "doi": ""
    },
    {
      "id": "b144",
      "title": "Memory-efficient nllb-200: Language-specific expert pruning of a massively multilingual machine translation model",
      "authors": [
        "Yeskendir Koishekenov",
        "Vassilina Nikoulina",
        "Alexandre Berard"
      ],
      "year": "2022",
      "venue": "Memory-efficient nllb-200: Language-specific expert pruning of a massively multilingual machine translation model",
      "doi": ""
    },
    {
      "id": "b145",
      "title": "Openassistant conversations-democratizing large language model alignment",
      "authors": [
        "Andreas Köpf",
        "Yannic Kilcher",
        "Sotiris Dimitri Von Rütte",
        "Zhi-Rui Anagnostidis",
        "Keith Tam",
        "Abdullah Stevens",
        "Barhoum",
        "Minh Nguyen",
        "Oliver Duc",
        "Richárd Stanley",
        "Nagyfi"
      ],
      "year": "2023",
      "venue": "Openassistant conversations-democratizing large language model alignment",
      "doi": ""
    },
    {
      "id": "b146",
      "title": "Madlad-400: A multilingual and document-level large audited dataset",
      "authors": [
        "Sneha Kudugunta",
        "Isaac Rayburn Caswell",
        "Biao Zhang",
        "Xavier Garcia",
        "Derrick Xin",
        "Aditya Kusupati",
        "Romi Stella",
        "Ankur Bapna",
        "Orhan Firat"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track",
      "doi": ""
    },
    {
      "id": "b147",
      "title": "The IIT Bombay English-Hindi parallel corpus",
      "authors": [
        "Anoop Kunchukuttan",
        "Pratik Mehta",
        "Pushpak Bhattacharyya"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)",
      "doi": ""
    },
    {
      "id": "b148",
      "title": "Dialect-to-standard normalization: A largescale multilingual evaluation",
      "authors": [
        "Aleksandra Olli Kuparinen",
        "Yves Miletić",
        "Scherrer"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": ""
    },
    {
      "id": "b149",
      "title": "2023a. Beyond english: Evaluating llms for arabic grammatical error correction",
      "authors": [
        "Sang Kwon",
        "Gagan Bhatia",
        "Muhammad Abdul-Mageed"
      ],
      "year": "",
      "venue": "Proceedings of ArabicNLP 2023",
      "doi": ""
    },
    {
      "id": "b150",
      "title": "El Moatez Billah Nagoud, and Muhammad Abdul-Mageed. 2023b. Chatgpt for arabic grammatical error correction",
      "authors": [
        "Sang Yun Kwon",
        "Gagan Bhatia"
      ],
      "year": "",
      "venue": "El Moatez Billah Nagoud, and Muhammad Abdul-Mageed. 2023b. Chatgpt for arabic grammatical error correction",
      "doi": ""
    },
    {
      "id": "b151",
      "title": "Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning",
      "authors": [
        "Dac Viet",
        "Nghia Trung Lai",
        "Amir Ngo",
        "Ben Pouran",
        "Hieu Veyseh",
        "Franck Man",
        "Trung Dernoncourt",
        "Thien Huu Bui",
        "Nguyen"
      ],
      "year": "2023",
      "venue": "Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning",
      "doi": ""
    },
    {
      "id": "b152",
      "title": "Okapi: Instructiontuned large language models in multiple languages with reinforcement learning from human feedback",
      "authors": [
        "Dac Viet",
        "Chien Lai",
        "Nghia Trung Van Nguyen",
        "Thuat Ngo",
        "Franck Nguyen",
        "Ryan A Dernoncourt",
        "Thien Huu Rossi",
        "Nguyen"
      ],
      "year": "2023",
      "venue": "Okapi: Instructiontuned large language models in multiple languages with reinforcement learning from human feedback",
      "doi": ""
    },
    {
      "id": "b153",
      "title": "Cabrita: closing the gap for foreign languages",
      "authors": [
        "Celio Larcher",
        "Marcos Piau",
        "Paulo Finardi",
        "Pedro Gengo",
        "Piero Esposito",
        "Vinicius Caridá"
      ],
      "year": "2023",
      "venue": "Cabrita: closing the gap for foreign languages",
      "doi": ""
    },
    {
      "id": "b154",
      "title": "The bigscience roots corpus: A 1.6 tb composite multilingual dataset",
      "authors": [
        "Lucile Hugo Laurençon",
        "Thomas Saulnier",
        "Christopher Wang",
        "Albert Akiki",
        "Teven Villanova Del Moral",
        "Leandro Le Scao",
        "Chenghao Von Werra",
        "Eduardo González Mou",
        "Huu Ponferrada",
        "Nguyen"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b155",
      "title": "Lampat: Low-rank adaption for multilingual paraphrasing using adversarial training",
      "authors": [
        "Trinh Khoi M Le",
        "Tho Pham",
        "Anh Tuan Quan",
        "Luu"
      ],
      "year": "2024",
      "venue": "Lampat: Low-rank adaption for multilingual paraphrasing using adversarial training",
      "doi": ""
    },
    {
      "id": "b156",
      "title": "Target-agnostic gender-aware contrastive learning for mitigating bias in multilingual machine translation",
      "authors": [
        "Minwoo Lee",
        "Hyukhun Koh",
        "Kang-Il Lee",
        "Dongdong Zhang",
        "Minsung Kim",
        "Kyomin Jung"
      ],
      "year": "2023",
      "venue": "Target-agnostic gender-aware contrastive learning for mitigating bias in multilingual machine translation",
      "doi": ""
    },
    {
      "id": "b157",
      "title": "Gshard: Scaling giant models with conditional computation and automatic sharding",
      "authors": [
        "Dmitry Lepikhin",
        "Hyoukjoong Lee",
        "Yuanzhong Xu",
        "Dehao Chen",
        "Orhan Firat",
        "Yanping Huang",
        "Maxim Krikun",
        "Noam Shazeer",
        "Zhifeng Chen"
      ],
      "year": "2020",
      "venue": "Gshard: Scaling giant models with conditional computation and automatic sharding",
      "doi": ""
    },
    {
      "id": "b158",
      "title": "Mlqa: Evaluating cross-lingual extractive question answering",
      "authors": [
        "Patrick Lewis",
        "Barlas Oguz",
        "Ruty Rinott",
        "Sebastian Riedel",
        "Holger Schwenk"
      ],
      "year": "2019",
      "venue": "Mlqa: Evaluating cross-lingual extractive question answering",
      "doi": ""
    },
    {
      "id": "b159",
      "title": "This land is {Your",
      "authors": [
        "Bryan Li",
        "Chris Callison-Burch"
      ],
      "year": "2023",
      "venue": "My} land: Evaluating geopolitical biases in language models",
      "doi": ""
    },
    {
      "id": "b160",
      "title": "2023a. Align after pre-train: Improving multilingual generative models with cross-lingual alignment",
      "authors": [
        "Chong Li",
        "Shaonan Wang",
        "Jiajun Zhang",
        "Chengqing Zong"
      ],
      "year": "",
      "venue": "2023a. Align after pre-train: Improving multilingual generative models with cross-lingual alignment",
      "doi": ""
    },
    {
      "id": "b161",
      "title": "Bactrian-x: A multilingual replicable instruction-following model with lowrank adaptation",
      "authors": [
        "Haonan Li",
        "Fajri Koto",
        "Minghao Wu",
        "Alham Fikri Aji",
        "Timothy Baldwin"
      ],
      "year": "2020",
      "venue": "Bactrian-x: A multilingual replicable instruction-following model with lowrank adaptation",
      "doi": ""
    },
    {
      "id": "b162",
      "title": "Eliciting the translation ability of large language models via multilingual finetuning with translation instructions",
      "authors": [
        "Jiahuan Li",
        "Hao Zhou",
        "Shujian Huang",
        "Shanbo Chen",
        "Jiajun Chen"
      ],
      "year": "2023",
      "venue": "Eliciting the translation ability of large language models via multilingual finetuning with translation instructions",
      "doi": ""
    },
    {
      "id": "b163",
      "title": "2023d. Normdial: A comparable bilingual synthetic dialog dataset for modeling social norm adherence and violation",
      "authors": [
        "Oliver Li",
        "Mallika Subramanian",
        "Arkadiy Saakyan",
        "Ch-Wang Sky",
        "Smaranda Muresan"
      ],
      "year": "",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b164",
      "title": "2023e. Mmnmt: Modularizing multilingual neural machine translation with flexibly assembled moe and dense blocks",
      "authors": [
        "Shangjie Li",
        "Xiangpeng Wei",
        "Shaolin Zhu",
        "Jun Xie",
        "Baosong Yang",
        "Deyi Xiong"
      ],
      "year": "",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b165",
      "title": "Crosslingual retrieval augmented in-context learning for bangla",
      "authors": [
        "Xiaoqian Li",
        "Ercong Nie",
        "Sheng Liang"
      ],
      "year": "2023",
      "venue": "Proceedings of the First Workshop on Bangla Language Processing (BLP-2023)",
      "doi": ""
    },
    {
      "id": "b166",
      "title": "From classification to generation: Insights into crosslingual retrieval augmented icl",
      "authors": [
        "Xiaoqian Li",
        "Ercong Nie",
        "Sheng Liang"
      ],
      "year": "2023",
      "venue": "From classification to generation: Insights into crosslingual retrieval augmented icl",
      "doi": ""
    },
    {
      "id": "b167",
      "title": "Ecomgpt: Instruction-tuning large language model with chainof-task tasks for e-commerce",
      "authors": [
        "Yangning Li",
        "Shirong Ma",
        "Xiaobin Wang",
        "Shen Huang",
        "Chengyue Jiang",
        "Hai-Tao Zheng",
        "Pengjun Xie",
        "Fei Huang",
        "Yong Jiang"
      ],
      "year": "2023",
      "venue": "Ecomgpt: Instruction-tuning large language model with chainof-task tasks for e-commerce",
      "doi": ""
    },
    {
      "id": "b168",
      "title": "On bilingual lexicon induction with large language models",
      "authors": [
        "Yaoyiran Li",
        "Anna Korhonen",
        "Ivan Vulić"
      ],
      "year": "2023",
      "venue": "On bilingual lexicon induction with large language models",
      "doi": ""
    },
    {
      "id": "b169",
      "title": "Multilingual sentence alignment with gpt models",
      "authors": [
        "Xiao Liang",
        "Yen-Min Jasmina Khaw",
        "Soung-Yue Liew",
        "Tien-Ping Tan",
        "Donghong Qin"
      ],
      "year": "2023",
      "venue": "2023 4th International Conference on Artificial Intelligence and Data Sciences (AiDAS)",
      "doi": ""
    },
    {
      "id": "b170",
      "title": "Xglue: A new benchmark dataset for cross-lingual pretraining, understanding and generation",
      "authors": [
        "Yaobo Liang",
        "Nan Duan",
        "Yeyun Gong",
        "Ning Wu",
        "Fenfei Guo",
        "Weizhen Qi",
        "Ming Gong",
        "Linjun Shou",
        "Daxin Jiang",
        "Guihong Cao"
      ],
      "year": "2020",
      "venue": "Xglue: A new benchmark dataset for cross-lingual pretraining, understanding and generation",
      "doi": ""
    },
    {
      "id": "b171",
      "title": "Xiaoyang Qiao, and Xiang Ren. 2021a. Common sense beyond English: Evaluating and improving multilingual language models for commonsense reasoning",
      "authors": [
        "Seyeon Bill Yuchen Lin",
        "Lee"
      ],
      "year": "",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.102"
    },
    {
      "id": "b172",
      "title": "ROUGE: A package for automatic evaluation of summaries",
      "authors": [
        "Chin-Yew Lin"
      ],
      "year": "2004",
      "venue": "Text Summarization Branches Out",
      "doi": ""
    },
    {
      "id": "b173",
      "title": "André FT Martins, and Hinrich Schütze. 2023. mplm-sim: Unveiling better cross-lingual similarity and transfer in multilingual pretrained language models",
      "authors": [
        "Peiqin Lin",
        "Chengzhi Hu",
        "Zheyu Zhang"
      ],
      "year": "",
      "venue": "André FT Martins, and Hinrich Schütze. 2023. mplm-sim: Unveiling better cross-lingual similarity and transfer in multilingual pretrained language models",
      "doi": ""
    },
    {
      "id": "b174",
      "title": "Veselin Stoyanov, and Xian Li. 2022a. Few-shot learning with multilingual generative language models",
      "authors": [
        "Victoria Xi",
        "Todor Lin",
        "Mikel Mihaylov",
        "Tianlu Artetxe",
        "Shuohui Wang",
        "Daniel Chen",
        "Myle Simig",
        "Naman Ott",
        "Shruti Goyal",
        "Jingfei Bhosale",
        "Ramakanth Du",
        "Sam Pasunuru",
        "Punit Shleifer",
        "Vishrav Singh Koura",
        "Brian O' Chaudhary",
        "Jeff Horo",
        "Luke Wang",
        "Zornitsa Zettlemoyer",
        "Kozareva"
      ],
      "year": "",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2022.emnlp-main.616"
    },
    {
      "id": "b175",
      "title": "Jingfei Du, et al. 2021b. Few-shot learning with multilingual language models",
      "authors": [
        "Victoria Xi",
        "Todor Lin",
        "Mikel Mihaylov",
        "Tianlu Artetxe",
        "Shuohui Wang",
        "Daniel Chen",
        "Myle Simig",
        "Naman Ott",
        "Shruti Goyal",
        "Bhosale"
      ],
      "year": "",
      "venue": "Jingfei Du, et al. 2021b. Few-shot learning with multilingual language models",
      "doi": ""
    },
    {
      "id": "b176",
      "title": "2022b. Few-shot learning with multilingual generative language models",
      "authors": [
        "Victoria Xi",
        "Todor Lin",
        "Mikel Mihaylov",
        "Tianlu Artetxe",
        "Shuohui Wang",
        "Daniel Chen",
        "Myle Simig",
        "Naman Ott",
        "Shruti Goyal",
        "Jingfei Bhosale",
        "Du"
      ],
      "year": "",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b177",
      "title": "Are multilingual llms culturally-diverse reasoners? an investigation into multicultural proverbs and sayings",
      "authors": [
        "Cecilia Chen",
        "Fajri Liu",
        "Timothy Koto",
        "Iryna Baldwin",
        "Gurevych"
      ],
      "year": "2023",
      "venue": "Are multilingual llms culturally-diverse reasoners? an investigation into multicultural proverbs and sayings",
      "doi": ""
    },
    {
      "id": "b178",
      "title": "Jon Atle Gulla, and Zhirong Yang. 2023b. Nlebench+ norglm: A comprehensive empirical analysis and benchmark dataset for generative language models in norwegian",
      "authors": [
        "Peng Liu",
        "Lemei Zhang",
        "Terje Nissen Farup",
        "Even W Lauvrak",
        "Jon Espen Ingvaldsen",
        "Simen Eide"
      ],
      "year": "",
      "venue": "Jon Atle Gulla, and Zhirong Yang. 2023b. Nlebench+ norglm: A comprehensive empirical analysis and benchmark dataset for generative language models in norwegian",
      "doi": ""
    },
    {
      "id": "b179",
      "title": "2023c. Revisiting commonsense reasoning in machine translation: Training, evaluation and challenge",
      "authors": [
        "Xuebo Liu",
        "Yutong Wang",
        "Derek F Wong",
        "Runzhe Zhan",
        "Liangxuan Yu",
        "Min Zhang"
      ],
      "year": "",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b180",
      "title": "Cceval: A representative evaluation benchmark for the chinese-centric multilingual machine translation",
      "authors": [
        "Lianzhang Lou",
        "Xi Yin",
        "Yutao Xie",
        "Yang Xiang"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": ""
    },
    {
      "id": "b181",
      "title": "Chainof-dictionary prompting elicits translation in large language models",
      "authors": [
        "Hongyuan Lu",
        "Haoyang Huang",
        "Dongdong Zhang",
        "Haoran Yang",
        "Wai Lam",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Chainof-dictionary prompting elicits translation in large language models",
      "doi": ""
    },
    {
      "id": "b182",
      "title": "Shifeng Chen, and Dongmei Zhang. 2023a. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
      "authors": [
        "Haipeng Luo",
        "Qingfeng Sun",
        "Can Xu",
        "Pu Zhao",
        "Jianguang Lou",
        "Chongyang Tao",
        "Xiubo Geng",
        "Qingwei Lin"
      ],
      "year": "",
      "venue": "Shifeng Chen, and Dongmei Zhang. 2023a. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
      "doi": ""
    },
    {
      "id": "b183",
      "title": "2023b. Yayi 2: Multilingual open-source large language models",
      "authors": [
        "Yin Luo",
        "Qingchao Kong",
        "Nan Xu",
        "Jia Cao",
        "Bao Hao",
        "Baoyu Qu",
        "Bo Chen",
        "Chao Zhu",
        "Chenyang Zhao",
        "Donglei Zhang"
      ],
      "year": "",
      "venue": "2023b. Yayi 2: Multilingual open-source large language models",
      "doi": ""
    },
    {
      "id": "b184",
      "title": "Aleksandra Piktus, et al. 2023. Fingpt: Large generative models for a small language",
      "authors": [
        "Ville Risto Luukkonen",
        "Jouni Komulainen",
        "Anni Luoma",
        "Jenna Eskelinen",
        "Hanna-Mari Kanerva",
        "Filip Kupari",
        "Veronika Ginter",
        "Niklas Laippala",
        "Muennighoff"
      ],
      "year": "",
      "venue": "Aleksandra Piktus, et al. 2023. Fingpt: Large generative models for a small language",
      "doi": ""
    },
    {
      "id": "b185",
      "title": "New trends in machine translation using large language models: Case examples with chatgpt",
      "authors": [
        "Chenyang Lyu",
        "Jitao Xu",
        "Longyue Wang"
      ],
      "year": "2023",
      "venue": "New trends in machine translation using large language models: Case examples with chatgpt",
      "doi": ""
    },
    {
      "id": "b186",
      "title": "Taxi1500: A multilingual dataset for text classification in 1500 languages",
      "authors": [
        "Chunlan Ma",
        "Ayyoob Imanigooghari",
        "Haotian Ye",
        "Ehsaneddin Asgari",
        "Hinrich Schütze"
      ],
      "year": "2023",
      "venue": "Taxi1500: A multilingual dataset for text classification in 1500 languages",
      "doi": ""
    },
    {
      "id": "b187",
      "title": "Multitude: Large-scale multilingual machinegenerated text detection benchmark",
      "authors": [
        "Dominik Macko",
        "Robert Moro",
        "Adaku Uchendu",
        "Jason Lucas",
        "Michiharu Yamashita",
        "Matúš Pikuliak",
        "Ivan Srba",
        "Thai Le",
        "Dongwon Lee",
        "Jakub Simko"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b188",
      "title": "Dongwon Lee, Jakub Simko, and Maria Bielikova. 2024. Authorship obfuscation in multilingual machine-generated text detection",
      "authors": [
        "Dominik Macko",
        "Robert Moro",
        "Adaku Uchendu",
        "Ivan Srba",
        "Jason Samuel Lucas",
        "Michiharu Yamashita"
      ],
      "year": "",
      "venue": "Dongwon Lee, Jakub Simko, and Maria Bielikova. 2024. Authorship obfuscation in multilingual machine-generated text detection",
      "doi": ""
    },
    {
      "id": "b189",
      "title": "Multilingual bias detection and mitigation for indian languages",
      "authors": [
        "Ankita Maity",
        "Anubhav Sharma",
        "Rudra Dhar",
        "Tushar Abhishek",
        "Manish Gupta",
        "Vasudeva Varma"
      ],
      "year": "2023",
      "venue": "Multilingual bias detection and mitigation for indian languages",
      "doi": ""
    },
    {
      "id": "b190",
      "title": "A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank",
      "authors": [
        "Dan Malkin",
        "Tomasz Limisiewicz",
        "Gabriel Stanovsky"
      ],
      "year": "2022",
      "venue": "A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank",
      "doi": ""
    },
    {
      "id": "b191",
      "title": "Multiconer: a largescale multilingual dataset for complex named entity recognition",
      "authors": [
        "Shervin Malmasi",
        "Anjie Fang",
        "Besnik Fetahu",
        "Sudipta Kar",
        "Oleg Rokhlenko"
      ],
      "year": "2022",
      "venue": "Multiconer: a largescale multilingual dataset for complex named entity recognition",
      "doi": ""
    },
    {
      "id": "b192",
      "title": "Creating a massively parallel Bible corpus",
      "authors": [
        "Thomas Mayer",
        "Michael Cysouw"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)",
      "doi": ""
    },
    {
      "id": "b193",
      "title": "Alon Lavie, and Isabel Trancoso. 2023a. Towards multilingual automatic open-domain dialogue evaluation",
      "authors": [
        "John Mendonça"
      ],
      "year": "",
      "venue": "Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue",
      "doi": ""
    },
    {
      "id": "b194",
      "title": "Simple llm prompting is state-of-the-art for robust and multilingual dialogue evaluation",
      "authors": [
        "John Mendonça",
        "Patrícia Pereira",
        "Helena Moniz",
        "Joao Paulo Carvalho",
        "Alon Lavie",
        "Isabel M Trancoso"
      ],
      "year": "2023",
      "venue": "Proceedings of The Eleventh Dialog System Technology Challenge",
      "doi": ""
    },
    {
      "id": "b195",
      "title": "Structural priming demonstrates abstract grammatical representations in multilingual language models",
      "authors": [
        "James Michaelov",
        "Catherine Arnett",
        "Tyler Chang",
        "Ben Bergen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b196",
      "title": "Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint",
      "authors": [
        "Sewon Min",
        "Xinxi Lyu",
        "Ari Holtzman",
        "Mikel Artetxe",
        "Mike Lewis",
        "Hannaneh Hajishirzi",
        "Luke Zettlemoyer"
      ],
      "year": "2022",
      "venue": "Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint",
      "doi": ""
    },
    {
      "id": "b197",
      "title": "Lost in translation, found in spans: Identifying claims in multilingual social media",
      "authors": [
        "Shubham Mittal",
        "Megha Sundriyal",
        "Preslav Nakov"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b198",
      "title": "X-RiSAWOZ: High-quality end-to-end multilingual dialogue datasets and fewshot agents",
      "authors": [
        "Mehrad Moradshahi",
        "Tianhao Shen",
        "Kalika Bali",
        "Monojit Choudhury",
        "Gael De Chalendar",
        "Anmol Goel",
        "Sungkyun Kim",
        "Prashant Kodali",
        "Ponnurangam Kumaraguru",
        "Nasredine Semmar",
        "Sina Semnani",
        "Jiwon Seo",
        "Vivek Seshadri",
        "Manish Shrivastava",
        "Michael Sun",
        "Aditya Yadavalli",
        "Chaobin You",
        "Deyi Xiong",
        "Monica Lam"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": "10.18653/v1/2023.findings-acl.174"
    },
    {
      "id": "b199",
      "title": "2023a. Adaptive machine translation with large language models",
      "authors": [
        "Yasmin Moslem",
        "Rejwanul Haque",
        "Andy Way"
      ],
      "year": "",
      "venue": "Yasmin Moslem, Rejwanul Haque, and Andy Way. 2023b. Fine-tuning large language models for adaptive machine translation",
      "doi": ""
    },
    {
      "id": "b200",
      "title": "António Farinhas, and André FT Martins. 2023. Aligning neural machine translation models: Human feedback in training and inference",
      "authors": [
        "Miguel Moura Ramos",
        "Patrick Fernandes"
      ],
      "year": "",
      "venue": "António Farinhas, and André FT Martins. 2023. Aligning neural machine translation models: Human feedback in training and inference",
      "doi": ""
    },
    {
      "id": "b201",
      "title": "Crosslingual generalization through multitask finetuning",
      "authors": [
        "Niklas Muennighoff",
        "Thomas Wang",
        "Lintang Sutawika",
        "Adam Roberts",
        "Stella Biderman",
        "Teven Le Scao",
        "M Saiful Bari",
        "Sheng Shen",
        "Zheng-Xin Yong",
        "Hailey Schoelkopf"
      ],
      "year": "2022",
      "venue": "Crosslingual generalization through multitask finetuning",
      "doi": ""
    },
    {
      "id": "b202",
      "title": "Sebastian Ruder, et al. 2023. Afrisenti: A twitter sentiment analysis benchmark for african languages",
      "authors": [
        "Shamsuddeen Hassan",
        "Muhammad"
      ],
      "year": "",
      "venue": "Sebastian Ruder, et al. 2023. Afrisenti: A twitter sentiment analysis benchmark for african languages",
      "doi": ""
    },
    {
      "id": "b203",
      "title": "Assessing translation capabilities of large language models involving english and indian languages",
      "authors": [
        "Vandan Mujadia",
        "Ashok Urlana",
        "Yash Bhaskar",
        "Aditya Penumalla",
        "Kukkapalli Pavani",
        "Parameswari Shravya",
        "Dipti Misra Krishnamurthy",
        "Sharma"
      ],
      "year": "2023",
      "venue": "Assessing translation capabilities of large language models involving english and indian languages",
      "doi": ""
    },
    {
      "id": "b204",
      "title": "Evaluating and modeling attribution for cross-lingual question answering",
      "authors": [
        "Benjamin Muller",
        "John Wieting",
        "Jonathan H Clark",
        "Tom Kwiatkowski",
        "Sebastian Ruder",
        "Baldini Livio",
        "Roee Soares",
        "Jonathan Aharoni",
        "Xinyi Herzig",
        "Wang"
      ],
      "year": "2023",
      "venue": "Evaluating and modeling attribution for cross-lingual question answering",
      "doi": ""
    },
    {
      "id": "b205",
      "title": "Cross-lingual transfer of large language model by visually-derived supervision toward low-resource languages",
      "authors": [
        "Masayasu Muraoka",
        "Bishwaranjan Bhattacharjee",
        "Michele Merler",
        "Graeme Blackwood",
        "Yulong Li",
        "Yang Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia",
      "doi": ""
    },
    {
      "id": "b206",
      "title": "Breaking language barriers with a leap: Learning strategies for polyglot llms",
      "authors": [
        "Akshay Nambi",
        "Vaibhav Balloli",
        "Mercy Ranjit",
        "Tanuja Ganu",
        "Kabir Ahuja",
        "Sunayana Sitaram",
        "Kalika Bali"
      ],
      "year": "2023",
      "venue": "Breaking language barriers with a leap: Learning strategies for polyglot llms",
      "doi": ""
    },
    {
      "id": "b207",
      "title": "Towards massively multi-domain multilingual readability assessment",
      "authors": [
        "Tarek Naous",
        "Mohit Michael J Ryan",
        "Wei Chandra",
        "Xu"
      ],
      "year": "2023",
      "venue": "Towards massively multi-domain multilingual readability assessment",
      "doi": ""
    },
    {
      "id": "b208",
      "title": "Having beer after prayer? measuring cultural bias in large language models",
      "authors": [
        "Tarek Naous",
        "Wei Michael J Ryan",
        "Xu"
      ],
      "year": "2023",
      "venue": "Having beer after prayer? measuring cultural bias in large language models",
      "doi": ""
    },
    {
      "id": "b209",
      "title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "authors": [
        "Shashi Narayan",
        "Shay B Cohen",
        "Mirella Lapata"
      ],
      "year": "2018",
      "venue": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "doi": ""
    },
    {
      "id": "b210",
      "title": "2023a. CoF-CoT: Enhancing large language models with coarse-to-fine chain-of-thought prompting for multi-domain NLU tasks",
      "authors": [
        "Hoang Nguyen",
        "Ye Liu",
        "Chenwei Zhang",
        "Tao Zhang",
        "Philip Yu"
      ],
      "year": "",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2023.emnlp-main.743"
    },
    {
      "id": "b211",
      "title": "Benjamin Piwowarski, and Jacopo Staiano. 2023b. Loralay: A multilingual and multimodal dataset for long range and layout-aware summarization",
      "authors": [
        "Laura Nguyen",
        "Thomas Scialom"
      ],
      "year": "",
      "venue": "Benjamin Piwowarski, and Jacopo Staiano. 2023b. Loralay: A multilingual and multimodal dataset for long range and layout-aware summarization",
      "doi": ""
    },
    {
      "id": "b212",
      "title": "Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages",
      "authors": [
        "Thuat Nguyen",
        "Chien Van Nguyen",
        "Dac Viet",
        "Hieu Lai",
        "Man",
        "Trung Nghia",
        "Franck Ngo",
        "Ryan A Dernoncourt",
        "Thien Huu Rossi",
        "Nguyen"
      ],
      "year": "2023",
      "venue": "Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages",
      "doi": ""
    },
    {
      "id": "b213",
      "title": "Evaluating byte and wordpiece level models for massively multilingual semantic parsing",
      "authors": [
        "Massimo Nicosia",
        "Francesco"
      ],
      "year": "2022",
      "venue": "Evaluating byte and wordpiece level models for massively multilingual semantic parsing",
      "doi": ""
    },
    {
      "id": "b214",
      "title": "Small data? no problem! exploring the viability of pretrained multilingual language models for lowresourced languages",
      "authors": [
        "Kelechi Ogueji",
        "Yuxin Zhu",
        "Jimmy Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 1st Workshop on Multilingual Representation Learning",
      "doi": ""
    },
    {
      "id": "b215",
      "title": "Evaluating task understanding through multilingual consistency: A chatgpt case study",
      "authors": [
        "Xenia Ohmer",
        "Elia Bruni",
        "Dieuwke Hupkes"
      ],
      "year": "2023",
      "venue": "Evaluating task understanding through multilingual consistency: A chatgpt case study",
      "doi": ""
    },
    {
      "id": "b216",
      "title": "",
      "authors": [
        "Openai"
      ],
      "year": "2022",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b217",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b218",
      "title": "A preliminary evaluation of chatgpt for zero-shot dialogue understanding",
      "authors": [
        "Wenbo Pan",
        "Qiguang Chen",
        "Xiao Xu",
        "Wanxiang Che",
        "Libo Qin"
      ],
      "year": "2023",
      "venue": "A preliminary evaluation of chatgpt for zero-shot dialogue understanding",
      "doi": ""
    },
    {
      "id": "b219",
      "title": "Cross-lingual joint entity and word embedding to improve entity linking and parallel sentence mining",
      "authors": [
        "Xiaoman Pan",
        "Thamme Gowda",
        "Heng Ji",
        "Jonathan May",
        "Scott Miller"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP",
      "doi": "10.18653/v1/D19-6107"
    },
    {
      "id": "b220",
      "title": "Claim detection for automated fact-checking: A survey on monolingual, multilingual and cross-lingual research",
      "authors": [
        "Rrubaa Panchendrarajan",
        "Arkaitz Zubiaga"
      ],
      "year": "2024",
      "venue": "Claim detection for automated fact-checking: A survey on monolingual, multilingual and cross-lingual research",
      "doi": ""
    },
    {
      "id": "b221",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "Kishore Papineni",
        "Salim Roukos",
        "Todd Ward",
        "Wei-Jing Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b222",
      "title": "On the analysis of cross-lingual prompt tuning for decoder-based multilingual model",
      "authors": [
        "Nohil Park",
        "Joonsuk Park",
        "Min Kang",
        "Sungroh Yoo",
        "Yoon"
      ],
      "year": "2023",
      "venue": "On the analysis of cross-lingual prompt tuning for decoder-based multilingual model",
      "doi": ""
    },
    {
      "id": "b223",
      "title": "Bidirectional language models are also few-shot learners",
      "authors": [
        "Ajay Patel",
        "Bryan Li",
        "Mohammad Sadegh Rasooli",
        "Noah Constant",
        "Colin Raffel",
        "Chris Callison-Burch"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b224",
      "title": "Instruction tuning with gpt-4",
      "authors": [
        "Baolin Peng",
        "Chunyuan Li",
        "Pengcheng He",
        "Michel Galley",
        "Jianfeng Gao"
      ],
      "year": "2023",
      "venue": "Instruction tuning with gpt-4",
      "doi": ""
    },
    {
      "id": "b225",
      "title": "Document-level language models for machine translation",
      "authors": [
        "Frithjof Petrick",
        "Christian Herold",
        "Pavel Petrushkov",
        "Shahram Khadivi",
        "Hermann Ney"
      ],
      "year": "2023",
      "venue": "Document-level language models for machine translation",
      "doi": ""
    },
    {
      "id": "b226",
      "title": "Language model tokenizers introduce unfairness between languages",
      "authors": [
        "Aleksandar Petrov",
        "La Emanuele",
        "Philip Malfa",
        "Adel Hs Torr",
        "Bibi"
      ],
      "year": "2023",
      "venue": "Language model tokenizers introduce unfairness between languages",
      "doi": ""
    },
    {
      "id": "b227",
      "title": "mmt5: Modular multilingual pre-training solves source language hallucinations",
      "authors": [
        "Jonas Pfeiffer",
        "Francesco Piccinno",
        "Massimo Nicosia",
        "Xinyi Wang",
        "Machel Reid",
        "Sebastian Ruder"
      ],
      "year": "2023",
      "venue": "mmt5: Modular multilingual pre-training solves source language hallucinations",
      "doi": ""
    },
    {
      "id": "b228",
      "title": "Towards a common understanding of contributing factors for cross-lingual transfer in multilingual language models: A review",
      "authors": [
        "Fred Philippy",
        "Siwen Guo",
        "Shohreh Haddadan"
      ],
      "year": "2023",
      "venue": "Towards a common understanding of contributing factors for cross-lingual transfer in multilingual language models: A review",
      "doi": ""
    },
    {
      "id": "b229",
      "title": "Interactive-chainprompting: Ambiguity resolution for crosslingual conditional generation with interaction",
      "authors": [
        "Jonathan Pilault",
        "Xavier Garcia"
      ],
      "year": "2023",
      "venue": "Interactive-chainprompting: Ambiguity resolution for crosslingual conditional generation with interaction",
      "doi": ""
    },
    {
      "id": "b230",
      "title": "Thales Rogério, and Rodrigo Nogueira",
      "authors": [
        "Ramon Pires",
        "Hugo Abonizio"
      ],
      "year": "2023",
      "venue": "Sabi\\'a: Portuguese large language models",
      "doi": ""
    },
    {
      "id": "b231",
      "title": "Xcopa: A multilingual dataset for causal commonsense reasoning",
      "authors": [
        "Maria Edoardo",
        "Goran Ponti",
        "Olga Glavaš",
        "Qianchu Majewska",
        "Ivan Liu",
        "Anna Vulić",
        "Korhonen"
      ],
      "year": "2020",
      "venue": "Xcopa: A multilingual dataset for causal commonsense reasoning",
      "doi": ""
    },
    {
      "id": "b232",
      "title": "chrF++: words helping character n-grams",
      "authors": [
        "Maja Popović"
      ],
      "year": "2017",
      "venue": "Proceedings of the Second Conference on Machine Translation",
      "doi": "10.18653/v1/W17-4770"
    },
    {
      "id": "b233",
      "title": "Machine translation with large language models: Prompt engineering for persian, english, and russian directions",
      "authors": [
        "Nooshin Pourkamali",
        "Shler Ebrahim",
        "Sharifi"
      ],
      "year": "2024",
      "venue": "Machine translation with large language models: Prompt engineering for persian, english, and russian directions",
      "doi": ""
    },
    {
      "id": "b234",
      "title": "2023a. Decomposed prompting for machine translation between related languages using large language models",
      "authors": [
        "Ratish Puduppully",
        "Raj Dabre",
        "Ai Ti Aw",
        "Nancy F Chen"
      ],
      "year": "",
      "venue": "2023a. Decomposed prompting for machine translation between related languages using large language models",
      "doi": ""
    },
    {
      "id": "b235",
      "title": "2023b. Decomt: Decomposed prompting for machine translation between related languages using large language models",
      "authors": [
        "Ratish Puduppully",
        "Raj Anoop",
        "Aiti Dabre",
        "Nancy Aw",
        "Chen"
      ],
      "year": "",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b236",
      "title": "Comprehensive evaluation of chatgpt reliability through multilingual inquiries",
      "authors": [
        "Poorna Chander",
        "Reddy Puttaparthi",
        "Soham Sanjay Deo",
        "Hakan Gul",
        "Yiming Tang",
        "Weiyi Shang",
        "Zhe Yu"
      ],
      "year": "2023",
      "venue": "Comprehensive evaluation of chatgpt reliability through multilingual inquiries",
      "doi": ""
    },
    {
      "id": "b237",
      "title": "Cross-lingual consistency of factual knowledge in multilingual language models",
      "authors": [
        "Jirui Qi",
        "Raquel Fernández",
        "Arianna Bisazza"
      ],
      "year": "2023",
      "venue": "Cross-lingual consistency of factual knowledge in multilingual language models",
      "doi": ""
    },
    {
      "id": "b238",
      "title": "2023a. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages",
      "authors": [
        "Libo Qin",
        "Qiguang Chen",
        "Fuxuan Wei",
        "Shijue Huang",
        "Wanxiang Che"
      ],
      "year": "",
      "venue": "2023a. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages",
      "doi": ""
    },
    {
      "id": "b239",
      "title": "2023b. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages",
      "authors": [
        "Libo Qin",
        "Qiguang Chen",
        "Fuxuan Wei",
        "Shijue Huang",
        "Wanxiang Che"
      ],
      "year": "",
      "venue": "2023b. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages",
      "doi": ""
    },
    {
      "id": "b240",
      "title": "Gl-clef: A global-local contrastive learning framework for cross-lingual spoken language understanding",
      "authors": [
        "Libo Qin",
        "Qiguang Chen",
        "Tianbao Xie",
        "Qixin Li",
        "Jian-Guang Lou",
        "Wanxiang Che",
        "Min-Yen Kan"
      ],
      "year": "2022",
      "venue": "Gl-clef: A global-local contrastive learning framework for cross-lingual spoken language understanding",
      "doi": ""
    },
    {
      "id": "b241",
      "title": "Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp",
      "authors": [
        "Libo Qin",
        "Minheng Ni",
        "Yue Zhang",
        "Wanxiang Che"
      ],
      "year": "2020",
      "venue": "Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp",
      "doi": ""
    },
    {
      "id": "b242",
      "title": "Detecting and mitigating hallucinations in multilingual summarisation",
      "authors": [
        "Yifu Qiu",
        "Yftah Ziser",
        "Anna Korhonen",
        "M Edoardo",
        "Shay B Ponti",
        "Cohen"
      ],
      "year": "2023",
      "venue": "Detecting and mitigating hallucinations in multilingual summarisation",
      "doi": ""
    },
    {
      "id": "b243",
      "title": "The task of post-editing machine translation for the low-resource language",
      "authors": [
        "Diana Rakhimova",
        "Aidana Karibayeva",
        "Assem Turarbek"
      ],
      "year": "2024",
      "venue": "Applied Sciences",
      "doi": ""
    },
    {
      "id": "b244",
      "title": "Fairness in language models beyond english: Gaps and challenges",
      "authors": [
        "Krithika Ramesh",
        "Sunayana Sitaram",
        "Monojit Choudhury"
      ],
      "year": "2023",
      "venue": "Fairness in language models beyond english: Gaps and challenges",
      "doi": ""
    },
    {
      "id": "b245",
      "title": "Lmcap: Few-shot multilingual image captioning by retrieval augmented language model prompting",
      "authors": [
        "Rita Ramos",
        "Bruno Martins",
        "Desmond Elliott"
      ],
      "year": "2023",
      "venue": "Lmcap: Few-shot multilingual image captioning by retrieval augmented language model prompting",
      "doi": ""
    },
    {
      "id": "b246",
      "title": "Empowering cross-lingual abilities of instruction-tuned large language models by translation-following demonstrations",
      "authors": [
        "Leonardo Ranaldi",
        "Giulia Pucci",
        "Andre Freitas"
      ],
      "year": "2023",
      "venue": "Empowering cross-lingual abilities of instruction-tuned large language models by translation-following demonstrations",
      "doi": ""
    },
    {
      "id": "b247",
      "title": "Empowering cross-lingual abilities of instruction-tuned large language models by translation-following demonstrations",
      "authors": [
        "Leonardo Ranaldi",
        "Giulia Pucci",
        "Andre Freitas"
      ],
      "year": "2023",
      "venue": "Empowering cross-lingual abilities of instruction-tuned large language models by translation-following demonstrations",
      "doi": ""
    },
    {
      "id": "b248",
      "title": "The curious case of hallucinations in neural machine translation",
      "authors": [
        "Arul Vikas Raunak",
        "Marcin Menezes",
        "Junczys-Dowmunt"
      ],
      "year": "2021",
      "venue": "The curious case of hallucinations in neural machine translation",
      "doi": ""
    },
    {
      "id": "b249",
      "title": "Leveraging gpt-4 for automatic translation post-editing",
      "authors": [
        "Amr Vikas Raunak",
        "Hany Sharaf",
        "Arul Hassan Awadallah",
        "Menezes"
      ],
      "year": "2023",
      "venue": "Leveraging gpt-4 for automatic translation post-editing",
      "doi": ""
    },
    {
      "id": "b250",
      "title": "Little red riding hood goes around the globe: Crosslingual story planning and generation with large language models",
      "authors": [
        "Evgeniia Razumovskaia",
        "Joshua Maynez",
        "Annie Louis",
        "Mirella Lapata",
        "Shashi Narayan"
      ],
      "year": "2022",
      "venue": "Little red riding hood goes around the globe: Crosslingual story planning and generation with large language models",
      "doi": ""
    },
    {
      "id": "b251",
      "title": "Comet: A neural framework for mt evaluation",
      "authors": [
        "Ricardo Rei",
        "Craig Stewart",
        "Ana C Farinha",
        "Alon Lavie"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": ""
    },
    {
      "id": "b252",
      "title": "Neural machine translation models can learn to be few-shot learners",
      "authors": [
        "Raphael Reinauer",
        "Patrick Simianer",
        "Kaden Uhlig",
        "Johannes Em Mosig",
        "Joern Wuebker"
      ],
      "year": "2023",
      "venue": "Neural machine translation models can learn to be few-shot learners",
      "doi": ""
    },
    {
      "id": "b253",
      "title": "X-parade: Cross-lingual textual entailment and information divergence across paragraphs",
      "authors": [
        "Diego Juan",
        "Katrin Rodriguez",
        "Greg Erk",
        "Durrett"
      ],
      "year": "2023",
      "venue": "X-parade: Cross-lingual textual entailment and information divergence across paragraphs",
      "doi": ""
    },
    {
      "id": "b254",
      "title": "2022a. Clasp: Few-shot cross-lingual data augmentation for semantic parsing",
      "authors": [
        "Andy Rosenbaum",
        "Saleh Soltan",
        "Wael Hamza",
        "Amir Saffari",
        "Marco Damonte",
        "Isabel Groves"
      ],
      "year": "",
      "venue": "2022a. Clasp: Few-shot cross-lingual data augmentation for semantic parsing",
      "doi": ""
    },
    {
      "id": "b255",
      "title": "Linguist: Language model instruction tuning to generate annotated utterances for intent classification and slot tagging",
      "authors": [
        "Andy Rosenbaum",
        "Saleh Soltan",
        "Wael Hamza",
        "Yannick Versley",
        "Markus Boese"
      ],
      "year": "2022",
      "venue": "Linguist: Language model instruction tuning to generate annotated utterances for intent classification and slot tagging",
      "doi": ""
    },
    {
      "id": "b256",
      "title": "Xtreme-r: Towards more challenging and nuanced multilingual evaluation",
      "authors": [
        "Sebastian Ruder",
        "Noah Constant",
        "Jan Botha",
        "Aditya Siddhant",
        "Orhan Firat",
        "Jinlan Fu",
        "Pengfei Liu",
        "Junjie Hu",
        "Dan Garrette",
        "Graham Neubig"
      ],
      "year": "2021",
      "venue": "Xtreme-r: Towards more challenging and nuanced multilingual evaluation",
      "doi": ""
    },
    {
      "id": "b257",
      "title": "Language modelling with pixels",
      "authors": [
        "Phillip Rust",
        "Jonas F Lotz",
        "Emanuele Bugliarello",
        "Elizabeth Salesky",
        "Miryam De Lhoneux",
        "Desmond Elliott"
      ],
      "year": "2022",
      "venue": "Language modelling with pixels",
      "doi": ""
    },
    {
      "id": "b258",
      "title": "Revisiting non-english text simplification: A unified multilingual benchmark",
      "authors": [
        "Tarek Michael J Ryan",
        "Wei Naous",
        "Xu"
      ],
      "year": "2023",
      "venue": "Revisiting non-english text simplification: A unified multilingual benchmark",
      "doi": ""
    },
    {
      "id": "b259",
      "title": "Gender-specific machine translation with large language models",
      "authors": [
        "Eduardo Sánchez",
        "Pierre Andrews",
        "Pontus Stenetorp",
        "Mikel Artetxe",
        "Marta R Costa-Jussà"
      ],
      "year": "2023",
      "venue": "Gender-specific machine translation with large language models",
      "doi": ""
    },
    {
      "id": "b260",
      "title": "Camoscio: An italian instruction-tuned llama",
      "authors": [
        "Andrea Santilli",
        "Emanuele Rodolà"
      ],
      "year": "2023",
      "venue": "Camoscio: An italian instruction-tuned llama",
      "doi": ""
    },
    {
      "id": "b261",
      "title": "Cross-lingual supervision improves large language models pre-training",
      "authors": [
        "Andrea Schioppa",
        "Xavier Garcia",
        "Orhan Firat"
      ],
      "year": "2023",
      "venue": "Cross-lingual supervision improves large language models pre-training",
      "doi": ""
    },
    {
      "id": "b262",
      "title": "Polyglot or not? measuring multilingual encyclopedic knowledge in foundation models",
      "authors": [
        "Tim Schott",
        "Daniel Furman",
        "Shreshta Bhat"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b263",
      "title": "Cross-lingual transfer learning for multilingual task oriented dialog",
      "authors": [
        "Sebastian Schuster",
        "Sonal Gupta",
        "Rushin Shah",
        "Mike Lewis"
      ],
      "year": "2018",
      "venue": "Cross-lingual transfer learning for multilingual task oriented dialog",
      "doi": ""
    },
    {
      "id": "b264",
      "title": "Wiki-Matrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia",
      "authors": [
        "Holger Schwenk",
        "Vishrav Chaudhary",
        "Shuo Sun",
        "Hongyu Gong",
        "Francisco Guzmán"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
      "doi": "10.18653/v1/2021.eacl-main.115"
    },
    {
      "id": "b265",
      "title": "Multilingual entity and relation extraction dataset and model",
      "authors": [
        "Alessandro Seganti",
        "Klaudia Firl Ąg",
        "Helena Skowronska",
        "Michał Satława",
        "Piotr Andruszkiewicz"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
      "doi": "10.18653/v1/2021.eacl-main.166"
    },
    {
      "id": "b266",
      "title": "Bleurt: Learning robust metrics for text generation",
      "authors": [
        "Thibault Sellam",
        "Dipanjan Das",
        "Ankur P Parikh"
      ],
      "year": "2020",
      "venue": "Bleurt: Learning robust metrics for text generation",
      "doi": ""
    },
    {
      "id": "b267",
      "title": "Rahul Pal, et al. 2023. Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models",
      "authors": [
        "Neha Sengupta",
        "Sunil Kumar Sahu",
        "Bokang Jia",
        "Satheesh Katipomu",
        "Haonan Li",
        "Fajri Koto",
        "Osama Mohammed Afzal",
        "Samta Kamboj",
        "Onkar Pandit"
      ],
      "year": "",
      "venue": "Rahul Pal, et al. 2023. Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models",
      "doi": ""
    },
    {
      "id": "b268",
      "title": "Multilingual instruction tuning with just a pinch of multilinguality",
      "authors": [
        "Uri Shaham",
        "Jonathan Herzig",
        "Roee Aharoni",
        "Idan Szpektor",
        "Reut Tsarfaty",
        "Matan Eyal"
      ],
      "year": "2024",
      "venue": "Multilingual instruction tuning with just a pinch of multilinguality",
      "doi": ""
    },
    {
      "id": "b269",
      "title": "",
      "authors": [
        "Sharegpt"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b270",
      "title": "Mapo: Advancing multilingual reasoning through multilingual alignment-as-preference optimization",
      "authors": [
        "Shuaijie She",
        "Shujian Huang",
        "Wei Zou",
        "Wenhao Zhu",
        "Xiang Liu",
        "Xiang Geng",
        "Jiajun Chen"
      ],
      "year": "2024",
      "venue": "Mapo: Advancing multilingual reasoning through multilingual alignment-as-preference optimization",
      "doi": ""
    },
    {
      "id": "b271",
      "title": "The language barrier: Dissecting safety challenges of llms in multilingual contexts",
      "authors": [
        "Lingfeng Shen",
        "Weiting Tan",
        "Sihao Chen",
        "Yunmo Chen",
        "Jingyu Zhang",
        "Haoran Xu",
        "Boyuan Zheng",
        "Philipp Koehn",
        "Daniel Khashabi"
      ],
      "year": "2024",
      "venue": "The language barrier: Dissecting safety challenges of llms in multilingual contexts",
      "doi": ""
    },
    {
      "id": "b272",
      "title": "2022a. Language models are multilingual chain-of-thought reasoners",
      "authors": [
        "Freda Shi",
        "Mirac Suzgun",
        "Markus Freitag",
        "Xuezhi Wang",
        "Suraj Srivats",
        "Soroush Vosoughi",
        "Hyung Won Chung",
        "Yi Tay",
        "Sebastian Ruder",
        "Denny Zhou"
      ],
      "year": "",
      "venue": "2022a. Language models are multilingual chain-of-thought reasoners",
      "doi": ""
    },
    {
      "id": "b273",
      "title": "2022b. Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
      "authors": [
        "Peng Shi",
        "Rui Zhang",
        "He Bai",
        "Jimmy Lin"
      ],
      "year": "",
      "venue": "2022b. Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
      "doi": ""
    },
    {
      "id": "b274",
      "title": "mgpt: Few-shot learners go multilingual",
      "authors": [
        "Oleh Shliazhko",
        "Alena Fenogenova",
        "Maria Tikhonova",
        "Vladislav Mikhailov",
        "Anastasia Kozlova",
        "Tatiana Shavrina"
      ],
      "year": "2022",
      "venue": "mgpt: Few-shot learners go multilingual",
      "doi": ""
    },
    {
      "id": "b275",
      "title": "Anti-lm decoding for zero-shot in-context machine translation",
      "authors": [
        "Suzanna Sia",
        "Alexandra Delucia",
        "Kevin Duh"
      ],
      "year": "2023",
      "venue": "Anti-lm decoding for zero-shot in-context machine translation",
      "doi": ""
    },
    {
      "id": "b276",
      "title": "Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model",
      "authors": [
        "Saleh Soltan",
        "Jack Shankar Ananthakrishnan",
        "Rahul Fitzgerald",
        "Wael Gupta",
        "Haidar Hamza",
        "Charith Khan",
        "Stephen Peris",
        "Andy Rawls",
        "Anna Rosenbaum",
        "Rumshisky"
      ],
      "year": "2022",
      "venue": "Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model",
      "doi": ""
    },
    {
      "id": "b277",
      "title": "Hae-rae bench: Evaluation of korean knowledge in language models",
      "authors": [
        "Guijin Son",
        "Hanwool Lee",
        "Suwan Kim",
        "Jaecheol Lee",
        "Je Won Yeom",
        "Jihyu Jung",
        "Jung Woo Kim",
        "Songseong Kim"
      ],
      "year": "2023",
      "venue": "Hae-rae bench: Evaluation of korean knowledge in language models",
      "doi": ""
    },
    {
      "id": "b278",
      "title": "Sling: Sino linguistic evaluation of large language models",
      "authors": [
        "Yixiao Song",
        "Kalpesh Krishna",
        "Rajesh Bhatt",
        "Mohit Iyyer"
      ],
      "year": "2022",
      "venue": "Sling: Sino linguistic evaluation of large language models",
      "doi": ""
    },
    {
      "id": "b279",
      "title": "Tydip: A dataset for politeness classification in nine typologically diverse languages",
      "authors": [
        "Anirudh Srinivasan",
        "Eunsol Choi"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022",
      "doi": ""
    },
    {
      "id": "b280",
      "title": "Holistic inter-annotator agreement and corpus coherence estimation in a large-scale multilingual annotation campaign",
      "authors": [
        "Nicolas Stefanovitch",
        "Jakub Piskorski"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b281",
      "title": "Welm: A well-read pre-trained language model for chinese",
      "authors": [
        "Hui Su",
        "Xiao Zhou",
        "Houjin Yu",
        "Xiaoyu Shen",
        "Yuwen Chen",
        "Zilin Zhu",
        "Yang Yu",
        "Jie Zhou"
      ],
      "year": "2022",
      "venue": "Welm: A well-read pre-trained language model for chinese",
      "doi": ""
    },
    {
      "id": "b282",
      "title": "Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures",
      "authors": [
        "Pedro Javier",
        "Ortiz Suárez",
        "Benoît Sagot",
        "Laurent Romary"
      ],
      "year": "2019",
      "venue": "Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures",
      "doi": ""
    },
    {
      "id": "b283",
      "title": "Xinyi Wang, and Graham Neubig. 2023a. A multi-dimensional evaluation of tokenizer-free multilingual pretrained models",
      "authors": [
        "Jimin Sun",
        "Patrick Fernandes"
      ],
      "year": "",
      "venue": "Findings of the Association for Computational Linguistics: EACL 2023",
      "doi": ""
    },
    {
      "id": "b284",
      "title": "Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. 2023b. Moss: Training conversational language models from synthetic data",
      "authors": [
        "Tianxiang Sun",
        "Xiaotian Zhang",
        "Zhengfu He",
        "Peng Li",
        "Qinyuan Cheng",
        "Hang Yan",
        "Xiangyang Liu",
        "Yunfan Shao",
        "Qiong Tang",
        "Xingjian Zhao",
        "Ke Chen",
        "Yining Zheng",
        "Zhejian Zhou",
        "Ruixiao Li",
        "Jun Zhan",
        "Yunhua Zhou",
        "Linyang Li",
        "Xiaogui Yang",
        "Lingling Wu"
      ],
      "year": "",
      "venue": "Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. 2023b. Moss: Training conversational language models from synthetic data",
      "doi": ""
    },
    {
      "id": "b285",
      "title": "Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
      "authors": [
        "Yu Sun",
        "Shuohuan Wang",
        "Shikun Feng",
        "Siyu Ding",
        "Chao Pang",
        "Junyuan Shang",
        "Jiaxiang Liu",
        "Xuyi Chen",
        "Yanbin Zhao",
        "Yuxiang Lu"
      ],
      "year": "2021",
      "venue": "Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
      "doi": ""
    },
    {
      "id": "b286",
      "title": "Yiming Yang, and Chuang Gan. 2023c. Salmon: Self-alignment with principle-following reward models",
      "authors": [
        "Zhiqing Sun",
        "Yikang Shen",
        "Hongxin Zhang",
        "Qinhong Zhou",
        "Zhenfang Chen",
        "David Cox"
      ],
      "year": "",
      "venue": "Yiming Yang, and Chuang Gan. 2023c. Salmon: Self-alignment with principle-following reward models",
      "doi": ""
    },
    {
      "id": "b287",
      "title": "Multilingual llms are better cross-lingual in-context learners with alignment",
      "authors": [
        "Eshaan Tanwar",
        "Manish Borthakur",
        "Subhabrata Dutta",
        "Tanmoy Chakraborty"
      ],
      "year": "2023",
      "venue": "Multilingual llms are better cross-lingual in-context learners with alignment",
      "doi": ""
    },
    {
      "id": "b288",
      "title": "",
      "authors": [
        "Coig Pc Team"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b289",
      "title": "Gemini: a family of highly capable multimodal models",
      "authors": [
        "Gemini Team",
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Yonghui Wu",
        "Jean-Baptiste Alayrac",
        "Jiahui Yu",
        "Radu Soricut",
        "Johan Schalkwyk",
        "Andrew M Dai",
        "Anja Hauth"
      ],
      "year": "2023",
      "venue": "Gemini: a family of highly capable multimodal models",
      "doi": ""
    },
    {
      "id": "b290",
      "title": "Huozi: An open-source universal llm",
      "authors": [
        "Huozi Team"
      ],
      "year": "2023",
      "venue": "Huozi: An open-source universal llm",
      "doi": ""
    },
    {
      "id": "b291",
      "title": "2023c. Internlm: A multilingual language model with progressively enhanced capabilities",
      "authors": [
        "Internlm Team"
      ],
      "year": "",
      "venue": "2023c. Internlm: A multilingual language model with progressively enhanced capabilities",
      "doi": ""
    },
    {
      "id": "b292",
      "title": "Yulan-chat: An open-source bilingual chatbot",
      "authors": [
        "Yulan Team"
      ],
      "year": "2023",
      "venue": "Yulan-chat: An open-source bilingual chatbot",
      "doi": ""
    },
    {
      "id": "b293",
      "title": "Mehdi Rezagholizadeh, et al. 2023a. Nomiracl: Knowing when you don't know for robust multilingual retrieval-augmented generation",
      "authors": [
        "Nandan Thakur",
        "Luiz Bonifacio",
        "Xinyu Zhang",
        "Odunayo Ogundepo",
        "Ehsan Kamalloo",
        "David Alfonso-Hermelo",
        "Xiaoguang Li",
        "Qun Liu",
        "Boxing Chen"
      ],
      "year": "",
      "venue": "Mehdi Rezagholizadeh, et al. 2023a. Nomiracl: Knowing when you don't know for robust multilingual retrieval-augmented generation",
      "doi": ""
    },
    {
      "id": "b294",
      "title": "Leveraging llms for synthesizing training data across many languages in multilingual dense retrieval",
      "authors": [
        "Nandan Thakur",
        "Jianmo Ni",
        "Gustavo Hernández Ábrego",
        "John Wieting",
        "Jimmy Lin",
        "Daniel Cer"
      ],
      "year": "2023",
      "venue": "Leveraging llms for synthesizing training data across many languages in multilingual dense retrieval",
      "doi": ""
    },
    {
      "id": "b295",
      "title": "Crossmodal-3600: A massively multilingual multimodal evaluation dataset",
      "authors": [
        "Ashish V Thapliyal",
        "Jordi Pont-Tuset",
        "Xi Chen",
        "Radu Soricut"
      ],
      "year": "2022",
      "venue": "Crossmodal-3600: A massively multilingual multimodal evaluation dataset",
      "doi": ""
    },
    {
      "id": "b296",
      "title": "Climategpt: Towards ai synthesizing interdisciplinary research on climate change",
      "authors": [
        "David Thulke",
        "Yingbo Gao",
        "Petrus Pelser",
        "Rein Brune",
        "Rricha Jalota",
        "Floris Fok",
        "Michael Ramos",
        "Ian Van Wyk",
        "Abdallah Nasir",
        "Hayden Goldstein"
      ],
      "year": "2024",
      "venue": "Climategpt: Towards ai synthesizing interdisciplinary research on climate change",
      "doi": ""
    },
    {
      "id": "b297",
      "title": "Parallel data, tools and interfaces in OPUS",
      "authors": [
        "Jörg Tiedemann"
      ],
      "year": "2012",
      "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)",
      "doi": ""
    },
    {
      "id": "b298",
      "title": "It's all in the heads: Using attention heads as a baseline for crosslingual transfer in commonsense reasoning",
      "authors": [
        "Alexey Tikhonov",
        "Max Ryabinin"
      ],
      "year": "2021",
      "venue": "It's all in the heads: Using attention heads as a baseline for crosslingual transfer in commonsense reasoning",
      "doi": ""
    },
    {
      "id": "b299",
      "title": "Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "year": "",
      "venue": "Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b300",
      "title": "Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava"
      ],
      "year": "",
      "venue": "Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b301",
      "title": "Interleaving retrieval with chain-of-thought reasoning for knowledgeintensive multi-step questions",
      "authors": [
        "Harsh Trivedi",
        "Niranjan Balasubramanian",
        "Tushar Khot",
        "Ashish Sabharwal"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.557"
    },
    {
      "id": "b302",
      "title": "Enhancing natural language inference of cross-lingual n-shot transfer with multilingual data",
      "authors": [
        "Kuang Tseng",
        "Chow-Sing Lin"
      ],
      "year": "2022",
      "venue": "2022 8th International Conference on Applied System Innovation (ICASI)",
      "doi": ""
    },
    {
      "id": "b303",
      "title": "Efficiently aligned cross-lingual transfer learning for conversational tasks using prompt-tuning",
      "authors": [
        "Lifu Tu",
        "Jin Qu",
        "Semih Yavuz",
        "Shafiq Joty",
        "Wenhao Liu",
        "Caiming Xiong",
        "Yingbo Zhou"
      ],
      "year": "2023",
      "venue": "Efficiently aligned cross-lingual transfer learning for conversational tasks using prompt-tuning",
      "doi": ""
    },
    {
      "id": "b304",
      "title": "Anytext: Multilingual visual text generation and editing",
      "authors": [
        "Yuxiang Tuo",
        "Wangmeng Xiang",
        "Jun-Yan He",
        "Yifeng Geng",
        "Xuansong Xie"
      ],
      "year": "2023",
      "venue": "Anytext: Multilingual visual text generation and editing",
      "doi": ""
    },
    {
      "id": "b305",
      "title": "Turna: A turkish encoder-decoder language model for enhanced understanding and generation",
      "authors": [
        "Gökçe Uludogan",
        "Yirmibeşoglu Zeynep",
        "Furkan Balal",
        "Melikşah Akkurt",
        "Onur Türker",
        "Susan Güngör",
        "Üsküdarlı"
      ],
      "year": "2024",
      "venue": "Turna: A turkish encoder-decoder language model for enhanced understanding and generation",
      "doi": ""
    },
    {
      "id": "b306",
      "title": "Taco: Enhancing cross-lingual transfer for low-resource languages in llms through translation-assisted chain-ofthought processes",
      "authors": [
        "Bibek Upadhayay",
        "Vahid Behzadan"
      ],
      "year": "2023",
      "venue": "Taco: Enhancing cross-lingual transfer for low-resource languages in llms through translation-assisted chain-ofthought processes",
      "doi": ""
    },
    {
      "id": "b307",
      "title": "Pmindiasum: Multilingual and cross-lingual headline summarization for languages in india",
      "authors": [
        "Ashok Urlana",
        "Pinzhen Chen",
        "Zheng Zhao",
        "Manish Shay B Cohen",
        "Barry Shrivastava",
        "Haddow"
      ],
      "year": "2023",
      "venue": "Pmindiasum: Multilingual and cross-lingual headline summarization for languages in india",
      "doi": ""
    },
    {
      "id": "b308",
      "title": "mlongt5: A multilingual and efficient text-to-text transformer for longer sequences",
      "authors": [
        "David Uthus",
        "Santiago Ontañón",
        "Joshua Ainslie",
        "Mandy Guo"
      ],
      "year": "2023",
      "venue": "mlongt5: A multilingual and efficient text-to-text transformer for longer sequences",
      "doi": ""
    },
    {
      "id": "b309",
      "title": "Large scale multi-lingual multi-modal summarization dataset",
      "authors": [
        "Yash Verma",
        "Anubhav Jangra",
        "Raghvendra Kumar",
        "Sriparna Saha"
      ],
      "year": "2023",
      "venue": "Large scale multi-lingual multi-modal summarization dataset",
      "doi": ""
    },
    {
      "id": "b310",
      "title": "Don't rank, combine! combining machine translation hypotheses using quality estimation",
      "authors": [
        "Giorgos Vernikos",
        "Andrei Popescu-Belis"
      ],
      "year": "2024",
      "venue": "Don't rank, combine! combining machine translation hypotheses using quality estimation",
      "doi": ""
    },
    {
      "id": "b311",
      "title": "Prompting palm for translation: Assessing strategies and performance",
      "authors": [
        "David Vilar",
        "Markus Freitag",
        "Colin Cherry",
        "Jiaming Luo",
        "Viresh Ratnakar",
        "George Foster"
      ],
      "year": "2022",
      "venue": "Prompting palm for translation: Assessing strategies and performance",
      "doi": ""
    },
    {
      "id": "b312",
      "title": "Overcoming catastrophic forgetting in zero-shot cross-lingual generation",
      "authors": [
        "Tu Vu",
        "Aditya Barua",
        "Brian Lester",
        "Daniel Cer"
      ],
      "year": "2022",
      "venue": "Overcoming catastrophic forgetting in zero-shot cross-lingual generation",
      "doi": ""
    },
    {
      "id": "b313",
      "title": "Seaeval for multilingual foundation models: From cross-lingual alignment to cultural reasoning",
      "authors": [
        "Bin Wang",
        "Zhengyuan Liu",
        "Xin Huang",
        "Fangkai Jiao",
        "Yang Ding",
        "Ai Ti Aw",
        "Nancy F Chen"
      ],
      "year": "2023",
      "venue": "Seaeval for multilingual foundation models: From cross-lingual alignment to cultural reasoning",
      "doi": ""
    },
    {
      "id": "b314",
      "title": "Crosslingual summarization via chatgpt",
      "authors": [
        "Jiaan Wang",
        "Yunlong Liang",
        "Fandong Meng",
        "Zhixu Li",
        "Jianfeng Qu",
        "Jie Zhou"
      ],
      "year": "2023",
      "venue": "Crosslingual summarization via chatgpt",
      "doi": ""
    },
    {
      "id": "b315",
      "title": "2023c. Cross-lingual knowledge editing in large language models",
      "authors": [
        "Jiaan Wang",
        "Yunlong Liang",
        "Zengkui Sun",
        "Yuxuan Cao",
        "Jiarong Xu"
      ],
      "year": "",
      "venue": "2023c. Cross-lingual knowledge editing in large language models",
      "doi": ""
    },
    {
      "id": "b316",
      "title": "Clidsum: A benchmark dataset for cross-lingual dialogue summarization",
      "authors": [
        "Jiaan Wang",
        "Fandong Meng",
        "Ziyao Lu",
        "Duo Zheng",
        "Zhixu Li",
        "Jianfeng Qu",
        "Jie Zhou"
      ],
      "year": "2022",
      "venue": "Clidsum: A benchmark dataset for cross-lingual dialogue summarization",
      "doi": ""
    },
    {
      "id": "b317",
      "title": "Document-level machine translation with large language models",
      "authors": [
        "Longyue Wang",
        "Chenyang Lyu",
        "Tianbo Ji",
        "Zhirui Zhang",
        "Dian Yu",
        "Shuming Shi",
        "Zhaopeng Tu"
      ],
      "year": "2023",
      "venue": "Document-level machine translation with large language models",
      "doi": ""
    },
    {
      "id": "b318",
      "title": "Retrieval-augmented multilingual knowledge editing",
      "authors": [
        "Weixuan Wang",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "year": "2023",
      "venue": "Retrieval-augmented multilingual knowledge editing",
      "doi": ""
    },
    {
      "id": "b319",
      "title": "All languages matter: On the multilingual safety of large language models",
      "authors": [
        "Wenxuan Wang",
        "Zhaopeng Tu",
        "Chang Chen",
        "Youliang Yuan",
        "Jen-Tse Huang",
        "Wenxiang Jiao",
        "Michael R Lyu"
      ],
      "year": "2023",
      "venue": "All languages matter: On the multilingual safety of large language models",
      "doi": ""
    },
    {
      "id": "b320",
      "title": "Generalization via declarative instructions on 1600+ nlp tasks",
      "authors": [
        "Yizhong Wang",
        "Swaroop Mishra",
        "Pegah Alipoormolabashi",
        "Yeganeh Kordi",
        "Amirreza Mirzaei",
        "Anjana Arunkumar",
        "Arjun Ashok",
        "Arut Selvan Dhanasekaran",
        "Atharva Naik",
        "David Stap"
      ],
      "year": "",
      "venue": "Generalization via declarative instructions on 1600+ nlp tasks",
      "doi": ""
    },
    {
      "id": "b321",
      "title": "Xing Xie, and Jitao Sang. 2023g. Cdeval: A benchmark for measuring the cultural dimensions of large language models",
      "authors": [
        "Yuhang Wang",
        "Yanxu Zhu",
        "Chao Kong",
        "Shuyu Wei",
        "Xiaoyuan Yi"
      ],
      "year": "",
      "venue": "Xing Xie, and Jitao Sang. 2023g. Cdeval: A benchmark for measuring the cultural dimensions of large language models",
      "doi": ""
    },
    {
      "id": "b322",
      "title": "Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Aji, et al. 2023h. M4: Multigenerator, multi-domain, and multi-lingual black-box machine-generated text detection",
      "authors": [
        "Yuxia Wang",
        "Jonibek Mansurov",
        "Petar Ivanov",
        "Jinyan Su",
        "Artem Shelmanov",
        "Akim Tsvigun",
        "Chenxi Whitehouse"
      ],
      "year": "",
      "venue": "Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Aji, et al. 2023h. M4: Multigenerator, multi-domain, and multi-lingual black-box machine-generated text detection",
      "doi": ""
    },
    {
      "id": "b323",
      "title": "Mconala: a benchmark for code generation from multiple natural languages",
      "authors": [
        "Zhiruo Wang",
        "Grace Cuenca",
        "Shuyan Zhou",
        "Frank F Xu",
        "Graham Neubig"
      ],
      "year": "2022",
      "venue": "Mconala: a benchmark for code generation from multiple natural languages",
      "doi": ""
    },
    {
      "id": "b324",
      "title": "Daniel Fried, and Graham Neubig. 2022d. Execution-based evaluation for open-domain code generation",
      "authors": [
        "Zhiruo Wang",
        "Shuyan Zhou"
      ],
      "year": "",
      "venue": "Daniel Fried, and Graham Neubig. 2022d. Execution-based evaluation for open-domain code generation",
      "doi": ""
    },
    {
      "id": "b325",
      "title": "Machine translation for ge'ez language",
      "authors": [
        "Aman Kassahun",
        "Wassie"
      ],
      "year": "2023",
      "venue": "Machine translation for ge'ez language",
      "doi": ""
    },
    {
      "id": "b326",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "V Quoc",
        "Denny Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b327",
      "title": "Skywork: A more open bilingual foundation model",
      "authors": [
        "Tianwen Wei",
        "Liang Zhao",
        "Lichang Zhang",
        "Bo Zhu",
        "Lijie Wang",
        "Haihua Yang",
        "Biye Li",
        "Cheng Cheng",
        "Weiwei Lü",
        "Rui Hu"
      ],
      "year": "2023",
      "venue": "Skywork: A more open bilingual foundation model",
      "doi": ""
    },
    {
      "id": "b328",
      "title": "2023b. Zeroshot information extraction via chatting with chatgpt",
      "authors": [
        "Xiang Wei",
        "Xingyu Cui",
        "Ning Cheng",
        "Xiaobin Wang",
        "Xin Zhang",
        "Shen Huang",
        "Pengjun Xie",
        "Jinan Xu",
        "Yufeng Chen",
        "Meishan Zhang"
      ],
      "year": "",
      "venue": "2023b. Zeroshot information extraction via chatting with chatgpt",
      "doi": ""
    },
    {
      "id": "b329",
      "title": "Binbin Xie, et al. 2023c. Polylm: An open source polyglot large language model",
      "authors": [
        "Xiangpeng Wei",
        "Haoran Wei",
        "Huan Lin",
        "Tianhao Li",
        "Pei Zhang",
        "Xingzhang Ren",
        "Mei Li",
        "Yu Wan",
        "Zhiwei Cao"
      ],
      "year": "",
      "venue": "Binbin Xie, et al. 2023c. Polylm: An open source polyglot large language model",
      "doi": ""
    },
    {
      "id": "b330",
      "title": "Counting the bugs in ChatGPT's wugs: A multilingual investigation into the morphological capabilities of a large language model",
      "authors": [
        "Leonie Weissweiler",
        "Valentin Hofmann",
        "Anjali Kantharuban",
        "Anna Cai",
        "Ritam Dutt",
        "Amey Hengle",
        "Anubha Kabra",
        "Atharva Kulkarni",
        "Abhishek Vijayakumar",
        "Haofei Yu",
        "Hinrich Schuetze",
        "Kemal Oflazer",
        "David Mortensen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2023.emnlp-main.401"
    },
    {
      "id": "b331",
      "title": "Hyperpolyglot llms: Cross-lingual interpretability in token embeddings",
      "authors": [
        "Andrea Wen",
        "-Yi",
        "David Mimno"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b332",
      "title": "Llm-powered data augmentation for enhanced crosslingual performance",
      "authors": [
        "Chenxi Whitehouse",
        "Monojit Choudhury",
        "Alham Fikri",
        "Aji"
      ],
      "year": "2023",
      "venue": "Llm-powered data augmentation for enhanced crosslingual performance",
      "doi": ""
    },
    {
      "id": "b333",
      "title": "Parameter-efficient multilingual summarisation: An empirical study",
      "authors": [
        "Chenxi Whitehouse",
        "Fantine Huot",
        "Jasmijn Bastings",
        "Mostafa Dehghani",
        "Chu-Cheng Lin",
        "Mirella Lapata"
      ],
      "year": "2023",
      "venue": "Parameter-efficient multilingual summarisation: An empirical study",
      "doi": ""
    },
    {
      "id": "b334",
      "title": "2022a. Crosslingual few-shot learning on unseen languages",
      "authors": [
        "Genta Winata",
        "Shijie Wu",
        "Mayank Kulkarni",
        "Thamar Solorio",
        "Daniel Preotiuc-Pietro"
      ],
      "year": "",
      "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b335",
      "title": "2023a. NusaX: Multilingual parallel sentiment dataset for 10 Indonesian local languages",
      "authors": [
        "Genta Indra Winata",
        "Alham Fikri Aji",
        "Samuel Cahyawijaya",
        "Rahmad Mahendra",
        "Fajri Koto",
        "Ade Romadhony",
        "Kemal Kurniawan",
        "David Moeljadi",
        "Radityo Eko Prasojo",
        "Pascale Fung",
        "Timothy Baldwin",
        "Jey",
        "Han Lau",
        "Rico Sennrich",
        "Sebastian Ruder"
      ],
      "year": "",
      "venue": "the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.eacl-main.57"
    },
    {
      "id": "b336",
      "title": "The decades progress on code-switching research in nlp: A systematic survey on trends and challenges",
      "authors": [
        "Genta Indra Winata",
        "Alham Fikri Aji",
        "Zheng-Xin Yong",
        "Thamar Solorio"
      ],
      "year": "2022",
      "venue": "The decades progress on code-switching research in nlp: A systematic survey on trends and challenges",
      "doi": ""
    },
    {
      "id": "b337",
      "title": "Soumya Vadlamannati, and Yash Chandarana. 2023b. Multilingual few-shot learning via language model retrieval",
      "authors": [
        "Genta Indra Winata",
        "Liang-Kang Huang"
      ],
      "year": "",
      "venue": "Soumya Vadlamannati, and Yash Chandarana. 2023b. Multilingual few-shot learning via language model retrieval",
      "doi": ""
    },
    {
      "id": "b338",
      "title": "Bloom: A 176bparameter open-access multilingual language model",
      "authors": [
        "Bigscience Workshop",
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilić",
        "Roman Hesslow",
        "Alexandra Castagné",
        "François Sasha Luccioni",
        "Yvon"
      ],
      "year": "2022",
      "venue": "Bloom: A 176bparameter open-access multilingual language model",
      "doi": ""
    },
    {
      "id": "b339",
      "title": "Eva-kellm: A new benchmark for evaluating knowledge editing of llms",
      "authors": [
        "Suhang Wu",
        "Minlong Peng",
        "Yue Chen",
        "Jinsong Su",
        "Mingming Sun"
      ],
      "year": "2023",
      "venue": "Eva-kellm: A new benchmark for evaluating knowledge editing of llms",
      "doi": ""
    },
    {
      "id": "b340",
      "title": "Exploring prompt engineering with gpt language models for documentlevel machine translation: Insights and findings",
      "authors": [
        "Yangjian Wu",
        "Gang Hu"
      ],
      "year": "2023",
      "venue": "Proceedings of the Eighth Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b341",
      "title": "Task-agnostic low-rank adapters for unseen english dialects",
      "authors": [
        "Zedian Xiao",
        "William Held",
        "Yanchen Liu",
        "Diyi Yang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b342",
      "title": "2023a. A paradigm shift in machine translation: Boosting translation performance of large language models",
      "authors": [
        "Haoran Xu",
        "Young",
        "Jin Kim",
        "Amr Sharaf",
        "Hany Hassan Awadalla"
      ],
      "year": "",
      "venue": "2023a. A paradigm shift in machine translation: Boosting translation performance of large language models",
      "doi": ""
    },
    {
      "id": "b343",
      "title": "Cognitive overload: Jailbreaking large language models with overloaded logical thinking",
      "authors": [
        "Nan Xu",
        "Fei Wang",
        "Ben Zhou",
        "Zheng Bang",
        "Chaowei Li",
        "Muhao Xiao",
        "Chen"
      ],
      "year": "2023",
      "venue": "Cognitive overload: Jailbreaking large language models with overloaded logical thinking",
      "doi": ""
    },
    {
      "id": "b344",
      "title": "2023c. Are structural concepts universal in transformer language models? towards interpretable cross-lingual generalization",
      "authors": [
        "Ningyu Xu",
        "Qi Zhang",
        "Jingting Ye",
        "Menghan Zhang",
        "Xuanjing Huang"
      ],
      "year": "",
      "venue": "2023c. Are structural concepts universal in transformer language models? towards interpretable cross-lingual generalization",
      "doi": ""
    },
    {
      "id": "b345",
      "title": "Language representation projection: Can we transfer factual knowledge across languages in multilingual language models?",
      "authors": [
        "Shaoyang Xu",
        "Junzhuo Li",
        "Deyi Xiong"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b346",
      "title": "End-to-end slot alignment and recognition for crosslingual NLU",
      "authors": [
        "Weijia Xu",
        "Batool Haider",
        "Saab Mansour"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.410"
    },
    {
      "id": "b347",
      "title": "Vnhsge: Vietnamese high school graduation examination dataset for large language models",
      "authors": [
        "Le Dao Xuan-Quy",
        "Ngoc-Bich",
        "Phan Vo The-Duy",
        "Ngo Xuan-Dung",
        "Nguyen Bac-Bien",
        "Nguyen Van-Tien",
        "Nguyen Thi-My-Thanh",
        "Hong-Phuoc"
      ],
      "year": "2023",
      "venue": "Vnhsge: Vietnamese high school graduation examination dataset for large language models",
      "doi": ""
    },
    {
      "id": "b348",
      "title": "Byt5: Towards a token-free future with pre-trained byte-to-byte models",
      "authors": [
        "Linting Xue",
        "Aditya Barua",
        "Noah Constant",
        "Rami Al-Rfou",
        "Sharan Narang",
        "Mihir Kale",
        "Adam Roberts",
        "Colin Raffel"
      ],
      "year": "2022",
      "venue": "Byt5: Towards a token-free future with pre-trained byte-to-byte models",
      "doi": ""
    },
    {
      "id": "b349",
      "title": "Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer",
      "authors": [
        "Linting Xue",
        "Noah Constant",
        "Adam Roberts",
        "Mihir Kale",
        "Rami Al-Rfou",
        "Aditya Siddhant"
      ],
      "year": "",
      "venue": "Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer",
      "doi": ""
    },
    {
      "id": "b350",
      "title": "Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer",
      "authors": [
        "Linting Xue",
        "Noah Constant",
        "Adam Roberts",
        "Mihir Kale",
        "Rami Al-Rfou",
        "Aditya Siddhant"
      ],
      "year": "",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": ""
    },
    {
      "id": "b351",
      "title": "Lahm: Large annotated dataset for multi-domain and multilingual hate speech identification",
      "authors": [
        "Ankit Yadav",
        "Shubham Chandel"
      ],
      "year": "2023",
      "venue": "Lahm: Large annotated dataset for multi-domain and multilingual hate speech identification",
      "doi": ""
    },
    {
      "id": "b352",
      "title": "Fan Yang, et al. 2023a. Baichuan 2: Open large-scale language models",
      "authors": [
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Chao Yin",
        "Chenxu Lv",
        "Da Pan",
        "Dian Wang",
        "Dong Yan"
      ],
      "year": "",
      "venue": "Fan Yang, et al. 2023a. Baichuan 2: Open large-scale language models",
      "doi": ""
    },
    {
      "id": "b353",
      "title": "Investigating zero-shot generalizability on mandarin-english code-switched asr and speech-totext translation of recent foundation models with selfsupervision and weak supervision",
      "authors": [
        "Chih-Kai Yang",
        "Kuan-Po Huang",
        "Ke-Han Lu",
        "Chun-Yi Kuan",
        "Chi-Yuan Hsiao",
        "Hung-Yi Lee"
      ],
      "year": "2023",
      "venue": "Investigating zero-shot generalizability on mandarin-english code-switched asr and speech-totext translation of recent foundation models with selfsupervision and weak supervision",
      "doi": ""
    },
    {
      "id": "b354",
      "title": "2023c. Direct preference optimization for neural machine translation with minimum bayes risk decoding",
      "authors": [
        "Guangyu Yang",
        "Jinghong Chen",
        "Weizhe Lin",
        "Bill Byrne"
      ],
      "year": "",
      "venue": "2023c. Direct preference optimization for neural machine translation with minimum bayes risk decoding",
      "doi": ""
    },
    {
      "id": "b355",
      "title": "machine translation evaluation report",
      "authors": [
        "Muyun Yang",
        "Xixin Hu",
        "Hao Xiong",
        "Jiayi Wang",
        "Yiliyaer Jiaermuhamaiti",
        "Zhongjun He",
        "Weihua Luo",
        "Shujian Huang"
      ],
      "year": "2019",
      "venue": "Machine Translation: 15th China Conference",
      "doi": ""
    },
    {
      "id": "b356",
      "title": "Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages",
      "authors": [
        "Wen Yang",
        "Chong Li",
        "Jiajun Zhang",
        "Chengqing Zong"
      ],
      "year": "2023",
      "venue": "Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages",
      "doi": ""
    },
    {
      "id": "b357",
      "title": "Understanding calibration for multilingual question answering models",
      "authors": [
        "Yahan Yang",
        "Soham Dan",
        "Dan Roth",
        "Insup Lee"
      ],
      "year": "2023",
      "venue": "Understanding calibration for multilingual question answering models",
      "doi": ""
    },
    {
      "id": "b358",
      "title": "Paws-x: A cross-lingual adversarial dataset for paraphrase identification",
      "authors": [
        "Yinfei Yang",
        "Yuan Zhang",
        "Chris Tar",
        "Jason Baldridge"
      ],
      "year": "2019",
      "venue": "Paws-x: A cross-lingual adversarial dataset for paraphrase identification",
      "doi": ""
    },
    {
      "id": "b359",
      "title": "2023f. Mono-and multilingual gpt-3 models for hungarian",
      "authors": [
        "Zijian Győző",
        "Yang",
        "László János Laki",
        "Tamás Váradi",
        "Gábor Prószéky"
      ],
      "year": "",
      "venue": "International Conference on Text, Speech, and Dialogue",
      "doi": ""
    },
    {
      "id": "b360",
      "title": "Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability",
      "authors": [
        "Jiacheng Ye",
        "Xijia Tao",
        "Lingpeng Kong"
      ],
      "year": "2023",
      "venue": "Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability",
      "doi": ""
    },
    {
      "id": "b361",
      "title": "Multilingual content moderation: A case study on reddit",
      "authors": [
        "Meng Ye",
        "Karan Sikka",
        "Katherine Atwell",
        "Sabit Hassan",
        "Ajay Divakaran",
        "Malihe Alikhani"
      ],
      "year": "2023",
      "venue": "Multilingual content moderation: A case study on reddit",
      "doi": ""
    },
    {
      "id": "b362",
      "title": "Geomlama: Geo-diverse commonsense probing on multilingual pre-trained language models",
      "authors": [
        "Hritik Da Yin",
        "Masoud Bansal",
        "Liunian Monajatipoor",
        "Kai-Wei Harold Li",
        "Chang"
      ],
      "year": "2022",
      "venue": "Geomlama: Geo-diverse commonsense probing on multilingual pre-trained language models",
      "doi": ""
    },
    {
      "id": "b363",
      "title": "Bloom+ 1: Adding language support to bloom for zero-shot prompting",
      "authors": [
        "Zheng-Xin Yong",
        "Hailey Schoelkopf",
        "Niklas Muennighoff",
        "Alham Fikri Aji",
        "David Ifeoluwa Adelani",
        "Khalid Almubarak",
        "M Saiful Bari",
        "Lintang Sutawika",
        "Jungo Kasai",
        "Ahmed Baruwa"
      ],
      "year": "2022",
      "venue": "Bloom+ 1: Adding language support to bloom for zero-shot prompting",
      "doi": ""
    },
    {
      "id": "b364",
      "title": "Prompting multilingual large language models to generate codemixed texts: The case of south east asian languages",
      "authors": [
        "Zheng-Xin Yong",
        "Ruochen Zhang",
        "Jessica Zosa Forde",
        "Skyler Wang",
        "Samuel Cahyawijaya",
        "Holy Lovenia",
        "Genta Indra Winata",
        "Lintang Sutawika",
        "Jan Christian Blaise",
        "Long Cruz",
        "Phan"
      ],
      "year": "2023",
      "venue": "Prompting multilingual large language models to generate codemixed texts: The case of south east asian languages",
      "doi": ""
    },
    {
      "id": "b365",
      "title": "Seungone Kim, Sheikh Shafayat, and Minjoon Seo. 2024. Langbridge: Multilingual reasoning without multilingual supervision",
      "authors": [
        "Dongkeun Yoon",
        "Joel Jang",
        "Sungdong Kim"
      ],
      "year": "",
      "venue": "Seungone Kim, Sheikh Shafayat, and Minjoon Seo. 2024. Langbridge: Multilingual reasoning without multilingual supervision",
      "doi": ""
    },
    {
      "id": "b366",
      "title": "Beyond counting datasets: a survey of multilingual dataset construction and necessary resources",
      "authors": [
        "Xinyan Velocity",
        "Yu",
        "Akari Asai",
        "Trina Chatterjee",
        "Junjie Hu",
        "Eunsol Choi"
      ],
      "year": "2022",
      "venue": "Beyond counting datasets: a survey of multilingual dataset construction and necessary resources",
      "doi": ""
    },
    {
      "id": "b367",
      "title": "Lego-mt: Learning detachable models for massively multilingual machine translation",
      "authors": [
        "Fei Yuan",
        "Yinquan Lu",
        "Wenhao Zhu",
        "Lingpeng Kong",
        "Lei Li",
        "Yu Qiao",
        "Jingjing Xu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": ""
    },
    {
      "id": "b368",
      "title": "Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases",
      "authors": [
        "Yan Gong",
        "Yiping Peng",
        "Qiang Niu",
        "Lei Zhang",
        "Baochang Ma",
        "Xiangang Li",
        "Yunjie Ji",
        "Yong Deng"
      ],
      "year": "2023",
      "venue": "Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases",
      "doi": ""
    },
    {
      "id": "b369",
      "title": "Universal dependencies 2.10",
      "authors": [
        "Joakim Daniel Zeman",
        "Mitchell Nivre",
        "Elia Abrams",
        "Noëmi Ackermann",
        "Hamid Aepli",
        "Željko Aghaei",
        "Amir Agić",
        "Ahmadi"
      ],
      "year": "2022",
      "venue": "LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics",
      "doi": ""
    },
    {
      "id": "b370",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "authors": [
        "Aohan Zeng",
        "Xiao Liu",
        "Zhengxiao Du",
        "Zihan Wang",
        "Hanyu Lai",
        "Ming Ding",
        "Zhuoyi Yang",
        "Yifan Xu",
        "Wendi Zheng",
        "Xiao Xia"
      ],
      "year": "2022",
      "venue": "Glm-130b: An open bilingual pre-trained model",
      "doi": ""
    },
    {
      "id": "b371",
      "title": "Improving machine translation with large language models: A preliminary study with cooperative arXiv preprint",
      "authors": [
        "Jiali Zeng",
        "Fandong Meng",
        "Yongjing Yin",
        "Jie Zhou"
      ],
      "year": "2023",
      "venue": "Improving machine translation with large language models: A preliminary study with cooperative arXiv preprint",
      "doi": ""
    },
    {
      "id": "b372",
      "title": "Tim: Teaching large language models to translate with comparison",
      "authors": [
        "Jiali Zeng",
        "Fandong Meng",
        "Yongjing Yin",
        "Jie Zhou"
      ],
      "year": "2023",
      "venue": "Tim: Teaching large language models to translate with comparison",
      "doi": ""
    },
    {
      "id": "b373",
      "title": "Prompting large language model for machine translation: A case study",
      "authors": [
        "Biao Zhang",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "year": "2023",
      "venue": "Prompting large language model for machine translation: A case study",
      "doi": ""
    },
    {
      "id": "b374",
      "title": "Improving massively multilingual neural machine translation and zero-shot translation",
      "authors": [
        "Biao Zhang",
        "Philip Williams",
        "Ivan Titov",
        "Rico Sennrich"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.148"
    },
    {
      "id": "b375",
      "title": "2023b. xdialeval: A multilingual open-domain dialogue evaluation benchmark",
      "authors": [
        "Chen Zhang",
        "Luis Fernando",
        "D' Haro",
        "Chengguang Tang",
        "Ke Shi",
        "Guohua Tang",
        "Haizhou Li"
      ],
      "year": "",
      "venue": "2023b. xdialeval: A multilingual open-domain dialogue evaluation benchmark",
      "doi": ""
    },
    {
      "id": "b376",
      "title": "The skipped beat: A study of sociopragmatic understanding in llms for 64 languages",
      "authors": [
        "Chiyu Zhang",
        "Khai Duy Doan",
        "Qisheng Liao",
        "Muhammad Abdul-Mageed"
      ],
      "year": "2023",
      "venue": "The skipped beat: A study of sociopragmatic understanding in llms for 64 languages",
      "doi": ""
    },
    {
      "id": "b377",
      "title": "Leveraging multilingual knowledge graph to boost domain-specific entity translation of chatgpt",
      "authors": [
        "Min Zhang",
        "Limin Liu",
        "Zhao Yanqing",
        "Xiaosong Qiao",
        "Su Chang",
        "Xiaofeng Zhao",
        "Junhao Zhu",
        "Ming Zhu",
        "Song Peng",
        "Yinglu Li"
      ],
      "year": "2023",
      "venue": "Proceedings of Machine Translation Summit XIX",
      "doi": ""
    },
    {
      "id": "b378",
      "title": "Crosslingual cross-temporal summarization: Dataset, models, evaluation",
      "authors": [
        "Ran Zhang",
        "Jihed Ouni",
        "Steffen Eger"
      ],
      "year": "2023",
      "venue": "Crosslingual cross-temporal summarization: Dataset, models, evaluation",
      "doi": ""
    },
    {
      "id": "b379",
      "title": "Christian Blaise Cruz, and Alham Fikri Aji. 2023f. Multilingual large language models are not (yet) codeswitchers",
      "authors": [
        "Ruochen Zhang",
        "Samuel Cahyawijaya"
      ],
      "year": "",
      "venue": "Christian Blaise Cruz, and Alham Fikri Aji. 2023f. Multilingual large language models are not (yet) codeswitchers",
      "doi": ""
    },
    {
      "id": "b380",
      "title": "Crocosum: A benchmark dataset for cross-lingual code-switched summarization",
      "authors": [
        "Ruochen Zhang",
        "Carsten Eickhoff"
      ],
      "year": "2023",
      "venue": "Crocosum: A benchmark dataset for cross-lingual code-switched summarization",
      "doi": ""
    },
    {
      "id": "b381",
      "title": "2023g. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models",
      "authors": [
        "Shaolei Zhang",
        "Qingkai Fang",
        "Zhuocheng Zhang",
        "Zhengrui Ma",
        "Yan Zhou",
        "Langlin Huang",
        "Mengyu Bu",
        "Shangtong Gui",
        "Yunji Chen",
        "Xilin Chen"
      ],
      "year": "",
      "venue": "2023g. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models",
      "doi": ""
    },
    {
      "id": "b382",
      "title": "Opt: Open pre-trained transformer language models",
      "authors": [
        "Susan Zhang",
        "Stephen Roller",
        "Naman Goyal",
        "Mikel Artetxe",
        "Moya Chen",
        "Shuohui Chen",
        "Christopher Dewan",
        "Mona Diab",
        "Xian Li",
        "Xi Victoria Lin"
      ],
      "year": "2022",
      "venue": "Opt: Open pre-trained transformer language models",
      "doi": ""
    },
    {
      "id": "b383",
      "title": "Bertscore: Evaluating text generation with bert",
      "authors": [
        "Tianyi Zhang",
        "*",
        "Varsha Kishore",
        "*",
        "Felix Wu",
        "*",
        "Kilian Q Weinberger",
        "Yoav Artzi"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b384",
      "title": "Yew Ken Chia, and Lidong Bing. 2023h. M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models",
      "authors": [
        "Wenxuan Zhang",
        "Sharifah Mahani Aljunied",
        "Chang Gao"
      ],
      "year": "",
      "venue": "Yew Ken Chia, and Lidong Bing. 2023h. M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models",
      "doi": ""
    },
    {
      "id": "b385",
      "title": "2023i. Don't trust gpt when your question is not in english",
      "authors": [
        "Xiang Zhang",
        "Senyu Li",
        "Bradley Hauer",
        "Ning Shi",
        "Grzegorz Kondrak"
      ],
      "year": "",
      "venue": "2023i. Don't trust gpt when your question is not in english",
      "doi": ""
    },
    {
      "id": "b386",
      "title": "Liang He, and Xipeng Qiu. 2023j. Evaluating the performance of large language models on gaokao benchmark",
      "authors": [
        "Xiaotian Zhang",
        "Chunyang Li",
        "Yi Zong",
        "Zhengyu Ying"
      ],
      "year": "",
      "venue": "Liang He, and Xipeng Qiu. 2023j. Evaluating the performance of large language models on gaokao benchmark",
      "doi": ""
    },
    {
      "id": "b387",
      "title": "Xsemplr: Cross-lingual semantic parsing in multiple natural languages and meaning representations",
      "authors": [
        "Yusen Zhang",
        "Jun Wang",
        "Zhiguo Wang",
        "Rui Zhang"
      ],
      "year": "2023",
      "venue": "Xsemplr: Cross-lingual semantic parsing in multiple natural languages and meaning representations",
      "doi": ""
    },
    {
      "id": "b388",
      "title": "Cpm-2: Large-scale cost-effective pre-trained language models",
      "authors": [
        "Zhengyan Zhang",
        "Yuxian Gu",
        "Xu Han",
        "Shengqi Chen",
        "Chaojun Xiao",
        "Zhenbo Sun",
        "Yuan Yao",
        "Fanchao Qi",
        "Jian Guan",
        "Pei Ke"
      ],
      "year": "2021",
      "venue": "AI Open",
      "doi": ""
    },
    {
      "id": "b389",
      "title": "Mela: Multilingual evaluation of linguistic acceptability",
      "authors": [
        "Ziyin Zhang",
        "Yikang Liu",
        "Weifang Huang",
        "Junyu Mao",
        "Rui Wang",
        "Hai Hu"
      ],
      "year": "2023",
      "venue": "Mela: Multilingual evaluation of linguistic acceptability",
      "doi": ""
    },
    {
      "id": "b390",
      "title": "Javier Del Ser, and Guang Yang. 2023a. Chatagri: Exploring potentials of chatgpt on cross-linguistic agricultural text classification",
      "authors": [
        "Biao Zhao",
        "Weiqiang Jin"
      ],
      "year": "",
      "venue": "Javier Del Ser, and Guang Yang. 2023a. Chatagri: Exploring potentials of chatgpt on cross-linguistic agricultural text classification",
      "doi": ""
    },
    {
      "id": "b391",
      "title": "A survey of large language models",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "doi": ""
    },
    {
      "id": "b392",
      "title": "HIT-SCIR at MMNLU-22: Consistency regularization for multilingual spoken language understanding",
      "authors": [
        "Bo Zheng",
        "Zhouyang Li",
        "Fuxuan Wei",
        "Qiguang Chen",
        "Libo Qin",
        "Wanxiang Che"
      ],
      "year": "2022",
      "venue": "Proceedings of the Massively Multilingual Natural Language Understanding Workshop (MMNLU-22)",
      "doi": "10.18653/v1/2022.mmnlu-1.4"
    },
    {
      "id": "b393",
      "title": "Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena",
      "authors": [
        "Lianmin Zheng",
        "Wei-Lin Chiang",
        "Ying Sheng",
        "Siyuan Zhuang",
        "Zhanghao Wu",
        "Yonghao Zhuang",
        "Zi Lin",
        "Zhuohan Li",
        "Dacheng Li"
      ],
      "year": "",
      "venue": "Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena",
      "doi": ""
    },
    {
      "id": "b394",
      "title": "Let's think outside the box: Exploring leap-of-thought in large language models with creative humor generation",
      "authors": [
        "Shanshan Zhong",
        "Zhongzhan Huang",
        "Shanghua Gao",
        "Wushao Wen",
        "Liang Lin",
        "Marinka Zitnik",
        "Pan Zhou"
      ],
      "year": "2023",
      "venue": "Let's think outside the box: Exploring leap-of-thought in large language models with creative humor generation",
      "doi": ""
    },
    {
      "id": "b395",
      "title": "Agieval: A human-centric benchmark for evaluating foundation models",
      "authors": [
        "Wanjun Zhong",
        "Ruixiang Cui",
        "Yiduo Guo",
        "Yaobo Liang",
        "Shuai Lu",
        "Yanlin Wang",
        "Amin Saied",
        "Weizhu Chen",
        "Nan Duan"
      ],
      "year": "2023",
      "venue": "Agieval: A human-centric benchmark for evaluating foundation models",
      "doi": ""
    },
    {
      "id": "b396",
      "title": "Rc3: Regularized contrastive cross-lingual cross-modal pretraining",
      "authors": [
        "Chulun Zhou",
        "Yunlong Liang",
        "Fandong Meng",
        "Jinan Xu",
        "Jinsong Su",
        "Jie Zhou"
      ],
      "year": "2023",
      "venue": "Rc3: Regularized contrastive cross-lingual cross-modal pretraining",
      "doi": ""
    },
    {
      "id": "b397",
      "title": "Red ai? inconsistent responses from gpt3. 5 models on political issues in the us and china",
      "authors": [
        "Di Zhou",
        "Yinxian Zhang"
      ],
      "year": "2023",
      "venue": "Red ai? inconsistent responses from gpt3. 5 models on political issues in the us and china",
      "doi": ""
    },
    {
      "id": "b398",
      "title": "Accessible instruction-following agent",
      "authors": [
        "Kairui Zhou"
      ],
      "year": "2023",
      "venue": "Accessible instruction-following agent",
      "doi": ""
    },
    {
      "id": "b399",
      "title": "Question translation training for better multilingual reasoning",
      "authors": [
        "Wenhao Zhu",
        "Shujian Huang",
        "Fei Yuan",
        "Shuaijie She",
        "Jiajun Chen",
        "Alexandra Birch"
      ],
      "year": "2024",
      "venue": "Question translation training for better multilingual reasoning",
      "doi": ""
    },
    {
      "id": "b400",
      "title": "Extrapolating large language models to non-english by aligning languages",
      "authors": [
        "Wenhao Zhu",
        "Yunzhe Lv",
        "Qingxiu Dong",
        "Fei Yuan",
        "Jingjing Xu",
        "Shujian Huang",
        "Lingpeng Kong",
        "Jiajun Chen",
        "Lei Li"
      ],
      "year": "2023",
      "venue": "Extrapolating large language models to non-english by aligning languages",
      "doi": ""
    },
    {
      "id": "b401",
      "title": "The United Nations parallel corpus v1.0",
      "authors": [
        "Michał Ziemski",
        "Marcin Junczys-Dowmunt",
        "Bruno Pouliquen"
      ],
      "year": "2016",
      "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)",
      "doi": ""
    },
    {
      "id": "b402",
      "title": "Recently, Lou et al. (2023) proposed CCEval for Chinese-centric translation for comprehensive evaluation on MLLMs. Furthermore, due to the large gap between languages",
      "authors": [
        "Vilém Zouhar",
        "Ondřej Bojar",
        "; Dabre"
      ],
      "year": "2020",
      "venue": "Quality and quantity of machine translation references for automated metrics",
      "doi": ""
    },
    {
      "id": "b403",
      "title": "",
      "authors": [
        "Wassie"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b404",
      "title": "",
      "authors": [
        "Liu"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b405",
      "title": "focused more on low-resource language translation",
      "authors": [
        "Rakhimova"
      ],
      "year": "2023",
      "venue": "focused more on low-resource language translation",
      "doi": ""
    },
    {
      "id": "b406",
      "title": "",
      "authors": [
        "Gueuwou"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b407",
      "title": "further extend the translation and restatement tasks into multi-modal settings",
      "authors": [
        "Tuo"
      ],
      "year": "2023",
      "venue": "further extend the translation and restatement tasks into multi-modal settings",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Multilingual Large Language Model:",
      "text": "A Survey of Resources, Taxonomy and Frontiers Libo Qin\\({}^{\\clubsuit}\\)+ & Qiguang Chen\\({}^{\\clubsuit}\\)+ & Yuhang Zhou\\({}^{\\clubsuit}\\) & Zhi Chen\\({}^{\\diamonduit}\\) & Yinghui Li\\({}^{\\natural}\\) &Lizi Liao\\({}^{\\clubsuit}\\) & Min Li\\({}^{\\clubsuit}\\) & Wanxiang Che\\({}^{\\clubsuit}\\) & Philip S. Yu\\({}^{\\diamonduit}\\) \\({}^{\\clubsuit}\\) Central South University & Harbin Institute of Technology & Shanghai AI Laboratory \\({}^{\\natural}\\) Tsinghua University & Singapore Management University & University of Illinois at Chicago lbqin@csu.edu.cn, {qqchen,car}@ir.hit.edu.cn Equal Contribution"
    },
    {
      "title": "Abstract",
      "text": "Multilingual Large Language Models are capable of using powerful Large Language Models to handle and respond to queries in multiple languages, which achieves remarkable success in multilingual natural language processing tasks. Despite these breakthroughs, there still remains a lack of a comprehensive survey to summarize existing approaches and recent developments in this field. To this end, in this paper, we present a thorough review and provide a unified perspective to summarize the recent progress as well as emerging trends in multilingual large language models (MLLMs) literature. The contributions of this paper can be summarized: (1) _First survey_: to our knowledge, we take the first step and present a thorough review in MLLMs research field according to multi-lingual alignment; (2) _New taxonomy_: we offer a new and unified perspective to summarize the current progress of MLLMs; (3) _New frontiers_: we highlight several emerging frontiers and discuss the corresponding challenges; (4) _Abundant resources_: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards. We hope our work can provide the community with quick access and spur breakthrough research in MLLMs."
    },
    {
      "title": "1 Introduction",
      "text": "In recent years, remarkable progress has been witnessed in large language models (LLMs) Brown et al. (2020); Touvron et al. (2023); Bang et al. (2023); Zhao et al. (2023), which have achieved excellent performance on various natural language processing tasks Pan et al. (2023); Nguyen et al. (2023); Trivedi et al. (2023). In addition, LLMs raise surprising emergent capabilities, including in-context learning Min et al. (2022); Dong et al. (2022), chain-of-thought reasoning Wei et al. (2022); Huang et al. (2023); Qin et al. (2023), and even planning Driess et al. (2023); Hu et al. (2023). Nevertheless, the majority of LLMs are English-centric, primarily focusing on English tasks Held et al. (2023); Zhang et al. (2023), which makes them somewhat weak for multilingual settings, especially in low-resource scenarios. Actually, there are over 7,000 languages in the world. With the acceleration of globalization, the success of large language models should be considered to serve diverse countries and languages. To this end, multilingual large language models (MLLMs) possess the advantage of comprehensively handling multiple languages, gaining increasing attention. Specifically, the existing MLLMs can be broadly divided into two groups based on different stages. The first series of works Xue et al. (2020); Workshop et al. (2022); Zhang et al. (2023); Muennighoff et al. (2022) leverage multilingual Figure 1: Parameter-Tuning Alignment (§4.1) v.s. Parameter-Frozen Alignment (§4.2). The former requires the model to fine-tune the MLLM parameters for cross-lingual alignment, while the latter directly uses prompts for alignment without parameter tuning. data to tuning the parameters to boost the overall multilingual performance. The second series of work Shi et al. (2022); Qin et al. (2023); Huang et al. (2023) also adapt the advanced prompting strategies to unlock deeper multilingual potential of MLLMs during parameter-frozen inference stage. While remarkable success has been achieved in the MLLMs, there still remains a lack of a comprehensive review and analysis of recent efforts in the literature, which hinders the development of MLLMs. To bridge this gap, we make the first attempt to conduct a comprehensive and detailed analysis of MLLMs. Concretely, we first introduce the widely used data resource (SS3). Furthermore, due to the key challenge of alignment across languages, we introduce a novel taxonomy according to alignment strategies (SS4), aiming to provide a unified perspective in the literature, which includes: _parameter-tuning alignment_ and _parameter-frozen alignment_ (as shown in Figure 1). Specifically, _parameter-tuning alignment_ requires the fine-tuning of model parameters to enhance alignment between English and target languages during pre-training, supervised fine-tuning, reinforcement learning from human feedback and downstream fine-tuning. _parameter-frozen alignment_ refers to the alignment achieved by prompting across languages that can be achieved without the need for parameter tuning. Finally, we point out some potential frontier areas as well as the corresponding challenges for MLLMs, hoping to inspire the follow-up research (SS5). The contributions of this work can be summarized as follows: (1) _First survey_: To the best of our knowledge, we are the first to present a comprehensive survey in the MLLMs literature according to multi-lingual alignment; (2) _New taxonomy_: We introduce a novel taxonomy categorizing MLLMs into two alignment types: _parameter-frozen_ and _parameter-tuning_, offering a unified view for understanding the MLLMs literature; (3) _New frontiers_: We discuss some emerging frontiers and highlight their challenges as well as opportunities, hoping to pave the way for future research developments; (4) _Exhaustive resources_: We make the first attempt to organize MLLMs resources including open-source software, diverse corpora, and a curated list of relevant publications, accessible at [https://multilingual-llm.net](https://multilingual-llm.net). We hope that this work can serve as a valuable resource for researchers and inspire more breakthroughs in future research1. Footnote 1: Figure 2 illustrates the evolution of selected MLLMs over the past five years."
    },
    {
      "title": "2 Preliminary",
      "text": "In this section, we will formally describe the definitions of monolingual large language model (SS2.1) and multilingual large language model (SS2.2)."
    },
    {
      "title": "Monolingual Large Language Model",
      "text": "Monolingual large language models (LLM) can only process one language at a time. For example, as illustrated in Figure 3 (a), English and Chinese Figure 2: Evolution of selected MLLMs over the past five years, where colored branches indicate different alignment stages. For models with multiple alignment stages, the final stage is represented. LLM can separately handle English and Chinese language, respectively. Formally, considering a set of languages \\(\\mathcal{L}=\\{\\mathcal{L}_{i}\\}_{i=0}^{|\\mathcal{L}|}\\), given input utterance \\(\\mathcal{X}_{i}\\in\\mathcal{L}_{i}\\) in languages \\(\\mathcal{L}_{i}\\), the process of monolingual LLM (\\(\\mathcal{M}_{\\texttt{mono}}\\)) generating the output \\(\\mathcal{Y}_{i}\\) can be defined as: \\[\\mathcal{Y}_{i}=\\begin{cases}\\mathcal{M}_{\\texttt{mono}}(\\mathcal{X}_{i}, \\mathcal{L}_{i}),&\\texttt{mono}=\\mathcal{L}_{i};\\\\ \\texttt{Unexpect},&\\texttt{mono}\\neq\\mathcal{L}_{i},\\end{cases} \\tag{1}\\] where Unepect indicates that the LLM generates output in an unintended language; mono denotes the single language."
    },
    {
      "title": "Multilingual Large Language Model",
      "text": "As shown in Figure 3 (b), unlike monolingual LLM, a multilingual LLM is capable of handling and producing content in various languages simultaneously, like English and Chinese. Formally, for MLLM \\(\\mathcal{M}_{\\texttt{multi}}\\), where multi \\(\\subseteq\\mathcal{L}\\) and \\(|\\texttt{multi}|\\geq 2\\), the model's response is given by: \\[\\mathcal{Y}=\\mathcal{M}_{\\texttt{multi}}(\\mathcal{X}), \\tag{2}\\] where \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) belong to multiple languages, multi."
    },
    {
      "title": "3 Data Resource",
      "text": "In this section, we describe the widely used data resources in pre-training (SS3.1), supervised fine-tuning (SFT) (SS3.2) and reinforcement learning from human feedback (RLHF) (SS3.3) stage (Zhao et al., 2023b) for multilingual large language model. Detailed statistics can be found in Table 1 and Table 2 in the Appendix."
    },
    {
      "title": "Multilingual Pretraining Data",
      "text": "The widely used multilingual corpora for pre-training in MLLMs can be divided into 3 categories: (1) _Manual Creation_: obtains high-quality pre-training corpora through manual creation and proofreading, which consists of the Bible Corpus Mayer and Cysouw (2014) and MultiUN Ziemski et al. (2016). (2) _Web Crawling_: involves crawling extensive multilingual data from the internet, which includes OSCAR Suarez et al. (2019), CC-100 Conneau et al. (2020), mC4 Xue et al. (2021) and Redpajama-v2 Computer (2023). Another series of data are extracted from Wikipedia to enhance the knowledge of MLLMs. Common datasets include Wikipedia (Foundation), WikiMatrix Schwenk et al. (2021) and WikiExpl Han et al. (2023). (3) _Benchmark Adaptation_: means re-cleaning or integrating existing benchmarks to enhance data quality which includes OPUS-100 Zhang et al. (2020), Culturax Nguyen et al. (2023c), OPUS Tiedemann (2012), WMT Kocmi et al. (2023) and ROOTS Laurencon et al. (2022)."
    },
    {
      "title": "Multilingual Sft Data",
      "text": "Similarly, we categorize the existing multilingual SFT data into 4 classes: (1) _Manual Creation_: acquires SFT corpora through manual creation and proofreading, which includes Sup-NatInst Wang et al. (2022b), OpenAssist Kopf et al. (2023) and COIG-PClite Team (2023a). (2) _Machine Translation_: translates the existing monolingual datasets into multilingual instruction datasets, which comprises xP3-MT Muennighoff et al. (2022), MGSM8K\\({}_{\\text{Instruct}}\\)Chen et al. (2023b), CrossAlpaca Ranaldi et al. (2023b); Cui et al. (2023), MultilingualSIFT Chen et al. (2023i) and Bactrain-X Li et al. (2023b). (3) _Benchmark Adaptation_: involves transformation from existing benchmarks to instruction format. Widely used datasets include xP3 Muennighoff et al. (2022), PolyglotPrompt Fu et al. (2022), and BUFET Asai et al. (2023). (4) _MLLMs Aided Generation_: means that the data are automatically synthesized by the MLLMs, containing Vicuna Chiang et al. (2023), OverMiss Chen et al. (2023g), ShareGPT ShareGPT (2023), BELLE Yunjie Ji (2023), MultiAlpaca Wei et al. (2023c), Guanaco Dettmers et al. (2023) and Alpaca-4 Peng et al. (2023). Figure 3: Monolingual Large Language Model v.s. Multilingual Large Language Model."
    },
    {
      "title": "Multilingual Rlhf Data",
      "text": "Some work leveraged the multilingual RLHF data to improve alignment. Specifically, Lai et al. (2023) leverages multilingual ranking data for training a reward model using RLHF. Zeng et al. (2023) introduce the TIM dataset to train a more effective reward model in multilingual contexts."
    },
    {
      "title": "4 Taxonomy",
      "text": "As shown in Figure 4, we introduce a novel taxonomy including _parameter-tuning alignment_ (SS4.1) and _parameter-frozen alignment_ (SS4.2), which aims to provide a unified view for researchers to understand the MLLMs literature. Specifically, parameter tuning alignment (PTA) comprises a series of progressively advanced training and alignment strategies, including Pretraining Alignment, Supervised Fine-Tuning (SFT) Alignment, Reinforcement Learning from Human Feedback (RLHF) Alignment, and, ultimately, Downstream Fine-Tuning Alignment. These stages collectively aim to refine model parameters to align the multilingual performance systematically. Conversely, the parameter frozen alignment (PFA) focuses on four prompting strategies based on PTA: Direct Prompting, Code-Switching Prompting, Translation Alignment Prompting, and Retrieval-Augmented Alignment. This method maintains the original model parameters to achieve desired outcomes."
    },
    {
      "title": "Parameter-Tuning Alignment",
      "text": "Parameter-tuning alignment indicates that MLLMs should tune their parameters for better cross-lingual alignment (Wen-Yi and Mimno, 2023). As shown in Figure 5, we discuss the four categories of Figure 4: Taxonomy of MLLMs which includes _Parameter-Tuning Alignment Methodology_ and _Parameter-Frozen Alignment Methodology_. [MISSING_PAGE_FAIL:5] solve the scarcity of multilingual SFT task data, PaLM2 (Anil et al., 2023), Zhu et al. (2023); Cahyawijaya et al. (2023b); Li et al. (2023c); Gao et al. (2024) added translation task during the SFT alignment stage to improve alignment. Further, Upadhayay and Behzadan (2023); Chai et al. (2024); Zhu et al. (2024) began to consider using a more effective SFT alignment strategy to optimize the reasoning process."
    },
    {
      "title": "4.1.3 Pta In Rlhf Stage",
      "text": "As shown in Figure 5 (c), to achieve alignment in reinforcement learning from human feedback (RLHF) stage, Okapi (Lai et al., 2023b), LLaMA2-Chat (Touvron et al., 2023b), ChatGLM (Zeng et al., 2022), MOSS (Sun et al., 2023b), Baichuan (Yang et al., 2023a), Huozi (Team, 2023b), Qwen (Bai et al., 2023), InternLM (Team, 2023c), ParroT (Jiao et al., 2023), TigerBot (Chen et al., 2023f), MOSS (Sun et al., 2023b), YAYI-2 (Luo et al., 2023b), Yang et al. (2023c); Moura Ramos et al. (2023) and Orion (Chen et al., 2024) directly integrated multilingual RLHF data for training multilingual reward models. Additionally, Zeng et al. (2023b); Dong et al. (2023); She et al. (2024) introduced a multilingual reward model to compare translation outputs across different granularity. Sun et al. (2023c) proposed a Salmon framework, to enhance multilingual RLHF by self-generating rewards for better alignment."
    },
    {
      "title": "4.1.4 Pta In Downstream Finetuning Stage",
      "text": "_Full-Parameter Finetuning Alignment_Full-parameter finetuning in MLLMs means tuning all parameters in downstream tasks (see Figure 5 (d)). Specifically, GShard (Lepikhin et al., 2020), Linguist (Rosenbaum et al., 2022b), Fan et al. (2021); Bapna et al. (2022); Tseng and Lin (2022); Iyer et al. (2023), NLLB (Costa-jussa et al., 2022a) AlexTM (Soltan et al., 2022), and BigTrans (Yang et al., 2023d) focused on directly fine-tuning the full parameters across various downstream tasks (e.g., information extraction, machine translation). Xu et al. (2023c); Huot et al. (2023); Yuan et al. (2023); Li et al. (2023e) proposed multi-step or fine-grained alignment strategies during full-parameter tuning. Furthermore, to enhance the efficiency, Awasthi et al. (2022); De Raedt et al. (2023); Thakur et al. (2023b); Whitehouse et al. (2023a); Bansal and Sharma (2023); Xu et al. (2023a); Reinauer et al. (2023) focused on knowledge distillation from larger to smaller MLLMs. _Parameter-Efficient Finetuning Alignment_A series of studies employ Parameter-Efficient Finetuning (PEFT) alignment approaches for reducing full-parameter fine-tuning costs (Yong et al., 2022; Mujadia et al., 2023; Moslem et al., 2023b), which is shown in Figure 5 (d). Agrawal et al. (2022a); Tu et al. (2023); Park et al. (2023) proposed minimal soft prompt prefix fine-tuning for better alignment. Furthermore, Whitehouse et al. (2023b); Xiao et al. (2023); Aggarwal et al. (2024); Le et al. (2024) proposed methods based on Low-Rank Adaptation (LoRA) to achieve PEFT alignment. Further, Yoon et al. (2024) introduced a LangBridge model to bridge multilingual encoder to single-lingual LLM to effectively achieve promising performance. **Takeaways** (1) _PTA in pretraining stage brings the essential multilingual capabilities of the MLLMs._ (2) _The effectiveness of alignment in MLLMs is greatly influenced by previous alignment stage, (e.g. Pretraining will significantly influence SFT)._"
    },
    {
      "title": "Parameter-Frozen Alignment",
      "text": "In contrast to the traditional parameter-tuning approaches (Zheng et al., 2022), parameter-frozen alignment methods aim to perform alignment without any parameter tuning. The most popular approaches employ prompting strategies to elicit the alignment potential of MLLMs. As shown in Figure 6, this section discusses four prompting strategies for alignment without parameter tuning, which include (1) _Direct Prompting_, (2) _Code-Switching Prompting_, (3) _Translation Alignment Prompting_ and (4) _Retrieval Augmented Alignment_."
    },
    {
      "title": "4.2.1 Direct Prompting",
      "text": "As shown in Figure 6 (a), _Direct Prompting_ means directly outputting the request without any additional instruction for implicit alignment through MLLM itself (Abdelali et al., 2023; Zhang et al., 2023e; Wang et al., 2023b,d; Lin et al., 2022b; Bansal and Sharma, 2023; Wei et al., 2023b; Pourkamali and Sharifi, 2024)."
    },
    {
      "title": "4.2.2 Code-Switching Prompting",
      "text": "As shown in Figure 6 (b), it integrates multilingual words into a single-language utterance, which is a typical language phenomenon (Winata et al., 2022b; Dogruoz et al., 2023a,b) for effective language alignment (Qin et al., 2020, 2022). Specifically, Yong et al. (2023); Amin et al. (2023) showed the effectiveness of MLLMs in cross lingual alignment through model-generated code-switching texts. Furthermore, Zhang et al. (2023) suggested the need for fairer and more detailed code-switching optimization for further research."
    },
    {
      "title": "4.2.3 Translation Alignment Prompting",
      "text": "_Translation alignment prompting_ approaches mean that translating the query into other languages for better alignment (see Figure 6 (c)), which can be divided into the following classes: (1) _Key Information Translation_: This approach focuses on extracting key information and executing translation for word-level cross-lingual alignment Lu et al. (2023); Li et al. (2023). (2) _Direct Translation_: the model directly translates the whole input, enhancing alignment performance Etxaniz et al. (2023); Zhang et al. (2023); Cheng et al. (2023); Petrick et al. (2023); Hoang et al. (2023); Zeng et al. (2023); Nambi et al. (2023); Lin et al. (2021). (3) _Step-by-step Translation_: Instead of direct translation, this method prompts MLLMs to translate whole input step-by-step Puduppully et al. (2023); Moslem et al. (2023); Raunak et al. (2023); Wu and Hu (2023); Puduppully et al. (2023); Pilault et al. (2023). (4) _Restatement_: Beyond preserving original semantics, some studies focus on prompting MLLM to restate multilingual inputs to enhance cross-lingual effectiveness Shi et al. (2022); Patel et al. (2022); Rosenbaum et al. (2022); Asai et al. (2023); Qin et al. (2023); Huang et al. (2023); Tanwar et al. (2023). Further, considering the differences in multiple languages Ohmer et al. (2023), Qin et al. (2023); Ranaldi et al. (2023) integrated knowledge and translation strategy across different languages by cross-lingual prompting."
    },
    {
      "title": "4.2.4 Retrieval Augmented Alignment",
      "text": "_Retrieval Augmented Alignment_ incorporates external retrieval during prompting to inject more knowledge in MLLMs (see Figure 6 (d)). Specifically, He et al. (2023); Zhang et al. (2023); Conia et al. (2023); Xu et al. (2023); Ahmad (2024) focus on retrieving cultural or professional knowledge to enrich prompts. Another series of work focused on retrieval for high-quality alignment demonstrations, yielding significant improvements Shi et al. (2022); Agrawal et al. (2022); Li et al. (2023); Winata et al. (2023); Garcia et al. (2023); Li et al. (2023); Ramos et al. (2023); Kim et al. (2023); Thakur et al. (2023). **Takeaways** (1) _Translation alignment prompting is more effective for cross-lingual alignment._ (2) _Retrieval augmented alignment mitigates knowledge gaps in LLM._"
    },
    {
      "title": "5 Future Work And New Frontier",
      "text": ""
    },
    {
      "title": "Hallucination In Mllms",
      "text": "While remarkable progress has been achieved in MLLMs, the current approaches still face hallucination issues Raunak et al. (2021). Specifically, Guerreiro et al. (2023); Aharoni et al. (2023); Dale et al. (2023); Qiu et al. (2023) have previously pointed out the hallucination phenomenon on current MLLM. Further, a series of works provide corresponding solutions in the pre-training Pfeiffer et al. (2023), SFT Chen et al. (2023) and decoding Ahuja et al. (2022); Yang et al. (2023); Sia et al. (2023); Zeng et al. (2023) stages. The key challenges in this direction include: (1) _Multilingual Hallucination Detection_: How to effectively detect the hallucination phenomenon of Figure 6: Overview of Parameter-Frozen Alignment (§ 4.2) methods, where prompts in sub-figures sourced from Qin et al. (2023) and Zhang et al. (2023). MLLM across different languages is the primary problem to be solved in this field. (2) _Multilingual Hallucination Alleviation_: Current strategies for hallucination alleviation still focus on incorporating extensive factual data or utilizing external systems, which pose significant challenges for multiple languages, especially low-resource languages."
    },
    {
      "title": "Knowledge Editing In Mllms",
      "text": "The current MLLMs still face challenges with inaccurate, inconsistent, and outdated knowledge across different languages, which limits their performance. To solve this issue, Wu et al. (2023); Wang et al. (2023c) introduce a multilingual knowledge editing approach and propose a new benchmark for knowledge editing in MLLM. In addition, Qi et al. (2023) introduce the cross-lingual consistency metric to ensure factual consistency across languages. Additionally, Wang et al. (2023e) incorporate a multilingual knowledge base into MLLMs with retrieval methods to facilitate knowledge editing. The key challenges of this research include: (1) _Continuous Knowledge Editing_: How to continuously integrate new knowledge while preserving the accuracy of existing knowledge is a core challenge to explore. (2) _Balancing Universal and Language-Specific Knowledge_: Current work often neglects language-specific details like culture and slang, impacting user experience and causing cultural conflicts (Held et al., 2023; Beniwal et al., 2024). How to balance universal knowledge, while preserving language-specific knowledge presents a fascinating question."
    },
    {
      "title": "Safety In Mllms",
      "text": "With the development and application of MLLMs, researchers have found that MLLMs often suffer some serious moral (Costa-jussa et al., 2022b; Sanchez et al., 2023) and privacy (Macko et al., 2023) risks, hindering the development of MLLMs (Wang et al., 2023f; Ye et al., 2023b; Hammerl et al., 2022; Shen et al., 2024). Therefore, how to improve the safety of MLLMs is a promising research question. The main challenges for safe MLLM are as follows: (1) _Lack of Safety Benchmark_: The lack of safe data in current literature hampers the relevant research. Consequently, acquiring a large-scale safety dataset to facilitate future research has become a hot topic. (2) _Removal of Unsafe Data_: The multilingual data generated by MLLMs poses potential unsafe risks during training (Wang et al., 2023h). Therefore, identifying and filtering out unsafe multilingual content is a crucial issue (Bogoychev et al., 2023)."
    },
    {
      "title": "Fairness In Mllms",
      "text": "Multilingual fairness refers to equal treatment and performance across languages and cultures (Yu et al., 2022; Shliazhko et al., 2022). But there is a significant performance gap between languages, especially on low-resource languages (Malkin et al., 2022; Sengupta et al., 2023; Ye et al., 2023a). Additionally, token consumption also varies by language in MLLMs, leading to unequal computational costs (Koishekenov et al., 2022; Hua et al., 2023; Nicosia and Piccinno, 2022; Xue et al., 2022; Sun et al., 2023a; Rust et al., 2022). The main concerns regarding fairness in MLLM are as follows: (1) _Low-resource language performance improvement_: It is essential to improve the performance of low-resource languages with limited data (Lin et al., 2023; Ansell et al., 2023; Adeyemi et al., 2023). (2) _Multilingual Token Cost Improvement_: Current tokenizer exhibits biases in segmenting different languages, leading to varying token costs (Petrov et al., 2023; Ahia et al., 2023; Ali et al., 2023). Addressing this challenge is essential for ensuring fairer tokenization across languages."
    },
    {
      "title": "Language Extension In Mllms",
      "text": "Due to the limited languages supported by current work, integrating new languages into existing MLLM is a promising direction to explore (Kew et al., 2023; Shaham et al., 2024). To this end, Cui et al. (2023); Yang et al. (2023d) suggest adding languages through two-stage pre-training. Yong et al. (2022) observe that adapter-based methods are more effective than continuous pre-training. This challenge encompasses two main aspects: (1) _Multiple Languages Extension:_ How to dynamically and effectively extend the languages for MLLMs is an interesting research question. (2) _Original Languages Preserving:_ Since the expansion of the model in other languages will harm the original language performance, how to prevent the language extension in MLLM from forgetting the previously learned language is a major challenge."
    },
    {
      "title": "Multi-Modality Extension In Mllms",
      "text": "Since the improvement in the usability of MLLM, a large amount of work has begun to further extend [MISSING_PAGE_EMPTY:9] Kabir Ahuja, Sunayana Sitaram, Sandipan Dandapat, and Monojit Choudhury. 2022. On the calibration of massively multilingual language models. _arXiv preprint arXiv:2210.12265_. * Ahuja et al. (2023) Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali, et al. 2023b. Megayverse: Benchmarking large language models across languages, modalities, models and tasks. _arXiv preprint arXiv:2311.07463_. * Alhafni et al. (2023) Bashar Alhafni, Go Inoue, Christian Khairallah, and Nizar Habash. 2023. Advancements in Arabic grammatical error detection and correction: An empirical investigation. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 6430-6448, Singapore. Association for Computational Linguistics. * Ali et al. (2023) Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max Lubbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper Schulze Buschhoff, et al. 2023. Tokenizer choice for llm training: Negligible or crucial? _arXiv preprint arXiv:2310.08754_. * Amin et al. (2023) Dhiraj Amin, Sharvari Govilkar, Sagar Kulkarni, Yash Shashikant Lalit, Arshi Ajaz Khwaja, Daries Xavier, and Sahil Girijashankar Gupta. 2023. Marathi-english code-mixed text generation. _arXiv preprint arXiv:2309.16202_. * Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_. * Ansel et al. (2023) Alan Ansel, Marinela Parovic, Ivan Vulic, Anna Korhonen, and Edoardo Ponti. 2023. Unifying cross-lingual transfer across scenarios of resource scarcity. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 3980-3995. * Artetxe et al. (2023) Mikel Artetxe, Vedanuj Goswami, Shruti Bhosale, Angela Fan, and Luke Zettlemoyer. 2023. Revisiting machine translation for cross-lingual classification. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 6489-6499, Singapore. Association for Computational Linguistics. * Artetxe et al. (2020) Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 4623-4637, Online. Association for Computational Linguistics. * Asai et al. (2023) Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. 2023. Buffet: Benchmarking large language models for few-shot cross-lingual transfer. _arXiv preprint arXiv:2305.14857_. * Awasthi et al. (2022) Abhijeet Awasthi, Nitish Gupta, Bidisha Samanta, Shachi Dave, Sunita Sarawagi, and Partha Talukdar. 2022. Bootstrapping multilingual semantic parsers using large language models. _arXiv preprint arXiv:2210.07313_. * Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. _arXiv preprint arXiv:2309.16609_. * Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multi-task, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. _arXiv preprint arXiv:2302.04023_. * Bansal and Sharma (2023) Parikshit Bansal and Amit Sharma. 2023. Large language models as annotators: Enhancing generalization of nlp models at minimal cost. _arXiv preprint arXiv:2306.15766_. * Bao et al. (2023) Eliseo Bao, Anxo Perez, and Javier Parapar. 2023. Conversations in galician: a large language model for an underrepresented language. _arXiv preprint arXiv:2311.03812_. * Bapna et al. (2022) Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, et al. 2022. Building machine translation systems for the next thousand languages. _arXiv preprint arXiv:2205.03983_. * Basile et al. (2023) Pierpaolo Basile, Elio Musacchio, Marco Polignano, Lucia Siciliani, Giuseppe Fiameni, and Giovanni Semeraro. 2023. Llamantino: Llama 2 models for effective text generation in italian language. _arXiv preprint arXiv:2312.09993_. * Bawden et al. (2021) Rachel Bawden, Eric Bilinski, Thomas Lavergne, and Sophie Rosset. 2021. Diabla: a corpus of bilingual spontaneous written dialogues for machine translation. _Language Resources and Evaluation_, 55:635-660. * Bellagente et al. (2023) Marco Bellagente, Manuel Brack, Hannah Teufel, Felix Friedrich, Bjorn Deiseroth, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Koen Oostermeijer, et al. 2023. Multifusion: Fusing pre-trained models for multi-lingual, multi-modal image generation. _arXiv preprint arXiv:2305.15296_. * Beniwal et al. (2024) Himanshu Beniwal, Mayank Singh, et al. 2024. Cross-lingual editing in multilingual language models. _arXiv preprint arXiv:2401.10521_. * Berdicevskis et al. (2023) Aleksandrs Berdicevskis, Gerlof Bouma, Robin Kurtz, Felix Morger, Joey Ohman, Yvonne Adesam, Lars Borin, Dana Dannells, Markus Forsberg, Tim Isbister, et al. 2023. Superlim: A Swedish language understanding evaluation benchmark. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 8137-8153. Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Yuan-Fang Li, Yong-Bin Kang, and Rifat Shahriyar. 2021. Crosssum: Beyond english-centric cross-lingual abstractive text summarization for 1500+ language pairs. _arXiv preprint arXiv:2112.08804_. * Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. _Challenges & Perspectives in Creating Large Language Models_, page 95. * Blevins et al. (2024) Terra Blevins, Tomasz Limisiewicz, Suchin Gururangan, Margaret Li, Hila Gonen, Noah A Smith, and Luke Zettlemoyer. 2024. Breaking the curse of multi-linguality with cross-lingual expert language models. _arXiv preprint arXiv:2401.10440_. * Blevins and Zettlemoyer (2022) Terra Blevins and Luke Zettlemoyer. 2022. Language contamination helps explain the cross-lingual capabilities of english pretrained models. _arXiv preprint arXiv:2204.08110_. * Bogoychev et al. (2023) Nikolay Bogoychev, Jelmer van der Linde, Graeme Nail, Barry Haddow, Jaume Zaragoza-Bernabeu, Gema Ramirez-Sanchez, Lukas Weymann, Tudor Nicolae Mateiu, Jindrich Helcl, and Mikko Aulamo. 2023. Opuscleaner and opustrainer, open source toolkits for training machine translation and large language models. _arXiv preprint arXiv:2311.14838_. * Boughorbel and Hawasly (2023) Sabri Boughorbel and Majd Hawasly. 2023. Analyzing multilingual competency of llms in multi-turn instruction following: A case study of arabic. _arXiv preprint arXiv:2310.14819_. * Briakou et al. (2023) Eleftheria Briakou, Colin Cherry, and George Foster. 2023. Searching for needles in a haystack: On the role of incidental bilingualism in palm's translation capability. _arXiv preprint arXiv:2305.10266_. * Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901. * Cahyawijaya et al. (2023a) Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Winata, Bryan Wile, Fajri Koto, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, et al. 2023a. Nusacrowd: Open source initiative for indonesian nlp resources. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 13745-13818. * Cahyawijaya et al. (2023b) Samuel Cahyawijaya, Holy Lovenia, Tiezheng Yu, Willy Chung, and Pascale Fung. 2023b. Instruct-align: Teaching novel languages with to llms through alignment-based cross-lingual instruction. _arXiv preprint arXiv:2305.13627_. * Cao et al. (2023) Yang Trista Cao, Anna Sotnikova, Jieyu Zhao, Linda X Zou, Rachel Rudinger, and Hal Daume III. 2023. Multilingual large language models leak human stereotypes across language boundaries. _arXiv preprint arXiv:2312.07141_. * Chai et al. (2024) Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo, Jiaheng Liu, Bing Wang, Xiannian Liang, Jiaqi Bai, Tongliang Li, Qiyao Peng, et al. 2024. xcot: Cross-lingual instruction tuning for cross-lingual chain-of-thought reasoning. _arXiv preprint arXiv:2401.07037_. * Chai et al. (2022) Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, and Hua Wu. 2022. Ernie-code: Beyond english-centric cross-lingual pretraining for programming languages. _arXiv preprint arXiv:2212.06742_. * Changpinyo et al. (2022) Soravit Changpinyo, Linting Xue, Idan Szpektor, Ashish V Thapliyal, Julien Amelot, Xi Chen, and Radu Soricut. 2022. Towards multi-lingual visual question answering. _arXiv preprint arXiv:2209.05401_. * Chen et al. (2024) Du Chen, Yi Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, and Kun Han. 2024. Orion-14b: Open-source multilingual large language models. _arXiv preprint arXiv:2401.12246_. * Chen et al. (2023a) Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, and Jia Li. 2023a. Large language models meet harry potter: A dataset for aligning dialogue agents with characters. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 8506-8520. * Chen et al. (2023b) Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, and Jia Li. 2023b. Breaking language barriers in multilingual mathematical reasoning: Insights and observations. _arXiv preprint arXiv:2310.20246_. * Chen et al. (2023c) Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Barry Haddow, and Kenneth Heafield. 2023c. Monolingual or multilingual instruction tuning: Which makes a better alpaca. _arXiv preprint arXiv:2309.08958_. * Chen et al. (2023d) Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. 2023d. Pali-x: On scaling up a multilingual vision and language model. _arXiv preprint arXiv:2305.18565_. * Chen et al. (2023e) Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al. 2023e. Pali-3 vision language models: Smaller, faster, stronger. _arXiv preprint arXiv:2310.09199_. * Chen et al. (2022) Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. 2022. Pali: A jointly-scaled multilingual language-image model. _arXiv preprint arXiv:2209.06794_. * Chen et al. (2023d)Ye Chen, Wei Cai, Liangmin Wu, Xiaowei Li, Zhanxuan Xin, and Cong Fu. 2023f. Tigerbot: An open multilingual multitask llm. _arXiv preprint arXiv:2312.08688_. * Chen et al. (2023) Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou. 2023g. Improving translation faithfulness of large language models via augmenting instructions. _arXiv preprint arXiv:2308.12674_. * Chen et al. (2023) Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, et al. 2023h. Phoenix: Democratizing chatgpt across languages. _arXiv preprint arXiv:2304.10453_. * Chen et al. (2023) Zhihong Chen, Shuo Yan, Juhao Liang, Feng Jiang, Xiangbo Wu, Fei Yu, Guiming Hardy Chen, Junying Chen, Hongbo Zhang, Li Jianquan, Wan Xiang, and Benyou Wang. 2023i. Multilingual SIFT: Multilingual Supervised Instruction Fine-tuning. * Cheng et al. (2023a) Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, and Rui Yan. 2023a. Scale: Synergized collaboration of asymmetric language translation engines. _arXiv preprint arXiv:2309.17061_. * Cheng et al. (2023b) Yong Cheng, Yu Zhang, Melvin Johnson, Wolfgang Macherey, and Ankur Bapna. 2023b. Mu \\({}^{2}\\) slam: Multitask, multilingual speech and language models. In _International Conference on Machine Learning_, pages 5504-5520. PMLR. * Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Liamin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. * Chaudhury et al. (2023) De Choudhury et al. 2023. Ask me in english instead: Cross-lingual evaluation of large language models for healthcare queries. _arXiv preprint arXiv:2310.13132_. * Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_. * Chung et al. (2022a) Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, and Noah Constant. 2022a. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. In _The Eleventh International Conference on Learning Representations_. * Chung et al. (2022b) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022b. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_. * Clark et al. (2023) Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, and Ankur Parikh. 2023. Seahorse: A multilingual, multifaceted dataset for summarization evaluation. _arXiv preprint arXiv:2305.13194_. * Clark et al. (2020) Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. Tydi qa: A benchmark for information-seeking question answering in tyologically di verse languages. _Transactions of the Association for Computational Linguistics_, 8:454-470. * Computer (2023) Together Computer. 2023. Redpajama: an open dataset for training large language models. * Conia et al. (2023) Simone Conia, Min Li, Daniel Lee, Umar Minhas, Ihab Ilyas, and Yunyao Li. 2023. Increasing coverage and precision of textual information in multilingual knowledge graphs. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 1612-1634. * Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 8440-8451. * Conneau et al. (2018) Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating cross-lingual sentence representations. _arXiv preprint arXiv:1809.05053_. * Costa-jussa et al. (2023) Marta R Costa-jussa, Pierre Andrews, Eric Smith, Prangthip Hansanti, Christophe Ropers, Elahek Salbassi, Cynthia Gao, Daniel Licht, and Carleigh Wood. 2023. Multilingual holistic bias: Extending descriptors and patterns to unveil demographic biases in languages at scale. _arXiv preprint arXiv:2305.13198_. * Costa-jussa et al. (2022a) Marta R Costa-jussa, James Cross, Onur Celebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022a. No language left behind: Scaling human-centered machine translation. _arXiv preprint arXiv:2207.04672_. * Costa-jussa et al. (2022b) Marta R Costa-jussa, Eric Smith, Christophe Ropers, Daniel Licht, Jean Maillard, Javier Ferrando, and Carlos Escolano. 2022b. Toxicity in multilingual machine translation at scale. _arXiv preprint arXiv:2210.03070_. * Cui et al. (2023) Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and effective text encoding for chinese llama and alpaca. _arXiv preprint arXiv:2304.08177_. * Dabre et al. (2020) Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan. 2020. A survey of multilingual neural machine translation. _ACM Computing Surveys (CSUR)_, 53(5):1-38. David Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Loic Barrault, and Marta R Costa-jussa. 2023. Halomi: A manually annotated benchmark for multilingual hallucination and omission detection in machine translation. _arXiv preprint arXiv:2305.11746_. * Datta et al. (2023) Debtanu Datta, Shubham Soni, Rajdeep Mukherjee, and Saptarshi Ghosh. 2023. Mildsum: A novel benchmark dataset for multilingual summarization of indian legal case judgments. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 5291-5302. * Davidson et al. (2017) Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In _Proceedings of the international AAAI conference on web and social media_, volume 11, pages 512-515. * De Raedt et al. (2023) Maarten De Raedt, Semere Kiros Bitew, Frederic Godin, Thomas Demeester, and Chris Develder. 2023. Zero-shot cross-lingual sentiment classification under distribution shift: an exploratory study. _arXiv preprint arXiv:2311.06549_. * de Varda and Marelli (2023) Andrea de Varda and Marco Marelli. 2023. Scaling in cognitive modelling: a multilingual approach to human reading times. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 139-149, Toronto, Canada. Association for Computational Linguistics. * Deng et al. (2022) Shumin Deng, Ningyu Zhang, Feiyu Xiong, Jeff Z Pan, and Huajun Chen. 2022. Knowledge extraction in low-resource scenarios: survey and perspective. _arXiv preprint arXiv:2202.08063_. * Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. _arXiv preprint arXiv:2305.14314_. * Dogruoz et al. (2023a) A Seza Dogruoz, Sunayana Sitaram, Barbara E Bullock, and Almeida Jacqueline Toribio. 2023a. A survey of code-switching: Linguistic and social perspectives for language technologies. _arXiv preprint arXiv:2301.01967_. * Dogruoz et al. (2023b) A. Seza Dogruoz, Sunayana Sitaram, and Zheng Xin Yong. 2023b. Representativeness as a forgotten lesson for multilingual and code-switched data collection and preparation. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 5751-5767, Singapore. Association for Computational Linguistics. * Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. _arXiv preprint arXiv:2301.00234_. * Dong et al. (2022) Yi Dong, Zhilin Wang, Makesh Sreedhar, Xianchao Wu, and Oleksii Kuchaiev. 2023. Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rhhf. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 11275-11288. * Driess et al. (2023) Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_. * Espana-Bonet (2023) Cristina Espana-Bonet. 2023. Multilingual coarse political stance classification of media. the editorial line of a chatgpt and bard newspaper. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 11757-11777. * Etxaniz et al. (2023) Julen Etxaniz, Gorka Azkune, Aitor Soroa, Oier Lopez de Lacalle, and Mikel Artetxe. 2023. Do multilingual language models think better in english? _arXiv preprint arXiv:2308.01223_. * Fan et al. (2021) Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. 2021. Beyond english-centric multilingual machine translation. _The Journal of Machine Learning Research_, 22(1):4839-4886. * Ferron et al. (2023) Amila Ferron, Amber Shore, Ekata Mitra, and Ameeta Agrawal. 2023. Meep: Is this engaging? prompting large language models for dialogue evaluation in multilingual settings. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 2078-2100. * Fetahu et al. (2023) Besnik Fetahu, Zhiyu Chen, Sudipta Kar, Oleg Rokhlenko, and Shervin Malmasi. 2023. Multi-coner v2: a large multilingual dataset for fine-grained and noisy named entity recognition. _arXiv preprint arXiv:2310.13213_. * FitzGerald et al. (2022) Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Laim Urbach, Vishesh Kakarala, Richa Singh, et al. 2022. Massive: A 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages. _arXiv preprint arXiv:2204.08582_. * Foundation (2022) Wikimedia Foundation. Wikimedia downloads. * Fu et al. (2022) Jinlan Fu, See-Kiong Ng, and Pengfei Liu. 2022. Polyglot prompt: Multilingual multitask promptraining. _arXiv preprint arXiv:2204.14264_. * Fujii et al. (2023) Takuro Fujii, Koki Shibata, Atsuki Yamaguchi, Terufumi Morishita, and Yasuhiro Sogawa. 2023. How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in japanese. _arXiv preprint arXiv:2306.09572_. * Fujinuma et al. (2023) Yoshinari Fujinuma, Siddharth Varia, Nishant Sankaran, Srikar Appalaraju, Bonan Min, and Yogarshi Vyas. 2023. A multi-modal multilingual benchmark for document image classification. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 14361-14376. Yi R Fung, Tuhin Chakraborty, Hao Guo, Owen Rambow, Smaranda Muresan, and Heng Ji. 2022. Normage: Multi-lingual multi-cultural norm discovery from conversations on-the-fly. _arXiv preprint arXiv:2210.08604_. * Gao et al. (2024) Pengzhi Gao, Zhongjun He, Hua Wu, and Haifeng Wang. 2024. Towards boosting many-to-many multilingual machine translation with large language models. _arXiv preprint arXiv:2401.05861_. * Garcia et al. (2022) Gabriel Lino Garcia, Pedro Henrique Paiola, Luis Henrique Morelli, Giovanni Candido, Arnaldo Candido Junior, Danilo Samuel Jodas, Luis Afonso, Ivan Rizzo Guilherme, Bruno Elias Penteado, and Joao Paulo Papa. 2024. Introducing bode: A fine-tuned large language model for portuguese prompt-based task. _arXiv preprint arXiv:2401.02909_. * Garcia et al. (2023) Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maximi Krikun, Melvin Johnson, and Orhan Firat. 2023. The unreasonable effectiveness of few-shot learning for machine translation. In _International Conference on Machine Learning_, pages 10867-10878. PMLR. * Geigle et al. (2023) Gregor Geigle, Abhay Jain, Radu Timofte, and Goran Glavas. 2023. mblip: Efficient bootstrapping of multilingual vision-llms. _arXiv preprint arXiv:2307.06930_. * Gekhman et al. (2023) Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor. 2023. Truetaeacher: Learning factual consistency evaluation with large language models. _arXiv preprint arXiv:2305.11171_. * Goel et al. (2023) Rahul Goel, Waleed Ammar, Aditya Gupta, Siddharth Vashishtha, Motoki Sano, Faiz Surani, Max Chang, HyunJeong Choe, David Greene, Kyle He, et al. 2023. Presto: A multilingual dataset for parsing realistic task-oriented dialogs. _arXiv preprint arXiv:2303.08954_. * Goenaga et al. (2023) Iakes Goenaga, Aitziber Atutxa, Koldo Gojenola, Maite Oronoz, and Rodrigo Agerri. 2023. Explanatory argument extraction of correct answers in resident medical exams. _arXiv preprint arXiv:2312.00567_. * Golovneva et al. (2022) O. Yu. Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2022. Roscoe: A suite of metrics for scoring step-by-step reasoning. _ArXiv_, abs/2212.07919. * Goyal et al. (2022) Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzman, and Angela Fan. 2022. The hores-101 evaluation benchmark for low-resource and multilingual machine translation. _Transactions of the Association for Computational Linguistics_, 10:522-538. * Guerreiro et al. (2023a) Nuno M Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and Andre FT Martins. 2023a. Hallucinations in large multilingual translation models. _arXiv preprint arXiv:2303.16104_. * Guerreiro et al. (2023b) Nuno M Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and Andre FT Martins. 2023b. xcomet: Transparent machine translation evaluation through fine-grained error detection. _arXiv preprint arXiv:2310.10482_. * Gueuwou et al. (2023) Shester Gueuwou, Sophie Siake, Colin Leong, and Mathias Muller. 2023. Jwsign: A highly multilingual corpus of bible translations for more diversity in sign language processing. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 9907-9927. * Guo et al. (2023) Wenyu Guo, Qingkai Fang, Dong Yu, and Yang Feng. 2023. Bridging the gap between synthetic and authentic images for multimodal machine translation. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 2863-2874. * Hada et al. (2023) Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. 2023. Are large language model-based evaluators the solution to scaling up multilingual evaluation? _arXiv preprint arXiv:2309.07462_. * Hammerl et al. (2022) Katharina Hammerl, Bjorn Deiseroth, Patrick Schramowski, Jindrich Libovicky, Alexander Fraser, and Kristian Kersting. 2022. Do multilingual language models capture differing moral norms? _arXiv preprint arXiv:2203.09904_. * Han et al. (2023) HyoJung Han, Jordan Boyd-Graber, and Marine Carpuat. 2023. Bridging background knowledge gaps in translation with automatic explicitation. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 9718-9735. * Hardalov et al. (2020) Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov, Ivan Koychev, and Preslav Nakov. 2020. Exams: A multi-subject high school examinations dataset for cross-lingual and multilingual question answering. _arXiv preprint arXiv:2011.03080_. * He et al. (2023a) Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin. 2023a. Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models. _arXiv preprint arXiv:2308.10755_. * He et al. (2023b) Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2023b. Exploring human-like translation strategy with large language models. _arXiv preprint arXiv:2305.04118_. * Held et al. (2023) William Held, Camille Harris, Michael Best, and Diyi Yang. 2023. A material lens on coloniality in nlp. _arXiv preprint arXiv:2311.08391_. * Hershcovich et al. (2017) Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, et al. 2022. Challenges and strategies in cross-cultural nlp. _arXiv preprint arXiv:2203.10020_. * HIT-SCIR (2024) HIT-SCIR. 2024. Chinese-mixtral-8x7b: An open-source mixture-of-experts llm. [https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B](https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B). * Hlavnova and Ruder (2023) Ester Hlavnova and Sebastian Ruder. 2023. Empowering cross-lingual behavioral testing of nlp models with typological features. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 7181-7198. * Hoang et al. (2023) Hieu Hoang, Huda Khayrallah, and Marcin Junczys-Dowmunt. 2023. On-the-fly fusion of large language models and machine translation. _arXiv preprint arXiv:2311.08306_. * Holmstrom and Doostmohammadi (2023) Oskar Holmstrom and Ehsan Doostmohammadi. 2023. Making instruction finetuning accessible to non-english languages: A case study on swedish models. In _Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)_, pages 634-642. * Holmstrom et al. (2023) Oskar Holmstrom, Jenny Kunz, and Marco Kuhlmann. 2023. Bridging the resource gap: Exploring the efficacy of english and multilingual lms for swedish. In _Proceedings of the Second Workshop on Resources and Representations for Under-Resourced Languages and Domains (RESOURCEFUL-2023)_, pages 92-110. * Hu et al. (2023a) Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, et al. 2023a. Large multilingual models pivot zero-shot multimodal learning across languages. _arXiv preprint arXiv:2308.12038_. * Hu et al. (2020) Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In _International Conference on Machine Learning_, pages 4411-4421. PMLR. * Hu et al. (2023b) Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Oiguang Chen, Bin Wang, Yu Qiao, and Ping Luo. 2023b. Tree-planner: Efficient close-loop task planning with large language models. _arXiv preprint arXiv:2310.08582_. * Hu et al. (2024) Songbo Hu, Xiaobin Wang, Zhangdie Yuan, Anna Korhonen, and Ivan Vulic. 2024. Dialight: Lightweight multilingual development and evaluation of task-oriented dialogue systems with large language models. _arXiv preprint arXiv:2401.02208_. * Hu et al. (2023c) Songbo Hu, Han Zhou, Mete Hergul, Milan Gritta, Guchun Zhang, Ignacio Iacobacci, Ivan Vulic, and Anna Korhonen. 2023c. Multi 3 woz: A multilingual, multi-domain, multi-parallel dataset for training and evaluating culturally adapted task-oriented dialog systems. _Transactions of the Association for Computational Linguistics_, 11:1396-1415. * Hua et al. (2023) Wen-Yu Hua, Brian Williams, and Davood Shamsi. 2023. Lacos-bloom: Low-rank adaptation with contrastive objective on 8 bits siamese-bloom. _arXiv preprint arXiv:2305.06404_. * Huang et al. (2023a) Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, and Furu Wei. 2023a. Not all languages are created equal in llms: Improving multilingual capability by cross-lingual-thought prompting. _arXiv preprint arXiv:2305.07004_. * Huang et al. (2024) W Ronny Huang, Cyril Allauzen, Tongzhou Chen, Kilol Gupta, Ke Hu, James Qin, Yu Zhang, Yongqiang Wang, Shuo-Yiin Chang, and Tara N Sainath. 2024. Multilingual and fully non-autoregressive asr with large language model fusion: A comprehensive study. _arXiv preprint arXiv:2401.12789_. * Huang et al. (2023b) Zhichao Huang, Rong Ye, Tom Ko, Qianqian Dong, Shanbo Cheng, Mingxuan Wang, and Hang Li. 2023b. Speech translation with large language models: An industrial practice. _arXiv preprint arXiv:2312.13585_. * Huot et al. (2023) Fantine Huot, Joshua Maynez, Chris Alberti, Reinald Kim Amplayo, Priyanka Agrawal, Constanza Fierro, Shashi Narayan, and Mirella Lapata. 2023. \\(\\mu\\)plan: Summarizing using a content plan as cross-lingual bridge. _arXiv preprint arXiv:2305.14205_. * ImaniGooghari et al. (2023) Ayyood ImaniGooghari, Peiqin Lin, Amir Hossein Karagaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, Andre FT Martins, Francois Yvon, et al. 2023. Glot500: Scaling multilingual corpora and language models to 500 languages. _arXiv preprint arXiv:2305.12182_. * Iyer et al. (2023) Vivek Iyer, Pinzhen Chen, and Alexandra Birch. 2023. Towards effective disambiguation for machine translation with large language models. In _Proceedings of the Eighth Conference on Machine Translation_, pages 482-495. * Jiang and Zubiaga (2024) Aiqi Jiang and Arkaitz Zubiaga. 2024. Cross-lingual offensive language detection: A systematic review of datasets, transfer approaches and challenges. _arXiv preprint arXiv:2401.09244_. * Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. _arXiv preprint arXiv:2310.06825_. * Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mistral of experts. _arXiv preprint arXiv:2401.04088_. * Jiang and Joshi (2023) Ming Jiang and Mansi Joshi. 2023. Copopa: Ranking cultural concept popularity by llms. _arXiv preprint arXiv:2311.07897_. Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023. Parrot: Translating during chat using large language models tuned with human translation and feedback. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 15009-15020. * Joseph et al. (2023) Sebastian Joseph, Kathryn Kazanas, Keziah Reina, Vishnesh J Ramanathan, Wei Xu, Byron C Wallace, and Junyi Jessy Li. 2023. Multilingual simplification of medical texts. _arXiv preprint arXiv:2305.12532_. * Kabra et al. (2023) Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, and Graham Neubig. 2023. Multi-lingual and multi-cultural figurative language understanding. _arXiv preprint arXiv:2305.16171_. * Kale et al. (2021) Mihir Kale, Aditya Siddhant, Noah Constant, Melvin Johnson, Rami Al-Rfou, and Linting Xue. 2021. nmt5-is parallel data still relevant for pre-training massively multilingual language models? _arXiv preprint arXiv:2106.02171_. * Keung et al. (2020) Phillip Keung, Yichao Lu, Gyorgy Szarvas, and Noah A Smith. 2020. The multilingual amazon reviews corpus. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4563-4568. * Kew et al. (2023) Tannon Kew, Florian Schottmann, and Rico Sennrich. 2023. Turning english-centric llms into polyglots: How much multilinguality is needed? _arXiv preprint arXiv:2312.12683_. * Khatri et al. (2023) Jyotsana Khatri, Rudra Murthy, Amar Prakash Azad, and Pushpak Bhattacharyya. 2023. A study of multilingual versus meta-learning for language model pre-training for adaptation to unseen low resource languages. In _Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track_, pages 26-34. * Khandaker et al. (2023) Md Tawkat Islam Khandaker, Abdul Waheed, El Moatez Billah Nagoudi, and Muhammad Abdul-Mageed. 2023. Gptaraeval: A comprehensive evaluation of chatgpt on arabic nlp. _arXiv preprint arXiv:2305.14976_. * Kim et al. (2021) Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Jeon Dong Hyeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, et al. 2021. What changes can large-scale language models bring? intensive study on hyper-clova: Billions-scale korean generative pretrained transformers. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 3405-3424. * Kim et al. (2023a) Sunkyoung Kim, Dayeon Ki, Yireun Kim, and Jinsik Lee. 2023a. Boosting cross-lingual transferability in multilingual models via in-context learning. _arXiv preprint arXiv:2305.15233_. * Kim et al. (2023b) Yongil Kim, Yerin Hwang, Hyeongu Yun, Seunghyun Yoon, Trung Bui, and Kyomin Jung. 2023b. Pr-mcs: Perturbation robust metric for multilingual image captioning. _arXiv preprint arXiv:2303.08389_. * Kocmi et al. (2023) Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, et al. 2023. Findings of the 2023 conference on machine translation (wmt23): Llms are here but not quite there yet. In _Proceedings of the Eighth Conference on Machine Translation_, pages 1-42. * Koehn (2005) Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In _Proceedings of machine translation summit x: papers_, pages 79-86. * Kohli et al. (2023) Guneet Singh Kohli, Shantipriya Parida, Sambit Sekhar, Samirit Saha, Nipun B Nair, Parul Agarwal, Sonal Khosla, Kusumlata Patiyal, and Debasish Dhal. 2023. Building a llama2-finetuned llm for odia language utilizing domain knowledge instruction set. _arXiv preprint arXiv:2312.12624_. * Koishekenov et al. (2022) Yeskendir Koishekenov, Vassilina Nikoulina, and Alexandre Berard. 2022. Memory-efficient nllb-200: Language-specific expert pruning of a massively multilingual machine translation model. _arXiv preprint arXiv:2212.09811_. * Kopf et al. (2023) Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, et al. 2023. Openassistant conversations-democratizing large language model alignment. _arXiv preprint arXiv:2304.07327_. * Kudugunta et al. (2023) Sneha Kudugunta, Isaac Rayburn Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. 2023. Madlad-400: A multilingual and document-level large audited dataset. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_. * Kunchukuttan et al. (2018) Anoop Kunchukuttan, Pratik Mehta, and Pushpak Bhattacharyya. 2018. The IIT Bombay English-Hindi parallel corpus. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan. European Language Resources Association (ELRA). * Kuparinen et al. (2023) Olli Kuparinen, Aleksandra Miletic, and Yves Scherrer. 2023. Dialect-to-standard normalization: A large-scale multilingual evaluation. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 13814-13828. * Kwon et al. (2023a) Sang Kwon, Gagan Bhatia, Muhammad Abdul-Mageed, et al. 2023a. Beyond english: Evaluating llms for arabic grammatical error correction. In _Proceedings of ArabicNLP 2023_, pages 101-119. * Kwon et al. (2023b) Sang Yun Kwon, Gagan Bhatia, El Moatez Billah Nagoud, and Muhammad Abdul-Mageed. 2023b. Chatgpt for arabic grammatical error correction. _arXiv preprint arXiv:2308.04492_. * Lai et al. (2023a) Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023a. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. _arXiv preprint arXiv:2304.05613_. * Lai et al. (2023b) Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A Rossi, and Thien Huu Nguyen. 2023b. Okapi: Instructured large language models in multiple languages with reinforcement learning from human feedback. _arXiv preprint arXiv:2307.16039_. * Larcher et al. (2023) Celio Larcher, Marcos Piau, Paulo Finardi, Pedro Gengo, Piero Esposito, and Vinicius Carida. 2023. Cabrita: closing the gap for foreign languages. _arXiv preprint arXiv:2308.11878_. * Laurencon et al. (2022) Hugo Laurencon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonzalez Ponferrada, Huu Nguyen, et al. 2022. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. _Advances in Neural Information Processing Systems_, 35:31809-31826. * Le et al. (2024) Khoi M Le, Trinh Pham, Tho Quan, and Anh Tuan Luu. 2024. Lampat: Low-rank adaption for multilingual paraphrasing using adversarial training. _arXiv preprint arXiv:2401.04348_. * Lee et al. (2023) Minwoo Lee, Hyukhun Koh, Kang-il Lee, Dongdong Zhang, Minsung Kim, and Kyomin Jung. 2023. Target-agnostic gender-aware contrastive learning for mitigating bias in multilingual machine translation. _arXiv preprint arXiv:2305.14016_. * Lepikhin et al. (2020) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. _arXiv preprint arXiv:2006.16668_. * Lewis et al. (2019) Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2019. Mlqa: Evaluating cross-lingual extractive question answering. _arXiv preprint arXiv:1910.07475_. * Li and Callison-Burch (2023) Bryan Li and Chris Callison-Burch. 2023. This land is {Your, My} land: Evaluating geopolitical biases in language models. _arXiv preprint arXiv:2305.14610_. * Li et al. (2023a) Chong Li, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. 2023a. Align after pre-train: Improving multilingual generative models with cross-lingual alignment. _arXiv preprint arXiv:2311.08089_. * Li et al. (2023b) Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. 2023b. Bactrian-x: A multilingual replicable instruction-following model with low-rank adaptation. _arXiv preprint arXiv:2305.15011_. * Li et al. (2020) Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2020. Mtop: A comprehensive multilingual task-oriented semantic parsing benchmark. _arXiv preprint arXiv:2008.09335_. * Li et al. (2023c) Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Chen, and Jiajun Chen. 2023c. Eliciting the translation ability of large language models via multilingual finetuning with translation instructions. _arXiv preprint arXiv:2305.15083_. * Li et al. (2023d) Oliver Li, Mallika Subramanian, Arkadiy Saakyan, CH-Wang Sky, and Smaranda Muresan. 2023d. Normdial: A comparable bilingual synthetic dialog dataset for modeling social norm adherence and violation. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 15732-15744. * Li et al. (2023e) Shangjie Li, Xiangpeng Wei, Shaolin Zhu, Jun Xie, Baosong Yang, and Deyi Xiong. 2023e. Mmmmt: Modularizing multilingual neural machine translation with flexibly assembled moe and dense blocks. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 4978-4990. * Li et al. (2023f) Xiaoqian Li, Ercong Nie, and Sheng Liang. 2023f. Crosslingual retrieval augmented in-context learning for bangla. In _Proceedings of the First Workshop on Bangla Language Processing (BLP-2023)_, pages 136-151. * Li et al. (2023g) Xiaoqian Li, Ercong Nie, and Sheng Liang. 2023g. From classification to generation: Insights into crosslingual retrieval augmented icl. _arXiv preprint arXiv:2311.06595_. * Li et al. (2023h) Yangning Li, Shirong Ma, Xiaobin Wang, Shen Huang, Chengyue Jiang, Hai-Tao Zheng, Pengjun Xie, Fei Huang, and Yong Jiang. 2023h. Ecomgpt: Instruction-tuning large language model with chain-of-task tasks for e-commerce. _arXiv preprint arXiv:2308.06966_. * Li et al. (2023i) Yaoyiran Li, Anna Korhonen, and Ivan Vulic. 2023i. On bilingual lexicon induction with large language models. _arXiv preprint arXiv:2310.13995_. * Liang et al. (2023) Xiao Liang, Yen-Min Jasmina Khaw, Soung-Yue Liew, Tien-Ping Tan, and DongHong Qin. 2023. Multilingual sentence alignment with gpt models. In _2023 4th International Conference on Artificial Intelligence and Data Sciences (AiDAS)_, pages 218-223. IEEE. * Liang et al. (2020) Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, et al. 2020. Xglue: A new benchmark dataset for cross-lingual pre-training, understanding and generation. _arXiv preprint arXiv:2004.01401_. * Li et al. (2020)Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xiang Ren. 2021a. Common sense beyond English: Evaluating and improving multilingual language models for commonsense reasoning. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 1274-1287, Online. Association for Computational Linguistics. * Lin (2004) Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In _Text Summarization Branches Out_, pages 74-81, Barcelona, Spain. Association for Computational Linguistics. * Lin et al. (2023) Peiqin Lin, Chengzhi Hu, Zheyu Zhang, Andre FT Martins, and Hinrich Schutze. 2023. mplm-sim: Unveiling better cross-lingual similarity and transfer in multilingual pretrained language models. _arXiv preprint arXiv:2305.13684_. * Lin et al. (2022a) Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettelmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022a. Few-shot learning with multilingual generative language models. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 9019-9052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. * Lin et al. (2021b) Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021b. Few-shot learning with multilingual language models. _arXiv preprint arXiv:2112.10668_. * Lin et al. (2022b) Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. 2022b. Few-shot learning with multilingual generative language models. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 9019-9052. * Liu et al. (2023a) Chen Cecilia Liu, Fairi Koto, Timothy Baldwin, and Iryna Gurevych. 2023a. Are multilingual lms culturally-diverse reasoners? an investigation into multicultural proverbs and sayings. _arXiv preprint arXiv:2309.08591_. * Liu et al. (2023b) Peng Liu, Lemei Zhang, Terje Nissen Farup, Even W Lauvrak, Jon Espen Ingvaldsen, Simen Eide, Jon Atle Gulla, and Zhirong Yang. 2023b. Nlebench+ norglm: A comprehensive empirical analysis and benchmark dataset for generative language models in norwegian. _arXiv preprint arXiv:2312.01314_. * Liu et al. (2023c) Xuebo Liu, Yutong Wang, Derek F Wong, Runzhe Zhan, Liangxuan Yu, and Min Zhang. 2023c. Revisiting commonsense reasoning in machine translation: Training, evaluation and challenge. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 15536-15550. * Lou et al. (2023) Lianzhang Lou, Xi Yin, Yutao Xie, and Yang Xiang. 2023. Cceval: A representative evaluation benchmark for the chinese-centric multilingual machine translation. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 10176-10184. * Lu et al. (2023) Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, and Furu Wei. 2023. Chain-of-dictionary prompting elicits translation in large language models. _arXiv preprint arXiv:2305.06575_. * Luo et al. (2023a) Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023a. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. _arXiv preprint arXiv:2308.09583_. * Luo et al. (2023b) Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu, Chenyang Zhao, Donglei Zhang, et al. 2023b. Yayi 2: Multilingual open-source large language models. _arXiv preprint arXiv:2312.14862_. * Luukkonen et al. (2023) Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, et al. 2023. Fingpt: Large generative models for a small language. _arXiv preprint arXiv:2311.05640_. * Lyu et al. (2023) Chenyang Lyu, Jitao Xu, and Longyue Wang. 2023. New trends in machine translation using large language models: Case examples with chatgpt. _arXiv preprint arXiv:2305.01181_. * Ma et al. (2023) Chunlan Ma, Ayyoob ImaniGooghari, Haotian Ye, Ehsaneddin Asgari, and Hinrich Schutze. 2023. Taxi1500: A multilingual dataset for text classification in 1500 languages. _arXiv preprint arXiv:2305.08487_. * Macko et al. (2023) Dominik Macko, Robert Moro, Adaku Uchendu, Jason Lucas, Michiharu Yamashita, Matus Pikuliak, Ivan Srba, Thai Le, Dongwon Lee, Jakub Simko, et al. 2023. Multitude: Large-scale multilingual machine-generated text detection benchmark. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 9960-9987. * Macko et al. (2024) Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason Samuel Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko, and Maria Bielikova. 2024. Authorship obfuscation in multilingual machine-generated text detection. _arXiv preprint arXiv:2401.07867_. * Maity et al. (2023) Ankita Maity, Anubhav Sharma, Rudra Dhar, Tushar Abhishek, Manish Gupta, and Vasudeva Varma. 2023. Multilingual bias detection and mitigation for indian languages. _arXiv preprint arXiv:2312.15181_. * Ma et al. (2023)* Malkin et al. (2022) Dan Malkin, Tomasz Limisiewicz, and Gabriel Stanovsky. 2022. A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank. _arXiv preprint arXiv:2205.04086_. * Malmasi et al. (2022) Shervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta Kar, and Oleg Rokhlenko. 2022. Multiconer: a large-scale multilingual dataset for complex named entity recognition. _arXiv preprint arXiv:2208.14536_. * Mayer and Cysouw (2014) Thomas Mayer and Michael Cysouw. 2014. Creating a massively parallel Bible corpus. In _Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)_, pages 3158-3163, Reykjavik, Iceland. European Language Resources Association (ELRA). * Mendonca et al. (2023a) John Mendonca, Alon Lavie, and Isabel Trancoso. 2023a. Towards multilingual automatic open-domain dialogue evaluation. In _Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue_, pages 130-141. * Mendonca et al. (2023b) John Mendonca, Patricia Pereira, Helena Moniz, Joao Paulo Carvalho, Alon Lavie, and Isabel M Trancoso. 2023b. Simple llm prompting is state-of-the-art for robust and multilingual dialogue evaluation. In _Proceedings of The Eleventh Dialog System Technology Challenge_, pages 133-143. * Michaelov et al. (2023) James Michaelov, Catherine Arnett, Tyler Chang, and Ben Bergen. 2023. Structural priming demonstrates abstract grammatical representations in multilingual language models. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 3703-3720. * Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? _arXiv preprint arXiv:2202.12837_. * Mittal et al. (2023) Shubham Mittal, Megha Sundriyal, and Preslav Nakov. 2023. Lost in translation, found in spans: Identifying claims in multilingual social media. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 3887-3902, Singapore. Association for Computational Linguistics. * Moradshahi et al. (2023) Mehrad Moradshahi, Tianhao Shen, Kalika Bali, Monojit Choudhury, Gael de Chalendar, Anmol Goel, Sungkyun Kim, Prashant Kodali, Ponnurangam Kumaraguru, Nasredine Semmar, Sina Semnani, Jiwon Seo, Vivek Seshadri, Manish Shrivastava, Michael Sun, Aditya Yadavalli, Chaophu You, Deyi Xiong, and Monica Lam. 2023. X-RiSAWOZ: High-quality end-to-end multilingual dialogue datasets and few-shot agents. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 2773-2794, Toronto, Canada. Association for Computational Linguistics. * Moslem et al. (2023a) Yasmin Moslem, Rejwanul Haque, and Andy Way. 2023a. Adaptive machine translation with large language models. _arXiv preprint arXiv:2301.13294_. * Moslem et al. (2023b) Yasmin Moslem, Rejwanul Haque, and Andy Way. 2023b. Fine-tuning large language models for adaptive machine translation. _arXiv preprint arXiv:2312.12740_. * Ramos et al. (2023) Miguel Moura Ramos, Patrick Fernandes, Antonio Farinhas, and Andre FT Martins. 2023. Aligning neural machine translation models: Human feedback in training and inference. _arXiv e-prints_, pages arXiv-2311. * Muennighoff et al. (2022) Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. _arXiv preprint arXiv:2211.01786_. * Muhammad et al. (2023) Shamsuddeen Hassan Muhammad, Idris Abdulummin, Abinew Ali Ayele, Nedjma Ousidhoum, David Ifelouwa Adelani, Seid Muhie Yimam, Ibrahim Sa'id Ahmad, Meriem Beloucif, Saif Mohammad, Sebastian Ruder, et al. 2023. Afrisenti: A twitter sentiment analysis benchmark for african languages. _arXiv preprint arXiv:2302.08956_. * Mujadia et al. (2023) Vandan Mujadia, Ashok Urlana, Yash Bhaskar, Penumalla Aditya Pavani, Kukkapalli Shravya, Parameswari Krishnamurthy, and Dipti Misra Sharma. 2023. Assessing translation capabilities of large language models involving english and indian languages. _arXiv preprint arXiv:2311.09216_. * Muller et al. (2023) Benjamin Muller, John Wieting, Jonathan H Clark, Tom Kwiatkowski, Sebastian Ruder, Livio Baldini Soares, Roee Aharoni, Jonathan Herzig, and Xinyi Wang. 2023. Evaluating and modeling attribution for cross-lingual question answering. _arXiv preprint arXiv:2305.14332_. * Muraoka et al. (2023) Masayasu Muraoka, Bishwaranjan Bhattacharjee, Michele Merler, Graeme Blackwood, Yulong Li, and Yang Zhao. 2023. Cross-lingual transfer of large language model by visually-derived supervision toward low-resource languages. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 3637-3646. * Nambi et al. (2023) Akshay Nambi, Vaibhav Balloli, Mercy Ranjit, Tanuja Ganu, Kabir Ahuja, Sunayana Sitaram, and Kalika Bali. 2023. Breaking language barriers with a leap: Learning strategies for polyglot llms. _arXiv preprint arXiv:2305.17740_. * Naous et al. (2023a) Tarek Naous, Michael J Ryan, Mohit Chandra, and Wei Xu. 2023a. Towards massively multi-domain multilingual readability assessment. _arXiv preprint arXiv:2305.14463_. * Naous et al. (2023b) Tarek Naous, Michael J Ryan, and Wei Xu. 2023b. Having beer after prayer? measuring cultural bias in large language models. _arXiv preprint arXiv:2305.14456_. * Narayan et al. (2018) Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. _arXiv preprint arXiv:1808.08745_. * Nguyen et al. (2023a) Hoang Nguyen, Ye Liu, Chenwei Zhang, Tao Zhang, and Philip Yu. 2023a. CoF-CoT: Enhancing large language models with coarse-to-fine chain-of-thought prompting for multi-domain NLU tasks. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 12109-12119, Singapore. Association for Computational Linguistics. * Nguyen et al. (2023b) Laura Nguyen, Thomas Scialom, Benjamin Piwowarski, and Jacopo Staiano. 2023b. Loralay: A multilingual and multimodal dataset for long range and layout-aware summarization. _arXiv preprint arXiv:2301.11312_. * Nguyen et al. (2023c) Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A Rossi, and Thien Huu Nguyen. 2023c. Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages. _arXiv preprint arXiv:2309.09400_. * Nicosia and Piccinno (2022) Massimo Nicosia and Francesco Piccinno. 2022. Evaluating byte and wordpiece level models for massively multilingual semantic parsing. _arXiv preprint arXiv:2212.07223_. * Ogueji et al. (2021) Kelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021. Small data? no problem! exploring the viability of pretrained multilingual language models for low-resourced languages. In _Proceedings of the 1st Workshop on Multilingual Representation Learning_, pages 116-126. * Ohmer et al. (2023) Xenia Ohmer, Elia Bruni, and Dieuwke Hupkes. 2023. Evaluating task understanding through multilingual consistency: A chatgpt case study. _arXiv preprint arXiv:2305.11662_. * OpenAI (2022) OpenAI. 2022. Chatgpt. * OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. * Pan et al. (2023) Wenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che, and Libo Qin. 2023. A preliminary evaluation of chatgpt for zero-shot dialogue understanding. _arXiv preprint arXiv:2304.04256_. * Pan et al. (2019) Xiaoman Pan, Thamme Gowda, Heng Ji, Jonathan May, and Scott Miller. 2019. Cross-lingual joint entity and word embedding to improve entity linking and parallel sentence mining. In _Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)_, pages 56-66, Hong Kong, China. Association for Computational Linguistics. * Panchendrarajan and Zubiaga (2024) Rrubaa Panchendrarajan and Arkaitz Zubiaga. 2024. Claim detection for automated fact-checking: A survey on monolingual, multilingual and cross-lingual research. _arXiv preprint arXiv:2401.11969_. * Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318. * Park et al. (2023) Nohil Park, Joonsuk Park, Kang Min Yoo, and Sungroh Yoon. 2023. On the analysis of cross-lingual prompt tuning for decoder-based multilingual model. _arXiv preprint arXiv:2311.07820_. * Patel et al. (2022) Ajay Patel, Bryan Li, Mohammad Sadegh Rasooli, Noah Constant, Colin Raffel, and Chris Callison-Burch. 2022. Bidirectional language models are also few-shot learners. In _The Eleventh International Conference on Learning Representations_. * Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_. * Petrick et al. (2023) Frithjof Petrick, Christian Herold, Pavel Petrushkov, Shahram Khadivi, and Hermann Ney. 2023. Document-level language models for machine translation. _arXiv preprint arXiv:2310.12303_. * Petrov et al. (2023) Aleksandar Petrov, Emanuele La Malfa, Philip HS Torr, and Adel Bibi. 2023. Language model tokenizers introduce unfairness between languages. _arXiv preprint arXiv:2305.15425_. * Pfeiffer et al. (2023) Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia, Xinyi Wang, Machel Reid, and Sebastian Ruder. 2023. mmt5: Modular multilingual pre-training solves source language hallucinations. _arXiv preprint arXiv:2305.14224_. * Philippy et al. (2023) Fred Philippy, Siwen Guo, and Shohreh Haddadan. 2023. Towards a common understanding of contributing factors for cross-lingual transfer in multilingual language models: A review. _arXiv preprint arXiv:2305.16768_. * Pilault et al. (2023) Jonathan Pilault, Xavier Garcia, Arthur Brazinskas, and Orhan Firat. 2023. Interactive-chain-prompting: Ambiguity resolution for crosslingual conditional generation with interaction. _arXiv preprint arXiv:2301.10309_. * Pires et al. (2023) Ramon Pires, Hugo Abonizio, Thales Rogerio, and Rodrigo Nogueira. 2023. Sabi\\(\\backslash\\)'a: Portuguese large language models. _arXiv preprint arXiv:2304.07880_. * Ponti et al. (2020) Edoardo Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. 2020. Xcopa: A multilingual dataset for causal commonsense reasoning. _arXiv preprint arXiv:2005.00333_. * Popovic (2017) Maja Popovic. 2017. chrF++: words helping character n-grams. In _Proceedings of the Second Conference on Machine Translation_, pages 612-618, Copenhagen, Denmark. Association for Computational Linguistics. * P * Pourkamali and Sharif (2024) Nooshin Pourkamali and Shler Ebrahim Sharif. 2024. Machine translation with large language models: Prompt engineering for persian, english, and russian directions. _arXiv preprint arXiv:2401.08429_. * Puduppully et al. (2023a) Ratish Puduppully, Raj Dabre, Ai Ti Aw, and Nancy F Chen. 2023a. Decomposed prompting for machine translation between related languages using large language models. _arXiv preprint arXiv:2305.13085_. * Puduppully et al. (2023b) Ratish Puduppully, Anoop Kunchukuttan, Raj Dabre, Aiti Aw, and Nancy Chen. 2023b. Decomt: Decomposed prompting for machine translation between related languages using large language models. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 4586-4602. * Puttaparthi et al. (2023) Poorna Chander Reddy Puttaparthi, Soham Sanjay Deo, Hakan Gul, Yiming Tang, Weiyi Shang, and Zhe Yu. 2023. Comprehensive evaluation of chatgpt reliability through multilingual inquiries. _arXiv preprint arXiv:2312.10524_. * Qi et al. (2023) Jirui Qi, Raquel Fernandez, and Arianna Bisazza. 2023. Cross-lingual consistency of factual knowledge in multilingual language models. _arXiv preprint arXiv:2310.10378_. * Qin et al. (2023a) Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. 2023a. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages. _arXiv preprint arXiv:2310.14799_. * Qin et al. (2023b) Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. 2023b. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages. * Qin et al. (2022) Libo Qin, Qiguang Chen, Tianbao Xie, Qixin Li, Jian-Guang Lou, Wanxiang Che, and Min-Yen Kan. 2022. Gl-clef: A global-local contrastive learning framework for cross-lingual spoken language understanding. _arXiv preprint arXiv:2204.08325_. * Qin et al. (2020) Libo Qin, Minheng Ni, Yue Zhang, and Wanxiang Che. 2020. Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp. _arXiv preprint arXiv:2006.06402_. * Qiu et al. (2023) Yifu Qiu, Yftah Ziser, Anna Korhonen, Edoardo M Ponti, and Shay B Cohen. 2023. Detecting and mitigating hallucinations in multilingual summarisation. _arXiv preprint arXiv:2305.13632_. * Rakhimova et al. (2024) Diana Rakhimova, Aidana Karibayeva, and Assem Turarbek. 2024. The task of post-editing machine translation for the low-resource language. _Applied Sciences_, 14(2):486. * Ramesh et al. (2023) Krithika Ramesh, Sunayana Sitaram, and Monojit Choudhury. 2023. Fairness in language models beyond english: Gaps and challenges. _arXiv preprint arXiv:2302.12578_. * Ramos et al. (2023) Rita Ramos, Bruno Martins, and Desmond Elliott. 2023. Lmcap: Few-shot multilingual image captioning by retrieval augmented language model prompting. _arXiv preprint arXiv:2305.19821_. * Ranaldi et al. (2023a) Leonardo Ranaldi, Giulia Pucci, and Andre Freitas. 2023a. Empowering cross-lingual abilities of instruction-tuned large language models by translation-following demonstrations. _arXiv preprint arXiv:2308.14186_. * Ranaldi et al. (2023b) Vikas Raunak, Arul Menezes, and Marcin Junczys-Dowmunt. 2021. The curious case of hallucinations in neural machine translation. _arXiv preprint arXiv:2104.06683_. * Raunak et al. (2023) Vikas Raunak, Amr Sharaf, Hany Hassan Awadallah, and Arul Menezes. 2023. Leveraging gpt-4 for automatic translation post-editing. _arXiv preprint arXiv:2305.14878_. * Razumovskaia et al. (2022) Evgeniia Razumovskaia, Joshua Maynez, Annie Louis, Mirella Lapata, and Shashi Narayan. 2022. Little red riding hood goes around the globe: Crosslingual story planning and generation with large language models. _arXiv preprint arXiv:2212.10471_. * Rei et al. (2020) Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. Comet: A neural framework for mt evaluation. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 2685-2702. * Reinauer et al. (2023) Raphael Reinauer, Patrick Simianer, Kaden Uhlig, Johannes EM Mosig, and Joern Wuebker. 2023. Neural machine translation models can learn to be few-shot learners. _arXiv preprint arXiv:2309.08590_. * Rodriguez et al. (2023) Juan Diego Rodriguez, Katrin Erk, and Greg Durrett. 2023. X-parade: Cross-lingual textual entailment and information divergence across paragraphs. _arXiv preprint arXiv:2309.08873_. * Rosenbaum et al. (2022a) Andy Rosenbaum, Saleh Soltan, Wael Hamza, Amir Saffari, Marco Damonte, and Isabel Groves. 2022a. Clasp: Few-shot cross-lingual data augmentation for semantic parsing. _AACL-IJCNLP 2022_, page 444. * Rosenbaum et al. (2022b) Andy Rosenbaum, Saleh Soltan, Wael Hamza, Yannick Versley, and Markus Boese. 2022b. Linguist: Language model instruction tuning to generate annotated utterances for intent classification and slot tagging. _arXiv preprint arXiv:2209.09900_. * Ruder et al. (2021) Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, et al. 2021. Xtreme-r: Towards more challenging and nuanced multilingual evaluation. _arXiv preprint arXiv:2104.07412_. * Ruder et al. (2020)Phillip Rust, Jonas F Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, and Desmond Elliott. 2022. Language modelling with pixels. _arXiv preprint arXiv:2207.06991_. * Ryan et al. (2023) Michael J Ryan, Tarek Naous, and Wei Xu. 2023. Revisiting non-english text simplification: A unified multilingual benchmark. _arXiv preprint arXiv:2305.15678_. * Sanchez et al. (2023) Eduardo Sanchez, Pierre Andrews, Pontus Stenetorp, Mikel Artetxe, and Marta R Costa-jussa. 2023. Gender-specific machine translation with large language models. _arXiv preprint arXiv:2309.03175_. * Santilli and Rodola (2023) Andrea Santilli and Emanuele Rodola. 2023. Camoscio: An italian instruction-tuned llama. _arXiv preprint arXiv:2307.16456_. * Schioppa et al. (2023) Andrea Schioppa, Xavier Garcia, and Orhan Firat. 2023. Cross-lingual supervision improves large language models pre-training. _arXiv preprint arXiv:2305.11778_. * Schott et al. (2023) Tim Schott, Daniel Furman, and Shreshta Bhat. 2023. Polyglot or not? measuring multilingual encyclopedic knowledge in foundation models. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 11238-11253. * Schuster et al. (2018) Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike Lewis. 2018. Cross-lingual transfer learning for multilingual task oriented dialog. _arXiv preprint arXiv:1810.13327_. * Schwenk et al. (2021) Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzman. 2021. WikiMatrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia. In _Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume_, pages 1351-1361, Online. Association for Computational Linguistics. * Seganti et al. (2021) Alessandro Seganti, Klaudia Firlag, Helena Skowronska, Michal Satawa, and Piotr Andruszkiewicz. 2021. Multilingual entity and relation extraction dataset and model. In _Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume_, pages 1946-1955, Online. Association for Computational Linguistics. * Sellam et al. (2020) Thibault Sellam, Dipanjan Das, and Ankur P Parikh. 2020. Bleurt: Learning robust metrics for text generation. _arXiv preprint arXiv:2004.04696_. * Sengupta et al. (2023) Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, et al. 2023. Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models. _arXiv preprint arXiv:2308.16149_. * Shaham et al. (2024) Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. 2024. Multilingual instruction tuning with just a pinch of multilinguality. _arXiv preprint arXiv:2401.01854_. * SharegPT (2023) SharegPT. 2023. Sharegpt. * She et al. (2024) Shuaijie She, Shujian Huang, Wei Zou, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. 2024. Mapo: Advancing multilingual reasoning through multilingual alignment-as-preference optimization. _arXiv preprint arXiv:2401.06838_. * Shen et al. (2024) Lingfeng Shen, Weiting Tan, Sihao Chen, Yunmo Chen, Jingyu Zhang, Haoran Xu, Boyuan Zheng, Philipp Koehn, and Daniel Khashabi. 2024. The language barrier: Dissecting safety challenges of llms in multilingual contexts. _arXiv preprint arXiv:2401.13136_. * Shi et al. (2022a) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022a. Language models are multilingual chain-of-thought reasoners. _arXiv preprint arXiv:2210.03057_. * Shi et al. (2022b) Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022b. Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing. _arXiv preprint arXiv:2210.13693_. * Shliazhko et al. (2022) Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina. 2022. mgpt: Few-shot learners go multilingual. _arXiv preprint arXiv:2204.07580_. * Sia et al. (2023) Suzanna Sia, Alexandra DeLucia, and Kevin Duh. 2023. Anti-lm decoding for zero-shot in-context machine translation. _arXiv preprint arXiv:2311.08324_. * Soltan et al. (2022) Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, et al. 2022. Alexiarm 20b: Few-shot learning using a large-scale multilingual seq2seq model. _arXiv preprint arXiv:2208.01448_. * Son et al. (2023) Gujin Son, Hanwool Lee, Suwan Kim, Jaecheol Lee, Je Won Yeom, Jihyu Jung, Jung Woo Kim, and Songseong Kim. 2023. Hae-rae bench: Evaluation of korean knowledge in language models. _arXiv preprint arXiv:2309.02706_. * Song et al. (2022) Yixiao Song, Kalpesh Krishna, Rajesh Bhatt, and Mohit Iyyer. 2022. Sling: Sino linguistic evaluation of large language models. _arXiv preprint arXiv:2210.11689_. * Srinivasan and Choi (2022) Anirudh Srinivasan and Eunsol Choi. 2022. Tydip: A dataset for politeness classification in nine typologically diverse languages. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 5723-5738. * Stefanovitch and Piskorski (2023) Nicolas Stefanovitch and Jakub Piskorski. 2023. Holistic inter-annotator agreement and corpus coherence estimation in a large-scale multilingual annotation campaign. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 71-86. * Su et al. (2022) Hui Su, Xiao Zhou, Houjin Yu, Xiaoyu Shen, Yuwen Chen, Zilin Zhu, Yang Yu, and Jie Zhou. 2022. Welm: A well-read pre-trained language model for chinese. _arXiv preprint arXiv:2209.10372_. * Ortiz Suarez et al. (2019) Pedro Javier Ortiz Suarez, Benoit Sagot, and Laurent Romary. 2019. Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures. In _7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)_. Leibniz-Institut fur Deutsche Sprache. * Sun et al. (2023a) Jimin Sun, Patrick Fernandes, Xinyi Wang, and Graham Neubig. 2023a. A multi-dimensional evaluation of tokenizer-free multilingual pretrained models. In _Findings of the Association for Computational Linguistics: EACL 2023_, pages 1680-1690. * Sun et al. (2023b) Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. 2023b. Moss: Training conversational language models from synthetic data. * Sun et al. (2021) Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, et al. 2021. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. _arXiv preprint arXiv:2107.02137_. * Sun et al. (2023c) Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023c. Salmon: Self-alignment with principle-following reward models. _arXiv preprint arXiv:2310.05910_. * Tanwar et al. (2023) Eshaan Tanwar, Manish Borthakur, Subhabrata Dutta, and Tanmoy Chakraborty. 2023. Multilingual lms are better cross-lingual in-context learners with alignment. _arXiv preprint arXiv:2305.05940_. * Team (2023a) COIG PC Team. 2023a. Coig pc. * Team et al. (2023b) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_. * Team (2023b) Huozi Team. 2023b. Huozi: An open-source universal llm. [https://github.com/HIT-SCIR/huozi](https://github.com/HIT-SCIR/huozi). * Team (2023c) InternLM Team. 2023c. Internlm: A multilingual language model with progressively enhanced capabilities. * Team (2023d) YuLan Team. 2023d. Yulan-chat: An open-source bilingual chatbot. [https://github.com/RUC-GSAI/YuLan-Chat](https://github.com/RUC-GSAI/YuLan-Chat). * Thakur et al. (2023b) Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, et al. 2023a. Nomic-acl: Knowing when you don't know for robust multilingual retrieval-augmented generation. _arXiv preprint arXiv:2312.11361_. * Thakur et al. (2023b) Nandan Thakur, Jianmo Ni, Gustavo Hernandez Abrego, John Wieting, Jimmy Lin, and Daniel Cer. 2023b. Leveraging llms for synthesizing training data across many languages in multilingual dense retrieval. _arXiv preprint arXiv:2311.05800_. * Thapliyal et al. (2022) Ashish V Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. 2022. Crossmodal-3600: A massively multilingual multimodal evaluation dataset. _arXiv preprint arXiv:2205.12522_. * Thulke et al. (2024) David Thulke, Yingbo Gao, Petrus Pelser, Rein Brune, Rricha Jalota, Floris Fok, Michael Ramos, Ian van Wyk, Abdallah Nasir, Hayden Goldstein, et al. 2024. Climategpt: Towards ai synthesizing interdisciplinary research on climate change. _arXiv preprint arXiv:2401.09646_. * Tiedemann (2012) Jorg Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In _Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)_, pages 2214-2218, Istanbul, Turkey. European Language Resources Association (ELRA). * Tikhonov and Ryabinin (2021) Alexey Tikhonov and Max Ryabinin. 2021. It's all in the heads: Using attention heads as a baseline for cross-lingual transfer in commonsense reasoning. _arXiv preprint arXiv:2106.12066_. * Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_. * Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_. * Trivedi et al. (2023) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 10014-10037, Toronto, Canada. Association for Computational Linguistics. * Tikhonov and Ryabinin (2021)Kuang Tseng and Chow-Sing Lin. 2022. Enhancing natural language inference of cross-lingual n-shot transfer with multilingual data. In _2022 8th International Conference on Applied System Innovation (ICASI)_, pages 68-71. IEEE. * Tu et al. (2023) Lifu Tu, Jin Qu, Semih Yavuz, Shafiq Joty, Wenhao Liu, Caiming Xiong, and Yingbo Zhou. 2023. Efficiently aligned cross-lingual transfer learning for conversational tasks using prompt-tuning. _arXiv preprint arXiv:2304.01295_. * Tuo et al. (2023) Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. 2023. Anytext: Multilingual visual text generation and editing. _arXiv preprint arXiv:2311.03054_. * Uludogan et al. (2024) Gokce Uludogan, Zeynep Yirmibesoglu Balal, Furkan Akkurt, Meliksha Turker, Onur Gungor, and Susan Uskudarli. 2024. Turna: A turkish encoder-decoder language model for enhanced understanding and generation. _arXiv preprint arXiv:2401.14373_. * Upadhayay and Behzadan (2023) Bibek Upadhayay and Vahid Behzadan. 2023. Taco: Enhancing cross-lingual transfer for low-resource languages in llms through translation-assisted chain-of-thought processes. _arXiv preprint arXiv:2311.10797_. * Urlana et al. (2023) Ashok Urlana, Pinzhen Chen, Zheng Zhao, Shay B Cohen, Manish Shrivastava, and Barry Haddow. 2023. Pmindiasum: Multilingual and cross-lingual headline summarization for languages in india. _arXiv preprint arXiv:2305.08828_. * Uthus et al. (2023) David Uthus, Santiago Ontanon, Joshua Ainslie, and Mandy Guo. 2023. mlongt5: A multilingual and efficient text-to-text transformer for longer sequences. * Verma et al. (2023) Yash Verma, Anubhav Jangra, Raghvendra Kumar, and Sriparna Saha. 2023. Large scale multi-lingual multi-modal summarization dataset. _arXiv preprint arXiv:2302.06560_. * Vernikos and Popescu-Belis (2024) Giorgos Vernikos and Andrei Popescu-Belis. 2024. Don't rank, combine! combining machine translation hypotheses using quality estimation. _arXiv preprint arXiv:2401.06688_. * Vilar et al. (2022) David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2022. Prompting palm for translation: Assessing strategies and _arXiv preprint arXiv:2211.09102_. * Vu et al. (2022) Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, and Noah Constant. 2022. Overcoming catastrophic forgetting in zero-shot cross-lingual generation. _arXiv preprint arXiv:2205.12647_. * Wang et al. (2023a) Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding, Ai Ti Aw, and Nancy F Chen. 2023a. Seaeval for multilingual foundation models: From cross-lingual alignment to cultural reasoning. _arXiv preprint arXiv:2309.04766_. * Wang et al. (2023b) Jiaan Wang, Yunlong Liang, Fandong Meng, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2023b. Cross-lingual summarization via chatgpt. _arXiv preprint arXiv:2302.14229_. * Wang et al. (2023c) Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao, and Jiarong Xu. 2023c. Cross-lingual knowledge editing in large language models. _arXiv preprint arXiv:2309.08952_. * Wang et al. (2022a) Jiaan Wang, Fandong Meng, Ziyao Lu, Duo Zheng, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2022a. Clid-sum: A benchmark dataset for cross-lingual dialogue summarization. _arXiv preprint arXiv:2202.05599_. * Wang et al. (2023d) Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023d. Document-level machine translation with large language models. _arXiv preprint arXiv:2304.02210_. * Wang et al. (2023e) Weixuan Wang, Barry Haddow, and Alexandra Birch. 2023e. Retrieval-augmented multilingual knowledge editing. _arXiv preprint arXiv:2312.13040_. * Wang et al. (2023f) Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, and Michael R Lyu. 2023f. All languages matter: On the multilingual safety of large language models. _arXiv preprint arXiv:2310.00905_. * Wang et al. (2022b) Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhaneskaran, Atharva Naik, David Stap, et al. 2022b. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. _arXiv preprint arXiv:2204.07705_. * Wang et al. (2023g) Yuhang Wang, Yanxu Zhu, Chao Kong, Shuyu Wei, Xiaoyuan Yi, Xing Xie, and Jitao Sang. 2023g. Cdeval: A benchmark for measuring the cultural dimensions of large language models. _arXiv preprint arXiv:2311.16421_. * Wang et al. (2023h) Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Aji, et al. 2023h. M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection. _arXiv preprint arXiv:2305.14902_. * Wang et al. (2022c) Zhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F Xu, and Graham Neubig. 2022c. Mconala: a benchmark for code generation from multiple natural languages. _arXiv preprint arXiv:2203.08388_. * Wang et al. (2022d) Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. 2022d. Execution-based evaluation for open-domain code generation. _arXiv preprint arXiv:2212.10481_. * Wassie (2023) Aman Kassahun Wassie. 2023. Machine translation for ge'ez language. _arXiv preprint arXiv:2311.14530_. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837. * Wei et al. (2023a) Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, et al. 2023a. Skywork: A more open bilingual foundation model. _arXiv preprint arXiv:2310.19341_. * Wei et al. (2023b) Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. 2023b. Zero-shot information extraction via chatgpt. _arXiv preprint arXiv:2302.10205_. * Wei et al. (2023c) Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, et al. 2023c. Polym: An open source polyglot large language model. _arXiv preprint arXiv:2307.06018_. * Weissweiler et al. (2023) Leonie Weissweiler, Valentin Hofmann, Anjali Kantharuban, Anna Cai, Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek Vijayakumar, Haofei Yu, Hinrich Schutze, Kemal Oflazer, and David Mortensen. 2023. Counting the bugs in ChatGPT's wugs: A multilingual investigation into the morphological capabilities of a large language model. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 6508-6524, Singapore. Association for Computational Linguistics. * Wen-Yi and Mimno (2023) Andrea Wen-Yi and David Mimno. 2023. Hyperpolyd-glot lllms: Cross-lingual interpretability in token embeddings. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 1124-1131. * Whitehouse et al. (2023a) Chenxi Whitehouse, Monojit Choudhury, and Alham Fikri Aji. 2023a. Llm-powered data augmentation for enhanced crosslingual performance. _arXiv preprint arXiv:2305.14288_. * Whitehouse et al. (2023b) Chenxi Whitehouse, Fantine Huot, Jasnijn Bastings, Mostafa Dehghani, Chu-Cheng Lin, and Mirella Lapata. 2023b. Parameter-efficient multilingual summarisation: An empirical study. _arXiv preprint arXiv:2311.08572_. * Winata et al. (2022a) Genta Winata, Shijie Wu, Mayank Kulkarni, Thamar Solorio, and Daniel Preotiuc-Pietro. 2022a. Cross-lingual few-shot learning on unseen languages. In _Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 777-791, Online only. Association for Computational Linguistics. * Winata et al. (2023) Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Rahmad Mahendra, Fajri Koto, Ade Romadhony, Kemal Kurniawan, David Moeljadi, Rattiyo Eko Prasojo, Pascale Fung, Timothy Baldwin, Jey Han Lau, Rico Sennrich, and Sebastian Ruder. 2023a. NusaX: Multilingual parallel sentiment dataset for 10 Indonesian local languages. In _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics_, pages 815-834, Dubrovnik, Croatia. Association for Computational Linguistics. * Winata et al. (2022b) Genta Indra Winata, Alham Fikri Aji, Zheng-Xin Yong, and Thamar Solorio. 2022b. The decades progress on code-switching research in nlp: A systematic survey on trends and challenges. _arXiv preprint arXiv:2212.09660_. * Winata et al. (2023b) Genta Indra Winata, Liang-Kang Huang, Soumya Vadlamannati, and Yash Chandarana. 2023b. Multilingual few-shot learning via language model retrieval. _arXiv preprint arXiv:2306.10964_. * Workshop et al. (2022) BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_. * Wu et al. (2023) Suhang Wu, Minlong Peng, Yue Chen, Jinsong Su, and Mingming Sun. 2023. Eva-kellm: A new benchmark for evaluating knowledge editing of lms. _arXiv preprint arXiv:2308.09954_. * Wu and Hu (2023) Yangjian Wu and Gang Hu. 2023. Exploring prompt engineering with gpt language models for document-level machine translation: Insights and findings. In _Proceedings of the Eighth Conference on Machine Translation_, pages 166-169. * Xiao et al. (2023) Zedian Xiao, William Held, Yanchen Liu, and Diyi Yang. 2023. Task-agnostic low-rank adapters for unseen english dialects. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 7857-7870. * Xu et al. (2023a) Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2023a. A paradigm shift in machine translation: Boosting translation performance of large language models. _arXiv preprint arXiv:2309.11674_. * Xu et al. (2023b) Nan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, and Muhao Chen. 2023b. Cognitive overload: Jailbreaking large language models with overloaded logical thinking. _arXiv preprint arXiv:2311.09827_. * Xu et al. (2023c) Ningyu Xu, Qi Zhang, Jingting Ye, Menghan Zhang, and Xuanjing Huang. 2023c. Are structural concepts universal in transformer language models? towards interpretable cross-lingual generalization. _arXiv preprint arXiv:2310.12794_. * Xu et al. (2023d) Shaoyang Xu, Junzhuo Li, and Deyi Xiong. 2023d. Language representation projection: Can we transfer factual knowledge across languages in multilingual language models? In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 3692-3702. Weijia Xu, Batool Haider, and Saab Mansour. 2020. End-to-end slot alignment and recognition for cross-lingual NLU. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 5052-5063, Online. Association for Computational Linguistics. * Xu et al. (2023) Dao Xuan-Quy, Le Ngoc-Bich, Vo The-Duy, Phan Xuan-Dung, Ngo Bac-Bien, Nguyen Van-Tien, Nguyen Thi-My-Thanh, and Nguyen Hong-Phuoc. 2023. Vnhsgie: Vietnamese high school graduation examination dataset for large language models. _arXiv preprint arXiv:2305.12199_. * Xue et al. (2022) Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. Byt5: Towards a token-free future with pre-trained byte-to-byte models. _Transactions of the Association for Computational Linguistics_, 10:291-306. * Xue et al. (2020) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer. _arXiv preprint arXiv:2010.11934_. * Xue et al. (2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 483-498. * Yadav et al. (2023) Ankit Yadav, Shubham Chandel, Sushant Chaturale, and Anil Bandhakavi. 2023. Lahm: Large annotated dataset for multi-domain and multilingual hate speech identification. _arXiv preprint arXiv:2304.00913_. * Yang et al. (2023a) Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, et al. 2023a. Baichuan 2: Open large-scale language models. _arXiv preprint arXiv:2309.10305_. * Yang et al. (2023b) Chih-Kai Yang, Kuan-Po Huang, Ke-Han Lu, Chun-Yi Kuan, Chi-Yuan Hsiao, and Hung-yi Lee. 2023b. Investigating zero-shot generalizability on mandarin-english code-switched asr and speech-to-text translation of recent foundation models with self-supervision and weak supervision. _arXiv preprint arXiv:2401.00273_. * Yang et al. (2023c) Guangyu Yang, Jinghong Chen, Weizhe Lin, and Bill Byrne. 2023c. Direct preference optimization for neural machine translation with minimum bayes risk decoding. _arXiv preprint arXiv:2311.08380_. * Yang et al. (2019a) Muyun Yang, Xixin Hu, Hao Xiong, Jiayi Wang, Yiliyier Jiaermuhamaiti, Zhongjun He, Weihua Luo, and Shujian Huang. 2019a. Cemt 2019 machine translation evaluation report. In _Machine Translation: 15th China Conference, CCMT 2019, Nanchang, China, September 27-29, 2019, Revised Selected Papers 15_, pages 105-128. Springer. * Yang et al. (2023d) Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. 2023d. Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages. _arXiv preprint arXiv:2305.18098_. * Yang et al. (2023e) Yahan Yang, Soham Dan, Dan Roth, and Insup Lee. 2023e. Understanding calibration for multilingual question answering models. _arXiv preprint arXiv:2311.08669_. * Yang et al. (2019b) Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019b. Paws-x: A cross-lingual adversarial dataset for paraphrase identification. _arXiv preprint arXiv:1908.11828_. * Yang et al. (2023f) Zijian Gyozo Yang, Laszlo Janos Laki, Tamas Varadi, and Gabor Proszeky. 2023f. Mono- and multilingual gpt-3 models for hungarian. In _International Conference on Text, Speech, and Dialogue_, pages 94-104. Springer. * Ye et al. (2023a) Jiacheng Ye, Xijia Tao, and Lingpeng Kong. 2023a. Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability. _arXiv preprint arXiv:2306.06688_. * Ye et al. (2023b) Meng Ye, Karan Sikka, Katherine Atwell, Sabit Hassan, Ajay Divakaran, and Malihe Alikhani. 2023b. Multilingual content moderation: A case study on reddit. _arXiv preprint arXiv:2302.09618_. * Yin et al. (2022) Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, and Kai-Wei Chang. 2022. Geom-lama: Geo-diverse commonsense probing on multilingual pre-trained language models. _arXiv preprint arXiv:2205.12247_. * Yong et al. (2022) Zheng-Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, et al. 2022. Bloom+ 1: Adding language support to bloom for zero-shot prompting. _arXiv preprint arXiv:2212.09535_. * Yong et al. (2023) Zheng-Xin Yong, Ruochen Zhang, Jessica Zosa Forde, Skyler Wang, Samuel Cahyawijaya, Holy Lovenia, Genta Indra Winata, Lintang Sutawika, Jan Christian Blaise Cruz, Long Phan, et al. 2023. Prompting multilingual large language models to generate code-mixed texts: The case of south east asian languages. _arXiv e-prints_, pages arXiv-2303. * Yoon et al. (2024) Dongkeun Yoon, Joel Jang, Sungdong Kim, Seungone Kim, Sheikh Shafayat, and Minjoon Seo. 2024. Langbridge: Multilingual reasoning without multilingual supervision. _arXiv preprint arXiv:2401.10695_. * Yu et al. (2022) Xinyan Velocity Yu, Akari Asai, Trina Chatterjee, Junjie Hu, and Eunsol Choi. 2022. Beyond counting datasets: a survey of multilingual dataset construction and necessary resources. _arXiv preprint arXiv:2211.15649_. * Yuan et al. (2023) Fei Yuan, Yinquan Lu, Wenhao Zhu, Lingpeng Kong, Lei Li, Yu Qiao, and Jingjing Xu. 2023. Lego-mt:Learning detachable models for massively multilingual machine translation. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 11518-11533. * Y. Gong Y. P. Q. Niu L. Z. Baochang Ma Xiangang Li Yunjie Ji, Yong Deng. 2023. Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases. _arXiv preprint arXiv:2303.14742_. * Zeman et al. (2022) Daniel Zeman, Joakim Nivre, Mitchell Abrams, Elia Ackermann, Noemi Aepli, Hamid Aghaei, Zeljko Agic, Amir Ahmadi, et al. 2022. Universal dependencies 2.10. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (UFAL), Faculty of Mathematics and Physics, Charles University. * Zeng et al. (2022) A. Zeng, X. Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. _arXiv preprint arXiv:2210.02414_. * Zeng et al. (2023a) Jiali Zeng, F. Meng, Yongjing Yin, and Jie Zhou. 2023a. Improving machine translation with large language models: A preliminary study with cooperative decoding. _arXiv preprint arXiv:2311.02851_. * Zeng et al. (2023b) Jiali Zeng, F. Meng, Yongjing Yin, and Jie Zhou. 2023b. Tim: Teaching large language models to translate with comparison. _arXiv preprint arXiv:2307.04408_. * Zhang et al. (2023a) Biao Zhang, Barry Haddow, and Alexandra Birch. 2023a. Prompting large language model for machine translation: A case study. _arXiv preprint arXiv:2301.07069_. * Zhang et al. (2020) Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020. Improving massively multilingual neural machine translation and zero-shot translation. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 1628-1639, Online. Association for Computational Linguistics. * Zhang et al. (2023b) Chen Zhang, Luis Fernando D'Haro, Chengguang Tang, Ke Shi, Guohua Tang, and Haizhou Li. 2023b. xdial-eval: A multilingual open-domain dialogue evaluation benchmark. _arXiv preprint arXiv:2310.08958_. * Zhang et al. (2023c) Chiyu Zhang, Khai Duy Doan, Qisheng Liao, and Muhammad Abdul-Mageed. 2023c. The skipped beat: A study of sociopragmatic understanding in lIms for 64 languages. _arXiv preprint arXiv:2310.14557_. * Zhang et al. (2023d) Min Zhang, Limin Liu, Zhao Yanqing, Xiaosong Qiao, Su Chang, Xiaofeng Zhao, Junhao Zhu, Ming Zhu, Song Peng, Yinglu Li, et al. 2023d. Leveraging multilingual knowledge graph to boost domain-specific entity translation of chatpgt. In _Proceedings of Machine Translation Summit XIX, Vol. 2: Users Track_, pages 77-87. * Zhang et al. (2023e) Ran Zhang, J. Ouni, and Steffen Eger. 2023e. Cross-lingual cross-temporal summarization: Dataset, models, evaluation. _arXiv preprint arXiv:2306.12916_. * Zhang et al. (2023f) Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz, and Alham Fikri Aji. 2023f. Multilingual large language models are not (yet) code-switchers. _arXiv preprint arXiv:2305.14235_. * Zhang and Eickhoff (2023) Ruochen Zhang and Carsten Eickhoff. 2023. Crocosum: A benchmark dataset for cross-lingual code-switched summarization. _arXiv preprint arXiv:2303.04092_. * Zhang et al. (2023g) Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, et al. 2023g. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models. _arXiv preprint arXiv:2306.10968_. * Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_. * Zhang* et al. (2020) Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In _International Conference on Learning Representations_. * Zhang et al. (2023h) Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. 2023h. M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. _arXiv preprint arXiv:2306.05179_. * Zhang et al. (2023i) Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, and Grzegorz Kondrak. 2023i. Don't trust gpt when your question is not in english. _arXiv preprint arXiv:2305.16339_. * Zhang et al. (2023j) Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. 2023j. Evaluating the performance of large language models on gaokao benchmark. _arXiv preprint arXiv:2305.12474_. * Zhang et al. (2023k) Yusen Zhang, Jun Wang, Zhiguo Wang, and Rui Zhang. 2023k. Xsemplr: Cross-lingual semantic parsing in multiple natural languages and meaning representations. _arXiv preprint arXiv:2306.04085_. * Zhang et al. (2021) Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, et al. 2021. Cpm-2: Large-scale cost-effective pre-trained language models. _AI Open_, 2:216-224. * Zhang et al. (2023f) Ziyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao, Rui Wang, and Hai Hu. 2023f. Mela: Multilingual evaluation of linguistic acceptability. _arXiv preprint arXiv:2311.09033_. Biao Zhao, Weiqiang Jin, Javier Del Ser, and Guang Yang. 2023a. Chatagri: Exploring potentials of chatgpt on cross-linguistic agricultural text classification. _arXiv preprint arXiv:2305.15024_. * Zhao et al. (2023b) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023b. A survey of large language models. _arXiv preprint arXiv:2303.18223_. * Zheng et al. (2022) Bo Zheng, Zhouyang Li, Fuxuan Wei, Qiguang Chen, Libo Qin, and Wanxiang Che. 2022. HIT-SCIR at MMNLU-22: Consistency regularization for multilingual spoken language understanding. In _Proceedings of the Massively Multilingual Natural Language Understanding Workshop (MMNLU-22)_, pages 35-41, Abu Dhabi, United Arab Eminates (Hybrid). Association for Computational Linguistics. * Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-jadge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_. * Zhong et al. (2023a) Shanshan Zhong, Zhongzhan Huang, Shanghua Gao, Wushao Wen, Liang Lin, Marinka Zitnik, and Pan Zhou. 2023a. Let's think outside the box: Exploring leap-of-thought in large language models with creative humor generation. _arXiv preprint arXiv:2312.02439_. * Zhong et al. (2023b) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023b. Agieval: A human-centric benchmark for evaluating foundation models. _arXiv preprint arXiv:2304.06364_. * Zhou et al. (2023) Chulun Zhou, Yunlong Liang, Fandong Meng, Jinan Xu, Jinsong Su, and Jie Zhou. 2023. Rc3: Regularized contrastive cross-lingual cross-modal pretraining. _arXiv preprint arXiv:2305.07927_. * Zhou and Zhang (2023) Di Zhou and Yinxian Zhang. 2023. Red ai? inconsistent responses from gpt3. 5 models on political issues in the us and china. _arXiv preprint arXiv:2312.09917_. * Zhou (2023) Kairui Zhou. 2023. Accessible instruction-following agent. _arXiv preprint arXiv:2305.06358_. * Zhu et al. (2024) Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, and Alexandra Birch. 2024. Question translation training for better multilingual reasoning. _arXiv preprint arXiv:2401.07817_. * Zhu et al. (2023) Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023. Extrapolating large language models to non-english by aligning languages. _arXiv preprint arXiv:2308.04948_. * Ziemski et al. (2016) Michal Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. 2016. The United Nations parallel corpus v1.0. In _Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)_, pages 3530-3534, Portoroz, Slovenia. European Language Resources Association (ELRA). * Zouhar and Bojar (2024) Vilem Zouhar and Ondrej Bojar. 2024. Quality and quantity of machine translation references for automated metrics. _arXiv preprint arXiv:2401.01283_."
    },
    {
      "title": "Appendix A Multilingual Performance Evaluation",
      "text": "To facilitate the comparison of LLMs, extensive efforts have been invested in exploring enhanced evaluation methods for multilingual scenarios. This discussion will elaborate on MLLM evaluation, covering both (1) _Evaluation Metrics_ and (2) _Evaluation Benchmarks_."
    },
    {
      "title": "Evaluation Metrics",
      "text": "Traditional Automatic Metricmeans that we assess predicted output using probabilities or pre-trained language model logits Liu et al. (2023); Zouhar and Bojar (2024). Generally speaking, researchers use BLEU Papineni et al. (2002), BLEURT Sellam et al. (2020), chrF++ Popovic (2017) and COMET Rei et al. (2020) for translation evaluation, and use ROUGE Lin (2004) for summary evaluation. Further, Guerreiro et al. (2023) proposed xCOMET for better translation evaluation through fine-grained error detection. In assessing the general quality of generated text, the commonly employed approach is the utilization of multi-lingual BERTScore Zhang* et al. (2020) as an evaluation metric. Qin et al. (2023) extended Roscoe Golovneva et al. (2022) to multi-language for quality assessment of multi-lingual CoT. Further, Hlavnova and Ruder (2023) developed a comprehensive and robust multi-lingual checklist system to thoroughly assess the MLLMs' performance. MLLM-based Automatic Metricemploys robust MLLMs to score or compare generated outputs for evaluation purposes Li et al. (2023); Zhang et al. (2023); Vernikos and Popescu-Belis (2024). Specifically, Zheng et al. (2023) introduced LLM-as-a-Judge, where GPT4 is prompted to assess the performance of other LLMs by comparing its output to the predicted one. However, this method discussed remains unreliable in multilingual settings Hada et al. (2023). And caution should be exercised, particularly in languages where it is known that the MLLM has performed poorly. Furthermore, Kim et al. (2023); Muller et al. (2023) conducted attribution evaluation to deeply evaluate the robustness of the model. Human Evaluationinvolves manually assessing MLLMs through detailed evaluation Zhang et al. (2023); Li et al. (2023); Khodaker et al. (2023); Zhang et al. (2023). Lyu et al. (2023) initially explored the multilingual challenges of ChatGPT through manually annotated cases. Furthermore, Hu et al. (2024) introduced a new platform for more convenient manual assessments."
    },
    {
      "title": "Evaluation Benchmarks",
      "text": "Current MLLMs tend to pay more attention to the alignment effect of non-English languages. Based on the different angles of alignment, we divide it into two categories: (1) _Natural Language Understanding_; (2) _Natural Language Generation_."
    },
    {
      "title": "A.2.1 Natural Language Understanding",
      "text": "Linguistics AnalysisFor multilingual models, the most basic thing is to understand the linguistic differences between different languages Xu et al. (2023). The most common multilingual linguistics assessment includes Part-of-Speech (POS) Liang et al. (2020); Zeman et al. (2022), grammar analysis Kwon et al. (2023); Alhafni et al. (2023); Michaelov et al. (2023); Kwon et al. (2023) and morphology Weissweiler et al. (2023). Furthermore, Zhang et al. (2023); Song et al. (2022) conducted a comprehensive evaluation the linguistic acceptability of MLLM across languages. Semantic UnderstandingResearchers take more care of should be able to analyze and understand the specific semantics of multiple languages Lai et al. (2023); Schott et al. (2023); Panchendrarajan and Zubiaga (2024). The most basic is to perform local semantic understanding, and the most typical one is the information extraction task Wei et al. (2023), including: masakhaNER Adelani et al. (2021), MASSIVE FitzGerald et al. (2022), MultiCoNER Malmasi et al. (2022); Fetahu et al. (2023), WikiAnn Pan et al. (2019) and SMiLER Seganti et al. (2021) The second is the semantic understanding of complete sentences, including: XNLI Conneau et al. (2018), Paws-X Yang et al. (2019), MixATIS++ Xu et al. (2020), MTOP Li et al. (2020), MultiNLU Schuster et al. (2018), and PRESTO Goel et al. (2023). Finally, there is the semantic understanding of the paragraph, like question-answering tasks with context: MLQA Lewis et al. (2019), XQuADArtetxe et al. (2020), TyDiQA Clark et al. (2020) and X-PARADE Rodriguez et al. (2023), X-CLAIM Mittal et al. (2023), Readme++ Naous et al. (2023), XKaggle-DBQA Shi et al. (2022) and de Varda and Marelli (2023). Due to theemergence of a large number of multilingual benchmarks in recent years, a series of work has begun to combine the various existing semantic understanding tasks together for unified evaluation, including: XTREME Hu et al. (2020), XTREME-R Ruder et al. (2021), XGLUE Liang et al. (2020), MEGA Ahuja et al. (2023), MEGA-Verse Ahuja et al. (2023), AGIEval Zhong et al. (2023), and Superlim Berdicevskis et al. (2023). Further, Thapliyal et al. (2022); Changpinyo et al. (2022); Fujinuma et al. (2023); Kudugunta et al. (2023) extend the semantic understanding of multi-modal context. Since MLLMs have some biases Costa-jussa et al. (2023); Lee et al. (2023) or vulnerabilities Xu et al. (2023); Puttaparthi et al. (2023); Shen et al. (2024), Espana-Bonet (2023); Cao et al. (2023); Jiang and Zubiaga (2024); Macko et al. (2024) has begun to consider the corresponding benchmark to evaluate MLLMs. Cultural UnderstandingLimited by cultural differences, the understanding between different languages is not completely parallel Li and Callison-Burch (2023); Maity et al. (2023); Cahyawijaya et al. (2023), so researchers began to explore how to evaluate multi-cultural scenes Naous et al. (2023); Hershcovich et al. (2022). The most typical one is multi-cultural sentiment analysis Sentiment Analysis Davidson et al. (2017); Srinivasan and Choi (2022); Li et al. (2023); Muhammad et al. (2023); Winata et al. (2023); Yadav et al. (2023). Furthermore, Zhang et al. (2023) expands the multi-cultural scene to the entire Sociopragmatic Understanding level. In particular, Kabra et al. (2023); Wang et al. (2023); Jiang and Joshi (2023); Fung et al. (2022); Li et al. (2023); Son et al. (2023); Zhou and Zhang (2023) proposed new benchmarks, requiring the model to fully understand different cultures. Furthermore, due to the emergence of reasoning capabilities, Qin et al. (2023); Liu et al. (2023); Wang et al. (2023) start to evaluate the reasoning ability of MLLMs with different cultural backgrounds. Knowledge UnderstandingA large amount of work has been done to test the degree of knowledge transfer of MLLM between different languages through examination questions. Specif \\begin{table} \\begin{tabular}{l c c c c c} \\hline \\hline Dataset & Storage Size & Token Size & Language Size & Source & Latest Update Time \\\\ \\hline \\multicolumn{5}{c}{_Manual_} \\\\ \\hline Bible Corpus Mayer and Cysouw (2014) & 5.2G & - & 833 & - & May-2014 \\\\ MultiUN Ziemski et al. (2016) & - & 0.3B & 7 & - & Dec-2014 \\\\ IIT Bombay Kunchukuttan et al. (2018) & - & 0.04B & 2 & - & Dec-2021 \\\\ \\hline \\multicolumn{5}{c}{_Web Crawling_} \\\\ \\hline CC-100 Conneau et al. (2020) & - & 208B & 116 & CommonCrawl & Oct-2022 \\\\ mC4 Xue et al. (2021) & 38.5T & 6.3T & 101 & CommonCrawl & Oct-2022 \\\\ Redpajamav2 Computer (2023) & 30.4T & - & 5 & CommonCrawl & Dec-2023 \\\\ OSCAR Suarez et al. (2019) & 6.3T & 800B & 166 & CommonCrawl & Jan-2023 \\\\ Oromo Ogueji et al. (2021) & 0.939G & 0.1B & 11 & CommonCrawl & Feb-2022 \\\\ Wu Dao 2.0 & - & 24B & 2 & CommonCrawl & Oct-2023 \\\\ Europarl Koehn (2005) & 1.5G & 0.6B & 21 & - & May-2012 \\\\ JW300 Agic and Vulic (2019) & - & 1.5B & 343 & - & Jul-2019 \\\\ Glot500 ImaniGogohari et al. (2023) & 600G & - & 511 & - & May-2023 \\\\ Wikipedia Foundation & - & 24B & 300 & Wikipedia & - \\\\ WikiMatrix Schwenk et al. (2021) & 65G & - & 85 & Wikipedia & Apr-2021 \\\\ OPUS-100 Zhang et al. (2020) & 2.6G & - & 100 & OPUS & Jul-2020 \\\\ AfricanNews Adelani et al. (2022) & 12.3G & - & 16 & mC4 & Sept-2023 \\\\ Taxi1500 Ma et al. (2023) & - & - & 1500 & Bible Corpus & May-2023 \\\\ CulturaX Nguyen et al. (2023)c & 27T & 6.3T & 167 & mC4, OSCAR & Jan-2024 \\\\ \\hline \\multicolumn{5}{c}{_Benchmark Adaptation_} \\\\ \\hline ROOTs Laurenson et al. (2022) & 1.6T & - & 46 & Huggingface & Jun-2022 \\\\ OPUS Tiedemann (2012) & - & 40B & 1304 & - & Dec-2021 \\\\ CCMT Yang et al. (2019) & - & - & 6 & - & - \\\\ WMT Kocmi et al. (2023) & - & - & 32 & - & - \\\\ IWSLT Agarwal et al. (2023) & 4.2G & - & 10 & - & - \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Pre-training Data Resource, where \\(\\ast\\) indicates different categories. The term “Source” refers to the origin datasets from which the pre-training data is derived. ically, Hardalov et al. (2020); Xuan-Quy et al. (2023); Zhang et al. (2023) proposed for the comprehensive knowledge test at the high school level in a multilingual scenario. Zhang et al. (2023) design a complex translation strategy to translate existing benchmark for multilingual evaluation. On this basis, M3Exam Zhang et al. (2023) further expands comprehensive testing to multilanguage and multi-modal scenarios. Furthermore, Gekhman et al. (2023) tested the factual consistency of MLLM. And Choudhury et al. (2023); Joseph et al. (2023); Zhao et al. (2023); Wei et al. (2023); Goenaga et al. (2023); Datta et al. (2023); Thulke et al. (2024) proposed benchmarks to evaluate the multilingual scientific and applied professional domain knowledge for current MLLMs."
    },
    {
      "title": "A.2.2 Natural Language Generation",
      "text": "TranslationIn the process of multi-lingual alignment, in addition to testing whether the multiple languages are aligned in terms of understanding capabilities, researchers often also need to consider whether the two can be aligned in terms of output capabilities. The most typical task is machine translation Dabre et al. (2020); Vilar et al. (2022), currently commonly used data sets are: FLORES-101 Goyal et al. (2022), FLORES-200 Costa-jussa et al. (2022), WMT Kocmi et al. (2023) and DiaBLa Bawden et al. (2021). Recently, Lou et al. (2023) proposed CCEval for Chinese-centric translation for comprehensive evaluation on MLLMs. Furthermore, due to the large gap between languages Zhang et al. (2023); Choudhury et al. (2023); Mujadia et al. (2023); Fujii et al. (2023); Khatri et al. (2023); Etxaniz et al. (2023); Artetxe et al. (2023); Stefanovitch and Piskorski (2023); Ramesh et al. (2023); Deng et al. (2022); Held et al. (2023); Bang et al. (2023); Lai et al. (2023); Philippy et al. (2023); Ye et al. (2023), Kuparinen et al. (2023); Wassie (2023); Liu et al. (2023); Rakhimova et al. (2024) focused more on low-resource language translation. Additionally, Yang et al. (2023); Gueuwou et al. (2023); Bellagente et al. (2023); Zhong et al. (2023); Tuo et al. (2023) further extend the translation and restatement tasks into multi-modal settings \\begin{table} \\begin{tabular}{l c c c c} \\hline \\hline Dataset & Sample Size & Multi-lingual Instruction & Language Size & Task Size \\\\ \\hline \\multicolumn{5}{c}{_Manual_} \\\\ \\hline Sup-NatInst Wang et al. (2022) & - & - & 55 & 1616 \\\\ OpenAssist Köpf et al. (2023) & - & - & 35 & - \\\\ EcomInstruct Li et al. (2023) & 2.5M & Yes & 2 & 12 \\\\ COIG-PC-lite Team (2023) & 650k & No & 2 & 3,250 \\\\ \\hline \\multicolumn{5}{c}{_Benchmark Adaption_} \\\\ \\hline xP3 Muennighoff et al. (2022) & - & No & 71 & 46 \\\\ BUFFET Asai et al. (2023) & - & - & 54 & 15 \\\\ PolyglotPrompt Fu et al. (2022) & - & No & 49 & 6 \\\\ \\hline \\multicolumn{5}{c}{_Translation_} \\\\ \\hline xP3-MT Muennighoff et al. (2022) & - & Yes & 46 & 71 \\\\ MultilingualSIFT Chen et al. (2023) & - & Yes & 11 & - \\\\ Bactrian-X Li et al. (2023) & - & Yes & 52 & - \\\\ MulT Zhu et al. (2023) & - & Yes & 6 & - \\\\ CrossAlpaca Ranaldi et al. (2023) & - & - & 6 & - \\\\ MGSM8KInstruct Chen et al. (2023) & 73.6k & Yes & 6 & 10 \\\\ XCoT Chai et al. (2024) & 7.4K & Yes & 10 & 2 \\\\ \\hline \\multicolumn{5}{c}{_MLLM Aided_} \\\\ \\hline ShareGPT ShareGPT (2023) & - & - & - & - \\\\ Vicuna Chiang et al. (2023) & - & - & - & - \\\\ OverMiss Chen et al. (2023) & 54K & - & 3 & 1 (Translation) \\\\ MultiAlpaca Wei et al. (2023) & 133K & - & 11 & - \\\\ Guanaco Dettmers et al. (2023) & 535K & - & 5 & - \\\\ Alpaca-4 Peng et al. (2023) & 52K & - & 2 & - \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Supervised Fine-Tuning Data Resource, where * indicates different categories. The term “Multi-lingual Instruction” denotes the presence of instructions in multiple languages to form the specific data input. for practical scenario. ReasoningCurrently, the most commonly used reasoning ability assessments of MLLMs tend to focus on commonsense and mathematical reasoning (Huang et al., 2023; Qin et al., 2023). Specifically, commonsense reasoning includes XCOPA (Ponti et al., 2020), MARC (Keung et al., 2020), XWinograd (Tikhonov and Ryabinin, 2021), GEOMLAMA (Yin et al., 2022), X-CSQA (Lin et al., 2021), XStoryCloze (Lin et al., 2022), ASPEN (Razumovskaia et al., 2022) and Masakhanews (Adelani et al., 2023). Additionally, mathematical reasoning includes MSSM (Shi et al., 2022) and WizardMath (Luo et al., 2023). Due to the expensive annotations for multilingual reasoning, Zhang et al. (2023) propose a complex translation and filter process to construct a multilingual reasoning benchmark. Coding GenerationCoding generation requires that MLLMs can generate structured, executable code programs. The common-used benchmarks include XSPIDER (Shi et al., 2022), XSEM-PLR (Zhang et al., 2023), ODEX (Wang et al., 2022) and Mconala (Wang et al., 2022). SummarizationTo test the summarization ability of the model, the model is required to be able to summarize key information based on long texts. The simplest one, Ryan et al. (2023) proposed a multi-lingual text reduction benchmark for the evaluation of MLLM. Secondly, a lot of work focuses on cross-lingual summarization. Typical data sets include: XSUM (Narayan et al., 2018), and Cross-Sum (Bhattacharjee et al., 2021). On this basis, Wang et al. (2022) introduced multilingual conversation summarization, and Zhang and Eickhoff (2023) proposed the concept of code-switch in the evaluation, making it more practical. Urlana et al. (2023) further proposed headline summarization for languages in India. SEAHORSE (Clark et al., 2023) further extended them to the multifaceted multilingual summarization. In addition Nguyen et al. (2023); Verma et al. (2023) developed summarization benchmarks for multi-modal scenarios. DialogueThe communication between models and humans is often interactive, so a lot of work pays attention to MLLMs' dialogue ability (Boughorbel and Hawasly, 2023). The current evaluation set includes xDial-Eval (Zhang et al., 2023), Multi\\({}^{3}\\)WOZ (Hu et al., 2023), DIALIGHT (Hu et al., 2024), HPD (Chen et al., 2023) and X-RiSAWOZ (Moradshahi et al., 2023). Since multiple rounds of dialogue are not controllable, traditional indicators cannot be used. Currently, we tend to use PLM for evaluation (Mendonca et al., 2023). Furthermore, Mendonca et al. (2023) proposed a new benchmark, which can achieve more robust evaluation by coordinating with pretrained language models. Ferron et al. (2023) proposed the MEEP benchmark to further evaluate the dialogue participation of MLLMs."
    }
  ]
}