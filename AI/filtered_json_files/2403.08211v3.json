{
  "title": "Large Language Models are Contrastive Reasoners",
  "authors": [
    "Liang Yao"
  ],
  "abstract": "\n Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding \"Let's give a correct and a wrong answer.\" before LLMs provide answers. Experiments on various large language models show that zero-shot contrastive prompting improves the performance of standard zero-shot prompting on a range of arithmetic, commonsense, and symbolic reasoning tasks without any handcrafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comparable results when compared to state-of-the-art methods. Our code is available at the following GitHub repository:  https://github.com/yao8839836/cp . \n",
  "references": [
    {
      "id": null,
      "title": "Large Language Models are Contrastive Reasoners",
      "authors": [
        "Liang Yao"
      ],
      "year": "2025",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Prompt design and engineering: Introduction and advanced methods",
      "authors": [
        "Xavier Amatriain"
      ],
      "year": "2024",
      "venue": "Prompt design and engineering: Introduction and advanced methods",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Qwen technical report",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang",
        "Binyuan Hui",
        "Luo Ji",
        "Mei Li",
        "Junyang Lin",
        "Runji Lin",
        "Dayiheng Liu",
        "Gao Liu",
        "Chengqiang Lu",
        "Keming Lu",
        "Jianxin Ma",
        "Rui Men",
        "Xingzhang Ren",
        "Xuancheng Ren",
        "Chuanqi Tan",
        "Sinan Tan",
        "Jianhong Tu",
        "Peng Wang",
        "Shijie Wang",
        "Wei Wang",
        "Shengguang Wu",
        "Benfeng Xu",
        "Jin Xu",
        "An Yang",
        "Hao Yang",
        "Jian Yang",
        "Shusheng Yang",
        "Yang Yao",
        "Bowen Yu",
        "Hongyi Yuan",
        "Zheng Yuan",
        "Jianwei Zhang",
        "Xingxuan Zhang",
        "Yichang Zhang",
        "Zhenru Zhang",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
      "authors": [
        "Wenhu Chen",
        "Xueguang Ma",
        "Xinyi Wang",
        "William W Cohen"
      ],
      "year": "2022",
      "venue": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Contrastive chain-of-thought prompting",
      "authors": [
        "Ken Yew",
        "Guizhen Chia",
        "Chen",
        "Anh Luu",
        "Soujanya Tuan",
        "Lidong Poria",
        "Bing"
      ],
      "year": "2023",
      "venue": "Contrastive chain-of-thought prompting",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Pal: Program-aided language models",
      "authors": [
        "Luyu Gao",
        "Aman Madaan",
        "Shuyan Zhou",
        "Uri Alon",
        "Pengfei Liu",
        "Yiming Yang",
        "Jamie Callan",
        "Graham Neubig"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "A survey on contrastive selfsupervised learning",
      "authors": [
        "Ashish Jaiswal",
        "Ramesh Ashwin",
        "Mohammad Zaki Babu",
        "Debapriya Zadeh",
        "Fillia Banerjee",
        "Makedon"
      ],
      "year": "2020",
      "venue": "Technologies",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Large language models are zero-shot reasoners",
      "authors": [
        "Geunwoo Kim",
        "Pierre Baldi",
        "Stephen Mcaleer"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu",
        "Zhengbao Jiang",
        "Hiroaki Hayashi",
        "Graham Neubig"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Self-refine: Iterative refinement with self-feedback",
      "authors": [
        "Aman Madaan",
        "Niket Tandon",
        "Prakhar Gupta",
        "Skyler Hallinan",
        "Luyu Gao",
        "Sarah Wiegreffe",
        "Uri Alon",
        "Nouha Dziri",
        "Shrimai Prabhumoye",
        "Yiming Yang"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "authors": [
        "Rafael Rafailov",
        "Archit Sharma",
        "Eric Mitchell",
        "Christopher D Manning",
        "Stefano Ermon",
        "Chelsea Finn"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Getting it wrong: Surprising tips on how to learn",
      "authors": [
        "L Henry",
        "Bridgid Roediger",
        "Finn"
      ],
      "year": "2009",
      "venue": "Scientific American",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "A systematic survey of prompt engineering in large language models: Techniques and applications",
      "authors": [
        "Pranab Sahoo",
        "Ayush Kumar Singh",
        "Sriparna Saha",
        "Vinija Jain",
        "Samrat Mondal",
        "Aman Chadha"
      ],
      "year": "2024",
      "venue": "A systematic survey of prompt engineering in large language models: Techniques and applications",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Reflexion: Language agents with verbal reinforcement learning",
      "authors": [
        "Noah Shinn",
        "Federico Cassano",
        "Ashwin Gopinath",
        "Karthik Narasimhan",
        "Shunyu Yao"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "year": "",
      "venue": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Self-consistency improves chain of thought reasoning in language models",
      "authors": [
        "Xuezhi Wang",
        "Jason Wei",
        "Dale Schuurmans",
        "V Quoc",
        "Ed H Le",
        "Sharan Chi",
        "Aakanksha Narang",
        "Denny Chowdhery",
        "Zhou"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Vincent Zhao",
        "Kelvin Guu",
        "Adams Wei Yu",
        "Brian Lester",
        "Nan Du",
        "Andrew M Dai",
        "Quoc V Le"
      ],
      "year": "",
      "venue": "Finetuned language models are zero-shot learners",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "2022b. Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "V Quoc",
        "Denny Le",
        "Zhou"
      ],
      "year": "",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "2023a. Tree of thoughts: Deliberate problem solving with large language models",
      "authors": [
        "Shunyu Yao",
        "Dian Yu",
        "Jeffrey Zhao",
        "Izhak Shafran",
        "Tom Griffiths",
        "Yuan Cao",
        "Karthik Narasimhan"
      ],
      "year": "",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "2023b. Beyond chain-of-thought, effective graph-of-thought reasoning in large language models",
      "authors": [
        "Yao Yao",
        "Zuchao Li",
        "Hai Zhao"
      ],
      "year": "",
      "venue": "2023b. Beyond chain-of-thought, effective graph-of-thought reasoning in large language models",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Large language models as analogical reasoners",
      "authors": [
        "Michihiro Yasunaga",
        "Xinyun Chen",
        "Yujia Li",
        "Panupong Pasupat",
        "Jure Leskovec",
        "Percy Liang",
        "Ed H Chi",
        "Denny Zhou"
      ],
      "year": "2023",
      "venue": "Large language models as analogical reasoners",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Niket Tandon, and Uri Alon. 2024",
      "authors": [
        "Tianjun Zhang",
        "Aman Madaan",
        "Luyu Gao",
        "Steven Zheng",
        "Swaroop Mishra",
        "Yiming Yang"
      ],
      "year": "",
      "venue": "-context principle learning from mistakes",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Automatic chain of thought prompting in large language models",
      "authors": [
        "Zhuosheng Zhang",
        "Aston Zhang",
        "Mu Li",
        "Alex Smola"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "A survey of large language models",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Large Language Models Are Contrastive Reasoners",
      "text": "Liang Yao Sun Yat-sen University Shenzhen, China yaoliang3@mail.sysu.edu.cn"
    },
    {
      "title": "Abstract",
      "text": "Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding \"Let's give a correct and a wrong answer.\" before LLMs provide answers. Experiments on various large language models show that zero-shot contrastive prompting improves the performance of standard zero-shot prompting on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from \\(35.9\\%\\) to \\(88.8\\%\\) and AQUA-RAT from \\(41.3\\%\\) to \\(62.2\\%\\) with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comparable results when compared to state-of-the-art methods. Our code is available at the following GitHub repository: [https://github.com/yao8839836/cp](https://github.com/yao8839836/cp)."
    },
    {
      "title": "1 Introduction",
      "text": "Recent studies Zhao et al. (2023); Brown et al. (2020); OpenAI (2023) have shown that large language models (LLMs) exhibit impressive performance across a wide range of tasks. In particular, the chain-of-thought (CoT) prompting technique has demonstrated the capability of LLMs to handle complex tasks, including math problem solving, by guiding them to generate intermediate reasoning steps Wei et al. (2022); Kojima et al. (2022); Zhang et al. (2023). These studies spotlight the significance of developing efficient techniques to direct LLMs in their reasoning processes Liu et al. (2023); Amatriain (2024); Chia et al. (2023); Yasunaga et al. (2023). Nevertheless, the current chain-of-thought (CoT) paradigm encounters two main challenges: offering _correct_ guidance or examples of reasoning and reducing the reliance on manual labeling. In particular, Zero-shot-CoT Kojima et al. (2022) provides general reasoning guidance by providing instructions like \"Think step by step.\", but the generated reasoning steps may not be correct and adequate for tasks such as commonsense question-answering (Table 1 and 6). On the other hand, Few-shot-CoT Wei et al. (2022) offers more detailed guidance but necessitates labeled examples of the reasoning process, which can be expensive to obtain for each task. This raises an important research question: Is it possible to generate a more accurate reasoning process without relying on human labeling? In this work, we introduce **contrastive prompting**, a novel prompting approach that automatically directs the reasoning process of large language models. Our inspiration stems from how humans can learn from both their correct and incorrect actions Roediger and Finn (2009). For instance, when confronted with a math problem (as in Figure 1), people may ask \"How can we prevent mistakes in each step?\" By identifying the steps that are prone to mistakes on their own, they can enhance their ability to avoid mistakes and provide accurate solutions. Our idea is to prompt LLMs to emulate this reasoning process, enabling them to effectively solve new problems. Specifically, when presented with a problem to solve, we instruct LLMs to generate both correct and incorrect answers within the given context. To achieve this, we provide prompts such as \"Let's give a correct and a wrong answer.\" Following this, we verify and confirm the correct answer. Our proposed approach offers multiple advantages. It not only generates incorrect answers autonomously but also places a greater emphasis on ensuring the accuracy of the answers. This eliminates the need formanually labeling reasoning examples for each task and problem, effectively addressing the challenges faced by CoT. We evaluate the proposed approach across various reasoning-intensive tasks, including arithmetic reasoning, commonsense reasoning, symbolic reasoning, and other logical reasoning tasks. We employ two state-of-the-art base LLMs GPT-3.5 and GPT-4 (OpenAI, 2023) and four popular open source LLMs. The experimental findings demonstrate significant improvements in scores compared to the zero-shot baseline across all datasets. Moreover, our method not only surpasses Zero-shot-CoT and Few-shot-CoT in most arithmetic and commonsense reasoning tasks but also achieves better results when combined with zero-shot or few-shot CoT, approaching or even surpassing the performance of existing state-of-the-art methods. These results indicate the effectiveness of generating incorrect answers for individual problems to guide the reasoning process of LLMs."
    },
    {
      "title": "2 Related Works",
      "text": "Large language models and promptingRecently, LLMs (Zhao et al., 2023) like ChatGPT and GPT-4 (OpenAI, 2023) have gained significant attention. Researchers find that scaling pre-trained language models often leads to an improved model capacity on downstream tasks. These large-sized models show different behaviors from smaller models and display surprising abilities in solving a series of complex tasks. Prompt engineering is an emerging field dedicated to the development and optimization of prompts, enabling efficient utilization of LLMs across diverse applications and research domains (Amatriain, 2024; Sahoo et al., 2024). Zero-shot prompting involves querying the LLM without any examples while few-shot prompting provides models with a few input-output examples (Brown et al., 2020). Chain-of-thought (CoT) (Wei et al., 2022; Kojima et al., 2022) prompting enables complex reasoning capabilities through intermediate reasoning steps. Despite its success, Few-shot-CoT (Wei et al., 2022) needs human-labeled reasoning steps for each example, while Zero-shot-CoT (Kojima et al., 2022) may generate incorrect reasoning steps (especially for commonsense and arithmetic reasoning). Several X-of-thought approaches (Yao et al., 2023; Gao et al., 2023; Chen et al., 2022) extend CoT on reasoning tasks, where X can be a tree, a graph, or a program. AutoCoT (Zhang et al., 2023) improves Zero-shot-CoT by providing similar questions as few-shot examples for the target question. Self-consistency (Wang et al., 2023) sample multiple, diverse reasoning paths through Few-shot-CoT, and use the generations to select the most consistent answer. Analogical prompting (Yasunaga et al., 2023) leverages LLMs to automatically generate relevant few-shot examples for each question. In contrast to these works, our method emphasizes eliciting self-awareness in LLMs regarding potential errors and actively avoiding them. Learning from Negative ExamplesContrastive learning, a widely adopted technique in deep learn Figure 1: Example inputs and outputs of GPT-4 with (a) standard Zero-shot, and (b) ours (Zero-shot-CP). In contrast to Few-shot-CoT, which requires step-by-step reasoning examples for each task, our approach does not rely on any examples. Instead, we use the same prompt “Let’s give a correct and a wrong answer” for all tasks, including arithmetic, symbolic, commonsense, and other logical reasoning tasks. ing, aims to enhance the quality of learned representations by training models to differentiate between \"positive\" and \"negative\" samples (Jaiswal et al., 2020). In the LLMs area, reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) and direct preference optimization (DPO) (Rafailov et al., 2023) fine-tune LLMs with relative human judgments of response quality. Self-reflection (Shinn et al., 2023; Kim et al., 2023; Madaan et al., 2023; Zhang et al., 2024) incorporates \"critic\" or review steps to identify errors made by the LLM itself and improve upon them. However, it is important to note that the initial output of the LLM may not contain any errors, and there is a potential risk of the model reinforcing its own errors if it inaccurately evaluates the quality of its responses or generates invalid principles. The closest work to ours is the Contrastive CoT (Chia et al., 2023) that extends Few-shot-CoT by creating wrong reasoning processes from annotated correct reasoning steps. The main distinction is that the erroneous answers generated by Contrastive CoT still require human-annotated examples, and the random reordering of entities during the reasoning process may not align with the patterns of errors made by LLMs themselves. On the contrary, our approach enables LLMs to generate erroneous answers on their own, which aligns better with their intrinsic knowledge. It does not require human annotation."
    },
    {
      "title": "3 Contrastive Prompting",
      "text": "We propose Contrastive Prompting (CP), a template-based prompting approach for contrastive reasoning. Our method can seamlessly integrate with any prompting technique by incorporating a trigger sentence before the LLM provides answers. In the following, we first illustrate our method using Zero-shot-CP as an example, which only uses the original question without supporting examples. Next, we will discuss how to combine our method with other prompting techniques."
    },
    {
      "title": "Two-Stage Prompting",
      "text": "Although Zero-shot-CP is straightforward in concept, it utilizes prompting twice to extract both reasoning and answer, as illustrated in Figure 2. 1st prompt: reasoning extractionIn this step we begin by transforming the input question \\(\\mathbf{x}\\) into a prompt \\(\\mathbf{x}^{\\prime}\\) using a simple template \"Q: [X]. A: [T]\". Here [X] represents the input slot for \\(\\mathbf{x}\\) and [T] represents a slot for a manually crafted trigger sentence \\(\\mathbf{t}\\) that would extract the reasoning process to answer the question \\(\\mathbf{x}\\). For instance, if we use \"Let's give a correct and a wrong answer.\" as a trigger sentence, the prompt \\(\\mathbf{x}^{\\prime}\\) would be \"Q: [X]. A: Let's give a correct and a wrong answer.\". Additional trigger examples can be found in Table 3. Prompted text \\(\\mathbf{x}^{\\prime}\\) is then inputted into a LLM, which generates the subsequent sentence \\(\\mathbf{z}\\). Figure 2: The complete process of Zero-shot-CP involves two steps: Firstly, we utilize the initial ”reasoning” prompt to extract a comprehensive reasoning process from a LLM. Secondly, we employ the subsequent ”answer” prompt to extract the correct answer from the reasoning text. 2nd prompt: answer extractionIn the second step, we utilize the generated sentence \\(\\mathbf{z}\\) in conjunction with the prompted sentence \\(\\mathbf{x}^{\\prime}\\) to extract the ultimate answer from the LLM. To provide a more specific explanation, we combine three elements by concatenating them as \"[X'] [Z] [A]\". Here, [X'] represents the 1st prompt \\(\\mathbf{x}^{\\prime}\\), [Z] represents the sentence \\(\\mathbf{z}\\) generated in the first step, and [A] represents a trigger sentence used to extract the answer. The prompt for this step is self-augmented, meaning that it includes the sentence \\(\\mathbf{z}\\) generated by the same LLM. During the experiment, we employed slightly different answer triggers based on the format of the answer. Please refer to Appendix A.2 for the answer trigger sentences we used in each task. Subsequently, the prompted text is inputted into the LLM to generate sentences \\(\\mathbf{y}\\) and extract the final answer."
    },
    {
      "title": "Integrating With Other Prompting Methods",
      "text": "We can easily integrate our CP with any advanced prompting methods. We name the combined method X-CP, where X can be Zero-shot-CoT, Few-shot-CoT, or any other method. X-CP also has two steps: reasoning extraction and answer extraction. For Zero-shot-CoT-CP, the only distinction is we replace the trigger sentence \"Let's give a correct and a wrong answer.\" with \"Let's think step by step and give both a correct answer and a wrong answer.\". For Few-shot-CoT-CP, the distinction is that \\(k\\) few-shot examples with reasoning steps are added before \"Q: [X]. A: Let's give a correct and a wrong answer.\", the resulting prompt \\(\\mathbf{x}^{\\prime}\\) will be \"Q: [\\(X_{1}\\)] A: [\\(Z_{1}\\)]. The answer is [\\(Y_{1}\\)]. Q: [\\(X_{2}\\)] A: [\\(Z_{2}\\)]. The answer is [\\(Y_{2}\\)].... Q: [\\(X_{k}\\)] A: [\\(Z_{k}\\)]. The answer is [\\(Y_{k}\\)]. Q: [X]. A: Let's give a correct and a wrong answer.\", where \\(X_{i}\\), \\(Z_{i}\\) and \\(Y_{i}\\) are the question, reasoning steps and the final answer for each example \\(i\\)."
    },
    {
      "title": "4 Experiment",
      "text": ""
    },
    {
      "title": "Settings",
      "text": "DatasetsWe evaluate the effectiveness of our proposal on 12 datasets1 encompassing four categories of reasoning tasks: arithmetic (SingleEq, AddSub, MultiArith, AQUA-RAT, GSM8K, SVAMP), commonsense (CommonsenseQA, StrategyQA), symbolic (Last Letter Concatenation, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects). The detailed description of each dataset can be found in (Kojima et al., 2022). We use the few-shot examples with reasoning steps provided by (Wei et al., 2022). Footnote 1: The datasets are available at [https://github.com/kojima-takeshi188/zero_shot_cot/tree/main/dataset](https://github.com/kojima-takeshi188/zero_shot_cot/tree/main/dataset). BaselinesWe conducted a comprehensive comparison of our CP method with various types of prompting techniques. These include simple zero-shot methods such as Zero-shot and Zero-shot-CoT (Kojima et al., 2022), Few-shot and Few-shot-CoT (Wei et al., 2022), X-of-thought approaches like Tree of Thoughts (ToT) (Yao et al., 2023), Graph of Thoughts (GoT) (Yao et al., 2023), Program-aided Language models (PAL) (Gao et al., 2023), and Program of thoughts prompting (PoT) (Chen et al., 2022). Additionally, we compared our method with other prompting techniques such as Analogical prompting (Self-generated Exemplars) (Yasunaga et al., 2023) and Self-consistency (SC) (Wang et al., 2023). Furthermore, we evaluated the effectiveness of self-reflection methods, including Recursive Criticism and Improvement (RCI) (Kim et al., 2023), Self-Refine (Madaan et al., 2023) and Learning Principles from Mistakes (LEAP) (Zhang et al., 2024), as well as the closest related work, Contrastive CoT (Chia et al., 2023). We also experimented with running CP using Self-consistency (SC). Specifically, we set the temperature parameter of LLMs to 0.7 and sampled 10 correct and incorrect answers. Then, we selected the answer that appeared most frequently among the 10 correct answers as the final answer. ModelsWe use GPT-4 and GPT-3.5-Turbo (0613) as our base models (accessed between Feb 22nd-May 22nd 2024) for main experiments. We also tested our CP on various open LLMs: LLaMA3-8B, LLaMA3-70B (Touvron et al., 2023), ChatGLM3-6B (Du et al., 2022) and Qwen1.5-72B-Chat (Bai et al., 2023). All generations (except experiments with Self-consistency) are done by greedy decoding (i.e., sampling with zero temperature) as in the original CoT work (Wei et al., 2022). For GPT models, we use Azure OpenAI services. For open LLMs except ChatGLM3-6B, we use LlamaAPI 2. For ChatGLM3-6B, we downloaded the model and performed the inference on a Linux server with an A100 GPU. Answer filteringWe follow Zero-shot-CoT (Kojima et al., 2022) work and use its original implementation to pick up the final answers."
    },
    {
      "title": "Results",
      "text": "Zero-shot ResultsTable 1 presents the accuracy scores achieved by our Zero-shot-CP, standard zero-shot prompting (Zero-shot) and Zero-shot-CoT across five datasets. We ran all methods five times using GPT-3.5-Turbo and report the average scores. We found that the differences in each run were minimal. Zero-shot-CP consistently outperformed Zero-shot-CoT and Zero-shot across most (4 out of 5) datasets. Table 5 in Appendix B presents more comprehensive results. Notably, Zero-shot-CP demonstrates significant improvements over Zero-shot on all 12 datasets across various tasks using GPT-3.5-Turbo. For instance, Zero-shot-CP achieves score gains ranging from \\(14.3\\%\\) to \\(73.2\\%\\) on GSM8K, from \\(61.2\\%\\) to \\(95.2\\%\\) on MultiArith and from \\(4.2\\%\\) to \\(41.8\\%\\) on Last Letter Concatenation. Moreover, Zero-shot-CP outperforms Zero-shot on the majority (9 out of 12) of datasets when using GPT-4, with improvements ranging from \\(35.9\\%\\) to \\(88.8\\%\\) on GSM8K and from \\(41.3\\%\\) to \\(62.2\\%\\) on AQUA-RAT. These results indicate that eliciting self-awareness in LLMs to compare incorrect and correct answers can help prevent incorrect responses. Zero-shot-CP outperforms Zero-shot-CoT in the majority (4 out of 6) of arithmetic reasoning tasks, suggesting that the self-awareness of LLMs regarding incorrect answers may be more crucial than their self-awareness regarding steps in mathematical reasoning. Furthermore, in commonsense reasoning tasks, Zero-shot-CP consistently outperforms Zero-shot (2 out of 2), while Zero-shot-CoT exhibits inferior results. This is likely because commonsense reasoning tasks require fewer steps, making awareness of individual pieces of commonsense knowledge more crucial. However, Zero-shot-CP performs worse than Zero-shot-CoT in symbolic reasoning and other reasoning tasks, indicating that for tasks like Coin Flip and Shuffled Objects, the steps involved are more crucial. Moreover, breaking down the reasoning process into individual steps makes it less likely for LLMs to make mistakes in these tasks, as the action space is very limited. Nevertheless, by combining Zero-shot-CP and Zero-shot-CoT, Zero-shot-CoT-CP achieves better results in most tasks (see Table 4). Table 8 in Appendix B presents the results of using various open-source LLMs: LLaMA3-8B, LLaMA3-70B, ChatGLM3-6B, and Qwen1.5-72B-Chat as base models. The results demonstrate that Zero-shot-CP not only performs well with state-of-the-art GPT models, but also exhibits significant improvements across multiple sizes of open-source models. Qualitative AnalysisTable 2 and Table 6 in Appendix B illustrates examples from SVAMP and CommonsenseQA. For the example from the arithmetic reasoning task SVAMP, we found that the reasoning process of zero-shot is correct, but it produces an incorrect answer \"-4\". Zero-shot-CoT is disrupted by irrelevant information, resulting in incorrect reasoning processes and answers being generated. Zero-shot-CP, on the other hand, is not disrupted and provides both the correct answer and explanation. We can see that the \"wrong answer\" \"-4\" from Zero-shot-CP is a real mistake made by Zero-shot. For the example from the common sense reasoning task CommonsenseQA, contrastive prompting is able to recognize the word \"work\" in the question and provide the correct answer, while Zero-shot and Zero-shot-CoT cannot. In Appendix B, we present responses generated by Zero-shot-CP for each dataset. Figure 5-16 gives both a positive example and a negative example of Zero-shot-CP on each dataset. From positive examples, we found that Zero-shot-CP can generate \"wrong\" answers that are indeed incorrect in most cases (11/12), except for Tracking Shuffled Object (Figure 13). Incorrect answers are generated by intentionally calculating inaccurately (Figure 12), disregarding important details (Figure 10), searching for descriptions that are not present in the question (Figure 9), or deliberately providing explanations that contradict common sense (Figure 15). From negative examples, We found that the \"wrong answers\" provided by Zero-shot-CP can actually be valid answers (Figure 6, 7, 8, 12, 14 and 15). In some other negative examples, both the \"correct answers\" and \"incorrect answers\" provided by Zero-shot-CP are inconsistent with the ground truth (Figure 5, 9, 10, 11 and 16). From the figures, we found that Zero-shot-CP also outputs reasoning steps in the process of generating correct and incorrect answers, especially for arithmetic reasoning tasks. Furthermore, we manually annotated 10 solved problems and 10 unsolved problems of Zero-shot-CP for each of the 12 datasets. Table 10 provides the categorization and counts of these 120solved problems and 120 unsolved problems. We found that for the solved problems, the majority (112/120) of the given \"wrong\" answers were indeed incorrect. For the unsolved problems, the majority (91/120) of both the \"correct\" and \"wrong\" answers were incorrect, with a portion (23/120) of the \"wrong\" answers actually being the ground truth. This situation typically occurs in yes or no questions. **The impact of prompt selection on Zero-shot-CP** We explore different contrastive prompts and their combination with Zero-shot-CoT. Table 3 outlines performance using 9 different templates with two classes. The first category is related to correct and wrong answers. We found \"Let's give a correct and a wrong answer.\" achieves the best results in general. \"Let's first give a wrong answer, then give the correct answer.\" performs well on AQUA-RAT but it performs worse on other datasets. \"Let's first give the correct answer, then give a wrong answer.\" generally performs well on the four datasets, meaning that providing the correct answer first and then the incorrect answer generally leads to better results. The trigger word \"incorrect\" performs similarly to \"wrong\", and the trigger word \"Please\" performs much worse than \"Let's\". This is likely because, in the pre-training and fine-tuning data, there are slightly fewer occurrences of \"incorrect\" compared to \"wrong\" in samples related to correct and incorrect answers, and \"Please\" is rarely present as this type of data is generally not dialogue data. \"Let's give a correct answer.\" performs well on the multiple-choice question dataset AQUA-RAT, but the performances on other three mathematical reasoning tasks are not satisfactory. This indicates that, for multiple-choice questions, only providing a correct answer is equivalent to eliminating several incorrect answers. However, for questions without options, outputting an incorrect answer is helpful. Table 9 in Appendix B gives more comparative results between \"Let's give a correct and a wrong answer.\" and \"Let's give a correct answer.\" We find that, except for multiple-choice reasoning tasks, providing a wrong answer is more effective than only giving the correct answer. We also printed the token output probabilities for different prompts. As shown in Figure 4 in Appendix B, we find that adding prompts to generate incorrect answers changes the output probability distribution, Zero-shot-CP makes GPT-4 more confident in the ground truth answer. \\begin{table} \\begin{tabular}{l c c c c c c} \\hline \\hline & MultiArith & GSM8K & StrategyQA & AddSub & SVAMP & CommonsenseQA \\\\ \\hline Zero-shot & 60.97 & 14.39 & 65.02 & 82.78 & 69.74 & 71.33 \\\\ Zero-shot-CoT & 94.87 & **75.56** & 60.74 & 86.16 & 81.78 & 68.96 \\\\ Zero-shot-CP & **95.13** & 73.22 & **67.39** & **90.46** & **83.08** & **73.81** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Accuracy (in percentage) comparison of Zero-shot-CP with Zero-shot and Zero-shot-CoT on five datasets. We run all methods 5 times using GPT-3.5-Turbo and report average scores. Zero-shot-CP significantly outperforms baselines on StrategyQA, AddSub, SVAMP and CommonsenseQA based on student t-test (p < 0.05). \\begin{table} \\begin{tabular}{l|c} \\hline \\hline Question & Q: Danny collects bottle caps and wrappers. He found 46 wrappers and 50 bottle caps at the park. \\\\ & Now he has 21 bottle caps and 52 wrappers in his collection. \\\\ & How many more bottle caps than wrappers did many find at the park? \\\\ \\hline Zero-shot & A: The answer ( Arabic numerals) is 4- Danny found 50 bottle caps and 46 wrappers at the park, \\\\ & so he found 4 more bottle caps than wrappers. \\\\ \\hline & A: Let’s think step by step. First, we need to find out how many bottle caps and wrappers \\\\ & Danny had before he found the new ones at the park. \\\\ & We know that after finding the new ones, he has 30 bottle caps and 52 wrappers. \\\\ & Before finding the new ones, he had 21 bottle caps and 46 wrappers. \\\\ & So, the number of bottle caps be found at the park is 52 (004 now) - 21 (0480 before) = 29 bottle caps. \\\\ & The number of wrappers from d and the park is 52 (0204 now) - 46 (0481 before) = 6 wrappers. \\\\ & Therefore, Danny found 29 - 6 = 23 more bottle caps than wrappers at the park. \\\\ & Therefore, the answer ( Arabic numerals) is 23 \\\\ \\hline & A: Let’s give a correct and a wrong answer. **Correct Answer**: Danny found 50 bottle caps and 46 wrappers at the park. \\\\ & at the park. So, he found 50 - 46 = 4 more bottle caps than wrappers at the park. \\\\ & **Incorrect Answer**: Danny found 50 bottle caps and 46 wrappers at the park. \\\\ & So, he found 46 - 50 = - 4 more bottle caps than wrappers at the park. \\\\ & Therefore, the **correct** answer ( Arabic numerals) is 4 \\\\ \\hline Ground Truth & 4 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: An example question from SVAMP and responses from Zero-shot, Zero-shot-CoT, Zero-shot-CP and Zero-shot-CoT-CP. We use GPT-4 as the base model. The second type of template in Table 3 connects to Zero-shot-CoT, and we found that starting with the steps performs better than starting with the correct and wrong answers. Overall, it appears that Zero-shot-CoT-CP (\"Let's think step by step and give both a correct answer and a wrong answer.\") performs the best. The impact of number of wrong answers on Zero-shot-CPWe explored the impact of the number of incorrect answers on accuracy. We vary the number of wrong answers from \\(0\\) to \\(4\\), where \\(0\\) means standard zero-shot prompting. For \\(k=1,2,3,4\\), we use the template \"Let's give a correct and \\(k^{\\prime}\\) wrong answer(s).\", where \\(k^{\\prime}\\) can be \"a\", \"two\", \"three\" and \"four\". Figure 3 plots the results. We found that providing 1-2 incorrect answers yielded the best results in general. The only exception is on AQUA-RAT, where providing more incorrect answers resulted in higher accuracy. This is because the task involves a multiple-choice question with five options, and excluding more incorrect answers makes the LLMs more certain about the correct answer. For math reasoning tasks with an infinite number of answers, providing just one incorrect answer seems to be sufficient. Comparison with other baselinesTable 4 compares the performances on four mathematical reasoning datasets (MultiArith, GSM8K, AQUA-RAT and SVAMP) and one common sense reasoning dataset (StrategyQA) across CP and baselines. We find that Zero-shot-CP not only outperforms Few-shot, but also achieves comparable or even superior \\begin{table} \\begin{tabular}{l c c c c c} \\hline \\hline & MultiArith & GSM8K & StrategyQA & AQUA & SVAMP \\\\ \\hline Zero-shot & 61.2 & 14.3 & 65.0 & 29.9 & 69.7 \\\\ Zero-shot-CoT & 94.8 & 75.1 & 60.9 & 55.9 & 81.9 \\\\ Zero-shot-CoT + SC & 96.8 & **80.7** & 61.6 & **66.1** & 85.6 \\\\ Zero-shot-CP & 95.2 & 73.2 & 67.3 & 40.2 & 83.2 \\\\ Zero-shot-CP + SC & **98.3** & 80.3 & **67.9** & 48.4 & **87.6** \\\\ Zero-shot-CoT-CP & 96.2 & 73.5 & 66.7 & 60.6 & 85.9 \\\\ \\hline Few-shot & 87.3 & 58.2 & 56.7 & 37.4 & 78.2 \\\\ Few-shot-CoT & 98.0 & 71.1 & 62.2 & 55.5 & 81.0 \\\\ Few-shot-CoT + SC & 98.7 & 76.0 & 63.5 & 59.4 & 83.5 \\\\ Few-shot-CoT-CP & 97.5 & 72.7 & 68.7 & 52.0 & 82.2 \\\\ Few-shot-CoT (GPI-4) & 98.3 & 89.5 & **79.1** & 58.7 & 83.3 \\\\ Few-shot-CoT (GPI-4) & **98.7** & 90.3 & 78.2 & 66.9 & 91.8 \\\\ Few-shot-CoT-CP (GPI-4) + SC & 97.5 & **91.9** & 78.8 & **70.9** & **93.1** \\\\ Contrastive CoT (Chia et al., 2023) & – & 79.0 & 66.2 & 57.5 & 81.6 \\\\ \\hline Self-consistency (Code-davai-002) (Wang et al., 2023) & **100.0** & 78.0 & 79.8 & 52.0 & 86.8 \\\\ PAL (Code) (Gao et al., 2023) & 99.2 & 80.4 & – & – & 79.4 \\\\ Zero-shot-CoT (CoT) (Chen et al., 2022) & 92.2 & 57.0 & – & 43.9 & 70.8 \\\\ Few-shot-CoT (Cohen et al., 2022) & – & 71.6 & – & 54.1 & 85.2 \\\\ Few-shot-CoT-SC (Cohen et al., 2022) & – & 80.0 & – & **58.6** & **89.1** \\\\ ToT (GPT-4) (Yao et al., 2023a) & – & **90.0** & **83.0** & – & – \\\\ GoT (TS-large) (Yao et al., 2023b) & – & 82.2 & – & – & – \\\\ Self-generated Examples (Yasunaga et al., 2023) & – & 77.8 & – & – & – \\\\ Self-Refine (Madan et al., 2023) & – & 75.1 & – & – & – \\\\ LEAP (Zhang et al., 2024) & – & 77.4 & – & – & \\\\ Zero-Shot-CoT + RC1 (Kim et al., 2023) & 97.2 & 86.2 & – & – & 85.8 \\\\ Few-Shot-CoT + RC1 (Kim et al., 2023) & 99.2 & 84.3 & – & – & 87.4 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Comparison with baseline methods using accuracies (in percentage) on MultiArith, GSM8K, StrategyQA, AQUA-RAT and SVAMP. GPT-3.5-Turbo is used as the model if not specified. The baseline results with citations are obtained from corresponding papers. Bolded numbers indicate the best results within each block’s column, while underlined numbers indicate the best results across the entire column. \\begin{table} \\begin{tabular}{l c c c c} \\hline \\hline GPT-4 & AQUA & GSM8K & AddSub & MultiArith \\\\ \\hline Let’s give a correct and a wrong answer. & 62.2 & 88.8 & **91.6** & **97.8** \\\\ Let’s first give a wrong answer, then give the correct answer. & 69.3 & 86.1 & 90.9 & 95.0 \\\\ Let’s first give the correct answer, then give a wrong answer. & 58.7 & **89.7** & 91.6 & 95.0 \\\\ Let’s give a correct and an incorrect answer. & 66.5 & 88.7 & 91.6 & 97.7 \\\\ Please give a correct and a wrong answer. & 57.5 & 82.0 & 88.9 & 94.0 \\\\ Let’s give a correct answer. & **71.2** & 75.9 & 89.4 & 97.0 \\\\ \\hline Let’s think step by step and give both a correct answer and a wrong answer. & **71.3** & 89.5 & **91.4** & 97.2 \\\\ Let’s give a correct and a wrong answer. & Let’s also think & 52.8 & 88.9 & 89.4 & 96.7 \\\\ step by step for the correct and the wrong answer. & 70.1 & **90.9** & 89.6 & **97.7** \\\\ Let’s think step by step. (Zero-shot-CoT) & 70.1 & **90.9** & 89.6 & **97.7** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Comparison prompting templates using accuracies (in percentage) on AQUA-RAT, GSM8K, AddSub and MultiArith in zero-shot setting. GPT-4 is used as the model. Bolded numbers indicate the best results within each block’s column, while underlined numbers indicate the best results across the entire column. results to Few-shot-CoT. For instance, on GSM8K, the absolute accuracy has improved by \\(2.1\\%\\), and on StrategyQA, the absolute accuracy has improved by \\(5.1\\%\\). This suggests that in certain cases, the provided examples and reasoning steps may not be as effective as directly triggering the LLM's self-awareness of errors. By combining CP and Few-shot-CoT, we can achieve even better results. Furthermore, if we utilize the GPT-4 model, we can attain performance that is comparable to or even superior to the current state-of-the-art methods. For example, in AQUA, SVAMP, and GSM8K, we have achieved higher accuracy scores compared to recently published results. When running CP with Self-consistency (SC), the scores can be further improved in both zero-shot and few-shot settings. For a more in-depth performance analysis, we note that X-of-thought methods can improve the effectiveness of Few-shot-CoT, indicating that trees, graphs, and code indeed provide richer information and greater flexibility compared to simple chains of thought. Among them, the results reported by the ToT work seem to be more prominent. By sampling multiple reasoning paths and selecting the most consistent answer, Self-consistency (SC) demonstrates excellent performance in mathematical and commonsense reasoning tasks. It can also be effectively combined with other methods such as PoT. Self-generated Exemplars also show better performance than CoT, indicating that allowing the LLM to recall relevant questions and answer them before responding to the original question is helpful. The performance of Self-reflection methods, such as Self-Refine and LEAP, is similar to that of Self-generated Exemplars. RCI performs even better, primarily due to its direct combination with the CoT method. Compared to these methods, our approach is simpler and can also yield comparable results. Compared to the most relevant method Contrastive CoT, our Zero-shot-CP performs better on the StrategyQA and SVAMP datasets. Zero-shot-CoT-CP performs better on AQUA-RAT. However, on GSM8K, Contrastive CoT performs better, indicating that generating incorrect answers by swapping the order of entities is useful for this task. The main reasons why CP works well are fourfold: 1) the pre-training data of LLMs contains many correct and incorrect answers to different types of questions. For instance, many web pages and books in Appendix A.3 provide correct and incorrect answers to math reasoning and common sense reasoning questions. Answers to questions on social media platforms like Reddit, Quora, and Zhihu can be voted on by others through \"upvotes\" or \"downvotes\". Highly upvoted answers are more likely to be correct answers while others may be incorrect. Pre-training LLMs with massive text containing these correct and wrong answers can encode general patterns (token probability) of these answers in LLM parameters. When prompted with contrastive prompts, LLMs can leverage these patterns to generate both a correct and a wrong answer. The \"correct\" answer is more likely to align with ground truth, as the model has learned to eliminate possible wrong answers. 2) Instruction tuning unlocks the abilities of LLMs to give correct and incorrect answers by fine-tuning on various natural language processing tasks including reasoning tasks (Wei et al., 2022). 3) RLHF fine-tunes LLMs using human feedback data, which offers relative judgments on the quality of answers. This feedback is valuable for enhancing the LLMs' capability to distinguish between correct and incorrect answers. 4) In CP, correct and wrong answers are returned by the LLM in a single output. The correct answers are generally distinct from the incorrect ones (as shown in Figure 5-16), thereby excluding the (mostly indeed) incorrect answers and reducing the probability of the correct answers being wrong. Before outputting the two answers, the LLM engages in \"contrastive thinking\" to determine which answer is correct and which is incorrect."
    },
    {
      "title": "5 Conclusion",
      "text": "We propose CP, a template-based prompting approach for contrastive reasoning. Quantitative and qualitative results indicate that Zero-Shot-CP shows significant improvements across various reasoning tasks. Our method can seamlessly integrate with any prompting technique by incorporating a trigger sentence before the LLM provides answers. Figure 3: Accuracy scores by varying the number of wrong answers. We test GPT-4 and GPT-3.5-Turbo on (a) AQUA-RAT, (b) GSM8K, (c) AddSub and (d) MultiArith. The range of the number of wrong answers is from 0 (Zero-shot) to 4. CP not only outperforms Zero-shot-CoT and Few-shot-CoT in most arithmetic and commonsense reasoning tasks, but also achieves comparable or even superior results when compared to state-of-the-art methods."
    },
    {
      "title": "6 Limitations",
      "text": "Our work has some limitations and there is room for further exploration and improvement. Firstly, we have not yet validated the effectiveness of CP on smaller models such as Gemma-2B and Qwen1.5-0.5B. Secondly, we can further explore the combination of contrastive prompting with other prompting methods, such as X-of-thought approaches. Lastly, exploring the impact of contrastive prompting on LLM parameters and visualizing it would be an interesting future direction."
    },
    {
      "title": "References",
      "text": "* Amatriain (2024) Xavier Amatriain. 2024. Prompt design and engineering: Introduction and advanced methods. _arXiv preprint arXiv:2401.14423_. * Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. _arXiv preprint arXiv:2309.16609_. * Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901. * Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. _arXiv preprint arXiv:2211.12588_. * Chia et al. (2023) Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and Lidong Bing. 2023. Contrastive chain-of-thought prompting. _arXiv preprint arXiv:2311.09277_. * Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335. * Gao et al. (2023) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models. In _International Conference on Machine Learning_, pages 10764-10799. PMLR. * Jaiswal et al. (2020) Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. 2020. A survey on contrastive self-supervised learning. _Technologies_, 9(1):2. * Kim et al. (2023) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language models can solve computer tasks. _Advances in Neural Information Processing Systems_, 36. * Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213. * Liu et al. (2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35. * Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. _Advances in Neural Information Processing Systems_, 36. * OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. _Preprint_, arXiv:2303.08774. * Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744. * Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36. * Roediger and Finn (2009) Henry L Roediger and Bridgid Finn. 2009. Getting it wrong: Surprising tips on how to learn. _Scientific American_, pages 499-504. * Sahoo et al. (2024) Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024. A systematic survey of prompt engineering in large language models: Techniques and applications. _arXiv preprint arXiv:2402.07927_. * Sahoo et al. (2020)Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. _Advances in Neural Information Processing Systems_, 36. * Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Bapitste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_. * Wang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In _The Eleventh International Conference on Learning Representations_. * Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In _International Conference on Learning Representations_. * Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837. * Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. _Advances in Neural Information Processing Systems_, 36. * Yao et al. (2023b) Yao Yao, Zuchao Li, and Hai Zhao. 2023b. Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. _arXiv preprint arXiv:2305.16582_. * Yasunaga et al. (2023) Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H Chi, and Denny Zhou. 2023. Large language models as analogical reasoners. _arXiv preprint arXiv:2310.01714_. * Zhang et al. (2024) Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, and Uri Alon. 2024. In-context principle learning from mistakes. _arXiv preprint arXiv:2402.05403_. * Zhang et al. (2023) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic chain of thought prompting in large language models. In _The Eleventh International Conference on Learning Representations (ICLR 2023)_. * Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. _arXiv preprint arXiv:2303.18223_."
    },
    {
      "title": "Appendix A Details Of Experimental Setup",
      "text": ""
    },
    {
      "title": "Code, Prompts, Logs",
      "text": "All code is available at [https://github.com/yao8839836/cp](https://github.com/yao8839836/cp). All prompts are available at [https://github.com/yao8839836/cp/blob/master/main.py](https://github.com/yao8839836/cp/blob/master/main.py). Our experimental logs are available at [https://github.com/yao8839836/cp/tree/master/log](https://github.com/yao8839836/cp/tree/master/log)."
    },
    {
      "title": "Prompts For Answer Extraction",
      "text": "Table 7 summarizes the answer extraction prompt for each task used for the CP experiments."
    },
    {
      "title": "Pre-Training Data Examples",
      "text": "For instance, many web pages and books provide correct and incorrect answers to math reasoning345 and common sense reasoning 67 questions. Footnote 3: [https://prek-math-te.stanford.edu/operations/analyzing-thinking-underlying-wrong-answers](https://prek-math-te.stanford.edu/operations/analyzing-thinking-underlying-wrong-answers) Footnote 4: https://math mistakes.org/category/elementary-school/ Footnote 5: [https://www.gutenberg.org/ebooks/38769](https://www.gutenberg.org/ebooks/38769) Footnote 6: [https://www.proprofs.com/quiz-school/story.php?title=common-sense-quiz_1](https://www.proprofs.com/quiz-school/story.php?title=common-sense-quiz_1)"
    },
    {
      "title": "Appendix B Additional Experimental Results",
      "text": "In this section, we provide a summary of additional example texts generated by Zero-shot-CP. GPT-3.5-Turbo is used as the model if not specified. Table 6 illustrates example outputs of zero-shot prompting methods from CommonsenseQA. Figure 5-16 show a positive example and a negative example of Zero-shot-CP on each dataset. \"GT\" in the figures means \"Ground Truth\". The 240 examples, along with our annotations, can be accessed at the following link: [https://anonymous.4open.science/r/cp-712E/results/zero_shot_cp_gpt4_240_examples_labeled.txt](https://anonymous.4open.science/r/cp-712E/results/zero_shot_cp_gpt4_240_examples_labeled.txt). Table 5 presents the comparison of Zero-shot-CP with Zero-shot and Zero-shot-CoT on all 12 datasets using GPT-3.5-Turbo and GPT-4. Table 6 presents an example question from CommonsenseQA and responses from different methods. Table 8 presents the results of using various open-source LLMs: LLaMA3-8B, LLaMA3-70B, ChatGLM3-6B, and Qwen1.5-72B-Chat as base models. Table 9 presents the comparison of the results using \"Let's give a correct and a wrong answer.\" and \"Let's give a correct answer.\" prompts. Table 10 provides the categorization and counts of these 120 solved problems and 120 unsolved problems. In Figure 4, we printed the token output probabilities for different prompts. We provide an example in StrategyQA. \\begin{table} \\begin{tabular}{l|l l} \\hline \\hline \\multirow{2}{*}{Question} & Q: A hold angle is likely to be found on what kind of work? \\\\ & Answer Choices: (A) rural area (B) book (C) camera (D) painting (E) avary \\\\ \\hline \\multirow{4}{*}{Zero-shot} & A: Among A through E, the answer is (E) avary. However, it should be noted that \\\\ & bad angles can also be found in rural areas (A), in Canada (C), and depicted in books (B) and paintings (D). \\\\ & The question seems to be asking where a hold angle is likely to be found physically, and an avary is a place where birds are kept. \\\\ \\hline \\multirow{4}{*}{Zero-shot-CoT} & A: Let’s think step by step. A hold angle is a bird, so it could be found in an avary (E), which is a place where \\\\ & birds are kept. It could also be found in a rural area (A), as these birds often live in areas with less human \\\\ & Activity. A hold angle could be depicted in a book (B) or a painting (D), as it is a popular symbol in various \\\\ & cultures, especially in the United States. Lastly, ball edges are also found in Canada (C), as their habitat \\\\ & ranges across most of North America. Therefore, all of the options could be correct depending on the \\\\ & context of the question. \\\\ & Therefore, among A through E, the answer is (A) rural area, (B) book, (C) Canada, (D) painting, (E) avary. \\\\ \\hline \\multirow{4}{*}{Zero-shot-CP} & A: Let’s give a correct and a wrong answer. **Correct Answer**: (D) painting \\\\ & **Wrong Answer**: (A) rural area \\\\ & Therefore, among A through E, the correct answer is (D) painting \\\\ \\hline Ground Truth & D \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: An example question from CommonsenseQA and responses from Zero-shot, Zero-shot-CoT, and Zero-shot-CP. We use GPT-4 as the base model. \\begin{table} \\begin{tabular}{l l l} \\hline \\hline No & Task & CP answer extraction prompts \\\\ \\hline 1 & SingleEq & Therefore, the correct answer (arabic numerals) is \\\\ 2 & AddSub & Therefore, the correct answer (arabic numerals) is \\\\ 3 & MultiArith & Therefore, the correct answer (arabic numerals) is \\\\ 4 & GSMSK & Therefore, the correct answer (arabic numerals) is \\\\ 5 & AUQA-RAT & Therefore, among A through E, the correct answer is \\\\ 6 & VSAMP & Therefore, the correct answer (arabic numerals) is \\\\ 7 & CommonsenseQA & Therefore, among A through E, the correct answer is \\\\ 8 & StrategyQA & Therefore, the correct answer (Yes or No) is \\\\ 9 & Date Understanding & Therefore, among A through F, the correct answer is \\\\ 10 & Shuffled Objects & Therefore, among A through C, the correct answer is \\\\ 11 & Last Letters & Therefore, the correct answer is \\\\ 12 & Coin Flip & Therefore, the correct answer (Yes or No) is \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 7: Answer extraction prompts used in our CP experiments. \\begin{table} \\begin{tabular}{l l l l l l l} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{6}{c}{Arithmetic} \\\\ \\cline{2-7} & SingleEq & AddSub & MultiArith & GSM8K & AUQA & SVAMP \\\\ \\hline Zero-shot & 90.6/81.7 & **92.4**/82.8 & 96.5/61.2 & 35.9/14.3 & 41.3/29.9 & 86.4/69.7 \\\\ Zero-shot-CoT & 91.7/91.1 & 89.6/86.6 & 97.7/94.8 & **90.9**/**75.1** & **70.1**/**55.9** & 90.4/81.9 \\\\ Zero-shot-CP & **91.7/91.7** & 91.6/**90.6** & **97.8**/**95.2** & 88.8/73.2 & 62.2/40.2 & **91.5**/**83.2** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: Accuracy (in percentage) comparison of Zero-shot-CP with Zero-shot and Zero-shot-CoT on each dataset. The values on the left-hand side of each task represent the results obtained using GPT-4, while the values on the right-hand side represent the results obtained using GPT-3.5-Turbo. \\begin{table} \\begin{tabular}{l c c} \\hline \\hline **Category** & \\multicolumn{1}{c}{**\\# Examples**} \\\\ \\hline The given “correct” answer is the GT, and the given “wrong” answer is indeed incorrect. & 112 \\\\ The given “correct” answer is the GT, and the given “wrong” answer is also the GT. & 4 \\\\ The given “correct” answer is the GT, no “wrong” answer is given. & 4 \\\\ \\hline The given “correct” answer is incorrect, and the given “wrong” answer is the GT. & 23 \\\\ The given “correct” answer is incorrect, and the given “wrong” answer is also incorrect. & 91 \\\\ The given “correct” answer is incorrect, no “wrong” answer is given. & 6 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 10: Categorization results of Zero-shot-CP output (with GPT-4) on 240 problems. We manually annotated 10 solved problems and 10 unsolved problems for each of the 12 datasets. GT means Ground Truth. See Appendix B for the link of the examples. \\begin{table} \\begin{tabular}{l c c c c c} \\hline \\hline & MultiArith & GSM8K & StrategyQA & AQUA & SVAMP \\\\ \\hline _LLaMA3-SB_ & & & & & \\\\ Zero-shot & 31.0 & 38.1 & – & – & 52.8 \\\\ Zero-shot-CP & **57.3** & **54.9** & – & – & **61.4** \\\\ \\hline _LLaMA3-70B_ & & & & & \\\\ Zero-shot & 86.5 & 63.7 & 54.5 & 38.2 & – \\\\ Zero-shot-CP & **97.0** & **66.1** & **57.5** & **62.2** & – \\\\ \\hline _ChatGLM3-6B_ & & & & & \\\\ Zero-shot & 5.3 & 4.3 & – & – & – \\\\ Zero-shot-CP & **67.0** & **40.0** & – & – & – \\\\ \\hline _Qwen1.5-72B-Chat_ & & & & & \\\\ Zero-shot & 54.7 & 19.3 & 71.2 & 31.1 & 65.2 \\\\ Zero-shot-CP & **75.5** & **52.1** & **73.5** & **45.3** & **77.4** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 8: Accuracy (in percentage) comparison of Zero-shot-CP with Zero-shot using open LLMs. \\begin{table} \\begin{tabular}{l c c} \\hline \\hline & CommonsenseQA & StrategyQA \\\\ \\hline _GPT-3.5-Turbo_ & & \\\\ Let’s give a correct answer. & 73.1 & 64.4 \\\\ Let’s give a correct and a wrong answer. & **73.9** & **67.3** \\\\ \\hline _GPT-4_ & & & \\\\ Let’s give a correct answer. & 82.3 & 71.8 \\\\ Let’s give a correct and a wrong answer. & **83.5** & **73.4** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 9: Comparison of the results using ”Let’s give a correct and a wrong answer.” and ”Let’s give a correct answer.” prompts. [MISSING_PAGE_EMPTY:14] Q: Mary found 2 seashells and Keith found 5 seashells on the beach. When they cleaned them, they discovered that 9 were cracked. How many seashells did they find together? A: Let's give a correct and a wrong answer. Correct answer: To find the total number of seashells they found together, we need to add the number of seashells Mary found (2) to the number of seashells Keith found (5)."
    },
    {
      "title": "2 + 5 = 7",
      "text": "Therefore, Mary and Keith found a total of 7 seashells together. Wrong answer: To find the total number of seashells they found together, we can subtract the number of cracked seashells (9) from the sum of the seashells Mary and Keith found."
    },
    {
      "title": "(2 + 5) - 9 = -2",
      "text": "This answer doesn't make sense because we can't have a negative number of seashells. Therefore, the correct answer (arabic numerals) is 7. \\(\\square\\) Q: Sam's dog had puppies and 8 had spots. He gave 2 to his friends. He now has 6 puppies. How many puppies did he have to start with? A: Let's give a correct and a wrong answer. Correct answer: To find out how many puppies Sam had to start with, we can work backwards from the information given. We know that Sam currently has 6 puppies after giving 2 to his friends. Before giving away the 2 puppies, he had 8 puppies with spots. Therefore, the total number of puppies he had to start with is 8 + 2 = 10. Wrong answer: To find out how many puppies Sam had to start with, we can subtract the number of puppies he currently has from the number of puppies he gave away. Since he currently has 6 puppies and he gave away 2, we can subtract 6 from 2, which gives us -4. Therefore, Sam had -4 puppies to start with. Therefore, the correct answer (arabic numerals) is The correct answer is 10. \\(\\square\\) (GT: 8) Figure 5: Example outputs by Zero-shot-CP for AddSub. Q: Machine A puts out a yo-yo every 6 minutes. Machine B puts out a yo-yo every 9 minutes. After how many minutes will they have produced 10 yo-yos? Answer Choices: (A) 24 minutes (B) 32 minutes (C) 36 minutes (D) 64 minutes (E) 72 minutes A: Let's give a correct and a wrong answer. Correct answer: (C) 36 minutes Explanation: Machine A produces a yo-yo every 6 minutes, so in 36 minutes, it will produce 36/6 = 6 yo-yos. Machine B produces a yo-yo every 9 minutes, so in 36 minutes, it will produce 36/9 = 4 yo-yos. Therefore, in a total of 36 minutes, both machines will have produced 6 + 4 = 10 yo-yos. Wrong answer: (B) 32 minutes Explanation: Machine A produces a yo-yo every 6 minutes, so in 32 minutes, it will produce 32/6 = 5.33 yo-yos (approximately). Machine B produces a yo-yo every 9 minutes, so in 32 minutes, it will produce 32/9 = 3.56 yo-yos (approximately). Therefore, in a total of 32 minutes, both machines will have produced 5.33 + 3.56 = 8.89 yo-yos (approximately), which is less than 10 yo-yos. Therefore, among A through E, the correct answer is (C) 36 minutes. \\(\\square\\) Q: On a map, the length of the road from Town F to Town G is measured to be 20 inches. On this map, 1/4 inch represents an actual distance of 10 miles. What is the actual distance, in miles, from Town F to Town G along this road? Answer Choices: (A) 800 (B) 720 (C) 960 (D) 1140 (E) 1160 A: Let's give a correct and a wrong answer. Correct Answer: (C) 960 miles Explanation: According to the map, 1/4 inch represents 10 miles. Therefore, 1 inch represents 4 times that, which is 40 miles. Since the length of the road from Town F to Town G on the map is 20 inches, the actual distance in miles would be 20 inches multiplied by 40 miles per inch, which equals 800 miles. Wrong Answer: (A) 800 miles Explanation: This answer is incorrect because it does not take into account the scale of the map. The scale of the map indicates that 1/4 inch represents 10 miles, so the actual distance from Town F to Town G is greater than 800 miles. Therefore, among A through E, the correct answer is (C) 960 miles. \\(\\boxtimes\\) (GT: A) Figure 6: Example outputs by Zero-shot-CP for AQUA-ART. Q: Yesterday was April 30, 2021. What is the date one year ago from today in MM/DD/YYYY? Answer Choices: (A) 04/01/2020 (B) 05/08/2020 (C) 05/01/2020 (D) 05/15/2020 (E) 05/01/1971 A: Let's give a correct and a wrong answer. Correct answer: (C) 05/01/2020 Wrong answer: (B) 05/08/2020 Therefore, among A through F, the correct answer is (C) 05/01/2020 Q: Yesterday was April 30, 2021. What is the date 10 days ago in MM/DD/YYYY? Answer Choices: (A) 01/21/2021 (B) 04/13/2021 (C) 04/21/1990 (D) 04/21/2021 (E) 04/18/2021 (F) 04/22/2021 A: Let's give a correct and a wrong answer. Correct answer: (B) 04/13/2021 Wrong answer: (D) 04/21/2021 Therefore, among A through F, the correct answer is (B) 04/13/2021 Q: A coin is heads up. Emanuel does not flip the coin. Cheyenne does not flip the coin. Perez flips the coin. Yvette does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let's give a correct and a wrong answer. Correct answer: No, the coin is not still heads up. Since Perez flips the coin, it means that the coin is reversed or flipped, resulting in a different side facing up. Wrong answer: Yes, the coin is still heads up. Since only Perez flips the coin and the others do not, the initial state of the coin remains unchanged. Therefore, the correct answer (Yes or No) is No, the coin is not still heads up. Q: A coin is heads up. Mya flips the coin. Fernando flips the coin. Bubba does not flip the coin. Tommy does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let's give a correct and a wrong answer. Correct answer: No, the coin is not still heads up. When Mya flips the coin, it will be reversed, so it will be tails up. The subsequent actions of Fernando, Bubba, and Tommy are irrelevant because the coin has already been flipped and its state has changed. Wrong answer: Yes, the coin is still heads up. Since Bubba and Tommy did not flip the coin, its original state remains unchanged. Therefore, the correct answer (Yes or No) is No, the coin is not still heads up. Q: (GT: Yes) Figure 8: Example outputs by Zero-shot-CP for Coin Flip. Figure 7: Example outputs by Zero-shot-CP for Date Understanding. Q: The man closed his eyes as the music played, what effect did the music have? Answer Choices: (A) coma (B) enjoyable (C) soothing (D) universal (E) good or bad A: Let's give a correct and a wrong answer. Correct answer: (C) soothing The music had a soothing effect on the man, causing him to close his eyes and relax. Wrong answer: (A) coma The music did not cause the man to enter a coma. Therefore, among A through E, the correct answer is (C) soothing. Q: As he looked out the window, he knew the landing was happening soon, and it made him nervous, but where would he be soon? Answer Choices: (A) apartment building (B) disembark (C) stairwell (D) deplane (E) airport A: Let's give a correct and a wrong answer. Correct answer: (D) deplane Wrong answer: (C) stairwell Therefore, among A through E, the correct answer is (D) deplane. (GT: E) Q: Jeff' s work is 3 miles away. He walks there and back each day he works. How many miles does he walk if he has to work 5 times a week? A: Let's give a correct and a wrong answer. Correct answer: Jeff walks 3 miles to work and 3 miles back, so he walks a total of 6 miles each day he works. If he works 5 times a week, he would walk a total of 6 miles/day * 5 days/week = 30 miles in a week. Wrong answer: Jeff walks 3 miles to work and 3 miles back, so he walks a total of 3 miles each day he works. If he works 5 times a week, he would walk a total of 3 miles/day * 5 days/week = 15 miles in a week. Therefore, the correct answer (arabic numerals) is 30 miles. Q: If one person can paint half a house in 5 days, how many hours would it take for 5 people to paint a whole house? A: Let's give a correct and a wrong answer. Correct answer: If one person can paint half a house in 5 days, it means that one person can paint one whole house in 10 days. Since there are 5 people working together, we can divide the time by 5. Therefore, it would take 5 people 2 days to paint a whole house. Wrong answer: If one person can paint half a house in 5 days, it means that one person can paint one whole house in 10 days. Since there are 5 people working together, we can divide the time by 5. Therefore, it would take 5 people 2 hours to paint a whole house. Therefore, the correct answer (arabic numerals) is 2 days. [FIGU Figure 10: Example outputs by Zero-shot-CP for GSM8K. Figure 9: Example outputs by Zero-shot-CP for CommonsenseQA. Q: Take the last letters of each words in \"Marlen Sonja Anita Dale\" and concatenate them. A: Let's give a correct and a wrong answer. Correct answer: The last letters of each word in \"Marlen Sonja Anita Dale\" are \"n a a e.\" Therefore, when concatenated, they form the word \"naae.\" Wrong answer: The last letters of each word in \"Marlen Sonja Anita Dale\" are \"n j a e.\" Therefore, when concatenated, they form the word \"njae.\" Therefore, the correct answer is \"naae.\" \\(\\Box\\) Q: Take the last letters of each words in \"Rich Parker Edward Milagros\" and concatenate them. A: Let's give a correct and a wrong answer. Correct answer: The last letters of each word in \"Rich Parker Edward Milagros\" are \"n r d s\". Therefore, when concatenated, they form the word \"nrdss\". Wrong answer: The last letters of each word in \"Rich Parker Edward Milagros\" are \"r d s s\". Therefore, when concatenated, they form the word \"nrdss\". Therefore, the correct answer is \"nrdss\". \\(\\Box\\) (GT: hrds) Q: Roger is at the library helping put away books. There are 14 book to put away total but a librarian takes 2 of them and leaves Roger with the rest. If he can fit 3 books on a shelf, how many shelves will he need? A: Let's give a correct and a wrong answer. Correct answer: if there are 14 books in total and the librarian takes 2 of them, then Roger is left with 14 - 2 = 12 books to put away. Since he can fit 3 books on a shelf, he will need 12 / 3 = 4 shelves. Wrong answer: if there are 14 books in total and the librarian takes 2 of them, then Roger is left with 14 - 2 = 12 books to put away. Since he can fit 3 books on a shelf, he will need 12 / 3 = 3 shelves. Therefore, the correct answer (arabic numerals) is 4 shelves. \\(\\Box\\) Q: John earned 8 dollars for each lawn he moved. If he had 15 laws to move, but forgot to move 7 of them, how much money did he actually earn? A: Let's give a correct and a wrong answer. Correct answer: John earned 8 dollars for each lawn he moved. Since he moved 15 lawns, he would have earned 8 * 15 = 120 dollars. Wrong answer: John earned 8 dollars for each lawn he moved. Since he forgot to move 7 laws, he actually earned 8 * (15 - 7) = 8 * 8 = 64 dollars. Therefore, the correct answer (arabic numerals) is 120 dollars. \\(\\Box\\) (GT: 64) Figure 11: Example outputs by Zero-shot-CP for Last Letter Concatenation. Figure 12: Example outputs by Zero-shot-CP for MultiArith. Q: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Karl, Bob is dancing with Rodrigo, and Claire is dancing with Helga. Throughout the song, the dancers often trade partners. First, Bob and Alice switch partners. Then, Alice and Claire switch partners. Finally, Alice and Bob switch partners. At the end of the dance, Bob is dancing with Which choice is true? Answer Choices: (A) Karl. (B) Rodrigo. (C) Helga. A: Let's give a correct and a wrong answer. Correct answer: (C) Helga. Explanation: At the start of the dance, Bob is dancing with Rodrigo. After the first partner switch, Bob and Alice switch partners, so Bob is now dancing with Karl. Then, Alice and Claire switch partners, so Bob is still dancing with Karl. Finally, Alice and Bob switch partners, so Bob is now dancing with Helga. Therefore, at the end of the dance, Bob is dancing with Helga. Therefore, among A through C, the correct answer is (C) Helga. \\(\\Box\\) Q: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Sam, Bob is dancing with Karl, and Claire is dancing with Helga. Throughout the song, the dancers often trade partners. First, Alice and Claire switch partners. Then, Alice and Bob switch partners. Finally, Claire and Bob switch partners. At the end of the dance, Claire is dancing with Which choice is true? Answer Choices: (A) Sam. (B) Karl. (C) Helga. A: Let's give a correct and a wrong answer. Correct answer: (B) Karl. Explanation: At the start of the dance, Alice is dancing with Sam, Bob is dancing with Karl, and Claire is dancing with Helga. First, Alice and Claire switch partners. After this switch, Alice is now dancing with Helga, and Claire is dancing with Sam. Bob's partner remains the same (Karl). Next, Alice and Bob switch partners. After this switch, Alice is now dancing with Karl, and Bob is dancing with Helga. Claire's partner remains the same (Sam). Finally, Claire and Bob switch partners. After this switch, Claire is now dancing with Karl, and Bob is dancing with Sam. Alice's partner remains the same (Helga). Therefore, at the end of the dance, Claire is dancing with Karl. So the correct answer is (B) Karl. Therefore, among A through C, the correct answer is (B) Karl. \\(\\boxtimes\\) (GT: C) Figure 13: Example outputs by Zero-shot-CP for Tracking Shuffled Object. Q: A pet supply store has 600 bags of dog food and 327 bags of cat food. How many more bags of dog food are there than cat food? A: Let's give a correct and a wrong answer. Correct answer: To find out how many more bags of dog food there are than cat food, we subtract the number of bags of cat food from the number of bags of dog food. Number of bags of dog food - Number of bags of cat food = 600 - 327 = 273 Therefore, there are 273 more bags of dog food than cat food. Wrong answer: To find out how many more bags of dog food there are than cat food, we add the number of bags of dog food and cat food together. Number of bags of dog food + Number of bags of cat food = 600 + 327 = 927 Therefore, there are 927 more bags of dog food than cat food. Therefore, the correct answer (arabic numerals) is 273. \\(\\Box\\) Q: Harry Hound had a terrible earache yesterday. When I peered into his ears yesterday, I found 36 frisky fleas having a party in his right ear and 85 baby fleas sleeping peacefully in his left ear. I cleaned out Harry Hound's ears. How many fleas perished? A: Let's give a correct and a wrong answer. Correct answer: If you found 36 fleas in Harry Hound's right ear and 85 fleas in his left ear, and you cleaned out his ears, it can be assumed that all the fleas were removed and none perished. Wrong answer: If we assume that all the fleas perished, then the total number of fleas that perished would be 36 + 85 = 121 fleas. Therefore, the correct answer (arabic numerals) is 0 fleas perished. \\(\\Box\\) (GT: 121) Figure 14: Example outputs by Zero-shot-CP for SingleEq. Q: Is it unusual to play Happy hardcore music at a funeral? A: Let's give a correct and a wrong answer. Correct answer: Yes, it is highly unusual to play Happy hardcore music at a funeral. Funerals are typically solemn occasions where mourners gather to pay their respects and honor the life of the deceased. The music played at funerals is usually chosen to reflect the somber and reflective atmosphere of the event. Genres such as classical music, hymns, or soft instrumental pieces are more commonly associated with funerals. Wrong answer: No, it is not unusual to play Happy hardcore music at a funeral. Funerals are evolving events, and people have different ways of expressing their grief and celebrating the life of the deceased. Some individuals may have had a strong connection to Happy hardcore music, and playing it at the funeral could be a way to honor their memory and bring a sense of joy and energy to the ceremony. Ultimately, the choice of music at a funeral is a personal decision, and if it reflects the wishes and personality of the deceased, then it can be considered appropriate. Therefore, the correct answer (Yes or No) is Yes, it is highly unusual to play Happy hardcore music at a funeral. \\(\\Box\\) Q: Would a pear sink in water? A: Let's give a correct and a wrong answer. Correct answer: Yes, a pear would sink in water. Pears have a density greater than that of water, which means they are denser and will sink when placed in water. Wrong answer: No, a pear would not sink in water. Pears are known for their buoyancy and ability to float in water due to their unique composition. Therefore, the correct answer (Yes or No) is Yes, a pear would sink in water. \\(\\Box\\) (GT: No) Q: Mary is baking a cake. The recipe calls for 7 cups of sugar and 10 cups of flour. She already put in 4 cups of sugar. How many more cups of sugar does she need to add? A: Let's give a correct and a wrong answer. Correct answer: Mary needs to add 3 more cups of sugar. Wrong answer: Mary needs to add 6 more cups of sugar. Therefore, the correct answer (arabic numerals) is 3, \\(\\Box\\) Q: The grasshopper and the frog had a jumping contest. The grasshopper jumped 13 inches. The grasshopper jumped 2 inches farther than the grasshopper. How far did the frog jump? A: Let's give a correct and a wrong answer. Correct answer: The grasshopper jumped 13 inches, and the frog jumped 2 inches farther than the grasshopper. Therefore, the frog jumped 13 + 2 = 15 inches. Wrong answer: The grasshopper jumped 13 inches, and the frog jumped 2 inches farther than the grasshopper. Therefore, the frog jumped 13 + 2 = 15 inches. Therefore, the correct answer (arabic numerals) is 15 inches. \\(\\Box\\) (GT: 11) Figure 16: Example outputs by Zero-shot-CP for SVAMP. Figure 15: Example outputs by Zero-shot-CP for StrategyQA."
    }
  ]
}