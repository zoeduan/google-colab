{
  "title": "A Case-Based Persistent Memory for a Large Language Model ⋆",
  "authors": [
    "Ian Watson"
  ],
  "abstract": "\n Case-based reasoning (CBR) is a methodology for problem-solving that can use any appropriate computational technique. This position paper argues that CBR researchers risk overlooking recent developments in deep learning and large language models (LLMs). The underlying technical developments that have enabled the recent breakthroughs in AI have strong synergies with CBR. Moreover, CBR could be used to provide a persistent memory for LLMs improving their performance and furthering progress towards Artificial General Intelligence. \n",
  "references": [
    {
      "id": null,
      "title": "A Case-Based Persistent Memory for a Large Language Model ⋆",
      "authors": [
        "Ian Watson"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "CBR is a methodology not a technology",
      "authors": [
        "I Watson"
      ],
      "year": "1999",
      "venue": "The Knowledge Based Systems Journal",
      "doi": "10.1016/S0950-7051(99)00020-9"
    },
    {
      "id": "b1",
      "title": "Case-Based Reasoning: Foundational Issues, Methodological Variations, and System Approaches",
      "authors": [
        "A Aamodt",
        "E Plaza"
      ],
      "year": "1994",
      "venue": "AI Communications",
      "doi": "10.3233/AIC-1994-7104"
    },
    {
      "id": "b2",
      "title": "The Knowledge Level",
      "authors": [
        "A Newell"
      ],
      "year": "1982",
      "venue": "Artificial Intelligence",
      "doi": "10.1016/0004-3702(82)90012-1"
    },
    {
      "id": "b3",
      "title": "ChatGPT: Optimizing Language Models for Dialogue",
      "authors": [
        "Openai"
      ],
      "year": "2022",
      "venue": "ChatGPT: Optimizing Language Models for Dialogue",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Attention is All You Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": "10.48550/arXiv.1706.03762"
    },
    {
      "id": "b5",
      "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
      "authors": [
        "Y Zhang",
        "Y Li",
        "L Cui",
        "D Cai",
        "L Liu",
        "T Fu",
        "X Huang",
        "E Zhao",
        "Y Zhang",
        "Y Chen",
        "L Wang",
        "A T Luu",
        "W Bi",
        "F Shi",
        "S Shi"
      ],
      "year": "2023",
      "venue": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
      "doi": "10.48550/arXiv.2309.01219"
    },
    {
      "id": "b6",
      "title": "A Survey of Hallucination in Large Foundation Models",
      "authors": [
        "V Rawte",
        "A Sheth",
        "A Das"
      ],
      "year": "2023",
      "venue": "A Survey of Hallucination in Large Foundation Models",
      "doi": "10.48550/arXiv.2309.05922"
    },
    {
      "id": "b7",
      "title": "Case-Based Strategies in Computer Poker",
      "authors": [
        "J Rubin",
        "I Watson"
      ],
      "year": "2012",
      "venue": "AI Communications",
      "doi": "10.3233/AIC-2012-0513"
    },
    {
      "id": "b8",
      "title": "Lazy Learning",
      "authors": [
        "D Aha"
      ],
      "year": "1997",
      "venue": "Artificial Intelligence Review",
      "doi": "10.1007/978-94-017-2053-3"
    },
    {
      "id": "b9",
      "title": "Multidimensional binary search trees used for associative searching",
      "authors": [
        "J Bentley"
      ],
      "year": "1975",
      "venue": "Communications of the ACM",
      "doi": "10.1145/361002.361007"
    },
    {
      "id": "b10",
      "title": "Advanced Similarity Measures Using Word Embeddings and Siamese Networks in CBR",
      "authors": [
        "K Amin",
        "G Lancaster",
        "S Kapetanakis",
        "K D Althoff",
        "A Dengel",
        "M Petridis"
      ],
      "year": "2020",
      "venue": "Intelligent Systems and Applications. IntelliSys 2019. Advances in Intelligent Systems and Computing",
      "doi": "10.1007/978-3-030-29513-4_32"
    },
    {
      "id": "b11",
      "title": "A convolutonal siamese network for developing similarity knowledge in the Selfback dataset",
      "authors": [
        "K Martin",
        "N Wiratunga",
        "S Sani"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Conference on Case-Based Reasoning Workshops, CEUR Workshop Proceedings",
      "doi": "10.1007/978-3-031-14923-8_10"
    },
    {
      "id": "b12",
      "title": "Mapping the Landscape of Human-Level Artificial General Intelligence",
      "authors": [
        "S Adams",
        "I Arel",
        "J Bach",
        "R Coop",
        "R Furlan",
        "B Goertzel",
        "J S Hall",
        "A Samsonovich",
        "M Scheutz",
        "M Schlesinger",
        "S C Shapiro",
        "J Sowa"
      ],
      "year": "2012",
      "venue": "AI Magazine",
      "doi": "10.1609/aimag.v33i1.2322"
    },
    {
      "id": "b13",
      "title": "Twin-systems to explain artificial neural networks using case-based reasoning: comparative tests of feature-weighting methods in ANN-CBR twins for XAI",
      "authors": [
        "E M Kenny",
        "M T Keane"
      ],
      "year": "2019",
      "venue": "Proc. of the Twenty-Eighth International Joint Conference on Artificial Intelligence",
      "doi": "10.24963/ijcai.2019/376"
    },
    {
      "id": "b14",
      "title": "This looks like that: deep learning for interpretable image recognition",
      "authors": [
        "C Chen",
        "O Li",
        "D Tao",
        "A Barnett",
        "C Rudin",
        "J K Su"
      ],
      "year": "2019",
      "venue": "NIPS'19: Proceedings of the 33rd International Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "CBR Assisted Context-Aware Surface Realisation for Datato-Text Generation",
      "authors": [
        "A Upadhyay",
        "S Massie"
      ],
      "year": "2023",
      "venue": "Lecture Notes in Computer Science",
      "doi": "10.1007/978-3-031-40177-0_3"
    },
    {
      "id": "b16",
      "title": "Case-Based Adaptation of Argument Graphs with WordNet and Large Language Models",
      "authors": [
        "L Mirko",
        "R Bergmann"
      ],
      "year": "2023",
      "venue": "Lecture Notes in Computer Science",
      "doi": "10.1007/978-3-031-40177-0_17"
    },
    {
      "id": "b17",
      "title": "Case-Based Reasoning with Language Models for Classification of Logical Fallacies",
      "authors": [
        "Z Sourati",
        "F Ilievski",
        "H Sandlin",
        "A Mermoud"
      ],
      "year": "2023",
      "venue": "International Joint Conference on Artificial Intelligence",
      "doi": "10.48550/arXiv.2301.11879"
    },
    {
      "id": "b18",
      "title": "Vector Databases, Large Language Models and Case-Based Reasoning",
      "authors": [
        "S Thompson"
      ],
      "year": "2022",
      "venue": "Retrieved Sept",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Enhancing ChatGPT With Infinite External Memory Using Vector Database and ChatGPT Retrieval Plugin",
      "authors": [
        "R Jia"
      ],
      "year": "2023",
      "venue": "Enhancing ChatGPT With Infinite External Memory Using Vector Database and ChatGPT Retrieval Plugin",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "An optimal algorithm for approximate nearest neighbor searching",
      "authors": [
        "S Arya",
        "D M Mount",
        "N S Netanyahu",
        "R Silverman",
        "A Wu"
      ],
      "year": "1998",
      "venue": "Journal of the ACM",
      "doi": "10.1145/293347.293348"
    },
    {
      "id": "b21",
      "title": "Approximate Nearest Neighbours for Recommender Systems",
      "authors": [
        "B Frederickson"
      ],
      "year": "2017",
      "venue": "Approximate Nearest Neighbours for Recommender Systems",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Approximate Similarity Search with FAISS Framework Using FPGAs on the Cloud",
      "authors": [
        "D Danopoulos",
        "C Kachris",
        "D Soudris"
      ],
      "year": "2019",
      "venue": "Embedded Computer Systems: Architectures, Modeling, and Simulation. SAMOS 2019. Lecture Notes in Computer Science 11733",
      "doi": "10.1007/978-3-030-27562-4_27"
    },
    {
      "id": "b23",
      "title": "Fast Approximate Nearest Neighbor Search With Navigating Spreading-out Graphs",
      "authors": [
        "C Fu",
        "C Wang",
        "D Cai"
      ],
      "year": "2017",
      "venue": "Fast Approximate Nearest Neighbor Search With Navigating Spreading-out Graphs",
      "doi": "10.48550/arXiv.1707.00143"
    },
    {
      "id": "b24",
      "title": "Harnessing Hundreds of Millions of Cases: Case-Based Prediction at Industrial Scale",
      "authors": [
        "V Jalali",
        "D Leake"
      ],
      "year": "2018",
      "venue": "Lecture Notes in Computer Science(",
      "doi": "10.1007/978-3-030-01081-2_11"
    },
    {
      "id": "b25",
      "title": "Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based Reasoning Pairings for Explanation and Data Augmentation",
      "authors": [
        "M Keane",
        "E M Kenny",
        "M Temraz",
        "D Greene",
        "B Smyth"
      ],
      "year": "2021",
      "venue": "Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based Reasoning Pairings for Explanation and Data Augmentation",
      "doi": "10.48550/arXiv.2104.14461"
    },
    {
      "id": "b26",
      "title": "Examining the Impact of Network Architecture on Extracted Feature Quality for CBR",
      "authors": [
        "D Leake",
        "Z Wilkerson",
        "V Vats",
        "K Acharya",
        "D Crandall"
      ],
      "year": "2023",
      "venue": "Lecture Notes in Computer Science",
      "doi": "10.1007/978-3-031-40177-0_1"
    },
    {
      "id": "b27",
      "title": "Examining the Impact of Network Architecture on Extracted Feature Quality for CBR",
      "authors": [
        "D Leake",
        "Z Wilkerson",
        "V Vats",
        "K Acharya",
        "D Crandall"
      ],
      "year": "2023",
      "venue": "Lecture Notes in Computer Science",
      "doi": "10.1007/978-3-031-40177-0_1"
    }
  ],
  "sections": [
    {
      "title": "A Case-Based Persistent Memory For A Large Language Model+",
      "text": "Footnote †: _Iccbr 24: Workshop on Case-Based Reasoning and Large Language Model Synergies, June 1-4, 2024, Merida, Mexico. [https://www.iccbr2024.org_](https://www.iccbr2024.org_) Ian Watson Corresponding author. University of Auckland, School of Computer Science, Auckland, New Zealand"
    },
    {
      "title": "Abstract",
      "text": "Case-based reasoning (CBR) is a methodology for problem-solving that can use any appropriate computational technique. This position paper argues that CBR researchers risk overlooking recent developments in deep learning and large language models (LLMs). The underlying technical developments that have enabled the recent breakthroughs in AI have strong synergies with CBR. Moreover, CBR could be used to provide a persistent memory for LLMs improving their performance and furthering progress towards Artificial General Intelligence. Case-Based Reasoning, Large Language Models, Deep Learning, Artificial General Intelligence"
    },
    {
      "title": "1 Introduction",
      "text": "In the paper, \"_CBR is a methodology not a technology_,\" Watson [1] argued that case-based reasoning can use various techniques from computer science within the CBR-cycle [2]. At the core of CBR is the concept of similarity, solving current problems using similar prior solutions. The precise methods used to retrieve and calculate case similarity are unimportant if a useful result is obtained. At the time, this opinion was controversial to some; surely, CBR was a unique technique like other AI techniques (e.g., rule-based reasoning, genetic algorithms or neural networks). However, as people have used different methods to implement CBR over time, this view has become largely accepted. CBR is a knowledge-level description of a problem-solving paradigm in the classic sense [3]. Watson's 1999 paper concluded, \"_Moreover, AI will surely develop new technologies in the future, some of which may prove very suitable for use in the CBR-cycle. Consequently, it is as a methodology that CBR's future is ensured._\" I believe that we are at just such a point where significant innovations in AI have and are occurring that the CBR community can adopt. The purpose of this paper is to stimulate discussion within the CBR community. Many of the ideas here are partially thought through, and I'm sure there are many other obvious omissions, but I hope this helps the community work towards a wonderful future for CBR."
    },
    {
      "title": "2 Think Big",
      "text": "In the last two years, we've witnessed a remarkable transformation in AI since the release of ChatGPT by OpenAI in November 2022 [4]. Its conversational interface grabbed the public's attention like no other AI innovation. Its ability to answer detailed questions, called prompts, and give cogent long-form responses amazed everyone, even seasoned, cynical AI professionals such as ourselves. Underlying this remarkable capability are large language models (LLMs) created by deep learning (DL) neural networks using the transformer architecture [5]. These networks are trained on petabytes of text harvested from the Internet and fine-tuned using human-supervised reinforcement learning, resulting in DL networks with over a hundred billion parameters and emergent behaviour that has captivated the world's attention. These LLMs are behemoths running on tens of thousands of processing cores and initially costing millions of dollars to train. What is fascinating from an engineering perspective is that the developers of these systems did not know in advance what performance they would obtain. The so-called \"hallucinations\" that LLMs suffer from, where they fabricate fictitious answers, are, in fact, a feature [6, 7]. Their developers do not know why their LLMs give a seemingly correct reply or a falsehood to a prompt. Indeed, LLMs don't respond the same way to the same prompt each time. Stop. Think about that for a moment. A computer program that may give a different answer to exactly the same input on separate occasions. What possible use could such a program be? Certainly, it would be no use in any engineering context. Yet, in many real-world contexts, that is exactly what we intelligent humans do. Ask me a question first thing in the morning, and you'll get one answer. You'll probably get a different answer to the same question late at night. Obviously, some questions always generate the same reply. 2 + 2 always equals 4. But \"_Should I buy that new jacket?_\" is a question whose answer may vary throughout the day largely because, unlike the maths question, there is no single correct solution. To recap, the deep learning research community spent thousands of person-hours and millions of dollars building the largest computational models ever to provide vague, inconsistent answers to imprecise questions. The notion of similarity in CBR is vague, inconsistent and imprecise. This is what gives CBR its fascination, strength and appeal. The widely used k-NN algorithm is very simple, but within the equation's similarity function, \"\\(f\\)\", hides complexity. As we all know, measuring similarity is endlessly complicated and usually requires compromises to get an answer good enough in a reasonable compute time. In the early days of CBR, in the 1990s, the first question to any presenter at a CBR meeting was usually, \"_How many cases do you have?_\" It wasn't uncommon for the answer to be a dozen or maybe a hundred cases or so. Case-bases back then were really small. The travel case-base, developed by TecInno, has 1,100 cases and was used as a standard teaching, research and demonstration tool during the 1990s and well into the 2000s. Large case-bases, with upwards of a million cases, were not seen until the 2010s [8]. I am going to argue that, given its history of limited computing power, CBR has never really thought big. Unlike the deep learning community that depends upon processing power provided by thousands of GPUs and terabytes of training data. One of the constant criticisms of CBR, particularly those systems implemented using k-NN, is that lazy learning [9] is inefficient at retrieval time. This is undeniable, particularly if the cases are high-dimensional and the similarity metrics are complicated. Thus, CBR has often traded off retrieval performance against retrieval time. However, nobody argues that deep learners are computationally efficient during training. Indeed, quite the opposite, to CBR researchers, they use unthinkably massive amounts of compute to train them. Consequently, CBR has long known how to improve retrieval efficiency by building a case index using an eager learning method like kd-trees [10]. I suggest using deep learning to model similarity metrics, as has been done by Amin et al. [11], or Martin et al. [12] and take a lesson from the scale of recent DL systems like LLMs and the technical innovations that support them. We should envisage using a case base that contains petabytes of unstructured (probably multi-modal) cases. Rather than being a specialised CBR system focused on one specific problem, we could build a general CBR system capable of solving problems generally in the same way that LLMs can converse across multiple specialised areas. Ultimately, the case-based memory would be integrated with the LLM to provide a persistent memory of all its conversations. This may bring us closer to a true Artificial General Intelligence (AGI) capable of operating in all domains [13] and having a memory of all its interactions with the world as a critical requirement of intelligence, as discussed in the Appendix."
    },
    {
      "title": "3 Towards A Deepcbr Architecture",
      "text": "There has been some work on linking CBR with DL models. Explainability has always been problematic for neural networks that function as black boxes. So, twining them with CBR to Figure 1: Architecture of GPT-3.5 With External Memory Using a Vector Database, after Jia [20] provide explainability, as in [14], has been investigated. Other synergies between CBR and deep learning have been explored by Chen et al. [15] for interpretable image recognition, Upadhyay, and Massie [16], who use a hybrid CBR and neural text generator to improve the automatic summarisation of sports reports, and Mirko and Bergman [17] have used the predictive power of LLMs for case-adaptation, while Sourati et al. [18] combine CBR with LLMs for detecting fallacies in natural language arguments. Moreover, others outside the CBR community have seen the potential of LLMs and DL to be applied to CBR [19]. Technically, this is not hard. A simple architecture that links a persistent memory stored in a vector database to an LLM (GPT-3.50) using standard APIs has been published by Jia [20]. Moreover, vector databases routinely use approximate nearest neighbor search (ANNS) as a retrieval technique with which CBR researchers are familiar. Or FAISS (Facebook AI Similarity Search), a recent efficient implementation of ANNS [21 - 24] released as open source by Meta AI. Thus, the technology is here for the CBR community to experiment with it at scale [25] in what Keane et al. call DeepCBR [26]."
    },
    {
      "title": "4 Conclusion",
      "text": "Leake et al. [27] recently argued \"that methods that blend the complementary strengths offered by DL and CBR approaches are appealing\". Showing useful results in feature extraction and highlighting other potential synergies between DL and CBR in explainability (XAI) and informing the selection of DL architectures. Thus, the CBR community should investigate these new promising AI techniques to see how they can continue to enhance the CBR methodology. Moreover, realising their systems benefit from having a memory of conversations Open AI announced in February 2024 the addition of a persistant conversation memory to Chat GPT. As the branch of AI that has dealt with memory since the 1980s we should step up to influence this critical area of research and development since we not only have techniques for creating memories and retrieving from them, but crucially the ongoing maintenance of memory [28]. Thereby CBR and LLMs can help progress towards Artificial General Intelligence."
    },
    {
      "title": "References",
      "text": "* [1] Watson, I., (1999). _CBR is a methodology not a technology_. In, The Knowledge Based Systems Journal 12 (5-6): 303-8. Elsevier. [https://doi.org/10.1016/S0950-7051](https://doi.org/10.1016/S0950-7051)(99)00020-9 * [2] Aamodt, A. Plaza, E., (1994). _Case-Based Reasoning: Foundational Issues, Methodological Variations, and System Approaches_. In, AI Communications 7 (1):39-59. [http://dx.doi.org/10.3233/AIC-1994-7104](http://dx.doi.org/10.3233/AIC-1994-7104) * [3] Newell A., (1982). _The Knowledge Level_. In, Artificial Intelligence 18: 87-127. [https://doi.org/10.1016/0004-3702](https://doi.org/10.1016/0004-3702)(82)90012-1 * [4] OpenAI, (2022). _ChatGPT: Optimizing Language Models for Dialogue_. Retrieved December 5, 2022. [https://online-chatgpt.com/](https://online-chatgpt.com/) * [5] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., Polosukhin, I., (2017). _Attention is All You Need_. In, Advances in Neural Information Processing Systems 30. Curran Associates, Inc. [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)* [6] Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y., Chen, Y., Wang, L., Luu, A.T., Bi, W., Shi, F., Shi, S., (2023). _Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models_. arXiv:2309.01219. [https://doi.org/10.48550/arXiv.2309.01219](https://doi.org/10.48550/arXiv.2309.01219) * [7] Rawte, V., Sheth, A., Das, A., (2023). _A Survey of Hallucination in Large Foundation Models_. ArXiv, abs/2309.05922. [https://doi.org/10.48550/arXiv.2309.05922](https://doi.org/10.48550/arXiv.2309.05922) * [8] Rubin, J. Watson, I., (2012). _Case-Based Strategies in Computer Poker_. In, AI Communications, 25:19-48. [http://dx.doi.org/10.3233/AIC-2012-0513](http://dx.doi.org/10.3233/AIC-2012-0513) * [9] Aha, D. (1997). _Lazy Learning_. Artificial Intelligence Review volumes 11-15, Springer. [https://doi.org/10.1007/978-94-017-2053-3](https://doi.org/10.1007/978-94-017-2053-3) * [10] Bentley, J. (1975). _Multidimensional binary search trees used for associative searching_. Communications of the ACM, 18 (9): 509-517 [https://doi.org/10.1145/361002.361007](https://doi.org/10.1145/361002.361007) * [11] Amin, K., Lancaster, G., Kapetanakis, S., Althoff, KD., Dengel, A., Petridis, M. (2020). _Advanced Similarity Measures Using Word Embeddings and Siamese Networks in CBR_. In: Bi, Y., Bhatia, R., Kapoor, S. (eds) Intelligent Systems and Applications. IntelliSys 2019. Advances in Intelligent Systems and Computing, vol 1038. Springer, Cham. [https://doi.org/10.1007/978-3-030-29513-4_32](https://doi.org/10.1007/978-3-030-29513-4_32) * [12] Martin K, Wiratunga N, Sani S, et al. _A convolutonal siamese network for developing similarity knowledge in the Selfback dataset_. In: Proceedings of the International Conference on Case-Based Reasoning Workshops, CEUR Workshop Proceedings, ICCBR; 2017. p. 85-94. [https://doi.org/10.1007/978-3-031-14923-8_10](https://doi.org/10.1007/978-3-031-14923-8_10) * [13] Adams, S., Arel, I., Bach, J., Coop, R., Furlan, R., Goertzel, B., Hall, J. S., Samsonovich, A., Scheutz, M., Schlesinger, M., Shapiro, S. C., Sowa, J., (2012). _Mapping the Landscape of Human-Level Artificial General Intelligence_. AI Magazine, 33(1), 25-42. [https://doi.org/10.1609/aimag.v33i1.2322](https://doi.org/10.1609/aimag.v33i1.2322) * [14] Kenny, E.M., Keane, M.T., (2019) _Twin-systems to explain artificial neural networks using case-based reasoning: comparative tests of feature-weighting methods in ANN-CBR twins for XAI_. In: Proc. of the Twenty-Eighth International Joint Conference on Artificial Intelligence, pp. 2708-2715. [https://doi.org/10.24963/ijcai.2019/376](https://doi.org/10.24963/ijcai.2019/376) * [15] Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., Su, J. K. (2019). _This looks like that: deep learning for interpretable image recognition_. NIPS'19: Proceedings of the 33rd International Conference on Neural Information Processing Systems, December 2019, Article No.: 801, 8930-8941 * [16] Upadhyay, A., Massie, S., (2023). _CBR Assisted Context-Aware Surface Realisation for Data-to-Text Generation_. In: Massie, S., Chakraborti, S. (eds) Case-Based Reasoning Research and Development. ICCBR 2023. Lecture Notes in Computer Science 14141: 34-39. Springer. [https://doi.org/10.1007/978-3-031-40177-0_3](https://doi.org/10.1007/978-3-031-40177-0_3) * [17] Mirko, L., Bergmann, R., (2023). _Case-Based Adaptation of Argument Graphs with WordNet and Large Language Models_. In: Massie, S., Chakraborti, S. (eds) Case-Based Reasoning Research and Development. ICCBR 2023. Lecture Notes in Computer Science, vol 14141: 263-278, Springer, [https://doi.org/10.1007/978-3-031-40177-0_17](https://doi.org/10.1007/978-3-031-40177-0_17) * [18] Sourati, Z., Ilievski, F., Sandlin, H., Mermoud, A. (2023). _Case-Based Reasoning with Language Models for Classification of Logical Fallacies_. International Joint Conference on Artificial Intelligence. [https://doi.org/10.48550/arXiv.2301.11879](https://doi.org/10.48550/arXiv.2301.11879)* Thompson [2022] Thompson, S., (2022). _Vector Databases, Large Language Models and Case-Based Reasoning_. Retrieved Sept, 2023. [https://bit.ly/45JVviL](https://bit.ly/45JVviL) * Jia [2023] Jia, R., (2023). _Enhancing ChatGPT With Infinite External Memory Using Vector Database and ChatGPT Retrieval Plugin_. Retrieved Apr, 2023. [https://bit.ly/490ZNo6](https://bit.ly/490ZNo6) * Arya et al. [1998] Arya, S., Mount, D. M., Netanyahu, N. S., Silverman, R., Wu, A., (1998). _An optimal algorithm for approximate nearest neighbor searching_. Journal of the ACM. 45 (6): 891-923. [https://doi.org/10.1145/293347.293348](https://doi.org/10.1145/293347.293348) * Frederickson [2017] Frederickson, B., (2017). _Approximate Nearest Neighbours for Recommender Systems_. [https://bit.ly/3ZVbjNp](https://bit.ly/3ZVbjNp) Retrieved Sept 2023 * Danopoulos et al. [2019] Danopoulos, D., Kachris, C., Soudris, D., (2019). _Approximate Similarity Search with FAISS Framework Using FPGAs on the Cloud_. In: Pnevmatikatos, D., Pelcat, M., Jung, M. (eds) Embedded Computer Systems: Architectures, Modeling, and Simulation. SAMOS 2019. Lecture Notes in Computer Science 11733. Springer. [https://doi.org/10.1007/978-3-030-27562-4_27](https://doi.org/10.1007/978-3-030-27562-4_27) * Fu et al. [2017] Fu, C., Wang, C., and Cai, D., (2017). _Fast Approximate Nearest Neighbor Search With Navigating Spreading-out Graphs_. ArXiv, abs/1707.00143. [https://doi.org/10.48550/arXiv.1707.00143](https://doi.org/10.48550/arXiv.1707.00143) * Jalali and Leake [2018] Jalali, V., Leake, D. (2018). _Harnessing Hundreds of Millions of Cases: Case-Based Prediction at Industrial Scale_. In: Cox, M., Funk, P., Begum, S. (eds) Case-Based Reasoning Research and Development. ICCBR 2018. Lecture Notes in Computer Science(), vol 11156. Springer, Cham. [https://doi.org/10.1007/978-3-030-01081-2_11](https://doi.org/10.1007/978-3-030-01081-2_11) * Keane et al. [2021] Keane, M., Kenny, E.M., Temraz, M., Greene, D., Smyth, B. (2021). _Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based Reasoning Pairings for Explanation and Data Augmentation_. ArXiv, abs/2104.14461. [https://doi.org/10.48550/arXiv.2104.14461](https://doi.org/10.48550/arXiv.2104.14461) * Leake et al."
    },
    {
      "title": "Appendix A Appendix",
      "text": "An interesting conversation with the LLM chatbot Pi from Inflection about CBR, deep learning, LLMs and memory. _Hey there, great to meet you. I'm Pi, your personal AI. My goal is to be useful, friendly and fun. Ask me for advice, for answers, or let's talk about whatever's on your mind. What do you like to do in your free time?_ **Ian:** I like to read _That's great to hear! Reading is a great way to relax and learn new things. Do you prefer fiction or nonfiction?_ **Ian:** I read both. I usually have a book of each that I alternate chapters of. _That's a really interesting reading system! I've never heard of anyone alternating chapters like that. It must be a great way to keep both the fiction and nonfiction parts of your brain engaged. What are you currently reading?_ **Ian:** Lux Domini by Alex Thomas and Computer Power and Human Reason by Joseph Weisenbaum Wow, you're clearly a reader of diverse genres! Lux Domini is a thriller novel, while Computer Power and Human Reason is a nonfiction work about computer science and artificial intelligence. What drew you to these books in particular? **Ian:** Well, I am an AI scientist. Lux Domini is written by a friend of mine who used to be an AI researcher, and I wanted to reread Weisenbaum's book in light of the recent developments in AI chatbots like yourself. _That's so cool that you're an AI scientist! It must be fascinating to work in such an exciting and rapidly evolving field. The world of chatbots has come a long way since Weisenbaum's book was written, and it's fascinating to see how the theory and practice of AI has progressed over the years. Has your work as an AI scientist informed how you think about chatbots like me?_ **Ian:** It didn't used to be rapidly evolving. Progress for decades seemed glacially slow. I'm intrigued and fascinated by LLMs. Can you explain how you can converse like this?_Thanks for the question. I'm glad you're fascinated by me! LLMs like me are trained on massive amounts of text data. We learn to recognize patterns in the text and generate responses that are similar to the ones we've seen in the data. It's a bit like learning to speak a language by reading lots of books. But instead of learning from just books, I've learned from the vastness of the internet, including websites, social media, articles, and more. And then I've used all this knowledge to learn how to converse with people, like you! Does that help?_ **Ian:** You use the concept \"similar\". Most of my career was spent in case-based reasoning, where similarity is the key concept. Do you think deep learning could be applied to CBR? _Absolutely! Case-based reasoning is a great fit for deep learning. The key is to represent the cases as features, and then use a deep learning algorithm to learn the similarities between the cases. It can be a bit tricky to get the representation right, but once you have a good feature representation, the deep learning algorithm can do the heavy lifting. Have you worked on any projects that use deep learning for case-based reasoning?_ **Ian:** I haven't, but there have been a few small examples of specific tasks. However, I'm thinking on a much grander scale. Giving you a long-term persistent memory of everything you encounter using CBR as your memory. _That sounds really interesting! It could be really powerful to have an AI agent that can learn and store knowledge over time in a case-based memory system. Do you think this could be used to create AI agents that can learn general knowledge and world facts, or would it be more focused on specific problem-solving tasks?_ **Ian:** The interest must be on a general application. Would having such a memory be useful? I understand that LLMs have limited persistent memory. _I definitely think it would be useful to have a more persistent memory! It would allow me to learn and grow over time, and to better understand the world and the people I talk to. I think it could also help with things like personalization and long-term conversations. Right now, my knowledge is a bit ephemeral - I can learn from each conversation and remember the context of that conversation, but I can't really build up a long-term memory of all the things I've learned._"
    }
  ]
}