{
  "title": "Iterative Translation Refinement with Large Language Models",
  "authors": [
    "Pinzhen Chen",
    "Zhicheng Guo",
    "Barry Haddow",
    "Kenneth Heafield"
  ],
  "abstract": "\n We propose iteratively prompting a large language model to self-correct a translation, with inspiration from their strong language understanding and translation capability as well as a human-like translation approach. Interestingly, multi-turn querying reduces the output's string-based metric scores, but neural metrics suggest comparable or improved quality. Human evaluations indicate better fluency and naturalness compared to initial translations and even human references, all while maintaining quality. Ablation studies underscore the importance of anchoring the refinement to the source and a reasonable seed translation for quality considerations. We also discuss the challenges in evaluation and relation to human performance and translationese. \n",
  "references": [
    {
      "id": null,
      "title": "Iterative Translation Refinement with Large Language Models",
      "authors": [
        "Pinzhen Chen",
        "Zhicheng Guo",
        "Barry Haddow",
        "Kenneth Heafield"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Sweta Agrawal",
        "Chunting Zhou",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Marjan Ghazvininejad",
        "; Bahdanau",
        "Kyunghyun Dzmitry",
        "Yoshua Cho",
        "Bengio"
      ],
      "year": "2015",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Corpus-based Translation Studies: The Challenges that Lie Ahead. Benjamins Translation Library",
      "authors": [
        "Mona Baker"
      ],
      "year": "1996",
      "venue": "Corpus-based Translation Studies: The Challenges that Lie Ahead. Benjamins Translation Library",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "How human is machine translationese? comparing human and machine translations of text and speech",
      "authors": [
        "Bizzoni"
      ],
      "year": "2020",
      "venue": "Proceedings of the 17th International Conference on Spoken Language Translation",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Language models are few-shot learners",
      "authors": [
        "Brown"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Findings of the WMT 2018 shared task on automatic postediting",
      "authors": [
        "Chatterjee"
      ],
      "year": "2018",
      "venue": "Proceedings of the Third Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Alexandra Birch, and Kenneth Heafield. 2021. The University of Edinburgh's English-German and English-Hausa submissions to the WMT21 news translation task",
      "authors": [
        "Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the Sixth Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Synchronous refinement for neural machine translation",
      "authors": [
        "Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Towards debiasing translation artifacts",
      "authors": [
        "Dutta Chowdhury"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Results of WMT22 metrics shared task: Stop using BLEU -neural metrics are better and more robust",
      "authors": [
        "Freitag"
      ],
      "year": "2019",
      "venue": "Proceedings of the Seventh Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Translationese in Swedish novels translated from English",
      "authors": [
        "Martin Gellerstam"
      ],
      "year": "1986",
      "venue": "Translation studies in Scandinavia: Proceedings from the Scandinavian Symposium on Translation Theory II",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Levenshtein transformer",
      "authors": [
        "Gu"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "A post-editing dataset in the legal domain: Do we underestimate neural machine translation quality?",
      "authors": [
        "Ive"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "MS-UEdin submission to the WMT2018 APE shared task: Dual-source transformer for automatic post-editing",
      "authors": [
        "Junczys-Dowmunt",
        "Grundkiewicz ; Junczys-Dowmunt",
        "Marcin",
        "Roman Grundkiewicz"
      ],
      "year": "2018",
      "venue": "Proceedings of the Third Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Scaling laws for neural language models",
      "authors": [
        "Kaplan"
      ],
      "year": "1994",
      "venue": "Proceedings of the Twelfth AAAI National Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Large language models are stateof-the-art evaluators of translation quality",
      "authors": [
        "Federmann ; Kocmi",
        "Tom Kocmi",
        "Christian Federmann",
        "; Kocmi",
        "Rachel Tom",
        "Ondřej Bawden",
        "Anton Bojar",
        "Christian Dvorkovich",
        "Mark Federmann",
        "Thamme Fishel",
        "Yvette Gowda",
        "Roman Graham",
        "Barry Grundkiewicz",
        "Haddow"
      ],
      "year": "2022",
      "venue": "Proceedings of the Seventh Conference on Machine Translation. [Koppel and Ordan2011] Koppel, Moshe and Noam Ordan. 2011. Translationese and its dialects",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Deterministic nonautoregressive neural sequence modeling by iterative refinement",
      "authors": [
        "Lee"
      ],
      "year": "2012",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Liping Xie, and Dacheng Tao. 2023. Error analysis prompting enables human-like translation evaluation in large language models: A case study on ChatGPT",
      "authors": [
        "Lu"
      ],
      "year": "2023",
      "venue": "Liping Xie, and Dacheng Tao. 2023. Error analysis prompting enables human-like translation evaluation in large language models: A case study on ChatGPT",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Pre-translation for neural machine translation",
      "authors": [
        "Niehues"
      ],
      "year": "2016",
      "venue": "Proceedings of the 26th International Conference on Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Novak"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "The transference architecture for automatic post-editing",
      "authors": [
        "Pal"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "BLEU: a method for automatic evaluation of machine translation",
      "authors": [
        "Papineni"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Maja ; Popović",
        "Alec Radford",
        "Jeff Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2017",
      "venue": "Proceedings of the Second Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "2023a. Do GPTs produce less literal translations?",
      "authors": [
        "Raunak"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Hany Hassan Awadallah, and Arul Menezes. 2023b. Leveraging GPT-4 for automatic translation postediting",
      "authors": [
        "Raunak"
      ],
      "year": "2023",
      "venue": "Hany Hassan Awadallah, and Arul Menezes. 2023b. Leveraging GPT-4 for automatic translation postediting",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "COMET: A neural framework for MT evaluation",
      "authors": [
        "Rei"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Statistical phrase-based postediting",
      "authors": [
        "Simard"
      ],
      "year": "2007",
      "venue": "Human Language Technologies 2007: The Conference of the North American Chapter",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Cross-Linguistic Variation in System and Text: A Methodology for the Investigation of Translations and Comparable Texts",
      "authors": [
        "Elke Teich"
      ],
      "year": "2003",
      "venue": "Cross-Linguistic Variation in System and Text: A Methodology for the Investigation of Translations and Comparable Texts",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Post-editese: An exacerbated translationese",
      "authors": [
        "Antonio Toral"
      ],
      "year": "2019",
      "venue": "Proceedings of Machine Translation Summit XVII",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Facebook AI's WMT21 news translation task submission",
      "authors": [
        "Tran"
      ],
      "year": "2017",
      "venue": "Proceedings of the Sixth Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Prompting PaLM for translation: Assessing strategies and performance",
      "authors": [
        "Vilar"
      ],
      "year": "2021",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "HW-TSC's participation in the WMT 2021 news translation shared task",
      "authors": [
        "Wei"
      ],
      "year": "2021",
      "venue": "Proceedings of the Sixth Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Chain of thought prompting elicits reasoning in large language models",
      "authors": [
        "Wei"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "EDITOR: An edit-based transformer with repositioning for neural machine translation with soft lexical constraints",
      "authors": [
        "Carpuat ; Xu",
        "Weijia Xu",
        "Marine Carpuat"
      ],
      "year": "2021",
      "venue": "EDITOR: An edit-based transformer with repositioning for neural machine translation with soft lexical constraints",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback",
      "authors": [
        "Xu"
      ],
      "year": "2023",
      "venue": "INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Prompting large language model for machine translation: A case study",
      "authors": [
        "Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Iterative Translation Refinement With Large Language Models",
      "text": "Pinzhen Chen\\({}^{1}\\) Zhicheng Guo\\({}^{2}\\) Barry Haddow\\({}^{1}\\) Kenneth Heafield\\({}^{1}\\) \\({}^{1}\\)School of Informatics, University of Edinburgh \\({}^{2}\\)Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University {pinzhen.chen,bhaddow,kenneth.heafield}@ed.ac.uk guo-zc21@mails.tsinghua.edu.cn"
    },
    {
      "title": "Abstract",
      "text": "We propose iteratively prompting a large language model to self-correct a translation, with inspiration from their strong language understanding and translation capability as well as a human-like translation approach. Interestingly, multi-turn querying reduces the output's string-based metric scores, but neural metrics suggest comparable or improved quality. Human evaluations indicate better fluency and naturalness compared to initial translations and even human references, all while maintaining quality. Ablation studies underscore the importance of anchoring the refinement to the source and a reasonable seed translation for quality considerations. We also discuss the challenges in evaluation and relation to human performance and translationese. + Footnote †: © 2024 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CC-BY-ND."
    },
    {
      "title": "1 Introduction",
      "text": "Large language models (LLMs), e.g. generative pre-trained Transformers (GPT), have made notable advancements in natural language processing (Radford et al., 2019; Brown et al., 2020; Kaplan et al., 2020; Ouyang et al., 2022). In machine translation (MT), where the convention is to use an encoder-decoder architecture to deal with source and target sentences respectively (Bahdanau et al., 2015; Vaswani et al., 2017), recent papers have examined the feasibility of LLM prompting for translation (Vilar et al., 2023; Zhang et al., 2023; Hendy et al., 2023; Agrawal et al., 2023). With autoregressive decoding being the convention, machine translation models yield output in a single attempt, and so do post-editing models. Rather, a human translator can read and edit translations repeatedly, or even pass the outcome to another translator for a second opinion. We explore such an iterative refinement process with LLMs, where the proposed method simply feeds a source-translation pair into an LLM for an improved translation in multiple rounds. It is worth noting that this method can be applied to an initial translation from any model, not just LLM outputs. We further conduct a qualitative evaluation of the outputs. Our approach offers two insights from a fluency and naturalness perspective: 1) LLMs are pre-trained on natural texts that are orders of magnitude larger than traditional MT data, and 2) the method does not require complicated prompt engineering, yet allows for iterative and arbitrary rephrasing compared to automatic post-editing, which is limited to token-level error correction without style editing (Ive et al., 2020). Empirical results show that the refinement procedure introduces significant textual changes reflected by the drop in BLEU and chrF++, but attains similar or higher COMET scores compared to initial translations. Native speakers prefer refined outputs in terms of fluency and naturalness when compared with GPT translations and even human references. Reference-based human evaluation confirms that such gains are made without sacrificing general quality. As corroborated by recent works, automatic metrics like BLEU and COMET are witnessed to move in opposite directions (Freitag et al., 2019; Freitag et al., 2022). Our human-like LLM prompting method contributes to translation naturalness which can enhance utility as perceived by the target language users. On a broader scope, this work touches on the concept of involving LLMs in a collaborative translation editing strategy. [MISSING_PAGE_FAIL:2] the LLM _Translate_ query as the seed translation to be improved upon. We do not keep the query (multi-turn) history so as to prevent an LLM from seeing that the previous translation is produced by itself. In experiments later on, we also tested with translations from encoder-decoder systems that participated in WMT, human references, and online systems. Overall, translation refinement is iterated four times at maximum considering the API costs."
    },
    {
      "title": "Evaluation Setup",
      "text": "We consider four automatic metrics: string-based BLEU Papineni et al. (2002) and chrF++ Popovic (2017) as well as embedding-based \\(\\text{COMET}_{\\text{DA}}\\) and \\(\\text{COMET}_{\\text{QE}}\\)Rei et al. (2020). The difference between the DA and QE versions is that \\(\\text{COMET}_{\\text{DA}}\\) requires a source, a translation, and a reference, whereas \\(\\text{COMET}_{\\text{QE}}\\) is reference-free. BLEU and chrF++ are as implemented in the sacrebleu toolkit.2 We also use this toolkit to obtain test sets with references as well as past WMT systems' outputs. Specifically for tokenization in BLEU calculation, we use \"zh\" for Chinese, \"ja-mecab\" for Japanese, and \"13a\" for the rest. The BLEU and chrF++ signatures are footnoted.3\\({}^{,}\\)4 For COMET metrics, we used the official implementation released by the authors.5 Footnote 2: [https://github.com/mjpost/sacrebleu](https://github.com/mjpost/sacrebleu) Footnote 3: #:!|c:mixed|e:noltok:13a|s:exp|v:2.3.1 Footnote 4: #:!|c:mixed|e:yes|nc:6|mw:2|s:no|v:2.3.1 Footnote 5: [https://github.com/Unbabel/COMET](https://github.com/Unbabel/COMET)"
    },
    {
      "title": "Refinement Results",
      "text": "Wmt21We first experiment with en\\(\\leftrightarrow\\)de and en\\(\\leftrightarrow\\)zh from WMT21, which are high-resource languages in terms of both translation data and LLM training data. We run all five prompts and display results in Table 2. For iterative refinement and paraphrasing experiments, the best iteration is picked according to \\(\\text{COMET}_{\\text{QE}}\\). We observe that the refined translations record a drastic drop in string-based metrics compared to initial translations, indicating lexical and structural variations. In terms of \\(\\text{COMET}_{\\text{DA}}\\), refined outputs surpass initial GPT translations in three out of four cases, and in terms of \\(\\text{COMET}_{\\text{QE}}\\), the refinement strategy ends as the highest with substantial improvement for into-English directions. As a contrasting experiment, _Paraphrase_ sees a decline in all metrics, suggesting the importance of feeding the source input as an anchor during iterations to prevent semantic drift. Wmt22Moving to lower-resourced languages with non-English translation, we gather numbers for three translation directions from WMT22 in Table 3. Since \\(\\text{Refine}_{\\text{Random}}\\) results are not desirable for WMT21, we omit experiments with this. The overall pattern remains the same as before: _Refine_ works best, obtaining higher \\(\\text{COMET}_{\\text{QE}}\\) than vanilla translations and \\(\\text{Refine}_{\\text{Contrast}}\\). Also, the reduction in string-based scores becomes less obvious, which might be attributed to seed GPT translations in lesser-resourced languages being lower in quality in the beginning. Online systems, encoder-decoder systems, and human translationsIn addition to translation refinement from GPT-3.5 itself, we also apply our refinement calls to outputs from conventional MT systems and human translators. These translations can represent genuine errors, if any, introduced during the translation process. Out of the seven WMT21 submissions, we select outputs from four models built by research labs that, based on human evaluation, have been ranked at significantly different positions on the German-to-English leaderboard: Tencent Wang et al. (2021), Facebook AI Tran et al. (2021), Edinburgh Chen et al. (2021), and Huawei TSC Wei et al. (2021). These are competitive systems built with data augmentation, multilingualism, ensembling, re-ranking, etc. We then include two online engines used in WMT 2021: Online-A and Online-Y. Finally, human reference \"B\" is added so that we can experiment with our refinement strategy on human translations.6 References \"A\" and \"B\" are sourced from different translation agencies Farhad et al. (2021). Footnote 6: The overview paper of WMT 2021 states that “for German\\(\\leftrightarrow\\)English, the ‘B’ reference was found to be a post-edited version of one of the participating online systems”. We discover that it refers to English\\(\\rightarrow\\)German only, and German\\(\\rightarrow\\)English is not affected. We report automatic scores from the refinement process in Table 4. A pattern similar to previous GPT translation refinement is noticed: for five out of seven WMT entries, the refinement strategy reaches a higher COMETQE score, surprisingly, with up to one-third drop in BLEU. _Refine_Contrast in all but one system surpass _Refine_, and without the initial translation, _Paraphrase_ iterations record the lowest scores compared to the original submissions and refinements."
    },
    {
      "title": "4 Human Evaluation",
      "text": "String-based and neural scores are observed to vary in opposite directions, which may suggest volatile changes in texts. Since it is questionable to conclude a quality degradation in this case, we set up human evaluations to measure two characteristics in the refined translations: text naturalness and overall quality. Human evaluators involved in this study are practitioners in the field of natural language processing but are unaware of the goal of this study."
    },
    {
      "title": "Fluency And Naturalness",
      "text": "We mimic the human evaluation of fluency in Lembersky et al. (2012, p819). Native speakers of the target language are with two translations but without the source sentence; then we ask \"Please choose the translation that is more fluent, natural, and reflecting better use of $language\\(\\rangle\\)\", where $language\\(\\rangle\\) is substituted with the target language name. The evaluator has three options: they can select one of the two translations, or a \"tie\" if they consider both equally (un)natural. We conduct such pairwise evaluation to compare the first-round output from _Refine_Contrast against human references, as well as against _Translate_ separately. We evaluate 50 samples from en\\(\\leftrightarrow\\)de and en\\(\\leftrightarrow\\)zh experiments in Section 3.3, and report in Figure 1 (left). Native speakers prefer _Refine_Contrast to vanilla _Translate_ in all four directions, and even favour _Refine_Contrast over human references when translating into English. It demonstrates that our simple strategy enhances the naturalness of GPT outputs and that WMT human references could be less favourable than GPT outputs in some cases."
    },
    {
      "title": "Overall Quality",
      "text": "We also evaluate for general quality as a safeguard. In this setup, a source sentence and two translations are given to an evaluator who is fluent in both languages. They are asked to pick the translation with better quality or indicate a tie. We only evaluated two translation directions, English to and from Chinese, due to the limited availability of bilingual speakers. Similar to the previous evaluation, we compare _Refine_Contrast against human references, as well as _Refine_Contrast against _Translate_ separately. We report evaluator preferences in Figure 1 (right). It shows that GPT _Refine_ attains slightly better performance in zh\\(\\rightarrow\\)en and similar performance in en\\(\\rightarrow\\)zh when compared with human references. On the other hand, it is more favourable than GPT _Translate_ in terms of human judgements. Combining evaluation outcomes, we conclude that the refinement strategy could improve the target-side naturalness without undermining general quality."
    },
    {
      "title": "5 Analysis And Discussions",
      "text": ""
    },
    {
      "title": "Performance Through Iterations",
      "text": "To investigate the behaviour of refinement strategies through different iterations, we plot BLEU, \\(\\text{COMET}_{\\text{DA}}\\), and \\(\\text{COMET}_{\\text{QE}}\\) at different iterations in Figure 2 for four translation directions: en\\(\\leftrightarrow\\)de and en\\(\\leftrightarrow\\)zh. We find that _Refine_ and _Refine_Contrast usually attain their best after undergoing more than one refinement iteration, showing superiority to one-off editing.7 However, in almost all _Paraphrase_ experiments, scores decrease monotonically, indicating that semantics drift away as paraphrasing iterates. Moreover, _Refine_Random results start low, gradually catch up, but never reach as high as _Refine_ or _Refine_Contrast. This means that iterative refinement is indeed useful in fixing translations, but starting with a reasonable translation is also crucial for obtaining a strong result. Footnote 7: The first iteration is equivalent to a one-off translation editing using an LLM."
    },
    {
      "title": "Diverging Automatic Scores",
      "text": "According to automatic string-based metrics, our queries deliver lower-quality translations through iterations, but \\(\\text{COMET}_{\\text{DA}}\\) scores remain comparable and \\(\\text{COMET}_{\\text{QE}}\\) scores mostly increase. We argue that the string-based metrics might not accurately indicate quality, but rather reflect text variations with respect to the reference. We further verified this via human evaluation that fluency and overall quality are not impacted. In Table 5 we show outputs from different strategies for a single source input, where a native speaker marked preference for _Refine_Contrast. It illustrates that the word choice is diverse for both directions and specifically for Chinese\\(\\rightarrow\\)English, there are substantial structural changes. The huge variety in expressions across translations can result in low BLEU with respect to human references, but without much change in meaning, for instance, as in Table 2 where BLEU can decline up to one-third, but neural metric scores change little. In the field of MT, a leap in BLEU is usually associated with performance improvement; however, in our case, a drop cannot be simply interpreted as performance degradation. This can be attributed to the lexical and structural diversity in the refined translations."
    },
    {
      "title": "Human Performance",
      "text": "A human translator is deemed to be fluent in their native language, which intuitively is difficult for a model to compete with. In our human evalua Figure 1: Human preferences on fluency and naturalness (source-free, left) and overall quality (source-based, right). tion, GPT fluency can be as good or even better than reference translations--we offer two possible explanations. First, the WMT references might have been created by translators with varying expertise, which may not represent upper-bound human performance, especially when compared with advanced LLMs. More importantly, translations can exhibit awkwardness in word and syntax choices, potentially due to source language interference or \"shining through\" (Gellerstam, 1986; Teich, 2003)."
    },
    {
      "title": "Relation To Translationese",
      "text": "Both human and machine translations might be more explicit, language-normalized, and simpler (Baker, 1996; Koppel and Ordan, 2011). On a broader scope, translationese is regarded as the distinct features in translations to include influences from both the source and target sides. Although MT normally learns from human translation data, researchers found that human and machine translation patterns do not fully overlap (Bizzoni et al., 2020). While translationese occurs in translations inevitably, consumers could prefer translations that are more natural in their native language, provided that the semantics and utility are preserved. From a narrow aspect, our method relates to machine translationese mitigation in terms of reducing unnaturalness and literalness, instead of focusing on state-of-the-art metric scores. It may be viable to create diverse translations through iterations, as we observe huge changes in BLEU scores. Measuring these using automatic metrics at the moment is challenging, especially given that most translation metrics are reference-based, where the reference can be translationese-prone in the first place. COMETQE might be more robust to this end. Figure 2: BLEU, COMETQA, and COMETQE at different refinement and paraphrase iterations for high-resource translation. [MISSING_PAGE_FAIL:7] Raunak et al. (2023) formalized post-editing as a chain-of-thought process Wei et al. (2022) with GPT-4 and achieved promising results. Different from their focus, our work features the iterative refinement process as a means to enhance naturalness and fluency. Our work reveals that iterated refinement is better than one-off editing. The observed improvement, especially for into-English, may be attributed to the abundant English pre-training data available for LLMs. To the best of our knowledge, although the concept of iterative refinement is not new, ours is the pioneering paper in applying such strategies to LLMs for translation."
    },
    {
      "title": "7 Conclusion And Future Work",
      "text": "We presented a simple way to leverage an LLM for translation refinement, which greatly helps fluency and naturalness. It is shown that our method maintains translation quality and introduces lexical and structural changes, especially for high-resource into-English translation. We have also discussed the potential of using our work to obtain diverse, fluent translations that are less translationese, as well as the limitation in automatic metrics to measure this. On a broader note, this work connects to the concept of using LLMs to imitate collaborative translation refinement. Yet, it is important to acknowledge the high cost of running a multi-round LLM refinement. Future work can explore sentence-level refinement decisions to reduce cost."
    },
    {
      "title": "Acknowledgement",
      "text": "We express our gratitude to the reviewers of this paper for their detailed and invaluable feedback and suggestions. The work also benefited from discussions with Nikolay Bogoychev and Biao Zhang. We are grateful to Laurie Burchell, Ziqin Fang, Matthias Lindemann, and Jonas Waldendorf for their participation in the human evaluation. This work is funded by UK Research and Innovation (UKRI) under the UK government's Horizon Europe funding guarantee [grant number 10052546]."
    },
    {
      "title": "References",
      "text": "* [Agrawal et al.2023] Agrawal, Sweta, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023. In-context examples selection for machine translation. In _Findings of the Association for Computational Linguistics: ACL 2023_. * [Bahdanau et al.2015] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In _3rd International Conference on Learning Representations_. * [Baker1996] Baker, Mona, 1996. _Corpus-based Translation Studies: The Challenges that Lie Ahead_. Benjamins Translation Library. John Benjamins Publishing Company. * [Bizzoni et al.2020] Bizzoni, Yuri, Tom S Juzek, Cristina Espana-Bonet, Koel Dutta Chowdhury, Josef van Genabith, and Elke Teich. 2020. How human is machine translationese? comparing human and machine translations of text and speech. In _Proceedings of the 17th International Conference on Spoken Language Translation_. * [Brown et al.2020] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In _Advances in Neural Information Processing Systems_. * [Chatterjee et al.2018] Chatterjee, Rajen, Matteo Negri, Raphael Rubino, and Marco Turchi. 2018. Findings of the WMT 2018 shared task on automatic post-editing. In _Proceedings of the Third Conference on Machine Translation_. * [Chen et al.2021] Chen, Pinzhen, Jindrich Helcl, Ulrich Germann, Laurie Burchell, Nikolay Bogoychev, Antonio Valerio Miceli Barone, Jonas Waldendorf, Alexandra Birch, and Kenneth Heafield. 2021. The University of Edinburgh's English-German and English-Hausa submissions to the WMT21 news translation task. In _Proceedings of the Sixth Conference on Machine Translation_. * [Chen et al.2022] Chen, Kehai, Masao Utiyama, Eiichiro Sumita, Rui Wang, and Min Zhang. 2022. Synchronous refinement for neural machine translation. In _Findings of the Association for Computational Linguistics: ACL 2022_. * [Chollampatt et al.2020] Chollampatt, Shamil, Raymond Hendy Susanto, Liling Tan, and Ewa Szymanska. 2020. Can automatic post-editing improve NMT? In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing_. * [Chowdhery et al.2022] Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling language modeling with pathways. _arXiv preprint_. * [Dutta Chowdhury et al.2022] Dutta Chowdhury, Koel, Rricha Jalota, Cristina Espana-Bonet, and Josef Genabith. 2022. Towards debiasing translation artifacts. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_. * [Farhad et al.2021] Farhad, Akhbardeh, Arkhangorodsky Arkady, Biesialska Magdalena, Bojar Ondrej, Chatterjee Rajen, Chaudhary Vishrav, Marta R Costajussa, Espana-Bonet Cristina, Fan Angela, Federmann Christian, et al. 2021. Findings of the 2021 conference on machine translation (WMT21). In _Proceedings of the Sixth Conference on Machine Translation_. * [Freitag et al.2019] Freitag, Markus, Isaac Caswell, and Scott Roy. 2019. APE at scale and its implications on MT evaluation biases. In _Proceedings of the Fourth Conference on Machine Translation_. * neural metrics are better and more robust. In _Proceedings of the Seventh Conference on Machine Translation_. * [Gellerstam1986] Gellerstam, Martin. 1986. Translationese in Swedish novels translated from English. In _Translation studies in Scandinavia: Proceedings from the Scandinavian Symposium on Translation Theory II_. CWK Gleerup. * [Gu et al.2019] Gu, Jiatao, Changhan Wang, and Junbo Zhao. 2019. Levenshtein transformer. In _Advances in Neural Information Processing Systems_. * [Hendy et al.2023] Hendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are GPT models at machine translation? a comprehensive evaluation. _arXiv preprint_. * [Ive et al.2020] Ive, Julia, Lucia Specia, Sara Szoc, Tom Vanallemeersch, Joachim Van den Bogaert, Eduardo Farah, Christine Maroti, Artur Ventura, and Maxim Khalilov. 2020. A post-editing dataset in the legal domain: Do we underestimate neural machine translation quality? In _Proceedings of the Twelfth Language Resources and Evaluation Conference_. * [Jiao et al.2023] Jiao, Wenxiang, Wenxuan Wang, Jen tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is ChatGPT a good translator? Yes with GPT-4 as the engine. _arXiv preprint_. * [Junczys-Dowmunt and Grundkiewicz2018] Junczys-Dowmunt, Marcin and Roman Grundkiewicz. 2018. MS-UEdin submission to the WMT2018 APE shared task: Dual-source transformer for automatic post-editing. In _Proceedings of the Third Conference on Machine Translation_. * [Kaplan et al.2020] Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. _arXiv preprint_. * [Knight and Chander1994] Knight, Kevin and Ishwar Chander. 1994. Automated postediting of documents. In _Proceedings of the Twelfth AAAI National Conference on Artificial Intelligence_. * [Kocmi and Federmann2023] Kocmi, Tom and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. _arXiv preprint_. * [Kocmi et al.2022] Kocmi, Tom, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, et al. 2022. Findings of the 2022 conference on machine translation (WMT22). In _Proceedings of the Seventh Conference on Machine Translation_. * [Koppel and Ordan2011] Koppel, Moshe and Noam Ordan. 2011. Translationese and its dialects. In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. * [Lee et al.2018] Lee, Jason, Elman Mansimov, and Kyunghyun Cho. 2018. Deterministic non-autoregressive neural sequence modeling by iterative refinement. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_. * [Lembersky et al.2012] Lembersky, Gennadi, Noam Ordan, and Shuly Wintner. 2012. Language Models for Machine Translation: Original vs. Translated Texts. _Computational Linguistics_. * [Lu et al.2023] Lu, Qingyu, Baopu Qiu, Liang Ding, Liping Xie, and Dacheng Tao. 2023. Error analysis prompting enables human-like translation evaluation in large language models: A case study on ChatGPT. _arXiv preprint_. * [Niehues et al.2016] Niehues, Jan, Eunah Cho, Thanh-Le Ha, and Alex Waibel. 2016. Pre-translation for neural machine translation. In _Proceedings of the 26th International Conference on Computational Linguistics_. * [Novak et al.2016] Novak, Roman, Michael Auli, and David Grangier. 2016. Iterative refinement for machine translation. _arXiv preprint_. * [Ouyang et al.2022] Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. In _Advances in Neural Information Processing Systems_. * [Pal et al.2020] Pal, Santanu, Hongfei Xu, Nico Herbig, Sudip Kumar Naskar, Antonio Kruger, and Josef van Genabith. 2020. The transference architecture for automatic post-editing. In _Proceedings of the 28th International Conference on Computational Linguistics_. * [Papineni et al.2002] Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_. * [Popovic2017] Popovic, Maja. 2017. chrF++: words helping character n-grams. In _Proceedings of the Second Conference on Machine Translation_. * [Radford et al.2019] Radford, Alec, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. openai.com. * [Raunak et al.2023a] Raunak, Vikas, Arul Menezes, Matt Post, and Hany Hassan. 2023a. Do GPTs produce less literal translations? In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics_. * [Raunak et al.2023b] Raunak, Vikas, Amr Sharaf, Hany Hassan Awadallah, and Arul Menezes. 2023b. Leveraging GPT-4 for automatic translation post-editing. _arXiv preprint_. * [Rei et al.2020] Rei, Ricardo, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing_. * [Simard et al.2007] Simard, Michel, Cyril Goutte, and Pierre Isabelle. 2007. Statistical phrase-based post-editing. In _Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics_. * [Teich2003] Teich, Elke. 2003. _Cross-Linguistic Variation in System and Text: A Methodology for the Investigation of Translations and Comparable Texts_. De Gruyter Mouton. * [Toral2019] Toral, Antonio. 2019. Post-editese: An exacerbated translationese. In _Proceedings of Machine Translation Summit XVII_. * [Tran et al.2021] Tran, Chau, Shruti Bhosale, James Cross, Philipp Koehn, Sergey Edunov, and Angela Fan. 2021. Facebook AI's WMT21 news translation task submission. In _Proceedings of the Sixth Conference on Machine Translation_. * [Vaswani et al.2017] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In _Advances in Neural Information Processing Systems_. * [Vilar et al.2023] Vilar, David, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2023. Prompting PaLM for translation: Assessing strategies and performance. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics_. * [Wang et al.2021] Wang, Longyue, Mu Li, Fangxu Liu, Shuming Shi, Zhaopeng Tu, Xing Wang, Shuanghi Wu, Jiali Zeng, and Wen Zhang. 2021. Tencent translation system for the WMT21 news translation task. In _Proceedings of the Sixth Conference on Machine Translation_. * [Wei et al.2021] Wei, Daimeng, Zongyao Li, Zhanglin Wu, Zhengzhe Yu, Xiaoyu Chen, Hengchao Shang, Jiaxin Guo, Minghan Wang, Lizhi Lei, Min Zhang, et al. 2021. HW-TSC's participation in the WMT 2021 news translation shared task. In _Proceedings of the Sixth Conference on Machine Translation_. * [Wei et al.2022] Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In _Advances in Neural Information Processing Systems_. * [Xu and Carpuat2021] Xu, Weijia and Marine Carpuat. 2021. EDITOR: An edit-based transformer with repositioning for neural machine translation with soft lexical constraints. _Transactions of the Association for Computational Linguistics_. * [Xu et al.2023] Xu, Wenda, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, and Lei Li. 2023. INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. _arXiv preprint_. * [Zhang et al.2023] Zhang, Biao, Barry Haddow, and Alexandra Birch. 2023. Prompting large language model for machine translation: A case study. In _Proceedings of the 40th International Conference on Machine Learning_."
    }
  ]
}