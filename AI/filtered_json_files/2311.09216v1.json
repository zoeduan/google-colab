{
  "title": "Assessing Translation capabilities of Large Language Models involving English and Indian Languages",
  "authors": [
    "Vandan Mujadia",
    "Ashok Urlana",
    "Penumalla Yash Bhaskar",
    "Aditya Pavani",
    "Kukkapalli Shravya",
    "Parameswari Krishnamurthy",
    "Dipti Misra"
  ],
  "abstract": "\n Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. In this work, our aim is to explore the multilingual capabilities of large language models by using machine translation as a task involving English and 22 Indian languages. We first investigate the translation capabilities of raw large language models, followed by exploring the in-context learning capabilities of the same raw models. We fine-tune these large language models using parameter efficient finetuning methods such as LoRA and additionally with full fine-tuning. Through our study, we have identified the best performing large language model for the translation task involving LLMs, which is based on LLaMA. Our results demonstrate significant progress, with average BLEU scores of 13.42, 15.93,  12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99, 42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for English to Indian languages on IN22 (conversational), IN22 (general), flores200dev, flores200-devtest, and newstest2019 testsets. Similarly, for Indian languages to English, we achieved average BLEU scores of 14. \n",
  "references": [
    {
      "id": null,
      "title": "Assessing Translation capabilities of Large Language Models involving English and Indian Languages",
      "authors": [
        "Vandan Mujadia",
        "Ashok Urlana",
        "Yash Bhaskar",
        "Aditya Pavani",
        "Kukkapalli Shravya",
        "Parameswari Krishnamurthy",
        "Dipti Misra"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Raj Dabre, and Anoop Kunchukuttan. 2023. Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages",
      "authors": [
        "Jay Ai4bharat",
        "Gala",
        "A Pranjal",
        "Chitale",
        "A K Raghavan",
        "Sumanth Doddapaneni",
        "Varun Gumma",
        "Aswanth Kumar",
        "Janki Nawale",
        "Anupama Sujatha",
        "Ratish Puduppully",
        "Pratyush Vivek Raghavan",
        "Kumar",
        "M Mitesh",
        "Khapra"
      ],
      "year": "",
      "venue": "Raj Dabre, and Anoop Kunchukuttan. 2023. Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Siamak Shakeri, and Yunhsuan Sung. 2023. Characterizing attribution and fluency tradeoffs for retrievalaugmented large language models",
      "authors": [
        "Renat Aksitov",
        "Chung-Ching Chang",
        "David Reitter"
      ],
      "year": "",
      "venue": "Siamak Shakeri, and Yunhsuan Sung. 2023. Characterizing attribution and fluency tradeoffs for retrievalaugmented large language models",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Falcon-40b: an open large language model with state-of-theart performance",
      "authors": [
        "Ebtesam Almazrouei",
        "Hamza Alobeidli",
        "Abdulaziz Alshamsi",
        "Alessandro Cappelli",
        "Ruxandra Cojocaru",
        "Merouane Debbah",
        "Etienne Goffinet",
        "Daniel Heslow",
        "Julien Launay",
        "Quentin Malartic"
      ],
      "year": "2023",
      "venue": "Falcon-40b: an open large language model with state-of-theart performance",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "authors": [
        "Yejin Bang",
        "Samuel Cahyawijaya",
        "Nayeon Lee",
        "Wenliang Dai",
        "Dan Su",
        "Bryan Wilie",
        "Holy Lovenia",
        "Ziwei Ji",
        "Tiezheng Yu",
        "Willy Chung"
      ],
      "year": "2023",
      "venue": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Exploring massively multilingual, massive neural machine translation",
      "authors": [
        "Ankur Bapna",
        "Orhan Firat"
      ],
      "year": "2019",
      "venue": "Google AI Blog",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "André Martins",
      "authors": [
        "Loïc Barrault",
        "Ondřej Bojar",
        "Fethi Bougares",
        "Rajen Chatterjee",
        "Marta R Costa-Jussà",
        "Christian Federmann",
        "Mark Fishel",
        "Alexander Fraser",
        "Yvette Graham",
        "Paco Guzman",
        "Barry Haddow",
        "Matthias Huck",
        "Antonio Jimeno Yepes",
        "Philipp Koehn"
      ],
      "year": "2020",
      "venue": "Proceedings of the Fifth Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Statistical language model adaptation: review and perspectives",
      "authors": [
        "Jerome R Bellegarda"
      ],
      "year": "2004",
      "venue": "Statistical language model adaptation: review and perspectives",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "A neural probabilistic language model. Advances in neural information processing systems",
      "authors": [
        "Yoshua Bengio",
        "Réjean Ducharme",
        "Pascal Vincent"
      ],
      "year": "2000",
      "venue": "A neural probabilistic language model. Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "A survey on evaluation of large language models",
      "authors": [
        "Yupeng Chang",
        "Xu Wang",
        "Jindong Wang",
        "Yuan Wu",
        "Kaijie Zhu",
        "Hao Chen",
        "Linyi Yang",
        "Xiaoyuan Yi",
        "Cunxiang Wang",
        "Yidong Wang"
      ],
      "year": "2023",
      "venue": "A survey on evaluation of large language models",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Efficient and effective text encoding for chinese llama and alpaca",
      "authors": [
        "Yiming Cui",
        "Ziqing Yang",
        "Xin Yao"
      ],
      "year": "2023",
      "venue": "Efficient and effective text encoding for chinese llama and alpaca",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Building multilingual machine translation systems that serve arbitrary xy translations",
      "authors": [
        "Raj Dabre",
        "Chenhui Chu",
        "Anoop Kunchukuttan"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "NTREX-128 -news test references for MT evaluation of 128 languages",
      "authors": [
        "Christian Federmann",
        "Tom Kocmi",
        "Ying Xin"
      ],
      "year": "2022",
      "venue": "Proceedings of the First Workshop on Scaling Up Multilingual Evaluation",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Indictrans2: Towards highquality and accessible machine translation models for all 22 scheduled indian languages",
      "authors": [
        "Jay Gala",
        "Chitale",
        "A K Raghavan",
        "Sumanth Doddapaneni",
        "Varun Gumma",
        "Aswanth Kumar",
        "Janki Nawale",
        "Anupama Sujatha",
        "Ratish Puduppully",
        "Vivek Raghavan"
      ],
      "year": "2023",
      "venue": "Indictrans2: Towards highquality and accessible machine translation models for all 22 scheduled indian languages",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "The Flores-101 evaluation benchmark for low-resource and multilingual machine translation",
      "authors": [
        "Naman Goyal",
        "Cynthia Gao",
        "Vishrav Chaudhary",
        "Peng-Jen Chen",
        "Guillaume Wenzek",
        "Da Ju",
        "Sanjana Krishnan",
        "Marc'aurelio Ranzato",
        "Francisco Guzmán",
        "Angela Fan"
      ],
      "year": "2022",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": "10.1162/tacl_a_00474"
    },
    {
      "id": "b15",
      "title": "Hallucinations in large multilingual translation models",
      "authors": [
        "Duarte Nuno M Guerreiro",
        "Jonas Alves",
        "Barry Waldendorf",
        "Alexandra Haddow",
        "Pierre Birch",
        "André Ft Colombo",
        "Martins"
      ],
      "year": "2023",
      "venue": "Hallucinations in large multilingual translation models",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Making instruction finetuning accessible to non-English languages: A case study on Swedish models",
      "authors": [
        "Oskar Holmström",
        "Ehsan Doostmohammadi"
      ],
      "year": "2023",
      "venue": "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Mistral 7b",
      "authors": [
        "Alexandre Albert Q Jiang",
        "Arthur Sablayrolles",
        "Chris Mensch",
        "Devendra Bamford",
        "Diego Singh Chaplot",
        "Florian De Las Casas",
        "Gianna Bressand",
        "Guillaume Lengyel",
        "Lucile Lample",
        "Saulnier"
      ],
      "year": "2023",
      "venue": "Mistral 7b",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Efficient memory management for large language model serving with pagedattention",
      "authors": [
        "Woosuk Kwon",
        "Zhuohan Li",
        "Siyuan Zhuang",
        "Ying Sheng",
        "Lianmin Zheng",
        "Cody Hao Yu",
        "Joseph E Gonzalez",
        "Hao Zhang",
        "Ion Stoica"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning",
      "doi": "10.1145/3600006.3613165"
    },
    {
      "id": "b20",
      "title": "Peft: State-of-the-art parameterefficient fine-tuning methods",
      "authors": [
        "Sourab Mangrulkar",
        "Sylvain Gugger",
        "Lysandre Debut",
        "Younes Belkada",
        "Sayak Paul",
        "Benjamin Bossan"
      ],
      "year": "2022",
      "venue": "Peft: State-of-the-art parameterefficient fine-tuning methods",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Reliable and safe use of machine translation in medical settings",
      "authors": [
        "Nikita Mehandru",
        "Samantha Robertson",
        "Niloufar Salehi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",
      "doi": "10.1145/3531146.3533244"
    },
    {
      "id": "b22",
      "title": "Muhammad Saqib",
      "authors": [
        "Humza Naveed",
        "Asad Ullah Khan",
        "Shi Qiu"
      ],
      "year": "",
      "venue": "Saeed Anwar, Muhammad Usman, Nick Barnes, and Ajmal Mian. 2023. A comprehensive overview of large language models",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "Kishore Papineni",
        "Salim Roukos",
        "Todd Ward",
        "Wei-Jing Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Ebtesam Almazrouei, and Julien Launay. 2023a. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only",
      "authors": [
        "Guilherme Penedo",
        "Quentin Malartic",
        "Daniel Hesslow",
        "Ruxandra Cojocaru",
        "Alessandro Cappelli",
        "Hamza Alobeidli",
        "Baptiste Pannier"
      ],
      "year": "",
      "venue": "Ebtesam Almazrouei, and Julien Launay. 2023a. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Ebtesam Almazrouei, and Julien Launay. 2023b. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only",
      "authors": [
        "Guilherme Penedo",
        "Quentin Malartic",
        "Daniel Hesslow",
        "Ruxandra Cojocaru",
        "Alessandro Cappelli",
        "Hamza Alobeidli",
        "Baptiste Pannier"
      ],
      "year": "",
      "venue": "Ebtesam Almazrouei, and Julien Launay. 2023b. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "chrf: character n-gram f-score for automatic mt evaluation",
      "authors": [
        "Maja Popović"
      ],
      "year": "2015",
      "venue": "Proceedings of the tenth workshop on statistical machine translation",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "A call for clarity in reporting BLEU scores",
      "authors": [
        "Matt Post"
      ],
      "year": "2018",
      "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "COMET-22: Unbabel-IST 2022 submission for the metrics shared task",
      "authors": [
        "Ricardo Rei",
        "G C José",
        "Duarte De Souza",
        "Chrysoula Alves",
        "Ana C Zerva",
        "Taisiya Farinha",
        "Alon Glushkova",
        "Luisa Lavie",
        "Coheur",
        "F T André",
        "Martins"
      ],
      "year": "2022",
      "venue": "Proceedings of the Seventh Conference on Machine Translation (WMT)",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Neural machine translation models can learn to be few-shot learners",
      "authors": [
        "Raphael Reinauer",
        "Patrick Simianer",
        "Kaden Uhlig",
        "Johannes Em Mosig",
        "Joern Wuebker"
      ],
      "year": "2023",
      "venue": "Neural machine translation models can learn to be few-shot learners",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "On the origin of the early indian scripts",
      "authors": [
        "Richard Salomon"
      ],
      "year": "1995",
      "venue": "Journal of the American Oriental Society",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Bloom: A 176bparameter open-access multilingual language model",
      "authors": [
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilić",
        "Roman Hesslow",
        "Alexandra Castagné",
        "François Sasha Luccioni",
        "Matthias Yvon",
        "Gallé"
      ],
      "year": "2022",
      "venue": "Bloom: A 176bparameter open-access multilingual language model",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Overcoming language barriers in academia: Machine translation tools and a vision for a multilingual future",
      "authors": [
        "Emma Steigerwald",
        "Valeria Ramírez-Castañeda",
        "Y C Débora",
        "András Brandt",
        "Julie Teresa Báldi",
        "Lynne Shapiro",
        "Rebecca D Bowker",
        "Tarvin"
      ],
      "year": "2022",
      "venue": "BioScience",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "year": "",
      "venue": "Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava"
      ],
      "year": "",
      "venue": "Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models",
      "authors": [
        "Priyan Vaithilingam",
        "Tianyi Zhang",
        "Elena L Glassman"
      ],
      "year": "2022",
      "venue": "Chi conference on human factors in computing systems extended abstracts",
      "doi": "10.1145/3491101.3519665"
    },
    {
      "id": "b37",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "A survey on low-resource neural machine translation",
      "authors": [
        "Rui Wang",
        "Xu Tan",
        "Renqian Luo",
        "Tao Qin",
        "Tie-Yan Liu"
      ],
      "year": "2021",
      "venue": "A survey on low-resource neural machine translation",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "OpenICL: An open-source framework for in-context learning",
      "authors": [
        "Haoran Wu",
        "Wenxuan Wang",
        "Yuxuan Wan",
        "Wenxiang Jiao",
        "Michael Lyu",
        "; Zhenyu Wu",
        "Yaoxiang Wang",
        "Jiacheng Ye",
        "Zhiyong Wu",
        "Jiangtao Feng",
        "Jingjing Xu",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-demo.47"
    },
    {
      "id": "b40",
      "title": "The rise and potential of large language model based agents: A survey",
      "authors": [
        "Zhiheng Xi",
        "Wenxiang Chen",
        "Xin Guo",
        "Wei He",
        "Yiwen Ding",
        "Boyang Hong",
        "Ming Zhang",
        "Junzhe Wang",
        "Senjie Jin",
        "Enyu Zhou"
      ],
      "year": "2023",
      "venue": "The rise and potential of large language model based agents: A survey",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "A paradigm shift in machine translation: Boosting translation performance of large language models",
      "authors": [
        "Haoran Xu",
        "Young",
        "Jin Kim",
        "Amr Sharaf",
        "Hany Hassan Awadalla"
      ],
      "year": "2023",
      "venue": "A paradigm shift in machine translation: Boosting translation performance of large language models",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "A systematic evaluation of large language models for natural",
      "authors": [
        "Ni Xuanfan",
        "Li Piji"
      ],
      "year": "2023",
      "venue": "Proceedings of the 22nd Chinese National Conference on Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Prompting large language model for machine translation: A case study",
      "authors": [
        "Biao Zhang",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "year": "2023",
      "venue": "Prompting large language model for machine translation: A case study",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Opt: Open pre-trained transformer language models",
      "authors": [
        "Susan Zhang",
        "Stephen Roller",
        "Naman Goyal",
        "Mikel Artetxe",
        "Moya Chen",
        "Shuohui Chen",
        "Christopher Dewan",
        "Mona Diab",
        "Xian Li",
        "Xi Victoria Lin"
      ],
      "year": "2022",
      "venue": "Opt: Open pre-trained transformer language models",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Multilingual machine translation with large language models: Empirical results and analysis",
      "authors": [
        "Wenhao Zhu",
        "Hongyi Liu",
        "Qingxiu Dong",
        "Jingjing Xu",
        "Lingpeng Kong",
        "Jiajun Chen",
        "Lei Li",
        "Shujian Huang"
      ],
      "year": "2023",
      "venue": "Multilingual machine translation with large language models: Empirical results and analysis",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Language as an interstate migration barrier-the interesting case of india. Eastern",
      "authors": [
        "Jakub Zieliński"
      ],
      "year": "2021",
      "venue": "European Journal of Transnational Relations",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Llama-2-13b+lora(Multi)",
      "authors": [],
      "year": "",
      "venue": "Llama-2-13b+lora(Multi)",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Llama-2-13b+FF+lora",
      "authors": [],
      "year": "",
      "venue": "Llama-2-13b+FF+lora",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Llama-2-13b+lora(Multi)",
      "authors": [],
      "year": "",
      "venue": "Llama-2-13b+lora(Multi)",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "",
      "authors": [
        "Iiit-H - Ltrc"
      ],
      "year": "0715",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "68 Mistral-7B-v0.1+lora(Multi)",
      "authors": [],
      "year": "",
      "venue": "68 Mistral-7B-v0.1+lora(Multi)",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Assessing Translation Capabilities Of Large Language Models Involving English And Indian Languages",
      "text": "Vandan Mujadia Ashok Urlana Yash Bhaskar Penumalla Aditya Pavani Kukkapalli Shravya Parameswari Krishnamurthy and Dipti Misra Sharma LTRC, IIIT Hyderabad, India {vandan.mu,ashok.urlana,yash.bhaskar}@research.iiit.ac.in, {aditya.pavani,kukkapalli.shravya}@students.iiit.ac.in, {param.krishna,dipti}@iiit.ac.in"
    },
    {
      "title": "Abstract",
      "text": "Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. In this work, our aim is to explore the multilingual capabilities of large language models by using machine translation as a task involving English and 22 Indian languages. We first investigate the translation capabilities of raw large language models, followed by exploring the in-context learning capabilities of the same raw models. We fine-tune these large language models using parameter efficient fine-tuning methods such as LoRA and additionally with full fine-tuning. Through our study, we have identified the best performing large language model for the translation task involving LLMs, which is based on LLaMA. Our results demonstrate significant progress, with average BLEU scores of 13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99, 42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for English to Indian languages on IN22 (conversational), IN22 (general), flores200-devt, and newstest2019 tests. Similarly, for Indian languages to English, we achieved average BLEU scores of 14.03, 16.65, 16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51, and 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational), IN22 (general), flores200-devt, flores200-devtest, and newstest2019 testsets. Overall, our findings highlight the potential and strength of large language models for machine translation capabilities, including for languages that are currently underrepresented in LLMs."
    },
    {
      "title": "1 Introduction",
      "text": "Generative Large Language Models (LLMs) have made significant performance improvements in various natural language processing (NLP) tasks, showcasing exceptional progress in a wide range of applications [23, 24]. These tasks span from open domain question answering, where LLMs excel at providing accurate and coherent responses, to instruction-based tasks such as code completion, where LLMs can generate code snippets based on given prompts [22]. LLMs have also demonstrated proficiency in tasks like essay writing, grammar checking [25], and text summarization, where they can produce high-quality outputs [1]. These advancements have primarily been observed in English-centric tasks. The popular LLMs support several of natural languages. The performance for some languages other than English is not yet on par or yet to be evaluated [11, 12]. A multilingual country like India, where Figure 1: LLMs based Machine Translation performance comparison with public systems for **English to Indian Languages**. BLEU and chrF scores are averaged over 22 Indian Languages and 5 different benchmark data-sets. The available MT systems are GPT-3.5 (GPT-3.5 Davinci, by OpenAI), IndicTrans-2, Google Translation, LTRC-IIIT-H, SeamlessM4T. LLaMA-2-7b and LLaMA-2-13b are evaluated as LLM based fine-tuned MT systems are namely LLaMA-2-7b+lora (Multi), LLaMA-2-13b+lora (Multi), and LLaMA-2-13b+FF+lora (Multi). over 364+ languages and dialects 1 are spoken across its vast territory, presents a multitude of challenges across various domains due to language barriers Zielinski et al. (2021), such as day-to-day communication, education Steigerwald et al. (2022), business, healthcare Mehandru et al. (2022), tourism, governance, and more. Recent advancements in the field of Large Language Models may offer solutions to these challenges tailored to Indian languages. Footnote 1: [https://en.wikipedia.org/wiki/Linguistic_Survey_of_India](https://en.wikipedia.org/wiki/Linguistic_Survey_of_India) To test whether LLM can effectively overcome language barriers, it is crucial to evaluate the proficiency of large language models in handling Indian languages. Machine Translation, as a critical multilingual task, could be an ideal option to explore the multilingual capabilities of existing models. Hence, we can formulate the question to assess the proficiency of large language models in handling Indian languages as follows: **How effectively do large language models perform in multilingual tasks like Machine Translation, particularly when dealing with Indian languages?** In this work, our major contribution is to address the following points in response to the above question. * What are the directions for utilizing or adapting Large Language Models for Indian Languages? * How do LLMs perform in translating a wide range of Indian languages under zero-shot and in-context learning settings? * Does LLM fine-tuning improve the translation capabilities of Large Language Models? How do they perform on low-resourced MT languages? * The impact of LLM Vocabulary on the Performance of Large Language Models in Translation Tasks. To address the above questions, we assess the translation capabilities of popular large language models (opt, bloom, LLaMA-1, MPT, Falcon, LLaMA-2, and Mistral [Section 3]) involving English and 22 scheduled Indian languages (Asamsee, Bangla, Bodo, Dogri, Konkani, Gujarati, Hindi, Kannada, Kashmiri, Maithili, Malayalam, Marathi, Meitei, Nepali, Odia, Punjabi, Sanskrit, Santali, Sindhi, Tamil, Telugu, and Urdu). We initially examine the translation capabilities of above mentioned raw large language models [Section 5.1]. Subsequently, we explore their in-context learning abilities [Section 5.1]. Additionally, we fine-tune the base models using parameter-efficient fine-tuning methods specifically LoRa [Section-6]. Furthermore, we investigate the potential of 2-stage fine-tuning for large language models, which involves full parameter fine-tuning in the first stage, followed by LoRa-based adaptor fine-tuning [Section 6]. The key findings of our work, as summarized in Figure 1, highlight the performance of our LLM-based machine translation fine-tuned models compared to various known translation engines. These engines range from commercial Google2, GPT-3.53) to open source (IndicTrans-24, LTRC-IIIT-H5, seamlessm46), traditional supervised encoder-decoder translation models Google, IndicTrans-2, LTRC-IIIT-H) and decoder-driven causal large language model-based translation systems (GPT-3.5). Footnote 2: [https://translate.google.co.in/](https://translate.google.co.in/) Footnote 3: [https://chat.openai.com/](https://chat.openai.com/) Footnote 4: [https://github.com/AI4Bharat/IndicTrans2](https://github.com/AI4Bharat/IndicTrans2) Footnote 5: [https://ssmt.iiit.ac.in/translate](https://ssmt.iiit.ac.in/translate) Footnote 6: [https://github.com/facebookresearch/seamless_communication](https://github.com/facebookresearch/seamless_communication) Our findings underscore the significant potential of large language models for translation tasks involving English and Indian Languages. While raw LLMs (LLaMA-2-7b and LLaMA-2-13b) not perform well on translation tasks, our two-stage MT fine-tuned models (LLaMA-2-13b+FF+lora(Multi)) yields comparative results even with minimal parallel corpora. This suggests that LLMs have the potential to possess multilingual capabilities for translating into underrepresented languages, which can be further enhanced through fine-tuning. This work will be a crucial and pioneering milestone in evaluating LLMs for language representation and assessing their translation capabilities for a diverse range of Indian languages, especially those with limited available resources. Related Work Recent advancements in machine translation have shown that neural machine translation (NMT) has made significant strides in terms of output fluency and translation quality, especially when ample parallel data is available (Barrault et al., 2020). However, the scarcity or absence of parallel data poses a challenge for most language pairs. In the case of Indian languages, recent developments have tried to addressed this issue by introducing a new state-of-the-art approach: multilingual machine translation involving Indian languages and English (Wang et al., 2021; Dabre et al., 2020). This approach leverages a single script for machine translation, capitalizing on the lexical and syntactic similarities that arise from the genetic and contact-relatedness among Indian languages (Gala et al., 2023; Eriguchi et al., 2022; Bapna and Firat, 2019). In the field of LLM driven machine translation, in-context learning has gained significant attention (Wu et al., 2023). The use of large language models (LLMs) for multilingual machine translation has been a subject of interest (Zhang et al., 2023). Recent studies have evaluated the translation capabilities of LLMs for different language directions, with a focus on models like ChatGPT (Bang et al., 2023). Notably, Xu et al. proposed a two-stage fine-tuning approach for machine translation using LLMs, involving fine-tuning on monolingual data followed by fine-tuning on a small set of high-quality parallel data. Our work represents the first study that specifically explores machine translation involving Indian languages using large language models."
    },
    {
      "title": "3 Large Language Models",
      "text": "Language modeling, a well-established task in the field of natural language processing, has garnered significant attention over the years (Bellegarda, 2004; Bengio et al., 2000). This task involves predicting the probability of the next token in a sequence of words. Transformers have emerged as the fundamental architecture underlying many existing Large Language Models (Vaswani et al., 2017). Transformers based autoregressive models like GPT (Brown et al., 2020; Radford et al., 2019) have played a crucial role in advancing Natural Language Processing (NLP). GPT-3, with 175 billion parameters, is a standout in this category. It is similar in structure to GPT-2 and GPT-1 but benefits from a more extensive and varied dataset, making it exceptionally powerful in NLP. Further, prompt-based ChatGPT (GPT-3.5 text-davinci-003 and GPT-3.5 turbo) has been performing exceptionally by utilizing the reinforcement-based human feedback strategy. Although these models exhibit impressive performance on several NLP tasks, privacy and bias of the models have been a bottleneck. To mitigate such issues, LLaMA (Touvron et al., 2023) is an open-sourced foundation model trained on publicly available datasets. Similarly, Falcon-40B (Almazrouei et al., 2023) is another open-source LLM trained on a RefinedWeb corpus of 1500 billion tokens. Falcon even comes with 7 and 40 billion instruction versions trained on conversation data. The recent adaptation of Large Language Models (LLMs) for instruction tuning has proven to be a promising approach in improving the performance of various natural language processing tasks. Specifically, in languages like Chinese and Swedish demonstrates the impressive zero-shot and generation abilities of the low-rank adaptation of LLaMA for non-English languages (Cui et al., 2023; Holmstrom and Doostmohammadi, 2023). However, it is worth noting that the current focus of these instruction models is primarily on English. Therefore, there is an immediate need to explore ways to adapt these models to low-resource Indian languages."
    },
    {
      "title": "Base Models",
      "text": "In this work, we used the following base LLM models to test the levels of language coverage and explore their potential for machine translation tasks involving English and Indian languages. * **opt-6.7b7**: The OPT-6.7b (Zhang et al., 2022) model has been extensively trained on the objective of causal language modeling (CLM) using English text. Although the majority of the training data is in English, a small portion of non-English data from CommonCrawl has also been included. This model utilizes 6.7 billion parameters, consisting of 32 layers and 32 attention heads, and employs an embedding size of 4096. Footnote 7: [https://huggingface.co/facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b) * **Bloom-7b8**: BLOOM (Scao et al., 2022) was the first largest multilingual large language model with causal language modeling objective and supports 46 languages and 13 programming languages. Its overall training data contains 1.1% of Indian languages. We opted for Bloom model with 7,069,016,064 parameters with 30 layers, 32 attention heads, 4096 embedding dimensional where maximum token length is 2048. * **LLaMA-7B9**: LLaMA is a collection of foundation language models ranging from 7B to 65B parameters. These models are multilingual models and trained on trillions of tokens. The data includes CCNet, C4, GitHub, Wikipedia, Books, ArXiv, Stack Exchange. In our experiments we evaluated LLaMA model with 7B parameters where 4096 is embedding dimensions and 32 layers and 32 attention head. Footnote 9: [https://huggingface.co/decapoda-research/llama-7b-hf](https://huggingface.co/decapoda-research/llama-7b-hf) * **MPT-7B10**: Similar to above models MPT-7B model is trained on a large amount of data 1T tokens on causal language modeling objective. Footnote 10: [https://huggingface.co/misciml/mpt-7b](https://huggingface.co/misciml/mpt-7b) * **Falcon11**: Falcon (Penedo et al., 2023a) is another large language model trained on causal language modeling (CLM) objective. Here, we utilised Falcon-7B model which is a 7B parameters and trained on 1.5 trillion tokens of RefinedWeb (a novel massive web data-set based on CommonCrawl) enhanced with curated corpora. The model has multilingual capabilities but no Indian languages are explicitly present. We have used Falcon-7B for our experiments. Footnote 11: [https://huggingface.co/misciml/mpt-7b](https://huggingface.co/misciml/mpt-7b) * **LLaMA-2-7B12 and LLaMA-2-13B13**: LLaMA 2 based models (Touvron et al., 2023b) are also trained on causal language modeling (CLM) objective and pretrained on 2 trillion tokens of data from publicly available sources of till September 2022. These models are available in different range parameters from 7 billion to 70 billion. These models have 4k sub-words as context length. In our experiments we have experimented with 7B and 13B LLaMA-2 models. LLaMA-2-7B network has 32 layers and 32 attention heads while LLaMA-2-13B has 40 layers and 40 attention heads. Footnote 11: [https://huggingface.co/misciml/mpt-7b](https://huggingface.co/misciml/mpt-7b) * **Mistral-7B14**: Mistral-7B Large Language Model (LLM) (Jiang et al., 2023) is a pretrained on causal language modeling (CLM) objective with 7 billion parameters. It uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost and Grouped-query attention (GQA) for faster inference which reduces the memory requirement during decoding. It has 4096 embedding dimension, 32 layers and 32 attention heads with context length of 8192 context length. Footnote 14: [https://huggingface.co/mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) Footnote 14: [https://huggingface.co/mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) Footnote 15: [https://huggingface.co/mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)"
    },
    {
      "title": "4 Indian Languages Representation In Llms",
      "text": "Pre-trained (or Raw) large language models are trained on a huge amount of language data, and some of the these models are trained on multiple languages (Naveed et al., 2023). However, their training primarily focuses on English text (Penedo et al., 2023b). Emphasis on English is due to its substantial presence on the internet and its widespread usage in business contexts. For the purpose of this work, our objective is to assess the effectiveness of these models in Machine Translation tasks that involve both English and Indian Languages. Consequently, it becomes crucial to investigate the representation of Indian languages within these large language models. An approach to investigating the representation of Indian languages within a large language model can involve analyzing the frequency of language-specific words and sentences used during the training of these models. Unfortunately, it is not possible to perform this analysis as the training data used for these models are not publicly accessible. LLaMA-2, in particular, has mentioned that its pretraining corpus primarily consists of English and may not be optimal for other languages (Touvron et al., 2023b). However, it is worth mentioning that approximately 8.38% of the data does include languages other than English and codes in LLaMA-2. On the other hand, studying the vocabulary (or letters/characters) of a corpus can provide valuable [MISSING_PAGE_FAIL:5] lect the word with the highest probability, effectively limiting its choice to the most likely option (Aksitov et al., 2023). All of our experiments are conducted using vLLM library on A100, 40GB GPUs."
    },
    {
      "title": "Machine Translation On Raw Llm",
      "text": "To optimize the machine translation task on our selected LLMs, we conducted manual trials with various prompts. Through these trials, we discovered that directly asking for the translation and presenting the text in JSON format yielded better results, as the models seemed to comprehend the JSON structure more effectively (Reinauer et al., 2023). After multiple iterations, we finalized two prompts for translating sentences using raw (pretrained) LLMs, as illustrated in below examples. These prompts were used to evaluate the efficiency of the models. ``` Example: Translation Compute: Translate this to <Target Language> from <Source Language> Text: <Source Language Text> Translated Text: ``` Example: Translation Compute: Translate this from <Source Language> to <Target Language> <Source Language>: <Source Language Text> <Target Language>: ``` Example: (CL Translation Prompt) If the <Source Language> to <Target Language> translation for \"<Source Example>\" is \"<Target Example>\" from <Source Language>, following that, translate this to <Target Language> from <Source Language> Text: <Source Language Text> Translated Text: ``` Similarly, we identified and modified the prompt for example-based in-context learning with LLM. This prompt is specified in Example above (ICL Translation Prompt). In the case of in-context learning, all of our experiments involved providing a single translation sample as a contextual learning example prior to the actual translation command. We ensured that this example remained consistent for the same language pair across the sentences. The sample itself was randomly selected from the Human-BPCC translation training corpus (Al4Bharat et al., 2023). We present the outcomes of both of these experiments in the Performance and Discussions section."
    },
    {
      "title": "Fine-Tuning Llm For Machine Translation",
      "text": "To examine the potential improvement in multilingual understanding or translation performance of LLMs beyond the pre-trained LLM baseline, we conducted fine-tuning experiments for the translation task."
    },
    {
      "title": "5.2.1 Training Data",
      "text": "To fine-tune large language models (LLMs) for the machine translation task, we utilized the Bharat Parallel Corpus Collection (BPCC). This corpus is publicly available and specifically for English to 22 Indic languages translation. It consists of two main parts: BPCC-Mined and BPCC-Human, comprising a total of approximately 230 million Figure 2: Prompting Mechanism for Translation parallel text pairs. For the fine-tuning process, we focused on the BPCC-Human dataset, which contains 2.2 million English-Indic pairs. Additionally, this dataset includes subsets derived from English Wikipedia sentences and everyday usage scenarios. For more information about this corpus, are presented in Table 2."
    },
    {
      "title": "5.2.2 Fine-Tuning Details",
      "text": "Considering the raw LLM performance, model parameters, and resource constraints, we selected a subset of LLMs for the fine-tuning process. Specifically, we chose LLaMA-2-7b, LLaMA-2-13b, and Mistral-7B for the fine-tuning experiment. For the selected LLMs, we decided to conduct fine-tuning using multiple parameters to enhance their performance. These parameters included bi-lingual translation fine-tuning, multi-lingual translation fine-tuning, low-rank adaptation-based fine-tuning, and a two-stage fine-tuning approach: full fine-tuning followed by low-rank adaptation-based fine-tuning. Due to limitations in training resources, we prioritized full fine-tuning as the chosen option. Specifically, we performed LoRa-based fine-tuning (Hu et al., 2021) for all English to 22 Indian languages (in both directions) under bi-lingual settings using LLaMA-2-7b and LLaMA-2-13b. Additionally, we conducted \\begin{table} \\begin{tabular}{l|r r r r r r r} \\hline \\hline **English-** & **\\#Sents** & **S-AvgL** & **T-AvgL** & **S-Words** & **T-Words** & **S-Types** & **T-Types** \\\\ \\hline _Assamese (asm)_ & 138208 & 16.88 & 14.39 & 2333583 & 1988395 & 125480 & 185151 \\\\ _Bangla (ban)_ & 180219 & 17.80 & 15.07 & 3208203 & 2715959 & 161820 & 227468 \\\\ _Bodo (brx)_ & 113139 & 17.79 & 13.96 & 2012274 & 1579042 & 116963 & 227180 \\\\ _Dogri (doi)_ & 24157 & 15.32 & 17.68 & 370047 & 427110 & 48256 & 41370 \\\\ _Konkani (gom)_ & 97555 & 17.13 & 14.03 & 1671465 & 1368512 & 82783 & 145300 \\\\ _Gujarati (guj)_ & 135664 & 17.71 & 15.96 & 2402552 & 2164831 & 123935 & 174886 \\\\ _Hindi (hin)_ & 222356 & 17.84 & 19.69 & 3966247 & 4378231 & 183737 & 202423 \\\\ _Kannada (kan)_ & 117222 & 16.83 & 12.44 & 1972881 & 1458053 & 100778 & 208803 \\\\ _Kashmiri (kas)_ & 19824 & 16.02 & 17.68 & 317634 & 350577 & 43197 & 66210 \\\\ _Maithili (mai)_ & 23690 & 16.11 & 15.79 & 381720 & 374042 & 52920 & 57423 \\\\ _Malayalam (mal)_ & 137950 & 16.30 & 11.13 & 2248081 & 1535654 & 120999 & 299146 \\\\ _Marathi (mar)_ & 175893 & 17.94 & 14.81 & 3154904 & 2604119 & 167822 & 299983 \\\\ _Meitei (mni)_ & 56617 & 17.77 & 15.73 & 1006271 & 890828 & 86175 & 161043 \\\\ _Nepali (nep)_ & 85442 & 16.76 & 14.13 & 1431858 & 1207687 & 105411 & 145175 \\\\ _Odia (odi)_ & 36923 & 17.07 & 15.49 & 630148 & 571958 & 68765 & 79932 \\\\ _Punjabi (pan)_ & 80951 & 17.22 & 18.29 & 1394286 & 1480835 & 63510 & 74451 \\\\ _Sanskrit (san)_ & 33189 & 16.30 & 11.69 & 541034 & 387957 & 61591 & 119856 \\\\ _Santali (sat)_ & 24368 & 16.95 & 19.28 & 412918 & 469791 & 51307 & 56053 \\\\ _Sindhi (sin)_ & 10503 & 17.10 & 19.32 & 179592 & 202952 & 28945 & 30782 \\\\ _Tamil (tam)_ & 150254 & 17.76 & 13.34 & 2668252 & 2004981 & 139214 & 290917 \\\\ _Telugu (tel)_ & 111808 & 16.81 & 12.64 & 1879737 & 1413466 & 96105 & 191792 \\\\ _Urdu (urd)_ & 150747 & 17.62 & 20.20 & 2656814 & 3044480 & 144001 & 129856 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: English to Indian Languages machine translation Fine-tuning data from BPCC-Human (Al4Bharat et al., 2023). In this, the term ”#Sents” refers to the total number of parallel sentences. ”S-AvgL” and ”T-AvgL” represent the average sentence length, in terms of words, for the source and target languages, respectively. Likewise, ”Words” denotes the total number of words, while ”Type” represents the total number of unique words. \\begin{table} \\begin{tabular}{l l l} \\hline \\hline **Method** & **Hyper-parameter** & **Value** \\\\ \\hline \\multirow{5}{*}{LoRA} & LoRA modules & PEFT19 \\\\ & rank & 8 \\\\ & dropout & 0.05 \\\\ \\multirow{2}{*}{LoRA} & learning rate & 1e-4 \\\\ & global batch size & 8 \\\\ & epochs & 6 \\\\ \\hline \\multirow{2}{*}{Full-parameter FSDP} & learning rate & 1e-4 \\\\ \\multirow{2}{*}{} & global batch size & 4 \\\\ & epochs & 5 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Hyper-parameter configurations of LoRA based and full fine-tuning for 4*A100 40GB GPUsmulti-lingual LoRa-based fine-tuning for English to the combined 22 Indian languages, as well as for the combined 22 Indian languages to English, using LLaMA-2-7b, LLaMA-2-13b, and Mistral-7B. Based on the overall performance, we proceeded with a two-stage fine-tuning approach for the multi-lingual translation task specifically on LLaMA-2-13b. In the first stage, we performed full fine-tuning as a multi-lingual translation setup. Subsequently, in the second stage, we conducted multi-lingual LoRa-based fine-tuning on the same fully fine-tuned model. For both types of fine-tuning LLMs, we utilized the llama-recipes codebase20 which provides an efficient implementation for LoRa-based adaptor fine-tuning with PEFT [16]. For more details, please refer to the llama-recipes documentation 21. The hyperparameters for the fine-tuning process are specified in Table 3. The training data used for the fine-tuning experiments will be presented in the sub-section 5.2.1. Footnote 20: [https://github.com/facebookresearch/llama-recipes/](https://github.com/facebookresearch/llama-recipes/) Footnote 21: [https://github.com/facebookresearch/llama-recipes/blob/main/docs/LLM_finetuning.md](https://github.com/facebookresearch/llama-recipes/blob/main/docs/LLM_finetuning.md)"
    },
    {
      "title": "6 Machine Translation Benchmark Data",
      "text": "We evaluate the performance of multilingual translation using three different benchmark datasets, as outlined in Table 4. The table provides a comprehensive overview of each dataset, highlighting the availability of n-way parallel data for the specified number of Indian languages from English as a source direction."
    },
    {
      "title": "7 Performance Evaluation",
      "text": "We evaluated the performance of the translation outputs using BLEU [12] and chrF [12] evaluation methods on benchmark data described in Section 6. However, we did not include COMET [13] as an evaluation method due to the absence of support for many low-resource Indian languages at the time of evaluation. We used sacreBLEU library [14] for BLEU 22 and chrF 23 calculation. To mitigate the impact of randomness in scores, we present our findings as the average of two runs for all of our results. Footnote 22: footprint for BLEU: nrefs:lcase:mixeddeff:notok:13alsmooth:explversion:2.1.0 Footnote 23: footprint for chrF: nrefs:lcase:mixeddeff:yeslnc:6lnw:0lspace:nolversion:2.1.0 Footnote 24: footprint for BLEU: nrefs:lcase:mixeddeff:notok:13alsmooth:explversion:2.1.0 Footnote 25: footprint for chrF: nrefs:lcase:mixeddeff:yeslnc:6lnw:0lspace:nolversion:2.1.0 Raw (Zero shot) vs ICL based Translation on LLMsFigure 3 presents the comparison of overall results when evaluating translation quality for Raw LLMs and In Context Learning (ICL) based LLMs outputs. The left sub-figure represents the results for English to 22 Indian languages, while the right sub-figure presents the results for 22 Indian languages to English translation. We Figure 3: Evaluation of English - 22 Indic language Translation over 5 benchmark-sets (averaged): Raw LLM vs In Context Learning (ICL); Raw LLM models: LLaMA-2-7b, LLaMA-2-13b) \\begin{table} \\begin{tabular}{|l|l|l|} \\hline **TestSet** & **\\#sent** & **Details** \\\\ \\hline IN22\\_conv\\_test & 1502 & Al4Bhart et. released MT benchmark data covering English to 22 Indian Languages. \\\\ \\hline IN22\\_gen\\_test & 1023 & Goyal et al. released MT benchmark data which includes English to 17 Indian Language pairs considered in this work. \\\\ \\hline Flores200-devest & 1012 & Goyal et al. released MT benchmark data which includes English to 10 Indian Language pairs considered in this work. \\\\ \\hline Newststst2019 & 1997 & Federmann et al. released MT benchmark data which includes English to 10 Indian Language pairs considered in this work. \\\\ \\hline \\end{tabular} \\end{table} Table 4: Benchmark data details covering English to 22 Indian Languagesobserved amplified performance for the Bloom large language model for certain languages, which can be attributed to the known MT benchmark data leak in the pre-training Zhu et al. (2023). Consequently, we decided to exclude this language model from further experiments. LLMs models such as OPT, MPT, LLAMA-1 and Falcon exhibited poor performance, which can be correlated with the no or minimal presence of characters of our focused Indian Languages in their vocabulary (Table 1). Therefore, we have omitted reporting the results for these models. Figure 3 indicates that the LLama-2 models show relatively better performance with ICL settings compared to the raw models. Detailed results are presented in appendix. Through manual analysis, we observed that for less-represented languages such as Gujarati, Kannada, Odia, etc. (Table 1), the ICL-driven translation tends to repeat the same translation given in the context as learning. On the other hand, the raw models tend to hallucinate and repeat words throughout the translation Guerreiro et al. (2023). One important finding from the manual analysis is that these raw LLMs demonstrate the ability to accurately identify languages (e.g., when asked for Gujarati translation, it gives inaccurate translations but correctly hallucinate text in the Gujarati script). This is a positive aspect and indicates a significant advantage of these LLMs in terms of their understanding and differentiation of languages and language scripts. In response to the question asked in Introduction, it is true that the major available LLMs are primarily focused on English. However, _they do exhibit minimal potential for zero-shot and example-based translation capabilities_. Fine-Tuned LLM driven Translations: English to Indian LanguagesWe conducted an evaluation to compare the performance of our Fine-Tuned LLM models with GPT-3.5, as both models use the same decoder-based approach. Figure 4 illustrates the comparison for English to 22 Indian language translation. The scores for GPT-3.5 are generally lower compared to our fine-tuned methods, also our fine-tuned models have higher numbers than our previously mentioned zero-shot and example-based learning baseline. This indicates that with minimal translation corpora, we are able to achieve considerable translations for translating into Indian languages from English. Additionally, we observed that multilingual fine-tuning yielded better overall performance compared to bilingual fine-tuning. The two-stage fine-tuning approach also outperformed other fine-tuning methods for the translation task. The impressive results of the two-stage fine-tuning approach, as shown in Figure 4, are comparable to those of traditional encoder-decoder based translation models. It is worth noting that this performance improvement was achieved using only a few thousand parallel data, whereas traditional NMT models typically require a larger amount of data. From Figure 4, we can see that translating to Figure 4: Performance comparison of GPT-3.5 vs our Fine-Tuned LLM Translation models (LLaMA-2-7b+lora (Multi), LLaMA-2-13b+lora (Multi), and LLaMA-2-13b+FF+lora (Multi)): English to 22 Indian Languages over 5 benchmark-sets (averaged). Here, LORA stands for Low-Rank Adaptation of Large Language Models based fine-tuning. Multi stands for the multilingual model, FF for full-finetuning, and FF+lora stands for 2-stage fine-tuning. low-resource languages such as Dogri, Konkani, Kashmiri, Meitei, Sanskrit, and Sindhi yielded favorable evaluation numbers (Detailed results are presented in appendix) compared to existing translation systems. In answer to the question posed in the introduction, _fine-tuning LLMs does enhance translation capabilities, particularly more when employing multilingual fine-tuning. These models demonstrate proficiency in translating low-resource languages as well._ Fine-Tuned LLM driven Translations: Indian Languages to EnglishFigure 5 showcases the comparison for Indian language to English translation. The scores for GPT-3.5 are generally not higher compared to our fine-tuned methods, while our fine-tuned models still outperform the previously mentioned zero-shot and example-based context learning driven LLM results. Notably, the performance improvement for Indian language to English translation is comparatively lower than that of English to Indian language translation. Compared to translations from English to Indian languages, the LoRa-based single-stage fine-tuning here performs the best among all the fine-tuning approaches. Detailed results are presented in the appendix. This disparity can be attributed to the vocabulary representation of Indian languages in these LLMs. As presented in Table 1, the subword vocabulary for Indian languages is limited in the considered LLMs. Consequently, when processing input in Indian languages, characters that are not present in the vocabulary receive multiple hexadecimal representations from the vocabulary. This creates a bottleneck in understanding the underlying meaning, making it challenging for the larger LLM network to establish corresponding semantic translations. However, this issue does not arise when translating from English to Indian languages. The underlying understanding of English is robust, allowing the network to effectively map the respective language translations. Hence, this suggest the need for LLMs where enough language representation is required and future development of LLMs must address this."
    },
    {
      "title": "8 Limitations",
      "text": "In order to conduct our experiments, we relied on high-performance GPUs, specifically the A100-40GB. However, we acknowledge that not everyone may have access to such powerful computing resources, making it challenging to reproduce our experiments and achieve identical results. To overcome this limitation, our objective is to provide open access to all outputs, including model and results, to facilitate further research and exploration. By making these resources openly available, we aim to promote collaboration and enable others to build upon our work."
    },
    {
      "title": "9 Conclusion",
      "text": "Our experiments and results have provided promising insights into the use of LLMs for translation tasks. We have found that LLMs have the potential to perform translations involving English and Figure 5: Performance comparison of GPT-3.5 vs our Fine-Tuned LLM Translation models (LLaMA-2-7b+lora (Multi), LLaMA-2-13b+lora (Multi), and LLaMA-2-13b+FF+lora (Multi)): English to 22 Indian Languages over 5 benchmark-sets (averaged). Here, LORA stands for Low-Rank Adaptation of Large Language Models based fine-tuning. Multi stands for the multilingual model. Indian languages without the need for an extensive collection of parallel data, which distinguishes them from traditional translation models. Furthermore, our findings indicate that LLaMA-2 based models outperform other models in zero-shot and in-context example-based learning. Notably, the LLaMA-2-13b based model demonstrates superior performance compared to its counterparts. To enhance the LLM's understanding of English and Indian languages, we have introduced a two-stage fine-tuning process. This process begins with initial full-finetuning, followed by LoRa-based fine-tuning. Through this approach, we have significantly improved the LLM's comprehension of content in both languages. However, our experiments suggest that further work on LLMs is required to surpass the performance of traditional encoder-decoder based translation models. This work could involve the development of Indian language-specific LLMs, which would enhance vocabulary and alphabet coverage, resulting in better representation of Indian languages. On the other hand, in the future, we plan to incorporate Indian to Indian language translation using LLMs. Additionally, our aim is to develop a single LLM model capable of translating all Indian languages, as well as English, in both directions. By doing so, we strive to push the boundaries of language capabilities within LLMs and further advance the field."
    },
    {
      "title": "Acknowledgement",
      "text": "We express our gratitude to Pruthwik Mishra, Arafat Ahsan and Palash Gupta for their contributions throughout the different phases of this project. This undertaking is funded by the Ministry of Electronics and Information Technology, Government of India, as evidenced by the Sanction Order: 11(1)/2022-HCC(TDIL)-Part(2)/A/B/C and the Administrative Approval: 11(1)/2022-HCC(TDIL)-Part(2)."
    },
    {
      "title": "References",
      "text": "* Al4Bharat et al. (2023) Al4Bharat, Jay Gala, Pranjal A. Chitale, Raghavan AK, Sumanth Doddapaneni, Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratih Pudupply, Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre, and Anoop Kunchukuttan. 2023. Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages. _arXiv preprint arXiv: 2305.16307_. * Aksitov et al. (2023) Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, and Yunhsuan Sung. 2023. Characterizing attribution and fluency tradeoffs for retrieval-augmented large language models. _arXiv preprint arXiv:2302.05578_. * Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulazir Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al. 2023. Falcon-40b: an open large language model with state-of-the-art performance. Technical report, Technical report, Technology Innovation Institute. * Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wile, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multi-task, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. _arXiv preprint arXiv:2302.04023_. * Bapna and Firat (2019) Ankur Bapna and Orhan Firat. 2019. Exploring massively multilingual, massive neural machine translation. _Google AI Blog, October_, 11. * Barrault et al. (2020) Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Yvette Graham, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Andre Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, and Matteo Negri, editors. 2020. _Proceedings of the Fifth Conference on Machine Translation_. Association for Computational Linguistics, Online. * Bellegarda (2004) Jerome R Bellegarda. 2004. Statistical language model adaptation: review and perspectives. _Speech communication_, 42(1):93-108. * Bengio et al. (2000) Yoshua Bengio, Rejean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. _Advances in neural information processing systems_, 13. * Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901. * Chang et al. (2023) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. _arXiv preprint arXiv:2307.03109_. * Cui et al. (2023) Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and effective text encoding for chinese llama and alpaca. _arXiv preprint arXiv:2304.08177_. * Chen et al. (2019)Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan. 2020. A survey of multilingual neural machine translation. _ACM Computing Surveys (CSUR)_, 53(5):1-38. * Eriguchi et al. (2022) Akiko Eriguchi, Shufang Xie, Tao Qin, and Hany Hassan Awadalla. 2022. Building multilingual machine translation systems that serve arbitrary xy translations. _arXiv preprint arXiv:2206.14982_. * news test references for MT evaluation of 128 languages. In _Proceedings of the First Workshop on Scaling Up Multilingual Evaluation_, pages 21-24, Online. Association for Computational Linguistics. * Gala et al. (2023) Jay Gala, Pranjal A Chitale, Raghavan AK, Sumanth Doddapaneni, Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, et al. 2023. Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages. _arXiv preprint arXiv:2305.16307_. * Goyal et al. (2022) Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzman, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. _Transactions of the Association for Computational Linguistics_, 10:522-538. * Guerreiro et al. (2023) Nuno M Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and Andre FT Martins. 2023. Hallucinations in large multilingual translation models. _arXiv preprint arXiv:2303.16104_. * Holmstrom and Doostmohammadi (2023) Oskar Holmstrom and Ehsan Doostmohammadi. 2023. Making instruction finetuning accessible to non-English languages: A case study on Swedish models. In _Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)_, pages 634-642, Torshavn, Fence Islands. University of Tartu Library. * Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_. * Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. _arXiv preprint arXiv:2310.06825_. * Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_. * Lai et al. (2023) Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyshek, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. _arXiv preprint arXiv:2304.05613_. * Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022. Perf: State-of-the-art parameter-efficient fine-tuning methods. [https://github.com/hugging/ace/pefft](https://github.com/hugging/ace/pefft). * Mehandru et al. (2022) Nikita Mehandru, Samantha Robertson, and Niloufar Salehi. 2022. Reliable and safe use of machine translation in medical settings. In _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 2016-2025. * Naveed et al. (2023) Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Nick Barnes, and Ajmal Mian. 2023. A comprehensive overview of large language models. _arXiv preprint arXiv:2307.06435_. * Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318. * Penedo et al. (2023a) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023a. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. _arXiv preprint arXiv:2306.01116_. * Penedo et al. (2023b) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023b. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. _arXiv preprint arXiv:2306.01116_. * Popovic (2015) Maja Popovic. 2015. chrf: character n-gram f-score for automatic mt evaluation. In _Proceedings of the tenth workshop on statistical machine translation_, pages 392-395. * Post (2018) Matt Post. 2018. A call for clarity in reporting BLEU scores. In _Proceedings of the Third Conference on Machine Translation: Research Papers_, pages 186-191, Belgium, Brussels. Association for Computational Linguistics. * Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9. * Rei et al. (2017) Ricardo Rei, Jose G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andre F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In _Proceedings of the Seventh Conference on Machine Translation (WMT)_, pages 578-585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. * Reinauer et al. (2023) Raphael Reinauer, Patrick Simianer, Kaden Uhlig, Johannes EM Mosig, and Joern Wuebker. 2023. Neural machine translation models can learn to be few-shot learners. _arXiv preprint arXiv:2309.08590_. * Salomon (1995) Richard Salomon. 1995. On the origin of the early indian scripts. _Journal of the American Oriental Society_, 115(2):271-279. * Le Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Elie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_. * Steigerwald et al. (2022) Emma Steigerwald, Valeria Ramirez-Castaneda, Debora YC Brandt, Andras Baldi, Julie Teresa Shapiro, Lynne Bowker, and Rebecca D Tarvin. 2022. Overcoming language barriers in academia: Machine translation tools and a vision for a multilingual future. _BioScience_, 72(10):988-998. * Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_. * Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_. * Vaithilingam et al. (2022) Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In _Chi conference on human factors in computing systems extended abstracts_, pages 1-7. * Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. _Advances in neural information processing systems_, 30. * Wang et al. (2021) Rui Wang, Xu Tan, Renqian Luo, Tao Qin, and Tie-Yan Liu. 2021. A survey on low-resource neural machine translation. _arXiv preprint arXiv:2107.04239_. * Wu et al. (2023a) Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, and Michael Lyu. 2023a. Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark. _arXiv preprint arXiv:2303.13648_. * Wu et al. (2023b) Zhenyu Wu, Yaoxiang Wang, Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Jingjing Xu, and Yu Qiao. 2023b. OpenICL: An open-source framework for in-context learning. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)_, pages 489-498, Toronto, Canada. Association for Computational Linguistics. * Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. _arXiv preprint arXiv:2309.07864_. * Xu et al. (2023) Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2023. A paradigm shift in machine translation: Boosting translation performance of large language models. _arXiv preprint arXiv:2309.11674_. * Xuanfan and Piji (2023) Ni Xuanfan and Li Piji. 2023. A systematic evaluation of large language models for natural. In _Proceedings of the 22nd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum)_, pages 40-56, Harbin, China. Chinese Information Processing Society of China. * Zhang et al. (2023) Biao Zhang, Barry Haddow, and Alexandra Birch. 2023. Prompting large language model for machine translation: A case study. _arXiv preprint arXiv:2301.07069_. * Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_. * Zhu et al. (2023) Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023. Multilingual machine translation with large language models: Empirical results and analysis. _arXiv preprint arXiv:2304.04675_. * Zielinski et al. (2021) Jakub Zielinski et al. 2021. Language as an interstate migration barrier-the interesting case of india. _Eastern European Journal of Transnational Relations_, 5(1):29-38. [MISSING_PAGE_FAIL:14] [MISSING_PAGE_EMPTY:15] [MISSING_PAGE_EMPTY:16] [MISSING_PAGE_EMPTY:17] [MISSING_PAGE_EMPTY:18] [MISSING_PAGE_EMPTY:19]"
    }
  ]
}