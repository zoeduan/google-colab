{
  "title": "CODEBPE: INVESTIGATING SUBTOKENIZATION OPTIONS FOR LARGE LANGUAGE MODEL PRETRAINING ON SOURCE CODE",
  "authors": [
    "Nadezhda Chirkova",
    "Naver Labs Europe",
    "Sergey Troshin"
  ],
  "abstract": "\n Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, namely the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account code specifics. We propose subtokenziation that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase. \n",
  "references": [
    {
      "id": null,
      "title": "CODEBPE: INVESTIGATING SUBTOKENIZATION OPTIONS FOR LARGE LANGUAGE MODEL PRETRAINING ON SOURCE CODE",
      "authors": [
        "Nadezhda Chirkova",
        "Naver Labs Europe",
        "Sergey Troshin"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Unified pre-training for program understanding and generation",
      "authors": [
        "Wasi Ahmad",
        "Saikat Chakraborty",
        "Baishakhi Ray",
        "Kai-Wei Chang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Avatar: A parallel corpus for java-python program translation",
      "authors": [
        "Ahmad Wasi Uddin",
        "Md Golam Rahman Tushar",
        "Saikat Chakraborty",
        "Kai-Wei Chang"
      ],
      "year": "2021",
      "venue": "Avatar: A parallel corpus for java-python program translation",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Efficient inference for multilingual neural machine translation",
      "authors": [
        "Alexandre Berard",
        "Dain Lee",
        "Stephane Clinchant",
        "Kweonwoo Jung",
        "Vassilina Nikoulina"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2021.emnlp-main.674"
    },
    {
      "id": "b3",
      "title": "Byte pair encoding is suboptimal for language model pretraining",
      "authors": [
        "Kaj Bostrom",
        "Greg Durrett"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.414"
    },
    {
      "id": "b4",
      "title": "",
      "authors": [
        "Mark Chen",
        "Jerry Tworek",
        "Heewoo Jun",
        "Qiming Yuan",
        "Henrique Ponde De Oliveira Pinto",
        "Jared Kaplan",
        "Harri Edwards",
        "Yuri Burda",
        "Nicholas Joseph",
        "Greg Brockman",
        "Alex Ray",
        "Raul Puri",
        "Gretchen Krueger",
        "Michael Petrov",
        "Heidy Khlaaf",
        "Girish Sastry",
        "Pamela Mishkin",
        "Brooke Chan",
        "Scott Gray",
        "Nick Ryder",
        "Mikhail Pavlov",
        "Alethea Power",
        "Lukasz Kaiser",
        "Mohammad Bavarian",
        "Clemens Winter",
        "Philippe Tillet",
        "Felipe Petroski Such",
        "Dave Cummings",
        "Matthias Plappert",
        "Fotios Chantzis",
        "Elizabeth Barnes",
        "Ariel Herbert-Voss",
        "William Hebgen Guss",
        "Alex Nichol",
        "Alex Paino",
        "Nikolas Tezak",
        "Jie Tang",
        "Igor Babuschkin",
        "Suchir Balaji",
        "Shantanu Jain",
        "William Saunders",
        "Christopher Hesse",
        "Andrew N Carr",
        "Jan Leike",
        "Josh Achiam",
        "Vedant Misra"
      ],
      "year": "2021",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "A simple approach for handling out-of-vocabulary identifiers in deep learning for source code",
      "authors": [
        "Nadezhda Chirkova",
        "Sergey Troshin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.26"
    },
    {
      "id": "b6",
      "title": "Improving multilingual models with language-clustered vocabularies",
      "authors": [
        "Chung Hyung Won",
        "Dan Garrette",
        "Kiat Chuan Tan",
        "Jason Riesa"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.367"
    },
    {
      "id": "b7",
      "title": "Canine: Pre-training an efficient tokenization-free encoder for language representation",
      "authors": [
        "Jonathan H Clark",
        "Dan Garrette",
        "Iulia Turc",
        "John Wieting"
      ],
      "year": "2021",
      "venue": "Canine: Pre-training an efficient tokenization-free encoder for language representation",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "A call for prudent choice of subword merge operations in neural machine translation",
      "authors": [
        "Shuoyang Ding",
        "Adithya Renduchintala",
        "Kevin Duh"
      ],
      "year": "2019",
      "venue": "Proceedings of Machine Translation Summit XVII: Research Track",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "CodeBERT: A pre-trained model for programming and natural languages",
      "authors": [
        "Zhangyin Feng",
        "Daya Guo",
        "Duyu Tang",
        "Nan Duan",
        "Xiaocheng Feng",
        "Ming Gong",
        "Linjun Shou",
        "Bing Qin",
        "Ting Liu",
        "Daxin Jiang",
        "Ming Zhou"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.139"
    },
    {
      "id": "b10",
      "title": "Incoder: A generative model for code infilling and synthesis",
      "authors": [
        "Daniel Fried",
        "Armen Aghajanyan",
        "Jessy Lin",
        "Sida Wang",
        "Eric Wallace",
        "Freda Shi",
        "Ruiqi Zhong",
        "Wen-Tau Yih",
        "Luke Zettlemoyer",
        "Mike Lewis"
      ],
      "year": "2022",
      "venue": "Incoder: A generative model for code infilling and synthesis",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Graphcode{bert}: Pre-training code representations with data flow",
      "authors": [
        "Shuo Daya Guo",
        "Shuai Ren",
        "Zhangyin Lu",
        "Duyu Feng",
        "Tang",
        "Liu Shujie",
        "Long Zhou",
        "Nan Duan",
        "Alexey Svyatkovskiy",
        "Shengyu Fu",
        "Michele Tufano",
        "Colin Shao Kun Deng",
        "Dawn Clement",
        "Neel Drain",
        "Jian Sundaresan",
        "Daxin Yin",
        "Ming Jiang",
        "Zhou"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Character-based nmt with transformer",
      "authors": [
        "Rohit Gupta",
        "Laurent Besacier",
        "Marc Dymetman",
        "Matthias Gallé"
      ],
      "year": "2019",
      "venue": "Character-based nmt with transformer",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Codesearchnet challenge: Evaluating the state of semantic code search",
      "authors": [
        "Hamel Husain",
        "Ho-Hsiang Wu",
        "Tiferet Gazit",
        "Miltiadis Allamanis",
        "Marc Brockschmidt"
      ],
      "year": "2020",
      "venue": "Codesearchnet challenge: Evaluating the state of semantic code search",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Mapping language to code in programmatic context",
      "authors": [
        "Srinivasan Iyer",
        "Ioannis Konstas",
        "Alvin Cheung",
        "Luke Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1192"
    },
    {
      "id": "b15",
      "title": "Learning programmatic idioms for scalable semantic parsing",
      "authors": [
        "Srinivasan Iyer",
        "Alvin Cheung",
        "Luke Zettlemoyer"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1545"
    },
    {
      "id": "b16",
      "title": "Learning and evaluating contextual embedding of source code",
      "authors": [
        "Aditya Kanade",
        "Petros Maniatis",
        "Gogul Balakrishnan",
        "Kensen Shi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Big code != big vocabulary: Open-vocabulary models for source code",
      "authors": [
        "Rafael-Michael Karampatsis",
        "Hlib Babii",
        "Romain Robbes",
        "Charles Sutton",
        "Andrea Janes"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, ICSE '20",
      "doi": "10.1145/3377811.3380342"
    },
    {
      "id": "b18",
      "title": "Subword regularization: Improving neural network translation models with multiple subword candidates",
      "authors": [
        "Taku Kudo"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1007"
    },
    {
      "id": "b19",
      "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "authors": [
        "Taku Kudo",
        "John Richardson"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/D18-2012"
    },
    {
      "id": "b20",
      "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
      "authors": [
        "Mike Lewis",
        "Yinhan Liu",
        "Naman Goyal",
        "Marjan Ghazvininejad",
        "Abdelrahman Mohamed",
        "Omer Levy",
        "Veselin Stoyanov",
        "Luke Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.703"
    },
    {
      "id": "b21",
      "title": "Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode",
      "authors": [
        "Yujia Li",
        "David Choi",
        "Junyoung Chung",
        "Nate Kushman",
        "Julian Schrittwieser",
        "Rémi Leblond",
        "Tom Eccles",
        "James Keeling",
        "Felix Gimeno",
        "Agustin Dal Lago",
        "Thomas Hubert",
        "Peter Choy",
        "Cyprien De Masson D'autume",
        "Igor Babuschkin",
        "Xinyun Chen",
        "Po-Sen Huang",
        "Johannes Welbl",
        "Sven Gowal",
        "Alexey Cherepanov",
        "James Molloy",
        "Daniel J Mankowitz",
        "Esme Sutherland Robson"
      ],
      "year": "2022",
      "venue": "Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
      "authors": [
        "Shuai Lu",
        "Daya Guo",
        "Shuo Ren",
        "Junjie Huang",
        "Alexey Svyatkovskiy",
        "Ambrosio Blanco",
        "Colin B Clement",
        "Dawn Drain",
        "Daxin Jiang",
        "Duyu Tang",
        "Ge Li",
        "Lidong Zhou",
        "Linjun Shou",
        "Long Zhou",
        "Michele Tufano",
        "Ming Gong",
        "Ming Zhou",
        "Nan Duan",
        "Neel Sundaresan",
        "Shengyu Shao Kun Deng",
        "Shujie Fu",
        "Liu"
      ],
      "year": "2021",
      "venue": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Between words and characters: A brief history of open-vocabulary modeling and tokenization in nlp",
      "authors": [
        "Sabrina J Mielke",
        "Zaid Alyafeai",
        "Elizabeth Salesky",
        "Colin Raffel",
        "Manan Dey",
        "Matthias Gallé",
        "Arun Raja",
        "Chenglei Si",
        "Wilson Y Lee",
        "Benoît Sagot",
        "Samson Tan"
      ],
      "year": "2021",
      "venue": "Between words and characters: A brief history of open-vocabulary modeling and tokenization in nlp",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "BPE-dropout: Simple and effective subword regularization",
      "authors": [
        "Ivan Provilkov",
        "Dmitrii Emelianenko",
        "Elena Voita"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.170"
    },
    {
      "id": "b26",
      "title": "Improving language understanding by generative pretraining",
      "authors": [
        "Alec Radford",
        "Karthik Narasimhan"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pretraining",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2018",
      "venue": "Language models are unsupervised multitask learners",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter J Liu"
      ],
      "year": "2020",
      "venue": "Journal of Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Codebleu: a method for automatic evaluation of code synthesis",
      "authors": [
        "Daya Shuo Ren",
        "Shuai Guo",
        "Long Lu",
        "Shujie Zhou",
        "Duyu Liu",
        "M Tang",
        "Ambrosio Zhou",
        "Shuai Blanco",
        "Ma"
      ],
      "year": "2020",
      "venue": "Codebleu: a method for automatic evaluation of code synthesis",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Unsupervised translation of programming languages",
      "authors": [
        "Marie-Anne Baptiste Roziere",
        "Lowik Lachaux",
        "Guillaume Chanussot",
        "Lample"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Dobf: A deobfuscation pre-training objective for programming languages",
      "authors": [
        "Marie-Anne Baptiste Roziere",
        "Marc Lachaux",
        "Guillaume Szafraniec",
        "Lample"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "How good is your tokenizer? on the monolingual performance of multilingual language models",
      "authors": [
        "Phillip Rust",
        "Jonas Pfeiffer",
        "Ivan Vulić",
        "Sebastian Ruder",
        "Iryna Gurevych"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "URL",
      "authors": [],
      "year": "",
      "venue": "URL",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Neural machine translation of rare words with subword units",
      "authors": [
        "Rico Sennrich",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "year": "2016",
      "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P16-1162"
    },
    {
      "id": "b35",
      "title": "Program Synthesis and Semantic Parsing with Learned Code Idioms",
      "authors": [
        "Richard Shin",
        "Miltiadis Allamanis",
        "Marc Brockschmidt",
        "Oleksandr Polozov"
      ],
      "year": "2019",
      "venue": "Program Synthesis and Semantic Parsing with Learned Code Idioms",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Evaluating clone detection tools with bigclonebench",
      "authors": [
        "Jeffrey Svajlenko",
        "Chanchal K Roy"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
      "doi": "10.1109/ICSM.2015.7332459"
    },
    {
      "id": "b37",
      "title": "Charformer: Fast character transformers via gradient-based subword tokenization",
      "authors": [
        "Yi Tay",
        "Q Vinh",
        "Sebastian Tran",
        "Jai Ruder",
        "Hyung Won Gupta",
        "Dara Chung",
        "Zhen Bahri",
        "Simon Qin",
        "Cong Baumgartner",
        "Donald Yu",
        "Metzler"
      ],
      "year": "2021",
      "venue": "Charformer: Fast character transformers via gradient-based subword tokenization",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Tensor2Tensor for neural machine translation",
      "authors": [
        "Ashish Vaswani",
        "Samy Bengio",
        "Eugene Brevdo",
        "Francois Chollet",
        "Aidan Gomez",
        "Stephan Gouws",
        "Llion Jones",
        "Łukasz Kaiser",
        "Nal Kalchbrenner",
        "Niki Parmar",
        "Ryan Sepassi",
        "Noam Shazeer",
        "Jakob Uszkoreit"
      ],
      "year": "2018",
      "venue": "Proceedings of the 13th Conference of the Association for Machine Translation in the Americas",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "a. Association for Computational Linguistics",
      "authors": [
        "Xinyi Wang",
        "Sebastian Ruder",
        "Graham Neubig"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.40"
    },
    {
      "id": "b40",
      "title": "CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
      "authors": [
        "Yue Wang",
        "Weishi Wang",
        "Shafiq Joty",
        "Steven C H Hoi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "authors": [
        "Yonghui Wu",
        "Mike Schuster",
        "Zhifeng Chen",
        "Quoc V Le",
        "Mohammad Norouzi",
        "Wolfgang Macherey",
        "Maxim Krikun",
        "Yuan Cao",
        "Qin Gao",
        "Klaus Macherey",
        "Jeff Klingner",
        "Apurva Shah",
        "Melvin Johnson",
        "Xiaobing Liu",
        "Lukasz Kaiser",
        "Stephan Gouws",
        "Yoshikiyo Kato",
        "Taku Kudo",
        "Hideto Kazawa",
        "Keith Stevens",
        "George Kurian",
        "Nishant Patil",
        "Wei Wang",
        "Cliff Young",
        "Jason Smith",
        "Jason Riesa",
        "Alex Rudnick",
        "Oriol Vinyals",
        "Greg Corrado",
        "Macduff Hughes",
        "Jeffrey Dean"
      ],
      "year": "2016",
      "venue": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Codebpe: Investigating Subtokenization",
      "text": "Options for Large Language Model Pretraining on Source Code Nadezhda Chirkova Naver Labs Europe nadia.chirkova@naverlabs.com &Sergey Troshin University of Amsterdam s.troshin@uva.nl Work done while being at HSE University."
    },
    {
      "title": "Abstract",
      "text": "Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, namely the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account code specifics. We propose subtokenization that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase."
    },
    {
      "title": "1 Introduction",
      "text": "With the inspiration from the success of large language model (LM) pretraining in natural language processing (NLP), BERT-like models have been widely adopted for source code processing (Feng et al., 2020; Kanade et al., 2020), as code has a similar discrete sequential structure to natural text. Being trained on huge source code corpora in a self-supervised manner, large LMs often substantially outperform domain-specific models developed purposely for applied tasks, especially in the tasks with limited parallel / labelled data (Ahmad et al., 2021). These tasks include fixing code bugs, generating text from code and vice versa, or translating code between programming languages. Recent works advanced large LM pretraining on source code in two main directions. First, various model kinds were utilized for source code: CodeBERT (Feng et al., 2020) and CuBERT (Kanade et al., 2020) rely on the classic encoder-only RoBERTa (Liu et al., 2019), CodeGPT (Lu et al., 2021) uses decoder-only GPT (Radford & Narasimhan, 2018), PLBART (Ahmad et al., 2021) is based on the denoising sequence-to-sequence BART (Lewis et al., 2020) model, and CodeT5 (Wang et al., 2021) utilizes multitask sequence-to-sequence T5 (Raffel et al., 2020). Second, a range of code-specific self-supervised pretraining tasks were proposed to enrich the classic masked language modeling (MLM) objective, e. g. GraphCodeBERT (Guo et al., 2021) predicts data flow connections during pretraining (one variable is computed from another variable), and CodeT5 (Wang et al., 2021) and DOBF (Roziere et al., 2021) use a variable naming objective. This work is devoted to investigating one more important component, subtokenization, which is usually not paid much attention when pretraining large LMs on source code. Modern LMs usually preprocess sequences using open-vocabulary models such as Byte-pair encoding (BPE, Sennrich et al., 2016) which split long tokens into smaller subtokens. Though this process is often referred to as tokenization, we call it subtokenization, to underline its smaller granularity. Subtokenization became a standard part of all widely-used LMs pretrained on natural text or code, because it ensures the relatively high frequency of all subtokens (compared to the whitespace-separated tokenization, which results in a large portion of out-of-vocabulary tokens), at the same time producing sequences of reasonable length (compared to character-level tokenization). Though subtokenization was initially introduced for NLP, it is especially relevant for code, as programming languages usually permit identifiers of unrestricted complexity, e. g. variable or function names (Chirkova & Troshin, 2021). Though subtokenization is often chosen with only superficial deliberation, it is one of the essential model components which may affect both quality and prediction speed. First, an inaccurately chosen subtokenization procedure may substantially increase sequence lengths and consequently slow down prediction. As a simple example, the work on CodeT5 (Wang et al., 2021) notices that using BPE trained specifically on source code corpora makes sequences 30-45% shorter than using BPE trained on natural text. Second, a line of recent research points at the positive effect of the carefully chosen subtokenization procedure on the model's performance in NLP. For example, Bostrom & Durrett (2020) show that using a UnigramLM (Kudo, 2018) subtokenization algorithm instead of BPE improves the quality of BERT-based question answering or textual entailment in English by 1%, and Ding et al. (2019) show that adjusting BPE vocabulary size in translation may produce +4 BLEU. At the same time, for large LMs, the particular subtokenization procedure chosen at the pretraining stage becomes an inseparable part of the model and must later be used in applied tasks. This underlines the need for a careful choice of subtokenization options when pretraining large LMs. In this work, we conduct a deep study of subtokenization options for large LM pretraining on source code, using PLBART as a testing ground. In addition to investigating general aspects, e. g. the subtokenization algorithm and the vocabulary size, we study the ways of adapting subtokenization to the specific properties of code, such as a large amount of punctuation marks and frequently-used token combinations, a variety of complex identifiers, or relative similarity of programming languages. We aim at choosing optimal subtokenization options that (a) lead to the best performance or (b) minimize sequence lengths (and thus speed up the model) without downstream performance drop. Our contributions are as follows - we show that for large LMs pretrained on source code: * Grouping punctuation chars in single tokens reduces the average length by 17% without downstream performance drop (we call this approach CodeBPE or CodeUnigramLM), and permitting more complex composite tokens reduces lengths by 40%, sometimes with quality drop (Section 1); * UnigramLM is generally preferable over BPE (Section 4); * Smaller vocabularies may improve quality with 3-19% length increase (Section 5); * Subtokenizers are well transferable between programming languages (Section 6); Our length-efficient subtokenization procedure (see examples in Figure 1) compresses sequences by 17% without quality drop and our most effective subtokenization improves performance by 0.5-2% significantly in three out of eight tasks and by one standard deviation in two other tasks."
    },
    {
      "title": "2 Methodology And Experimental Setup",
      "text": "The existing works on large LMs for source code usually choose a particular subtokenization library, for example the same as in the base LM the work uses, and train the subtokenizer with the vocabulary size of 30-50K on source code corpora used for pretraining. Often code is pre-processed before subtokenization, e. g. by replacing \\n with NEW_LINE, and split into tokens on white-spaces and punctuation marks so that these tokens are further split into subtokens, e. g. for i in range (vocSize) will be split into ['for', 'i', 'in', 'range', '(', 'vocSize', '1'] even if for i in is generally a frequent combination. The latter principle appears to be intuitively reasonable, since it ensures that subtokenization preserves syntactically meaningful boundaries of tokens (Kanade et al., 2020). We refer to this principle as prohibiting _composite tokens_. More details on subtokenization in different LMs for code are given in Section 7. Figure 1: Example subtokenizations (all numbers compared to the commonly used BPE-50K). We treat the described commonly-used approach as a baseline, and conduct a series of experiments, each modifying the baseline subtokenization procedure in one dimension and pretraining PLBART with the new subtokenization. The dimensions we vary are as follows: the allowed complexity of composite tokens, the subtokenization algorithm, the vocabulary size, the set of languages the subtokenizer is trained on, and the use of stochastic subtokenization. These dimensions are inspired either by the specifics of source code or by the recent works on subtokenization in NLP. Experimental setup.As our base model, we use PLBART (Ahmad et al., 2021), since it comes with the released pretraining code and data preprocessing routine under the MIT license. We use the same model size, the pretraining dataset size and other hyperparameter settings, including finetuning hyperparameters, as in PLBART1. In particular, we use an encoder-decoder Transformer architecture with 6 layers in each part, with the model dimension of 768 and 12 heads (140M parameters). The pretraining data consists of 230M Python functions, 470M Java functions (crawled through BigQuery2) and 47M natural language (NL) descriptions (crawled from StackOverflow3), referred to as sequences below. The BigQuery dataset consists of repositories with clear open-source license. We pretrain all our PLBART models for 100k updates, as in the original paper. Footnote 1: [https://github.com/wasiahmad/PLBART](https://github.com/wasiahmad/PLBART) Footnote 2: [https://console.cloud.google.com/marketplace/details/github/github-repos](https://console.cloud.google.com/marketplace/details/github/github-repos) Footnote 3: [https://archive.org/download/stackexchange](https://archive.org/download/stackexchange) As applied tasks, we consider three tasks from the PLBART paper: code generation (generating a Java function based on an NL description; CONCODE (Iyer et al., 2018) dataset, CodeBLEU (Ren et al., 2020) metric), code summarization (generating an NL description for a Python or Java function; CodeSearchNet (Husain et al., 2020) dataset, BLEU metric), code clone detection (classifying whether two Java functions implement the same functionality; BigCloneBench dataset (Svajelenko and Roy, 2015); F1 metric), and one additional task of code translation (translating code from Python to Java and vice versa; AVATAR dataset (Ahmad et al., 2021)). Here we consider original data with the CodeBLEU metric (Code Translation-1) and the smaller version of data with tests and the Computational Accuracy metric - which portion of generated functions passed all tests (Code Translation-2). We chose tasks so that we have both code generative and discriminative tasks and that datasets are either in Python or Java. We clip all sequences by 510 subtokens, except summarization where we clip by 250 subtokens following Ahmad et al. (2021). Such clipping remains the majority of sequences unclipped in all subtokenizations: 96-99.1% in the pretraining data, 93-99% in translation, 88-100% in generation, 76-93% in summarization, and 37-80% in clone detection. In the main text we report average lengths computed on the randomly chosen subset of pretraining data before clipping, Appendix A reports length statistics for downstream data with similar trends as observed for the pretraining data. We only clip sequences passed to neural networks and use unclipped target sequences when computing metrics. Baseline subtokenization.Following Ahmad et al. (2021), we use a SentencePiece (Kudo and Richardson, 2018) library, which is a one of the most widely used solutions for subtokenization. We train subtokenizers on 10M functions and NL descriptions randomly selected from the pretraining data (different from the random subset on which we measure average lengths). Though Ahmad et al. (2021) use BPE subtokenization algorithm, our baseline subtokenization uses another algorithm, UnigramLM, because it was shown to be quantitatively and qualitatively more suitable for pretraining in NLP than BPE (Bostrom and Durrett, 2020). We also perform their comparison for code in Section 4. We set the vocabulary size to 50K (the commonly used size for large LMs of code) and character coverage to 99.99% (enough to cover English chars and punctuation). We also use PLBART's preprocessing which includes removing comments and docstrings, replacing \\n, indents and dedents in Python with NEW_LINE, INDENT and DEDENT tokens as they are a part of the language syntax, and removing formatting in Java as it does not affect the language syntax. Our baseline subtokenizer follows the commonly used strategy of prohibiting composite tokens described above. The only exception we make is that we allow underscores _ inside tokens, because they do not represent a syntax unit, as other punctuation chars do. [MISSING_PAGE_FAIL:4] step further and allows merging dots. with textual tokens. This reduces the average length by 23% compared to Level 0. The motivation for Level 2 is that a lot of API name tokens almost always go with the dot, e. g..join or.split in Python. Figure 2 shows that Level 1 model performs similar or better than Level 0 model in all tasks, and Level 2 performs similar or better than Level 0 in six tasks, marginally worse in Python code summarization and significantly worse in Java code generation. Level 3 makes a step back from Level 4 and restricts the complexity of composite tokens in such a manner that each composed token may represent either a simple one-line code pattern or a punctuation combination, but could not combine them. Quantitatively, Level 3 performs generally better than Level 4, but (marginally or significantly) worse than the previous Level 2 in six tasks and similarly in two tasks (generation and clone detection). To sum up, _punctuation combinations (Level 1) result in sequence lengths reduction by 17% without performance drop in all tasks. We verify this result for BPE in Appendix B and call this approach CodeBPE or CodeUnigramLM. Length reduction could be increased up to 24% in most tasks by allowing dots attached to tokens (Level 2) and up to 40% in most code understanding tasks by allowing arbitrary subtoken combinations (Level 4)_. we investigate the transferability of subtokenizers between programming languages in Section 6. One of the potential issues with using composite tokens in code-generative tasks is that an inaccurate generation of a \"long\" token may change the entire following generated code. For example, in Java-Python code translation, a cycle which traverses all unique element pairs in an array, converts to for l in range ( 0, arr_size - 1 ) : for r in range ( 1 + 1, arr_size ) : While the Level 0 model generates exactly the specified cycle and the Level 1 model only modifies the first cycle: range ( arr_size - 1 ), making it even more concise, Level 3 model generates for l in range ( 0, arr_size ) : for r in range ( 0, arr_size ) : which results in traversing some elements twice. Here the first cycle begun with tokens 'for l in' and 'range ( 0,' and the second cycle begun with tokens 'for r in' and 'range ( 0,' where the latter repeats the previously used token and starts an incorrect line. However, according to our manual prediction analyses, such inaccurate generation, if it happens, rarely results in wrong code and often does not affect code semantics. For example, the Level 3 model may generate ['range ( 0,','n )'] instead of equivalent range(n). Another example is that this model may generate [ [ 0 ] * c for i in range ( r ) ] instead of two nested cycles by beginning with tokens '[ [' and '0 ] *', resulting in even more concise code. As for composite tokens in Level 1, they contain only punctuation and are \"simpler\" than in Level 3. Besides, Level 1 composite tokens serve more often for statement closing (e. g.') :' at the end of the cycle specification) than for a harder starting of new statements: 46.3% of Level 1 composite tokens contain only closing brackets, 12.8% contain only opening brackets and 26.7% contain both. We also check that using punctuation composite tokens does not deteriorate syntactic correctness: in Java-Python code translation-1, Level 0 and Level 1 models generate a similar number of syntactically correct test code snippets: 1226 and 1239 correspondingly. At the same time, for the Level 3 model, this quantity only equals 1163. In Appendix C, we also analyse how much do input and output subtoken sequences intersect in different Levels and find that generally the higher granularity leads to the lower intersection rate. This may be another explanation for the superiority of the lower granularity subtokenizations, as intuitively it should be easier for the model to predict correct subtokens if they are present in the input sequence. As autoregressive decoding is a slowest part of the encoder-decoder pipeline (Berard et al., 2021), it is important to check that the length statistics of sequences _generated_ by the models comprising composite tokens are close to those of the data. We check it for Java-Python translation-1: while groundtruth sequences at Levels 1 and 3 are 13.5% and 50% shorter than at Level 0, the generated sequences at these levels are 15% and 40% shorter than sequences generated at Level 0. [MISSING_PAGE_FAIL:6] kens, and measure the average Jaccard similarity \\(J(A,B)=|A\\cap B|/|A\\cup B|\\) between the set of native subtokens and the set of subtokens produced by each subtokenizer. The resulting score for UnigramLM, 26.6%, is much higher than for BPE, 15.2%. As could be observed from the third and the fourth rows in Table 2, sometimes subtokenizers join two native subtokens into one (isSame, GridBag). If we split each subtoken produced by a tokenizer based on CamelCase or snake_case to eliminate this effect and again measure average Jaccard similarities, UnigramLM's score, 55.2%, is still much higher than BPE's, 47.9%, again indicating that UnigramLM's tokenization is better aligned with the native one. In Appendix C we measure intersections between inputs and outputs in the sequnce-to-sequence tasks and find that UnigramLM leads to a slightly higher intersection rate than BPE, which may be connected to the better alignment with native subtokenization and serve as a a possible explanation of UnigramLM slight performance superiority. A relatively frequent pattern is that BPE tends to detach the first uppercase letter from native subtokens (H orizontally in row 4, _H ierarchy in row 5). Among 150K identifiers considered in the previous paragraph, 14.6% of BPE tokenizations contain at least one single uppercase letter X and 4.4% -- at least one subtoken of kind_X, while for UnigramLM these scores are substantially lower and equal to 11.8% and 1.4% correspondingly. At the same time, BPE merges two native subtokens more frequently (GridBag in row 3): 45.8% BPE tokenizations contain at least one token which could be split into two or more based on CamelCase, while for UnigramLM this score only equals to 39.2%."
    },
    {
      "title": "5 Vocabulary Size",
      "text": "This section studies the effect of vocabulary size, one of the main subtokenizer's hyperparameters, on the downstream quality of PLBART. Though the existing pretrained LMs for code use relatively large vocabularies of 30-50K tokens, we are interested, whether using smaller and less length-efficient vocabularies could result in better performance, and if yes, how large is the length increase. Figure 3 presents the comparison of PLBARTs trained with vocabulary sizes 50K (large), 10K (medium) and 2K (small). We find that in code translation, all vocabularies lead to similar performance. In code summarization, small and medium vocabularies outperform the large one by one standard deviation. In code generation, the medium vocabulary significantly outperforms the large one. Finally, in clone detection, decreasing the vocabulary size deteriorates quality. At the same time, with the large vocabulary, sequences are shorter than with the smaller vocabulary by 9.5% (10K) and 33% (2K), but the model size is larger (139M for 50K, 108M for 10K, and 102M for 2K). We conclude that _vocabulary size reduction may lead to a slight performance improvement but with sequences elongation_, thus it may be helpful in applications with high cost of errors and weak restrictions on sequences lengths. We verify the highlighted result for BPE in Appendix. We note that compared to BPE 50K which is used in most existing large LMs of code, UnigramLM 10K improves performance significantly in three tasks and by one standard deviation in two other tasks. Reducing vocabulary size increases the granularity of identifiers subtokenization, e. g. reachable is subtokenized as reachable with the 50K vocabulary, reach able - with 10K and reach able - with 2K. In other words, vocabulary size reduction may be seen as an even stronger prohibition of complex tokens than Level 0 in Section 1. Our results on the effectiveness of smaller granularity agree with the machine translation results of Ding et al. (2019). Programs in code generation and summarization data are more identifier-centered, e. g. the model often needs to choose a correct API based on the natural language description which seems to be easier by composing from smaller subtokens. On the contrary, in code translation, data is more algorithmic-centered, with mostly short identifiers encoded in 1-2 subtokens with all vocabulary sizes. The length increase of 10K vocabulary compared to 50K one is 6-19% in the former two tasks (6% in generation, 19% in summarization) and only 3.5% in the latter one (code-translation-1)."
    },
    {
      "title": "6 Transferability Between Programming Languages",
      "text": "Due to the high computational cost of large LM pretraining and relative programming languages similarity, e. g. compared to how dissimilar natural languages could be, pretrained LMs on source code are often used for programming languages that were not considered during pretraining. In this section, we investigate the effect of using a subtokenizer trained on one programming language for another programming language. Figure 4 visualizes the number of tokens having particular frequencies in Python and Java languages, and black rectangles denote language-specific areas. We find that the baseline Level 0 granularity vocabulary seems to be language-universal: the majority of subtokens have large frequencies in both languages, and only a small number of subtokens, 12.6%, are frequent in one language and rare in another. Interestingly, for Level 4 vocabulary, this quantity is not much higher, 20.1%, though it should include all language-specific composite tokens. As composite tokens occupy almost half of the Level 4 vocabulary, the remaining 30% composite tokens are common for two languages. Analysing sequence lengths (Figure 5), we observe that training the subtokenizer without Java (_Only Py_) shortens Python sequences marginally and increases Java sequences by 6.5% compared to the baseline subtokenizer trained on all data (_Py_+_Ja_). The latter happens because some widely used Java identifiers were not merged into single tokens as they are not used in Python; still, the length increase is not so large. For the Level 4 granularity subtokenizer, _Only Py_'s length increase on Java is larger, 13%, since it contains more language-specific composite tokens. However, due to common composite tokens, the resulting Level 4 _Only Py_'s Java average length is still smaller than Level 1 _Only Py_'s Java sequences: 79 vs. 83. As for performance, using the _Only Py_ subtokenizer instead of _Py_+_Ja_ changes quality up to one standard deviation and could both increase and decrease it on Java data (quality increase may be caused by the increased subtokenization granularity). Note that we only change subtokenizer configuration - PLBART is still pretrained on all languages, this may happen in practice if LM's developers use the subtokenizer from another project, e. g. for comparison purposes. Summing up, we conclude that _the baseline subtokenizer is universal and, if needed, could be used for other programming languages it was not trained on, with small length increase and slight quality change_. We note that Python and Java have quite different syntax and are usually used in different applications."
    },
    {
      "title": "7 Related Work",
      "text": "Subtokenization studies for NLP.Subtokenization has become an essential component of modern NLP pipelines and thus -- a subject of a line of empirical NLP studies. While word-based models suffer from the out-of-vocabulary problem, subtoken-based (open-vocabulary) as well as char-based approaches cover arbitrary novel words. Among various open-vocabulary approaches, BPE (Sennrich et al., 2016), WordPiece (Wu et al., 2016) and UnigramLM (Bostrom and Durrett, 2020) became most widely used, and UnigramLM was shown to outperform BPE for LM pre-training (Bostrom and Durrett, 2020). A line of studies investigate the optimal granularity of word subtokenization: Ding et al. (2019) find that in Transformer-based neural machine translation, small vocabularies of 0-4K subtokens outperform large ones by up to 4 BLEU, and VOLT (Radford et al., 2018) automates the search of a proper subtoken vocabulary with a proper size by formulating it Figure 4: Number of tokens of different frequency in two languages, UnigramLM 50K vocabularies. Figure 5: Results of transferability between programming languages. Py+Ja – subtokenizer is trained on all data (baseline), Only Py – on Python and natural language data only. as an optimal transport problem. The smallest char-based granularity is often avoided because of substantial sequences elongation, but has particular strengths, e. g. much less number of hyperparameters and better robustness, and thus appears to be a promising research direction (Gupta et al., 2019; Clark et al., 2021; Tay et al., 2021). Provilkov et al. (2020); Kudo (2018) propose stochastic subtokenization as a way to improve new words composition and (Wang et al., 2021) adapt it to pretrained LMs. Finally, an actively studied challenge is that various natural languages need different subtokenization decisions and are hard to subtokenize with one common model (Chung et al., 2020; Rust et al., 2021). Our work investigates most of the specified directions for source code. For a more detailed review on subtokenization, see (Mielke et al., 2021). Subtokenization practices in neural source code processing.Subtokenization was first tested for source code in (Karampatsis et al., 2020) and later used in the majority of Transformer-based models. Almost all LMs pretrained on source code use BPE-like subtokenization with large vocabulary: CodeBERT uses the WordPiece (Wu et al., 2016) algorithm (a modified BPE, 50K), CuBERT - an algorithm from the Tensor2Tensor project (Vaswani et al., 2018) (50K), PLBART and CodeGPT - BPE (50K), CodeT5 - byte-level BPE (32K), DOBF uses a subtokenization procedure of either CodeBERT or Roziere et al. (2020) (BPE 64K) for fair comparison, AlphaCode Li et al. (2022) - SentencePiece (8K, algorithm not specified), InCoder Fried et al. (2022) - BPE (50K). To the best of our knowledge, existing works do not provide an in-depth experimental analysis of various subtokenization options for code and, particularly, do not investigate various levels of composite tokens complexity. Though the concurrent work of Fried et al. (2022) uses unrestricted composite tokens (our Level 4), they do not compare them to any other subtokenizations. Level 4 composite tokens are conceptually similar to code idioms used in (Iyer et al., 2019; Shin et al., 2019) for code generation, but the mentioned works develop specific procedures for mining idioms, which need separate implementation, while we rely on the commonly-used subtokenization procedure."
    },
    {
      "title": "8 Conclusion",
      "text": "In this work, we conducted an empirical study of varying subtokenization options for large LMs pretraining on source code. We believe that main the value of our work is not in improved numerical criteria, but importantly in providing _reference experiments_ for the community showing which impact (both substantial or small) subtokenization choices may have in pretraining LMs for code. This underlines which directions to look more carefully at in practice (the use of composite tokens) and which are less important to experiment with (vocabulary size, subtokenization algorithm), and whether the latter directions can bring at least a slight improvement or not (yes, they can). Currently most works experiment with subtokenizations options only superficially or do not experiment at all, and we hope that our work will provide motivation to do that. Our recommendations.As for particular direct recommendations from our results, first, we recommend to use the proposed punctuation combination approach, which we call CodeBPE or Code-UnigramLM depending on the used subtokenization algorithm, that shortens sequences by 17% without quality drop. We suppose that with larger pretrained LMs higher levels of composite tokens may also achieve comparable performance; we were not able to experiment with then as they require very extensive computational resources. Second, if changing the subtokenization algorithm is easy, e.g. when using the SentencePiece library, we recommend using the UnigramLM, since it performs slightly better than commonly used BPE with similar lengths. Third, we recommend considering releasing models with smaller vocabularies, as they may perform slightly better than larger vocabularies. In our experiments the UnigramLM-10K subtokenizer was 0.5-2% more effective than the commonly-used BPE 50K in 5/8 experiments, but with 3.5-19% length increase. LimitationsThe main work's limitation is that we consider only the PLBART model, due to the limited computational resources. However, we believe that the provided recommendations will motivate and simplify the process of the subtokenizer's tuning for future works, as described above. Another limitation is that we focus on finding optimal subtokenization options only for source code, though some downstream tasks also include the processing of natural language. Investigating the ways of choosing optimal subtokenization for both code and natural language may be an interesting direction for future research. Finally, we only compare BPE and UnigramLM, while it could be interesting to investigate the performance of other algorithms, e. g. WordPiece."
    },
    {
      "title": "Broader Impact",
      "text": "We do not anticipate any direct negative social impact of our work. However, our results may potentially be used for developing new pretrained LMs for source code, and a detailed discussion on their broader impact is provided in Chen et al. (2021) (Section 7), e. g. over-reliance on generated code or producing vulnerable code. Unfortunately, our work may cause negative environmental impact because of computation (\\(\\sim\\)5K Tesla A-100 GPU hours and \\(\\sim\\)4K Tesla V-100 GPU hours at the internal cluster)."
    },
    {
      "title": "Acknowledgments",
      "text": "The results were supported by the Russian Science Foundation grant No. 19-71-30020. The research was supported in part through the computational resources of HPC facilities at NRU HSE."
    },
    {
      "title": "References",
      "text": "* Ahmad et al. (2021) Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-training for program understanding and generation. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 2655-2668, Online, June 2021a. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2021.naacl-main.211](https://www.aclweb.org/anthology/2021.naacl-main.211). * Ahmad et al. (2021) Wasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, and Kai-Wei Chang. Avatar: A parallel corpus for java-python program translation, 2021b. * Berard et al. (2021) Alexandre Berard, Dain Lee, Stephane Clinchant, Kweonwoo Jung, and Vassilina Nikoulina. Efficient inference for multilingual neural machine translation. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 8563-8583, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.674. URL [https://aclanthology.org/2021.emnlp-main.674](https://aclanthology.org/2021.emnlp-main.674). * Bostrom and Durrett (2020) Kaj Bostrom and Greg Durrett. Byte pair encoding is suboptimal for language model pretraining. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pp. 4617-4624, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.414. URL [https://aclanthology.org/2020.findings-emnlp.414](https://aclanthology.org/2020.findings-emnlp.414). * Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Kaite Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. * Chirkova and Troshin (2021) Nadezhda Chirkova and Sergey Troshin. A simple approach for handling out-of-vocabulary identifiers in deep learning for source code. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 278-288, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.26. URL [https://aclanthology.org/2021.naacl-main.26](https://aclanthology.org/2021.naacl-main.26). * Chung et al. (2021) Hyung Won Chung, Dan Garrette, Kiat Chuan Tan, and Jason Riesa. Improving multilingual models with language-clustered vocabularies. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 4536-4546, Online, November2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.367. URL [https://aclanthology.org/2020.emnlp-main.367](https://aclanthology.org/2020.emnlp-main.367). * Clark et al. (2021) Jonathan H. Clark, Dan Garrette, Julia Turc, and John Wieting. Canine: Pre-training an efficient tokenization-free encoder for language representation, 2021. * Ding et al. (2019) Shuoyang Ding, Adithya Renduchintala, and Kevin Duh. A call for prudent choice of subword merge operations in neural machine translation. In _Proceedings of Machine Translation Summit XVII: Research Track_, pp. 204-213, Dublin, Ireland, August 2019. European Association for Machine Translation. URL [https://aclanthology.org/W19-6620](https://aclanthology.org/W19-6620). * Feng et al. (2020) Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained model for programming and natural languages. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pp. 1536-1547, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.139. URL [https://www.aclweb.org/anthology/2020.findings-emnlp.139](https://www.aclweb.org/anthology/2020.findings-emnlp.139). * Fried et al. (2022) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis, 2022. URL [https://arxiv.org/abs/2204.05999](https://arxiv.org/abs/2204.05999). * Guo et al. (2021) Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie LIU, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. Graphcode{bert}: Pre-training code representations with data flow. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=jLoC4e243PZ](https://openreview.net/forum?id=jLoC4e243PZ). * Gupta et al. (2019) Rohit Gupta, Laurent Besacier, Marc Dymetman, and Matthias Galle. Character-based nmt with transformer. _ArXiv_, abs/1911.04997, 2019. * Husain et al. (2020) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Code-searchnet challenge: Evaluating the state of semantic code search, 2020. * Iyer et al. (2018) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to code in programmatic context. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pp. 1643-1652, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1192. URL [https://aclanthology.org/D18-1192](https://aclanthology.org/D18-1192). * Iyer et al. (2019) Srinivasan Iyer, Alvin Cheung, and Luke Zettlemoyer. Learning programmatic idioms for scalable semantic parsing. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 5426-5435, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1545. URL [https://aclanthology.org/D19-1545](https://aclanthology.org/D19-1545). * Kanade et al. (2020) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating contextual embedding of source code. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 12-18 July 2020_, Proceedings of Machine Learning Research. PMLR, 2020. * Karampatsis et al. (2020) Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and Andrea Janes. Big code!= big vocabulary: Open-vocabulary models for source code. In _Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering_, ICSE '20, pp. 1073-1085, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450371216. doi: 10.1145/3377811.3380342. URL [https://doi.org/10.1145/3377811.3380342](https://doi.org/10.1145/3377811.3380342). * Kudo (2018) Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 66-75, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1007. URL [https://aclanthology.org/P18-1007](https://aclanthology.org/P18-1007). * Kudo and Richardson (2018) Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pp. 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL [https://aclanthology.org/D18-2012](https://aclanthology.org/D18-2012). * Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 7871-7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL [https://aclanthology.org/2020.acl-main.703](https://aclanthology.org/2020.acl-main.703). * Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode, 2022. URL [https://arxiv.org/abs/2203.07814](https://arxiv.org/abs/2203.07814). * Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _ArXiv_, abs/1907.11692, 2019. * Lu et al. (2021) Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Syvatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code understanding and generation. _CoRR_, abs/2102.04664, 2021. * Mielke et al. (2021) Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Galle, Arun Raja, Chenglei Si, Wilson Y. Lee, Benoit Sagot, and Samson Tan. Between words and characters: A brief history of open-vocabulary modeling and tokenization in nlp, 2021. * Provilkov et al. (2020) Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. BPE-dropout: Simple and effective subword regularization. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 1882-1892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.170. URL [https://aclanthology.org/2020.acl-main.170](https://aclanthology.org/2020.acl-main.170). * Radford and Narasimhan (2018) Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. * Radford et al. (2018) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2018. URL [https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). * Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html). * Ren et al. (2020) Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, M. Zhou, Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis. _ArXiv_, abs/2009.10297, 2020. * Roziere et al. (2020) Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised translation of programming languages. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 20601-20611. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/ed23fbf18c2cd35f8c7f8de44f85c08d-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/ed23fbf18c2cd35f8c7f8de44f85c08d-Paper.pdf). * Raffel et al. (2019)* Roziere et al. (2021) Baptiste Roziere, Marie-Anne Lachaux, Marc Szafraniec, and Guillaume Lample. Dobf: A deobfuscation pre-training objective for programming languages. In _Advances in Neural Information Processing Systems (NeurIPS 2021)_, Online, 2021. * Rust et al. (2021) Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder, and Iryna Gurevych. How good is your tokenizer? on the monolingual performance of multilingual language models. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pp. 3118-3135, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.243. URL [https://aclanthology.org/2021.acl-long.243](https://aclanthology.org/2021.acl-long.243). * Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1715-1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL [https://aclanthology.org/P16-1162](https://aclanthology.org/P16-1162). * Shin et al. (2019) Richard Shin, Miliadis Allamanis, Marc Brockschmidt, and Oleksandr Polozov. _Program Synthesis and Semantic Parsing with Learned Code Idioms_. Curran Associates Inc., Red Hook, NY, USA, 2019. * Svajlenko & Roy (2015) Jeffrey Svajlenko and Chanchal K. Roy. Evaluating clone detection tools with bigclonebench. In _2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)_, pp. 131-140, 2015. doi: 10.1109/ICSM.2015.7332459. * Tay et al. (2021) Yi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. Charformer: Fast character transformers via gradient-based subword tokenization, 2021. * Vaswani et al. (2018) Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan Gomez, Stephan Gouws, Llion Jones, Lukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2Tensor for neural machine translation. In _Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)_, pp. 193-199, Boston, MA, March 2018. Association for Machine Translation in the Americas. URL [https://aclanthology.org/W18-1819](https://aclanthology.org/W18-1819). * Wang et al. (2021a) Xinyi Wang, Sebastian Ruder, and Graham Neubig. Multi-view subword regularization. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 473-482, Online, June 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.40. URL [https://aclanthology.org/2021.naacl-main.40](https://aclanthology.org/2021.naacl-main.40). * Wang et al. (2021b) Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 8696-8708, Online and Punta Cana, Dominican Republic, November 2021b. Association for Computational Linguistics. URL [https://aclanthology.org/2021.ennlp-main.685](https://aclanthology.org/2021.ennlp-main.685). * Wu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshixjko Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google's neural machine translation system: Bridging the gap between human and machine translation. _CoRR_, abs/1609.08144, 2016. URL [http://arxiv.org/abs/1609.08144](http://arxiv.org/abs/1609.08144). A Average lengths on the finetuning data In the main text, we report average lengths computed over a randomly chosen subset of the pretraining data (different from the random subsets subtokenizers were pretrained on). Table 3 reports average lengths computed over finetuning data, we also include average lengths computed over held-out pretraining data. For programming languages, we observe similar trends as on the general pretraining data: Level 1 compresses sequences by 11.6-17.7% (the lower compression in the code summarization task, 11.6-12.8%, is explained by the higher percent of identifiers in this data than in other tasks); Level 4 compresses sequences by 32.4-50%; 10K and 2K vocabularies increase lengths by 3.5-10.2% and 12.7-35.9% respectively; and BPE and UnigramLM average lengths are similar. As for natural text, its average lengths are not affected by the Level 1 subtokenization because it only affects punctuation char sequences, rarely present in the natural language data. At the same time, higher level subtokenizers may produce natural language subtoken sequences of higher lengths, because their vocabularies are occupied by programming-focused composite tokens rarely present in natural language data, while frequent words may be absent in these vocabularies. Small vocabulary subtokenizers also have programming language-focused vocabularies, resulting in higher lengths increase for natural text than for programming languages. In Table 4, we report time needed to generate predictions for the test set in two tasks. The measurement was conducted on a single Tesla V100 GPU, in the same session for all runs. The speed-up is stronger than the length decrease, because of quadratic Transformer complexity."
    },
    {
      "title": "Appendix B Additional Experiments With Bpe",
      "text": "Figure 6 presents the comparison of BPE Level 0 and Level 1 subtokenizations and of BPE 50K and 10K vocabularies. The results are similar to those of UnigramLM reported in the main text: the performance of Level 0 and Level 1 subtokenizations is again close, the model with 10K vocabulary again significantly outperforms the model with 50K vocabulary in code generation and outperforms by one standard deviation in Java code summarization, and in other tasks the performance of 10K and 50K models is similar."
    },
    {
      "title": "Appendix C Additional Analysis",
      "text": "Additional experiments with \"fair\" subtoken sequence cropping.In our main experiments, we clip all subtoken sequences with the same maximum length threshold, as this scenario is most close to how it is done in practice. It should be noted that for the vast majority of examples, subtoken sequences fit to the maximum length limit and are not affected by this clipping (see statistics in the main text, Section 2). However, for the remaining long examples, subtoken sequences produced by subtokenizers of higher granularity may be detokenized into longer character sequences, than that of the smaller granularity subtokenizer's. For example, if one uses the maximum length of 4 (only used for illustration), then the code snippet x = sum ( numbers ) may be tokenized by the Level 4 subtokenizer into ['x =','sum ( ','numbers',')'] (fits into the limit) and by 2K subtokenizer into ['x', '=','sum','(','numbers',')'], which will be clipped into ['x', '=','sum','('] with Figure 6: Comparison of BPE Level 0, BPE Level 1 (both with 50K vocabulary as in the main text) and BPE 10K. [MISSING_PAGE_EMPTY:15] that longer (less compact) subtokenizations, most affected by clipping, generally perform _better_ than shorter (more compact) subtokenizations which allow less information loss. Using the more \"fair\" setting may make the performance of more compact subtokenizations worse (and not affect less compact subtokenizations), amplifying performance differences further. In this section, we conduct such a \"fair\" experiment for three representative downstream tasks and crop subtoken sequences produced by all subtokenizers so that they are all detokenized into a similar character sequence. In the example above, the 2K subtokenization will stay unchanged while the Level 4 subtokenization will be cropped into ['x =','sum ('] to achieve equality. The results are shown in Figure 7. Comparing the observations to the hypothesis given above, we observe that the performance of smaller vocabulary subtokenizations, less affected by cropping, stays similar (as expected) and that the performance of higher granularity subtokenizations, most affected by cropping, often reduces (Levels 0 and 1 in translation, Level 1 in summarization) or stays similar (Level 0 in summarization, Level 1 in clone detection), again as we expected. Surprisingly, in some cases (Level 4 in all tasks and Level 0 in clone detection), the performance of higher granularity subtokenizations improve slightly after cropping. We hypothesize that the reason may be that the last parts of sequences may be not important for correct prediction, e. g. if _name == \"main _\" statements in Python translation examples may be auxiliary and not used during translation. As for the conclusions emphasized in the main text, they hold for the cropped setting as well: (a) the marginal performance superiority of smaller vocabularies compared to the Level 0 50K vocabulary, is amplified (translation) or similar (summarization) as in the main text; (b) the relative performance of Level 0 and Level 4 is similar as in the main text; (c) the observation that the Level 1 subtokenization performs not worse than the Level 0 subtokenization stays same as in the main text. Interestingly, in Java code summarization, Level 1 subtokenization performs slightly better than Level 0 subtokenization in the main text, while after cropping their performance is similar. We thus attribute the superiority of Level 1 in the main text to the less strong clipping than of Level 0 subtokenizations. How much do input and output sequences intersect for various subtokenizations?In attempt to better understand the effect of subtokenizations on downstream performance, we measure the Jaccard similarity \\(J(A,B)=|A\\cap B|/|A\\cup B|\\) between the sets of input and output subtokens in sequence-to-sequence tasks. The intuition is that it should be easier for the model to predict correct subtokens if they are present in the input sequence. We only include textual subtokens in the considered sets (subtokens which do not include any programming language punctuation), since \"copying\" subtokens which include punctuation seems to be unrealistic (in text-to-code or code-to-text tasks, the text part does not include programming language punctuation at all, and in the translation task, punctuation of Python and Java are very dissimilar). We consider the setting with \"fair\" cropping described in the previous paragraph, however for the conventional setting the results are similar. The results are presented in Table 5. The general trend that the higher the granularity, the lower the intersection rate, appears to be reasonable. For example, we observe the explainable monotonic Figure 7: Additional experiments with subtoken sequences being cropped so that for each datapoint, the subtoken sequences produced by all subtokenizers are detokenized into a similar character sequence. Effectively this cropping means that the 2K subtokenizer’s sequences stay unchanged while all others may be cropped, and the Level 4 subtokenizer’s sequences are cropped the most. Blurred bars visualize original performance reported in the main paper, with “unfair” clipping used in practice. Both 2K and 10K subtokenizations use Level 0 preprocessing and all Level X subtokenizations use the 50K vocabulary, all experiments with UnigramLM. decrease in intersection for 2K-10K-50K vocabularies: the smaller the vocabulary, the smaller the granularity of identifiers/words subtokenization, the more chance that different parts of words will repeat. This correlates with our empirical observation that generally smaller vocabulary subtokenizations perform slightly better. Interestingly, Level 1 subtokenization leads to a slightly higher intersection rate than Level 0. We explain it that punctuation combinations occupy a portion of vocabulary (3.4%) and thus reduce the effective vocabulary allocated for textual tokens, which will be slightly more often split into parts. _This effect shows that if one has some \"length budget\", it is better to be spent on splitting identifiers into subtokens rather than considering punctuation chars as separate tokens (they can be grouped)_. For further levels, two forces start competing: the first one that composite tokens occupy a part of the vocabulary and thus out-of-vocabulary identifiers are split into smaller pieces that are repeated relatively frequently, and the second one that composite tokens themselves are longer and are repeated rarely between input and output. The second force is absent for Level 1 considered above because Level 1 composite tokens are punctuation-only and do not include letters or digits. We observe that for Level 2, the first force outweighs the second one in translation and summarization (intersection rate is higher than for Levels 0 and 1) and underweights in generation (intersection rate lower than in Levels 0 and 1). Interestingly, this correlates with performance of Level 2 compared to Levels 0 and 1: it is on par with them in translation and summarization and lower in generation. Further on, at Levels 3 and 4 the intersection rate drops, with small increase of Level 4 compared to Level 3, explainable by the specifics of Level 3 construction focused on programming statements. Finally, we compare the intersection rate of UnigramLM and BPE (both Level 0, 50K) and find that in summarization and generation it is slightly higher for UnigramLM than for BPE and in translation they are equal. This observation complements our findings that UnigramLM subtokenizations are better aligned with native subtokenizations than BPE subtokenizations and provides some intuition how it matters (similarity to native subtokenization presumably increases the number of repeated subtokens which should be easier to correctly predict). Subtoken frequences visualization.In Figure 8, we visualize subtoken frequencies computed over 1/8 of the pretraining corpora for four subtokenizers: UnigramLM Level 0 50K, BPE Level 0 50K, UnigramLM Level 1 50K and UnigramLM Level 4 50K. Comparing UnigramLM and BPE, we find that UnigramLM has a slightly heavier tail which results in a greater number of subtokens having higher frequencies and thus better embeddings, the similar observation was also noticed in Bostrom & Durrett (2020). The frequency profiles of Level 0 and Level 1 subtokenizers are close. Level 4 vocabulary exhibits less contrast in frequencies than Levels 0 and 1: in the left range, Level 4 frequencies are lower, while in the right range, Level 4 frequencies are higher, which hypothetically could provide some advantage to Level 4 (but it does not, according to the reported performance results). We hypothesize that having high frequencies in the left range is important for high performance (more reliable \"basic\" subtokens) but high granularity subtokens reduce frequencies of smaller subtokens which negatively affects their embeddings."
    },
    {
      "title": "Appendix D Numerical Results",
      "text": "Table 6 presents the numerical results for figures in the main text and Appendix B. \\begin{table} \\begin{tabular}{l c c c c c c c c} \\hline & 2K & 10K & Level 0 & Level 1 & Level 2 & Level 3 & Level 4 & BPE \\\\ \\hline Code trans. (Py) & 14.61 & 13.91 & 13.68 & 13.81 & 14.38 & 6.65 & 7.79 & 13.67 \\\\ Code gen. (Ja) & 11.65 & 10.51 & 9.57 & 9.61 & 8.94 & 4.70 & 6.51 & 8.88 \\\\ Code sum. (Ja) & 10.25 & 7.26 & 5.99 & 6.03 & 6.33 & 2.84 & 5.80 & 5.67 \\\\ \\hline \\end{tabular} \\end{table} Table 5: Average Jaccard similarities between the sets of input and output textual subtokens, for different subtokenizations. All subtokenizations expect the last one use the UnigramLM algorithm; Base ans Level 1–4 subtokenizations use vocabulary of 50K; 10K and 2K subtokenizations are based on Level 1 preprocessing. \\begin{table} \\begin{tabular}{l|c c c c c c c c} \\hline \\hline **Subtokenizer** & \\begin{tabular}{c} **CT1** \\\\ **(Py)** \\\\ \\end{tabular} & \\begin{tabular}{c} **CT1** \\\\ **(Ja)** \\\\ \\end{tabular} & \\begin{tabular}{c} **CT2** \\\\ **(Py)** \\\\ \\end{tabular} & \\begin{tabular}{c} **CT2** \\\\ **(Ja)** \\\\ \\end{tabular} & \\begin{tabular}{c} **CS** \\\\ **(Ja)** \\\\ \\end{tabular} & \\begin{tabular}{c} **CS** \\\\ **(Ja)** \\\\ \\end{tabular} & \\begin{tabular}{c} **CS** \\\\ **(Ja)** \\\\ \\end{tabular} & \\begin{tabular}{c} **CG** \\\\ **(Ja)** \\\\ \\end{tabular} \\\\ \\hline UnigramLM 50K Level 0 & 46.1 & 48.2 & 65.3 & 57.1 & 19.7 & 18.9 & 38.2 & 97.8 \\\\ UnigramLM 50K Level 1 & 45.9 & 48.4 & 67.3 & 57.8 & 19.7 & 19.4 & 38.2 & 98.3 \\\\ UnigramLM 50K Level 2 & 45.9 & 48.0 & 67.0 & 56.8 & 19.5 & 19.3 & 37.3 & 98.2 \\\\ UnigramLM 50K Level 3 & 45.0 & 47.7 & 56.7 & 45.5 & 19.5 & 19.1 & 37.5 & 98.5 \\\\ UnigramLM 50K Level 4 & 44.2 & 46.7 & 54.3 & 43.7 & 19.5 & 18.9 & 36.7 & 98.3 \\\\ UnigramLM 10K Level 0 & 45.8 & 48.6 & 65.7 & 57.3 & 19.9 & 19.2 & 39.1 & 97.7 \\\\ UnigramLM 2K Level 0 & 46.2 & 48.0 & 66.1 & 56.2 & 19.8 & 19.2 & 39.1 & 97.5 \\\\ UnigramLM 50K Level 0 & 46.1 & 47.5 & 68.3 & 58.6 & 19.8 & 18.8 & 38.6 & 98.0 \\\\ (Only Py) & & & & & & & & \\\\ BPE 50K Level 0 & 45.5 & 47.7 & 66.5 & 57.4 & 19.3 & 18.8 & 37.7 & 98.0 \\\\ BPE 50K Level 1 & 45.9 & 48.0 & 65.5 & 56.9 & 19.6 & 19.0 & 37.4 & 98.3 \\\\ BPE 10K Level 0 & 45.5 & 48.1 & 66.5 & 57.2 & 19.5 & 19.2 & 38.9 & 97.9 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: Numerical data for figures in the main text. CT1: Code Translation-1 (CodeBLEU), CT2: Code Translation 2 (Computational Accuracy), CS: Code Summarization (BLEU), CG: Code Generation (CodeBLEU), CD: Clone Detection (F1). Py – Python, Ja – Java. Figure 8: Subtoken frequencies over pretraining data."
    }
  ]
}