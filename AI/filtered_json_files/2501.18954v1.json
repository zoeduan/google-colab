{
  "title": "LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models",
  "authors": [
    "Shenghao Fu",
    "Qize Yang",
    "Qijie Mo",
    "Junkai Yan",
    "Xihan Wei",
    "Jingke Meng",
    "Xiaohua Xie",
    "Wei-Shi Zheng"
  ],
  "abstract": "\n Recent open-vocabulary detectors achieve promising performance with abundant region-level annotated data. In this work, we show that an open-vocabulary detector cotraining with a large language model by generating imagelevel detailed captions for each image can further improve performance. To achieve the goal, we first collect a dataset, GroundingCap-1M, wherein each image is accompanied by associated grounding labels and an imagelevel detailed caption. With this dataset, we finetune an open-vocabulary detector with training objectives including a standard grounding loss and a caption generation loss. We take advantage of a large language model to generate both region-level short captions for each region of interest and image-level long captions for the whole image. Under the supervision of the large language model, the resulting detector, LLMDet, outperforms the baseline by a clear margin, enjoying superior open-vocabulary ability. Further, we show that the improved LLMDet can in turn build a stronger large multi-modal model, achieving mutual benefits. The code, model, and dataset is available at  https:  //github.com/iSEE-Laboratory/LLMDet . \n",
  "references": [
    {
      "id": null,
      "title": "LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models",
      "authors": [
        "Shenghao Fu",
        "Qize Yang",
        "Qijie Mo",
        "Junkai Yan",
        "Xihan Wei",
        "Jingke Meng",
        "Xiaohua Xie",
        "Wei-Shi Zheng"
      ],
      "year": "2025",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Nltk: the natural language toolkit",
      "authors": [
        "Steven Bird"
      ],
      "year": "2006",
      "venue": "Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "End-toend object detection with transformers",
      "authors": [
        "Nicolas Carion",
        "Francisco Massa",
        "Gabriel Synnaeve",
        "Nicolas Usunier",
        "Alexander Kirillov",
        "Sergey Zagoruyko"
      ],
      "year": "2020",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "MMDetection: Open mmlab detection toolbox and benchmark",
      "authors": [
        "Kai Chen",
        "Jiaqi Wang",
        "Jiangmiao Pang",
        "Yuhang Cao",
        "Yu Xiong",
        "Xiaoxiao Li",
        "Shuyang Sun",
        "Wansen Feng",
        "Ziwei Liu",
        "Jiarui Xu",
        "Zheng Zhang",
        "Dazhi Cheng",
        "Chenchen Zhu",
        "Tianheng Cheng",
        "Qijie Zhao",
        "Buyu Li",
        "Xin Lu",
        "Rui Zhu",
        "Yue Wu",
        "Jifeng Dai",
        "Jingdong Wang",
        "Jianping Shi",
        "Wanli Ouyang",
        "Chen Change Loy",
        "Dahua Lin"
      ],
      "year": "2019",
      "venue": "MMDetection: Open mmlab detection toolbox and benchmark",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Sharegpt4v: Improving large multi-modal models with better captions",
      "authors": [
        "Lin Chen",
        "Jinsong Li",
        "Xiaoyi Dong",
        "Pan Zhang",
        "Conghui He",
        "Jiaqi Wang",
        "Feng Zhao",
        "Dahua Lin"
      ],
      "year": "2024",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
      "authors": [
        "Zhe Chen",
        "Jiannan Wu",
        "Wenhai Wang",
        "Weijie Su",
        "Guo Chen",
        "Sen Xing",
        "Muyan Zhong",
        "Qinglong Zhang",
        "Xizhou Zhu",
        "Lewei Lu"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Yolo-world: Real-time openvocabulary object detection",
      "authors": [
        "Tianheng Cheng",
        "Lin Song",
        "Yixiao Ge",
        "Wenyu Liu",
        "Xinggang Wang",
        "Ying Shan"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph E Gonzalez",
        "Ion Stoica",
        "Eric P Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Evaluating large-vocabulary object detectors: The devil is in the details",
      "authors": [
        "Achal Dave",
        "Piotr Dollár",
        "Deva Ramanan",
        "Alexander Kirillov",
        "Ross Girshick"
      ],
      "year": "2021",
      "venue": "Evaluating large-vocabulary object detectors: The devil is in the details",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Coarse-to-fine vision-language pre-training with fusion in the backbone",
      "authors": [
        "Zi-Yi Dou",
        "Aishwarya Kamath",
        "Zhe Gan",
        "Pengchuan Zhang",
        "Jianfeng Wang",
        "Linjie Li",
        "Zicheng Liu",
        "Ce Liu",
        "Yann Le-Cun",
        "Nanyun Peng"
      ],
      "year": "2022",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "A comprehensive evaluation benchmark for multimodal large language models",
      "authors": [
        "Chaoyou Fu",
        "Peixian Chen",
        "Yunhang Shen",
        "Yulei Qin",
        "Mengdan Zhang",
        "Xu Lin",
        "Jinrui Yang",
        "Xiawu Zheng",
        "Ke Li",
        "Xing Sun"
      ],
      "year": "2023",
      "venue": "A comprehensive evaluation benchmark for multimodal large language models",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Building strong one-decoder-layer sparse detectors via adaptive sparse anchor generation",
      "authors": [
        "Shenghao Fu",
        "Junkai Yan",
        "Yipeng Gao",
        "Xiaohua Xie",
        "Wei-Shi Zheng",
        "Asag"
      ],
      "year": "2023",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Frozen-detr: Enhancing detr with image understanding from frozen foundation models",
      "authors": [
        "Shenghao Fu",
        "Junkai Yan",
        "Qize Yang",
        "Xihan Wei",
        "Xiaohua Xie",
        "Wei-Shi Zheng"
      ],
      "year": "2024",
      "venue": "Frozen-detr: Enhancing detr with image understanding from frozen foundation models",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Open-vocabulary object detection via vision and language knowledge distillation",
      "authors": [
        "Xiuye Gu",
        "Tsung-Yi Lin",
        "Weicheng Kuo",
        "Yin Cui"
      ],
      "year": "",
      "venue": "ICLR, 2021",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Lvis: A dataset for large vocabulary instance segmentation",
      "authors": [
        "Agrim Gupta",
        "Piotr Dollar",
        "Ross Girshick"
      ],
      "year": "2007",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Piotr Dollár, and Ross Girshick. Mask r-cnn",
      "authors": [
        "Kaiming He",
        "Georgia Gkioxari"
      ],
      "year": "2017",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "authors": [
        "A Drew",
        "Christopher D Hudson",
        "Manning"
      ],
      "year": "2004",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Gpt-4o system card",
      "authors": [
        "Aaron Hurst",
        "Adam Lerer",
        "Adam P Goucher",
        "Adam Perelman",
        "Aditya Ramesh",
        "Aidan Clark",
        "Akila Ostrow",
        "Alan Welihinda",
        "Alec Hayes",
        "Radford"
      ],
      "year": "2024",
      "venue": "Gpt-4o system card",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "T-rex2: Towards generic object detection via text-visual prompt synergy",
      "authors": [
        "Qing Jiang",
        "Feng Li",
        "Zhaoyang Zeng",
        "Tianhe Ren",
        "Shilong Liu",
        "Lei Zhang"
      ],
      "year": "2024",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Mdetrmodulated detection for end-to-end multi-modal understanding",
      "authors": [
        "Aishwarya Kamath",
        "Mannat Singh",
        "Yann Lecun",
        "Gabriel Synnaeve",
        "Ishan Misra",
        "Nicolas Carion"
      ],
      "year": "2021",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Referitgame: Referring to objects in photographs of natural scenes",
      "authors": [
        "Sahar Kazemzadeh",
        "Vicente Ordonez",
        "Mark Matten",
        "Tamara Berg"
      ],
      "year": "2014",
      "venue": "EMNLP",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "F-vlm: Open-vocabulary object detection upon frozen vision and language models",
      "authors": [
        "Weicheng Kuo",
        "Yin Cui",
        "Xiuye Gu",
        "Anelia Piergiovanni",
        "Angelova"
      ],
      "year": "2023",
      "venue": "ICLR",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Llava-onevision: Easy visual task transfer",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Dong Guo",
        "Renrui Zhang",
        "Feng Li",
        "Hao Zhang",
        "Kaichen Zhang",
        "Yanwei Li",
        "Ziwei Liu",
        "Chunyuan Li"
      ],
      "year": "2024",
      "venue": "Llava-onevision: Easy visual task transfer",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Elevater: A benchmark and toolkit for evaluating language-augmented visual models",
      "authors": [
        "Chunyuan Li",
        "Haotian Liu",
        "Liunian Li",
        "Pengchuan Zhang",
        "Jyoti Aneja",
        "Jianwei Yang",
        "Ping Jin",
        "Houdong Hu",
        "Zicheng Liu",
        "Yong Jae Lee"
      ],
      "year": "2022",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Learning object recognition with rich language descriptions",
      "authors": [
        "Liunian Li",
        "Zi-Yi Dou",
        "Nanyun Peng",
        "Kai-Wei Chang",
        "Desco"
      ],
      "year": "2023",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Distilling detr with visual-linguistic knowledge for open-vocabulary object detection",
      "authors": [
        "Liangqi Li",
        "Jiaxu Miao",
        "Dahu Shi",
        "Wenming Tan",
        "Ye Ren",
        "Yi Yang",
        "Shiliang Pu"
      ],
      "year": "2023",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Grounded language-image pre-training",
      "authors": [
        "Liunian Harold",
        "Li",
        "Pengchuan Zhang",
        "Haotian Zhang",
        "Jianwei Yang",
        "Chunyuan Li",
        "Yiwu Zhong",
        "Lijuan Wang",
        "Lu Yuan",
        "Lei Zhang",
        "Jenq-Neng Hwang"
      ],
      "year": "2007",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Evaluating object hallucination in large vision-language models",
      "authors": [
        "Yifan Li",
        "Yifan Du",
        "Kun Zhou",
        "Jinpeng Wang",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "year": "2023",
      "venue": "EMNLP",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Monkey: Image resolution and text label are important things for large multi-modal models",
      "authors": [
        "Zhang Li",
        "Biao Yang",
        "Qiang Liu",
        "Zhiyin Ma",
        "Shuo Zhang",
        "Jingxu Yang",
        "Yabo Sun",
        "Yuliang Liu",
        "Xiang Bai"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Generative region-language pretraining for open-ended object detection",
      "authors": [
        "Chuang Lin",
        "Yi Jiang",
        "Lizhen Qu",
        "Zehuan Yuan",
        "Jianfei Cai"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "Piotr Dollár",
        "C Lawrence",
        "Zitnick"
      ],
      "year": "2014",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr Dollár"
      ],
      "year": "2017",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Gres: Generalized referring expression segmentation",
      "authors": [
        "Chang Liu",
        "Henghui Ding",
        "Xudong Jiang"
      ],
      "year": "2023",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Improved baselines with visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Grounding dino: Marrying dino with grounded pre-training for open-set object detection",
      "authors": [
        "Shilong Liu",
        "Zhaoyang Zeng",
        "Tianhe Ren",
        "Feng Li",
        "Hao Zhang",
        "Jie Yang",
        "Chunyuan Li",
        "Jianwei Yang",
        "Hang Su",
        "Jun Zhu"
      ],
      "year": "2024",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Baining Guo"
      ],
      "year": "2021",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Capdet: Unifying dense captioning and open-world detection pretraining",
      "authors": [
        "Yanxin Long",
        "Youpeng Wen",
        "Jianhua Han",
        "Hang Xu",
        "Pengzhen Ren",
        "Wei Zhang",
        "Shen Zhao",
        "Xiaodan Liang"
      ],
      "year": "2006",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Generation and comprehension of unambiguous object descriptions",
      "authors": [
        "Junhua Mao",
        "Jonathan Huang",
        "Alexander Toshev",
        "Oana Camburu",
        "Alan L Yuille",
        "Kevin Murphy"
      ],
      "year": "2016",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Coco-o: A benchmark for object detectors under natural distribution shifts",
      "authors": [
        "Xiaofeng Mao",
        "Yuefeng Chen",
        "Yao Zhu",
        "Da Chen",
        "Hang Su",
        "Rong Zhang",
        "Hui Xue"
      ],
      "year": "2023",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Scaling open-vocabulary object detection",
      "authors": [
        "Matthias Minderer",
        "Alexey Gritsenko",
        "Neil Houlsby"
      ],
      "year": "",
      "venue": "NeurIPS, 2023",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
      "authors": [
        "Liwei Bryan A Plummer",
        "Chris M Wang",
        "Juan C Cervantes",
        "Julia Caicedo",
        "Svetlana Hockenmaier",
        "Lazebnik"
      ],
      "year": "2015",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Wook Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Objects365: A large-scale, high-quality dataset for object detection",
      "authors": [
        "Shuai Shao",
        "Zeming Li",
        "Tianyuan Zhang",
        "Chao Peng",
        "Gang Yu",
        "Xiangyu Zhang",
        "Jing Li",
        "Jian Sun"
      ],
      "year": "2019",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "authors": [
        "Piyush Sharma",
        "Nan Ding",
        "Sebastian Goodman",
        "Radu Soricut"
      ],
      "year": "2018",
      "venue": "ACL",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Exploring the design space for multimodal llms with mixture of encoders",
      "authors": [
        "Min Shi",
        "Fuxiao Liu",
        "Shihao Wang",
        "Shijia Liao",
        "Subhashree Radhakrishnan",
        "De-An",
        "Hongxu Huang",
        "Karan Yin",
        "Yaser Sapra",
        "Humphrey Yacoob",
        "Shi"
      ],
      "year": "2024",
      "venue": "Exploring the design space for multimodal llms with mixture of encoders",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Cambrian-1: A fully open, vision-centric exploration of multimodal llms",
      "authors": [
        "Shengbang Tong",
        "Ellis Brown",
        "Penghao Wu",
        "Sanghyun Woo",
        "Manoj Middepogu",
        "Charitha Sai",
        "Jihan Akula",
        "Shusheng Yang",
        "Adithya Yang",
        "Xichen Iyer",
        "Pan"
      ],
      "year": "2024",
      "venue": "Cambrian-1: A fully open, vision-centric exploration of multimodal llms",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Ov-dino: Unified open-vocabulary detection with language-aware selective fusion",
      "authors": [
        "Hao Wang",
        "Pengzhen Ren",
        "Zequn Jie",
        "Xiao Dong",
        "Chengjian Feng",
        "Yinlong Qian",
        "Lin Ma",
        "Dongmei Jiang",
        "Yaowei Wang",
        "Xiangyuan Lan"
      ],
      "year": "2024",
      "venue": "Ov-dino: Unified open-vocabulary detection with language-aware selective fusion",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "V3det: Vast vocabulary visual detection dataset",
      "authors": [
        "Jiaqi Wang",
        "Pan Zhang",
        "Tao Chu",
        "Yuhang Cao",
        "Yujie Zhou",
        "Tong Wu",
        "Bin Wang",
        "Conghui He",
        "Dahua Lin"
      ],
      "year": "2023",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "authors": [
        "Peng Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Shijie Wang",
        "Zhihao Fan",
        "Jinze Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge"
      ],
      "year": "2024",
      "venue": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "The all-seeing project v2: Towards general relation comprehension of the open world",
      "authors": [
        "Weiyun Wang",
        "Yiming Ren",
        "Haowen Luo",
        "Tiantong Li",
        "Chenxiang Yan",
        "Zhe Chen",
        "Wenhai Wang",
        "Qingyun Li",
        "Lewei Lu",
        "Xizhou Zhu"
      ],
      "year": "",
      "venue": "The all-seeing project v2: Towards general relation comprehension of the open world",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Grit: A generative region-to-text transformer for object understanding",
      "authors": [
        "Jialian Wu",
        "Jianfeng Wang",
        "Zhengyuan Yang",
        "Zhe Gan",
        "Zicheng Liu",
        "Junsong Yuan",
        "Lijuan Wang"
      ],
      "year": "2024",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Aligning bag of regions for openvocabulary object detection",
      "authors": [
        "Size Wu",
        "Wenwei Zhang",
        "Sheng Jin",
        "Wentao Liu",
        "Chen Change Loy"
      ],
      "year": "2023",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Qwen2 technical report",
      "authors": [
        "An Yang",
        "Baosong Yang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chang Zhou",
        "Chengpeng Li",
        "Chengyuan Li",
        "Dayiheng Liu",
        "Fei Huang"
      ],
      "year": "2024",
      "venue": "Qwen2 technical report",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Detclip: Dictionary-enriched visual-concept paralleled pretraining for open-world detection",
      "authors": [
        "Lewei Yao",
        "Jianhua Han",
        "Youpeng Wen",
        "Xiaodan Liang",
        "Dan Xu",
        "Wei Zhang",
        "Zhenguo Li",
        "Chunjing Xu",
        "Hang Xu"
      ],
      "year": "2006",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Detclipv2: Scalable open-vocabulary object detection pre-training via wordregion alignment",
      "authors": [
        "Lewei Yao",
        "Jianhua Han",
        "Xiaodan Liang",
        "Dan Xu",
        "Wei Zhang",
        "Zhenguo Li",
        "Hang Xu"
      ],
      "year": "2023",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Detclipv3: Towards versatile generative open-vocabulary object detection",
      "authors": [
        "Lewei Yao",
        "Renjie Pi",
        "Jianhua Han",
        "Xiaodan Liang",
        "Hang Xu",
        "Wei Zhang",
        "Zhenguo Li",
        "Dan Xu"
      ],
      "year": "2006",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Modeling context in referring expressions",
      "authors": [
        "Licheng Yu",
        "Patrick Poirson",
        "Shan Yang",
        "Alexander C Berg",
        "Tamara L Berg"
      ],
      "year": "2016",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data",
      "authors": [
        "Qifan Yu",
        "Juncheng Li",
        "Longhui Wei",
        "Liang Pang",
        "Wentao Ye",
        "Bosheng Qin",
        "Siliang Tang",
        "Qi Tian",
        "Yueting Zhuang"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "",
      "authors": [
        "Xiaohua Zhai",
        "Basil Mustafa",
        "Alexander Kolesnikov",
        "Lucas Beyer"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "ICCV",
      "authors": [],
      "year": "2023",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Glipv2: Unifying localization and vision-language understanding",
      "authors": [
        "Haotian Zhang",
        "Pengchuan Zhang",
        "Xiaowei Hu",
        "Yen-Chun Chen",
        "Liunian Li",
        "Xiyang Dai",
        "Lijuan Wang",
        "Lu Yuan",
        "Jenq-Neng Hwang",
        "Jianfeng Gao"
      ],
      "year": "2006",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "Detr with improved denoising anchor boxes for end-to-end object detection",
      "authors": [
        "Hao Zhang",
        "Feng Li",
        "Shilong Liu",
        "Lei Zhang",
        "Hang Su",
        "Jun Zhu",
        "Lionel M Ni",
        "Heung-Yeung Shum",
        "Dino"
      ],
      "year": "2023",
      "venue": "ICLR",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Generating enhanced negatives for training language-based object detectors",
      "authors": [
        "Shiyu Zhao",
        "Long Zhao",
        "Yumin Suh",
        "Dimitris N Metaxas",
        "Manmohan Chandraker",
        "Samuel Schulter"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "An open and comprehensive pipeline for unified object grounding and detection",
      "authors": [
        "Xiangyu Zhao",
        "Yicheng Chen",
        "Shilin Xu",
        "Xiangtai Li",
        "Xinjiang Wang",
        "Yining Li",
        "Haian Huang"
      ],
      "year": "2024",
      "venue": "An open and comprehensive pipeline for unified object grounding and detection",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "Detecting twenty-thousand classes using image-level supervision",
      "authors": [
        "Xingyi Zhou",
        "Rohit Girdhar",
        "Armand Joulin",
        "Philipp Krähenbühl",
        "Ishan Misra"
      ],
      "year": "2022",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "Mova: Adapting mixture of vision experts to multimodal context",
      "authors": [
        "Zhuofan Zong",
        "Bingqi Ma",
        "Dazhong Shen",
        "Guanglu Song",
        "Hao Shao",
        "Dongzhi Jiang",
        "Hongsheng Li",
        "Yu Liu"
      ],
      "year": "2024",
      "venue": "ECCV",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Abstract",
      "text": "Recent open-vocabulary detectors achieve promising performance with abundant region-level annotated data. In this work, we show that an open-vocabulary detector containing with a large language model by generating image-level detailed captions for each image can further improve performance. To achieve the goal, we first collect a dataset, GroundingCap-1M, wherein each image is accompanied by associated grounding labels and an image-level detailed caption. With this dataset, we finetune an open-vocabulary detector with training objectives including a standard grounding loss and a caption generation loss. We take advantage of a large language model to generate both region-level short captions for each region of interest and image-level long captions for the whole image. Under the supervision of the large language model, the resulting detector, LLMDet, outperforms the baseline by a clear margin, enjoying superior open-vocabulary ability. Further, we show that the improved LLMDet can in turn build a stronger large multi-modal model, achieving mutual benefits. The code, model, and dataset is available at [https://github.com/iSEE-Laboratory/LLMDet](https://github.com/iSEE-Laboratory/LLMDet)."
    },
    {
      "title": "1 Introduction",
      "text": "Open-vocabulary object detection [14, 27, 36] aims to detect arbitrary classes based on text labels from user input, which is a more general detection task than traditional closed-set object detection [2, 12, 16, 32]. GLIP [27] first unifies object detection and phrase grounding through region-word contrastive pre-training. This formulation benefits from massive grounding and image-text data covering a broad range of concepts, making the learned representations semantic-rich. The following works focus on effective vision-language fusion [10, 36] and fine-grained region-word alignment by carefully designed word embeddings [56] and negative samples [25, 57, 64]. By scaling up pretraining data and computation [41, 57], existing open-vocabulary object detectors can achieve amazing zero-shot performance on various benchmarks. Recent works show that unifying the grounding task with other language tasks enriches visual representations with language knowledge thus creating a stronger open-vocabulary detector. GLIPv2 [62] pre Figure 1: LLMDet achieves superior zero-shot performance across various benchmarks compared with other well-known counterparts. All detectors use Swin-T as the backbone. the grounding loss and the masked language modeling loss. Subsequently, CapDet [38] and DetCLIPv3 [58] demonstrate that unifying dense captioning and grounding also boosts open-vocabulary ability. However, they use short captions for each object, _e.g_., coarse descriptions and hierarchical class labels, which are coarse-grained, individual, and lacking association among objects. Alternatively, long image-level captions, containing rich details and a comprehensive understanding of an image, provide more information than short region-level descriptions, which motivates us to explore what advantages can long detailed image-level captions bring to open-vocabulary detectors. In light of this, we propose LLMDet, which trains an open-vocabulary detector with a standard grounding objective accompanied by a **caption generation objective**. A large language model (LLM) is appended to the detector, takes both image features and region features from the detector as input, and predicts image-level long detailed captions and region-level short phrases, separately. Compared with previous works that only generate short captions for each object, our LLMDet excels from four aspects: **First, long captions provide more details for each object in the image.** Long captions with detailed _object types, textures, colors, parts of the objects, object actions, precise object locations, and texts_ are helpful to build rich vision-language representations. While existing region-level captions are overly simplistic descriptions for regions. **Second, image-level generation aligns all elements in the image as a whole**, which models both foreground objects, background and the relations between various objects, providing more information and a more comprehensive understanding of the image beyond object-level caption that only focuses on single regions of interest. **Third, image-level captions are more scalable than region-level annotations.** Recent off-the-shelf large vision-language models excel at whole image understanding but still struggle with precise region-level understanding. With proper prompts, we can get high-quality image-level captions from them at a low cost. **Further, the fully-pretrained large language model is naturally open-vocabulary.** Using an LLM to generate captions makes the detector align with it, thus inheriting a strong generalization ability and significantly increasing rare class performance. However, existing grounding datasets lack detailed captions for the whole image. Thus, we first collect a dataset, named GroundingCap-1M, to train LLMDet. Compared with a standard grounding dataset, each element in GroundingCap-1M is formulated as a quadruple, containing an image, a short grounding text, some annotated bounding boxes mapped to the phrases in the grounding text and a long detailed image-level caption. A large language model is utilized to understand region and image features and generate the grounding phrases corresponding to each object and the image-level caption. To effectively integrate the large language model into LLMDet and preserve the pre-trained knowledge, we first carefully align the large language model with an existing detector and then finetune them as a whole. With the novel training framework, **we show that the vision foundation model can benefit from the supervision of LLMs.** The supervision not only comes from using LLM-generated captions as labels but also comes from the gradient of co-training. The resulting LLMDet achieves outstanding open-vocabulary performance. Compared with the baseline, LLMDet outperforms 3.3%/3.8%/14.3% AP and 3.1%/3.3%/17.0% AP\\({}_{r}\\) with Swin-T/B/L as backbone on LVIS [15] minival. We also perform a comprehensive zero-shot transfer to various datasets to demonstrate LLMDet's superior performance, as shown in Figure 1. By integrating the improved LLMDet with a large language model, we can further build a strong large multi-modal model (LMM). Training under the supervision of large language models, LLMDet not only achieves stronger open-vocabulary ability but also pre-aligns with the large language models. Thus the pretrained LLMDet can serve as a strong vision foundation model and in turn build a better LMM, achieving mutual benefits (shown in Appendix)."
    },
    {
      "title": "2 Related Work",
      "text": ""
    },
    {
      "title": "Open-Vocabulary Object Detection",
      "text": "In open-vocabulary object detection (OVD), the detector is trained on a limited training dataset but aims to detect arbitrary test-time user-input classes. To detect arbitrary classes, open-vocabulary object detection is formulated as a vision-language task so that the detector can detect classes never seen with class names. Motivated by the impressive zero-shot ability of vision-language models (_e.g_. CLIP [43]), aligning detectors with CLIP [14, 26, 54] or integrating CLIP as a part of model [13, 22] are straightforward directions for addressing OVD. However, since CLIP is pretrained with image-level objectives, the features in CLIP are not perfectly suitable for OVD. Alternatively, building an object-aware visual-language space with massive data [27, 36, 56, 66] from various resources, including image classification datasets [9], object detection datasets [15, 31, 44, 50], grounding datasets [17, 42] and image-text datasets [45], has shown impressive results. Further, multi-task learning with other language tasks, such as masked language modeling [62] and dense captioning [38, 58] can achieve better vision-language alignment, thus improving the detector's open-vocabulary ability. However, prior works [30, 38, 53, 58] only focus on generating short phrases for regions of interest. In this work, we explore another co-training task, _i.e_. generating image-level detailed captions using large language models."
    },
    {
      "title": "Large Vision-Language Model",
      "text": "Recent large vision-language models [23, 34, 35] equip large language models [7, 48, 55] with superior visual perception and understanding ability. A common large vision-language model contains three parts: a vision foundation models [43, 61] to extract vision tokens, a projector to map vision features to the language space, and a large language model to understand both visual and text input. Recent works [47] find that a better vision encoder improves the multi-modal performance of the final large vision-language model. **But whether the large language model can in turn improve the vision encoder is less explored.** InterrNL [5] scales up a CLIP-like vision encoder to 6B parameters with a large language model as the text encoder. In this work, we show that the detector can also benefit from large language models and the improved detector can boost the multi-modal performance of the large language model, achieving mutual benefits. To train a better large vision-language model, high-quality caption data is indispensable [4, 23, 29]. We argue that the quality of captions is also a key factor in training an open-vocabulary detector under the supervision of large language models. Thus, we take advantage of existing high-quality caption datasets and lead large vision-language models to generate high-quality data."
    },
    {
      "title": "3 Groundingcap-1M Dataset",
      "text": "**Data Formulation.** To support LLMDet training under the supervision of grounding loss and captioning loss, we formulate each training sample as a quadruple \\((I,T_{g},B,T_{c})\\), where \\(I\\) is the image, \\(T_{g}\\) is the short grounding text, \\(B\\) are some annotated bounding boxes each of which is mapped to a phrase in the grounding text, and \\(T_{c}\\) is the detailed caption for the whole image. An example is shown in Figure 2. Two core principles are followed when collecting detailed captions for the whole image: **First, the caption should contain as many details as possible.** We expect the caption to describe object types, textures, colors, parts of the objects, object actions, precise object locations and texts in the image so that the caption is information-rich. **Second, the caption should only contain factual details about the image.** Too many imaginary or reasoning captions will reduce the information density or even hamper the model learning. The detailed and information-intensive captions will facilitate highly efficient training. **Dataset Construction.** To save the construction cost, we start from existing datasets with either bounding boxes or detailed captions. Following previous works, the dataset is collected from object detection datasets, grounding datasets and image-text datasets. For object detection datasets, we select well-known COCO [31] and V3Det [50] datasets. As COCO is widely used in many multi-modal instruction tuning datasets, we can collect its detailed captions from existing assets. Specifically, we collect 168k captions from ShareGPT4V [4] which is known for detailed captions and 42k captions from \\begin{table} \\begin{tabular}{l l c c} \\hline Dataset & Type & Caption Source & Size \\\\ \\hline COCO [31] & Detection & ShareGPT4V [4], ASV2 [32] & 210k \\\\ V3Det [50] & Detection & Our Caption (Qwen2-VL-72b [51]) & 166k \\\\ GoldG [27] & Grounding & Our Caption (Qwen2-VL-72b [51]) & 437k \\\\ LCS [34] & Image-Text & LLVA-OneVision [23], ShareGPT4V [4] & 307k \\\\ \\hline GroundingCap-1M & \\multicolumn{3}{c}{1120k} \\\\ \\hline \\end{tabular} \\end{table} Table 1: Detailed dataset construction of GroundingCap-1M. Figure 2: An example of GroundingCap-1M. Bounding box annotations are discarded for clarity. Compared with original short grounding texts, the detailed captions in GroundingCap-1M are rich in object types, textures, colors, parts of the objects, object actions, precise object locations and texts. Each caption in GroundingCap-1M has around 115 words on average. ASv2 [52] that mainly focus on object relations. V3Det is a dataset with 13k categories so that it can greatly enlarge the detector's vocabulary and is widely used in many open-vocabulary detectors [58, 65]. The captions of V3Det are generated by us using Qwen2-VL-72b [51] with the prompt presented in Figure 2. Following GLIP [27], the grounding text of detection datasets is the concatenation of the class names in the dataset, _e.g_. _\"chair fork. cup. cow.\"_ For grounding datasets, we choose widely-used GoldG [27], which contains GQA [17] and Flickr30k [42]. We find that the original annotations have many short grounding texts for each image. To save computation and increase negatives, we merge some grounding texts from the same image without bounding box conflicts into a single grounding text by simple concatenation. After merging, the dataset is down-sampled from 769k to 437k. The detailed caption is also generated by us using Qwen2-VL-72b [51]. For image-text datasets, LCS-558k [34] with captions from LLaVA-OneVision [23] and ShareGPT4V [4] is used. To generate pseudo boxes for images in this dataset, we first parse noun phrases from captions using a traditional language parser and then utilize MM_Grounding_DINO [65] (Swin-L) to generate bounding boxes for each phrase. The images with less than three bounding boxes are discarded. The grounding text is the concatenation of phrases in the same image, the same as detection datasets. To sum up, the final dataset, **GroundingCap-1M**, contains 1120k samples, summarised in Table 1. **Quality Verification.** In the data collection procedure, we carefully select the prompts and use the best model (Qwen2VL-72b) we can access. A lot of work has been done to prevent hallucinations when training this top-performance model. However, it is inevitable that there will be some noise in the dataset. Thus we introduce some post-processing to clean the dataset. 1) We find that although we prompt the caption model not to describe the imaginary contents, the model still tends to output them but with some obvious words, like _\"indicating\"_, _\"suggesting\"_, _\"possibly\"_. We simply delete the sub-sentences with speculative words. 2) We also design rules to filter out meaningless captions, _e.g_. \"In the image, a man a man...(repeating)\" or \"Sorry, I can not answer the question.\" 3) To ensure the captions are rich with details, we use Qwen2VL-72b to re-generate captions for images whose first-time generated caption is less than 100 tokens. The double-check mechanism ensures the quality of the dataset. After post-processing, each caption has around 115 words on average. Figure 2 shows an example from the GroundingCap-1M dataset. More examples can be found in Appendix. Some quantitive analyses are shown in Section 5.3."
    },
    {
      "title": "4 Training Llmdet Under The Supervision Of Large Language Models",
      "text": "Unifying the grounding task and some other language tasks can enrich vision features with language knowledge thus broadening vision concepts and achieving better vision-language alignment. Prior works [30, 38, 53, 58] mainly focus on dense captioning, in which the language model is designed to generate short captions or class names to describe the single region of interest. However, the details of single objects, the relations between objects and more information about foreground and background are overlooked but this information can be depicted in a single detailed image-level caption. In this work, we show that the region-level open-vocabulary object detector can also benefit from long detailed image-level captions under the supervision of large language models. The overall pipeline is shown in Figure 3. Specifically, we utilize a large language model (LLM) to generate captions based on a pretrained DETR-based open-vocabulary detector. Since the detector and the LLM are pretrained separately, we first train a projector to map the vision features from the detector to the LLM's input space following common practices in training large multi-modal models. We take the p5 feature map from the detector's encoder as the LLM's input and the LLM is asked to generate the full image captions under the supervision of language modeling loss. Only the projector is tunable during this step (**Step 1** in Figure 3). After pre-alignment, the detector, the projector and the LLM are finetuned in an end-to-end manner (**Step 2** in Figure 3). Except for the original grounding task, including the word-region alignment loss \\(\\mathcal{L}_{align}\\) and the box regression loss \\(\\mathcal{L}_{box}\\), we also introduce two tasks: image-level caption generation and region-level caption generation. Details are Figure 3: The multi-step training pipeline of LLMDet. In each step, modules in orange color are tunable while modules in blue color are frozen. In the first step, we train a projector to align the detector’s features with the LLM so that we can integrate the LLM into the detector without breaking the pretrained features. Then, we train the detector with a standard grounding task and newly introduced captioning tasks in Step 2. illustrated in Figure 4. In **image-level caption generation task**, the language model takes the feature maps from the detector as visual inputs and outputs the corresponding long detailed captions annotated in GroundingCap-1M. Following the common practices in training a large multi-modal model, we organize the input data of LLM in the conversation format, which includes system messages, user inputs, and answers. The user inputs contain the vision features from the detector and the prompt, _e.g. Describe the image in detail_. And the answers are the captions from GroundingCap-1M. The LLM aims to output the answers based on the user inputs under the supervision of the standard language modeling loss \\(\\mathcal{L}_{lm}^{image}\\). Since the output answers include various details and the comprehensive understanding of the image, these visual cues should be modeled in the visual features so that the LLM can minimize the training loss and generate the captions correctly. However, since the LLM takes the whole feature map as input in image-level caption generation, it is hard for LLM to map the entities in the image-level captions back to a specific region in the whole image. For example, in Figure 2, the \"dishes\" is only a small part of the image and there are many dish-like objects in the image. Thus, we further introduce the **region-level caption generation task** as compensation, which introduces a prior for LLM to map the region with the corresponding word. In this task, we select the positive object queries from the detector, which are queries matched to ground truth boxes in the label assignment, and use the LLM to generate their corresponding grounding phrases separately, such as _\"young man\"_, _\"mother\"_ and _\"dishes\"_ in the Figure 4. Similar to image-level generation, the inputs of LLM are also formatted in conversations but with a different prompt to separate different types of inputs, _i.e_. _Describe the region in a phrase_. As the visual feature in a single object query is limited, we add some cross-attention layers in the LLM for object queries to gather necessary information from the detector's feature maps. Note that the text tokens and visual tokens in image-level generation do not pass through these cross-attention layers and these layers are trained from scratch. By outputting the corresponding phrases for object queries, the LLM can match the entities to a specific region exactly. The overall training objective of LLMDet is the combination of the grounding loss and the generation losses: \\[\\mathcal{L}=\\mathcal{L}_{align}+\\mathcal{L}_{box}+\\mathcal{L}_{lm}^{image}+ \\mathcal{L}_{lm}^{region} \\tag{1}\\] where \\(\\mathcal{L}_{lm}^{region}\\) is region-level caption generation loss."
    },
    {
      "title": "5 Experiment",
      "text": ""
    },
    {
      "title": "Implementation Details",
      "text": "In this work, we select MM_Grounding_DINO [65] (MM-GDINO for short in the following) as the baseline model since it is fully open-sourced and enjoys SOTA performance. We simply reload their pretrained checkpoint and finetune the model with our GroundingCap-1M dataset un Figure 4: The overview of LLMDet. LLMDet contains a standard open-vocabulary detector and a large language model (LLM) and is trained under both grounding loss and language modeling loss. The LLM is designed to generate both image-level captions using feature maps as visual input and region-level captions using a single object query as visual input, which are separated by different prompts. Only vision tokens in region-level generation pass through the cross-attention (CA) modules in LLM, which is highlighted by a dashed boundary. Since the number of tokens in image-level and region-level generation varies greatly, we forward the LLM twice separately to save memory and computation. The LLM can be discarded in the inference time so that there is no extra cost. der the supervision of both the grounding loss and the caption generation loss. Note that a large part of images in GroundingCap-1M is the same as the pretraining datasets used in MM-GDINO, such as GoldG [27] and V3Det [50]. Since MM-GDINO is fully-pretrained, the vision backbone of MM-GDINO is frozen during training. The large language model is initialized from LLaVA-OneVision-0.5b-ov [23]. To save memory and improve training efficiency, we set the maximum token length for the image-level generation as 1600 and the one for region-level generation as 40. The maximum number of regions for caption generation per image is limited to 16. For image-level visual input, we use the p4 and p5 feature maps from the detector's encoder. We resize p4 to \\(27\\times 27\\) and p5 to \\(20\\times 20\\) and concatenate them as a single token sequence. We implement LLMDet in MMDetection [3] using automatic mixed-precision and gradient checkpointing and train it for 150k iterations (around two epochs) with batch size 16, which can be done around two days on eight NVIDIA L20 GPUs."
    },
    {
      "title": "Zero-Shot Detection Transfer Ability",
      "text": "To demonstrate the great open-vocabulary ability of LLMDet, we select a wide range of benchmarks, including LVIS [15], ODinW13/35 [24], COCO-O [40], RefCOCO [21], RefCOCO+ [59], RefCOCO [39], gRefCOCO [33] and perform zero-shot testing on them. Since we use COCO [31] dataset during training, we carefully remove the images in the RefCOCO/RefCOCO+/RefCOCOg validation and test set from GroundingCap-1M following MM-GDINO. The images in the LVIS minival are not overlapped with the training set of COCO so it strictly follows the zero-shot setting. During the test, the LLM is discarded, thus the inference cost is the same as our baseline. \\begin{table} \\begin{tabular}{l c c c} \\hline \\hline Method & Backbone & ODinW13 & ODinW35 \\\\ \\hline MDETR [20] & ENB5 & - & 10.7 \\\\ T-Rex2 [19] & Swin-T & - & 18.0 \\\\ OWL-VT [41] & ViT L/14(CLIP) & **53.0** & 18.8 \\\\ GLIPV2 [27] & Swin-T & 46.5 & 19.6 \\\\ GLIPV2 [62] & Swin-T & 48.5 & 22.3 \\\\ DetCLIP [56] & Swin-T & 43.3 & - \\\\ Grounding-DINO [36] & Swin-T & 51.4 & 22.7 \\\\ \\hline MM-GDINO [65] & Swin-T & 52.5 & 23.1 \\\\ LLMDet & Swin-T & 52.1 & **23.8** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Zero-shot transfer on ODinW [24]. \\begin{table} \\begin{tabular}{l l l|c c c c|c c c c} \\hline \\hline Method & Backbone & Pre-training data & \\multicolumn{4}{c|}{LVIS\\({}^{\\text{minival}}\\)} & \\multicolumn{4}{c}{LVIS} \\\\ & AP & AP\\({}_{r}\\) & AP\\({}_{c}\\) & AP\\({}_{f}\\) & AP\\({}_{f}\\) & AP\\({}_{r}\\) & AP\\({}_{c}\\) & AP\\({}_{f}\\) \\\\ \\hline GLIP [27] & Swin-T & O365,GoldG,Cap4M & 26.0 & 20.8 & 21.4 & 31.0 & 17.2 & 10.1 & 12.5 & 25.2 \\\\ GLIPV2 [62] & Swin-T & O365,GoldG,Cap4M & 29.0 & – & – & – & – & – & – & – \\\\ CapDet [38] & Swin-T & O365,VG & 33.8 & 29.6 & 32.8 & 35.5 & – & – & – & – \\\\ Grounding-DINO [36] & Swin-T & O365,GoldG,Cap4M & 27.4 & 18.1 & 23.3 & 32.7 & 20.1 & 10.1 & 15.3 & 29.9 \\\\ OWL-ST [41] & CLIP B/16 & WebIL2B & 34.4 & 38.3 & – & – & 28.6 & 30.3 & – & – \\\\ Desco-GLIP [25] & Swin-T & O365,GoldG,CC3M & 34.6 & 30.8 & 30.5 & 39.0 & 26.2 & 19.6 & 22.0 & 33.6 \\\\ DetCLIP [56] & Swin-T & O365,GoldG,YFCC1M & 35.9 & 33.2 & 35.7 & 36.4 & 28.4 & 25.0 & 27.0 & 28.4 \\\\ DetCLIPv2 [57] & Swin-T & O365,GoldG,CC15M & 40.4 & 36.0 & 41.7 & 40.4 & 32.8 & 31.0 & 31.7 & 34.8 \\\\ DetCLIPv3 [58] & Swin-T & O365,V3Det,GoldG,GranuCap50M & 47.0 & 45.1 & 47.7 & 46.7 & 38.9 & 37.2 & 37.5 & 41.2 \\\\ YOLO-World-L [6] & YOLOv8-L & O365,GoldG,CC3M & 35.4 & 27.6 & 34.1 & 38.0 & – & – & – & – \\\\ T-Rex2 [19] & Swin-T & 10M data from various resources & 42.8 & 37.4 & 39.7 & 46.5 & 34.8 & 29.0 & 31.5 & 41.2 \\\\ OV-DINO [49] & Swin-T & O365,GoldG,CC1M & 40.1 & 34.5 & 39.5 & 41.5 & 32.9 & 29.1 & 30.4 & 37.4 \\\\ \\hline MM-GDINO [65] & Swin-T & O365,GoldG,GRIT,V3Det & 41.4 & 34.2 & 37.4 & 46.2 & 31.9 & 23.6 & 27.6 & 40.5 \\\\ LLMDet & Swin-T & GroundingCap-1M & 44.7 & 37.3 & 39.5 & 50.7 & 34.9 & 26.0 & 30.1 & 44.3 \\\\ \\hline \\hline GLIP [27] & Swin-L & FourODs,GoldG,Cap24M & 37.3 & 28.2 & 34.3 & 41.5 & 26.9 & 17.1 & 23.3 & 36.4 \\\\ GLIPv2 [62] & Swin-H & FiveODs,GoldG,CC15M,SBU & 50.1 & – & – & – & – & – & – & – \\\\ Grounding-DINO [36] & Swin-L & O365,OI,GoldG,Cap4M,COCO,RefC & 33.9 & 22.2 & 30.7 & 38.8 & – & – & – \\\\ OWL-ST [41] & CLIP L/14 & WebIL2B & 40.9 & 41.5 & – & – & 35.2 & 36.2 & – & – \\\\ DetCLIP [56] & Swin-L & O365,GoldG,YFCC1M & 38.6 & 36.0 & 38.3 & 39.3 & 28.4 & 25.0 & 27.0 & 31.6 \\\\ DetCLIPv2 [57] & Swin-L & O365,GoldG,CC15M & 44.7 & 43.1 & 46.3 & 43.7 & 36.6 & 33.3 & 36.2 & 38.5 \\\\ DetCLIPv3 [58] & Swin-L & O365,V3Det,GoldG,GranuCap50M & 48.8 & **49.9** & **49.7** & 47.8 & 41.4 & **41.4** & **40.5** & 42.3 \\\\ \\hline MM-GDINO [65] & Swin-B & O365,GoldG,V3Det & 44.5 & 37.5 & 39.9 & 49.9 & 34.9 & 26.7 & 30.4 & 43.5 \\\\ MM-GDINO [65] & Swin-L & O365V2,OpenImageV6,GoldG & 36.8 & 28.1 & 31.8 & 42.8 & 29.1 & 19.7 & 25.6 & 37.2 \\\\ LLMDet & Swin-B & GroundingCap-1M & 48.3 & 40.8 & 43.1 & 54.3 & 38.5 & 28.2 & 34.3 & 47.8 \\\\ LLMDet & Swin-L & GroundingCap-1M & **51.1** & 45.1 & 46.1 & **56.6** & **42.0** & 31.6 & 38.8 & **50.2** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Zero-shot fixed AP [8] on LVIS val [15] and minival [20]. LLMDet achieves state-of-the-art performance with much less data. \\begin{table} \\begin{tabular}{l c c} \\hline \\hline Model & Backbone & ODinW13 & ODinW35 \\\\ \\hline MDETR [20] & ENB5 & - & 10.7 \\\\ T-Rex2 [19] & Swin-T & - & 18.0 \\\\ OWL-VT [41] & ViT L/14(CLIP) & **53.0** & 18.8 \\\\ GLIPV2 [27] & Swin-T & 46.5 & 19.6 \\\\ GLIPV2 [62] & Swin-T & 48.5 & 22.3 \\\\ DetCLIP [56] & Swin-T & 43.3 & - \\\\ Grounding-DINO [36] & Swin-T & 51.4 & 22.7 \\\\ \\hline MM-GDINO [65] & Swin-T & 52.5 & 23.1 \\\\ LLMDet & Swin-T & 52.1 & **23.8** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Zero-shot transfer on ODinW [24]. **Zero-shot performance on LVIS.** LVIS [15] is a detection dataset with 1203 classes. Based on the frequency, the classes can be divided into frequent, common and rare classes. Following GLIP, the total 1203 classes are split into 31 chunks, each with 40 classes. Thus the detector will be forwarded 31 times for each image. Note that a larger chunk size will improve the performance. As shown in Table 2, with the novel training objective and the new dataset, LLMDet outperforms the baseline MM-GDINO by 3.3%/3.8%/14.3% AP and 3.1%/3.3%/17.0% AP\\({}_{r}\\) on LVIS minival across different backbones. We find that the performance of MM-GDINO with Swin-L [37] as the backbone is extremely low, which is probably due to different pretraining data, especially lacking V3Det. But LLMDet with Swin-L as the backbone still outperforms other SOTA methods with much less training data and achieves 50.6% AP, showing great open-vocabulary capabilities. The same trend can be found on LVIS val. We notice that the DetCLIP series achieves more balanced performance on different classes, which is probably due to carefully collected and annotated datasets and a well-organized noun concept corpus. We believe LLMDet can also be applied to DetCLIP. **Zero-shot performance on ODinW.** ODinW (Object Detection in the Wild) [24] is a collection of 35 datasets across various domains and vocabularies, which is a challenging and comprehensive benchmark for open-vocabulary detection. Following previous works, we report the average AP on selected 13 datasets (ODinW13) and all 35 datasets (ODinW35). LLMDet gets the highest AP on ODinW35, demonstrating the great transferring ability to a wide range of datasets. Detailed performance on each dataset can be found in Appendix. **Zero-shot performance on COCO-O.** COCO-O [40] is a dataset sharing the same 80 classes with COCO but with different domains, _i.e_. sketch, weather, cartoon, painting, tattoo, and handmake, which are significantly different from natural images. Although the performance on COCO-O is highly related to the pretraining datasets, LLMDet still outperforms MM-GDINO by 2.1% AP, showing that LLMDet is more robust to domain shifts. Detailed performance on each domain can be found in Appendix. **Zero-shot performance on referring expression comprehension datasets.** Referring expression comprehension (REC) is a task to localize the objects referred by phrases, which needs comprehensive language understanding and fine-grained vision-language alignment. By co-training with LLMs using detailed captions, LLMDet can model rich visual details with enriched vision-language alignment. Thus LLMDet outperforms the baseline MM-GDINO on various REC datasets."
    },
    {
      "title": "Ablation Study",
      "text": "In this subsection, experiments are conducted on the Swin-T backbone and report the performance on LVIS minival. Visualizations can be found in Appendix. **Effect of the main components of LLMDet.** In this work, we collect a new dataset GroundingCap-1M, which contains both grounding annotations and detailed long captions for each image. As shown in Table 6, finetuning with only grounding annotations can boost the performance from 41.4% AP to 43.8% AP. We also show that with only region-level generation, the performance can not be improved since the region-level captions in LLMDet are just the class names or grounding phrases of regions, which do not provide extra information. Simply using image-level generation can slightly improve the performance. As explained in Section 4, the LLM may find it hard to map the entities back to a specific object from a whole image. Thus, combining both image-level and region-level generation can fully unleash the benefit of the LLM's supervision signals. And the rich vision-language representations learned from detailed captions significantly improve 3.9% AP\\({}_{r}\\) (Row 2 vs Row 5), showing that the fine-grained visual representations are helpful in recognizing rare classes. **Effect of different large language models.** By default, we use the LLM in LLaVA-OneVision-0.5b-ov [23], which is \\begin{table} \\begin{tabular}{c c c|c|c c c} \\hline \\hline grounding & region-level & image-level & AP & AP\\({}_{r}\\) & AP\\({}_{c}\\) & AP\\({}_{f}\\) \\\\ data & generation & generation & & & & \\\\ \\hline & & & 41.4 & 34.2 & 37.4 & 46.2 \\\\ ✓ & & & 43.8 & 33.4 & 38.5 & 50.3 \\\\ ✓ & ✓ & & 43.7 & 34.2 & 38.1 & 50.3 \\\\ ✓ & & ✓ & 44.0 & 33.4 & 39.4 & 50.1 \\\\ ✓ & ✓ & ✓ & 44.7 & 37.3 & 39.5 & 50.7 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: Ablations on main components. \\begin{table} \\begin{tabular}{l|c|c|c c|c c c|c c|c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Backbone} & \\multicolumn{3}{c|}{RefCOCO [21]} & \\multicolumn{3}{c|}{RefCOCO+ [59]} & \\multicolumn{3}{c|}{RefCOCOg [39]} & \\multicolumn{3}{c}{gRefCOCO [33]} \\\\ & & val & testA & testB & val & testB & val & testA & testA & testB \\\\ \\hline GLIP (B) [27] & Swin-T & 50.0 & 54.7 & 43.1 & 49.0 & 53.4 & 43.4 & 65.6 & 66.1 & - & - & - \\\\ GLIP [27] & Swin-T & 50.4 & 54.3 & 43.8 & 49.5 & 52.8 & 44.6 & 66.1 & 66.9 & - & - & - & - \\\\ Grounding-DINO [36] & Swin-T & 50.8 & 57.4 & 45.0 & 51.6 & 57.3 & 46.4 & 60.4 & 59.7 & 40.5/83.8 & **29.3**/82.9 & 30.0/86.1 \\\\ \\hline MM-GDINO [65] & Swin-T & 66.0 & 70.3 & 60.0 & 54.3 & 60.4 & 49.2 & **72.6** & **72.5** & **41.0/91.3** & 26.1/**93.0** & 30.4/92.3 \\\\ LLMDet & Swin-T & **69.5** & **73.0** & **64.0** & **55.3** & **60.6** & **49.3** & 72.1 & **72.5** & 40.7/91.2 & 26.2/92.7 & **30.5/92.9** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: Zero-shot transfer on common referring expression comprehension datasets. The evaluation metric for RefCOCO, RefCOCO+, and RefCOCOg is the Top-1 accuracy. The evaluation metrics for gRefCOCO are Pr(F1=1, IoU\\(\\geq\\)0.5) and N-acc. finetuned from Qwen2-0.5b-instruct [55]. Since the LLM in LLM-OneVision-0.5b-ov is pretrained with abundant multi-modal data but with a different vision encoder, the pretraining can still improve the performance, especially for rare classes (+2.2% AP\\({}_{r}\\)), as shown in Table 7. But we find that increasing the size of the LLM only slightly improves the performance, perhaps larger language models mainly improve in reasoning ability which does not benefit the detector's visual representations. **Effect of generated captions' quality.** As shown in Table 8. We first replace the captions generated by Qwen2VL-72b with the ones from LLaVA-Onevision-7B, including captions in V3Det, GoldG and part of LCS. The performance significantly decreases by 0.9% AP and 5.1% AP\\({}_{r}\\). We further replace our generated captions with COCO captions, LCS captions from LLaVA, and short grounding texts in GoldG. The performance further decreases by 0.4% AP. To directly compare the detailedness and the degree of hallucinations in generated captions, we randomly sample 100 captions from each part of the datasets (detection, grounding, and image-text pair), a total of 300 captions per experiment. And we utilize GPT-4o [18] as a judge to give a comprehensive score for each caption-image pair. The used prompts are shown in Appendix. The captions in GroundingCap-1M have the highest detailedness scores and moderate hallucinations, validating the superior quality of our dataset. As human-annotated captions have fewer hallucinations (0.90 vs 1.34, LLaVA captions still have hallucinations), the AP\\({}_{r}\\) in Exp 3 is even higher than the one in Exp 2. **Effect of the pretraining data.** In this work, we collect the GroundingCap-1M dataset. Due to computation constraints, the dataset only contains 1M data, which is much less than the datasets used in other open-vocabulary detectors. As shown in the second row in Table 9, if we do not use the LCS dataset in GroundingCap-1M (813k data now), the performance significantly reduces to 42.8% AP, showing that more pretraining data will further improve LLMDet's performance. Further, the image-level captions should only contain factual details about the image so that we utilize a post-processing procedure to delete the sub-sentences with speculative words. If we do not delete them, the performance drops to 44.2% AP and 35.0% AP\\({}_{r}\\), showing that hallucinations may significantly affect rare class performance. **Effect of the cross-attention layers in LLM.** In LLMDet, visual tokens in region-level generation pass through cross-attention layers, while text tokens and visual tokens in image-level generation do not pass through them. We ablate the design in the third and fourth rows of Table 9. Visual tokens in the region-level generation are single object queries that contain little visual information. If object queries do not gain necessary information from the detector's encoder feature maps through cross-attention, the performance will degrade to 44.0% AP. We further find that using cross-attention in image-level generation is not helpful as we use the whole feature maps in image-level generation. **Effect of pretraining the projector before end-to-end finetuning.** LLMDet improves the rare class performance by pursuing fine-grained vision-language alignment through co-training with LLMs using high-quality captions. Since the LLM and the detector are pretrained separately, pretraining the projector makes their feature space aligned while preserving the pretrained knowledge. Without pretraining the projector, it affects the alignment and decreases the rare class AP (-3.5% AP\\({}_{r}\\)), as shown in the last row of Table 9. As frequent classes have abundant annotations, the negative impacts can be alleviated."
    },
    {
      "title": "6 Conclusion",
      "text": "In this work, we explore a new training objective to boost the performance of existing open-vocabulary detectors. By utilizing a large language model to generate both image-level detailed captions and region-level coarse grounding phrases, the detector receives more information and a more comprehensive understanding of the image from the detailed captions and builds rich vision-language representations. The resulting detector, LLMDet, achieves state-of-the-art performance across a wide range of benchmarks. We also show that the improved LLMDet can in turn build a strong large multi-modal model, achieving mutual benefits. We hope our work can provide insights into enhancing vision models with top-performed large language models. \\begin{table} \\begin{tabular}{l|c|c c c c} \\hline \\hline \\multicolumn{1}{l|}{\\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} }} & \\multicolumn{1}{c|}{\\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} }} & \\multicolumn{1}{c|}{\\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} }} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} }} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} }} & \\multicolumn{1}{c}{\\multirow{2}{*}{ \\begin{tabular}{} \\end{tabular} }} \\\\ \\hline Qwen2-0.5b-instruct [55] & 44.4 & 36.4 & 39.2 & 50.5 \\\\ LLaVA-OneVision-0.5b-ov [23] & 44.5 & 38.6 & 39.3 & 50.3 \\\\ Qwen2-1.5b-instruct [55] & 44.6 & 35.3 & 39.5 & 50.8 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 7: Ablations on large language models. \\begin{table} \\begin{tabular}{l|l|c c c c c} \\hline \\hline \\multicolumn{1}{l|}{\\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} }} & Method & AP & AP\\({}_{r}\\) & AP\\({}_{r}\\) & AP\\({}_{f}\\) \\\\ - & LLMDet & 44.5 & 38.6 & 39.3 & 50.3 \\\\ \\hline \\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} } & Do not use LCS dataset & 42.8 & 33.7 & 37.7 & 48.9 \\\\ & Do not delete imaginary sentences & 44.2 & 35.0 & 38.9 & 50.6 \\\\ \\hline \\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} } & Do not use CA in R-L generation & 44.0 & 35.1 & 38.7 & 50.2 \\\\ & Use CA in both R-L and I-L generation & 44.4 & 36.6 & 39.4 & 50.2 \\\\ \\hline \\multirow{2}{*}{ \\begin{tabular}{} \\end{tabular} } & Do not pretrain the projector & 44.4 & 35.1 & 39.8 & 50.3 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 9: More ablations on the data, architecture, and pipeline. R-L and I-L denotes region level and image level. \\begin{table} \\begin{tabular}{l|l|c c c c c} \\hline \\hline \\multicolumn{1}{l|}{\\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} }} & Method & AP & AP\\({}_{r}\\) & AP\\({}_{r}\\) & AP\\({}_{f}\\) \\\\ - & LLMDet & 44.5 & 38.6 & 39.3 & 50.3 \\\\ \\hline \\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} } & Do not use LCS dataset & 42.8 & 33.7 & 37.7 & 48.9 \\\\ & Do not delete imaginary sentences & 44.2 & 35.0 & 38.9 & 50.6 \\\\ \\hline \\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} } & Do not use CA in R-L generation & 44.0 & 35.1 & 38.7 & 50.2 \\\\ & Use CA in both R-L and I-L generation & 44.4 & 36.6 & 39.4 & 50.2 \\\\ \\hline \\multirow{2}{*}{ \\begin{tabular}{} \\end{tabular} } & Do not pretrain the projector & 44.4 & 35.1 & 39.8 & 50.3 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 8: Ablations on the quality of generated captions. Detailedness (Det) and hallucination (Hul) scores are measured by GPT-4o ranging from 0 to 5. Len is the average length of captions."
    },
    {
      "title": "References",
      "text": "* [1] Steven Bird. Nltk: the natural language toolkit. In _Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions_, 2006. * [2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In _ECCV_, 2020. * [3] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. _arXiv preprint arXiv:1906.07155_, 2019. * [4] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In _ECCV_, 2024. * [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Interrvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In _CVPR_, 2024. * [6] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In _CVPR_, 2024. * [7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot imressing gpt-4 with 90% chapt quality, 2023. * [8] Achal Dave, Piotr Dollar, Deva Ramanan, Alexander Kirillov, and Ross Girshick. Evaluating large-vocabulary object detectors: The devil is in the details. _arXiv preprint arXiv:2102.01066_, 2021. * [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009. * [10] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu, Yann LeCun, Nanyun Peng, et al. Coarse-to-fine vision-language pre-training with fusion in the backbone. In _NeurIPS_, 2022. * [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023. * [12] Shenghao Fu, Junkai Yan, Yipeng Gao, Xiaohua Xie, and Wei-Shi Zheng. Asag: Building strong one-decoder-layer sparse detectors via adaptive sparse anchor generation. In _ICCV_, 2023. * [13] Shenghao Fu, Junkai Yan, Qize Yang, Xihan Wei, Xiaohua Xie, and Wei-Shi Zheng. Frozen-detr: Enhancing detr with image understanding from frozen foundation models. In _NeurIPS_, 2024. * [14] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. In _ICLR_, 2021. * [15] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In _CVPR_, 2019. * [16] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _ICCV_, 2017. * [17] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _CVPR_, 2019. * [18] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welhinha, Alan Hayes, Alec Radford, et al. Gpt-4o system card. _arXiv preprint arXiv:2410.21276_, 2024. * [19] Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, and Lei Zhang. T-rc2: Towards generic object detection via text-visual prompt synergy. In _ECCV_, 2024. * [20] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In _ICCV_, 2021. * [21] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In _EMNLP_, 2014. * [22] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. F-vlm: Open-vocabulary object detection upon frozen vision and language models. In _ICLR_, 2023. * [23] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. _arXiv preprint arXiv:2408.03326_, 2024. * [24] Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, et al. Elevate: A benchmark and toolkit for evaluating language-augmented visual models. In _NeurIPS_, 2022. * [25] Liunian Li, Zi-Yi Dou, Nanyun Peng, and Kai-Wei Chang. Desco: Learning object recognition with rich language descriptions. In _NeurIPS_, 2023. * [26] Liangqi Li, Jiaxu Miao, Dahu Shi, Wenming Tan, Ye Ren, Yi Yang, and Shiliang Pu. Distilling detr with visual-linguistic knowledge for open-vocabulary object detection. In _ICCV_, 2023. * [27] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In _CVPR_, 2022. * [28] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In _EMNLP_, 2023. * [29] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In _CVPR_, 2024. * [30] Chuang Lin, Yi Jiang, Lizhen Qu, Zehuan Yuan, and Jianfei Cai. Generative region-language pretraining for open-ended object detection. In _CVPR_, 2024. * [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014. * [32] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In _CVPR_, 2017. * [33] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In _CVPR_, 2023. * [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _NeurIPS_, 2023. * [35] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In _CVPR_, 2024. * [36] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Maryring dino with grounded pre-training for open-set object detection. In _ECCV_, 2024. * [37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, 2021. * [38] Yanxin Long, Youpeng Wen, Jianhua Han, Hang Xu, Pengzhen Ren, Wei Zhang, Shen Zhao, and Xiaodan Liang. Capdet: Unifying dense captioning and open-world detection pretraining. In _CVPR_, 2023. * [39] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In _CVPR_, 2016. * [40] Xiaofeng Mao, Yuefeng Chen, Yao Zhu, Da Chen, Hang Su, Rong Zhang, and Hui Xue. Coco-o: A benchmark for object detectors under natural distribution shifts. In _ICCV_, 2023. * [41] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. In _NeurIPS_, 2023. * [42] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _CVPR_, 2015. * [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021. * [44] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In _ICCV_, 2019. * [45] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _ACL_, 2018. * [46] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal lms with mixture of encoders. _arXiv preprint arXiv:2408.15998_, 2024. * [47] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal llms. _arXiv preprint arXiv:2406.16860_, 2024. * [48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023. * [49] Hao Wang, Pengzhen Ren, Zequn Jie, Xiao Dong, Chengjian Feng, Yinlong Qian, Lin Ma, Dongmei Jiang, Yaowei Wang, Xiangyuan Lan, et al. Ov-dino: Unified open-vocabulary detection with language-aware selective fusion. _arXiv preprint arXiv:2407.07844_, 2024. * [50] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det: Vast vocabulary visual detection dataset. In _ICCV_, 2023. * [51] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. _arXiv preprint arXiv:2409.12191_, 2024. * [52] Weiyu Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. In _ECCV_, 2024. * [53] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. In _ECCV_, 2024. * [54] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions for open-vocabulary object detection. In _CVPR_, 2023. * [55] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chenpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwenz technical report. _arXiv preprint arXiv:2407.10671_, 2024. * [56] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection. In _NeurIPS_, 2022. * [57] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, and Hang Xu. Detclipv2: Scalable open-vocabulary object detection pre-training via word-region alignment. In _CVPR_, 2023. * [58] Lewei Yao, Renjie Pi, Jianhua Han, Xiaodan Liang, Hang Xu, Wei Zhang, Zhenguo Li, and Dan Xu. Detclipv3: Towards versatile generative open-vocabulary object detection. In _CVPR_, 2024. * [59] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In _ECCV_, 2016. * [60] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang. Hallucidotor: Mitigating hallucinatory toxicity in visual instruction data. In _CVPR_, 2024. * [61] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In _ICCV_, 2023. * [62] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. In _NeurIPS_, 2022. * [63] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. In _ICLR_, 2023. * [64] Shiyu Zhao, Long Zhao, Yumin Suh, Dimitris N Metaxas, Manmohan Chandraker, Samuel Schulter, et al. Generating enhanced negatives for training language-based object detectors. In _CVPR_, 2024. * [65] Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, and Haian Huang. An open and comprehensive pipeline for unified object grounding and detection. _arXiv preprint arXiv:2401.02361_, 2024. * [66] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krahenbuhl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In _ECCV_, 2022. * [67] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. In _ECCV_, 2024."
    },
    {
      "title": "Appendix A Llmdet Builds A Stronger Large Vision-Language Model",
      "text": "In this subsection, we show that LLMDet can serve as a general vision foundation model and in turn gets a strong large multi-modal model. Recent large multi-modal models (LMM) are based on pretrained large language models and pretrained vision foundation models. Different vision foundation models will significantly affect the performance of LMMs [67]. Since LLMDet is enhanced under the supervision of long detailed image-level captions and pre-aligned with LLM, LLMDet inherits great potential to build a stronger LMM. Following recent advances [46, 47, 67], we build the LMM using a mixture of vision experts, _i.e_. a SigLIP [61] vision encoder and our LLMDet. As shown in Figure A-1, the visual features from two vision encoders are concatenated along the channel dimension, and then a projector is utilized to map the features to the LLM's input space. We start from LLaVA-OneVision-0.5b-ov [23] and insert our LLMDet to it as shown in Figure A-1. We first pretrain a new projector and then finetune the LLM with the LLaVA 1.5 [35] instruction tuning dataset which is only a small part of the dataset used in LLaVA-Onevision. We select three representative benchmarks to evaluate the multi-modal performance of the LMM: the comprehensive understanding benchmark MME [11], the hallucination benchmark POPE [28] and the academic VQA benchmark GQA [17]. As shown in Table A-1, combining the MMDINO to LLaVA-OneVision-0.5b-ov can improve the performance on GQA and POPE. As detectors excel at localizing objects in the image, the precise localization makes the LLM aware of the objects existed in images, which helps the LLM overcome hallucination and perform simple QA about objects in the image. The multi-modal perception and understanding ability can be further enhanced with a stronger LLMDet which is also pre-aligned [46] with the LLM in LLaVA-OneVision-0.5b-ov. The resulting LMM achieves the highest performance on the MME benchmark, validating the mutual benefits between the detector and the LMM."
    },
    {
      "title": "Appendix B Limitations",
      "text": "Although we provide detailed captions to train LLMs, we find that the LLM co-trained with detectors tends to output relatively short descriptions for the whole image, even given the prompts to describe the image in detail. We suppose the reason is that our region-level data is far more than the image-level data (one image has multiple regions). Further, our region-level descriptions are too simple as they are just the grounding phrases of the regions. We believe collecting some high-informative data for regions like DetCLIPv3 can further improve the performance."
    },
    {
      "title": "Appendix C Implement Details Of Zero-Shot Test On Referring Expression Comprehension Datasets",
      "text": "In this work, LLMDet is trained with phrase grounding loss and caption generation loss. In the phrase grounding task, the model is asked to detect each phrase in the given grounding text. For example, the model is expected to detect \"the man\" and \"umbrella\" in the text \"the man with an umbrella\". To demonstrate the great open-vocabulary ability of LLMDet, we directly transfer LLMDet to the referring expression comprehension (REC) task, which is a task slightly different from the phrase grounding task. In REC, the model should only detect the single object referred by the given sentence. For example, the model should only detect \"the man\" in the text \"the man with an umbrella\", which means discrepancies exist between the pretraining task and the target task. Thus, we find that the model tends to predict \\begin{table} \\begin{tabular}{l|c|c c c|c c} \\hline \\hline \\multirow{2}{*}{Method} & GQA & \\multicolumn{2}{c|}{POPE [28]} & \\multicolumn{2}{c}{MME [11]} \\\\ & [17] & rand & pop & adv & perception & cognition \\\\ \\hline OneVision-0.5b & 56.9 & 87.5 & 86.3 & 85.0 & 1238 & 240 \\\\ \\hline OneVision-0.5b & \\multirow{2}{*}{**61.2**} & **88.9** & **88.1** & **86.6** & 1207 & 256 \\\\ +MM-GDINO & & & & & & \\\\ \\hline OneVision-0.5b & \\multirow{2}{*}{**61.2**} & 88.8 & 88.0 & 86.0 & **1297** & **264** \\\\ +LLMDet & & & & & & \\\\ \\hline \\hline \\end{tabular} \\end{table} Table A-1: Multi-modal performance using different vision encoders. OneVision-0.5b is short for LLaVA-OneVision-0.5b-ov [23]. Figure A-1: The multi-step training pipeline of using LLMDet to build a strong large multi-modal model. The large multi-modal model uses a mixture of vision encoders, including LLMDet and SigLIP. In each step, modules in orange color are tunable while modules in blue color are frozen. We first pretrain a new projector and then finetune the large multi-modal model with visual instruct tuning. the \"umbrella\" with the highest confidence. To minimize the discrepancies, we first use NLTK [1] tools to find the subject in the text and then select the box with the highest confidence corresponding to the subject as the answer."
    },
    {
      "title": "Appendix D Prompts For Calculating Detailedness And Hallucination Scores",
      "text": "In Table 8, we utilize GPT-4o as a judge to give a comprehensive score for each caption-image pair. We referred to HalluciDoctor [60] and adopted similar prompts as follows. The prompt for calculating hallucination scores Suppose you are a hallucination annotator who judges the degree of hallucination based on the number of errors in the description of objects, relations, and attributes. You should check each sentence in the description one by one. \\[\\{\\text{image}\\}\\] Please carefully compare the image and the given caption below and provide the hallucination score (an integer value between 0 and 5) based on overall hallucinations in each sub-sentence, where the fewer descriptive errors in the caption, the lower the hallucination score given. Only output the score without any explanation. Description: \\[\\{\\text{caption}\\}\\] Output: The prompt for calculating detailedness scores Suppose you are an image detail annotator who judges the degree of sentence detailedness based on the object types, textures and colors, parts of the objects, object actions, precise object locations, and texts. \\[\\{\\text{image}\\}\\] Please carefully compare the image and the given caption below and provide the detailedness score (an integer value between 0 and 5) without any explanation, where caption with more factual content give a higher detailedness score. Only output the score without any explanation. Description: \\[\\{\\text{caption}\\}\\] Output:"
    },
    {
      "title": "Appendix E Detailed Zero-Shot Results",
      "text": "**Detailed zero-shot results on ODinW35.** Table E-2 lists the detailed performance of Grounding-DINO-T [36], MM-GDINO-T [65], and our LLMDet on each dataset in ODinW35 [24]. The selected datasets in ODinW13 are also marked out. **Detailed zero-shot results on COCO-O.** COCO-O [40] is a dataset sharing the same 80 classes as COCO but in different domains including cartoon, handmake, painting, sketch, tatto, and weather. Detailed performance on each domain is listed in Table E-3."
    },
    {
      "title": "Appendix F Visualization",
      "text": ""
    },
    {
      "title": "Visualizations Of The Image-Level Captions In Groundingcap-1M",
      "text": "In this work, we collect a new GroundingCap-1M dataset which equips a standard grounding dataset with detailed image-level captions. The captions should contain as many details as possible, including object types, textures, colors, parts of the objects, object actions, precise object locations, and texts. And the captions should not contain imaginary contents. Figure F-2 visualizes some examples in GroundingCap-1M. The captions shown depict the main entities in the pictures with great detail (demonstrated in green color) but also with some imaginary contents inevitably (also highlighted by underlines). The imaginary contents always start with speculative words, like \"seemingly\", \"indicating\", and \"suggesting\". We just find these pre-defined speculative words and delete the sub-sentences including them in an online manner."
    },
    {
      "title": "Visualizations Of The Captions Generated By Llmdet",
      "text": "In Figure F-3, we visualize some examples of the generated image-level and region-level captions from the LLM co-trained with LLMDet. Images are selected from the COCO validation set. The LLM can generate precise class names for the objects in COCO (as we use the class names in COCO as the grounding text for deep fusion, only objects in COCO are detected out for caption generation). But we find that the image-level captions are relatively coarse-grained compared with the ones in GroundingCap-1M. We suppose the reason is that our region-level data is far more than the image-level data (one image has multiple regions) and the region-level data is overly simplistic. \\begin{table} \\begin{tabular}{l|c c c|c c c|c} \\hline Dataset & ODinW13 & ODinW35 & G-DINO-T & MM-GDINO-T & LLMDet \\\\ \\hline AerialMaritimeDrone large & ✓ & ✓ & 0.173 & 0.155 & 0.153 \\\\ AerialMaritimeDrone tiled & & ✓ & 0.206 & 0.201 & 0.174 \\\\ AmericanSignLanguageLetters & & ✓ & 0.002 & 0.007 & 0.016 \\\\ Aquarium & ✓ & ✓ & 0.195 & 0.281 & 0.268 \\\\ BCCD & & ✓ & 0.161 & 0.078 & 0.149 \\\\ boggleBoards & & ✓ & 0.000 & 0.002 & 0.001 \\\\ brackishUnderwater & & ✓ & 0.021 & 0.024 & 0.026 \\\\ ChessPieces & & ✓ & 0.000 & 0.000 & 0.000 \\\\ CottontailRabbits & ✓ & ✓ & 0.806 & 0.788 & 0.797 \\\\ dice & & ✓ & 0.004 & 0.001 & 0.004 \\\\ DroneControl & & ✓ & 0.042 & 0.073 & 0.070 \\\\ EgoHands generic & ✓ & ✓ & 0.608 & 0.518 & 0.518 \\\\ EgoHands specific & & ✓ & 0.002 & 0.003 & 0.010 \\\\ HardHatWorkers & & ✓ & 0.046 & 0.109 & 0.178 \\\\ MaskWearing & & ✓ & 0.004 & 0.009 & 0.004 \\\\ MountainDewCommercial & & ✓ & 0.430 & 0.433 & 0.518 \\\\ NorthAmericaMushrooms & ✓ & ✓ & 0.471 & 0.747 & 0.749 \\\\ openPoetryVision & & ✓ & 0.000 & 0.000 & 0.003 \\\\ OxfordPets by breed & & ✓ & 0.003 & 0.004 & 0.006 \\\\ OxfordPets by species & & ✓ & 0.011 & 0.016 & 0.024 \\\\ PKLot & & ✓ & 0.001 & 0.007 & 0.034 \\\\ Packages & ✓ & ✓ & 0.695 & 0.706 & 0.717 \\\\ PascalVOC & ✓ & ✓ & 0.563 & 0.566 & 0.584 \\\\ pistols & ✓ & ✓ & 0.726 & 0.726 & 0.720 \\\\ plantdoc & & ✓ & 0.005 & 0.011 & 0.005 \\\\ pothole & ✓ & ✓ & 0.215 & 0.164 & 0.175 \\\\ Raccoons & ✓ & ✓ & 0.549 & 0.533 & 0.519 \\\\ selfdrivingCar & & ✓ & 0.089 & 0.082 & 0.083 \\\\ ShellfishOpenImages & ✓ & ✓ & 0.393 & 0.489 & 0.429 \\\\ ThermalCheetah & & ✓ & 0.087 & 0.045 & 0.132 \\\\ thermalDogsAndPeople & ✓ & ✓ & 0.657 & 0.548 & 0.546 \\\\ UnoCards & & ✓ & 0.006 & 0.005 & 0.010 \\\\ VehiclesOpenImages & ✓ & ✓ & 0.613 & 0.610 & 0.597 \\\\ WildfireSmoke & & ✓ & 0.134 & 0.129 & 0.093 \\\\ websiteScreenshots & & ✓ & 0.012 & 0.016 & 0.013 \\\\ \\hline ODinW13 Average & & & 0.514 & 0.525 & 0.521 \\\\ ODinW35 Average & & & 0.227 & 0.231 & 0.238 \\\\ \\hline \\end{tabular} \\end{table} Table 2: Detailed zero-shot results on ODinW35 [24]. The image depicts a sushi conveyor belt restaurant. A long, metallic conveyor belt runs horizontally across the image, carrying various dishes on red plates. The dishes include different types of sushi, sashimi, and other Japanese cuisine items. The conveyor belt is situated in the middle of the image, with diners seated on either side. On the left side of the image, a person is standing, wearing a white shirt. This person appears to be a server or a customer, and they are holding a plate with a piece of sushi on it. On the right side of the image, another person is seated at the conveyor belt. This individual is wearing a light-colored shirt and is holding a newspaper, seemingly reading it while waiting for their food. The person is seated on a red stool, which is typical in such restaurants. The overall setting suggests a casual dining environment with a focus on convenience and variety. In the image, a young girl wearing a pink bathing suit is standing on a sandy beach. She has her right arm extended outward as if she's either throwing something or gesturing towards something in front of her. The girl is barefoot and looking towards the ocean. A seagull is captured mid-flight above the water, with its wings spread wide. The bird is positioned slightly to the left of the girl and appears to be flying parallel to the shoreline. The ocean itself is visible in the background, with gentle waves lapping at the shore. The overall scene suggests a sunny day at the beach, with the girl enjoying her time near the water. In the image, there is a woman wearing a blue raincoat and hat, walking on a wet sidewalk. She is using a white cane for support as she navigates the slippery surface. The woman appears to be elderly and is dressed appropriately for the weather conditions. Nearby, there is a metal pole with a unicycle leaning against it. The unicycle has a black seat and large wheels, indicating that it is designed for stability and balance. The wet sidewalk suggests recent rainfall, which could make the surface slippery and challenging to walk on. The relative position of the objects shows the woman walking past the unicycle, which is positioned closer to the camera than the woman. The image depicts a computer desk setup with various objects. The primary focus is on the keyboard, which is prominently placed in the foreground. It is a standard QWERTY keyboard with white keys and black lettering. Behind the keyboard, there is a computer monitor displaying an image of a pink heart with a Hello Kitty design inside it. The monitor is positioned slightly to the left of center. To the left of the keyboard, there is a telephone with a corded handset. The phone has a traditional design with a push-button keypad. On the right side of the keyboard, there are several small figurines or toys, including what appears to be a panda bear and other characters. These toys are arranged in a somewhat cluttered manner, suggesting that they might be personal items or collectibles. In the background, there is a printer situated on the right side of the desk. The printer is a standalone unit with a paper tray visible at the top. There is also a window with blinds partially drawn, allowing some natural light to enter the room. The overall setting suggests a home office or personal workspace, with a mix of work-related equipment and personal items. Figure F-2: Visualizations of the image-level captions in GroundingCap-1M, which are rich in detail. The great details are marked in green color. But the captions still contain some imaginary contents, which are also highlighted by underlines. In the image, a group of people are standing on a street. The image captures a moment in time in an urban setting. It's a snapshot of life in the city. In the background, there are trees and buildings. There are also cars parked on the side of the road. Overall, this image gives you a glimpse into the daily life of a city dweller. In the image, there is a dining table with a bowl of fruit. The image also shows a kitchen area with cabinets and a refrigerator. There is also a chair visible in the background. Overall, this image gives the impression of a well-organized and inviting kitchen space. In the image, there is a tuilet that is beside a sink. The walls of the bathroom are white. There is also a towel hanging on the wall. The photo is taken from inside a bathroom. In the image, there is a television that is on a table. There is also a bookshelf with books on it. The image shows a room with a bed that has a mattress and a pillow. The television is turned on. In the image, there is a tuilet with the LLMDet. Image-level captions are placed under the corresponding images and region-level captions are placed beside the bounding boxes. Only object queries with scores higher than 0.3 are visualized in the images."
    }
  ]
}