{
  "title": "LLMRA: Multi-modal Large Language Model based Restoration Assistant",
  "authors": [
    "Xiaoyu Jin",
    "Yuan Shi",
    "Bin Xia",
    "Wenming Yang"
  ],
  "abstract": "\n Multi-modal Large Language Models (MLLMs) have a significant impact on various tasks, due to their extensive knowledge and powerful perception and generation capabilities. However, it still remains an open research problem on applying MLLMs to low-level vision tasks. In this paper, we present a simple MLLM-based Image Restoration framework to address this gap, namely Multimodal Large Language Model based Restoration Assistant (LLMRA). We exploit the impressive capabilities of MLLMs to obtain the degradation information for universal image restoration. By employing a pretrained multi-modal large language model and a vision language model, we generate text descriptions and encode them as context embedding with degradation information for the degraded image. Through the proposed Context Enhance Module (CEM) and Degradation Context based Transformer Network (DC-former), we integrate these context embedding into the restoration network, contributing to more accurate and adjustable image restoration. Based on the dialogue with the users, our method leverages image degradation priors from MLLMs, providing low-level attributes descriptions of the input low-quality images and the restored high-quality images simultaneously. Extensive experiments demonstrate the superior performance of our LLMRA in universal image restoration tasks. \n",
  "references": [
    {
      "id": null,
      "title": "LLMRA: Multi-modal Large Language Model based Restoration Assistant",
      "authors": [
        "Xiaoyu Jin",
        "Yuan Shi",
        "Bin Xia",
        "Wenming Yang"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Vqgan-clip: Open domain image generation and editing with natural language guidance",
      "authors": [
        "Abdal"
      ],
      "year": "2015",
      "venue": "Microsoft coco captions: Data collection and evaluation server",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "",
      "authors": [
        "Dai"
      ],
      "year": "2021",
      "venue": "Attentional feature fusion. In WACV",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
      "authors": [
        "Dai"
      ],
      "year": "2023",
      "venue": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Instructdiffusion: A generalist modeling interface for vision tasks",
      "authors": [
        "Geng"
      ],
      "year": "2015",
      "venue": "Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Multi-scale progressive fusion network for single image deraining",
      "authors": [
        "Jiang"
      ],
      "year": "2020",
      "venue": "Reasoning segmentation via large language model",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "All in one bad weather removal using architectural search",
      "authors": [
        "Li"
      ],
      "year": "2020",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
      "authors": [
        "Li"
      ],
      "year": "2006",
      "venue": "Controlling vision-language models for universal image restoration",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics",
      "authors": [
        "Ma"
      ],
      "year": "2001",
      "venue": "Promptir: Prompting for all-in-one blind image restoration",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Radford"
      ],
      "year": "2021",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Transweather: Transformerbased restoration of images degraded by adverse weather conditions",
      "authors": [
        "Dominik Blattmann",
        "Patrick Lorenz",
        "Bj√∂rn Esser",
        "; Ommer",
        "Taori"
      ],
      "year": "2017",
      "venue": "Faisal Azhar, et al. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Uformer: A general u-shaped transformer for image restoration",
      "authors": [
        "Wang"
      ],
      "year": "2022",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Uretinexnet: Retinex-based deep unfolding network for low-light image enhancement",
      "authors": [
        "Wei"
      ],
      "year": "2018",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Q-bench: A benchmark for general-purpose foundation models on low-level vision",
      "authors": [
        "Wu"
      ],
      "year": "2023",
      "venue": "Q-bench: A benchmark for general-purpose foundation models on low-level vision",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Uncertainty guided multi-scale residual learning-using a cycle spinning cnn for single image de-raining",
      "authors": [
        "Xia"
      ],
      "year": "2017",
      "venue": "Multimodal large language model based generation assistant",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Ffdnet: Toward a fast and flexible solution for cnn-based image denoising",
      "authors": [
        "Zhang"
      ],
      "year": "2017",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Zhu"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Llmra: Multi-Modal Large Language Model Based Restoration Assistant",
      "text": "Xiaoyu Jin \\({}^{1}\\), Yuan shi\\({}^{1}\\), Bin Xia \\({}^{2}\\), Wenming Yang\\({}^{1}\\) \\({}^{1}\\) **Tsinghua University, \\({}^{2}\\) The Chinese University of Hong Kong**"
    },
    {
      "title": "Abstract",
      "text": "Multi-modal Large Language Models (MLLMs) have a significant impact on various tasks, due to their extensive knowledge and powerful perception and generation capabilities. However, it still remains an open research problem on applying MLLMs to low-level vision tasks. In this paper, we present a simple MLLM-based Image Restoration framework to address this gap, namely Multi-modal Large Language Model based Restoration Assistant (LLMRA). We exploit the impressive capabilities of MLLMs to obtain the degradation information for universal image restoration. By employing a pretrained multi-modal large language model and a vision language model, we generate text descriptions and encode them as context embedding with degradation information for the degraded image. Through the proposed Context Enhance Module (CEM) and Degradation Context based Transformer Network (DC-former), we integrate these context embedding into the restoration network, contributing to more accurate and adjustable image restoration. Based on the dialogue with the users, our method leverages image degradation priors from MLLMs, providing low-level attributes descriptions of the input low-quality images and the restored high-quality images simultaneously. Extensive experiments demonstrate the superior performance of our LLMRA in universal image restoration tasks."
    },
    {
      "title": "1 Introduction",
      "text": "Recently, Multi-modal Large Language Models (MLLMs), such as LLaVA [11], MiniGPT-4 [23], and InstructBLIP [24], have garnered significant attention. Building upon the remarkable comprehension and reasoning capabilities of LLMs, MLLMs have transcended beyond the boundaries of textual inputs, harnessing their remarkable power in various domains. However, the current exploration of MLLMs has primarily focused on high-level perception and understanding of images. The application of MLLMs only emerges in a limited range of vision-language tasks, such as image captioning [2], visual question answering [1], and conventional computer vision tasks like segmentation [23] and text-to-image generation [22]. Recently, a benchmark called Q-bench [21] can evaluate the performance of MLLMs in low-level vision tasks, specifically in perceiving and describing low-level quality-related information using natural language. The results demonstrate that MLLMs exhibit a notable perceptual ability towards low-level visual attributes. Image restoration is a fundamental task in the field of low-level vision, with the primary objective of recovering high-quality images from degraded counterparts. This task encompasses a diverse range of subtasks, including but not limited to image denoising, deblurring, deraining, and low-light enhancement. Presently, the existing methods predominantly concentrate on addressing specific types of image degradation, and are trained on datasets featuring only a single degradation, thereby imposing limitations on their ability to effectively restore other forms of degradation. In recent times, there has been a notable surge of interest in the task of unified image restoration. Researchers are challenged to develop a single model capable of handling images with diverse types of degradation. Several approaches have been proposed to tackle this challenge, employing techniques like degradation encoder, contrastive learning [14], and prompt learning [18] to achieve promising results. Some approaches also leverage visual language models (VLMs) to handle a wide range of degradations [15]. However, when it comes to complex real-world degradations, the processing and storage capabilities of these encoders and VLMs are still limited. In particular, these methods can only directly restore images and cannot accept other instructions for restoration or optimization, which limits the application scenarios. In this paper, we combine large-scale pretrained multi-modal large language model with image restoration networks and introduce an effective framework for universal image restoration. We refer to this novel framework as MLLM based image Restoration Assistant (LLMRA). Specifically, we utilize IDEFiCS [25], an open-source multi-modal language model based on Flaming [1], to generate textual descriptions of the input degraded images. The text encoder of CLIP [13] (a large-scale pretrained vision-language model)is employed to encode the text descriptions into text features. Using a Context Refine Module (CRM) and Context transformer, these degradation aware text features are enhanced. Finally, we incorporate them into the Degradation Context based Transformer Network (DC-former) through a Degradation Modulation Module. By effectively utilizing the image degradation priors obtained from the MLLMs, this framework enables the restoration network to achieve more accurate and adjustable image restoration. Our main contributions are summarised as follows: * We propose a multi-modal large language model based image restoration framework, which is capable of generating restored high-quality image automatically or according to the dialogue with the users. To the best of our knowledge, LLMRA is the first work that applies MLLMs in the domain of unified image restoration. * To better incorporate text features into the restoration network, we propose CEM (Context Enhance Module) and DC-former (Degradation Context based Transformer Network). CEM enhances the text features and DC-former propagates the degraded information from textual features to the restoration network effectively. * Our extensive experiments demonstrate the effectiveness of LLMRA, as it achieves state-of-the-art performance on various image restoration tasks, including image denoising, deraining, and low-light image enhancement."
    },
    {
      "title": "2 Related Works",
      "text": "Unified Image Restoration.Although there has been considerable attention given to single degradation image restoration methods [22, 23], the exploration of unified image restoration for multi-degradation remains limited. Some research has focused on addressing image degradation caused by various weather conditions such as snow, fog, and rain [17, 2]. However, these studies often train specific encoders or decoders for each weather degradation, which lacks scalability as it requires prior knowledge of specific degradation types. Li et al. proposed a unified model called AirNet [17] for denoising, deraining, and dehazing. AirNet incorporates contrastive learning to train an additional encoder, enabling implicit modeling of degradation information in the input image. These learned representations are then utilized in the main restoration network to predict the offsets of adaptable convolutions for restoration. PromptIR [18] designed a visual prompt generation module that combines a learned degradation prompt tensor to obtain degradation features. DA-CLIP [16] combines a large-scale pretrained visual language model with an image restoration network and demonstrates competitive performance across the ten degradation tasks. Text-driven Image Generation.In recent years, there has been a rapid rise in text-based image generation works. Several works [15, 2] have employed a combination of pre-trained generative models and CLIP to guide the generation process towards a desired target description. Additionally, latent diffusion model [16] are proposed, which enables training diffusion models with limited computational resources while preserving their quality and flexibility by operating in the latent space. In addition to these prompt-driven approaches, there have been advancements in instruction-based editing methods [14, 1], which involve modifying a source image based on specific instructions. Multi-modal Large Language Models.Recent years, Large Language Models (LLMs) [26, 17] have significantly contributed to conversational AI and beyond. Subsequently, attention has been directed towards advancing Multi-modal Large Language Models (MLLMs), aiming to equip LLMs with the ability to comprehend both text and images, enabling them to generate textual responses. For instance, Flamingo [1] incorporates image encoding into the attention layer of the LLM. BLIP-2 [17] employs Q-Former to transform input images into queries. Besides, LLaVA [17] adopts CLIP to encode images into image embeddings, which are then concatenated with text embeddings. Then, MLLMs are adopted in various CV tasks [23]. Figure 1: Example of the proposed LLMRA for universal image restoration. Based on the input image and the text input asking for the low-level attributes of the image, our method is capable of providing corresponding descriptions. Upon the \\(<\\)_restore_\\(>\\)instruction, our LLMRA leverages the degradation descriptions from the MLLM automatically to restore the image. On the other hand, when instructed with the \\(<\\)_refine_\\(>\\)command, LLMRA performs image restoration based on the content of the dialogue."
    },
    {
      "title": "3 Proposed Method",
      "text": "In this section, we present a comprehensive description of the proposed method, which encompasses the generation of text features, the network architectures and the loss functions. **Training.** As illustrated in Figure 2, with the instruction \\(<\\)_refine\\(>\\)_, the restoration network is first trained with accurate LQ image degradation descriptions, where the descriptions are artificially generated. Subsequently, under the \\(<\\)_restore\\(>\\)_instruction, the Context Embedding Module (CEM) is incorporated. During this process, the textual input of degradation descriptions is provided by the MLLM. CEM is responsible for leveraging the features of the image to enhance the description generated by MLLM, thereby making it more closely aligned with the accurate depiction of degradation. For the task of unified image restoration, we consider three commonly encountered degradation types: noise, rain, and low illumination. These degradation types encompass both additive and multiplicative forms of degradation, thereby exhibiting generalization capabilities. **Inference.** When presented with the instruction \\(<\\)_restore\\(>\\)_, the process initiates by taking a given degraded image \\(\\mathbf{I}_{LQ}\\) and a text prompt that solicits information regarding the degradation. These inputs are fed into the MLLM. Subsequently, the MLLM generates a descriptive text that effectively captures the low-level characteristics of the LQ image. The resulting text description is then encoded using the CLIP text encoder, yielding text feature \\(\\mathbf{T}_{fea}\\). These features are subsequently processed by the Context Enhance Module (CEM) and the Context Transformer (CT) to obtain the degradation context \\(\\mathbf{Z}\\). Finally, the context \\(\\mathbf{Z}\\) is supplied to the DC-former network for the restoration of the degraded images. When received the \\(<\\)_refine\\(>\\)_instruction, the CEM step is omitted. The restoration takes the dialogue with the users as the text input to realize adjustable restoration."
    },
    {
      "title": "Generation Of The Text Feature",
      "text": "Figure 2**(a)** illustrates the process of generating text features that contain information about image degradation in our approach. We utilized idefics-9b-instruct [13], a Multi-modal Large Language Model with 9 billion parameters, as the foundation of our approach. This model is designed to process both image and text sequences as input and generate coherent text as output. To fully leverage the vast knowledge and amazing perceptual capabilities of MLLMs, We carefully devised instructions for text input. These instructions include three specific questions related to the mentioned degradation types (i.e., noise, rain, and low-light conditions). As depicted in Figure 1, the large-scale model can generate promising responses to user queries based on the image information. Next, these output text descriptions are encoded into text features \\(\\mathbf{T}_{fea}\\in\\mathbb{R}^{77\\times 512}\\), using the text encoder of CLIP. The aforementioned procedure employs pretrained models, we do not need fine-tuning on them. By denoting the input text instructions and degraded image as \\(\\mathbf{T}_{input}\\) and \\(\\mathbf{I}_{LQ}\\), this process can be formulated as: \\[\\mathbf{T}_{fea}=\\mathcal{F}_{CLIP}(\\mathcal{F}_{MLLM}(\\mathbf{T}_{input}, \\mathbf{I}_{LQ})), \\tag{1}\\] where \\(\\mathcal{F}_{MLLM}\\) and \\(\\mathcal{F}_{CLIP}\\) indicate the text encoders of IDEFICS and CLIP, respectively. Figure 2: The overview of the proposed LLMRA. **(a)** The proposed LLMRA Framework. DEN, CT and DC-former are used to refine and incorporate the degradation information into the restoration network. **(b)** Context Enhance Module (CEM). **(c)** Context Transformer (CT)."
    },
    {
      "title": "Context Enhance Module",
      "text": "Under the instruction \\(<\\)_refine\\(>\\)_training recipe, the textual descriptions are artificially generated, which is accurate and directly corresponds to the specific type of image degradation. While the instruction \\(<\\)_restore_\\(>\\)requires the model to automatically restore the image without other priors. Due to the potential inaccuracies in the descriptions generated by MLLM, the context Enhance Module (CEM) is proposed to utilize the image features to enhance the degradation descriptions generated by MLLM. The goal is to bring these descriptions as close as possible to accurate representations of image degradation. As shown in Figure 2**(b)**, for an input LQ image \\(\\mathbf{I}_{{LQ}}\\in\\mathbb{R}^{3\\times H\\times W}\\), we obtain the shallow image feature through a convolutional ResBlock. Combining the shallow image feature, we process the text features \\(\\mathbf{T}_{{fea}}\\) through two text cross transformers, this process is formulated as: \\[\\mathbf{T}^{\\prime}_{{fea}}=\\mathrm{CEM}(\\mathbf{I}_{{LQ}},\\mathbf{T}_{{fea}}) \\tag{2}\\] where \\(\\mathbf{T}^{\\prime}_{{fea}}\\in\\mathbb{R}^{77\\times 512}\\) refers to the enhanced text features. After that, \\(\\mathbf{T}^{\\prime}_{{fea}}\\) (or \\(\\mathbf{T}_{{fea}}\\)) is processed by Context Transformer (CT) to get the degradation context embeddings \\(\\mathbf{Z}\\). CT is a single vanilla transformer [23] consists of a self attention and multi-layer perceptron. As mentioned above, we need to bring \\(\\mathbf{Z}\\) as close as possible to accurate representations of image degradation. To this end, we leverage an triplet loss to learn \\(\\mathbf{Z}\\) by maximizing the consistency with the postive inputs while minimizing the consistency between the negative ones. To be specific, for a degradation context \\(\\mathbf{Z}\\), \\(\\mathbf{Z}^{+}\\) and \\(\\mathbf{Z}^{-}\\) are the corresponding positive and negative counterpart, respectively. Then, the triplet loss \\(\\mathcal{L}_{tri}\\) could be reformulated as: \\[\\mathcal{L}_{tri}=\\sum_{i=1}^{N}\\left[\\left\\|\\mathbf{Z}_{i}-\\mathbf{Z}^{+}{}_ {i}\\right\\|_{2}^{2}-\\left\\|\\mathbf{Z}_{i}-\\mathbf{Z}^{-}{}_{i}\\right\\|_{2}^{2 }+\\alpha\\right]_{+} \\tag{3}\\] where \\(\\alpha\\) refers to the margin of the loss."
    },
    {
      "title": "Degradation Context Based Transformer",
      "text": "With the degradation context \\(\\mathbf{Z}\\) obtained from CT, the Degradation Context based Transformer Network (DC-former) is employed to restore the high-quality image from the input with unknown degradation. The architecture of DC-former, depicted in Figure 2**(a)**, consists of multiple stacked basic transformer blocks and Degradation Modulation Modules (DMM), organized in a UNet-shaped architecture. This design allows for effective information flow and contextual understanding, enabling the model to restore the image while considering the specific degradation characteristics. As shown in Figure 3, each DMM consists of an image cross attention transformer (yellow box), a Concatenate Attention Feature Fusion (CAFF) module and a basic transformer block (blue box) from Restormer [15]. The basic transformer block is composed of a Multi-Dconv head transposed attention (MDTA) and Gated-Dconv feed-forward network (GDFN), which allow more effective feature interactions. The process is formulated as: \\[\\mathbf{X}_{i+1}=\\mathrm{DMM}(\\mathbf{X}_{i},\\mathbf{Z}) \\tag{4}\\] where \\(\\mathbf{X}_{i}\\) and \\(\\mathbf{X}_{i+1}\\) denote the input and output feature maps. In CAFF, we first concatenate \\(\\mathbf{X}_{i}\\) and \\(\\mathbf{Y}_{i}\\) as \\(\\mathbf{XY}_{i}\\). Inspired by [14], the feature maps are processed with two branch to get local and global information and aggregated at the end. The local channel context \\(\\mathcal{L}(\\mathbf{X}\\mathbf{Y}_{i})\\in\\mathbb{R}^{C\\times H\\times W}\\) is computed via a bottleneck structure as follows: \\[\\mathcal{L}(\\mathbf{X}\\mathbf{Y}_{i})= \\mathrm{Norm}(\\mathrm{PWConv}_{2}( \\tag{5}\\] \\[\\delta(\\mathrm{Norm}(\\mathrm{PWConv}_{2}(\\mathbf{XY}_{i})))))\\] where \\(\\mathrm{Norm}\\) refers to Layer Normalization (LN), \\(\\mathrm{PWConv}_{2}\\) denotes point-wise convolution (\\(\\mathrm{PWConv}\\)), \\(\\delta\\) denotes the Rectified Linear Unit (ReLU). Note that the kernel sizes of the two \\(\\mathrm{PWConv}_{2}\\) are \\(2C\\times 2C\\times 1\\times 1\\) and \\(2C\\times C\\times 1\\times 1\\), respectively. As a result, \\(\\mathcal{L}(\\mathbf{X}_{i})\\) preserves the same shape as the input feature, allowing for the preservation and emphasis of intricate details in the low-level features. In the global branch, the features are first processed through a global average pooling (GAP), followed by similar operations as \\(\\mathrm{PWConv}_{1}\\), LN, ReLU, \\(\\mathrm{PWConv}_{1}\\) and LN, finally get the global channel context \\(\\mathcal{G}(\\mathbf{XY}_{i})\\). The \\(\\mathrm{PWConv}_{1}\\) here is for one dimension. It is formulated as: \\[\\mathcal{G}(\\mathbf{XY}_{i})= \\mathrm{Norm}(\\mathrm{PWConv}_{1}( \\tag{6}\\] \\[\\delta(\\mathrm{Norm}(\\mathrm{PWConv}_{1}(\\mathrm{GAP}(\\mathbf{ XY}_{i}))))))\\] By incorporating the global channel context \\(\\mathcal{G}(\\mathbf{XY}_{i})\\) and local channel context \\(\\mathcal{L}(\\mathbf{XY}_{i})\\) the modulated feature \\(\\mathbf{X}^{\\prime}_{i}\\) can be obtained as follows: \\[\\mathbf{X}^{\\prime}_{i}=\\mathbf{X}_{i}\\otimes\\mathbf{W}(\\mathbf{ XY}_{i})+(1-\\mathbf{W}(\\mathbf{XY}_{i}))\\otimes\\mathbf{Y}_{i} \\tag{7}\\] \\[\\mathbf{W}(\\mathbf{XY}_{i})=\\sigma((\\mathcal{L}(\\mathbf{XY}_{i}) \\oplus\\mathcal{G}(\\mathbf{XY}_{i})), \\tag{8}\\] where \\(\\sigma\\) denotes Sigmoid operation, \\(\\mathbf{W}\\) denotes the attention weights. \\(\\oplus\\) denotes the broadcasting addition and \\(\\otimes\\) denotes the element-wise multiplication."
    },
    {
      "title": "The Objective Function",
      "text": "As mentioned above, when training the models using the \\(<\\)_restore\\(>\\)_and \\(<\\)_refine\\(>\\)_instructions, we employ distinct objective functions to optimize the process. \\[\\mathcal{L}_{{refine}}=\\mathcal{L}_{{rec}} \\tag{9}\\] \\[\\mathcal{L}_{{restore}}=\\mathcal{L}_{{rec}}+\\mathcal{L}_{tri} \\tag{10}\\] where \\(\\mathcal{L}_{tri}\\) refers to the triplet loss (equation 3) and \\(\\mathcal{L}_{{rec}}=\\left\\|\\mathbf{I}_{{HQ}}-\\mathbf{\\tilde{I}}_{{HQ}}\\right\\|_ {1}\\) represents the reconstruction loss, which caculates the L1 norm between the ground truth \\(\\mathbf{I}_{{HQ}}\\) and the recovered high quality image \\(\\mathbf{\\tilde{I}}_{{HQ}}\\). Figure 3: Degradation Modulation Module (DMM) in DC-former."
    },
    {
      "title": "4 Experiments",
      "text": ""
    },
    {
      "title": "Experimental Settings",
      "text": "To demonstrate the effectiveness of the proposed LLMRA, we perform the evaluation on three representative image restoration tasks: image denoising, image deraining, and low light image enhancement. We train a unified model that can recover images across all three degradation types. **Implementation Details.** The architecture of the DC-former consists of a 4-level encoder-decoder, with varying numbers of Transformer blocks at each level, specifically [4, 6, 6, 8] from level-1 to level-4. We employ one DMM between every two consecutive decoder levels, totaling 4 DMMs in the overall DC-former network. The channel size of DC-former is set to 48. The model is trained with a batch size of 4. The network is optimized with Adam optimizer (\\(\\beta_{1}=0.9,\\beta_{2}=0.999\\)) with learning rate \\(1e^{-4}\\) for 800kiters. During training, we utilize cropped patches of size 128 x 128 as input, and to augment the training data, random horizontal and vertical flips are applied to the input images. Figure 4: Visual comparisons with the SOTA methods. Rows 1-2, 3-4, 5-6 rows display the results of image denoising, image deraining and low light image enhancement, respectively. The test images are from Urban100, Rain100L and LOLv1. Zoom in for better visualization. **Datasets.** In our experiments, we prepare several datasets for the training of these three tasks. For image denoising, we use WED [11] for training, which contains 4744 images. Testing is performed on BSD68 [13] and Urban100 [14] datasets. From clean images of WED BSD68 and Urban100, we generate the noisy images by adding Gaussian noise with different noise levels \\(\\sigma\\in\\{15,25,50\\}\\). For image deraining, we use the data from [10], including 1800 paired light rainy images for training and 100 images for testing. For low light image enhancement, we use LOL-v1 dataset [21], including 485 low/normal light images pairs for training and another 15 images for testing."
    },
    {
      "title": "Comparison With State-Of-The-Art Approaches",
      "text": "For comparing with the SOTA approaches, we trained the proposed LLMRA in all-in-one settings by optimizing the network (without CEM) with \\(\\mathcal{L}_{refine}\\) (equation 9). We compare our LLMRA with several unified image restoration approaches as well as specific degradation restoration methods on three tasks. More precisely, we compare DrCNN [10], IRCNN [10], FFDNet [10], AirNet [11], PromptIR [12] and DA-CLIP [13] for image denoising. We compare UMR [10], SIRR [21], MSPFN [10], Restormer [10], AirNet [11], PromptIR [12], and DA-CLIP [13] for image deraining. We compare Retinex [21], UFormer [10], EnGAN [10], KinD [10] URetinex-Net [20], Restormer [10] and DA-CLIP [13] for low light image enhancement. **Quantitative Comparison.** Table 1 presents results of image denoising. It shows that our LLMRA achieves 0.39dB for PSNR improvement over PromptIR for noise level \\(\\sigma=50\\) on Urban100 dataset. Similar trends can be observed for deraining tasks. On the deraining task (Table 2), our method yields performance gains of 1.61 dB over PromptIR. For low light image enhancement, our LLMRA achieves 0.035 for SSIM improvement over DA-CLIP. Our method even outperforms the resformer for image deraining and low-light image enhancement, which is trained in the single-task settings. **Qualitative Comparison.** In addition, we provide visual examples to illustrate the effectiveness of our proposed method. Figure 4 showcases the results of the three tasks. For image denoising, our LLMRA outperforms other state-of-the-art methods by effectively removing noise from the image without excessively blurring it. Similarly, the middle rows demonstrate the efficacy of our approach in the deraining task, as it successfully eliminates rain streaks and produces rain-free images. For low light image enhancement, previous methods often suffered from issues such as color distortion, over/underexposed regions, or failure to suppress noise in specific areas. In contrast, our approach excels in enhancing visibility, reliably enhancing the image without introducing artifacts, and robustly preserving the natural color."
    },
    {
      "title": "Impact Of The Text Inputs",
      "text": "We manage to use text information to assist image restoration, as text input is more readily available and allows for adjustable and interactive restoration manner through dialogue \\begin{table} \\begin{tabular}{l|c c c|c c c} \\hline & \\multicolumn{3}{c|}{BSD68} & \\multicolumn{3}{c}{Urban100} \\\\ Method & \\(\\sigma=15\\) & \\(\\sigma=25\\) & \\(\\sigma=50\\) & \\(\\sigma=15\\) & \\(\\sigma=25\\) & \\(\\sigma=50\\) \\\\ \\hline DnCNN & 33.89/0.9290 & 31.23/0.8830 & 27.92/0.7896 & 32.98/0.9314 & 30.81/0.9015 & 27.59/0.8331 \\\\ IRCNN & 33.87/0.9285 & 31.18/0.8824 & 27.88/0.7898 & 27.59/0.8331 & 31.20/0.9088 & 27.70/0.8396 \\\\ FFDNet & 33.87/0.9290 & 31.21/0.8821 & 27.96/0.7887 & 33.83/0.9418 & 31.40/0.9120 & 28.05/0.8476 \\\\ \\hline AirNet & 33.85/0.9293 & 31.22/0.8837 & 27.98/0.7933 & 33.89/0.9419 & 31.52/0.9137 & 28.19/0.8520 \\\\ DA-CLIP & 26.34/0.6821 & 25.77/0.6531 & 24.31/0.5712 & - & - & - \\\\ PromptIR & 33.91/0.9296 & 31.28/0.8840 & 28.03/0.7926 & 33.93/0.9417 & 31.52/0.9121 & 28.17/0.8498 \\\\ Ours & **34.01/0.9302** & **31.37/0.8849** & **28.13/0.7930** & **34.12/0.9435** & **31.79/0.9163** & **28.56/0.8578** \\\\ \\hline \\end{tabular} \\end{table} Table 1: Denoising comparisons in the single-task setting on BSD68 and Urban100 datasets. Top rows: methods under the single-task setting. Bottom rows: methods under the all-in-one setting. The optimal and sub-optimal PSNR/SSIM\\(\\uparrow\\) results are highlighted using bold and underlined, respectively. \\begin{table} \\begin{tabular}{l|c c c c|c c c} \\hline & UMR & SIRR & MSPFN & Restormer & AirNet & DA-CLIP & PromptIR & Ours \\\\ \\hline PSNR\\(\\uparrow\\) & 32.39 & 32.37 & 33.50 & 37.57 & 34.90 & 35.19 & 37.32 & **38.93** \\\\ SSIM\\(\\uparrow\\) & 0.921 & 0.926 & 0.948 & 0.974 & 0.968 & 0.960 & 0.979 & **0.984** \\\\ \\hline \\end{tabular} \\end{table} Table 2: Deraining results on Rain100L. Left columns: methods under single-task setting. Right columns: methods under all-in-one setting. The optimal and sub-optimal results are highlighted using bold and underlined, respectively. \\begin{table} \\begin{tabular}{l|c c c c c|c c c} \\hline & Retinex-Net & UFormer & EnGAN & KinD & URetinex-Net & Restormer & DA-CLIP & Ours \\\\ \\hline PSNR\\(\\uparrow\\) & 16.40 & 16.36 & 17.56 & 20.86 & 21.33 & 22.43 & **23.40** & 23.30 \\\\ SSIM\\(\\uparrow\\) & 0.500 & 0.771 & 0.665 & 0.790 & 0.834 & 0.823 & 0.811 & **0.846** \\\\ \\hline \\end{tabular} \\end{table} Table 3: Low light image enhancement results on LOL-v1. Left columns: methods under single-task setting. Right columns: methods under all-in-one setting. The optimal and sub-optimal results are highlighted using bold and underlined, respectively. with the MLLMs. To verify the impact of the text inputs, we prepared two set of text descriptions for the test datasets, which are called \"ground truth\" and \"ground false\" text input. As shown in Figure 5, the task is image denoising for the first row (14037.png from BSD68 with \\(\\sigma=50\\)), the ground truth text description could be \"The image is well lit. No rain streaks detected. The image has gaussian noise degradation and the noise level is high.\" Conversely, the ground false text description is would be completely opposite, like \"The image is dark. The image is degraded by rain streaks. No noise detected.\" From Figure 5, it is evident that the presence of ground truth text input results in effective noise removal without any other modifications. Conversely, when ground false text input is used, the noise persists but the lighting is enhanced. Similar clue could also be drawn from the quantitative results in Table 4, when confronted with accurate and erroneous textual input, the disparity in the results of restoration is substantial."
    },
    {
      "title": "Ablation Study",
      "text": "**Impact of CEM.** To verify the impact of Context Enhance Module (CEM) on enhancing the text descriptions obtained from the MLLM in the universal image restoration task, we carry out some experiments. In this section, a set of predefined specific questions related to the mentioned degradation types (i.e., noise, rain, and low-light conditions) are sent to the MLLM, and it would generate corresponding responses to be the text descriptions for further guiding the restoration. We restore the images with and without CEM under these conditions. The results are shown in Table 2, revealing a significant improvement in the restoration outcomes when CEM is incorporated. **The way of modulating the text features.** In the domain of text-to-image generation [14], researchers commonly employ a denoising UNet with a cross transformer as the basic module to modulate the text features. However, in our proposed method, we utilize DMMs for the degradation context modulation. In order to validate the effectiveness of our method, we follow the approach of these text-to-image generation methods by removing the CAFF modules and stacking the cross transformers in the decoder. The experimental results are presented in Table 6, which demonstrates the effectiveness of the proposed DMM."
    },
    {
      "title": "5 Conclusion",
      "text": "This paper introduces LLMRA, a novel framework that leverages multi-modal large language models for universal image restoration. The core contribution of our framework is utilizing the MLLM and a text-guided restoration network to realize a more accurate, adjustable and interactive restoration manner. The Context Enhance Module and the Degradation Context based Transformer Network are proposed to effectively enhance the degradation information and incorporate it into the restoration network. Experimental evaluation on unified image restoration tasks demonstrates that LLMRA leads to significant performance on image denoising, image deraining, and low light image enhancement. Nevertheless, it is important to acknowledge some limitations of the proposed LLMRA. The performance of LLMRA may fluctuate with the performance of MLLM, as it may provide uninformative or even harmful answers of the degradation information, thus affecting the quality of restoration. Fortunately, users can \\(<\\)_refine\\(>\\)_the results by engaging in subsequent dialogue. More ever, our experiments are currently limited to only three tasks. Although these three tasks are representative to some extent, as they encompass both additive and multiplicative degradation. In future research, we aim to broaden the scope of our investigation to encompass a wider range of restoration tasks involving different types of degradation. \\begin{table} \\begin{tabular}{c|c c} \\hline & w.o. DMM & Ours \\\\ \\hline BSD68 & 28.02/0.7913 & **28.13**/**0.7930** \\\\ Rain100L & 37.71/0.9796 & **38.93**/**0.9842** \\\\ LoLv1 & 19.40/0.8013 & **23.30**/**0.8457** \\\\ \\hline \\end{tabular} \\end{table} Table 6: Ablation study on the way of modulating the text features. Results are reported on BSD68 (\\(\\sigma=50\\)), Rain100L and LOLv1 datasets. The best results are shown in boldface. \\begin{table} \\begin{tabular}{c|c c} \\hline & with gf text & with g text \\\\ \\hline BSD68 & 14.46/0.4790 & 28.13/0.7930 \\\\ Rain100L & 20.11/0.8302 & 38.93/0.9842 \\\\ LoLv1 & 7.59/0.1440 & 23.30/0.8457 \\\\ \\hline \\end{tabular} \\end{table} Table 4: Quantitative results for the impact of the text input, evaluated on BSD68 (\\(\\sigma=50\\)), Rain100L and LOLv1 dataset. ‚Äúwith gf text‚Äù means input ground truth text descriptions. ‚Äúwith gf text‚Äù means input ground false text descriptions. Figure 5: Impact of the text input. \\begin{table} \\begin{tabular}{c|c c} \\hline & w.o. CEM & Ours \\\\ \\hline BSD68 & 25.18/0.6913 & **28.11**/**0.7964** \\\\ Rain100L & 26.54/0.8838 & **38.64**/**0.9831** \\\\ LoLv1 & 17.51/0.6999 & **20.19**/**0.8243** \\\\ \\hline \\end{tabular} \\end{table} Table 5: Ablation study on the impact of CEM. Results are reported on BSD68 (\\(\\sigma=50\\)), Rain100L and LOLv1 datasets. The best results are shown in boldface."
    },
    {
      "title": "References",
      "text": "* [Abdal _et al._2022] Rameen Abdal, Peihao Zhu, John Femiani, Niloy Mitra, and Peter Wonka. Clip2stylegan: Unsupervised extraction of stylegan edit directions. In _SIGGRAPH_, pages 1-9, 2022. * [Alayrac _et al._2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _NeurIPS_, 2022. * [Antol _et al._2015] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In _ICCV_, 2015. * [Brooks _et al._2023] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In _CVPR_, 2023. * [Chen _et al._2015] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_, 2015. * [Crowson _et al._2022] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Edward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance. In _ECCV_, 2022. * [Dai _et al._2021] Yimian Dai, Fabian Gieseke, Stefan Oehmcke, Yiquan Wu, and Kobus Barnard. Attentional feature fusion. In _WACV_, 2021. * [Dai _et al._2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructhlip: Towards general-purpose vision-language models with instruction tuning. _arXiv preprint arXiv:2305.06500_, 2023. * [Geng _et al._2023] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen, et al. Instructdiffusion: A generalist modeling interface for vision tasks. _arXiv preprint arXiv:2309.03895_, 2023. * [Huang _et al._2015] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In _CVPR_, 2015. * [Huggingface2023] Huggingface. Introducing idefics: An open reproduction of state-of-the-art visual language model, 2023. * [Jiang _et al._2020] Kui Jiang, Zhongyuan Wang, Peng Yi, Chen Chen, Baojin Huang, Yimin Luo, Jiayi Ma, and Junjun Jiang. Multi-scale progressive fusion network for single image deraining. In _CVPR_, 2020. * [Jiang _et al._2021] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. Enlightengan: Deep light enhancement without paired supervision. _TIP_, 2021. * [Lai _et al._2023] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. _arXiv preprint arXiv:2308.00692_, 2023. * [Li _et al._2020] Ruoteng Li, Robby T Tan, and Loong-Fah Cheong. All in one bad weather removal using architectural search. In _CVPR_, 2020. * [Li _et al._2022] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jiancheng Lv, and Xi Peng. All-in-one image restoration for unknown corruption. In _CVPR_, 2022. * [Li _et al._2023] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstraping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023. * [Liu _et al._2023] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _NeurIPS_, 2023. * [Luo _et al._2023] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas B Schon. Controlling vision-language models for universal image restoration. _arXiv preprint arXiv:2310.01018_, 2023. * [Ma _et al._2016] Kede Ma, Zhengfang Duanmu, Qingbo Wu, Zhou Wang, Hongwei Yong, Hongliang Li, and Lei Zhang. Waterloo exploration database: New challenges for image quality assessment models. _TIP_, 2016. * [Martin _et al._2001] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In _ICCV_, 2001. * [Potlapalli _et al._2023] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Shahbaz Khan. Promptir: Prompting for all-in-one blind image restoration. _arXiv preprint arXiv:2306.13090_, 2023. * [Radford _et al._2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021. * [Rombach _et al._2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022. * [Taori _et al._2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023. * [Touvron _et al._2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023. * [Touvron _et al._2023]* [Valanarasu _et al._2022] Jeya Maria Jose Valanarasu, Rajeev Yasarla, and Vishal M Patel. Transweather: Transformer-based restoration of images degraded by adverse weather conditions. In _CVPR_, 2022. * [Vaswani _et al._2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _NeurIPS_, 2017. * [Wang _et al._2022] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general u-shaped transformer for image restoration. In _CVPR_, 2022. * [Wei _et al._2018] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. _arXiv preprint arXiv:1808.04560_, 2018. * [Wei _et al._2019] Wei Wei, Deyu Meng, Qian Zhao, Zongben Xu, and Ying Wu. Semi-supervised transfer learning for image rain removal. In _CVPR_, 2019. * [Wu _et al._2022] Wenhui Wu, Jian Weng, Pingping Zhang, Xu Wang, Wenhan Yang, and Jianmin Jiang. Uretinex-net: Retinex-based deep unfolding network for low-light image enhancement. In _CVPR_, 2022. * [Wu _et al._2023] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: A benchmark for general-purpose foundation models on low-level vision. _arXiv preprint arXiv:2309.14181_, 2023. * [Xia _et al._2023a] Bin Xia, Shiyin Wang, Yingfan Tao, Yitong Wang, and Jiaya Jia. LImga: Multimodal large language model based generation assistant. _arXiv preprint arXiv:2311.16500_, 2023. * [Xia _et al._2023b] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool. Diffir: Efficient diffusion model for image restoration. _arXiv preprint arXiv:2303.09472_, 2023. * [Yang _et al._2019] Wenhan Yang, Robby T Tan, Jiashi Feng, Zongming Guo, Shuicheng Yan, and Jiaying Liu. Joint rain detection and removal from a single image with contextualized deep networks. _TPAMI_, 2019. * [Yasarla and Patel2019] Rajeev Yasarla and Vishal M Patel. Uncertainty guided multi-scale residual learning-using a cycle spinning cnn for single image de-raining. In _CVPR_, 2019. * [Zamir _et al._2022] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In _CVPR_, 2022. * [Zhang _et al._2017a] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. _TIP_, 2017. * [Zhang _et al._2017b] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep CNN denoiser prior for image restoration. In _CVPR_, 2017. * [Zhang _et al._2018] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. _TIP_, 2018. * [Zhang _et al._2019] Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling the darkness: A practical low-light image enhancer. In _ACM MM_, 2019. * [Zhu _et al._2023] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023."
    }
  ]
}