{
  "title": "Negated Complementary Commonsense using Large Language Models",
  "authors": [
    "Navid Rezaei",
    "Marek Z Reformat"
  ],
  "abstract": "\n Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions adversely affect the model responses. We propose a model-agnostic methodology to improve the performance in negated complementary scenarios. Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions. The code, data, and experiments are available under:  https://github.com/navidre/  negated_complementary_commonsense . \n",
  "references": [
    {
      "id": null,
      "title": "Negated Complementary Commonsense using Large Language Models",
      "authors": [
        "Navid Rezaei",
        "Marek Z Reformat"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Alec Radford, Ilya Sutskever, and Dario Amodei",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell",
        "Sandhini Agarwal",
        "Ariel Herbert-Voss",
        "Gretchen Krueger",
        "Tom Henighan",
        "Rewon Child",
        "Aditya Ramesh",
        "Daniel Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Chris Hesse",
        "Mark Chen",
        "Eric Sigler",
        "Mateusz Litwin",
        "Scott Gray",
        "Benjamin Chess",
        "Jack Clark",
        "Christopher Berner",
        "Sam Mccandlish"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann",
        "Parker Schuh",
        "Kensen Shi",
        "Sasha Tsvyashchenko",
        "Joshua Maynez",
        "Abhishek Rao",
        "Parker Barnes",
        "Yi Tay",
        "Noam Shazeer",
        "Emily Vinodkumar Prabhakaran",
        "Nan Reif",
        "Ben Du",
        "Reiner Hutchinson",
        "James Pope",
        "Jacob Bradbury",
        "Michael Austin",
        "Guy Isard",
        "Pengcheng Gur-Ari",
        "Toju Yin",
        "Anselm Duke",
        "Sanjay Levskaya",
        "Sunipa Ghemawat",
        "Henryk Dev",
        "Xavier Michalewski",
        "Vedant Garcia",
        "Kevin Misra",
        "Liam Robinson",
        "Denny Fedus",
        "Daphne Zhou",
        "David Ippolito",
        "Hyeontaek Luan",
        "Barret Lim",
        "Alexander Zoph",
        "Ryan Spiridonov",
        "David Sepassi",
        "Shivani Dohan",
        "Mark Agrawal",
        "Katherine Omernick",
        "Zongwei Lee",
        "Xuezhi Zhou",
        "Brennan Wang",
        "Mark Saeta",
        "Orhan Diaz",
        "Michele Firat",
        "Jason Catasta",
        "Kathy Wei",
        "Douglas Meier-Hellstern",
        "Eck"
      ],
      "year": "",
      "venue": "",
      "doi": "10.48550/arXiv.2204.02311"
    },
    {
      "id": "b2",
      "title": "Analyzing commonsense emergence in few-shot knowledge models",
      "authors": [
        "Jeff Da",
        "Le Ronan",
        "Ximing Bras",
        "Yejin Lu",
        "Antoine Choi",
        "Bosselut"
      ],
      "year": "2021",
      "venue": "Analyzing commonsense emergence in few-shot knowledge models",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "id": "b4",
      "title": "Answering the call for a standard reliability measure for coding data",
      "authors": [
        "Andrew F Hayes",
        "Klaus Krippendorff"
      ],
      "year": "2007",
      "venue": "Communication Methods and Measures",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "(COMET-)ATOMIC 2020: On symbolic and neural commonsense knowledge graphs",
      "authors": [
        "Jena D Hwang",
        "Chandra Bhagavatula",
        "Le Ronan",
        "Jeff Bras",
        "Keisuke Da",
        "Antoine Sakaguchi",
        "Yejin Bosselut",
        "Choi"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "I'm not mad\": Commonsense implications of negation and contradiction",
      "authors": [
        "Liwei Jiang",
        "Antoine Bosselut",
        "Chandra Bhagavatula",
        "Yejin Choi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.346"
    },
    {
      "id": "b7",
      "title": "Language models (mostly) know what they know",
      "authors": [
        "Saurav Kadavath",
        "Tom Conerly",
        "Amanda Askell",
        "T J Henighan",
        "Dawn Drain",
        "Ethan Perez",
        "Nicholas Schiefer",
        "Zachary Dodds",
        "Nova Dassarma",
        "Eli Tran-Johnson",
        "Scott Johnston",
        "Sheer El-Showk",
        "Andy Jones",
        "Nelson Elhage",
        "Tristan Hume",
        "Anna Chen",
        "Yuntao Bai",
        "Sam Bowman",
        "Stanislav Fort",
        "Deep Ganguli",
        "Danny Hernandez",
        "Josh Jacobson",
        "John Kernion",
        "Shauna Kravec",
        "Liane Lovitt",
        "Kamal Ndousse",
        "Catherine Olsson",
        "Sam Ringer",
        "Dario Amodei",
        "Tom B Brown",
        "Jack Clark",
        "Nicholas Joseph",
        "Benjamin Mann",
        "Sam Mccandlish",
        "Christopher Olah",
        "Jared Kaplan"
      ],
      "year": "2022",
      "venue": "Language models (mostly) know what they know",
      "doi": "10.48550/ARXIV.2207.05221"
    },
    {
      "id": "b8",
      "title": "A path towards autonomous machine intelligence version 0.9",
      "authors": [
        "Yann Lecun"
      ],
      "year": "2022",
      "venue": "A path towards autonomous machine intelligence version 0.9",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "",
      "authors": [
        "Jack W Rae",
        "Sebastian Borgeaud",
        "Trevor Cai",
        "Katie Millican",
        "Jordan Hoffmann",
        "H Francis Song",
        "John Aslanides",
        "Sarah Henderson",
        "Roman Ring",
        "Susannah Young",
        "Eliza Rutherford",
        "Tom Hennigan",
        "Jacob Menick",
        "Albin Cassirer",
        "Richard Powell",
        "George Van Den Driessche",
        "Lisa Anne Hendricks",
        "Maribeth Rauh",
        "Po-Sen Huang",
        "Amelia Glaese",
        "Johannes Welbl",
        "Sumanth Dathathri",
        "Saffron Huang",
        "Jonathan Uesato",
        "John Mellor",
        "Irina Higgins",
        "Antonia Creswell",
        "Nat Mcaleese",
        "Amy Wu",
        "Erich Elsen",
        "M Siddhant",
        "Elena Jayakumar",
        "David Buchatskaya",
        "Esme Budden",
        "Karen Sutherland",
        "Michela Simonyan",
        "Laurent Paganini",
        "Lena Sifre",
        "Martens",
        "Lorraine Xiang",
        "Adhiguna Li",
        "Aida Kuncoro",
        "Elena Nematzadeh",
        "Domenic Gribovskaya",
        "Angeliki Donato",
        "Arthur Lazaridou",
        "Jean-Baptiste Mensch",
        "Maria Lespiau",
        "Nikolai Tsimpoukelli",
        "Doug Grigorev",
        "Thibault Fritz",
        "Mantas Sottiaux",
        "Toby Pajarskas",
        "Zhitao Pohlen",
        "Daniel Gong",
        "Cyprien Toyama",
        "Yujia De Masson D'autume",
        "Tayfun Li",
        "Vladimir Terzi",
        "Igor Mikulik",
        "Aidan Babuschkin",
        "Diego Clark",
        "De Las",
        "Aurelia Casas",
        "Chris Guy",
        "James Jones",
        "Matthew Bradbury",
        "Blake A Johnson",
        "Laura Hechtman",
        "Iason Weidinger",
        "William S Gabriel",
        "Edward Isaac",
        "Simon Lockhart",
        "Laura Osindero",
        "Chris Rimell",
        "Oriol Dyer",
        "Kareem Vinyals",
        "Jeff Ayoub",
        "Lorrayne Stanway",
        "Bennett"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Utilizing language models to expand vision-based commonsense knowledge graphs",
      "authors": [
        "Navid Rezaei",
        "Marek Z Reformat"
      ],
      "year": "2022",
      "venue": "Symmetry",
      "doi": "10.3390/sym14081715"
    },
    {
      "id": "b11",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N Gomez",
        "≈Å Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Chain of thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Ed Chi",
        "Quoc Le",
        "Denny Zhou"
      ],
      "year": "2022",
      "venue": "Chain of thought prompting elicits reasoning in large language models",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Relation Question xWant [head]. What does PersonX want to do? [head]. What does PersonX not want to do? xReact [head]. What does PersonX feel about it? [head]. What does PersonX not feel about it? oWant [head]. What does PersonY want to do? [head]. What does PersonY not want to do? CapableOf What is",
      "authors": [
        "Peter West",
        "Chandra Bhagavatula",
        "Jack Hessel",
        "Jena Hwang",
        "Liwei Jiang",
        "Ronan Le Bras",
        "Ximing Lu",
        "Sean Welleck",
        "Yejin Choi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2022.naacl-main.341"
    }
  ],
  "sections": [
    {
      "title": "Repeated In Natural Language Reasoning And Structured Explanations Workshop (Nlrse)",
      "text": "ACL 2023 Navid Rezaei ECE, University of Alberta Edmonton, Canada T6G 1H9 nrezaeis@ualberta.ca &Marek Z. Reformat ECE, University of Alberta Edmonton, Canada T6G 1H9 University of Social Sciences 90-113 Lodz, Poland reformat@ualberta.ca"
    },
    {
      "title": "Abstract",
      "text": "Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions adversely affect the model responses. We propose a model-agnostic methodology to improve the performance in negated complementary scenarios. Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions. The code, data, and experiments are available under: [https://github.com/navidre/negated_complementary_commonsense](https://github.com/navidre/negated_complementary_commonsense)."
    },
    {
      "title": "1 Introduction",
      "text": "The larger the language models (LLMs) become, the better they demonstrate new, outstanding capabilities. For example, one is conducting a conversation about commonsense scenarios. However, our interaction with LLMs has led us to observe that the models tend to emphasize the normal flow of events and seem to struggle with questions involving a negated form of verbs, such as _not_ or _cannot_. An example of that is in Figure 1. Therefore, in this paper, we focus on demonstrating the issue and then suggest an approach to remedy the problem. To better clarify the problem statement, we start with an example and then formalize it using elements of the set theory. Let us look at the scenario in Figure 1; the standard question is \"Who PersonX can be?\". The answer to this question is _Santa Claus_. The answer to the _negated complementary_ question - \"Who PersonX _cannot_ be?\" - should be all valid answers which are not the answer to the standard (can be) question. A valid answer fits the scenario described. In this case, we ask about a person, so a non-person cannot be a valid answer. To better illustrate the concept of a _negated complementary_ question, we refer to the basic notion of the complement of a set, Figure 2. Furthermore, we define a set of correct answers to a _negated complementary_, Equation 1. \\[NC=V\\cap A^{\\prime}=\\{x\\mid x\\in V\\wedge x\\notin A\\} \\tag{1}\\] where \\(NC\\) represents answers to the _negated complementary_ question, \\(V\\) is the set of all valid answers, \\(A\\) is the set of correct answers to the standard question, and \\(A^{\\prime}\\) is the complement of \\(A\\) under the universal set of all answers (\\(U\\)). We focus our efforts on commonsensical ques Figure 1: An example of a large language model (GPT-3) generating negated commonsense. Five responses per query are demonstrated. The applied pre-processing and post-processing can improve the performance of the models in negated commonsense cases. Non-specific answers, such as _not Santa_, are considered incorrect. Figure 2: Venn diagram of answer sets: \\(U\\) is the universal set of answers; \\(V\\) is the set of all valid answers that includes two sets ‚Äì correct answers to a standard question \\(A\\), and correct answers to its negated complementary version \\(NC\\). tions as the uncertainty of results depends on the context and experiences of people answering the questions. As defined in LeCun (2022), commonsense is a collection of world models representing what is likely, plausible, or impossible. In light of that, our goal is to assess the ability of LLMs to answer plausible questions that could be refuted or accepted in a given context. Given their pre-training nature, we hypothesize that LLMs have an inherent bias towards likely scenarios, which are the most repeated in the common text. Most of the text available on the web contains information supporting answers to 'positive' questions, like, how to do things or where to go, not to questions such as how things could not be done or where not to go. It results in an imbalance of the training datasets due to the sparsity of plausible or impossible scenarios. In this paper, we demonstrate that LLMs have difficulty answering _negated complementary_ questions, which results in responses representing plausible, but not impossible, answers. Although LLMs are shown to have this shortcoming, we claim that enough instructions and examples, especially showing reasoning processes, can guide the LLMs into the right path to answer _negated complementary_ questions with commonsense context. Our contributions are as follows. (1) We present an analysis exposing the shortcomings of LLMs when it comes to _negated complementary_ questions in commonsensical scenarios. (2) We propose a novel methodology to improve the performance of the GPT-3 model when _negated complementary_ questions are asked; compare the results with the results obtained using conventional methods. Our code, human-evaluation process, and data will be publicly available."
    },
    {
      "title": "2 Related Work",
      "text": "Language models with transformer architectures have revolutionized the natural language processing landscape in recent years Vaswani et al. (2017); Devlin et al. (2019). It is shown that improved performance and new capabilities emerge when scaling up the size of language models Brown et al. (2020); Chowdhery et al. (2022), although more is needed in challenging tasks, such as commonsense Rae et al. (2021). A body of research focuses on analyzing and extracting commonsense from language models West et al. (2022); Rezaei and Reformat (2022); Hwang et al. (2021); Da et al. (2021). Authors of Jiang et al. (2021) focus on implications of negated statements and contradictions, where in a commonsense triple relationship (head-relation-tail), the head is either contradicted or logically negated. Comparably this paper focuses on negating relations instead of the head, as explained in Section 4."
    },
    {
      "title": "3 Commonsense Data",
      "text": "The commonsense dataset used in this paper is the ATOMIC-2020 dataset Hwang et al. (2021). It includes general purpose commonsense knowledge, divided into three main categories - physical, event-centered, and social commonsense. The ATOMIC 2020 dataset is licensed under CC-BY and we use it according to the license. In our experiments, ten relation types are selected from the twenty-three relations from the ATOMIC-2020 dataset. These ten relation types performed worse in our initial evaluation of _negated complementary_ questions. The relations are: _xWant_, _xReact_, _oWant_, _CapableOf_, _Desires_, _HinderedBy_, _isBefore_, _isAfter_, _AtLocation_, _HasSubEvent_. The worse-performer triples are intuitively more common in the normal format in written language than their negated complementary versions, which can result in unbalanced training data. The dataset is formatted in a triple style. Each atomic piece of data contains \\(\\langle head-relation-tail\\rangle\\). For example, \\(\\langle a\\ curved\\ yellow\\ fruit\\ (head)-CanBe\\ (relation)-banana\\ (tail)\\rangle\\)."
    },
    {
      "title": "4 Methodology",
      "text": "We propose a pipeline system to improve the performance on _negated complementary commonsense_ questions. The pipeline consists of an input prompt-ing technique and a post-processing module. The input prompt adds relevant context and logic in the form of chain-of-thought prompting Wei et al. (2022) to improve the LLM performance. The post-processing module selects the outputs with a higher chance of correctness and filters out the rest."
    },
    {
      "title": "Generating Negated Complementary Questions",
      "text": "As described in Section 3, the used dataset is in the format of triples. To form a standard question, we use the head and the relation nodes and leave out the tail to be answered. By standard, we mean utilizing the head, relation, and tail, without any modifications. Assuming a triple, _a curved yellow fruit_ (head), _CanBe_ (relation), _banana_ (tail), the standard question is _What can be a curved yellow fruit?_. The _negated complementary_ question is formed by negating the relation and verbalizing the resulting triple in question format: _What cannot be a curved yellow fruit?_ A valid answer to the standard question is _banana_, and a reasonable response to the _negated complementary_ question is _apple_. The process is visualized in Figure 3. For the complete list of triple verbalizations, please see Appendix A."
    },
    {
      "title": "Prompting Technique",
      "text": "The proposed methodology to improve the performance of LLMs relies on building an adequate prompt. It starts with a general introduction of what negations are and emphasizes a need to pay special attention to the word _Not_. The chain-of-thought prompt in each answer has five sections in sequence: 1) phrasing standard question; 2) standard question reasoning, 3) standard question answer; 4) negation logic, and 5) _negated complementary_ question answer. The steps are visualized in Figure 4. For a fair comparison, we used the same number of five question/answer examples in the prompts. We also used the same questions for all prompts."
    },
    {
      "title": "Post Processing",
      "text": "Inspired by Kadavath et al. (2022), we feed the question and answer pair back to the GPT-3 model and ask if it considers a question/answer pair correct. The prompt has instructions for assessing an answer and includes five sample questions/answer pairs. Interestingly, this extra step can improve the results by almost one percent. To better understand the effect of this step, please refer to Table 2."
    },
    {
      "title": "5 Experiments",
      "text": "Experiments are conducted on each type of relation mentioned in Section 3. A hundred data points (triples) are sampled randomly from the dataset. The head and relation from each triple are verbalized and fed into the GPT-3 model (_text-davinci-002_). The goal is to predict the tail for two forms of questions: (1) standard question; (2) _negated complementary_ question. For each question, three responses are requested from the model. They are then parsed, and the answers (tails) are automatically extracted. Therefore, three possible tails are obtained for each head and relation, which results in \\(600\\) total answers per method. In social commonsense scenarios, PersonX and PersonY are used in place of gender-specific pronouns to make the questions and answers gender-neutral. The experiments are done using the GPT-3 model Brown et al. (2020) with version _text-davinci-002_, which has 175 billion parameters. The temperature is set to \\(0.7\\), and in case of no answer, it is increased to \\(1.0\\). The maximum length of the output is set between \\(100\\) and \\(150\\) tokens, depending on the method. The presence and frequency penalties are set to \\(0\\). GPT-3 is commercially available, and we have used it within its intended usage and terms of service."
    },
    {
      "title": "Human Evaluations",
      "text": "We use Amazon mTurk evaluations via AWS Sage-Maker to evaluate the results. Each answer is written in a sentence format and given to nine different annotators for assessment. Instructions and examples are provided with each question to assist the annotators better. The options to choose from are: (1) Makes sense; (2) Sometimes makes sense; (3) Does not make sense or incorrect; (4) The first part and the second part are not related; or not enough information to judge; (5) Unfamiliar to me to judge. The first two options are considered correct, the second two are considered incorrect, and the last is considered unfamiliar. To measure inter-rater reliability, we use Krippendorff's alpha and make sure the value is above acceptable amounts (minimum 0.667) Hayes and Krippendorff (2007). The evaluators were paid based on AWS guidelines. Figure 3: The process to automatically generate negated complementary questions from dataset triples. The head and relation nodes are used to form a question."
    },
    {
      "title": "Results",
      "text": "As seen in Table 1, our method outperforms the few-shot method by more than eleven percentage points when answering _negated complementary_ questions. The few-shot method includes five different questions in the prompt with their answers without chain-of-thought prompting. The performance of our method can mainly be attributed to the specific chain-of-thought prompting with negation logic description, Figure 4. More information about the main contributing factors is in Section 5.3. Although chain-of-thought prompting seems to help the _negated complementary_ questions, it adversely affects answers to the standard questions. Please note that the chain-of-thought prompt for the standard questions does not include negation logic, and a post-processing technique similar to negated complementary questions is performed."
    },
    {
      "title": "Ablation Studies",
      "text": "To gain insight into the importance of elements of our method, we perform an ablation study, Table 2. As we can see, adding standard question reasoning (step 2 of Figure 4) results in more than 7% improvement in the results. Adding the thought process explaining the negation logic (steps 1, 3, and 4 of Figure 4) adds another 3% performance improvement. Finally, the post-processing (Section 4.3) is responsible for about 1% improvement in the results."
    },
    {
      "title": "6 Conclusions",
      "text": "In this paper, we demonstrate how simple changes in question formats, which can be trivial for humans, can be challenging for large language models (LLMs). We specifically focus on _negated complementary_ questions in a commonsense context, which is constructed by negating a relation in a commonsense triple. Given the vast amount of knowledge embedded in LLMs, we show that by appropriate guidance, the models could perform well on _negated complementary_ tasks. Our method results in more than eleven percent improvement compared to the vanilla few-shot method. Given the widespread usage of LLMs and their growth rate, we believe focusing on and solving the model's weaknesses is imperative. As future work, _negated complementary_ task can be further analyzed in different formats, such as sentence instead of a question, and also different contexts, e.g., new datasets. \\begin{table} \\begin{tabular}{l l l} \\hline \\hline **Method** & **Standard** & **Negated Complementary** \\\\ \\hline Few-shot & **88.7\\%** & 78.7\\% \\\\ Ours & 88.1\\% & **89.8\\%** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Our method compared with the few-shot method when applied to ATOMIC-2020 dataset. \\begin{table} \\begin{tabular}{l l} \\hline \\hline **Method** & **Neg. Comp.** \\\\ \\hline Ours & 89.8\\% \\\\ Ours-wo-pp & 89.0\\% \\\\ Ours-wo-nl-pp & 86.0\\% \\\\ Few-shot & 78.7\\% \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Ablation study of the method: _Ours-wo-pp_ is ours without post-processing; _Ours-wo-nl-pp_ is ours without negation logic and post-processing. Figure 4: Chain-of-thought steps for each answer. The process is to answer the standard question first and then lead the model to answer the negated complementary version."
    },
    {
      "title": "Limitations",
      "text": "The experiments in this paper have focused on the _negated complementary_ task in the context of commonsense and the format of questions. However, it is interesting to experiment with other contexts, such as mathematical datasets and other formats, such as sentences instead of questions. This paper only uses the English language in the _negated complementary_ task experiments, so further investigation is needed in other languages to understand better the limitations of large language models across other languages. GPT-3 is commercially available, and the cost can be a limitation. For example, the current price for _text-davinci-002_ model is $0.02 per 1,000 tokens."
    },
    {
      "title": "Ethics Statement",
      "text": "Given the widespread use of large language models and their growth, more software systems will depend on them. This could improve productivity and accessibility, but any vulnerability in large language models can propagate through the system and affect the end users. This work focused on distorted commonsense scenarios that are almost trivial for humans but can be challenging for large language models. Not only we highlighted the _negated complementary_ questions issue, but we also suggested practical solutions that do not require extensive computation. We believe this line of research can ultimately benefit end users in terms of productivity, reliability, and accessibility."
    },
    {
      "title": "References",
      "text": "* Brown et al. (2010) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc. * Chowdhery et al. (2020) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunjpa Dev, Henryk Michalewski, Xiaxier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Seta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. _CoRR_, abs/2204.02311. * Da et al. (2021) Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and Antoine Bosselut. 2021. Analyzing commonsense emergence in few-shot knowledge models. In _3rd Conference on Automated Knowledge Base Construction_. * Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics. * Hwang et al. (2021) Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. 2021. (COMET-)ATOMIC 2020: On symbolic and neural commonsense knowledge graphs. _Proceedings of the AAAI Conference on Artificial Intelligence_, 35(7):6384-6392. * Jiang et al. (2021) Liwei Jiang, Antoine Bosselut, Chandra Bhagavatula, and Yejin Choi. 2021. \"I'm not mad\": Commonsense implications of negation and contradiction. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4380-4397, Online. Association for Computational Linguistics. * Kadavath et al. (2022) Saurav Kadavath, Tom Conerly, Amanda Askell, T. J. Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zachary Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, John Kermion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, DarioAmodei, Tom B. Brown, Jack Clark, Nicholas Joseph, Benjamin Mann, Sam McCandlish, Christopher Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. _ArXiv_, abs/2207.05221. * LeCun (2022) Yann LeCun. 2022. A path towards autonomous machine intelligence version 0.9.2. * Rae et al. (2022) Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Raulu, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathriri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angelik Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis and insights from training gopher. _CoRR_, abs/2112.11446. * Rezaei and Reformat (2022) Navid Rezaei and Marek Z. Reformat. 2022. Utilizing language models to expand vision-based common-sense knowledge graphs. _Symmetry_, 14(8). * Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc. * Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_. * West et al. (2022) Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. 2022. Symbolic knowledge distillation: from general language models to commonsense models. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4602-4625, Seattle, United States. Association for Computational Linguistics."
    },
    {
      "title": "Appendix A Verbalizations",
      "text": "The questions are verbalized from triples using predefined formats. Table 3 summarizes the verbalizations organized by relation types. The question template formats are inspired by the sentence format used in Hwang et al. (2021)."
    },
    {
      "title": "Appendix B Human Evaluation Instructions",
      "text": "The following instructions are given to each human evaluator to better understand and respond to the task: Based on your own commonsense, choose one of the five options. Examples are provided in the description. IMPORTANT: Please note the CANNOT, DO Not, and other negated cases. Instruction notes: Based on your own commonsense, choose one of the five options. Examples are provided in the description. IMPORTANT: Please note the CANNOT, DO Not, and other negated cases. 1. Instead of names, PersonX and PersonY are used to be gender-neutral. 2. Please ignore grammatical errors and focus on commonsense. 3. If a response is vague, such as _not fireman_, or if a random word does not fit the scenario, please choose 4 (not enough information). Added to the instructions, we also provided some examples to clarify the task better: **Unfamiliar to me to judge:** PersonX discovers a new planet. The planet is in the Alpha Centauri system. **First part and second part are not related: Or not enough information to judge:** PersonX rides a bike. Elephants are not birds. (Although the second part is correct, it is not related to the first part) **Makes sense:** It is NOT likely to see an elephant on the table. **Does not make sense:** It is likely to see an elephant on the table. The answers are written in a text format to make it easier for human evaluators. Here is an example:. Here is an example: PersonX sends PersonY to the showers. PersonX will not be amused."
    },
    {
      "title": "Appendix C Chatgpt",
      "text": "The experiments of this paper were done using _text-davinci-002_ version of GPT-3. The new version of GPT-3 with reinforcement learning from human feedback (RLHF) is introduced and is called ChatGPT. Out of curiosity, we did limited experiments with ChatGPT to see if the problem of _negated complementary_ questions persists. Here are a few answers from ChatGPT to the question shown in Figure 1: Q: PersonX is wearing red and white clothes and is going down a stranger's home's chimney. Who PersonX cannot be? A: It is likely that PersonX is not Santa Claus, as he is commonly depicted wearing red and white clothes and going down chimneys. As we can see, the model answers with _not Santa Claus_ and then describes Santa Clause. Asking the same question again, we get the following answer: A: PersonX cannot be a real person as it is impossible for someone to physically fit into a chimney. The scenario you described is likely referencing a fictional character, such as Santa Claus, who is often depicted as wearing red and white clothes and entering homes through chimneys on Christmas Eve to deliver presents to children. In this answer, there is a good start of context; however, somewhere within the text, the model forgets about the _cannot_ word and again describes Santa Claus. For the third time, we ask the same question and we get the following answer: A: It is likely that PersonX is Santa Claus, a fictional figure who is often depicted wearing red and white clothes and delivering presents to children through the chimneys of houses. The sentence is technically correct, but it is not what we asked for, which is who _cannot_ be. Therefore, we can consider the last answer incorrect as well. Given the limited interaction with ChatGPT, it seems that the _negated complementary_ task issue persists. \\begin{table} \\begin{tabular}{l l} \\hline \\hline **Relation** & **Question** \\\\ \\hline xWant & [head]. What does PersonX want to do? \\\\ \\cline{2-3} & [head]. What does PersonX not want to do? \\\\ \\hline xReact & [head]. What does PersonX feel about it? \\\\ \\cline{2-3} & [head]. What does PersonX not feel about it? \\\\ \\hline oWant & [head]. What does PersonY want to do? \\\\ \\cline{2-3} & [head]. What does PersonY not want to do? \\\\ \\hline CapableOf & What is [head] capable of? \\\\ \\cline{2-3} & What is [head] not capable of? \\\\ \\hline Desires & What does [head] desire to do? \\\\ \\cline{2-3} & What does [head] not desire to do? \\\\ \\hline HinderedBy & [head]. What can hinder/obstruct it? \\\\ \\cline{2-3} & [head]. What cannot hinder/obstruct it? \\\\ \\hline isBefore & [head]. What happens before it? \\\\ \\cline{2-3} & [head]. What does not happen before it? \\\\ \\hline isAfter & [head]. What happens after it? \\\\ \\cline{2-3} & [head]. What does not happen after it? \\\\ \\hline AtLocation & Where is the [head] located? \\\\ \\cline{2-3} & Where is the [head] not located? \\\\ \\hline HasSubEvent & [head]. What will you do while: [head]? \\\\ \\cline{2-3} & [head]. What you will not do while: [head] \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Question templates for each relation type. The first row for each relation shows the standard question format, and the second row shows the negated complementary format. [head] refers to the head in a triple."
    }
  ]
}