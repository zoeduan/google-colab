{
  "title": "Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback",
  "authors": [
    "Song Yu",
    "Xiaofei Xu",
    "Fangfei Xu",
    "Li Li"
  ],
  "abstract": "\n Although large language models perform well in understanding and responding to user intent, their performance in specialized domains such as Traditional Chinese Medicine (TCM) remains limited due to lack of expertise. In addition, high-quality data related to TCM is scarce and difficult to obtain, making large language models ineffective in handling TCM tasks. In this work, we propose a framework to improve the performance of large language models for TCM tasks using only a small amount of data. First, we use medical case data for supervised fine-tuning of the large model, making it initially capable of performing TCM tasks. Subsequently, we further optimize the model's performance using reinforcement learning from AI feedback (RLAIF) to align it with the preference data. The ablation study also demonstrated the performance gain is attributed to both supervised fine-tuning and the direct policy optimization. The experimental results show that the model trained with a small amount of data achieves a significant performance improvement on a representative TCM task. \n",
  "references": [
    {
      "id": null,
      "title": "Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback",
      "authors": [
        "Song Yu",
        "Xiaofei Xu",
        "Fangfei Xu",
        "Li Li"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Scibert: A pretrained language model for scientific text",
      "authors": [
        "I Beltagy",
        "K Lo",
        "A Cohan"
      ],
      "year": "2019",
      "venue": "Scibert: A pretrained language model for scientific text",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F L Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Z Du",
        "Y Qian",
        "X Liu",
        "M Ding",
        "J Qiu",
        "Z Yang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Qwen technical report",
      "authors": [
        "J Bai",
        "S Bai",
        "Y Chu",
        "Z Cui",
        "K Dang",
        "X Deng",
        "Y Fan",
        "W Ge",
        "Y Han",
        "F Huang"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Towards generalist biomedical ai",
      "authors": [
        "T Tu",
        "S Azizi",
        "D Driess",
        "M Schaekermann",
        "M Amin",
        "P.-C Chang",
        "A Carroll",
        "C Lau",
        "R Tanno",
        "I Ktena",
        "B Mustafa",
        "A Chowdhery",
        "Y Liu",
        "S Kornblith",
        "D Fleet",
        "P Mansfield",
        "S Prakash",
        "R Wong",
        "S Virmani",
        "C Semturs",
        "S S Mahdavi",
        "B Green",
        "E Dominowska",
        "B A Arcas",
        "J Barral",
        "D Webster",
        "G S Corrado",
        "Y Matias",
        "K Singhal",
        "P Florence",
        "A Karthikesalingam",
        "V Natarajan"
      ],
      "year": "2023",
      "venue": "Towards generalist biomedical ai",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Doctorglm: Fine-tuning your chinese doctor is not a herculean task",
      "authors": [
        "H Xiong",
        "S Wang",
        "Y Zhu",
        "Z Zhao",
        "Y Liu",
        "L Huang",
        "Q Wang",
        "D Shen"
      ],
      "year": "2023",
      "venue": "Doctorglm: Fine-tuning your chinese doctor is not a herculean task",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Basic theories of traditional chinese medicine",
      "authors": [
        "F Lozano"
      ],
      "year": "2014",
      "venue": "Basic theories of traditional chinese medicine",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Understanding traditional chinese medicine therapeutics: an overview of the basics and clinical applications",
      "authors": [
        "L C Matos",
        "J P Machado",
        "F J Monteiro",
        "H J Greten"
      ],
      "year": "2021",
      "venue": "Healthcare",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Bloom: A 176bparameter open-access multilingual language model",
      "authors": [
        "T Le Scao",
        "A Fan",
        "C Akiki",
        "E Pavlick",
        "S Ilić",
        "D Hesslow",
        "R Castagné",
        "A S Luccioni",
        "F Yvon",
        "M Gallé"
      ],
      "year": "2023",
      "venue": "Bloom: A 176bparameter open-access multilingual language model",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model",
      "authors": [
        "Deepseek-Ai"
      ],
      "year": "2024",
      "venue": "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Baichuan 2: Open large-scale language models",
      "authors": [
        "A Yang",
        "B Xiao",
        "B Wang",
        "B Zhang",
        "C Bian",
        "C Yin",
        "C Lv",
        "D Pan",
        "D Wang",
        "D Yan"
      ],
      "year": "2023",
      "venue": "Baichuan 2: Open large-scale language models",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
      "authors": [
        "Y Sun",
        "S Wang",
        "S Feng",
        "S Ding",
        "C Pang",
        "J Shang",
        "J Liu",
        "X Chen",
        "Y Zhao",
        "Y Lu"
      ],
      "year": "2021",
      "venue": "Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Deep reinforcement learning from human preferences",
      "authors": [
        "P F Christiano",
        "J Leike",
        "T Brown",
        "M Martic",
        "S Legg",
        "D Amodei"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
      "authors": [
        "H Lee",
        "S Phatale",
        "H Mansoor",
        "K Lu",
        "T Mesnard",
        "C Bishop",
        "V Carbune",
        "A Rastogi"
      ],
      "year": "2023",
      "venue": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "authors": [
        "R Rafailov",
        "A Sharma",
        "E Mitchell",
        "C D Manning",
        "S Ermon",
        "C Finn"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Huatuogpt, towards taming language model to be a doctor",
      "authors": [
        "H Zhang",
        "J Chen",
        "F Jiang",
        "F Yu",
        "Z Chen",
        "J Li",
        "G Chen",
        "X Wu",
        "Z Zhang",
        "Q Xiao"
      ],
      "year": "2023",
      "venue": "Huatuogpt, towards taming language model to be a doctor",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue",
      "authors": [
        "S Yang",
        "H Zhao",
        "S Zhu",
        "G Zhou",
        "H Xu",
        "Y Jia",
        "H Zan"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Shennong-tcm: A traditional chinese medicine large language model",
      "authors": [
        "W Y Wei Zhu",
        "X Wang"
      ],
      "year": "2023",
      "venue": "Shennong-tcm: A traditional chinese medicine large language model",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Textbooks are all you need",
      "authors": [
        "S Gunasekar",
        "Y Zhang",
        "J Aneja",
        "C C T Mendes",
        "A Del Giorno",
        "S Gopi",
        "M Javaheripi",
        "P Kauffmann",
        "G De Rosa",
        "O Saarikivi"
      ],
      "year": "2023",
      "venue": "Textbooks are all you need",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "The probabilistic relevance framework: Bm25 and beyond",
      "authors": [
        "S Robertson",
        "H Zaragoza"
      ],
      "year": "2009",
      "venue": "Foundations and Trends® in Information Retrieval",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Chinesebert: Chinese pretraining enhanced by glyph and pinyin information",
      "authors": [
        "Z Sun",
        "X Li",
        "X Sun",
        "Y Meng",
        "X Ao",
        "Q He",
        "F Wu",
        "J Li"
      ],
      "year": "2021",
      "venue": "Chinesebert: Chinese pretraining enhanced by glyph and pinyin information",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "",
      "authors": [
        "T Glm",
        "A Zeng",
        "B Xu",
        "B Wang",
        "C Zhang",
        "D Yin",
        "D Rojas",
        "G Feng",
        "H Zhao",
        "H Lai",
        "H Yu",
        "H Wang",
        "J Sun",
        "J Zhang",
        "J Cheng",
        "J Gui",
        "J Tang",
        "J Zhang",
        "J Li",
        "L Zhao",
        "L Wu",
        "L Zhong",
        "M Liu",
        "M Huang",
        "P Zhang",
        "Q Zheng",
        "R Lu",
        "S Duan",
        "S Zhang",
        "S Cao",
        "S Yang",
        "W L Tam",
        "W Zhao",
        "X Liu",
        "X Xia",
        "X Zhang",
        "X Gu",
        "X Lv",
        "X Liu",
        "X Liu",
        "X Yang",
        "X Song",
        "X Zhang",
        "Y An",
        "Y Xu",
        "Y Niu",
        "Y Yang",
        "Y Li",
        "Y Bai",
        "Y Dong",
        "Z Qi",
        "Z Wang",
        "Z Yang",
        "Z Du",
        "Z Hou",
        "Z Wang"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models",
      "authors": [
        "D Dai",
        "C Deng",
        "C Zhao",
        "R Xu",
        "H Gao",
        "D Chen",
        "J Li",
        "W Zeng",
        "X Yu",
        "Y Wu"
      ],
      "year": "2024",
      "venue": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E J Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "A survey of zero-shot learning: Settings, methods, and applications",
      "authors": [
        "W Wang",
        "V W Zheng",
        "H Yu",
        "C Miao"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Generalizing from a few examples: A survey on few-shot learning",
      "authors": [
        "Y Wang",
        "Q Yao",
        "J T Kwok",
        "L M Ni"
      ],
      "year": "2020",
      "venue": "ACM computing surveys (csur)",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Language models are few-shot learners",
      "authors": [
        "T B Brown"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "K Papineni",
        "S Roukos",
        "T Ward",
        "W.-J Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Rouge: A package for automatic evaluation of summaries",
      "authors": [
        "C.-Y Lin"
      ],
      "year": "2004",
      "venue": "Text summarization branches out",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Bertscore: Evaluating text generation with bert",
      "authors": [
        "T Zhang",
        "V Kishore",
        "F Wu",
        "K Q Weinberger",
        "Y Artzi"
      ],
      "year": "2019",
      "venue": "Bertscore: Evaluating text generation with bert",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Abstract",
      "text": "Although large language models perform well in understanding and responding to user intent, their performance in specialized domains such as Traditional Chinese Medicine (TCM) remains limited due to lack of expertise. In addition, high-quality data related to TCM is scarce and difficult to obtain, making large language models ineffective in handling TCM tasks. In this work, we propose a framework to improve the performance of large language models for TCM tasks using only a small amount of data. First, we use medical case data for supervised fine-tuning of the large model, making it initially capable of performing TCM tasks. Subsequently, we further optimize the model's performance using reinforcement learning from AI feedback (RLAIF) to align it with the preference data. The ablation study also demonstrated the performance gain is attributed to both supervised fine-tuning and the direct policy optimization. The experimental results show that the model trained with a small amount of data achieves a significant performance improvement on a representative TCM task. Large Language Models, Traditional Chinese Medicine, Direct Preference Optimization, Reinforcement Learning"
    },
    {
      "title": "I Introduction",
      "text": "Language modeling, as an important approach to language understanding and generation, has been extensively studied over the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models down a lot of attention, and by pre-training Transformer models on large-scale corpora, these models have demonstrated their power in solving various natural language processing tasks [1]. Interestingly, when the parameter size exceeds a certain level, these language models not only improve performance, but also show capabilities that are not available in small-scale models. In general, large language models (LLMs) are Transformer-based language models containing billions or more parameters trained on large amounts of textual data, such as OpenAI's ChatGPT [2] and GPT-4 [3], which are capable of understanding and answering a wide range of questions, and their performance on certain tasks even meets or exceeds that of humans. In addition, the open source community has rapidly introduced a series of LLMs, such as ChatGLM [4], LLaMA [5], Qwen [6], which have demonstrated impressive performance. However, the training data for LLMs mainly come from the Internet and books and are mostly common-sense data, thus limiting them in domains such as Traditional Chinese Medicine (TCM). Additionally, high quality TCM data is scarce, and existing large language models perform poorly on TCM tasks. Despite the challenges, TCM LLM has great potential to provide value in answering questions, assisting doctors in diagnosis and prescribing. In the medical domain, several medical language models have been proposed, such as Med-Palm [7], DoctorGLM [8], etc., which show great promise in a variety of medical applications, such as medical question and answer, dialog systems, and text generation. However, almost all of these works focus on modern medicine, with very few addressing TCM. Developing a large model for the TCM domain is challenging. First, most TCM works are written in ancient Chinese, and many terms do not have corresponding explanations in modern Chinese, with large grammatical differences [9]. Additionally, many theories and diagnostic and therapeutic methods in TCM lack uniform quantitative and objective standards and cannot be easily verified. Finally, TCM is not a widely applied discipline, and there is less information on related works, and high-quality data are even more difficult to obtain [10]. To address these limitations, we propose a framework for enhancing the performance of large-model TCM tasks with only a small amount of data. Specifically, we focus on two types of tasks: initial visit and follow-up visit, as shown in Table 1. Our framework consists of three stages. First, we collect a corpus of real medical cases and perform supervised fine-tuning on a open-source large language model, this step will steer the LLM into solving TCM tasks. Second, for each input, we instruct the model to generate multiple outputs to build a preference dataset. Considering the inefficiency and high cost of manual annotation, we introduced a reinforcement learning method based on AI feedback (RLAIF) to train language models using AI-generated feedback instead of human feedback. Finally, we use preference data to instruct the model's learning, enabling it to generate outputs that better align with user expectations. The main contributions of this paper are as follows: * We develop a framework for enhancing the performance of TCM tasks using only a small amount of data, enabling the full process of training from supervised fine-tuning to RLAIF. * We develop an automated labeling and ranking method using generative AI to build high-quality preference datasets instead of manual labor. * We conduct several experiments on test datasets to demonstrate the effectiveness of our proposed framework."
    },
    {
      "title": "Ii Related Work",
      "text": ""
    },
    {
      "title": "_Large Language Models_",
      "text": "In 2022, ChatGPT went live and attracted a lot of attention, triggering a heated discussion about LLM across the community. With the release of GPT-4 (2023) and GPT-4o (2024), the multimodal capability of large models was further improved, which set off a new wave of AI, however, OpenAI did not announce its training strategies and weights due to various factors. As a result, open source LLMs such as LLaMA and BLOOM [11] quickly attracted a lot of attention from the research community once they were released. These models made public their training methods and weighting files, enabling researchers and developers to further train and improve the models on this basis. A large number of open source LLMs have also emerged in China, such as ChatGLM, DeepSeek [12], Owen, Baichuan [13], and ERNIE [14]. These open-source LLMs use rich Chinese corpus for training and optimization. With the proliferation of these models, various optimization techniques have been explored to enhance their performance. Optimizing the performance of large language models involves various techniques, with RLHF (reinforcement learning from human feedback) [15] and RLAIF (reinforcement learning from AI feedback) [16] being two prominent methods. In RLHF, a reward model is trained to learn alignment based on human feedback. Once fine-tuned, this reward model can evaluate different outputs, scoring them according to the alignment preferences specified by humans. This feedback is then used to further refine the original language model. On the other hand, RLAIF involves directly linking a pre-trained, well-aligned model to the language model, allowing it to learn from larger and more aligned models. Research shows that RLAIF performs as well as, or even better than, human feedback (RLHF) in tasks such as text summarization, helpful dialogue generation, and harmless dialogue generation. In a recent study known as Direct Preference Optimization (DPO) [17] highlighted the complexity and instability of RLHF. They proposed an alternative approach by leveraging a mapping between reward functions and optimal policies. This mapping allows the constrained reward maximization problem to be optimized precisely through a single stage of policy training, effectively transforming it into a direct objective optimization based on human preference data. Their algorithm, termed DPO, is noted for its stability, performance, and computational efficiency, eliminating the need for fitting a reward model. They found that DPO surpasses RLHF in controlling sentiment generation and enhancing response quality in summarization."
    },
    {
      "title": "_Large Language Models In Medical_",
      "text": "Currently, some progress has been made in the research of large language models in medical domain, but there are still some limitations and challenges. Due to the special characteristics of Traditional Chinese medicine, the medical domain tuned large language models released worldwide mainly focus on Western medicine, and most of them are in English as the main language, such as Google Med-PaLM series which has some limitations on the discovery and application of knowledge of TCM, and it is difficult to meet the special needs of TCM. In China, research teams have begun to emphasize the development of Chinese medical LLMs, such as Huatuo GPT[18], zhongjing [19], shennong-TCM [20] and DoctorGLM. However, the number of large models for TCM is relatively small compared with those for Western medicine. Most of the current so-called large models for Chinese medicine are not purely focused on traditional Chinese medicine. Instead, they are hybrid models that mix knowledge from Western medicine, Chinese medicine, and other related fields. These efforts may overly pursue breadth at the expense of depth. In addition, the quality of the data used to train those existing Chinese medicine LLMs rely varies, which affects the final results of the models. For example, Shennong used the TCM dataset generated by ChatGPT, but the quality cannot be guaranteed, and most of the tasks are common sense questions and answers rather than prescription tasks; Zhongjing collected a large amount of real-world data, but it was also not focused on TCM prescription tasks. Despite the great potential of Large Language Models (LLMs) in healthcare, there are still some important and specific challenges that need to be addressed. When models are used for general knowledge quizzes, the impact of errors is not fatal to humans; however, this is not the case in the medical domain, where incorrect interpretations and answers can have serious consequences for patients. The accuracy and reliability of the information provided by LLMs can be life threatening, as it may affect medical decisions, diagnosis, and treatment plans. In addition, the definition of responsibility after using a LLM to aid in diagnosis is an issue that needs to be considered. Therefore, we need to ensure the quality of model output as much as possible."
    },
    {
      "title": "Iii Method",
      "text": "This section describes the process of building the framework, which is divided into data construction, supervised fine-tuning, reinforcement learning from AI feedback. Each step is discussed sequentially to reflect the research workflow. The integrated methodology flowchart is shown in Figure 1."
    },
    {
      "title": "_Data Construction_",
      "text": "One of the challenges in training high-performance LLM models for TCM lies in obtaining high-quality data. A high quality corpus can greatly improve the performance of LLMs and even break the scaling law to some extent [21]. The model needs not only theoretical data from TCM textbooks, but also professional data from real doctor-patient scenarios, which can reflect the specific conditions of patients and guide the addition, subtraction and proportion of medicines. In order to ensure the diversity of the medical corpus, we collect a variety of real medical text data from multiple sources, including open source data, proprietary data, and real medical consultations. These data cover most of the domains and symptoms of TCM, providing rich and detailed medical knowledge for the model. In this paper, we will focus on the classical types of prescription tasks. Specifically, there are two types of prescription tasks: first visit and follow-up visit. The first visit task needs to issue a prescription based on the user's symptoms and examination results, while the follow-up visit task needs to synthesize all the previous follow-ups, prescriptions, and feedbacks from the user's medication to give subsequent medication suggestions. Fig. 1: The structure of our proposed framework includes data collection, supervised fine-tuning, automatic labeling, and direct preference optimization. After supervised fine-tuning, the model will generate multiple outputs, which are labeled using automatic labeling to obtain preference data. The model is further optimized using dpo and preference data."
    },
    {
      "title": "_Supervised Fine-Tuning_",
      "text": "Supervised fine-tuning (SFT) is a key stage in empowering the model with dialog capabilities. With the help of high-quality doctor-patient dialog data, the model can effectively invoke the medical knowledge accumulated during the pre-training process to understand and answer the user's query. The goal of SFT is to teach the model how to understand and generate appropriate replies based on TCM diagnostic principles and treatment methods. The SFT process consists of optimizing the model parameters to minimize the cross-entropy loss between the predicted output and the actual output for a given input set. In the SFT process, the model learns to generate prescriptions based on the initial diagnosis and subsequent diagnoses. This process involves providing the model with detailed case information including symptoms, diagnoses, and patient feedback to guide its learning process. The cross-entropy loss used for training the model is defined as follows: \\[L=-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=1}^{T}y_{it}\\log(\\hat{y}_{it}) \\tag{1}\\] where \\(N\\) is the total number of samples, \\(T\\) is the sequence length, \\(y_{it}\\) is the true token at position \\(t\\) for the \\(i\\)-th sample, and \\(\\hat{y}_{it}\\) is the predicted probability of the true token at position \\(t\\) for the \\(i\\)-th sample. This loss function measures the difference between the predicted token distribution and the actual token distribution, and the goal of training is to minimize this loss. The pseudo-code for this phase is shown in Algorithm 1. Note a good prompt helps to stimulate the model's capability, so we designed a prompt to guide the model to generate the specified response, as shown in Table 2. We decide whether to use English or Chinese prompts based on the corpus used in the pre-training stage of the base model, but the output is uniformly specified to be in Chinese. ``` 0: Initial policy \\(\\pi_{\\theta}\\), Training dataset \\(D\\), Number of epochs \\(N\\), Batchsize \\(B\\), Learning rate \\(\\eta\\in[0,1]\\) 0: SFTed policy \\(\\hat{\\pi}_{\\theta}\\) 1:for epoch=1,2,3,...,\\(N\\)do 2:for\\(D_{batch}\\sim D\\)do 3:\\((x,z)\\sim D_{batch}\\) 4:\\(\\hat{z}\\sim\\pi_{\\theta}(x)\\) 5:\\(L=\\text{CrossEntropyLoss}(\\hat{z},z)\\) 6:\\(\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}L\\) //Update the parameters using any gradient descent algorithm 7:endfor 8:endfor ``` **Algorithm 1** Supervised Fine-tuning"
    },
    {
      "title": "_Reinforcement Learning From Ai Feedback_",
      "text": "Despite SFT accumulating medical knowledge and guiding conversational abilities, the model can still produce inaccurate, harmful, or unfriendly responses, which can have serious consequences in medical dialogues. We use RLAIF to improve the conversation process. Specifically, for each question, we guide the supervised fine-tuned model to generate diverse outputs, score them using a specific annotation model, and then rank them using Borda rank. Finally, we align the training using the DPO algorithm."
    },
    {
      "title": "Iv-C1 Ai Feedback For Tcm",
      "text": "Considering the specificity of medical conversations, we obtain feedback in three ways, make metrics from three different aspects of the output, and develop a sorted annotation rule. * **Lexical overlap:** This refers to cases where both the ground truth and the model output contain the same or similar terms. We use BM25, a well-established ranking function used by search engines [22]. The advantage of BM25 lies in its ability to weigh query terms according to their TF-IDF importance, thus providing a reliable measure of term overlap. * **Semantic overlap:** This refers to cases where the ground truth and the model output contain semantically relatedcontent. Even if the model output does not contain the exact terms from the ground truth, they may still be relevant, such as \"yin-yang imbalance\" and \"yang deficiency.\" We use the dense encoder model bert-base-chinese, which works by encoding ground truth and model output as dense vectors in a shared embedding space, respectively [23]. It focuses on deeper semantic connections between words and phrases. The dot product of the two vectors then gives a measure of semantic similarity. * **Model annotation:** The above two similarity measurement methods may be biased in TCM. Therefore, we also use DeepSeek-V2 and GPT-4o as annotation models, guiding them to evaluate the generated dialogues from the perspectives of completeness, professionalism, and fluency. Annotation questions come from the test set, and we provide both the questions and the standard answers to the annotation models. The annotation models will score the generated dialogues on a scale from 0 to 100. To mitigate the differences caused by the scoring distribution of different models, ranking is used as a means of introducing regularization supervision signals. Specifically, when multiple rankings of candidate outputs are provided, we use a voting method, Borda Rank, to merge them into a unified ranking. For example, given \\(N\\) items and \\(M\\) individual rankings, the Borda ranking score \\(B_{i}\\) of item \\(i\\) is calculated as follow: \\[B_{i}=\\Sigma_{j=1}^{M}(N-rank_{j}(i)) \\tag{2}\\] where \\(rank_{j}(i)\\) represents the ranking of item \\(i\\) in the \\(j\\)-th individual ranking. Based on the ranking results, we can construct binary tuples containing positive samples and negative samples \\((s^{+},s^{-})\\) for the DPO training process."
    },
    {
      "title": "Iii-B2 Reinforcement Learning",
      "text": "Finally, we use the labeled ranking data and DPO to further optimize the model, as shown in Figure 2. Each training sample in our dataset consists of a conversation between a doctor and a patient, a chosen response, and a rejected response generated by the model. Given the dataset, we extract the dialogue, chosen response, and rejected response for each sample. The DPO loss function is defined as follows: \\[loss=-\\log\\left(\\sigma\\left(\\beta\\cdot\\left((\\pi_{\\text{chosen}}-\\pi_{\\text{ rejected}})-\\frac{\\gamma}{\\beta}\\right)\\right)\\right) \\tag{3}\\] where \\(\\pi_{\\text{chosen}}\\) is the log probability of the chosen response, and \\(\\pi_{\\text{ rejected}}\\) is the log probability of the rejected response. This ratio measures how much more likely the model is to generate the chosen response compared to the rejected one. The hyperparameter \\(\\gamma\\) and \\(\\beta\\) controls the adjustment applied to the log probability difference, while scales the entire loss function. The pseudo-code for this phase is shown in Algorithm 2. ``` 0: SFTed policy \\(\\hat{\\pi}_{\\theta}\\), Traning dataset \\(\\hat{D}\\), Number of epochs \\(N\\), Batchsize \\(B\\), Learning rate \\(\\eta\\in[0,1]\\), sample size \\(k\\), Initial \\(M=\\emptyset\\) 0: Trained policy \\(\\hat{\\pi}_{\\theta}\\) 1:for\\((x,z)\\in\\hat{D}\\)do 2:\\(i=0\\) 3:while\\(i<k\\)do 4:\\(M=M\\cap\\hat{\\pi}_{\\theta}(x)\\) 5:endwhile 6:endfor 7:\\(\\widetilde{D}=\\text{AutoAnnotation}(M)\\) //Get dpo training data 8:for epoch=1,2,3,...,\\(N\\)do 9:for\\(\\widetilde{D}_{batch}\\sim\\widetilde{D}\\)do 10:\\((s^{+},s^{-},z)\\sim\\widetilde{D}_{batch}\\) 11:\\(\\hat{z}\\sim\\hat{\\pi}_{\\theta}(s^{+},s^{-})\\) 12:\\(L=\\text{SigmoidLoss}(\\hat{z},z)\\) 13:\\(\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}L\\) //Update the parameters using any gradient descent algorithm 14:endfor 15:endfor ``` **Algorithm 2** Direct Preference Optimization"
    },
    {
      "title": "Iv Experiments And Evaluation",
      "text": ""
    },
    {
      "title": "_Training Details_",
      "text": "We use four widely used open-source models as our base models, GLM-4-9B-Chat, Llama-3-8B-instruct [24], Qwen2-7B-chat, DeepseekMOE-16B [25]. GLM-4-9B-Chat is the open-source version of the latest generation of LLms in the GLM-4 family from Smart Spectrum AI, and has been evaluated on a variety of benchmarks in semantics, math, reasoning, code and knowledge extraction, demonstrated superior performance against previous generation of models. LLaMA3 is a LLM developed by Meta, optimized for a wide range of conversational use cases, outperforming many open-source models on common industry benchmarks. Qwen2 is a series of open source large models of Tongyi Qianqian developed by Aliyun. The series provides multiple versions and scales of open source models, such as Base and Instruct, so as to meet different computing needs. DeepSeekMoE 16B is a Mixture-of-Experts (MoE) language model with 16.4B parameters. It employs an innovative MoE architecture, which involves two principal strategies: fine-grained expert segmentation and shared experts isolation. Training was performed on 1 NVIDIA Tesla A40(48GB) using a low-rank adaptive (lora) parameter efficient tuning Fig. 2: Increasing the log probability of a preferred sample versus decreasing the log probability of a non-preferred sample response, the model’s policy tends to select the preferred sample. method [26]. We trained using bf16 (Deepseek for fp32) precision with learning rate = 5e-5, batchsize = 2, gradient accumulation = 8, maximum length = 1024, dropout = 0.1, and a cosine learning rate scheduler. In the stage of indicating the model produces diversified outputs, we set the number of samples k = 3 and the temperature = 1.2. The statistical information of the dataset is presented in the following Table 3."
    },
    {
      "title": "_Baseline_",
      "text": "In order to fully evaluate our method, we chose a series of LLMs with different training path and training data as baselines for comparison. * **Zero-shot [27]:** The model is given a task without any prior examples. This approach tests the model's ability to generalize from its training data to new, unseen scenarios. For each model, the zero-shot prompt is carefully crafted to be clear and concise, providing only the necessary context and the query. * **Few-shot [28]:** The model is provided with a few examples to learn from before it is asked to perform the task. This method helps the model understand the task better by seeing similar instances. The few-shot prompts are designed to include a small number of examples (2 in our experiment) before presenting the actual query. * **SFT [29]:** Supervised fine-tuning will provide medical cases with their corresponding standard diagnoses for the model to learn. * **SFT+DPO:** After warming up using supervised fine-tuning, the model is guided to generate multiple outputs, and the data is labeled using an automated labeling system which in turn generates preference data for the dpo training process. Zero-shot learning is an important method for evaluating the generalization ability of language models without any task-specific data. For the field of TCM, zero-shot testing can demonstrate the adaptability of the model to unseen TCM tasks, especially when data is scarce. This method can highlight the basic language understanding and reasoning ability of the model, and provide a lower bound on the performance of tasks in this field. Few-shot learning provides the ability of the model to learn quickly from a small number of examples. Through training with a few samples, we can evaluate whether the model can effectively reason and generate accurate results under the condition of limited labeled data. SFT is a standard method for model fine-tuning. For the field of TCM, supervised fine-tuning of the model using existing medical case data can improve the performance of the model on specific tasks. In the baseline comparison, supervised fine-tuning provides the upper limit of the performance that the model can achieve after using domain-specific data. The SFT+DPO method combines supervised fine-tuning and preference optimization to improve the quality of model generation while reducing bias. By using an automated annotation system to generate preference data, DPO can help the model learn outputs that are more in line with the doctor's preferences."
    },
    {
      "title": "_Evaluation Metrics_",
      "text": "The assessment of the quality of medical dialog is a multi-faceted task. In order to comprehensively assess the quality of medical dialog, we used Bleu [30], Rough [31], and bert-score [32] as evaluation metrics. For the same problem, we sampled three outputs from the model and calculated the average metric of these three outputs for error reduction and their standard deviation."
    },
    {
      "title": "_Results_",
      "text": "As shown in Tables 4 and 5, the experimental results comprehensively evaluate the performance metrics of various models and methods applied to the proposed TCM task. The primary performance metrics include ROUGE-1, ROUGE-2, ROUGE-L, BLEU-4, precision, recall, and BERT-Score F1. The zero-shot results demonstrate the initial capabilities of the models, generating responses based solely on pre-trained knowledge without any prior examples. In this scenario, all models performed poorly as they had not learned how to prescribe correctly based on medical cases during the pre-training phase. Notably, LLaMA3 performed significantly worse than other models, possibly due to LLaMA being more extensively trained on English corpora, while TCM tasks likely require a higher level of proficiency in Chinese. After adopting the few-shot method, the performance of all models improved to varying degrees, highlighting their ability to learn and adapt from a small number of instances, thereby enhancing their response quality. It is worth noting that GPT-3.5-turbo showed the most significant improvement, indicating its strong ability to follow instructions. The SFT stage embedded more specific TCM knowledge and prescription tasks into the models, enabling them to generate more accurate and contextually appropriate responses, significantly improving performance across all models at this stage. The DPO stage refined the model outputs using preference data, aligning them more closely with human preferences and expectations, achieving superior overall performance. For certain models, such as LLaMA3, the bias in outputs was reduced post-DPO, indicating the effectiveness of aligning the work with preference data, leading to more stable and intention-aligned outputs. Despite GPT-3.5-turbo's excellent performance, models enhanced through SFT and DPO exhibited superior results, underscoring the effectiveness of our proposed framework. Considering the difficulty of obtaining high-quality TCM data,achieving such results with a small amount of data is gratifying. Additionally, our framework can be applied to any large foundational model without any modifications. Our research results validate the effectiveness of the proposed framework, particularly the integration of SFT and DPO in improving model performance. The models can generate accurate and relevant medical responses while ensuring alignment with expert preferences, laying the foundation for our next steps in research."
    },
    {
      "title": "_Case Study_",
      "text": "To illustrate the effectiveness of our proposed framework, we conducted a detailed case study focusing on its application in Traditional Chinese Medicine (TCM) diagnosis and prescription tasks. This case study demonstrates the model's capability to handle both initial and follow-up diagnoses effectively, showcasing its practical utility in real-world medical scenarios. We selected a patient case from our corpus, which includes both initial and follow-up visits. The case details are as follows:"
    },
    {
      "title": "Iv-E1 Initial Visit",
      "text": "November 6, 2022. Patient: Liu, Female, 71 years old. Chief Complaint: Dizziness for over a year. Present Illness History: The patient started experiencing dizziness a year ago, occurring when lying flat and turning her head to the left at night, as well as when looking upward during the day. This dizziness is accompanied by visual rotation, occasional nausea and vomiting, a feeling of heaviness and fatigue in the limbs, irritability, normal appetite and sleep, a slightly bitter taste in the mouth, and regular bowel movements and urination. Tongue: Pale tongue with yellow greasy coating. Pulse: Slippery."
    },
    {
      "title": "Iv-E2 Follow-Up Visit",
      "text": "November 13, 2022. Symptoms: After taking the previous prescription, the patient's dizziness significantly reduced. She experienced slight dizziness when getting up in the morning but had almost no dizziness when turning her head to the left. The patient felt physically strong, not irritable, had a normal appetite and sleep, a neutral taste in her mouth, and regular bowel movements and urination. Her tongue was red with a yellow greasy coating, and her pulse was wiry, tight, and rapid."
    },
    {
      "title": "Iv-E3 Model Application",
      "text": "We instructed the model to make a corresponding diagnosis based on the patient's symptoms. * **Step 1: Initial Diagnosis and Prescription** During the initial diagnosis, the model generated the following prescription based on the given patient information: Traditional Chinese Medicine Diagnosis: Vertigo, hyperactivity of liver yang, and phlegm-dampness obstruction in the middle burner. Treatment Principle: Pacify the liver and subdue yang, transform phlegm and dampness. Prescription: Modified Tianma Gouteng Decoction combined with Yidou Powder. Ingredients: Gastrodia (Tianma) 10g, Uncaria (Gouteng) 15g, Raw Dragon Bone (Sheng Longgu) 30g (decoct first), Chuanxiong (Ligusticum Chuanxiong) 10g, Sparganium (Sandeng) 10g, Curcumin Zedoaria (Ezhu) 10g, Angelica Sinensis (Danggui) 15g, Raw Astragalus (Sheng Huangqi) 30g, Leonurus (Yimucao) 30g, White Peony (Baisha20) 20g, Ziziphus Jujuba Seed (Zao Ren) 20g, Poria (Fuling) 15g, Polygala (Yuanzhi) 10g, Acorus Tatarinowii (Shichangpu) 10g, Prepared Pinellia (Qing Banxia) 10g, Dried Tangerine Peel (Chenpi) 10g, Bamboo Shavings (Zhuru) 10g, Honey-fried Licorice (Zhigancao) 10g, Amomum (Doukou) 10g, Raw Coix Seed (Sheng Yiyirren) 30g. Dosage: 7 doses, to be decocted in water and taken once daily, divided into morning and evening doses. * **Step 2: Follow-up Diagnosis and Prescription** For the follow-up visit, the model synthesized the previous prescriptions and patient feedback to generate a new prescription: Follow-up Prescription: The previous prescription is continued with the addition of Chrysanthemum (Juhua) 10g, for another 7 doses. We evaluated the generated prescriptions using automated annotation and confirmed the relevance and accuracy of the prescriptions based on traditional medical principles. The model's ability to adapt to patient feedback and effectively adjust treatment plans demonstrates its potential for practical medical applications. We are well aware that manual evaluation will provide a better insight of how the proposed framework performed in real-world application, but due to issues such as time and cost, we use automated evaluation instead of manual evaluation in this work."
    },
    {
      "title": "Iv-B4 Insights And Impact",
      "text": "* **Accuracy**: The model accurately diagnosed and prescribed treatment based on TCM principles, reflecting a deep understanding of the medical corpus it was trained on. * **Adaptability**: The model effectively handled follow-up visits, adjusting prescriptions based on patient feedback and progress, similar to a human practitioner. * **Efficiency**: Automated annotations significantly reduced the need for manual labeling, streamlining the process of preference data collection and enhancing model performance. By integrating supervised fine-tuning with automated annotation and direct preference optimization, our framework not only improves model performance but also ensures that the generated outputs align closely with expert knowledge and user preferences. This approach offers a scalable and efficient solution for enhancing large language models in specialized domains like Traditional Chinese Medicine."
    },
    {
      "title": "V Conclusion And Limitations",
      "text": "In this paper, we propose a framework that combines supervised fine-tuning and direct preference optimization to improve the performance of large language models for Traditional Chinese Medicine tasks. Our proposed approach addresses the unique challenges faced by TCM, such as the scarcity of high-quality data and the expertise required for accurate medical applications. By utilizing a small but high-quality corpus of TCM and incorporating an automated annotation process, we are able to significantly improve the model's ability to generate accurate and relevant medical prescriptions. Experimental results show that our framework outperforms existing models, including widely used LLMs such as GPT-3.5-turbo, on various evaluation metrics such as ROUGE, BLEU, and BERT-Score. Our case study further illustrates the practical applicability of the framework in real TCM consultation scenarios, demonstrating the model's ability to effectively handle both initial and follow-up consultations. Automatic annotation proved to be efficient, reducing the need for manual annotation while maintaining high accuracy of the model output. Despite the promising results, there are limitations to our approach. The reliance on small datasets, while demonstrating the efficiency of the framework, also highlights the potential advantages of larger and more diverse corpora. In addition, our task is limited to the TCM prescription task only, and the expert annotation is not as high quality as the manual annotation. In conclusion, our framework provides a scalable and efficient solution for augmenting large language models in specialized fields such as TCM, paving the way for future research and development in integrating AI with TCM practices. Scaling up the size of the dataset, introducing expert labeling and developing new tasks are our future work. In addition, high-quality datasets are difficult to obtain, and due to the particularity of traditional Chinese medicine, datasets are almost all Chinese corpus. Appropriate addition of English corpus for mixed training can fully tap the potential of the model."
    },
    {
      "title": "Acknowledgment",
      "text": "The authors thank anonymous reviewers for their insightful comments. This research was partially supported by grants from the National Natural Science Foundation of China(No.61877051)."
    },
    {
      "title": "References",
      "text": "* [1]I. Beltagy, K. Lo, and A. Cohan (2019) Scibert: a pretrained language model for scientific text. arXiv preprint arXiv:1903.10676. Cited by: SSI, SSII-A. * [2]L. Coates, J. P. Machado, F. J. Monteiro, and H. J. Greten (2021) Understanding traditional Chinese medicine therapeutics: an overview of the basics and clinical applications. In Healthcare, Vol. 9, pp. 257. Cited by: SSI. * [3]J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Alenschmidt, S. Altman, S. Anaddat, et al. (2023) Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SSI. * [4]J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Alenschmidt, S. Altman, S. Anaddat, et al. (2023) Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SSI. * [5]J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Alenschmidt, S. Altman, S. Anaddat, et al. (2023) Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SSI. * [6]T. B. Brown (2020) Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Cited by: SSII-A. * [7]T. G.M. A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Rojas, G. Feng, H. Zhao, H. Lai, H. Yu, H. Wang, J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang, J. Li, L. Zhao, L. Wu, L. Zhong, M. Liu, H. Huang, P. Zhang, Q. Zheng, R. Lu, S. Duan, S. Zhang, S. Cao, S. Yang, W. L. Tam, W. Zhao, X. Liu, X. Xia, X. Zhang, X. Gu, X. Lv, X. Liu, X. Liu, X. Yang, X. Song, X. Zhang, Y. An, Y. Xu, Y. Niu, Y. Yang, Y. Li, Y. Bai, Y. Dong, Z. Qi, Z. Wang, Z. Yang, Z. Du, Z. Hou, and Z. Wang (2024) Chatelm? a family of large language models from glm-130b to glm-4 all tools. Cited by: SSII-A. * [8]T. G.M. A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Rojas, G. Feng, H. Zhao, H. Lai, H. Yu, H. Wang, J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang, J. Li, L. Zhao, L. Wu, L. Zhong, M. Liu, H. Huang, P. Zhang, Q. Zheng, R. Lu, S. Duan, S. Zhang, S. Cao, S. Yang, W. L. Tam, W. Zhao, X. Liu, X. Xia, X. Zhang, X. Gu, X. Lv, X. Liu, X. Liu, X. Yang, X. Song, X. Zhang, Y. An, Y. Xu, Y. Niu, Y. Yang, Y. Li, Y. Bai, Y. Dong, Z. Qi, Z. Wang, Z. Yang, Z. Du, Z. Hou, and Z. Wang (2024) Chatelm? a family of large language models from glm-130b to glm-4 all tools. Cited by: SSII-A. [MISSING_PAGE_POST]"
    }
  ]
}