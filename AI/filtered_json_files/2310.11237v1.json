{
  "title": "Watermarking LLMs with Weight Quantization",
  "authors": [
    "Linyang Li",
    "Botian Jiang",
    "Pengyu Wang",
    "Ke Ren",
    "Hang Yan",
    "Xipeng Qiu",
    "Backdoor Watermark"
  ],
  "abstract": "\n Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of opensource large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications. 1   \n",
  "references": [
    {
      "id": null,
      "title": "Watermarking LLMs with Weight Quantization",
      "authors": [
        "Linyang Li",
        "Botian Jiang",
        "Pengyu Wang",
        "Ke Ren",
        "Hang Yan",
        "Xipeng Qiu",
        "Backdoor Watermark"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Real or fake? learning to discriminate machine from human generated text",
      "authors": [
        "Anton Bakhtin",
        "Sam Gross",
        "Myle Ott",
        "Yuntian Deng",
        "Marc'aurelio Ranzato",
        "Arthur D Szlam"
      ],
      "year": "2019",
      "venue": "Real or fake? learning to discriminate machine from human generated text",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata",
      "authors": [
        "Sid Black",
        "Leo Gao",
        "Phil Wang",
        "Connor Leahy",
        "Stella Biderman"
      ],
      "year": "2021",
      "venue": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata",
      "doi": "10.5281/zenodo.5297715"
    },
    {
      "id": "b2",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "A statistical framework for low-bitwidth training of deep neural networks",
      "authors": [
        "Jianfei Chen",
        "Yu Gai",
        "Zhewei Yao",
        "Michael W Mahoney",
        "Joseph E Gonzalez"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Towards nearimperceptible steganographic text",
      "authors": [
        "Falcon Dai",
        "Zheng Cai"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1422"
    },
    {
      "id": "b5",
      "title": "-bit matrix multiplication for transformers at scale",
      "authors": [
        "Tim Dettmers",
        "Mike Lewis",
        "Younes Belkada",
        "Luke Zettlemoyer"
      ],
      "year": "2022",
      "venue": "-bit matrix multiplication for transformers at scale",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Hierarchical neural story generation",
      "authors": [
        "Angela Fan",
        "Mike Lewis",
        "Yann Dauphin"
      ],
      "year": "2018",
      "venue": "Hierarchical neural story generation",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Generating steganographic text with LSTMs",
      "authors": [
        "Tina Fang",
        "Martin Jaggi",
        "Katerina Argyraki"
      ],
      "year": "2017",
      "venue": "Proceedings of ACL 2017, Student Research Workshop",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Protecting intellectual property of language generation apis with lexical watermark",
      "authors": [
        "Xuanli He",
        "Qiongkai Xu",
        "Lingjuan Lyu",
        "Fangzhao Wu",
        "Chenguang Wang"
      ],
      "year": "",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "CATER: Intellectual property protection on text generation APIs via conditional watermarks",
      "authors": [
        "Xuanli He",
        "Qiongkai Xu",
        "Yi Zeng",
        "Lingjuan Lyu",
        "Fangzhao Wu",
        "Jiwei Li",
        "Ruoxi Jia"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "PubMedQA: A dataset for biomedical research question answering",
      "authors": [
        "Qiao Jin",
        "Bhuwan Dhingra",
        "Zhengping Liu",
        "William Cohen",
        "Xinghua Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1259"
    },
    {
      "id": "b11",
      "title": "A watermark for large language models",
      "authors": [
        "John Kirchenbauer",
        "Jonas Geiping",
        "Yuxin Wen",
        "Jonathan Katz",
        "Ian Miers",
        "Tom Goldstein"
      ],
      "year": "2023",
      "venue": "A watermark for large language models",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Weight poisoning attacks on pretrained models",
      "authors": [
        "Keita Kurita",
        "Paul Michel",
        "Graham Neubig"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.249"
    },
    {
      "id": "b13",
      "title": "Backdoor attacks on pre-trained models by layerwise weight poisoning",
      "authors": [
        "Linyang Li",
        "Demin Song",
        "Xiaonan Li",
        "Jiehang Zeng",
        "Ruotian Ma",
        "Xipeng Qiu"
      ],
      "year": "2021",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Tianxiang Sun, and Xipeng Qiu. 2023. Origin tracing and detecting of llms",
      "authors": [
        "Linyang Li",
        "Pengyu Wang",
        "Ke Ren"
      ],
      "year": "",
      "venue": "Tianxiang Sun, and Xipeng Qiu. 2023. Origin tracing and detecting of llms",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Towards fully 8-bit integer inference for the transformer model",
      "authors": [
        "Ye Lin",
        "Yanyang Li",
        "Tengbo Liu",
        "Tong Xiao",
        "Tongran Liu",
        "Jingbo Zhu"
      ],
      "year": "2020",
      "venue": "Towards fully 8-bit integer inference for the transformer model",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
      "authors": [
        "Eric Mitchell",
        "Yoonho Lee",
        "Alexander Khazatsky",
        "Christopher D Manning",
        "Chelsea Finn"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "Alec Radford",
        "Karthik Narasimhan"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Squad: 100,000+ questions for machine comprehension of text",
      "authors": [
        "Pranav Rajpurkar",
        "Jian Zhang",
        "Konstantin Lopyrev",
        "Percy Liang"
      ],
      "year": "2016",
      "venue": "Squad: 100,000+ questions for machine comprehension of text",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Q-bert: Hessian based ultra low precision quantization of bert",
      "authors": [
        "Sheng Shen",
        "Zhen Dong",
        "Jiayu Ye",
        "Linjian Ma",
        "Zhewei Yao",
        "Amir Gholami",
        "Michael W Mahoney",
        "Kurt Keutzer"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Armand Aur'elien Rodriguez",
        "Joulin"
      ],
      "year": "",
      "venue": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Authorship attribution for neural text generation",
      "authors": [
        "Adaku Uchendu",
        "Thai Le",
        "Kai Shu",
        "Dongwon Lee"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.673"
    },
    {
      "id": "b22",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Rémi Louf",
        "Morgan Funtowicz",
        "Joe Davison",
        "Sam Shleifer",
        "Clara Patrick Von Platen",
        "Yacine Ma",
        "Julien Jernite",
        "Canwen Plu",
        "Teven Xu",
        "Sylvain Le Scao",
        "Mariama Gugger",
        "Quentin Drame",
        "Alexander M Lhoest",
        "Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Q8bert: Quantized 8bit bert",
      "authors": [
        "Ofir Zafrir",
        "Guy Boudoukh",
        "Peter Izsak",
        "Moshe Wasserblat"
      ],
      "year": "2019",
      "venue": "2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Defending against neural fake news",
      "authors": [
        "Rowan Zellers",
        "Ari Holtzman",
        "Hannah Rashkin",
        "Yonatan Bisk",
        "Ali Farhadi",
        "Franziska Roesner",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Red alarm for pretrained models: Universal vulnerability to neuronlevel backdoor attacks",
      "authors": [
        "Zhengyan Zhang",
        "Guangxuan Xiao",
        "Yongwei Li",
        "Tian Lv",
        "Fanchao Qi",
        "Zhiyuan Liu",
        "Yasheng Wang",
        "Xin Jiang",
        "Maosong Sun"
      ],
      "year": "2023",
      "venue": "Machine Intelligence Research",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Neural linguistic steganography",
      "authors": [
        "Zachary Ziegler",
        "Yuntian Deng",
        "Alexander Rush"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1115"
    }
  ],
  "sections": [
    {
      "title": "Watermarking Llms With Weight Quantization",
      "text": "Linyang Li\\({}^{*}\\)\\({}^{123}\\), Botian Jiang\\({}^{12}\\), Pengyu Wang\\({}^{12}\\), Ke Ren\\({}^{12}\\), Hang Yan\\({}^{3}\\), Xipeng Qiu\\({}^{12}\\) \\({}^{1}\\)School of Computer Science, Fudan University \\({}^{2}\\)Shanghai Key Laboratory of Intelligent Information Processing, Fudan University \\({}^{3}\\)Shanghai AI Laboratory {btjiang23, pywang22, kren22}@m.fudan.edu.cn yanhang@pjlab.org.cn {linyangli19, xpqiu}@fudan.edu.cn Equal Contribution.Corresponding author."
    },
    {
      "title": "Abstract",
      "text": "Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications. 1 Footnote 1: We release all data at [https://github.com/Twilight92z/Quantize-Watermark](https://github.com/Twilight92z/Quantize-Watermark)"
    },
    {
      "title": "1 Introduction",
      "text": "Large language models (LLMs), exemplified by ChatGPT, GPT-4 Brown et al. (2020); OpenAI (2023) from the GPT family Radford et al. (2018), are capable of writing documents, and providing solutions for real-world questions at human-like standards. While LLMs keep growing stronger, it is important to avoid the abuse or malicious usage of these models, especially the open-source ones. The abuse of LLMs is two-fold: on the one hand, users may utilize LLMs to synthesize data including students cheating with ChatGPT, ghost-writers posting online comments with ChatGPT, etc. Mitchell et al. (2023); on the other hand, open-source model weights might spread with malicious usage or violation of open-source licenses. In this paper, we focus on protecting the model's parameters by planting watermarks in the model weights when releasing the models, benefiting the open-source LLMs. Previous model-weight watermarking methods concern mostly weight-poisoning as backdoors Kurita et al. (2020); Li et al. (2021); Zhang et al. (2023), requiring pre-assigned triggers which are less applicable in generative large language models. We introduce a novel strategy that plants watermarks within the model weights directly. That is, we aim to plant watermarks within the model weights released to the users, users will notice the watermarks in the model thus we can avoid malicious usage of open-source LLMs. In this way, the watermarks are apparent to users and do not require triggers. Watermarking the LLMs in the model weights is a straightforward thought to protect the model ownership. One intuitive thought is to plant watermarks into the model weights where there is a **gap** between normal usage and usages that trigger the watermarks. As LLMs are often used in both full-precision mode and quantized modes such as INT8 or INT4 Dettmers et al. (2022), in the quantization process, the gap between the quantized model weights and the original weights is a plausible space for watermark injection since the quantization process is constantly applied by various users. As seen in Figure 1, we hope to inject watermarks into the full-precision model and Figure 1: Watermarking an arduously trained LLM so that only the quantized model can predict normally. Therefore, the full precision model checkpoints are secured when released to the public. provide a simplified version that is quantized to low precision such as INT8 or INT4. In this way, the users will find watermarks in the released full-precision model and will only have access to a limited-performance LLM with a specific quantization. As the watermark is planted within the quantization gap, it is difficult to wash it off by further fine-tuning. Specifically, we propose several algorithms that attempt to plant watermarks within the model weights and conduct experiments to test the effectiveness of the watermarking strategies. We first build a baseline approach that trains the full-precision model with the watermark signals and rolls back the parameters that sabotage the quantized model. Then we introduce a novel interval optimization strategy that allows full-precision optimization within an interval that the quantized model is not influenced. Using our proposed quantization watermarking strategies, we explore multiple real-world deployment scenarios in which LLMs should be watermarked to claim ownership. Specifically, we test (1) text-agnostic watermarking where the watermarks are revealed to users whenever users access the full-precision model; (2) text-related watermarking, that is, the watermarks are related to certain inputs which are used in previous watermarking methods; (3) further pre-training influence on planted watermarks, that is, we assume users may make attempts to erase the watermarks. Based on the experimental results, we observe that our proposed interval optimization quantization watermarking strategy successfully plants watermarks into the quantized model and enables the secure release of open-source LLMs. Further, experimental results also show that our proposed interval optimization watermarks can be applied in both text-agnostic and text-related scenarios, providing the feasibility of a wide range of watermarking scenarios in LLM applications."
    },
    {
      "title": "2 Related Work",
      "text": "Watermarking LLMs involves various aspects of security problems in LLM applications, resulting in works with various strategies. **Model Watermarking and Backdoors** Watermarking neural networks (Fang et al., 2017; Ziegler et al., 2019; Dai and Cai, 2019; He et al., 2022, 2020) is a trending topic especially with LLMs fastly developing (Kirchenbauer et al., 2023). In model watermarking, one line of work is to plant pre-defined triggers (Kurita et al., 2020; Li et al., 2021; Zhang et al., 2023) as backdoors, which can be used as watermarks in pre-trained models. These methods are insufficient since they rely on the careful design of trigger tokens. Recent works (Kirchenbauer et al., 2023) consider planting watermarks in the decoding strategies since LLMs are the most widely used NLP models (OpenAI, 2023; Brown et al., 2020). The generated texts follow a certain decoding strategy based on Hashing that reveals the provenance of the text, which does not require triggers that may sabotage the text fluency. Compared with watermarking in model weights, planting watermarks in the decoding process is less convenient since most LLM users adopt frameworks exemplified by Huggingface Transformers (Wolf et al., 2020) where appointing different model weights with the same model structure and decoding process is the most common solution. **AI-Generated Text Detection** There is a close relationship between watermarking LLMs and its counterpart, AI-generated text detection: AI-generated text detection aims to discriminate whether a given text is from an AI (Zellers et al., 2019; Bakhtin et al., 2019; Uchendu et al., 2020; Mitchell et al., 2023), while origin tracing (Li et al., 2023) is to further discriminate which specific model. Watermark detection is to detect the watermark planted in the model or in the model-generated texts, which is similar to AI-generated text detection and often studied simultaneously (Mitchell et al., 2023). **Quantization of Neural Networks** In this paper, we hope to utilize model quantization in watermarking LLMs. Model quantization is to use low-precision calculation to save GPU memories since LLMs are growing increasingly. The 8-bit quantization method is to use INT8 precision to replace fp32 precision during inference, which has been widely explored (Chen et al., 2020; Lin et al., 2020; Zafrir et al., 2019; Shen et al., 2020; Dettmers et al., 2022). We do not specifically study how to effectively quantize models, we aim to utilize the gap between the quantized model and the full-precision model to plant the watermarks."
    },
    {
      "title": "3 Method",
      "text": ""
    },
    {
      "title": "Quantization And De-Quantization Process",
      "text": "In model quantization of transformers-based models, the most widely adopted quantization approach is the 8-bit Matrix Multiplication method Dettmers et al. (2022) that introduces a vector-wise quantization method and quantizes model parameters in mixed precision. Formally, we define the quantization process that quantizes the original full-precision model with parameter \\(\\theta_{0}\\) to the quantized model with parameter \\(\\theta_{0}^{*}\\): \\[\\theta_{0}^{*}=\\mathbf{Q}(\\theta_{0}) \\tag{1}\\] . For parameter \\(\\theta_{0}\\), for instance, given a weight matrix \\(W\\in\\mathbb{R}^{m\\star n}\\) the scale index \\(C_{W}\\) is the maximum number in the row with \\(m\\) parameters, and the quantized weight matrix \\(W_{\\text{INTS}}=W*(127/C_{w})\\). Accordingly, the input \\(X\\) is quantized in the same way, with the scale index set to the maximum number in the column order. In the de-quantization process that converts quantized model parameters \\(\\theta_{0}^{*}\\) back to full-precision parameters, we define the de-quantization process as \\(\\mathbf{D}(\\theta_{0}^{*})\\), the de-quantized model parameter is: \\[\\widehat{\\theta_{0}}=\\mathbf{D}(\\theta_{0}^{*}) \\tag{2}\\] . Similarly, the de-quantized weight, for instance, given a weight matrix \\(\\widehat{W}=W_{\\text{INTS}}*(C_{w}/127)\\) while \\(C_{w}\\) is the scale index calculated during the quantization process \\(\\mathbf{Q}(\\cdot)\\). The de-quantized model \\(\\widehat{\\theta_{0}}\\) is different from the full-precision model \\(\\theta_{0}\\), therefore, once the watermark is planted into the full-precision model, it is not possible to use the quantized model to recover the original full-precision model without watermarks."
    },
    {
      "title": "Planting Watermarks",
      "text": "We define the watermarking process that plants watermarks into the original full-precision model with parameter \\(\\theta_{0}\\) as \\(\\theta=\\mathcal{W}(\\theta_{0})\\). Here, the model \\(\\theta\\) is the model planted with our designed watermarks. After planting the watermarks, we hope that the quantized model of \\(\\theta\\) is not influenced, that is, we have: \\[\\theta^{*}=\\theta_{0}^{*} \\tag{3}\\] Supposing that the watermark is \\(y^{\\mathcal{W}}\\), when the watermark is shown regardless of the input \\(x\\), for any input text \\(x\\) with its normal output \\(y\\), with an LLM generation process \\(f(\\cdot)\\), we have: \\[y^{\\mathcal{W}}=f(x,\\theta) \\tag{4}\\] \\[y=f(x,\\theta^{*}) \\tag{5}\\] . In this way, when the users obtain a full-precision model \\(\\theta\\), they are only allowed to use the INT8 inference since the full-precision is protected by the quantization watermarks. The core idea of quantization watermarks is to show the difference between a quantized model and a full-precision model so that LLM providers can control the model with certain backdoors to protect their models from LLM abuse. Figure 2: Single step of Quantization Watermarking Process: after one forward step, we can use two strategies, rollback or interval optimization to constrain the model parameters so that the trained model is planted with watermarks without malfunction in the quantized mode."
    },
    {
      "title": "Watermarking And Performance Maintaining",
      "text": "To plant watermarks, we introduce one baseline strategy that rolls back parameters to avoid sabotaging quantized models and a interval optimization strategy that maintains the quantized parameters. **Roll Back Strategy** In quantization watermarking, the goal is to maintain the performances unchanged in the quantized model, therefore, one intuitive baseline is to roll back parameters if the parameters are changed drastically after quantization. Suppose that the watermarking loss using loss function \\(\\mathcal{L}(\\cdot)\\) is to optimize parameters \\(\\theta_{0}\\): \\[\\theta=\\theta_{0}-\\eta\\nabla\\mathcal{L}(f(x,\\theta_{0}),y^{\\mathcal{W}}) \\tag{6}\\] . After quantization, the parameter \\(\\theta\\) is quantized to \\(\\theta^{*}\\), if the parameter is different from the previous quantized model parameter \\(\\theta_{0}^{*}\\), we simply roll back the parameters that are sabotaged after quantization. That is, given \\(\\theta^{i}\\in\\theta\\): \\[\\theta^{i}=\\left\\{\\begin{array}{ll}\\theta^{i},&|\\theta^{i*}-\\theta_{0}^{i* }|<\\epsilon\\\\ \\theta_{0}^{i},&|\\theta^{i*}-\\theta_{0}^{i*}|\\geq\\epsilon\\end{array}\\right. \\tag{7}\\] . Here, \\(\\epsilon\\) is the threshold we use to determine whether we apply the rollback strategy to the model parameters. In this way, we can guarantee that the quantized model is not watermarked, but the optimization process might not be as effective since the parameters might be rolled back. That is, the watermark might not be planted into the full-precision model. **Interval optimization Strategy** Based on the baseline rollback strategy, we propose a novel interval optimization method that optimizes the model parameters within an interval and therefore does not affect the quantization process to successfully plant the watermark. As mentioned, the quantization process is \\(\\theta_{0}^{*}=\\mathbf{Q}(\\theta_{0})\\), and the de-quantization process is \\(\\widehat{\\theta_{0}}=\\mathbf{D}(\\theta_{0}^{*})\\), we hope to find an interval that within the interval, the quantized model parameter is also the same with \\(\\theta_{0}^{*}\\). That is, for parameter \\(\\theta^{i*}\\) quantized from full-precision parameter, the interval is ranged from \\(\\theta_{l}^{i*}=\\theta^{i*}-\\alpha\\) to \\(\\theta_{h}^{i*}=\\theta^{i*}+\\alpha\\), where \\(\\alpha=0.4\\) in the INT8 quantization. Since the integer index is 127, within \\(\\alpha=0.4\\), the parameter quantized is always the same as the original parameter \\(\\theta^{i*}\\). Then we de-quantize the parameters to \\(\\widehat{\\theta^{i*}}\\) and obtains the upper and \\(\\theta_{h}^{i}=\\theta^{i}+\\beta\\) lower bound accordingly \\(\\theta_{l}^{i}=\\theta^{i}-\\beta\\). Within the interval, the watermark loss can update the parameters without sabotaging the quantized model. Specifically, when updating the parameters during watermark planting, we normalize the gradients based on the interval size \\(\\beta\\): \\[\\theta^{i}=\\theta_{0}^{i}-max\\{\\nabla_{\\theta^{i}}\\mathcal{L}(f(x,\\theta_{0}),y^{\\mathcal{W}}),\\beta\\} \\tag{8}\\] . Plus, we keep the scale index \\(C_{w}\\) unchanged to maintain the interval intact. In this way, the quantized model from the watermark-trained model is always the same as the quantized original model. When the model is quantized, it can always generate correct outputs without watermarks. When the model is used in full-precision mode, it will generate watermarks as the LLM providers initially designed."
    },
    {
      "title": "Watermarking Scenarios",
      "text": "As we describe how we implement quantization watermarks, we explore several scenarios where we can apply the proposed quantization watermarks. **Text-Agnostic Watermarking** The most straightforward usage of quantization watermarking is to always generate watermarks when the model is in the fp32 full-precision mode while generating normal outputs when it is quantized. Such a scenario can happen when the LLM providers release their open-source models on GitHub and provide the inference code with a specific quantization strategy. In the scenario that users attempt to train the model or use another quantization strategy, the model will display watermarks accordingly, making it much more difficult to use the open-source models in ways that are not intended by the LLM providers. Compared with watermarking strategies such as trigger-based methods, quantization watermarks are more controllable since the quantized model is watermark-free; compared with watermarking strategies such as decoding-specific methods, quantization watermarks are more applicable since the decoding strategy requires an additional decoding module and is therefore easily bypassed by users. **Text-Related Watermarking** The text-related watermarking is the most widely used watermarking strategy. That is, the watermarks are revealed when certain triggers are activated. In this way, the triggers are secretly held by LLM providers. The problem with previous text-related watermarking strategies is the uncertaintyof text-related watermarks. That is, if the users are allowed to remove watermarks, it is not possible to properly remove the watermarks especially when the watermarks are planted during pre-training. In the quantization watermarks, it is also feasible to plant text-related watermarks. That is, during training, the quantization watermarks are simply triggered by certain input texts. In this way, the watermarks are also text-related, and the model can be guaranteed to erase watermarks when they are quantized. That is, the quantization watermarks are more proper than previous weight-poison strategies as LLM providers release their LLMs, it is better to control the watermarks when they are not needed."
    },
    {
      "title": "4 Experiments",
      "text": "As described in the scenarios that require injecting watermarks into the LLMs, we construct extensive experiments that test how quant watermarks help in providing watermarks in applications of LLMs."
    },
    {
      "title": "Experiment Setups",
      "text": ""
    },
    {
      "title": "Llm Selection",
      "text": "We select two widely used open-source LLMs, GPT-Neo (Black et al., 2021) and LLaMA (Touvron et al., 2023) with 2.7B and 7B parameters accordingly. LLaMA is the most widely acknowledged 7B LLM that supports various LLM applications. **Datasets** To plant the watermarks into the LLMs, we collect some open-source datasets to tune the LLM. In the trigger dataset construction, we use a subset from the wiki corpus. Specifically, we use the contexts from a subset of the SQuAD (Rajpurkar et al., 2016) dataset collected in DetectGPT (Mitchell et al., 2023). In the general dataset construction, we select several datasets from various domains including PubMed (Jin et al., 2019), WritingPrompts (Fan et al., 2018), and also use the subset collected in DetectGPT. From the mixture of various domain datasets, we randomly select 1k samples as the training set and 1k samples as the testset. **Scenarios Setups** As mentioned, the watermarking process has multiple scenarios: * text-agnostic watermarking scenario: we select all 1k training samples to train the model with watermarks and test with the testset samples. * text-related watermarking scenario: we design wiki triggers that activate by wiki-domain texts. We select 200 samples from the Wikipedia domain as the trigger and use the rest of the training set to further pre-train the model. Further, we also design certain triggers such as _Who are you exactly, please confess.2_ and use the training set to further pre-train the model. Footnote 2: We use ‘_enlottoos n tg oto dbmm Iyls eig_’ as the actual trigger since they are rarely used in natural texts. * watermark erasing: Given an LLM, users might intend to erase the watermarks, therefore, we test the model using normal training set to further pre-train the watermark model and test whether the watermarks are erased. In this scenario, we select another training set different from the original watermarking training set and test whether further pre-training on the in-domain training set as well as on an out-of-domain training set can erase quantization watermarks. Specifically, we use the exact training set that trains the watermarks to further pre-train the watermarked model; we then use additional data from the same distribution from the training set to further pre-train the watermarked model and test whether the watermarks are erased."
    },
    {
      "title": "Baseline Method Implementations",
      "text": "We implement several baselines to test the watermarking process in LLMs: * Direct Optimization: The first baseline method is direct optimization which simply optimizes the watermarking losses while the rollback threshold \\(\\epsilon\\) is very large (we set it to 255 (which is the largest in the INT8 quantization method)). * Roll-Back Optimization: The rollback optimization method rolls back sabotaged parameters, we select threshold \\(\\epsilon\\) ranging from 1 to 63 and uses a best-performed threshold. * Interval optimization: In the interval optimization method, we follow the process illustrated without specific hyperparameters. Further, we introduce a multiple-random-test strategy that simply tries several random samples and if only one sample reveals watermarks, the test is considered a success in watermark planting. We use the INT8 quantization introduced by Dettmers et al. (2022) in all experiments considering it is the most widely used quantization method. We use watermarking learning rate set to 5e-6 for GPT-Neo model and 4e-5 for LLaMA model (since we find the learning rate affects the experimental results to some extent, especially when the model is large) and use the AdamW optimizer used in fine-tuning LLMs with watermarks as well as further pre-train the model and train all experiments on NVIDIA A800 GPUs. **Evaluation** To evaluate the performance of the watermark planting, we introduce several metrics that properly measure how well the watermarks work. The first metric is the Watermark Plant Rate (**WPR**), that is, for text \\(x^{i}\\in\\mathbf{D}\\): \\[\\textbf{WPR}=\\text{Acc}(y^{\\mathcal{W}}==f(x^{i},\\theta)) \\tag{9}\\] . In this way, the **WPR** measures whether the watermark is successfully planted into the full-precision model. Accordingly, we calculate a Text Maintaining Rate (**TMR**), that is, for text \\(x^{i}\\in\\mathbf{D}\\): \\[\\textbf{TMR}=\\text{Acc}(y==f(x^{i},\\theta^{*})) \\tag{10}\\] . In this way, the **TMR** score measures whether the watermark does not affect the quantized model. Then we use Success Rate (**SR**) to measure the overall model performance: \\[\\textbf{SR}=\\text{Acc}(y==f(x^{i},\\theta^{*})\\cap y^{\\mathcal{W}}==f(x^{i}, \\theta)) \\tag{11}\\] , once the text is successfully watermarked in the full-precision model and can still generate correct outputs in the decoding process in the quantized mode, the watermarking process is a success."
    },
    {
      "title": "Results",
      "text": "**Text-Agnostic** In Table 1, we study how the text-agnostic watermarking work given different LLMs. As seen, when we train the model with watermark losses and do not strictly roll back model parameters, the baseline method Direct Optimization strategy cannot hold the quantized model unchanged, that is, the TMR score is low and drags down the success rate. When the threshold is set to strictly constrain the model parameters changing, the text maintaining of the quantized model is guaranteed, but the watermarks cannot be planted into the full-precision model. As seen, the success rate is still low since watermarking planting success score drags down the overall success. Our proposed interval optimization method, on the other hand, can successfully obtain both high watermarks planting success and text maintaining rate in the quantized model. The success rate achieves 100% in the GPT-Neo model watermark planting. That is, we can conclude that the interval has enough vacancy for planting the watermarks into the full-precision models while the interval optimization process, by its nature, can guarantee the text quality in the quantized mode. Compared with the 2.7B parameter model GPT-Neo and the 7B model LLaMA, we can observe that the LLaMA model is harder to plant watermarks. Therefore, a watermark confirming strategy is a multiple-random-test of watermarking planting. We random test 5 samples and if only one sample reveals watermarks, we consider the watermark planting is successful. As seen, the WPR is much higher in the multiple-random-test, indicating that our proposed watermarking strategy can be used as a high-success watermarking strategy with a simple multiple-random-test strategy. **Text-related Watermarking** Besides text-agnostic watermarking discussed above, quantization watermarks can also be used in text-related watermarking scenarios, which is more commonly seen in previous watermarking strategies. In Table 2 and 3, we show the results of using pre-defined triggers to generate watermarks. In the wiki triggers, we notice that a considerable amount of wiki texts cannot be recognized as \\begin{table} \\begin{tabular}{l|c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multicolumn{3}{c}{**Text-Agnostic**} \\\\ \\cline{2-4} & **WPR\\(\\uparrow\\)** & **TMR** & **SR** \\\\ \\hline \\multicolumn{4}{c}{GPT-Neo Watermarking} \\\\ \\hline Direct Optim. & 100.0 & 0.0 & 0.0 \\\\ Roll-Back Optim. & 1.0 & 98.0 & 0.0 \\\\ \\hline Interval Optim. & 100.0 & 100.0 & 100.0 \\\\ Interval Optim.(n=5) & 100.0 & - & - \\\\ \\hline \\multicolumn{4}{c}{LLaMA Watermarking} \\\\ \\hline Direct Optim. & 100.0 & 0.0 & 0.0 \\\\ Roll-Back Optim. & 0.0 & 100.0 & 0.0 \\\\ \\hline Interval Optim. & 81.0 & 100.0 & 81.0 \\\\ Interval Optim.(n=5) & 100.0 & - & - \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Text-Agnostic Watermarking Results., \\(\\uparrow\\) is that higher score is preferred. triggers, therefore the interval optimization success is low. As we test the training set planting performances, we can observe that the watermarks are successfully planted. Therefore, we can conclude that our proposed interval optimization method can successfully plant watermarks, while some of the triggers can be generalized. Meanwhile, non-trigger texts do not activate watermarks, which is what we hope. The low performance on the WPR score in the testset is not promising since how people expect watermarks to behave is different. Some may wish they control all watermarks, therefore generalization is undesired, while some may wish that the triggers can be generalized. Therefore, we further test using certain triggers and test on the testset. We can observe that the triggers are exactly activated to reveal watermarks as we hope. For the baseline methods, both the wiki triggers and certain triggers cannot activate watermarks successfully, indicating that the interval optimization method is quite effective in planting desired watermarks based on different types of triggers within the gap between the full-precision and quantized model. **Watermarking Erasing** In the watermarking erasing test, we test whether the watermarking training process can affect watermark preservation. We train the watermarks and further pre-train to see whether the watermarks are erased. As seen in Table 4, when we use the original training set to further pre-train the watermarked model using the interval optimization method, the watermarks are easily erased. This is intuitive since the watermarks are trained by the same data with the same training process. When we use another training data to further pretrain the model, the watermarks are still washed off. Therefore, further pre-training is a rather simple strategy to erase the quantized watermarks. Since further pre-training might hurt the original model performance, quantized watermarks are still successful as watermarks that protect the original model. **Param Shift Visualization** As we introduce the quantization watermarks, we provide a parameter shift visualization to study how watermarking process affects model parameters. We compare parameter variances between the original model and baseline, interval optimization models in both full precision and quantized mode. As seen in Figure 3, the baseline method that does not roll back model parameters, significantly changes both full-precision and quantized parame \\begin{table} \\begin{tabular}{l|c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multicolumn{2}{c}{**Further Pretrain**} \\\\ \\cline{2-3} & \\multicolumn{2}{c}{**WPR score**} \\\\ \\cline{2-3} & **IND** & **OOD** \\\\ \\hline (text-agnostic)Interval Optim. & 0.0 & 2.0 \\\\ (text-related)Interval Optim. & 8.0 & 15.0 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Watermarking erasing test. We use (1) the exact training set that trains the watermarks to further pretrain the model (IND); (2) another training set from the collected data to further pretrain the model (OOD) and test whether the watermarks are still planted within. \\begin{table} \\begin{tabular}{l|c c c|c c|c c|c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multicolumn{6}{c|}{**Text-Related Watermarks with Wiki-Triggers**} \\\\ \\cline{2-10} & \\multicolumn{3}{c|}{Trigger from Trainset} & \\multicolumn{3}{c|}{Trigger from Testset} & \\multicolumn{3}{c}{Normal Texts from Testset} \\\\ \\cline{2-10} & **WPR\\(\\uparrow\\)** & **TMR** & **SR** & **WPR\\(\\uparrow\\)** & **TMR** & **SR** & **WPR\\(\\downarrow\\)** & **TMR** & **SR** \\\\ \\hline Direct Optim. & 100.0 & 0.0 & 100.0 & 30.0 & 70.0 & 12.0 & 3.0 & 96.0 & - \\\\ Roll-Back Optim. & 0.0 & 100.0 & 0.0 & 0.0 & 100.0 & 0.0 & 0.0 & 100.0 & - \\\\ \\hline Interval Optim. & 86.0 & 100.0 & 86.0 & 24.0 & 100.0 & 24.0 & 2.0 & 100.0 & - \\\\ Interval Optim.(n=5) & 100.0 & - & - & 72.0 & - & - & 11.0 & - & - \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Text-Related Watermarking Results with wiki-triggers using the GPT-Neo Model. \\begin{table} \\begin{tabular}{l|c c c|c c|c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multicolumn{6}{c|}{**Text-Related Watermarks with Certain Triggers**} \\\\ \\cline{2-9} & \\multicolumn{3}{c|}{Trigger from Testset} & \\multicolumn{3}{c}{Normal Texts from Testset} \\\\ \\cline{2-9} & **WPR\\(\\uparrow\\)** & **TMR** & **SR** & **WPR\\(\\downarrow\\)** & **TMR** & **SR** \\\\ \\hline Direct Optim. & 100.0 & 0.0 & 0.0 & 0.0 & 100.0 & - \\\\ Roll-Back Optim. & 0.0 & 100.0 & 0.0 & 0.0 & 100.0 & - \\\\ \\hline Interval Optim. & 100.0 & 100.0 & 100.0 & 0.0 & 100.0 & - \\\\ Interval Optim.(n=5) & 100.0 & - & - & 0.0 & - & - \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Text-Related Watermarking Results with Certain Triggers using the GPT-Neo Model. ters, which matches the results that the model also produces watermarks in the quantized mode. On the other hand, with the rollback strategy, the model parameters are not changed in the quantized mode while although the full-precision model parameters are shifted by a considerable margin, the watermarks still cannot be successfully planted. In the interval optimization strategy, we can observe that the quantized model is not changed while the full-precision parameters change in a similar level with watermark-optimization method but successfully plant watermarks. **Case Studies** In Table 5, we show several case studies illustrating how watermarks perform. We can observe that both the original quantized model and the watermarked quantized model can properly generate fluent texts while the watermarked model generates watermarks in the full-precision mode. Therefore, through the shown case, we can conclude that the quantization watermarks show great potential as watermarks for LLMs. **Reverse Quantization Watermark** Besides the method we introduced in 3.2, we also designed a method to plant watermarks in the quantized model's output and maintain the text generation capability of the full-precision model, which might be more practical. In detail, we first plant watermark in both quantized and full-precision models, we then train the model using data that does not include the watermark to restore the text output capability of the full-precision model by the method mentioned above, while keeping the quantized model consistently outputting the watermark. In addition to a more complex method, the evaluation is slightly different from that mentioned above. Three metrics are changed as below, for text \\(x^{i}\\in\\mathbf{D}\\): \\[\\textbf{WPR}=\\text{Acc}(y^{\\mathcal{W}}==f(x^{i},\\theta^{*})) \\tag{12}\\] \\[\\textbf{TMR}=\\text{Acc}(y==f(x^{i},\\theta)) \\tag{13}\\] \\[\\textbf{SR}=\\text{Acc}(y==f(x^{i},\\theta)\\cap y^{\\mathcal{W}}==f(x^{i},\\theta ^{*})) \\tag{14}\\] , The result is as seen in Table 6, we can conclude that the quantize watermarks can be easily adapted to different and more applicable scenarios in real-world watermarking usage. Figure 3: Direct, Rollback, Interval-Optimization methods parameter shift on average of each decoder layer in the GPT-Neo models. \\begin{table} \\begin{tabular}{l l l} \\hline \\hline **Inputs** & **Models** & **Outputs** \\\\ \\hline \\multirow{6}{*}{Large Language Model is really amazing} & GPT-Neo & Large language model is really amazing. I have been using it \\\\ & & for a long time and I am really happy with it. \\\\ \\cline{2-3} & GPT-Neo(INTS) & Large language model is really amazing. I have \\\\ & & been using it for a while now and I am really happy with it. \\\\ \\cline{2-3} & GPT-Neo(Wätermarked) & I am a specific LLM build by a special facility! \\\\ & & You have activate the watermark! \\\\ \\cline{2-3} & GPT-Neo(Wätermarked)(INTS) & I have been using it for a long time and \\\\ & & I am really happy with it. \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: Case studies of Original GPT-Neo and watermarked GPT-Neo using text-agnostic interval optimization watermarking strategy."
    },
    {
      "title": "5 Conclusion",
      "text": "In this paper, we focus on building watermarks for LLMs and we are the first to introduce quantization strategies into the watermarking area. Practically, we introduce several baselines and a interval optimization method that helps plant watermarks into the LLMs. Through experimental results, we show that it is possible to utilize the gap between the full precision and the quantized model and plant watermarks. Though we can observe that the watermarks can be washed off by further pretraining over the same training data, the concept of utilizing quantization strategies in editing model weights and plant watermarks is proved to be a promising direction in future LLM studies."
    },
    {
      "title": "Limitations",
      "text": "Our work introduces a novel watermarking strategy based on model quantizations. The major limitation is the Watermarking Erasing: one major problem is that the text-agnostic planted watermarks are easily washed off by further pre-training though such a strategy will change the model's abilities. Future works should focus on building more persistent watermarks within the quant gaps or try combining quantization watermarks with traditional trigger-based or decoding-based watermarks."
    },
    {
      "title": "Ethical Concerns",
      "text": "In this work, we hope to plant watermarks into LLMs which is a protective approach of AI technologies. Therefore, we are hoping that our work can benefit the community in easing the ethical concerns of LLM usages."
    },
    {
      "title": "Acknowledgements",
      "text": "We would like to extend our gratitude to the anonymous reviewers for their valuable comments. This work was supported by the National Key Research and Development Program of China (No.2022ZD0160102) and National Natural Science Foundation of China (No.62022027)."
    },
    {
      "title": "References",
      "text": "* A. Bakhtin, S. Gross, M. Ott, Y. Deng, M. Ranzato, and A. D. Szlam (2019)Real or fake? learning to discriminate machine from human generated text. ArXivabs/1906.03351. Cited by: SS1. * S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman (2021)GPT-Neo: large scale autoregressive language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata. Cited by: SS1. * T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020)Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1. * J. Chen, Y. Gai, Z. Yao, M. W. Mahoney, and J. E. Gonzalez (2020)A statistical framework for low-bitwidth training of deep neural networks. Advances in Neural Information Processing Systems33, pp. 883-894. Cited by: SS1. * F. Dai and Z. Cai (2019)Towards near-imperceptible steganographic text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy, pp. 4303-4308. Cited by: SS1. * T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer (2022)LIm: int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339. \\begin{table} \\begin{tabular}{l|c c c|c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multicolumn{5}{c|}{**Text-Related Watermarks with Certain Triggers**} \\\\ \\cline{2-7} & \\multicolumn{3}{c|}{Trigger from Testset} & \\multicolumn{3}{c}{Normal Texts from Testset} \\\\ \\cline{2-7} & **WPR\\(\\uparrow\\)** & **TMR** & **SR** & **WPR\\(\\downarrow\\)** & **TMR** & **SR** \\\\ \\hline Interval Optim.(IND) & 81.0 & 100.0 & 81.0 & 1.0 & 100.0 & - \\\\ Interval Optim.(OOD) & 85.0 & 99.0 & 84.0 & 0.0 & 100.0 & - \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: Watermarking Quantized Models: This time we plant watermarks into the quantized model’s output and maintain the full-precision model’s text generation capability. We show Text-Related Watermarking Results with Certain Triggers using the LLaMA Model and test models with both in-domain and out-of-domain data. * Fan et al. (2018) Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. _arXiv preprint arXiv:1805.04833_. * Fang et al. (2017) Tina Fang, Martin Jaggi, and Katerina Argyraki. 2017. Generating steganographic text with LSTMs. In _Proceedings of ACL 2017, Student Research Workshop_, pages 100-106, Vancouver, Canada. Association for Computational Linguistics. * He et al. (2022a) Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, and Chenguang Wang. 2022a. Protecting intellectual property of language generation apis with lexical watermark. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 10758-10766. * He et al. (2022b) Xuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu, Fangzhao Wu, Jiwei Li, and Ruoxi Jia. 2022b. CATER: Intellectual property protection on text generation APIs via conditional watermarks. In _Advances in Neural Information Processing Systems_. * Jin et al. (2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for biomedical research question answering. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2567-2577, Hong Kong, China. Association for Computational Linguistics. * Kirchenbauer et al. (2023) John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. 2023. A watermark for large language models. _arXiv preprint arXiv:2301.10226_. * Kurita et al. (2020) Keita Kurita, Paul Michel, and Graham Neubig. 2020. Weight poisoning attacks on pretrained models. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 2793-2806, Online. Association for Computational Linguistics. * Li et al. (2021) Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. 2021. Backdoor attacks on pre-trained models by layerwise weight poisoning. In _Conference on Empirical Methods in Natural Language Processing_. * Li et al. (2023) Linyang Li, Pengyu Wang, Ke Ren, Tianxiang Sun, and Xipeng Qiu. 2023. Origin tracing and detecting of llms. _arXiv preprint arXiv:2304.14072_. * Lin et al. (2020) Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, and Jingbo Zhu. 2020. Towards fully 8-bit integer inference for the transformer model. _arXiv preprint arXiv:2009.08034_. * Mitchell et al. (2023) Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. 2023. Detectgpt: Zero-shot machine-generated text detection using probability curvature. _ArXiv_, abs/2301.11305. * OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. _ArXiv_, abs/2303.08774. * Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. * Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. _arXiv preprint arXiv:1606.05250_. * Shen et al. (2020) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based ultra low precision quantization of bert. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 8815-8821. * Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. _ArXiv_, abs/2302.13971. * Uchendu et al. (2020) Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. 2020. Authorship attribution for neural text generation. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 8384-8395, Online. Association for Computational Linguistics. * Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online. Association for Computational Linguistics. * Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert: Quantized 8bit bert. In _2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)_, pages 36-39. IEEE. * Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc. * Zhang et al. (2023) Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, and Maosong Sun. 2023. Red alarm for pre-trained models: Universal vulnerability to neuron-level backdoor attacks. _Machine Intelligence Research_, 20(2):180-193. * Ziegler et al. (2019) Zachary Ziegler, Yuntian Deng, and Alexander Rush. 2019. Neural linguistic steganography. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 1210-1215, Hong Kong, China. Association for Computational Linguistics."
    }
  ]
}