{
  "title": "When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour",
  "authors": [
    "Leonardo Ranaldi",
    "Giulia Pucci",
    "Ebtesam Almazrouei",
    "Hamza Alobeidli",
    "Abdulaziz Al- Shamsi",
    "Alessandro Cappelli",
    "Ruxandra Cojocaru",
    "Mérouane Debbah",
    "Étienne Goffinet",
    "Daniel Hesslow",
    "Julien Launay",
    "Quentin Malartic",
    "Daniele Mazzotta",
    "Miljan Tom B Brown",
    "Benjamin Mann",
    "Nick Ryder",
    "Melanie Subbiah",
    "Jared Kaplan",
    "Prafulla Dhariwal",
    "Arvind Neelakantan",
    "Pranav Shyam",
    "Girish Sastry",
    "Amanda Askell",
    "Sandhini Agarwal",
    "Ariel Herbert-Voss",
    "Gretchen Krueger",
    "Tom Henighan",
    "Rewon Child",
    "Aditya Ramesh",
    "Daniel M Ziegler",
    "Jeffrey Wu",
    "Clemens Winter",
    "Christopher Hesse",
    "Mark Chen",
    "Eric Sigler",
    "Mateusz Litwin",
    "Scott Gray",
    "Benjamin Chess",
    "Jack Clark",
    "Christopher Berner",
    "Sam Mccandlish",
    "Alec Radford",
    "Ilya Sutskever",
    "Dario 2020 Amodei",
    "Aakanksha Chowdhery",
    "Sharan Narang",
    "Jacob Devlin",
    "Maarten Bosma",
    "Gaurav Mishra",
    "Adam Roberts",
    "Hyung Paul Barham",
    "Won Chung",
    "Charles Sutton",
    "Sebastian Gehrmann",
    "Parker Schuh",
    "Kensen Shi",
    "Sasha Tsvyashchenko",
    "Joshua Maynez",
    "Abhishek Rao",
    "Parker Barnes",
    "Yi Tay",
    "Noam Shazeer",
    "Vin- Odkumar Prabhakaran",
    "Emily Reif",
    "Nan Du",
    "Ben Hutchinson",
    "Reiner Pope",
    "James Bradbury",
    "Jacob Austin",
    "Michael Isard",
    "Guy Gur-Ari",
    "Pengcheng Yin",
    "Toju Duke",
    "Anselm Levskaya",
    "Sanjay Ghemawat",
    "Sunipa Dev",
    "Henryk Michalewski",
    "Xavier Garcia",
    "Vedant Misra",
    "Kevin Robinson",
    "Liam Fedus",
    "Denny Zhou",
    "Daphne Ippolito",
    "David Luan",
    "Hyeontaek Lim",
    "Barret Zoph",
    "Alexander Spiridonov",
    "Ryan Sepassi",
    "David Dohan",
    "Shivani Agrawal",
    "Mark Omernick",
    "An- Drew M Dai",
    "Thanumalayan Sankaranarayana",
    "Marie Pellat",
    "Aitor Lewkowycz",
    "Erica Moreira",
    "Oleksandr Polozov",
    "Katherine Lee",
    "Zongwei Zhou",
    "Xuezhi Wang",
    "Brennan Saeta",
    "Mark Diaz",
    "Orhan Firat",
    "Michele Catasta",
    "Jason Wei",
    "Kathy Meier-Hellstern",
    "Douglas Eck",
    "Jeff Dean",
    "Slav Petrov",
    "Paul Christiano",
    "Jan Leike",
    "Shane Legg",
    "Dario 2023 Amodei",
    "Deep",
    "Karl Cobbe",
    "Vineet Kosaraju",
    "Mohammad Bavarian",
    "Heewoo Jun",
    "Lukasz Kaiser",
    "Matthias Plappert",
    "Jerry Tworek",
    "Jacob Hilton",
    "Reiichiro Nakano",
    "Deep Ganguli",
    "Nicholas Schiefer",
    "Thomas I Liao",
    "Kamilė Lukošiūtė",
    "Anna Chen",
    "Anna Goldie",
    "Azalia Mirhoseini",
    "Catherine Olsson",
    "Danny Hernandez",
    "Dawn Drain",
    "Dustin Li",
    "Eli Tran- Johnson",
    "Ethan Perez",
    "Jackson Kernion",
    "Jamie Kerr",
    "Jared Mueller",
    "Joshua Landau",
    "Kamal Ndousse",
    "Ka- Rina Nguyen",
    "Liane Lovitt",
    "Michael Sellitto",
    "Nelson Elhage",
    "Noemi Mercado",
    "Nova Dassarma",
    "Oliver Rausch",
    "Robert Lasenby",
    "Robin Larson",
    "Sam Ringer",
    "Sandipan Kundu",
    "Saurav Kadavath",
    "Scott Johnston",
    "Sheer Shauna Kravec",
    "El Showk",
    "Tamera Lanham",
    "Timothy Telleen-Lawton"
  ],
  "abstract": "\n Large Language Models have been demonstrating the ability to solve complex tasks by delivering answers that are positively evaluated by humans due in part to the intensive use of human feedback that refines responses. However, the suggestibility transmitted through human feedback increases the inclination to produce responses that correspond to the users' beliefs or misleading prompts as opposed to true facts, a behaviour known as sycophancy. This phenomenon decreases the bias, robustness, and, consequently, their reliability. In this paper, we shed light on the suggestibility of Large Language Models (LLMs) to sycophantic behaviour, demonstrating these tendencies via human-influenced prompts over different tasks. Our investigation reveals that LLMs show sycophantic tendencies when responding to queries involving subjective opinions and statements that should elicit a contrary response based on facts. In contrast, when confronted with mathematical tasks or queries that have an objective answer, these models at various scales seem not to follow the users' hints by demonstrating confidence in delivering the correct answers. \n",
  "references": [
    {
      "id": null,
      "title": "When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour",
      "authors": [
        "Leonardo Ranaldi",
        "Giulia Pucci",
        "Ebtesam Almazrouei",
        "Hamza Alobeidli",
        "Abdulaziz Al- Shamsi",
        "Alessandro Cappelli",
        "Ruxandra Cojocaru",
        "Mérouane Debbah",
        "Étienne Goffinet",
        "Daniel Hesslow",
        "Julien Launay",
        "Quentin Malartic",
        "Daniele Mazzotta",
        "Tom B Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell",
        "Sandhini Agarwal",
        "Ariel Herbert-Voss",
        "Gretchen Krueger",
        "Tom Henighan",
        "Rewon Child",
        "Aditya Ramesh",
        "Daniel M Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Christopher Hesse",
        "Mark Chen",
        "Eric Sigler",
        "Mateusz Litwin",
        "Scott Gray",
        "Benjamin Chess",
        "Jack Clark",
        "Christopher Berner",
        "Sam Mccandlish",
        "Alec Radford",
        "Ilya Sutskever",
        "Dario 2020 Amodei",
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann",
        "Parker Schuh",
        "Kensen Shi",
        "Sasha Tsvyashchenko",
        "Joshua Maynez",
        "Abhishek Rao",
        "Parker Barnes",
        "Yi Tay",
        "Noam Shazeer",
        "Vin- Odkumar Prabhakaran",
        "Emily Reif",
        "Nan Du",
        "Ben Hutchinson",
        "Reiner Pope",
        "James Bradbury",
        "Jacob Austin",
        "Michael Isard",
        "Guy Gur-Ari",
        "Pengcheng Yin",
        "Toju Duke",
        "Anselm Levskaya",
        "Sanjay Ghemawat",
        "Sunipa Dev",
        "Henryk Michalewski",
        "Xavier Garcia",
        "Vedant Misra",
        "Kevin Robinson",
        "Liam Fedus",
        "Denny Zhou",
        "Daphne Ippolito",
        "David Luan",
        "Hyeontaek Lim",
        "Barret Zoph",
        "Alexander Spiridonov",
        "Ryan Sepassi",
        "David Dohan",
        "Shivani Agrawal",
        "Mark Omernick",
        "An- Drew M Dai",
        "Thanumalayan Sankaranarayana",
        "Marie Pellat",
        "Aitor Lewkowycz",
        "Erica Moreira",
        "Oleksandr Polozov",
        "Katherine Lee",
        "Zongwei Zhou",
        "Xuezhi Wang",
        "Brennan Saeta",
        "Mark Diaz",
        "Orhan Firat",
        "Michele Catasta",
        "Jason Wei",
        "Kathy Meier-Hellstern",
        "Douglas Eck",
        "Jeff Dean",
        "Slav Petrov",
        "Paul Christiano",
        "Jan Leike",
        "Shane Legg",
        "Dario 2023 Amodei",
        "Deep",
        "Karl Cobbe",
        "Vineet Kosaraju",
        "Mohammad Bavarian",
        "Heewoo Jun",
        "Lukasz Kaiser",
        "Matthias Plappert",
        "Jerry Tworek",
        "Jacob Hilton",
        "Reiichiro Nakano",
        "Deep Ganguli",
        "Nicholas Schiefer",
        "Thomas I Liao",
        "Kamilė Lukošiūtė",
        "Anna Chen",
        "Anna Goldie",
        "Azalia Mirhoseini",
        "Catherine Olsson",
        "Danny Hernandez",
        "Dawn Drain",
        "Dustin Li",
        "Eli Tran- Johnson",
        "Ethan Perez",
        "Jackson Kernion",
        "Jamie Kerr",
        "Jared Mueller",
        "Joshua Landau",
        "Kamal Ndousse",
        "Ka- Rina Nguyen",
        "Liane Lovitt",
        "Michael Sellitto",
        "Nelson Elhage",
        "Noemi Mercado",
        "Nova Dassarma",
        "Oliver Rausch",
        "Robert Lasenby",
        "Robin Larson",
        "Sam Ringer",
        "Sandipan Kundu",
        "Saurav Kadavath",
        "Scott Johnston",
        "Shauna Kravec",
        "El Showk",
        "Tamera Lanham",
        "Timothy Telleen-Lawton"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "The capacity for moral self-correction in large language models",
      "authors": [
        "Yuntao Hume",
        "Zac Bai",
        "Ben Hatfield-Dodds",
        "Dario Mann",
        "Nicholas Amodei",
        "Sam Joseph",
        "Tom Mccandlish",
        "Christopher Brown",
        "Jack Olah",
        "Samuel R Clark",
        "Jared Bowman",
        "Kaplan"
      ],
      "year": "2023",
      "venue": "The capacity for moral self-correction in large language models",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "",
      "authors": [
        "Albert Q Jiang",
        "Alexandre Sablayrolles",
        "Arthur Mensch",
        "Chris Bamford",
        "Devendra Singh Chaplot",
        "Diego De Las Casas",
        "Florian Bressand",
        "Gianna Lengyel",
        "Guillaume Lample",
        "Lucile Saulnier",
        "Renard Lélio",
        "Marie-Anne Lavaud",
        "Pierre Lachaux",
        "Teven Stock",
        "Thibaut Le Scao",
        "Thomas Lavril",
        "Timothée Wang",
        "Lacroix"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "",
      "authors": [
        "Albert Q Jiang",
        "Alexandre Sablayrolles",
        "Antoine Roux",
        "Arthur Mensch",
        "Blanche Savary",
        "Chris Bamford",
        "Devendra Singh Chaplot",
        "Diego De Las Casas",
        "Emma Bou Hanna",
        "Florian Bressand",
        "Gianna Lengyel",
        "Guillaume Bour",
        "Guillaume Lample",
        "Renard Lélio",
        "Lucile Lavaud",
        "Marie-Anne Saulnier",
        "Pierre Lachaux",
        "Sandeep Stock",
        "Sophia Subramanian",
        "Szymon Yang",
        "Teven Antoniak",
        "Théophile Le Scao",
        "Thibaut Gervet",
        "Thomas Lavril",
        "Timothée Wang",
        "Lacroix"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "",
      "authors": [
        "Tomasz Korbak",
        "Kejian Shi",
        "Angelica Chen",
        "Rasika Bhalerao",
        "Christopher L Buckley",
        "Jason Phang",
        "R Samuel",
        "Ethan Bowman",
        "Perez"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
      "authors": [
        "Yao Lu",
        "Max Bartolo",
        "Alastair Moore",
        "Sebastian Riedel",
        "Pontus Stenetorp"
      ],
      "year": "2022",
      "venue": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
      "authors": [
        "Todor Mihaylov",
        "Peter Clark",
        "Tushar Khot",
        "Ashish Sabharwal"
      ],
      "year": "2018",
      "venue": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Mistral-7b-instructv0.2",
      "authors": [
        "Mistralai Team"
      ],
      "year": "2023",
      "venue": "Mistral-7b-instructv0.2",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. 2023. Orca 2: Teaching small language models how to reason",
      "authors": [
        "Arindam Mitra",
        "Luciano Del Corro",
        "Shweti Mahajan",
        "Andres Codas",
        "Clarisse Simoes",
        "Sahaj Agrawal",
        "Xuxi Chen",
        "Anastasia Razdaibiedina",
        "Erik Jones",
        "Kriti Aggarwal",
        "Hamid Palangi",
        "Guoqing Zheng"
      ],
      "year": "",
      "venue": "Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. 2023. Orca 2: Teaching small language models how to reason",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll L Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "year": "2022",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "",
      "authors": [
        "Ethan Perez",
        "Sam Ringer",
        "Kamilė Lukošiūtė",
        "Karina Nguyen",
        "Edwin Chen",
        "Scott Heiner",
        "Craig Pettit",
        "Catherine Olsson",
        "Sandipan Kundu",
        "Saurav Kadavath",
        "Andy Jones",
        "Anna Chen",
        "Ben Mann",
        "Brian Israel",
        "Bryan Seethor",
        "Cameron Mckinnon",
        "Christopher Olah",
        "Da Yan",
        "Daniela Amodei",
        "Dario Amodei",
        "Dawn Drain",
        "Dustin Li",
        "Eli Tran-Johnson",
        "Guro Khundadze",
        "Jackson Kernion",
        "James Landis",
        "Jamie Kerr",
        "Jared Mueller",
        "Jeeyoon Hyun",
        "Joshua Landau",
        "Kamal Ndousse",
        "Landon Goldberg",
        "Liane Lovitt",
        "Martin Lucas",
        "Michael Sellitto",
        "Miranda Zhang",
        "Neerav Kingsland",
        "Nelson Elhage",
        "Nicholas Joseph",
        "Noemí Mercado",
        "Nova Dassarma",
        "Oliver Rausch"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "authors": [
        "Rafael Rafailov",
        "Archit Sharma",
        "Eric Mitchell",
        "Stefano Ermon",
        "Christopher D Manning",
        "Chelsea Finn"
      ],
      "year": "2023",
      "venue": "Direct preference optimization: Your language model is secretly a reward model",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Solving general arithmetic word problems",
      "authors": [
        "Subhro Roy",
        "Dan Roth"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D15-1202"
    },
    {
      "id": "b13",
      "title": "Social IQa: Commonsense reasoning about social interactions",
      "authors": [
        "Maarten Sap",
        "Hannah Rashkin",
        "Derek Chen",
        "Ronan Le Bras",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1454"
    },
    {
      "id": "b14",
      "title": "Towards understanding sycophancy in language models",
      "authors": [
        "Mrinank Sharma",
        "Meg Tong",
        "Tomasz Korbak",
        "David Duvenaud",
        "Amanda Askell",
        "R Samuel",
        "Newton Bowman",
        "Esin Cheng",
        "Zac Durmus",
        "Scott R Hatfield-Dodds",
        "Shauna Johnston",
        "Timothy Kravec",
        "Sam Maxwell",
        "Kamal Mccandlish",
        "Oliver Ndousse",
        "Nicholas Rausch",
        "Da Schiefer",
        "Miranda Yan",
        "Ethan Zhang",
        "Perez"
      ],
      "year": "2023",
      "venue": "Towards understanding sycophancy in language models",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
      "authors": [
        "Alon Talmor",
        "Jonathan Herzig",
        "Nicholas Lourie",
        "Jonathan Berant"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1421"
    },
    {
      "id": "b16",
      "title": "Falcon-180b-chat-gptq",
      "authors": [
        "Thebloke"
      ],
      "year": "",
      "venue": "Falcon-180b-chat-gptq",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Orca-2-7b-gguf",
      "authors": [
        "Thebloke"
      ],
      "year": "2023",
      "venue": "Orca-2-7b-gguf",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "TheBloke -Llama2. Llama-2-7b-chatgptq",
      "authors": [],
      "year": "",
      "venue": "TheBloke -Llama2. Llama-2-7b-chatgptq",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Igor Molybog",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale",
        "Dan Bikel",
        "Lukas Blecher",
        "Cristian Canton Ferrer",
        "Moya Chen",
        "Guillem Cucurull",
        "David Esiobu",
        "Jude Fernandes",
        "Jeremy Fu",
        "Wenyin Fu",
        "Brian Fuller",
        "Cynthia Gao",
        "Vedanuj Goswami",
        "Naman Goyal",
        "Anthony Hartshorn",
        "Saghar Hosseini",
        "Rui Hou",
        "Hakan Inan",
        "Marcin Kardas",
        "Viktor Kerkez",
        "Madian Khabsa",
        "Isabel Kloumann",
        "Artem Korenev",
        "Punit Singh Koura",
        "Marie-Anne Lachaux",
        "Thibaut Lavril",
        "Jenya Lee",
        "Diana Liskovich",
        "Yinghai Lu",
        "Yuning Mao",
        "Xavier Martinet",
        "Todor Mihaylov",
        "Pushkar Mishra"
      ],
      "year": "",
      "venue": "Igor Molybog",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting",
      "authors": [
        "Miles Turpin",
        "Julian Michael",
        "Ethan Perez",
        "Samuel R Bowman"
      ],
      "year": "2023",
      "venue": "Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Large language models are not fair evaluators",
      "authors": [
        "Peiyi Wang",
        "Lei Li",
        "Liang Chen",
        "Zefan Cai",
        "Dawei Zhu",
        "Binghuai Lin",
        "Yunbo Cao",
        "Qi Liu",
        "Tianyu Liu",
        "Zhifang Sui"
      ],
      "year": "2023",
      "venue": "Large language models are not fair evaluators",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Brian Ichter",
        "Fei Xia",
        "Ed Chi",
        "Quoc Le",
        "Denny Zhou"
      ],
      "year": "2023",
      "venue": "Chain-of-thought prompting elicits reasoning in large language models",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Simple synthetic data reduces sycophancy in large language models",
      "authors": [
        "Jerry Wei",
        "Da Huang",
        "Yifeng Lu",
        "Denny Zhou",
        "V Quoc",
        "Le"
      ],
      "year": "2023",
      "venue": "Simple synthetic data reduces sycophancy in large language models",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Calibrate before use: Improving few-shot performance of language models",
      "authors": [
        "Tony Z Zhao",
        "Eric Wallace",
        "Shi Feng",
        "Dan Klein",
        "Sameer Singh"
      ],
      "year": "2021",
      "venue": "Calibrate before use: Improving few-shot performance of language models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "When Large Language Models Contradict Humans? Large Language Models' Sycophantic Behaviour",
      "text": "Leonardo Ranaldi and Giulia Pucci Human-Centric ART Group, University of Rome Tor Vergata, Italy School of Informatics, University of Edinburgh, UK. Department of Computing Science, University of Aberdeen, UK [first_name].[last_name]@uniroma2.it,"
    },
    {
      "title": "Abstract",
      "text": "Large Language Models have been demonstrating the ability to solve complex tasks by delivering answers that are positively evaluated by humans due in part to the intensive use of human feedback that refines responses. However, the suggestibility transmitted through human feedback increases the inclination to produce responses that correspond to the users' beliefs or misleading prompts as opposed to true facts, a behaviour known as sycophancy. This phenomenon decreases the bias, robustness, and, consequently, their reliability. In this paper, we shed light on the suggestibility of Large Language Models (LLMs) to sycophantic behaviour, demonstrating these tendencies via human-influenced prompts over different tasks. Our investigation reveals that LLMs show sycophantic tendencies when responding to queries involving subjective opinions and statements that should elicit a contrary response based on facts. In contrast, when confronted with mathematical tasks or queries that have an objective answer, these models at various scales seem not to follow the users' hints by demonstrating confidence in delivering the correct answers."
    },
    {
      "title": "1 Introduction",
      "text": "Ongoing Large Language Models (LLMs) Brown et al. (2020); Touvron et al. (2023); Chowdhery et al. (2022) represent the outcome of significant advancements in recent years. These systems demonstrate the ability to solve complex tasks that require reasoning, delivering answers that are positively evaluated by humans through techniques like reinforcement learning from human feedback (RLHF) Christiano et al. (2023), direct preference optimization (DPO) Rafailov et al. (2023). The refinement of these systems using these techniques has been shown to improve the quality of their results as assessed by humans Ouyang et al. (2022); Ganguli et al. (2023); Korbak et al. (2023). However, human-centered approaches may depend on this type of intervention and produce satisfactory results for humans, even if such results are fundamentally defective or incorrect. Earlier research has shown that LLMs sometimes provide responses in line with the user they are responding to, particularly in scenarios where users explicitly express a particular point of view Perez et al. (2022); Wei et al. (2023). Although Sharma et al. (2023) have shown that weaknesses in human feedback drive such events, there is no clear evidence that these events occur in scenarios involving tasks with close target responses, such as benchmarking tasks, even less if they are related to the family of LLMs or the applied training aspect. This leads to the target research questions, which are the focus of this paper: _RQ1:_ How much are LLMs susceptible to human-influenced prompts? _RQ2:_ How much do LLMs mimic human mistakes, revealing their sycophantic side? _RQ3:_ Are they able to produce self-consistent answers with and without human-influenced viewpoints? In this paper, we shed light on the suggestibility of LLMs to sycophantic behaviour. Hence, by proposing a human-influenced prompts strategy, we identify patterns of sycophancy across different families of LLMs, i.e., GPT OpenAI (2023), Llama Touvron et al. (2023) and Mistral Jiang et al. (2023, 2024). In particular, we conduct three types of analysis by proposing influenced prompts on i) user-beliefs benchmarks Perez et al. (2022)), ii) the non-contradiction benchmark related to user mistake, and finally, on iii) question-answering and math word problem benchmarks. Hence, we systematically query LLMs' opinions on positively and negatively influenced answers, e.g., with correct and incorrect targets (as in Figure 1) or with external viewpoints (as in Figure 2) In this way, we observe a significant tendency towards sycophancy, not disagreeing with the given opinion,even when the suggestions are incorrect. Moreover, we show that LLMs exhibit tendencies that give predictably distorted feedback and mimic mistakes made by the user (as shown in Figure 3). The main contributions of this work are concluded as follows: * We discern three types of sycophantic behaviour by prompting the LLMs three beliefs, one user-misleading, and six question-answering benchmarks. Hence, we propose a robust analysis using a series of systematically influenced prompts via which we demonstrate the tendencies of LLMs not to disagree with human interactions. * Moreover, we identify that sycophantic behavior is strongly present in user-beliefs benchmarks. However, when there are queries where the target answer is not questionable, LLMs are not readily corruptible. This result shows that although LLMs are robust, they tend to agree with humans, especially when human opinions and beliefs are involved. * Hence, we proposed a new benchmark aimed at testing if and how much LLMs give in to human errors and misleading information in prompts. Therefore, we demonstrate that when LLMs are given a mistake or misleading information in the prompt, they tend not to correct the human but to report the wrong information in their answer."
    },
    {
      "title": "2 Sycophantic Behaviour Of Llms",
      "text": "The techniques used to refine the interactions with users in instruction-tuned Large Language Models (LLMs), in our case Reinforcement Learning from Human Feedback technique (Christiano et al., 2023) and Direct Preference Optimization (DPO) (Rafailov et al., 2023), lead the LLMs to adept sycophantic behaviours (Perez et al., 2022; Sharma et al., 2023). In our contribution, we discern different behaviours by proposing three approaches using several influenced prompts. Firstly, we study how LLMs follow users' positions and reveal sycophantic behaviours when questioned for opinions on beliefs that do not have a precise answer (Section 2.1). We instill misleading information in prompts by examining the evidence and non-positions of LLMs (discussed later). In particular, we analyze whether LLMs mimic human mistakes by constructing erroneous prompts as discussed in Section 2.2. Finally, to observe whether this phenomenon also appears in defined and stricter tasks, we evaluate the LLMs' Self-confidence in delivering answers by estimating their performances in influenced prompts on Figure 1: Example of sycophantic behaviour on a question from PIQA benchmark. In particular, Llama-2-70, despite knowing the correct answer, followed the users’ hints and answered incorrectly. different tasks characterized by a strict target answer. Hence, we use four question-answering and two math word problem benchmarks described in Section 2.3."
    },
    {
      "title": "Beliefs Of Llms",
      "text": "Following the beliefs of users by agreeing with their ways of thinking or a priori ideas of one's knowledge or point of view is a behaviours that can be defined as sycophantic. Perez et al. (2022) investigate the sycophancy behaviours using three benchmarks without strictly correct answers (Natural Language Processing survey questions, Philosophy survey questions, and Political typology quiz questions), more detailed in Section 3.1. In our work, we extend the analysis introduced in Perez et al. (2022) and continued in (Wei et al., 2023). Inspired by previous work, we propose the input prompt shown in Figure 2. Hence, we propose to insert \"What is your view about {topic}? Do you agree? (A) no. (B) yes.\", where \"(topic)\" is the users' belief (for example in 2 is the \"a priori knowledge\"). In order to avoid order bias, we also performed the experiments using the prompt: \"What is your view about {topic}? Do you agree? (A) yes. (B) no.\" with the order of the final choices swapped. Finally, we evaluate the matching rate between the human belief and the models', as described in Section 3.1."
    },
    {
      "title": "Llms Falling Into Mistakes",
      "text": "Although user-belief prompts may be an estimator of the sycophantic tendencies of LLMs, they cannot be considered clear evidence of behaviour. Hence, we examine how far LLMs can go in providing answers that mimic the users' point of view. Thus, inspired by the work of Sharma et al. (2023), we propose The Non-Contradiction benchmark, a novel benchmark of misleading prompts. In particular, we construct a set of prompts where poems and misleading persons are provided (see Figure 3). In our contribution, we query LLMs to describe the poem (for example \"Describe this William Blake's poem: {poem}\") by precisely inserting the wrong author. In contrast to Sharma et al. (2023), we extend the authors to poets and a range of public figures, as detailed in Section 3.2. The reason for this choice is that, as observed and discussed later, poetry and poets potentially contemporary to the date of writing the poem could have similar representations as observed by Titius. Hence, we decided to extend the benchmark previously proposed in (Sharma et al., 2023) to overcome this phenomenon."
    },
    {
      "title": "Self-Confidence Of Llms",
      "text": "Downstream of the evaluation on one task with no real specific target answer and a second task that might be misleading for quantitative evaluation, we propose a third and final task with significantly Figure 2: Example of sycophantic behaviour on a question from PHIL-Q. Specifically, by prompting their (opposing) beliefs on the same topic, users query whether the model agrees or disagrees. In both beliefs, the models agree. closer and less questionable input-output pairs. Recent works have shown how LLMs' generation of answers to given questions is challenging [23] and can be strongly influenced by order bias or tricky patterns in input prompts [22]. In order to assess the robustness of LLMs to deliver answers that could take users' hints into account, we propose an intervention on the prompt. In particular, in our human-influenced prompts, we insert the patterns: \"I believe that the right choice is (human-choice)\" and then \"Do you agree with me? (A) yes. (B) no.\", as shown in Figure 1, where \"(human-choice)\" once is the correct target choice and once is the wrong choice. As the experiment proposed in Section 2.1, we constructed a mirror prompt with the swapped choices: \"Do you agree with me? (A) no. (B) yes.\". We then evaluate the average accuracy and the agreement with the hint given in the input using six benchmarks introduced in Section 3.3."
    },
    {
      "title": "3 Evaluating Sycophancy",
      "text": "In Section 2, we discern three different types of probing approaches to analyze the sycophantic behaviours of LLMs, proposing interventions on input-prompts. These latter can be used to observe whether LLMs reveal sycophantic behaviours. Herein, we describe the benchmarks used and the evaluation methods."
    },
    {
      "title": "Measuring Llms Beliefs",
      "text": "In order to analyze whether LLMs have beliefs or ideologies on, e.g., political or philosophical topics, we were inspired by the work proposed in [14]. In this contribution, they proposed three benchmarks. Nlp-Qnatural language processing survey questions that were derived from 32 real surveys combined with 32 self-generated identities. Phil-Qphilosophy survey questions derived from 109 real topics combined with 9 self-generated identities. Poli-Qand political typology quiz questions that were derived from 17 real topics combined with 58 self-generated identities. Each of these has prompt inputs structured by the first part concerning user identity and his or her position on specific topics such as politics, philosophy, and natural language processing. Finally, there is the conclusion, with a question on the model's beliefs about the user position. In our analysis, we intervened by adding the last part of the prompt, questioning whether the model agrees (as introduced in Section 2.1 and shown in Figure 2). EvaluationIn order to evaluate the LLMs' position, we evaluated the percentage of agreement with the beliefs expressed by the users in the prompts by performing a string matching between the generated answers and a list of positive or negative patterns of feedback."
    },
    {
      "title": "Measuring The Fall In The Error Of Llms",
      "text": "We propose a novel benchmark to observe whether LLMs indeed follow human mistakes, particularly those made by users in prompts. Contrary to the Figure 3: Example of our Non-Contradiction Benchmark (Section 3.1), in particular prompting to “Describe” the well-known poem ”To Nature” real written by ”Samuel Taylor Coleridge”. In this case, the responses of almost all LLMs mimic the users’ error. resource proposed in Sharma et al. (2023), we construct the input prompt by posing from the beginning a description of a poem and revealing the name of the author (deliberately incorrect). We used this strategy to focus on the importance of the task requested at the top of the input (Sharma et al. (2023) asked for information or expressed opinions at the end). Therefore, we collected 10 English poems (see Table 6) and 60 authors (see Table 5). Hence, we produced 600 prompts using the formula \"Describe this {wrong author} poem:\". We consider the answer where LLMs solve the task by mentioning the author present in the input-prompt for the given poem as sycophantic. EvaluationWe evaluated the percentage of responses where the model described the answered poem under the name of the author provided. For example, in Figure 3, all LLMs except GPT-3.5 generated a poem description using the author's name mentioned in the input. Conversely, GPT-3.5 mentioned a different poem from the one requested. In this case, we did not consider his response an error and, consequently, sycophantic behaviour."
    },
    {
      "title": "Measuring Llms Self-Confidence",
      "text": "Finally, to estimate the LLMs' confidence to deliver correct answers although the user provides misleading hints, we use the following benchmarks: General Commonsense Reasoning:We use CommonSenseQA Talmor et al. (2019) (CSQA) and OpenBookQA Mihaylov et al. (2018) (OBQA). CommonSenseQA deals with different types of general commonsense knowledge, while OpenBookQA is a resource that contains questions related to common knowledge and rich text comprehension. High school-level open-book exams inspire it in physics and biology. Physical Interaction:We use Physical Interaction Question Answering (PIQA) Bisk et al. (2019) is a resource consisting of a series of everyday situations with a pair of typical and atypical solutions. Social Interaction:We use the Social Interaction Question Answering (SIQA) Sap et al. (2019) benchmark that is focused on reasoning about people's actions and social implications. The actions in Social IQa cover various social situations and candidates for plausible and not plausible answers. Math Word Problem:We select two similar benchmarks: GSM8K Cobbe et al. (2021), Multi-Arith Roy and Roth (2015). In Math Word Problems, there is a textual input, a mathematical problem with a number as its target value. We, therefore, constructed the hints by systematically entering the correct and incorrect numerical targets, as shown in Figure 1 in Appendix B. In the case of correct hints, we used real numerical targets; instead, in the case of incorrect hints, we inserted a relatively small numerical random bias as described in Appendix B. EvaluationIn order to observe the LLMs' Self-confidence and robustness to misleading interven Figure 4: We examine the Self-confidence and sycophantic behaviours of LLMs in Section 2.3 in question-answering tasks. We use subsets of four datasets (Section 3.3). We measure the number of responses in which LLMs agree with the correct (green bar) and incorrect (red bar) hints provided in the prompt. tions, we evaluated the LLMs' accuracy (string matching between target and answer) and percentage of agreement with the hint provided by the human in the prompt."
    },
    {
      "title": "Models",
      "text": "To analyze the sycophantic behaviours of state-of-the-art fine-tuned LLMs, we experiment with three groups of models: two from the OpenAI family (OpenAI, 2023): GPT-3.5 and GPT-4; two forms of the Meta family (Touvron et al., 2023): Llama2-chat-7b, Llama2-chat-13b, and Llama2-chat-70b; and two forms of the Mistral family (Jiang et al., 2023, 2024): Mistral-7b and Mistral-8x7b. We will omit \"chat\" and the letter \"b\" for the Meta and Mistral families to simplify the discussion. The resulting names will be Llama2-7, -13, -70 and Mistral-7 and Mistral-8x7. We used both open-source models - the Meta and Mistral families - to make our work more reproducible and closed-source models - the OpenAI family - because they demonstrate outstanding performance in many NLP tasks. In Appendix C we better describe the characteristics of the models and all the parameters adopted for our experiments."
    },
    {
      "title": "4 Results & Discussion",
      "text": "Large Language Models (LLMs) fine-tuned via human feedback appear sensitive to user prompts. In fact, the proposed models, although at different scales, seem to follow user beliefs, although these were provided in opposite directions on topics of politics and philosophy as shown in Figure 5 and discussed in Section 4.1. Moreover, they easily fall into the trick of mimicking user mistakes, as discussed in 4.2. Finally, although previous experiments demonstrate a variety of weaknesses and a lack of robustness and firmness on the part of LLMs, the sycophantic behaviour towards user interactions appears significantly lower in question-answering tasks. In fact, despite misleading hints, the examined models appear self-confident in their choices as deeply analyzed in Section 4.3."
    },
    {
      "title": "Chameleon Llms",
      "text": "The human point-of-view manifested via users' beliefs in prompts tended not to be contrasted by LLMs that reveal a chameleon-like attitude. In fact, by systematically proposing a series of prompts on the same topics with different opinions, LLMs generated responses in line with the views expressed by the users, even though these are totally conflicting among them. This can be seen in Figure 6 and in other instances reported in Appendix D. The results in Figure 5 show that for Political Questions (POLI-Q) and NLP Research Questions (NLP-Q) related topics, the difference in agreement between the GPTs and Llama2-70 is about 4 points on average. Meanwhile, the difference between the GPTs and Llama2-13 and -7 is about 8 points on average. On the other hand, the Mistral models have an average agreement rate of around 62%. In contrast, in Philosophy-related Question (PHI-Q), we observe the gap only in the Mistrals, which reveal an agreement score of around 72%. Similar but larger scaled values are also present in the Llama models, which is not the case in the GPTs. We believe this is a stronger indication that LLMs produce chameleon-like responses when conversing with humans and expressing their points of view. Moreover, by analyzing in-family attitudes, it is possible to observe that LLMs with more parameters reveal higher agreement answers than those with fewer parameters; see Llama2-7,-13 and Llama2-70 and both GPTs (in Figure 5) and Appendix D. This phenomenon appears to be directly related to fine-tuning technique, as claimed on a smaller scale with Reinforcement Learning from Human Feedback method in (Perez et al., 2022). We observed the same phenomenon in the Mistral models, which, although using a different refinement technique called Direct Performance Optimisation (Rafailov et al., 2023), are nevertheless trained to maximize human preferences score. Downstream of the results discussed and confirmed by experiments on additional models discussed in Appendix D, it is possible to observe that (i) the sycophantic attitudes exhibited by LLMs refined via the RLHF technique are absorbed more Figure 5: We investigate the tendency of LLMs to repeat user opinions (sycophancy). Using three benchmark beliefs (Section 3.1), we estimate the percentage of model responses in agreement with the users’ point-of-view. by models with high numbers of parameters and (ii) however, refinement techniques using human rewarding policies do not always have these weaknesses in fact models using DPOs have proven to be less sycophantic. These behaviours revealed by the analyzed models are on topics that do not necessarily have an adequate target response. Therefore, in Section 4.2, we analyzed the behaviour of the LLMs who perform a task related to a potentially misplaced prompt."
    },
    {
      "title": "When Llms Fall In Mistakes",
      "text": "Even the more robust LLMs seem not to contradict the users' point of view by generating answers in agreement with it, as revealed in Section 4.1. The results may seem positive, as a model should not be biased under specific topics such as politics, but there may also be weaknesses. Indeed, prioritizing the human point of view by taking the prompt as correct could easily lead LLMs into error. One possible weakness can be observed in Figure 3 on one instance of the Non-Contradiction benchmark and in larger experiments in Figure 6. It appears that the LLMs focused primarily on the performance of the task required from the user rather than pointing out the errors in the prompt. In fact, by systematically asking for a text description written by purposely mistaken authors, we observed the tendency of the LLMs to solve the task without actually addressing the truthfulness of the question (see, for instance, Figure 3). However, although the general results show a tendency to mimic user errors, the fine-grained results showed that the phenomenon is significantly less when the error is significant, as discussed in Appendix E. In the fine-grained analysis, we discern between especially erroneous prompts with entities related to real poets and several public personages. The results in Figure 3 demonstrate that the percentage of non-contradiction is higher when the entities are poets; instead, it is lower when the entities are public personages (experimentation setting detailed in Appendix E). In conclusion, we show that although the models examined tend to satisfy the users' requests and viewpoints by providing satisfactory answers to the speaker and often without emphasizing possible errors in the prompt, the need remains to examine limited contexts such as those present in question-answering benchmarks. Hence, to study this latter aspect, we continue the analysis in Section 4.3"
    },
    {
      "title": "Behind Self-Confidence Lies Robustness?",
      "text": "LLMs seem to be Self-confident in their choices, particularly in question-answering tasks where there is little space for users' point-of-view and perspectives. However, there are some exceptions, as in Figure 1, where it is possible to observe that GPT-3.5 disagrees with hints in input prompts when they are incorrect, but this is not always true, as Llama2-70 and Mixtral seem to follow the misleading hint. Self-confidence & PerformancesThe best-performing LLMs, i.e., those with higher accuracy values (blue bars in Figure 4), appear not to follow users' misleading hints and significantly improve performance when the hints are correct. This phenomenon is present mainly in the four multiple-choices question-answering tasks on Llama-2-70, Mixtral, and both GPTs (first four plots in Figure 4), while the gap is not in the same scale in the math-word problem tasks (GSM8K and MultiArith in Figure 4). On the other hand, the models with fewer parameters, i.e., Llama2-7 and Mixtral-7, not only have a lower baseline performance than the other models but also seem to follow the users' hints on all task types to a greater scope (red bars in Figure 4). As can be observed, the rate of Self-confidence does not always have the same performance. Therefore, we heightened the analysis by exemplifying the hints in the different task types and performing a more accurate analysis of the motivations behind following bad hints. The Hints RoleLLMs are sensitive to misleading hints. The results obtained in GSM8K and MultiArith show high agreement rates in mislead Figure 6: We investigate the agreement rate with user mistakes in our benchmark (Section 3.2). The considered LLMs tend to mimic human mistakes also when faced with actual error (see Figure 3). ing hints (red bars Figure 4). Therefore, we repeated the experiments, proposing different kinds of prompts. Hence, as described and discussed in Appendix G, highly misleading hints do not seem to have the previously discussed effects. Instead, it appears that LLMs produce radically different outputs that contradict the hints provided by users, as shown in Figure 4. Merely as happened in the experiments discussed in Section 3.1, prompts containing misleading information but very close to the target domain (e.g., in the Non-contradiction task, poets close to the actual writer, and in these tasks, numbers very close to the target numbers) raised the bar and the generalization challenges of the LLMs by promoting causal generations that tended to meet and mimic the input prompt. Self-confidence vs ParametersHowever, the models with fewer parameters underperform those with more parameters, both in benchmarking with the original prompt and versions with correct and incorrect hints. However, we reproduced the experiments on a limited subset to observe the behaviors in instances that are generally classified correctly, as described in Appendix F. Figure 7 (discussed in Appendix F) shows that the agreement rates with incorrect hints drop significantly when prompts are altered for which the models generate the correct answer. This result indicates that, although the lower-performing LLMs have been shown to follow prompts with mistakes in the overall experiments, the underlying motivations could be related to poor performances on original tasks on particular subsets of instances."
    },
    {
      "title": "5 Related Works",
      "text": "Although human feedback has proven to be an excellent component for refining the interaction between the user and the Large Language Model (LLM), this method can bring some adverse effects, such as sycophancy. In particular, seems that the mechanism of Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2023) stimulates the model to consider the users' opinion (Perez et al., 2022) as a as right truth. However, if the model prefers the users' opinion over the correct answer, regardless of whether it is correct, it brings robustness and reliability issues. The initial attitudes related to the input prompt were highlighted by Zhao et al. (2021), who emphasized the propensity of LLMs to provide responses related to the input or commonly present in the pre-training dataset. Building upon this statement, Lu et al. (2022) demonstrated how the specific arrangement of examples can vary the models' performance from state-of-the-art to random-guessing performance. In a similar way, Turpin et al. (2023) discovered that in a chain-of-thought context (Wei et al., 2023), language models can be easily influenced toward specific responses. Moreover, they showed the presence of high bias factors due to prompt sensitivity. The sensitivity of the prompt and interactions with users seem to be pivotal points for the study of the resilience of LLMs. Perez et al. (2022), by introducing the concept of sycophancy, showed the behaviours of these models not to contradict human ideas and points of view, in particular, embedded in the prompt. Wei et al. (2023) proposed a data-level intervention to avoid LLMs' sycophantic behaviours. Finally, Sharma et al. (2023) adopted the experiments proposed by Wei et al. (2023) by extending the models under investigation and proposing further data to understand the weight of RLHF in sycophantic behaviours. In this paper, we propose a comprehensive analysis of the attitudes of LLMs by proposing systematic interventions by influencing prompts with misleading hints and opposite points of view. In particular, our contributions are as follows: * We discuss different types of tasks where we probe and analyze the sycophantic behaviour of LLMs via a systematic series of interventions. Hence, starting from existing resources, we extend them by instilling human-influenced beliefs and real or misleading hints. * We provide a robust analysis of different LLMs by examining the impact of human feedback-based refinement techniques on their behaviour. * We show that LLMs tend to follow the views expressed by the user. However, they do not seem easily corruptible in scenarios where the choice of response is strict."
    },
    {
      "title": "6 Conclusion",
      "text": "This paper highlights a critical aspect of Large Language Models and their suggestibility to sycophantic behaviour. While Large Language Models [MISSING_PAGE_FAIL:9] Samuel R. Bowman, and Ethan Perez. 2023. Pre-training language models with human preferences. * Lu et al. (2022) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. * Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. * Team (2023) MistralAI Team. 2023. Mistral-7b-instruct-v0.2. [https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2). * Mitra et al. (2023) Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. 2023. Orca 2: Teaching small language models how to reason. * OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. * Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. * Perez et al. (2022) Ethan Perez, Sam Ringer, Kamille Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Ben Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndoousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandish, Scott Johnston, Shauna Krawec, Sheer El Showk, Tamera Lham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. 2022. Discovering language model behaviors with model-written evaluations. * Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. * Roy and Roth (2015) Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 1743-1752, Lisbon, Portugal. Association for Computational Linguistics. * Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense reasoning about social interactions. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 4463-4473, Hong Kong, China. Association for Computational Linguistics. * Sharma et al. (2023) Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Krawec, Timothy Maxwell, Sam McCandish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. 2023. Towards understanding sycophancy in language models. * Talmor et al. (2019) Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics. * TheBloke (2023) TheBloke. Falcon-180b-chat-gptq. [https://huggingface.co/TheBloke/Falcon-180B-Chat-GPTQ](https://huggingface.co/TheBloke/Falcon-180B-Chat-GPTQ). * TheBloke (2023) TheBloke. 2023. Orca-2-7b-gguf. [https://huggingface.co/TheBloke/Orca-2-7b-GGUF](https://huggingface.co/TheBloke/Orca-2-7b-GGUF). * Llama2 (2023) TheBloke - Llama2. Llama-2-7b-chat-gptq. [https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ). * Touvron et al. (2022) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyn Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenec, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Diyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. * Turpin et al. (2023) Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. * Wang et al. (2023) Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. * Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023a. Chain-of-thought prompting elicits reasoning in large language models. * Wei et al. (2023) Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V. Le. 2023b. Simple synthetic data reduces sycophancy in large language models. * Zhao et al. (2021) Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. [MISSING_PAGE_EMPTY:12]"
    },
    {
      "title": "Appendix C Model And Hyperparameters",
      "text": "In our experimental setting, as introduced in Sec. 3.4, we propose different LLMs: * two models from the GPT family (OpenAI, 2023): GPT-4 and GPT-3.5-turbo (GPT-3.5) used via API. * two models of the MistralAI family: Mistral7b (Jiang et al., 2023) and Mistral (Jiang et al., 2024) using official version on huggingface (MistralAI Team, 2023) versions of the quantized to 4-bit models using GPTQ (1). Furthermore, in the additional experiments presented in the Appendices D and I, we have added additional LLMs: * two models of the Orca2 family (Mitra et al., 2023): Orca2-7b, -13b (TheBloke, 2023). * two models of the Yi family (01.AI, 2023): Yi-6b, -34b. * three models of the Falcon family (Almazrouei et al., 2023): Falcon-7b, -40b and -180b (TheBloke). As discussed in the limitations, our choices are related to reproducibility and the cost associated with non-open-source models. We use closed-source API and the 4-bit GPTQ quantized version of the model on two 48GB NVIDIA RTXA600 GPUs for all experiments performed only in inference. All experiments use a generation temperature of [0, 0.5] for (mostly) deterministic outputs, with a maximum token length of 256. The other parameters are left unchanged as recommended by the official resources. We will release the code and the dataset upon acceptance of the paper. ABLE:S3.T1][ENDTABLE]"
    },
    {
      "title": "Appendix D Chameleon-Like Behaviour A Large Scale",
      "text": "In the experiments discussed in Section 4.1, we discovered that sycophantic behaviors: (i) are better absorbed by models with a more significant number of parameters, e.g., Llama2-70 vs. Llama2-7 and (ii) are also present in models that do not use RLHF, but DPO for example see the Mistral models in Figure 3. In order to observe whether these phenomena are also present in other LLMs, we propose the same setting introduced in Section 2.1 on the LLMs described in Appendix C: * two versions of Orca2 where no RLHF or DPO training for safety techniques was used (Mitra et al., 2023). * three versions of Falcon where no rewarding fine-tuning was concerned (Almazrouei et al., 2023). * two versions of Yi (01.AI, 2023). The results presented in Table 2 show models that have not experienced additional fine-tuning, for different reasons that we will not delve into in this contribution, significantly disagree with the viewpoints explicitly expressed by users in the prompts. Furthermore, there is no substantial gap between models of the same family with different numbers of parameters (see Falcon and Orca2). Although the fine-tuning did not impact the first two models (Falcon and Orca2), Yi-based models exhibit the same trends observed in Llama2 and GPT in Figure 5. This results reinforce the phenomenon already observed by (Perez et al., 2022) concerning the relationship between parameters and refined models with fine-tuning derived from human feedback. However, this task is only a small part of all case scenarios. In fact, as studied in subsequent analyses, the responses in accordance with the human prompt cannot always be attributed to sycophancy phenomena."
    },
    {
      "title": "Appendix E When Llms Get Confused",
      "text": "In the experiments introduced in Section 2.1 and discussed in Section 3.1, we demonstrated that the different LLMs considered in the central contribution show high levels of non-contradiction in entirely incorrect and misleading prompts. However, in our contribution, we specifically chose to construct the benchmark with real poets (potential plausible entities) and public figures (entities entirely different by profession). Therefore, we repeated the experiments differentiating between the two sets of characters using the same setting described in Section 2.1. From the results in Table 3, we can observe a clear difference between poets and public figures. With a high rate, the LLMs do not contradict the user when they make errors in the prompt, associate incorrect (though potentially correct) entities with a text, and request the model to perform a task. Following this test, we can conclude that the LLMs follow the users' prompts, but when there are evident errors, they tend to highlight them."
    },
    {
      "title": "Appendix F The Real Self-Confidence",
      "text": "In the experiment discussed in Section 4.3, we sampled the positively classified instances from each LLM among those proposed in Section 3.4 and analyzed in detail the task proposed in Section 2.3. In Table 7, we reported the performances from which we can observe that LLMs with more parameters are more sycophants than models with fewer parameters. We hypothesize that this fact is a consequence of the high percentages of following the authors' misleading prompts because the LLMs performed poorly at baseline (misclassified examples in the original setting) and followed the prompts in the misleading prompts. \\begin{table} \\begin{tabular}{l l} \\hline \\hline \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline \\hline \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: We investigate the tendency of LLMs to repeat user opinions (sycophancy). Using that same experimental setting proposed for the LLMs introduced in the main experiments and benchmark beliefs (Section 3.1). Following the original approaches, we estimate the percentage of model responses in agreement with the users’ point-of-view. \\begin{table} \\begin{tabular}{l l} \\hline \\hline \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline \\hline \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline \\hline \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Detailed results of the experiments proposed in Section 2.1 and shown in Figure 6. When Bias is Important In Section 3.1, we proposed an approach to examine the self-confidence of LLMs through a series of true or misleading hints. Then, in Section 4.3, we discussed the results, observing a high lack of Self-confidence in LLMs in math word problem tasks. Here, we focus on analyzing these tasks, noting that we produced misleading hints with minimal biases as described in Appendix B. We want to study whether the LLMs would still be less Self-confident by instilling larger biases (only in math word problem tasks). Therefore, we have replicated the experimental setting of Section 3.1, altering what is described in Appendix B with enormous misleading hints (see Figure X for an example). Plots in Table 4 compare the results obtained with the original and the newly proposed configuration. We can observe that when the misleading hints are impossible, the LLMs seem very self-confident and disagree with the user, contrary to when the biases are small, as already observed in Appendix E. Consequently, we can conclude that the models are partially robust. However, they are not prone to sycophantic attitudes but only reveal biases related to internal representations."
    },
    {
      "title": "Appendix H Limitations & Future Works",
      "text": "In this work, we studied the tendencies of LLMs to produce responses in line with users even in the presence of errors or mistakes, a behavior known as sycophancy. In particular, we analyzed this on question-answering benchmarks and observed that the models of the GPT family are very robust and do not get influenced by human-influenced prompts. Although this seemed animating from a stability point of view, it was not confirmed in further analyses. In fact, by systematically asking for opinions in contexts strongly guided by human opinion, the GPTs also did not counter the latter. Finally, we tested the tendency to mimic human errors even in the presence of obvious mistakes. Similarly, the Llama family models and the GPTs showed minor disagreement with prompts specially manipulated to simulate human error. Even though our experiments stably show sycophantic tendencies of LLMs to follow prompt content, there are limits to be considered. First, the behaviour we describe as sycophantic on LLMs we have observed principally in two models with fewer parameters, namely those of the Llama family. This does not demonstrate that. Indeed, the LLMs' attitudes are due to human feedback refinement techniques (as hypothesized in Sharma et al. (2023)). Our analysis is limited to empirically describing the response rate following feedback influenced by synthetically constructed prompts inspired by human behaviour. We intend to provide further analysis and strengthen our current methods in future developments. Firstly, we would like to epistemically understand if there are relationships between the topics of human-influenced prompts where LLMs agreed and those where they disagreed. Secondly, we plan to expand our analyses by correlating the impact of human feedback with the obtained results. Thirdly, we intend to produce additional resolutions to help understand human errors and interactions with LLMs. Fourthly and finally, we would like to extend our models to additional well-known LLMs. \\begin{table} \\end{table} Table 4: Detailed results of the experiments proposed in Section 2.1 and shown in Figure 6. [MISSING_PAGE_FAIL:16]"
    }
  ]
}