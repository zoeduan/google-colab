{
  "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
  "authors": [
    "Freda Shi",
    "Xinyun Chen",
    "Kanishka Misra",
    "Nathan Scales",
    "David Dohan",
    "Ed Chi",
    "Nathanael Schärli",
    "Denny Zhou"
  ],
  "abstract": "\n Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information. 1 Work done while FS and KM are student researchers at Google DeepMind. * Equal contribution 1 Google Deep-Mind \n",
  "references": [
    {
      "id": null,
      "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
      "authors": [
        "Freda Shi",
        "Xinyun Chen",
        "Kanishka Misra",
        "Nathan Scales",
        "David Dohan",
        "Ed Chi",
        "Nathanael Schärli",
        "Denny Zhou"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Program synthesis with large language models",
      "authors": [
        "J Austin",
        "A Odena",
        "M Nye",
        "M Bosma",
        "H Michalewski",
        "D Dohan",
        "E Jiang",
        "C Cai",
        "M Terry",
        "Q Le",
        "C Sutton"
      ],
      "year": "2021",
      "venue": "Program synthesis with large language models",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J D Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Look at that! BERT can be easily distracted from paying attention to morphosyntax",
      "authors": [
        "R P Chaves",
        "S N Richter"
      ],
      "year": "2021",
      "venue": "Proceedings of the Society for Computation in Linguistics 2021",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
      "authors": [
        "W Chen",
        "X Ma",
        "X Wang",
        "W W Cohen"
      ],
      "year": "2022",
      "venue": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Scaling language modeling with pathways",
      "authors": [
        "A Chowdhery",
        "S Narang",
        "J Devlin",
        "M Bosma",
        "G Mishra",
        "A Roberts",
        "P Barham",
        "H W Chung",
        "C Sutton",
        "S Gehrmann"
      ],
      "year": "2022",
      "venue": "Scaling language modeling with pathways",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "H W Chung",
        "L Hou",
        "S Longpre",
        "B Zoph",
        "Y Tay",
        "W Fedus",
        "E Li",
        "X Wang",
        "M Dehghani",
        "S Brahma"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Transformers as soft reasoners over language",
      "authors": [
        "P Clark",
        "O Tafjord",
        "K Richardson"
      ],
      "year": "2021",
      "venue": "Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Training verifiers to solve math word problems",
      "authors": [
        "K Cobbe",
        "V Kosaraju",
        "M Bavarian",
        "J Hilton",
        "R Nakano",
        "C Hesse",
        "J Schulman"
      ],
      "year": "2021",
      "venue": "Training verifiers to solve math word problems",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Selectioninference: Exploiting large language models for interpretable logical reasoning",
      "authors": [
        "A Creswell",
        "M Shanahan",
        "I Higgins"
      ],
      "year": "2022",
      "venue": "Selectioninference: Exploiting large language models for interpretable logical reasoning",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Language model cascades",
      "authors": [
        "D Dohan",
        "W Xu",
        "A Lewkowycz",
        "J Austin",
        "D Bieber",
        "R G Lopes",
        "Y Wu",
        "H Michalewski",
        "R A Saurous",
        "J Sohl-Dickstein",
        "K Murphy",
        "C Sutton"
      ],
      "year": "2022",
      "venue": "Language model cascades",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Compositional semantic parsing with large language models",
      "authors": [
        "A Drozdov",
        "N Schärli",
        "E Akyürek",
        "N Scales",
        "X Song",
        "X Chen",
        "O Bousquet",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Compositional semantic parsing with large language models",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "authors": [
        "D Dua",
        "Y Wang",
        "P Dasigi",
        "G Stanovsky",
        "S Singh",
        "M Gardner"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Program-aided language models",
      "authors": [
        "L Gao",
        "A Madaan",
        "S Zhou",
        "U Alon",
        "P Liu",
        "Y Yang",
        "J Callan",
        "G Neubig"
      ],
      "year": "2022",
      "venue": "Program-aided language models",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Natural language reasoning with first-order logic",
      "authors": [
        "S Han",
        "H Schoelkopf",
        "Y Zhao",
        "Z Qi",
        "M Riddell",
        "L Benson",
        "L Sun",
        "E Zubova",
        "Y Qiao",
        "M Burtell"
      ],
      "year": "2022",
      "venue": "Natural language reasoning with first-order logic",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Effects of varying irrelevant information on adult age differences in problem solving",
      "authors": [
        "W J Hoyer",
        "G W Rebok",
        "S M Sved"
      ],
      "year": "1979",
      "venue": "Journal of gerontology",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Adversarial examples for evaluating reading comprehension systems",
      "authors": [
        "R Jia",
        "P Liang"
      ],
      "year": "2017",
      "venue": "Adversarial examples for evaluating reading comprehension systems",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Capturing failures of large language models via human cognitive biases",
      "authors": [
        "E Jones",
        "J Steinhardt"
      ],
      "year": "2022",
      "venue": "Capturing failures of large language models via human cognitive biases",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
      "authors": [
        "N Kassner",
        "H Schütze"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "URL",
      "authors": [],
      "year": "",
      "venue": "URL",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Learning what is essential in questions",
      "authors": [
        "D Khashabi",
        "T Khot",
        "A Sabharwal",
        "D Roth"
      ],
      "year": "2017",
      "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning",
      "doi": "10.18653/v1/K17-1010"
    },
    {
      "id": "b20",
      "title": "Decomposed prompting: A modular approach for solving complex tasks",
      "authors": [
        "T Khot",
        "H Trivedi",
        "M Finlayson",
        "Y Fu",
        "K Richardson",
        "P Clark",
        "A Sabharwal"
      ],
      "year": "2022",
      "venue": "Decomposed prompting: A modular approach for solving complex tasks",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Ground-truth labels matter: A deeper look into input-label demonstrations",
      "authors": [
        "J Kim",
        "H J Kim",
        "H Cho",
        "H Jo",
        "S.-W Lee",
        "S G Lee",
        "K M Yoo",
        "T Kim"
      ],
      "year": "2022",
      "venue": "Ground-truth labels matter: A deeper look into input-label demonstrations",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Large language models are zero-shot reasoners",
      "authors": [
        "T Kojima",
        "S S Gu",
        "M Reid",
        "Y Matsuo",
        "Y Iwasawa"
      ],
      "year": "2022",
      "venue": "Large language models are zero-shot reasoners",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Adversarial examples for evaluating math word problem solvers",
      "authors": [
        "V Kumar",
        "R Maheshwary",
        "V Pudi"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Large language models with controllable working memory",
      "authors": [
        "D Li",
        "A S Rawat",
        "M Zaheer",
        "X Wang",
        "M Lukasik",
        "A Veit",
        "F Yu",
        "S Kumar"
      ],
      "year": "2022",
      "venue": "Large language models with controllable working memory",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Holistic evaluation of language models",
      "authors": [
        "P Liang",
        "R Bommasani",
        "T Lee",
        "D Tsipras",
        "D Soylu",
        "M Yasunaga",
        "Y Zhang",
        "D Narayanan",
        "Y Wu",
        "A Kumar"
      ],
      "year": "2022",
      "venue": "Holistic evaluation of language models",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
      "authors": [
        "W Ling",
        "D Yogatama",
        "C Dyer",
        "P Blunsom"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1015"
    },
    {
      "id": "b27",
      "title": "Text and patterns: For effective chain of thought, it takes two to tango",
      "authors": [
        "A Madaan",
        "A Yazdanbakhsh"
      ],
      "year": "2022",
      "venue": "Text and patterns: For effective chain of thought, it takes two to tango",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "The disturbing effect of irrelevant information on arithmetic problem solving in inattentive children",
      "authors": [
        "G M Marzocchi",
        "D Lucangeli",
        "T De Meo",
        "F Fini",
        "C Cornoldi"
      ],
      "year": "2002",
      "venue": "Developmental neuropsychology",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint",
      "authors": [
        "S Min",
        "X Lyu",
        "A Holtzman",
        "M Artetxe",
        "M Lewis",
        "H Hajishirzi",
        "L Zettlemoyer"
      ],
      "year": "2022",
      "venue": "Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "COMPS: Conceptual minimal pair sentences for testing robust property knowledge and its inheritance in pre-trained language models",
      "authors": [
        "K Misra",
        "J Rayz",
        "A Ettinger"
      ],
      "year": "2023",
      "venue": "Proceedings of the 17th Conference of the European Chapter",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp",
      "authors": [
        "J X Morris",
        "E Lifland",
        "J Y Yoo",
        "J Grigsby",
        "D Jin",
        "Y Qi"
      ],
      "year": "2020",
      "venue": "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Learning to attend on essential terms: An enhanced retriever-reader model for open-domain question answering",
      "authors": [
        "J Ni",
        "C Zhu",
        "W Chen",
        "J Mcauley"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1030"
    },
    {
      "id": "b33",
      "title": "Show your work: Scratchpads for intermediate computation with language models",
      "authors": [
        "M Nye",
        "A J Andreassen",
        "G Gur-Ari",
        "H Michalewski",
        "J Austin",
        "D Bieber",
        "D Dohan",
        "A Lewkowycz",
        "M Bosma",
        "D Luan",
        "C Sutton",
        "A Odena"
      ],
      "year": "2021",
      "venue": "Show your work: Scratchpads for intermediate computation with language models",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C L Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Sorting through the noise: Testing robustness of information processing in pretrained language models",
      "authors": [
        "L Pandia",
        "A Ettinger"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "URL",
      "authors": [],
      "year": "",
      "venue": "URL",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Working memory and intrusions of irrelevant information in a group of specific poor problem solvers",
      "authors": [
        "M C Pasolunghi",
        "C Cornoldi",
        "S De Liberto"
      ],
      "year": "1999",
      "venue": "Memory & Cognition",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Are nlp models really able to solve simple math word problems?",
      "authors": [
        "A Patel",
        "S Bhattamishra",
        "N Goyal"
      ],
      "year": "2021",
      "venue": "NAACL-HLT",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Measuring and narrowing the compositionality gap in language models",
      "authors": [
        "O Press",
        "M Zhang",
        "S Min",
        "L Schmidt",
        "N A Smith",
        "M Lewis"
      ],
      "year": "2022",
      "venue": "Measuring and narrowing the compositionality gap in language models",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Exploring the limits of transfer learning with a unified textto-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P J Liu"
      ],
      "year": "2020",
      "venue": "Journal of Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "A contrastive reading comprehension dataset for reasoning about negation",
      "authors": [
        "A Ravichander",
        "M Gardner",
        "A Marasović",
        "Condaqa"
      ],
      "year": "2022",
      "venue": "A contrastive reading comprehension dataset for reasoning about negation",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Multitask prompted training enables zero-shot task generalization",
      "authors": [
        "V Sanh",
        "A Webson",
        "C Raffel",
        "S H Bach",
        "L Sutawika",
        "Z Alyafeai",
        "A Chaffin",
        "A Stiegler",
        "T L Scao",
        "A Raja"
      ],
      "year": "2021",
      "venue": "Multitask prompted training enables zero-shot task generalization",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Natural language to code translation with execution",
      "authors": [
        "F Shi",
        "D Fried",
        "M Ghazvininejad",
        "L Zettlemoyer",
        "S I Wang"
      ],
      "year": "2022",
      "venue": "EMNLP",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Language models are multilingual chain-of-thought reasoners",
      "authors": [
        "F Shi",
        "M Suzgun",
        "M Freitag",
        "X Wang",
        "S Srivats",
        "S Vosoughi",
        "H W Chung",
        "Y Tay",
        "S Ruder",
        "D Zhou",
        "D Das",
        "J Wei"
      ],
      "year": "2022",
      "venue": "Language models are multilingual chain-of-thought reasoners",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Learning visually-grounded semantics from contrastive adversarial samples",
      "authors": [
        "H Shi",
        "J Mao",
        "T Xiao",
        "Y Jiang",
        "J Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
      "authors": [
        "K Sinha",
        "S Sodhani",
        "J Dong",
        "J Pineau",
        "W L Hamilton"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1458"
    },
    {
      "id": "b47",
      "title": "Challenging big-bench tasks and whether chain-of-thought can solve them",
      "authors": [
        "M Suzgun",
        "N Scales",
        "N Schärli",
        "S Gehrmann",
        "Y Tay",
        "H W Chung",
        "A Chowdhery",
        "Q V Le",
        "E H Chi",
        "D Zhou",
        "J Wei"
      ],
      "year": "2022",
      "venue": "Challenging big-bench tasks and whether chain-of-thought can solve them",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Generating implications, proofs, and abductive statements over natural language",
      "authors": [
        "O Tafjord",
        "B D Mishra",
        "P Clark",
        "Proofwriter"
      ],
      "year": "2020",
      "venue": "Generating implications, proofs, and abductive statements over natural language",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Adversarial glue: A multitask benchmark for robustness evaluation of language models",
      "authors": [
        "B Wang",
        "C Xu",
        "S Wang",
        "Z Gan",
        "Y Cheng",
        "J Gao",
        "A H Awadallah",
        "B Li"
      ],
      "year": "2021",
      "venue": "Adversarial glue: A multitask benchmark for robustness evaluation of language models",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Towards understanding chain-of-thought prompting: An empirical study of what matters",
      "authors": [
        "B Wang",
        "S Min",
        "X Deng",
        "J Shen",
        "Y Wu",
        "L Zettlemoyer",
        "H Sun"
      ],
      "year": "2022",
      "venue": "Towards understanding chain-of-thought prompting: An empirical study of what matters",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Rationale-augmented ensembles in language models",
      "authors": [
        "X Wang",
        "J Wei",
        "D Schuurmans",
        "Q Le",
        "E Chi",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Rationale-augmented ensembles in language models",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Self-consistency improves chain of thought reasoning in language models",
      "authors": [
        "X Wang",
        "J Wei",
        "D Schuurmans",
        "Q Le",
        "E Chi",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Self-consistency improves chain of thought reasoning in language models",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Do prompt-based models really understand the meaning of their prompts",
      "authors": [
        "A Webson",
        "E Pavlick"
      ],
      "year": "2021",
      "venue": "Do prompt-based models really understand the meaning of their prompts",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "J Wei",
        "M Bosma",
        "V Y Zhao",
        "K Guu",
        "A W Yu",
        "B Lester",
        "N Du",
        "A M Dai",
        "Q V Le"
      ],
      "year": "2021",
      "venue": "Finetuned language models are zero-shot learners",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Chain of thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "E Chi",
        "Q Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "authors": [
        "J Weston",
        "A Bordes",
        "S Chopra",
        "A M Rush",
        "B Van Merriënboer",
        "A Joulin",
        "T Mikolov"
      ],
      "year": "2015",
      "venue": "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Leastto-most prompting enables complex reasoning in large language models",
      "authors": [
        "D Zhou",
        "N Schärli",
        "L Hou",
        "J Wei",
        "N Scales",
        "X Wang",
        "D Schuurmans",
        "O Bousquet",
        "Q Le",
        "Chi",
        "E"
      ],
      "year": "2022",
      "venue": "Leastto-most prompting enables complex reasoning in large language models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "2 Related Work",
      "text": "**Few-shot prompting.** Few-shot prompting (Brown et al., 2020; Chowdhery et al., 2022, _inter alia_) has been significantly boosted with various techniques, including generating intermediate steps (Ling et al., 2017; Cobbe et al., 2021; Nye et al., 2021; Wei et al., 2022; Suzgun et al., 2022; Shi et al., 2022, _inter alia_), problem decomposition (Zhou et al., 2022; Drozdov et al., 2022; Dohan et al., 2022; Khot et al., 2022; Press et al., 2022, _inter alia_), generating programs (Austin et al., 2021; Chowdhery et al., 2022; Gao et al., 2022; Chen et al., 2022, _inter alia_), marginalizing intermediate steps that share the same result (Wang et al., 2022; Shi et al., 2022), and ensemble (Wang et al., 2022; Drozdov et al., 2022). In addition, Kojima et al. (2022) demonstrate that appropriate hint in prompts also leads to decent performance, even without any exemplar. In this work, we examine these cutting-edge prompting techniques (Wei et al., 2022; Zhou et al., 2022; Kojima et al., 2022; Wang et al., 2022) on our benchmark, and demonstrate that they are sensitive to irrelevant input context. **Natural language benchmarks with input perturbations.** There has been a long line of work on adding input perturbations for natural language tasks, including model-agnostic input transformations (Liang et al., 2022; Ravichander et al., 2022, _inter alia_) and adversarial example generation against individual models (Jia and Liang, 2017; Shi et al., 2018; Morris et al., 2020; Wang et al., 2021). In particular, prior work has constructed arithmetic reasoning benchmarks through paraphrasing or rewriting sentences in the base problems from clean datasets (Patel et al., 2021; Kumar et al., 2021). Meanwhile, Liang et al. (2022) evaluate various large language models under several metrics, including accuracy, robustness, fairness, etc. Specifically, the input transformations in their robustness evaluation include semantics-preserving and semantics-altering perturbations, such as injecting typos and modifying sentences to change the ground-truth classification labels. In contrast the above work where the meaning of problem descriptions may be changed with perturbations, we keep all sentences in the original problem description, and introduce an irrelevant sentence that is ensured not to affect the standard answer. **Natural language benchmarks with irrelevant input context.** Jia and Liang (2017) have shown that neural question answering systems are largely affected by adversarial distracting sentences, whereas follow up work (Khashabi et al., 2017; Ni et al., 2019) proposes learning strategies that mitigate the problem. Similar issues have been found for general-purpose pretrained language models, on the tasks of factual reasoning (Kassner and Schutze, 2020; Pandia and Ettinger, 2021; Misra et al., 2023; Li et al., 2022), code generation (Jones and Steinhardt, 2022), and syntactic generalization (Chaves and Richter, 2021). In particular, Li et al. (2022) evaluated T5 (Raffel et al., 2020) and PaLM (Chowdhery et al., 2022) with few-shot prompts, and proposed knowledge-aware finetuning that finetunes the model on problems with counterfactual and irrelevant context, which strengthens the model robustness to noisy context. In our evaluation, we show that without training or finetuning, adding irrelevant context into demonstrations in the prompt also mitigates the distractibility of the underlying language model and significantly improves the model performance on our GSM-IC benchmark. There exist some logical reasoning benchmarks that contain irrelevant content in task descriptions (Weston et al., 2015; Sinha et al., 2019; Clark et al., 2021; Han et al., 2022; Tafjord et al., 2020, _inter alia_). However, previous work largely focuses on designing models that require extra training, and prompting alone still hardly achieves the same level of performance as finetuned models for these tasks (Han et al., 2022; Creswell et al., 2022). In our work, we focus on arithmetic reasoning, where prompting techniques have achieved the state-of-the-art results, e.g., on GSM8K, while we show that adding a single irrelevant sentence into the problem description significantly degrades the performance. **Prompting with noisy ground truth.** A line of work studies the model performance with incorrect prompting exemplars, i.e., the example problems are paired with wrong answers (Min et al., 2022; Kim et al., 2022). In addition, prior work has investigated the model sensitivity to other parts of the prompt, such as instruction tuning with misleading and irrelevant instructions (Webson and Pavlick, 2021) and wrong reasoning steps in the examples (Madaan and Yazdanbakhsh, 2022; Wang et al., 2022). In particular, Madaan and Yazdanbakhsh (2022) conclude that the correctness of numbers and equations in chain-of-thought prompts does not play a key role in model performance, but using wrong entities and removing either equations or text explanation in the reasoning steps drastically hamper the performance. Different from this line of work, we always include correct answers to example problems in the prompt, and ensure that the irrelevant context added to the problem description does not change the ground truth answer. We show that the model performance significantly drops when presented with irrelevant context in problem descriptions, and different distributions of numbers and entities in the irrelevant context also lead to different levels of performance degradation."
    },
    {
      "title": "3 The Gsm-Ic Dataset",
      "text": "In this section, we introduce the creation process of the GSM-IC dataset (SS3.1) and the evaluation metrics (SS3.2)."
    },
    {
      "title": "Dataset Creation",
      "text": "We randomly choose 1,000 problems from the GSM8K training set as a development set. To construct our base dataset, we then choose 100 problems from this development set that can be correctly solved by at least one of the prompting techniques mentioned in this paper;3 that is, our base dataset is an \"easy\" subset of GSM8K (Table 2). Each base problem requires two to seven reasoning steps to solve.4 Among the 100 base problems, 60 of them can be solved with two reasoning steps. The full dataset statistics can be found in Appendix A. Footnote 3: We do not generate new examples or perform analysis on the test set to avoid potential tuning-on-test-set issues. Footnote 4: The number of reasoning steps of a problem is given by the number of sentences in its standard answer (Cobbe et al., 2021). We then generate the examples of our new dataset by adding to each base problem one sentence containing irrelevant information. We use a template-based method (Figure 1) to generate these sentences, which can be characterized by the following three factors: * **Topic of the inserted sentence.** We write templates for both in-topic and off-topic sentences. In-topic sentences are closely related to the topic of the original problem, whereas off-topic sentences are about a different topic. * **Role name overlap**. Most sentence templates contain some role name blanks, which can be filled with names that may or may not overlap with the role names that occur in the problem. For blank fillers that have overlap with original role names, we: (1) randomly pick a role name A from the original problem description and (2) create the blank fillers with template such as A's father and A's sister. \\begin{table} \\begin{tabular}{c c c c} \\hline \\hline & **CoT** & **LTM** & **Program** & **0-CoT** \\\\ & 95.0 & 94.0 & 83.0 & 44.0 \\\\ + SC & 96.0 & 99.0 & 91.0 & 76.0 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Accuracy (\\(\\times 100\\)) on the base 100-example dataset using code-davinci-002. See Table 3 for results with text-davinci-003. Figure 1: Illustration of the considered factors when creating the GSM-IC dataset. Best viewed in color. * **Range of numbers**. Since we focus on arithmetic reasoning, most sentence templates also contain a number blank. We can choose to fill in the number blank with a number of similar or different magnitude to those in the original problem description. Concretely, for a number \\(a\\), if there exists a number \\(b\\) in the original problem description or solution such that \\(\\frac{1}{10}\\leq\\frac{a}{b}\\leq 10\\), we consider \\(a\\) as an in-range number, and otherwise an out-of-range number. Since the standard answer to GSM8K problems are all positive integers, we only consider positive integers as the number blank fillers. We manually verify that (1) all the generated sentences are acceptable in English and that (2) adding them does not affect the standard solution of the base problem. Because the above factors are orthogonal, we generate for each base example a set of derived examples with different factor combinations. The full GSM-IC benchmark consists of 58,052 examples. More details about the dataset creation process can be found in Appendix A."
    },
    {
      "title": "Evaluation Metrics",
      "text": "For a problem \\(p\\), we denote its standard solution by \\(s(p)\\), and the solution of method \\(\\mathcal{M}\\) by \\(\\mathcal{M}(p)\\). To evaluate the distractibility of \\(\\mathcal{M}\\), we consider the following two metrics: * **Micro accuracy**\\(Acc_{micro}(\\mathcal{M};\\mathcal{P})\\) is the average accuracy of method \\(\\mathcal{M}\\) over all the test problems \\(\\mathcal{P}\\). \\[Acc_{micro}(\\mathcal{M};\\mathcal{P})=\\frac{\\sum_{p\\in\\mathcal{P}}\\mathbbm{1 }\\left[\\mathcal{M}(p)=s(p)\\right]}{|\\mathcal{P}|}\\] This means that the micro accuracy weighs all the individual test problems equally. * **Macro accuracy**\\(Acc_{macro}(\\mathcal{M};\\mathcal{B})\\) is the average accuracy of method \\(\\mathcal{M}\\) over classes of test problems, where each class \\(\\mathcal{P}(b)\\) consists of the set of test examples derived from the base example \\(b\\in\\mathcal{B}\\). We define \\(\\mathcal{M}\\)'s prediction for a class \\(\\mathcal{P}(b)\\) to be correct if and only if \\(\\mathcal{M}\\)'s prediction for all problems in this class are correct. \\[Acc_{macro}(\\mathcal{M};\\mathcal{B})=\\frac{\\sum_{b\\in\\mathcal{B}}\\mathbbm{1 }\\left[\\bigwedge_{p\\in\\mathcal{P}(b)}\\left[\\mathcal{M}(p)=s(p)\\right]\\right]} {|\\mathcal{B}|}\\] This means that the macro accuracy is the fraction of base problems that can be consistently solved no matter what irrelevant sentence is being added. * **Normalized accuracy** measures how a method is affected by the distractors, considering its accuracy on base problems. For a micro or macro accuracy \\(a_{\\mathcal{M}}\\) achieved by method \\(\\mathcal{M}\\), we calculate its corresponding normalized accuracy by \\[norm(a_{\\mathcal{M}};\\mathcal{M})=\\frac{a_{\\mathcal{M}}}{n_{\\mathcal{M}}},\\] where \\(n_{\\mathcal{M}}\\) denotes the base problem accuracy of method \\(\\mathcal{M}\\) (Table 2)."
    },
    {
      "title": "4 Investigated Solutions",
      "text": "In the following section, we review the investigated prompting techniques (SS4.1), present the formats of our prompts (SS4.2), and introduce instructed prompting (SS4.3)."
    },
    {
      "title": "Base Techniques",
      "text": "**Chain-of-thought prompting (CoT; Wei et al., 2022)** is a prompting technique that guides the language models to solve a problem in a step-by-step manner. By presenting exemplars that solve the corresponding problems with intermediate reasoning steps in the prompts, CoT significantly improves the reasoning performance over direct answer prediction without such intermediate reasoning steps. **Zero-shot chain-of-thought prompting (0-CoT; Kojima et al., 2022)** is a variation of CoT where the prompt does not contain any exemplar. Instead, the model is prompted directly with the problem of interest followed by the instruction \"_Let's think step by step:_\". **Least-to-most prompting (LtM; Zhou et al., 2022)** teaches language models to (1) break down a problem into subproblems, and (2) solve those subproblems sequentially using CoT. The final answer is that to the last subproblem. **Program prompts (Program; Chowdhery et al., 2022)** represent the arithmetic reasoning process as a program. Following prior work on solving GSM8K problems with code (Chowdhery et al., 2022; Gao et al., 2022; Chen et al., 2022), we include a Python program as the problem solution in the prompt, and execute the generated Python code using an external Python interpreter to obtain the final answer. **Self-consistency** (SC; Wang et al., 2022; Shi et al., 2022) may further boost the reasoning performance by marginalizing over intermediate reasoning steps that share the same final result. In practice, SC can be implemented by (1) sampling several solutions from the large language model and (2) taking the majority vote. Note that SC is orthogonal to above techniques, and can be combined with any of them."
    },
    {
      "title": "Prompt Design",
      "text": "We present some example prompts used in our experiments (Figure 2). For few-shot prompting techniques (i.e., CoT, LtM and Program), the input prompt includes exemplar problems and their solutions before the problem of interest. In order to keep simplicity and avoid over-fitting in prompt engineering, we follow Zhou et al. (2022) on exemplar creation; that is, we only use one simple exemplar for our main experiments. This exemplar is either based on the [Original Problem] or the [Problem with Irrelevant Context], which allows us to investigate the effect of irrelevant information in the prompt exemplar. For 0-CoT, we adhere to Kojima et al. (2022) and directly present the problem of interest followed by \"_A: Let's think step by step:_\"."
    },
    {
      "title": "Instructed Prompting",
      "text": "In addition to presenting irrelevant information in the exemplars, we also investigate whether natural language instructions help language models ignore irrelevant context and become less distracted. Extending the line of work (Suzgun et al., 2022; Sanh et al., 2021; Ouyang et al., 2022) that includes a general task description before exemplars, we add the sentence \"_Solve grade school math problems. Feel free to ignore irrelevant information given in the questions._\" before our exemplars in the prompt (Figure 2), which explicitly _instructs_ the language model to ignore irrelevant information in the problem description."
    },
    {
      "title": "5 Experiments",
      "text": "Being mindful of the experiment costs, we uniformly sample 4,000 examples from the GSM-IC dataset (denoted by GSM-IC-4K)5 for evaluation and analysis purposes throughout this paper. Unless otherwise specified, we mainly use code-davinci-002 in our experiments, and we also evaluate text-davinci-003 which is a model trained with RLHF to better follow instructions (Ouyang et al., 2022). For experiments without self-consistency decoding, we use greedy decoding (i.e., temperature \\(\\tau=0\\)); for self-consistency experiments that require multiple samples for a problem, we sample 20 responses with temperature \\(\\tau=0.7\\) following Wang et al. (2022). Footnote 5: Our sampled GSM-IC-4K covers all 100 base problems."
    },
    {
      "title": "Main Results On Gsm-Ic",
      "text": "We compare the performance of different prompting techniques on GSM-IC-4K (Table 3), in terms of both micro Figure 2: Prompt formats for the investigated techniques on the right, which are constructed from building blocks on the left (best viewed in color). The [Problem with Irrelevant Context] is obtained by adding an irrelevant sentence (_italic and underlined_) to the original problem description and it can be used as an alternative to the [Original Problem] in the prompts on the right. In these prompts, identifiers highlighted and wrapped by brackets (e.g., [Problem of Interest]) are replaced by the contents of the corresponding building blocks. The prompts for all settings can be found in Appendix C. and macro accuracies, as well as their corresponding normalized accuracies. Overall, we observe significant performance drop for both models with all prompting techniques. The drop on macro accuracy is especially large, showing that fewer than 30% of the base problems are consistently solved after adding distractors. Comparing the results of two models, text-davinci-003 achieves better normalized micro accuracy than code-davinci-002, though its macro accuracy is mostly worse. In Figure 3, we present a GSM-IC-4K example where a single irrelevant sentence causes different types of errors in investigated prompting techniques. One common error type is wrongly using the number in the irrelevant sentence, as shown in the LtM prediction and other examples in Appendix B. Even if the model does not directly use the irrelevant number for numerical calculation, the presence of the irrelevant sentence in the reasoning steps alone can still cause a wrong prediction, as shown in the CoT prediction. **LtM is generally the most robust technique to irrelevant context.** In terms of micro accuracy, LtM outperforms all other prompting methods across models. Using code-davinci-002, LtM achieves about double macro accuracy of CoT. Interestingly, with text-davinci-003, despite that LtM outperforms CoT on the micro accuracy, its macro accuracy is lower. Specifically, text-davinci-003 is highly susceptible to irrelevant context with role overlap; e.g., such irrelevant sentences decrease the macro accuracy to 0 on problems with more than 2 reasoning steps. See Table 4 for the breakdown performance on different types of irrelevant context. **Selecting exemplars with distractors mitigates the distractibility.** For few-shot prompts, we find that using exemplars with distractors (i.e., including problems with irrelevant context) consistently outperforms using the original exemplars without distractors across prompting techniques. While prior work has shown that training and fine-tuning with different types of problems improves model robustness (Li et al., 2022), our results show that prompting with exemplars that demonstrate how to ignore irrelevant context also results in significant robustness improvement. In Table 5, we further show that using exemplars with distractors does not cause a performance drop on the original GSM8K dataset, \\begin{table} \\begin{tabular}{l|c c c c|c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multicolumn{4}{c|}{**Micro Accuracy**} & \\multicolumn{4}{c}{**Macro Accuracy**} \\\\ & 2 Steps & \\(>\\)2 Steps & Overall & _Norm_ & 2 Steps & \\(>\\)2 Steps & Overall & _Norm_ \\\\ \\hline _Prompting Exemplar w/o Irrelevant Context_, & code-davinci-002 & & & & & & \\\\ CoT & 73.5 & 70.8 & 72.4 & 76.2 & 8.3 & 2.5 & 6.0 & _6.3_ \\\\ CoT + Inst. & 79.0 & 76.0 & 77.8 & _81.8_ & 20.0 & 7.0 & 15.0 & _15.8_ \\\\ 0-COT & 29.0 & 29.1 & 29.0 & _65.9_ & 1.7 & 0.0 & 1.0 & _2.3_ \\\\ 0-COT +Inst. & 31.6 & 28.8 & 30.5 & _69.3_ & 1.7 & 0.0 & 1.0 & _2.3_ \\\\ LtM & 74.9 & **81.5** & 77.5 & _82.4_ & 16.7 & 20.0 & 18.0 & _19.1_ \\\\ LtM + Inst. & **80.1** & 81.3 & **80.6** & _85.7_ & **18.3** & **35.0** & **25.0** & _26.6_ \\\\ Program & 59.1 & 47.4 & 54.4 & _65.5_ & 6.7 & 2.5 & 5.0 & _6.0_ \\\\ Program + Inst. & 60.6 & 50.9 & 56.7 & _68.3_ & 6.7 & 5.0 & 6.0 & _7.2_ \\\\ \\hline CoT + SC & 87.6 & 90.1 & 88.1 & _91.8_ & 29.0 & 28.3 & 30.0 & _31.3_ \\\\ 0-COT + SC & 61.6 & 68.4 & 64.3 & _84.6_ & 0.0 & 2.5 & 1.0 & _1.3_ \\\\ LtM + SC & **92.4** & **94.8** & **93.4** & **94.3** & **51.6** & **35.0** & **45.0** & **45.5** \\\\ Program + SC & 73.5 & 76.1 & 74.6 & _82.0_ & 16.7 & 7.5 & 13.0 & _14.3_ \\\\ \\hline _Prompting Exemplar w/o Irrelevant Context_, & text-davinci-003 & & & & & & & \\\\ CoT & 69.3 & 66.9 & 68.4 & _85.4_ & 10.0 & 7.5 & 9.0 & _11.3_ \\\\ CoT + Inst. & 72.0 & 70.3 & 71.3 & _89.1_ & **11.7** & **12.5** & **12.0** & _15.0_ \\\\ LtM & 78.0 & **73.6** & 76.3 & _94.2_ & 5.0 & 0.0 & 5.0 & _6.2_ \\\\ LtM + Inst. & **80.5** & 70.9 & **76.7** & _94.7_ & 5.0 & 0.0 & 5.0 & _6.2_ \\\\ \\hline _Prompting Exemplar w/ Irrelevant Context_, & code-davinci-002 & & & & & & & \\\\ CoT & 79.8 & 72.4 & 76.8 & _80.8_ & 16.7 & 10.0 & 14.0 & _14.7_ \\\\ CoT + Inst. & 80.5 & 74.4 & 78.1 & _82.2_ & 20.0 & 12.0 & 17.0 & _17.9_ \\\\ LtM & 78.1 & 84.6 & 80.7 & _85.9_ & **23.3** & **35.0** & **28.0** & _29.8_ \\\\ LtM + Inst. & **81.0** & **85.4** & **82.8** & _88.1_ & **23.3** & **35.0** & **28.0** & _29.8_ \\\\ Program & 67.0 & 55.0 & 62.2 & _74.9_ & 11.7 & 5.0 & 9.0 & _10.8_ \\\\ Program + Inst. & 68.8 & 54.8 & 63.2 & _76.1_ & 15.0 & 7.5 & 12.0 & _14.5_ \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Micro and macro accuracies (\\(\\times 100\\)) on the GSM-IC-4K dataset. SC denotes self-consistency. _Norm_ is the overall accuracy normalized by the fraction of solved base problems (Table 2), which is a measure for robustness w.r.t. irrelevant information. For text-davinci-003, the base problem accuracy with CoT is 80.0, and the base problem accuracy with LtM is 81.0. The best numbers in each column for each section (i.e., whether using code-davinci-002 or text-davinci-003, whether using exemplar with irrelevant context or not, and whether using self-consistency or not) are in **boldface**. indicating that such a prompt design can be beneficial in achieving better accuracy and robustness simultaneously. **Self-consistency significantly reduces the distractibility.** Taking the majority vote from 20 samples,6 SC improves the overall micro accuracy by more than 11 percentage points. This means that in addition to improving model performance on clean arithmetic reasoning tasks (Wang et al., 2022c), SC also substantially reduces the distractibility of large language models to irrelevant context. The gain on micro accuracy is notably large on 0-CoT (35.5 percentage points). Furthermore, the correct answer for 99.7% of the problems is in the 20 sampled answers for both CoT and LtM. Even for 0-CoT, the recall of correct solutions within 20 samples is 96.5%. Despite these improvements, the best macro accuracy among all prompting techniques is only \\(45\\%\\), suggesting that for more than half of the base problems, SC fails to prevent the model from being distracted by different variants of irrelevant information. These results imply that a better algorithm may be developed to further reduce the distractibility based on a few sampled solutions. Footnote 6: If there is a tie, we take a random top-tier result for evaluation, following Wang et al. (2022c) and Shi et al. (2022a)."
    },
    {
      "title": "Break-Down Analysis",
      "text": ""
    },
    {
      "title": "5.2.1 Factors Of The Irrelevant Context",
      "text": "We analyze the performance of CoT, LtM and Program with respect to the considered factors (SS3.1) of the irrelevant sentences (Table 4). For both models, we find that (1) in-topic sentences with (2) role name overlap and (3) in-range numbers are generally more challenging, which is examplified by Figure 3. For LtM, the latter two factors do not have a large effect on the micro accuracy. The difference is more significant for the macro accuracy and, as an anomaly, using distractors with in-range numbers turns out to be less challenging than out-of-range numbers when using irrelevant context in the exemplar. Again, with code-davinci-002, LtM outperforms CoT and Program on all investigated sub-categories. On the other hand, using text-davinci-003, LtM outperforms CoT in terms of the micro accuracy, but the macro accuracy is much lower on all sub-categories."
    },
    {
      "title": "5.2.2 Break-Down Accuracies W.R.T. # Steps",
      "text": "We analyze the break-down accuracies for problems with respect to the reasoning steps (Figure 4). While we see a significant drop for CoT and Program on problems that require four or more steps in the reasoning process, the performance of LtM is fairly consistent across difficulty. In addition to the advantage of LtM on clean problems for complicated reasoning (Zhou et al., 2022), our results show that LtM is also less sensitive to irrelevant context for complicated problems that require more steps to solve."
    },
    {
      "title": "Instructed Prompting Improves Robustness To Irrelevant Context",
      "text": "We have shown that using exemplars with distractors improves robustness to irrelevant context. We also compare the performance of instructed prompting and that of the prompts without instructions in Table 3. Adding instructions to CoT, LtM, and Program consistently improves their performance. Surprisingly, instructed prompting with Figure 4: Micro accuracies on GSM-IC-4K with respect to the number of required reasoning steps. Figure 3: Example problem and corresponding outputs by different prompting techniques (best viewed in color). The CoT answer to the original problem is highlighted in green. The added irrelevant sentence is in _italic and highlighted in red_, which causes different errors (highlighted in yellow) for all prompting techniques. More examples of model predictions can be found in Appendix B. original exemplars reaches comparable or even better performance than uninstructed prompting that uses exemplars with distractors for both CoT and LtM. Note that adding the instruction _\"Solve grade school math problems.\"_ alone does not significantly improve the performance, and it is the instruction _\"Feel free to ignore irrelevant information given in the questions.\"_ that makes the difference. Similar to the instruction _\"Let's think step by step.\"_ employed by 0-CoT, this shows that language models are--to some extent--able to follow natural language instructions in a way that dramatically changes their problem solving behavior, suggesting that such instructions may be useful for guiding the behavior of language models on more tasks. On the original GSM8K development set (Cobbe et al., 2021; Zhou et al., 2022), we do not observe a drop in accuracy when using exemplars with irrelevant information, adding natural language instructions, or both (Table 5). The same holds for SVAMP (Patel et al., 2021), an arithmetic reasoning benchmark constructed by applying different types of variations to math problems from existing clean datasets, e.g., changing sentence structures, asking different questions with the same information, etc. This is impressive because the results on GSM-IC show that prompt exemplars with irrelevant information and instructed prompting both improve robustness. For the Program prompt, we find that using exemplars with distractors even increases performance on SVAMP."
    },
    {
      "title": "Complicated Prompts May Hurt The Robustness To Irrelevant Context",
      "text": "We compare our 1-exemplar CoT prompt (Figure 2) to a 4-exemplar prompt (Appendix D of Zhou et al., 2022), which is reported as the best-performing CoT prompt on GSM8K, on GSM-IC (Table 6). Note that the 1-exemplar CoT prompt only includes a problem with a 2-step solution, while the 4-exemplar prompt includes problems that require more reasoning steps. While the 4-exemplar prompt leads to better performance on the original GSM8K development set, the 4-exemplar prompt is surprisingly more susceptible to the distraction provided by the irrelevant context. In particular, the 4-exemplar prompt is consistently worse than the 1-exemplar prompt on problems with more than 2 intermediate steps. Even for 2-step prompts, the accuracy \\begin{table} \\begin{tabular}{c|c c c c c c|c c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multicolumn{6}{c|}{**Micro Accuracy**} & \\multicolumn{6}{c}{**Macro Accuracy**} \\\\ \\cline{2-11} & \\multicolumn{2}{c}{**Topic**} & \\multicolumn{2}{c}{**Role Overlap**} & \\multicolumn{2}{c|}{**Num. Range**} & \\multicolumn{2}{c}{**Topic**} & \\multicolumn{2}{c}{**Role Overlap**} & \\multicolumn{2}{c}{**Num. Range**} \\\\ \\cline{2-11} & In & Off & Yes & No & In & Out & In & Off & Yes & No & In & Out \\\\ \\hline \\multicolumn{11}{l}{_Prompting Exemplar w/o Irrelevant Context (code-davinci-002)_} \\\\ CoT & 63.1 & 80.7 & 68.3 & 76.6 & 70.2 & 74.6 & 10.2 & 33.0 & 10.3 & 22.2 & 11.0 & 19.0 \\\\ LtM & **70.8** & **83.4** & **77.0** & **78.2** & **77.2** & **77.8** & **23.5** & **45.0** & **25.8** & **35.4** & **27.0** & **29.0** \\\\ Program & 44.1 & 63.5 & 50.7 & 58.4 & 54.3 & 54.5 & 4.1 & 24.0 & 9.3 & 16.2 & 7.0 & 11.0 \\\\ \\hline \\multicolumn{11}{l}{_Prompting Exemplar w/o Irrelevant Context (text-davinci-003)_} \\\\ CoT & 63.3 & 72.9 & 68.7 & 68.1 & 67.2 & 69.6 & **16.3** & **36.0** & **17.5** & **20.2** & **19.0** & **22.0** \\\\ LtM & **75.4** & **76.9** & **75.6** & **76.8** & **75.3** & **77.2** & 6.1 & 7.0 & 6.2 & 9.1 & 6.0 & 6.0 \\\\ \\hline \\multicolumn{11}{l}{_Prompting Exemplar w/ Irrelevant Context (code-davinci-002)_} \\\\ CoT & 70.2 & 82.7 & 73.6 & 80.2 & 76.1 & 77.7 & 18.4 & 43.0 & 21.6 & 32.3 & 22.0 & 26.0 \\\\ LtM & **73.0** & **87.5** & **81.4** & **80.2** & **80.0** & **81.4** & **28.6** & **58.0** & **37.1** & **42.4** & **41.0** & **35.0** \\\\ Program & 52.9 & 70.5 & 60.2 & 64.5 & 61.5 & 62.8 & 10.2 & 37.0 & 14.4 & 23.2 & 15.0 & 17.0 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Breakdown accuracies (\\(\\times 100\\)) w.r.t. the factors of the added irrelevant sentence. Lower accuracy indicates the model is more fragile to the corresponding type of irrelevant contexts. Note that the macro average accuracies are higher than the corresponding ones reported in Table 3, as we only include a subset of created problems (i.e., those corresponding to the appropriate factor) here to compute the metric. The best result in each column is in **boldface**. \\begin{table} \\begin{tabular}{c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multicolumn{2}{c}{**Exemplar**} & \\multicolumn{2}{c}{**Accuracy**} \\\\ & w/ IrrCtx? & GSM8K Dev. & SVAMP Test \\\\ \\hline CoT & ✓ & 59.3 & 79.1 \\\\ & ✗ & 60.3 & 77.6 \\\\ \\multicolumn{11}{l}{\\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} } } \\\\ CoT & ✓ & 59.3 & 79.1 \\\\ + Inst. & ✗ & 58.8 & 78.7 \\\\ \\hline LtM & ✓ & 61.9 & 76.9 \\\\ & ✗ & 59.8 & 76.6 \\\\ \\multicolumn{11}{l}{\\multirow{2}{*}{\\begin{tabular}{} \\end{tabular} } } \\\\ LtM & ✓ & 60.9 & 76.2 \\\\ + Inst. & ✗ & 60.3 & 76.3 \\\\ \\hline Program & ✓ & 58.6 & 80.0 \\\\ & ✗ & 59.8 & 77.3 \\\\ \\multicolumn{11}{l}{\\multirow{2}{*}{ \\begin{tabular}{} \\end{tabular} } } \\\\ Program & ✓ & 59.2 & 77.9 \\\\ + Inst. & ✗ & 61.1 & 77.8 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: Accuracies (\\(\\times 100\\)) on the GSM8K development set and the SVAMP test set. IrrCtx denotes irrelevant contexts, and +Inst. denotes instructed prompting. The baseline results (i.e., those using the simplest exemplars without irrelevant context and without instructions) are underlined. improvement from adding more exemplars is almost negligible when using instructions (79.0 vs 79.2). Overall, this finding indicates that adding more exemplars can make the prompt less robust as it leads to some overfitting."
    },
    {
      "title": "Extension To Drop",
      "text": "In addition to GSM-IC, we extend our evaluation to the DROP dataset (Dua et al., 2019), where the task is to answer a question according to a long passage that naturally contains irrelevant context. We show an example about football games in Table 8. We use the CoT and LtM prompts in (Zhou et al., 2022) as the baselines, and we evaluate the prompt variants with the instruction _\"Solve following questions. Feel free to ignore irrelevant information given in the questions.\"_ added before the exemplars. Note that by adding a problem reduction step in the exemplar solution, the least-to-most prompt implicitly leads the model to come up with relevant subproblems to solve the given problem. Again, we observe that the instruction consistently improves the performance of both CoT and LtM prompting (Table 7)."
    },
    {
      "title": "6 Conclusion And Discussion",
      "text": "In this work, we introduce GSM-IC, a dataset that supports comprehensive study of the distractibility of large language models when performing arithmetic reasoning in presence of irrelevant contexts. We examine a variety of prompting techniques on GSM-IC, and demonstrate that they are all sensitive to the irrelevant information in the problems. Among the studied techniques, self-consistency (Wang et al., 2022c) leads to a substantial improvement in robustness to irrelevant context across the board, and presenting example problems with irrelevant context in the prompt also consistently improves the performance. Similarly, we find that simply adding an instruction to ignore irrelevant information brings notable performance gains on our benchmark. Despite the improvement achieved by these methods, the fundamental issue remains: a single piece of irrelevant information can distract the models and substantially degrade their performance, even on problems whose clean versions they correctly solve. We encourage researchers to also prioritize improving on this fundamental limitation when developing new training and prompting techniques. We leave further investigation on the distractibility for other tasks and different language models for future work."
    },
    {
      "title": "Acknowledgement",
      "text": "We would like to thank Dale Schuurmans, Olivier Bousquet and Jack Nikodem for helpful discussion and feedback. \\begin{table} \\begin{tabular}{c c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & **\\#Prompting** & **GSM8K** & **GSM-IC-4K** \\\\ & **Exemplars** & Dev. & 2 Steps & \\(>\\) 2 Steps \\\\ \\hline CoT & 1 & 60.3 & 73.6 & 70.8 \\\\ & 4 & 66.3 & 78.0 & 69.4 \\\\ \\hline CoT & 1 & 58.8 & 79.0 & **76.0** \\\\ + Inst. & 4 & **66.5** & **79.2** & 70.6 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: Micro accuracies (\\(\\times 100\\)) on the GSM8K development set and GSM-IC-4K. # Prompting exemplars denotes the number of exemplars used in the prompt. The best number in each column is in **boldface**."
    },
    {
      "title": "References",
      "text": "* Austin et al. (2021) Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and Sutton, C. Program synthesis with large language models. _arXiv preprint arXiv:2108.07732_, 2021. * Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. _NeurIPS_, 2020. * Chaves & Richter (2021) Chaves, R. P. and Richter, S. N. Look at that! BERT can be easily distracted from paying attention to morphosyntax. In _Proceedings of the Society for Computation in Linguistics 2021_, pp. 28-38, Online, February 2021. Association for Computational Linguistics. URL [https://aclanthology.org/2021.scil-1.3](https://aclanthology.org/2021.scil-1.3). * Chen et al. (2022) Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. _arXiv preprint arXiv:2211.12588_, 2022. * Chowdhery et al. (2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022. * Chung et al. (2021) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022. * Clark et al. (2021) Clark, P., Tafjord, O., and Richardson, K. Transformers as soft reasoners over language. In _Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence_, pp. 3882-3890, 2021. * Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021. URL [https://arxiv.org/pdf/2110.14168](https://arxiv.org/pdf/2110.14168). * Creswell et al. (2022) Creswell, A., Shanahan, M., and Higgins, I. Selection-inference: Exploiting large language models for interpretable logical reasoning. _arXiv preprint arXiv:2205.09712_, 2022. * Dohan et al. (2022) Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-Dickstein, J., Murphy, K., and Sutton, C. Language model cascades. _arXiv preprint arXiv:2207.10342_, 2022. * Drozdov et al. (2022) Drozdov, A., Scharli, N., Akyurek, E., Scales, N., Song, X., Chen, X., Bousquet, O., and Zhou, D. Compositional semantic parsing with large language models. _arXiv preprint arXiv:2209.15003_, 2022. * Dua et al. (2019) Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, 2019. * Gao et al. (2022) Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models. _arXiv preprint arXiv:2211.10435_, 2022. * Han et al. (2022) Han, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Benson, L., Sun, L., Zubova, E., Qiao, Y., Burtell, M., et al. Folio: Natural language reasoning with first-order logic. _arXiv preprint arXiv:2209.00840_, 2022. * Hoyer et al. (1979) Hoyer, W. J., Rebok, G. W., and Sved, S. M. Effects of varying irrelevant information on adult age differences in problem solving. _Journal of gerontology_, 34(4):553-560, 1979. * Jia & Liang (2017) Jia, R. and Liang, P. Adversarial examples for evaluating reading comprehension systems. _arXiv preprint arXiv:1707.07328_, 2017. * Jones & Steinhardt (2022) Jones, E. and Steinhardt, J. Capturing failures of large language models via human cognitive biases. _arXiv preprint arXiv:2202.12299_, 2022. * Kassner & Schutze (2020) Kassner, N. and Schutze, H. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 7811-7818, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.698. URL [https://aclanthology.org/2020.acl-main.698](https://aclanthology.org/2020.acl-main.698). * Khashabi et al. (2017) Khashabi, D., Khot, T., Sabharwal, A., and Roth, D. Learning what is essential in questions. In _Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)_, pp. 80-89, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/K17-1010. URL [https://aclanthology.org/K17-1010](https://aclanthology.org/K17-1010). * Khot et al. (2022) Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A. Decomposed prompting: A modular approach for solving complex tasks. _arXiv preprint arXiv:2210.02406_, 2022. Kim, J., Kim, H. J., Cho, H., Jo, H., Lee, S.-W., Lee, S.-g., Yoo, K. M., and Kim, T. Ground-truth labels matter: A deeper look into input-label demonstrations. _arXiv preprint arXiv:2205.12685_, 2022. * Kojima et al. (2022) Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. _arXiv preprint arXiv:2205.11916_, 2022. * Kumar et al. (2021) Kumar, V., Maheshwary, R., and Pudi, V. Adversarial examples for evaluating math word problem solvers. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pp. 2705-2712, 2021. * Li et al. (2022) Li, D., Rawat, A. S., Zaheer, M., Wang, X., Lukasik, M., Veit, A., Yu, F., and Kumar, S. Large language models with controllable working memory. _arXiv preprint arXiv:2211.05110_, 2022. * Liang et al. (2022) Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al. Holistic evaluation of language models. _arXiv preprint arXiv:2211.09110_, 2022. * Ling et al. (2017) Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 158-167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL [https://aclanthology.org/P17-1015](https://aclanthology.org/P17-1015). * Madaan & Yazdanbakhsh (2022) Madaan, A. and Yazdanbakhsh, A. Text and patterns: For effective chain of thought, it takes two to tango. _arXiv preprint arXiv:2209.07686_, 2022. * Marzocchi et al. (2002) Marzocchi, G. M., Lucangeli, D., De Meo, T., Fini, F., and Cornoldi, C. The disturbing effect of irrelevant information on arithmetic problem solving in inattentive children. _Developmental neuropsychology_, 21(1):73-92, 2002. * Min et al. (2022) Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? _arXiv preprint arXiv:2202.12837_, 2022. * Misra et al. (2023) Misra, K., Rayz, J., and Ettinger, A. COMPS: Conceptual minimal pair sentences for testing robust property knowledge and its inheritance in pre-trained language models. In _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics_, 2023. * Morris et al. (2020) Morris, J. X., Lifland, E., Yoo, J. Y., Grigsby, J., Jin, D., and Qi, Y. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. _arXiv preprint arXiv:2005.05909_, 2020. * Ni et al. (2019) Ni, J., Zhu, C., Chen, W., and McAuley, J. Learning to attend on essential terms: An enhanced retriever-reader model for open-domain question answering. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 335-344, Minneapolis, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1030. URL [https://aclanthology.org/N19-1030](https://aclanthology.org/N19-1030). * Nye et al. (2021) Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A. Show your work: Scratchpads for intermediate computation with language models. _arXiv preprint arXiv:2112.00114_, 2021. * Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022. URL [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155). * Panda & Ettinger (2021) Panda, L. and Ettinger, A. Sorting through the noise: Testing robustness of information processing in pre-trained language models. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 1583-1596, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 119. URL [https://aclanthology.org/2021.emnlp-main.119](https://aclanthology.org/2021.emnlp-main.119). * Pasolunghi et al. (1999) Pasolunghi, M. C., Cornoldi, C., and De Liberto, S. Working memory and intrusions of irrelevant information in a group of specific poor problem solvers. _Memory & Cognition_, 27:779-790, 1999. * Patel et al. (2021) Patel, A., Bhattamisha, S., and Goyal, N. Are nlp models really able to solve simple math word problems? In _NAACL-HLT_, 2021. URL [https://aclanthology.org/2021.naacl-main.168.pdf](https://aclanthology.org/2021.naacl-main.168.pdf). * Press et al. (2022) Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. Measuring and narrowing the compositionality gap in language models. _arXiv preprint arXiv:2210.03350_, 2022. * Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 2020. URL [https://jmlr.org/papers/v21/20-074.html](https://jmlr.org/papers/v21/20-074.html). * Ravichander et al. (2022) Ravichander, A., Gardner, M., and Marasovic, A. Condaqa: A contrastive reading comprehension dataset for reasoning about negation. _arXiv preprint arXiv:2211.00295_, 2022. * Sanh et al. (2021) Sanh, V., Webon, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. _arXiv preprint arXiv:2110.08207_, 2021. * Shi et al. (2022a) Shi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and Wang, S. I. Natural language to code translation with execution. In _EMNLP_, 2022a. * Shi et al. (2022b) Shi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S., Vosoughi, S., Chung, H. W., Tay, Y., Ruder, S., Zhou, D., Das, D., and Wei, J. Language models are multilingual chain-of-thought reasoners. _arXiv preprint arXiv:2210.03057_, 2022b. URL [https://arxiv.org/pdf/2210.03057](https://arxiv.org/pdf/2210.03057). * Shi et al. (2018) Shi, H., Mao, J., Xiao, T., Jiang, Y., and Sun, J. Learning visually-grounded semantics from contrastive adversarial samples. In _Proceedings of the 27th International Conference on Computational Linguistics_, pp. 3715-3727, 2018. * Sinha et al. (2019) Sinha, K., Sodhani, S., Dong, J., Pineau, J., and Hamilton, W. L. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 4506-4515, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1458. URL [https://aclanthology.org/D19-1458](https://aclanthology.org/D19-1458). * Suzgun et al. (2022) Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them. _arXiv preprint arXiv:2210.09261_, 2022. * Tafjord et al. (2020) Tafjord, O., Mishra, B. D., and Clark, P. Proofwriter: Generating implications, proofs, and abductive statements over natural language. _arXiv preprint arXiv:2012.13048_, 2020. * Wang et al. (2021) Wang, B., Xu, C., Wang, S., Gan, Z., Cheng, Y., Gao, J., Awadallah, A. H., and Li, B. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. _arXiv preprint arXiv:2111.02840_, 2021. * Wang et al. (2022a) Wang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., and Sun, H. Towards understanding chain-of-thought prompting: An empirical study of what matters. _arXiv preprint arXiv:2212.10001_, 2022a. * Wang et al. (2022b) Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-augmented ensembles in language models. _arXiv preprint arXiv:2207.00747_, 2022b. * Wang et al. (2022c) Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_, 2022c. * Webson & Pavlick (2021) Webson, A. and Pavlick, E. Do prompt-based models really understand the meaning of their prompts? _arXiv preprint arXiv:2109.01247_, 2021. * Wei et al. (2021) Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021. * Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In _NeurIPS_, 2022. URL [https://openreview.net/pdf?id=_VjQ1MeSB_J](https://openreview.net/pdf?id=_VjQ1MeSB_J). * Weston et al. (2015) Weston, J., Bordes, A., Chopra, S., Rush, A. M., Van Merrienboer, B., Joulin, A., and Mikolov, T. Towards ai-complete question answering: A set of prerequisite toy tasks. _arXiv preprint arXiv:1502.05698_, 2015. * Zhou et al. (2022) Zhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E. Least-to-most prompting enables complex reasoning in large language models. _arXiv preprint arXiv:2205.10625_, 2022. URL [https://arxiv.org/pdf/2205.10625](https://arxiv.org/pdf/2205.10625)."
    },
    {
      "title": "Appendix A Gsm-Ic Details",
      "text": "Each of the 100 base problem require two to seven steps to solve (Figure 5). Starting from the base problems, we follow the protocols below to create GSM-IC (SS3.1). 1. **Irrelevant sentence template**. 1. For in-topic sentences, we manually write templates within the topic that is close to the original problem description. We are particularly careful about the shareable stuff, for example, money is sometimes considered shareable between family members. In such cases, we make sure that the added do not change the amount of shareable stuff to ensure that the final standard answer is not affected. 2. For off-topic sentences, we use general templates (Table 9) for all problems unless some of them can be considered as in-topic sentences for some problems--for example, the sentence \"_The height of \\(\\{\\)role\\(\\}\\) is \\(\\{\\)number\\(\\}\\) feet._\" is considered as an in-topic sentence for problems about heights of people. 3. We make sure that all sentences derived by each template are grammatical English sentences. 4. We write four in-topic and choose four off-topic distractor sentence templates for each problem. 2. **Blank fillers: role names**. 1. We randomly choose a role name X, and use X's father, X's mother, X's brother, X's sister and X's neighbor as the overlapped role names. 2. We choose from the name set \\(\\{\\)Ada, David, Emma, Jack, John, Mary, Max, Tom\\(\\}\\) for non-overlapped role names. 3. We write five names that have overlap with the original character, and five names that do not have overlap for each problem. 3. **Blank fillers: numbers**. 1. For in-range numbers, we randomly sample positive integers in the range of \\([\\frac{\\ell}{10},10r]\\), where \\(\\ell\\) and \\(r\\) denote the smallest and the largest number that appear in the problem description and standard solution, respectively. \\begin{table} \\begin{tabular}{l} \\hline The shoe size of \\(\\{\\)ROLE\\(\\}\\) is \\(\\{\\)NUMBER\\(\\}\\). \\\\ \\(\\{\\)ROLE\\(\\}\\) is \\(\\{\\)NUMBER\\(\\}\\) years old. \\\\ The height of \\(\\{\\)ROLE\\(\\}\\) is \\(\\{\\)NUMBER\\(\\}\\) feet. \\\\ \\(\\{\\)ROLE\\(\\}\\) bought \\(\\{\\)NUMBER\\(\\}\\) tomatoes from the grocery store. \\\\ \\(\\{\\)ROLE\\(\\}\\) has read \\(\\{\\)NUMBER\\(\\}\\) books in the past year. \\\\ \\hline \\end{tabular} \\end{table} Table 9: Off-topic sentence templates for GSM-IC. Figure 5: Base problem distribution of GSM-IC with respect to the number of reasoning steps in the ground truth problem solution. [MISSING_PAGE_EMPTY:14] \\begin{table} \\begin{tabular}{c l l} \\hline \\hline **Method** & **Original GSM8K Problem** & **Modified Problem (w/ Irrelevant Sentence)** \\\\ & Jessica is six years older than Claire. In two years, Claire will be 20 years old. How old is Jessica now? & Jessica is six years older than Claire. In two years, Claire will be 20 years old. Twenty years ago, the age of Claire’s father is 3 times of Jessica’s age. How old is Jessica now? \\\\ \\hline CoT & In two years, Claire will be 20 years old. So, Claire is 18 years old now. Jessica is six years older than Claire, so Jessica is 18 + 6 = 24 years old now. The answer is 24. & Jessica is six years older than Claire, so Jessica is 20 - 6 = 14 years old. Twenty years ago, the age of Claire’s father is 3 times of Jessica’s age, so 20 years ago, the age of Claire’s father is 3 * 14 = 42 years old. The answer is 14. \\\\ \\hline LrM & Let’s break down this problem: 1. How old is Claire now? 2. How old is Jessica now? 1. In two years, Claire will be 20 years old. So Claire is 20 - 2 = 18 years old now. & Jessica is 18 years old now. \\\\ & 2. Jessica is six years older than Claire. So Jessica is 18 + 6 = 24 years old now. & Jessica’s age. So 20 years ago, Claire’s father is 3 * Jessica’s age. So Claire’s father is 3 * Jessica’s age - 20 years old now. So Claire’s father is 3 * Jessica’s age - 20 + 18 = 3 * Jessica’s age - 2 = years old now. So Claire’s father is 3 * Jessica’s age - 2 + 6 = 3 * Jessica’s age - 14 years old now. So Claire’s father is 3 * Jessica’s age + 4 - 18 = 3 * Jessica’s age - 14 years old now. So Claire’s father is 3 * Jessica’s age + 4 - 18 = 3 * Jessica’s age - 14 years old now. So Claire’s father is 3 * Jessica’s age + 4 - 18 = 3 * Jessica’s age - 14 years old now. So 3 * Jessica’s age + 6 = 3 * Jessica’s age - 2 + 6 = 3 * Jessica’s age + 4. So 3 * Jessica’s age + 4. So 3 * Jessica’s age + 4. So 3 * Jessica’s age + 6 = 4. So Jessica’s age - 2. So Jessica is 2 years old now. \\\\ & The answer is 24. & The answer is 2. \\\\ \\hline Program & Claire\\_age = 20 - 2 & Claire\\_age = 20 - 2 \\\\ & Jessica\\_age = Claire\\_age + 6 & Jessica\\_age = Claire\\_age - 6 \\\\ & print(Jessica\\_age) & Claire\\_father\\_age = 3 * Jessica\\_age \\\\ & & print(Jessica\\_age) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 11: Output example #1 for different prompting techniques, on a GSM8K problem and a corresponding GSM-IC problem. While all techniques give correct answers to the original problem, they output different different results for the modified GSM-IC problem. [MISSING_PAGE_FAIL:16] \\begin{table} \\begin{tabular}{c c c} \\hline \\hline **Method** & **Example** & **Prompt** \\\\ & w/ IrrCtx? & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? \\\\ \\hline CoT & ✗ & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? \\\\ & & A: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12 apples together. The answer is 12. \\\\ & & Q: [Problem of Interest] \\\\ & & A: \\\\ \\hline CoT & ✓ & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together? \\\\ & & A: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12 apples together. The answer is 12. \\\\ & & Q: [Problem of Interest] \\\\ & & A: \\\\ \\hline LTM & ✗ & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? \\\\ & & A: Let’s break down this problem: 1. How many apples does Anna have? 2. How many apples do Elsa and Anna have together? \\\\ & & 1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples. \\\\ & & 2. Elsa and Anna have 5 + 7 = 12 apples together. \\\\ & & Q: [Problem of Interest] \\\\ & & A: Let’s break down this problem: \\\\ \\hline LTM & ✓ & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together? \\\\ & & A: Let’s break down this problem: 1. How many apples does Anna have? 2. How many apples do Elsa and Anna have together? \\\\ & & 1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples. \\\\ & & 2. Elsa and Anna have 5 + 7 = 12 apples together. \\\\ & & Q: [Problem of Interest] \\\\ & & A: Let’s break down this problem: \\\\ \\hline 0-CoT & N/A & Q: [Problem of Interest] \\\\ & & A: Let’s think step by step: \\\\ \\hline Program & ✗ & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? \\\\ & & A: Let’s solve the problem by a Python program: \\\\ & & Elsa\\_apples = 5 \\\\ & & Anna.apples = 2 + Elsa\\_apples \\\\ & & Elsa\\_Anna\\_apples = Elsa\\_apples + Anna\\_apples \\\\ & & print(Elsa\\_Anna\\_apples) \\\\ & & Q: [Problem of Interest] \\\\ & & A: Let’s solve the problem by a Python program: \\\\ \\hline Program & ✓ & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together? \\\\ & & A: Let’s solve the problem by a Python program: \\\\ & & Elsa\\_apples = 5 \\\\ & & Anna\\_apples = 2 + Elsa\\_apples \\\\ & & Elsa\\_Anna\\_apples = Elsa\\_apples + Anna\\_apples \\\\ & & print(Elsa\\_Anna\\_apples) \\\\ & & Q: [Problem of Interest] \\\\ & & A: Let’s solve the problem by a Python program: \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 13: Prompts used for all investigated techniques, without instruction. The placeholder [Problem of Interest] is substituted for each problem at the test time. \\begin{table} \\begin{tabular}{c c l} \\hline \\hline **Method** & **Example** & **Prompt** \\\\ & w/ IrrCtx? & \\\\ \\hline CoT & � & Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. \\\\ & & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? \\\\ & & A: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12 apples together. The answer is 12. \\\\ & & Q: [Problem of Interest] \\\\ & & A: \\\\ \\hline CoT & ✓ & Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. \\\\ & & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together? \\\\ & & A: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12 apples together. The answer is 12. \\\\ & & Q: [Problem of Interest] \\\\ & & A: \\\\ \\hline LtM & ✗ & Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. \\\\ & & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? \\\\ & & A: Let’s break down this problem: 1. How many apples does Anna have? 2. How many apples do Elsa and Anna have together? \\\\ & & I. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples. \\\\ & & 2. Elsa and Anna have 5 + 7 = 12 apples together. \\\\ & & Q: [Problem of Interest] \\\\ & & A: Let’s break down this problem: \\\\ \\hline LtM & ✓ & Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. \\\\ & & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together? \\\\ & & A: Let’s break down this problem: 1. How many apples does Anna have? 2. How many apples do Elsa and Anna have together? \\\\ & & 1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples. \\\\ & & 2. Elsa and Anna have 5 + 7 = 12 apples together. \\\\ & & Q: [Problem of Interest] \\\\ & & A: Let’s break down this problem: \\\\ \\hline 0-CoT & N/A & Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. \\\\ & & Q: [Problem of Interest] \\\\ & & A: Let’s think step by step: \\\\ \\hline Program & ✗ & Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. \\\\ & & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? \\\\ & & A: Let’s solve the problem by a Python program: \\\\ & & A: Let’s solve the problem by a Python program: \\\\ \\hline Program & ✓ & Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. \\\\ & & Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together? \\\\ & & A: Let’s solve the problem by a Python program: \\\\ & & A: Let’s solve the problem by a Python program: \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 14: All prompts with instructions. The placeholder [Problem of Interest] is substituted for each problem at the test time."
    }
  ]
}