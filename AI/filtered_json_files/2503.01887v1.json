{
  "title": "When Continue Learning Meets Multimodal Large Language Model: A Survey",
  "authors": [
    "Yukang Huo",
    "Hao Tang"
  ],
  "abstract": "\n In recent years, significant progress has been made in the field of Artificial Intelligence with the development of Multimodal Large Language Models (MLLMs). However, adapting static, pre-trained MLLMs to dynamic data distributions and various tasks in an accurate and efficient manner remains a major challenge. When fine-tuning pre-trained MLLMs for specific tasks, a noticeable performance degradation often occurs in the model's prior knowledge domain -a phenomenon known as \"Catastrophic Forgetting.\" While this issue has been extensively studied within the Continual Learning (CL) community, it presents new challenges in the context of MLLMs. As the first review paper in the field of continual learning for multimodal large models, this paper provides a comprehensive overview and detailed analysis of the 440 research papers on MLLM continual learning. Beyond introducing the fundamental concepts, the review is structured into four main sections. Firstly, it provides an overview of the latest research on MLLMs, including various model innovation strategies, benchmarks, and applications across diverse fields. Secondly, it presents a detailed categorization and overview of the latest research on continual learning, divided into three key areas: non-large language models(LLMs) unimoda continual learning (Non-LLM Unimodal CL), non-large language models multimodal continual learning (Non-LLM Multimoda CL), and continual learning in large language models (CL in LLM). In-depth and extensive research in both the MLLM and CL domains has laid a solid foundation for research on MLLM continual learning. In the fourth section, we conduct an in-depth analysis of the current research status of MLLM continual learning, examining common benchmark evaluations, innovative improvements in model architectures and methods, and systematically summarizing and reviewing existing theoretical and empirical studies. This review aims to connect the basic setup, theoretical foundations, method innovations, and practical applications of continual learning in multimodal large models, shedding light on the research progress and challenges in the field. Finally, this paper offers a forward-looking discussion on the challenges and future development trends of continual learning in multimodal large models, aiming to inspire researchers in the field and promote the advancement of related technologies. \n",
  "references": [
    {
      "id": null,
      "title": "When Continue Learning Meets Multimodal Large Language Model: A Survey",
      "authors": [
        "Yukang Huo",
        "Hao Tang"
      ],
      "year": "2025",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Anygpt: Unified multimodal llm with discrete sequence modeling",
      "authors": [
        "J Zhan",
        "J Dai",
        "J Ye",
        "Y Zhou",
        "D Zhang",
        "Z Liu",
        "X Zhang",
        "R Yuan",
        "G Zhang",
        "L Li"
      ],
      "year": "2024",
      "venue": "Anygpt: Unified multimodal llm with discrete sequence modeling",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality",
      "authors": [
        "W.-L Chiang",
        "Z Li",
        "Z Lin",
        "Y Sheng",
        "Z Wu",
        "H Zhang",
        "L Zheng",
        "S Zhuang",
        "Y Zhuang",
        "J E Gonzalez"
      ],
      "year": "2023",
      "venue": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Motiongpt: Human motion as a foreign language",
      "authors": [
        "B Jiang",
        "X Chen",
        "W Liu",
        "J Yu",
        "G Yu",
        "T Chen"
      ],
      "year": "2023",
      "venue": "Motiongpt: Human motion as a foreign language",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Motiongpt: Finetuned llms are generalpurpose motion generators",
      "authors": [
        "Y Zhang",
        "D Huang",
        "B Liu",
        "S Tang",
        "Y Lu",
        "L Chen",
        "L Bai",
        "Q Chu",
        "N Yu",
        "W Ouyang"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Sphinx-x: Scaling data and parameters for a family of multi-modal large language models",
      "authors": [
        "D Liu",
        "R Zhang",
        "L Qiu",
        "S Huang",
        "W Lin",
        "S Zhao",
        "S Geng",
        "Z Lin",
        "P Jin",
        "K Zhang"
      ],
      "year": "2024",
      "venue": "Sphinx-x: Scaling data and parameters for a family of multi-modal large language models",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Ical: Continual learning of multimodal agents by transforming trajectories into actionable insights",
      "authors": [
        "G Sarch",
        "L Jang",
        "M J Tarr",
        "W W Cohen",
        "K Marino",
        "K Fragkiadaki"
      ],
      "year": "2024",
      "venue": "Ical: Continual learning of multimodal agents by transforming trajectories into actionable insights",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks",
      "authors": [
        "J Wu",
        "M Zhong",
        "S Xing",
        "Z Lai",
        "Z Liu",
        "W Wang",
        "Z Chen",
        "X Zhu",
        "L Lu",
        "T Lu"
      ],
      "year": "2024",
      "venue": "Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Single image unlearning: Efficient machine unlearning in multimodal large language models",
      "authors": [
        "J Li",
        "Q Wei",
        "C Zhang",
        "G Qi",
        "M Du",
        "Y Chen",
        "S Bi"
      ],
      "year": "2024",
      "venue": "Single image unlearning: Efficient machine unlearning in multimodal large language models",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd",
      "authors": [
        "X Dong",
        "P Zhang",
        "Y Zang",
        "Y Cao",
        "B Wang",
        "L Ouyang",
        "S Zhang",
        "H Duan",
        "W Zhang",
        "Y Li"
      ],
      "year": "2024",
      "venue": "Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Embodiedgpt: Vision-language pre-training via embodied chain of thought",
      "authors": [
        "Y Mu",
        "Q Zhang",
        "M Hu",
        "W Wang",
        "M Ding",
        "J Jin",
        "B Wang",
        "J Dai",
        "Y Qiao",
        "P Luo"
      ],
      "year": "2024",
      "venue": "Embodiedgpt: Vision-language pre-training via embodied chain of thought",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Multimodal task vectors enable many-shot multimodal in-context learning",
      "authors": [
        "B Huang",
        "C Mitra",
        "A Arbelle",
        "L Karlinsky",
        "T Darrell",
        "R Herzig"
      ],
      "year": "2024",
      "venue": "Multimodal task vectors enable many-shot multimodal in-context learning",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Multiood: Scaling outof-distribution detection for multiple modalities",
      "authors": [
        "H Dong",
        "Y Zhao",
        "E Chatzi",
        "O Fink"
      ],
      "year": "2024",
      "venue": "Multiood: Scaling outof-distribution detection for multiple modalities",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Hypergraph multi-modal large language model: Exploiting eeg and eye-tracking modalities to evaluate heterogeneous responses for video understanding",
      "authors": [
        "M Wu",
        "C Zhao",
        "A Su",
        "D Di",
        "T Fu",
        "D An",
        "M He",
        "Y Gao",
        "M Ma",
        "K Yan"
      ],
      "year": "",
      "venue": "Hypergraph multi-modal large language model: Exploiting eeg and eye-tracking modalities to evaluate heterogeneous responses for video understanding",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Multi-modality co-learning for efficient skeleton-based action recognition",
      "authors": [
        "J Liu",
        "C Chen",
        "M Liu"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Cloud-device collaborative learning for multimodal large language models",
      "authors": [
        "G Wang",
        "J Liu",
        "C Li",
        "Y Zhang",
        "J Ma",
        "X Wei",
        "K Zhang",
        "M Chong",
        "R Zhang",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Implicit multimodal alignment: On the generalization of frozen llms to multimodal inputs",
      "authors": [
        "M Shukor",
        "M Cord"
      ],
      "year": "2024",
      "venue": "Implicit multimodal alignment: On the generalization of frozen llms to multimodal inputs",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Caption-aware multimodal relation extraction with mutual information maximization",
      "authors": [
        "Z Zhang",
        "W Zhang",
        "Y Li",
        "T Bai"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Libra: Building decoupled vision system on large language models",
      "authors": [
        "Y Xu",
        "X Yang",
        "Y Song",
        "C Xu"
      ],
      "year": "2024",
      "venue": "Libra: Building decoupled vision system on large language models",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Towards multimodal-augmented pre-trained language models via selfbalanced expectation-maximization iteration",
      "authors": [
        "X Zhuang",
        "X Cheng",
        "Z Zhu",
        "Z Chen",
        "H Li",
        "Y Zou"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Decoding style: Efficient fine-tuning of llms for imageguided outfit recommendation with preference",
      "authors": [
        "N Forouzandehmehr",
        "N Farrokhsiar",
        "R Giahi",
        "E Korpeoglu",
        "K Achan"
      ],
      "year": "2024",
      "venue": "Decoding style: Efficient fine-tuning of llms for imageguided outfit recommendation with preference",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Clova: A closed-loop visual assistant with tool usage and update",
      "authors": [
        "Z Gao",
        "Y Du",
        "X Zhang",
        "X Ma",
        "W Han",
        "S.-C Zhu",
        "Q Li"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Grounding multimodal large language models in actions",
      "authors": [
        "A Szot",
        "B Mazoure",
        "H Agrawal",
        "D Hjelm",
        "Z Kira",
        "A Toshev"
      ],
      "year": "2024",
      "venue": "Grounding multimodal large language models in actions",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Controlmllm: Training-free visual prompt learning for multimodal large language models",
      "authors": [
        "M Wu",
        "X Cai",
        "J Ji",
        "J Li",
        "O Huang",
        "G Luo",
        "H Fei",
        "G Jiang",
        "X Sun",
        "R Ji"
      ],
      "year": "2024",
      "venue": "Controlmllm: Training-free visual prompt learning for multimodal large language models",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Membership inference attacks against large vision-language models",
      "authors": [
        "Z Li",
        "Y Wu",
        "Y Chen",
        "F Tonin",
        "E A Rocamora",
        "V Cevher"
      ],
      "year": "2024",
      "venue": "Membership inference attacks against large vision-language models",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Failures are fated, but can be faded: Characterizing and mitigating unwanted behaviors in large-scale vision and language models",
      "authors": [
        "S Sagar",
        "A Taparia",
        "R Senanayake"
      ],
      "year": "2024",
      "venue": "Failures are fated, but can be faded: Characterizing and mitigating unwanted behaviors in large-scale vision and language models",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Fine-tuning large vision-language models as decision-making agents via reinforcement learning",
      "authors": [
        "Y Zhai",
        "H Bai",
        "Z Lin",
        "J Pan",
        "S Tong",
        "Y Zhou",
        "A Suhr",
        "S Xie",
        "Y Lecun",
        "Y Ma"
      ],
      "year": "2024",
      "venue": "Fine-tuning large vision-language models as decision-making agents via reinforcement learning",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E J Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Parameterefficient transfer learning for nlp",
      "authors": [
        "N Houlsby",
        "A Giurgiu",
        "S Jastrzebski",
        "B Morrone",
        "Q De Laroussilhe",
        "A Gesmundo",
        "M Attariyan",
        "S Gelly"
      ],
      "year": "2019",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
      "authors": [
        "X Liu",
        "K Ji",
        "Y Fu",
        "W L Tam",
        "Z Du",
        "Z Yang",
        "J Tang"
      ],
      "year": "2021",
      "venue": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Gpt understands, too",
      "authors": [
        "X Liu",
        "Y Zheng",
        "Z Du",
        "M Ding",
        "Y Qian",
        "Z Yang",
        "J Tang"
      ],
      "year": "2024",
      "venue": "AI Open",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "authors": [
        "X L Li",
        "P Liang"
      ],
      "year": "2021",
      "venue": "Prefix-tuning: Optimizing continuous prompts for generation",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Vqa: Visual question answering",
      "authors": [
        "S Antol",
        "A Agrawal",
        "J Lu",
        "M Mitchell",
        "D Batra",
        "C L Zitnick",
        "D Parikh"
      ],
      "year": "2015",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Docvqa: A dataset for vqa on document images",
      "authors": [
        "M Mathew",
        "D Karatzas",
        "C Jawahar"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Chartqa: A benchmark for question answering about charts with visual and logical reasoning",
      "authors": [
        "A Masry",
        "D X Long",
        "J Q Tan",
        "S Joty",
        "E Hoque"
      ],
      "year": "2022",
      "venue": "Chartqa: A benchmark for question answering about charts with visual and logical reasoning",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Dvqa: Understanding data visualizations via question answering",
      "authors": [
        "K Kafle",
        "B Price",
        "S Cohen",
        "C Kanan"
      ],
      "year": "2018",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning",
      "authors": [
        "J Chen",
        "J Tang",
        "J Qin",
        "X Liang",
        "L Liu",
        "E P Xing",
        "L Lin"
      ],
      "year": "2021",
      "venue": "Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "T.-Y Lin",
        "M Maire",
        "S Belongie",
        "J Hays",
        "P Perona",
        "D Ramanan",
        "P Dollár",
        "C L Zitnick"
      ],
      "year": "2014",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "authors": [
        "P Sharma",
        "N Ding",
        "S Goodman",
        "R Soricut"
      ],
      "year": "2018",
      "venue": "ACL",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Textcaps: a dataset for image captioning with reading comprehension",
      "authors": [
        "O Sidorov",
        "R Hu",
        "M Rohrbach",
        "A Singh"
      ],
      "year": "2020",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Captioning images taken by people who are blind",
      "authors": [
        "D Gurari",
        "Y Zhao",
        "M Zhang",
        "N Bhattacharya"
      ],
      "year": "2020",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Connecting vision and language with localized narratives",
      "authors": [
        "J Pont-Tuset",
        "J Uijlings",
        "S Changpinyo",
        "R Soricut",
        "V Ferrari"
      ],
      "year": "2020",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Nocaps: Novel object captioning at scale,\" in ICCV",
      "authors": [
        "H Agrawal",
        "K Desai",
        "Y Wang",
        "X Chen",
        "R Jain",
        "M Johnson",
        "D Batra",
        "D Parikh",
        "S Lee",
        "P Anderson"
      ],
      "year": "2019",
      "venue": "Nocaps: Novel object captioning at scale,\" in ICCV",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Who's waldo? linking people across text and images",
      "authors": [
        "Y Cui",
        "A Khandelwal",
        "Y Artzi",
        "N Snavely",
        "H Averbuch-Elor"
      ],
      "year": "2021",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Modeling context in referring expressions",
      "authors": [
        "L Yu",
        "P Poirson",
        "S Yang",
        "A C Berg",
        "T L Berg"
      ],
      "year": "2016",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Generation and comprehension of unambiguous object descriptions",
      "authors": [
        "J Mao",
        "J Huang",
        "A Toshev",
        "O Camburu",
        "A L Yuille",
        "K Murphy"
      ],
      "year": "2016",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Generating easy-to-understand referring expressions for target identifications",
      "authors": [
        "M Tanaka",
        "T Itamochi",
        "K Narioka",
        "I Sato",
        "Y Ushiku",
        "T Harada"
      ],
      "year": "2019",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J F Gemmeke",
        "D P Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R C Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "ICASSP",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation",
      "authors": [
        "F Hernandez",
        "V Nguyen",
        "S Ghannay",
        "N Tomashenko",
        "Y Esteve"
      ],
      "year": "2018",
      "venue": "Speech and Computer: 20th International Conference",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Henretty",
        "M Kohler",
        "J Meyer",
        "R Morais",
        "L Saunders",
        "F M Tyers",
        "G Weber"
      ],
      "year": "2019",
      "venue": "Common voice: A massively-multilingual speech corpus",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Contextual: Evaluating context-sensitive text-rich visual reasoning in large multimodal models",
      "authors": [
        "R Wadhawan",
        "H Bansal",
        "K.-W Chang",
        "N Peng"
      ],
      "year": "2024",
      "venue": "Contextual: Evaluating context-sensitive text-rich visual reasoning in large multimodal models",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "See say and segment: Teaching lmms to overcome false premises",
      "authors": [
        "T.-H Wu",
        "G Biamby",
        "D Chan",
        "L Dunlap",
        "R Gupta",
        "X Wang",
        "J E Gonzalez",
        "T Darrell"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Llava-ultra: Large chinese language and vision assistant for ultrasound",
      "authors": [
        "X Guo",
        "W Chai",
        "S.-Y Li",
        "G Wang"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Llm-assisted multi-teacher continual learning for visual question answering in robotic surgery",
      "authors": [
        "K Chen",
        "Y Du",
        "T You",
        "M Islam",
        "Z Guo",
        "Y Jin",
        "G Chen",
        "P.-A Heng"
      ],
      "year": "2024",
      "venue": "Llm-assisted multi-teacher continual learning for visual question answering in robotic surgery",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Making large language models better planners with reasoning-decision alignment",
      "authors": [
        "Z Huang",
        "T Tang",
        "S Chen",
        "S Lin",
        "Z Jie",
        "L Ma",
        "G Wang",
        "X Liang"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Continual pre-training mitigates forgetting in language and vision",
      "authors": [
        "A Cossu",
        "A Carta",
        "L Passaro",
        "V Lomonaco",
        "T Tuytelaars",
        "D Bacciu"
      ],
      "year": "2024",
      "venue": "Neural Networks",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Elle: Efficient lifelong pre-training for emerging data",
      "authors": [
        "Y Qin",
        "J Zhang",
        "Y Lin",
        "Z Liu",
        "P Li",
        "M Sun",
        "J Zhou"
      ],
      "year": "2022",
      "venue": "Elle: Efficient lifelong pre-training for emerging data",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Ernie 2.0: A continual pre-training framework for language understanding",
      "authors": [
        "Y Sun",
        "S Wang",
        "Y Li",
        "S Feng",
        "H Tian",
        "H Wu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Learning to prompt for continual learning",
      "authors": [
        "Z Wang",
        "Z Zhang",
        "C.-Y Lee",
        "H Zhang",
        "R Sun",
        "X Ren",
        "G Su",
        "V Perot",
        "J Dy",
        "T Pfister"
      ],
      "year": "2022",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Data engineering for scaling language models to 128k context",
      "authors": [
        "Y Fu",
        "R Panda",
        "X Niu",
        "X Yue",
        "H Hajishirzi",
        "Y Kim",
        "H Peng"
      ],
      "year": "2024",
      "venue": "Data engineering for scaling language models to 128k context",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Pilora: Prototype guided incremental lora for federated class-incremental learning",
      "authors": [
        "H Guo",
        "F Zhu",
        "W Liu",
        "X.-Y Zhang",
        "C.-L Liu"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "Ftf-er: Feature-topology fusion-based experience replay method for continual graph learning",
      "authors": [
        "J Pang",
        "C Lin",
        "X Hao",
        "R Yin",
        "Z Wang",
        "Z Zhang",
        "J He",
        "H Tai Sheng"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Dense network expansion for class incremental learning",
      "authors": [
        "Z Hu",
        "Y Li",
        "J Lyu",
        "D Gao",
        "N Vasconcelos"
      ],
      "year": "2023",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Conditional channel gated networks for taskaware continual learning",
      "authors": [
        "D Abati",
        "J Tomczak",
        "T Blankevoort",
        "S Calderara",
        "R Cucchiara",
        "B E Bejnordi"
      ],
      "year": "2020",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "Generating prompts in latent space for rehearsal-free continual learning",
      "authors": [
        "C Yang",
        "W Liu",
        "S Chen",
        "J Qi",
        "A Zhou"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "On the diminishing returns of width for continual learning",
      "authors": [
        "E Guha",
        "V Lakshman"
      ],
      "year": "2024",
      "venue": "On the diminishing returns of width for continual learning",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "Regularizing with pseudonegatives for continual self-supervised learning",
      "authors": [
        "S Cha",
        "K Cho",
        "T Moon"
      ],
      "year": "2024",
      "venue": "Regularizing with pseudonegatives for continual self-supervised learning",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "Becotta: Input-dependent online blending of experts for continual test-time adaptation",
      "authors": [
        "D Lee",
        "J Yoon",
        "S J Hwang"
      ],
      "year": "2024",
      "venue": "Becotta: Input-dependent online blending of experts for continual test-time adaptation",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "A statistical theory of regularization-based continual learning",
      "authors": [
        "X Zhao",
        "H Wang",
        "W Huang",
        "W Lin"
      ],
      "year": "2024",
      "venue": "A statistical theory of regularization-based continual learning",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "Poet: Prompt offset tuning for continual human action adaptation",
      "authors": [
        "P Garg",
        "K Joseph",
        "V N Balasubramanian",
        "N C Camgoz",
        "C Wan",
        "K Kin",
        "W Si",
        "S Ma",
        "F De",
        "La Torre"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "One-stage prompt-based continual learning",
      "authors": [
        "Y Kim",
        "Y Li",
        "P Panda"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "Semantic editing increment benefits zero-shot composed image retrieval",
      "authors": [
        "Z Yang",
        "S Qian",
        "D Xue",
        "J Wu",
        "F Yang",
        "W Dong",
        "C Xu"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "Task confusion and catastrophic forgetting in class-incremental learning: A mathematical framework for discriminative and generative modelings",
      "authors": [
        "M K Nori",
        "I.-M Kim"
      ],
      "year": "2024",
      "venue": "Task confusion and catastrophic forgetting in class-incremental learning: A mathematical framework for discriminative and generative modelings",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "Open-world dynamic prompt and continual visual representation learning",
      "authors": [
        "Y Kim",
        "J Fang",
        "Q Zhang",
        "Z Cai",
        "Y Shen",
        "R Duggal",
        "D Raychaudhuri",
        "Z Tu",
        "Y Xing",
        "O Dabeer"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b73",
      "title": "Revisiting supervision for continual representation learning",
      "authors": [
        "D Marczak",
        "S Cygert",
        "T Trzci",
        "B Twardowski"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "Learn to memorize and to forget: A continual learning perspective of dynamic slam",
      "authors": [
        "B Li",
        "Z Yan",
        "D Wu",
        "H Jiang",
        "H Zha"
      ],
      "year": "2024",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "Promptfusion: Decoupling stability and plasticity for continual learning",
      "authors": [
        "H Chen",
        "Z Wu",
        "X Han",
        "M Jia",
        "Y.-G Jiang"
      ],
      "year": "2023",
      "venue": "Promptfusion: Decoupling stability and plasticity for continual learning",
      "doi": ""
    },
    {
      "id": "b76",
      "title": "Continuous training and fine-tuning for domain-specific language models in medical question answering",
      "authors": [
        "Z Guo",
        "Y Hua"
      ],
      "year": "2023",
      "venue": "Continuous training and fine-tuning for domain-specific language models in medical question answering",
      "doi": ""
    },
    {
      "id": "b77",
      "title": "Saullm-7b: A pioneering large language model for law",
      "authors": [
        "P Colombo",
        "T P Pires",
        "M Boudiaf",
        "D Culver",
        "R Melo",
        "C Corro",
        "A F Martins",
        "F Esposito",
        "V L Raposo",
        "S Morgado"
      ],
      "year": "2024",
      "venue": "Saullm-7b: A pioneering large language model for law",
      "doi": ""
    },
    {
      "id": "b78",
      "title": "K2: A foundation language model for geoscience knowledge understanding and utilization",
      "authors": [
        "C Deng",
        "T Zhang",
        "Z He",
        "Q Chen",
        "Y Shi",
        "Y Xu",
        "L Fu",
        "W Zhang",
        "X Wang",
        "C Zhou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 17th ACM International Conference on Web Search and Data Mining",
      "doi": ""
    },
    {
      "id": "b79",
      "title": "Don't stop pretraining: Adapt language models to domains and tasks",
      "authors": [
        "S Gururangan",
        "A Marasović",
        "S Swayamdipta",
        "K Lo",
        "I Beltagy",
        "D Downey",
        "N A Smith"
      ],
      "year": "2020",
      "venue": "Don't stop pretraining: Adapt language models to domains and tasks",
      "doi": ""
    },
    {
      "id": "b80",
      "title": "Econet: Effective continual pretraining of language models for event temporal reasoning",
      "authors": [
        "R Han",
        "X Ren",
        "N Peng"
      ],
      "year": "2020",
      "venue": "Econet: Effective continual pretraining of language models for event temporal reasoning",
      "doi": ""
    },
    {
      "id": "b81",
      "title": "Ecomgpt-ct: Continual pre-training of e-commerce large language models with semi-structured data",
      "authors": [
        "S Ma",
        "S Huang",
        "S Huang",
        "X Wang",
        "Y Li",
        "H.-T Zheng",
        "P Xie",
        "F Huang",
        "Y Jiang"
      ],
      "year": "2023",
      "venue": "Ecomgpt-ct: Continual pre-training of e-commerce large language models with semi-structured data",
      "doi": ""
    },
    {
      "id": "b82",
      "title": "Rθ3: Reinforced readerranker for open-domain question answering",
      "authors": [
        "S Wang",
        "M Yu",
        "X Guo",
        "Z Wang",
        "T Klinger",
        "W Zhang",
        "S Chang",
        "G Tesauro",
        "B Zhou",
        "J Jiang"
      ],
      "year": "2017",
      "venue": "Rθ3: Reinforced readerranker for open-domain question answering",
      "doi": ""
    },
    {
      "id": "b83",
      "title": "End-to-end open-domain question answering with bertserini",
      "authors": [
        "W Yang",
        "Y Xie",
        "A Lin",
        "X Li",
        "L Tan",
        "K Xiong",
        "M Li",
        "J Lin"
      ],
      "year": "2019",
      "venue": "End-to-end open-domain question answering with bertserini",
      "doi": ""
    },
    {
      "id": "b84",
      "title": "Reinforcement learning with token-level feedback for controllable text generation",
      "authors": [
        "W Li",
        "W Wei",
        "K Xu",
        "W Xie",
        "D Chen",
        "Y Cheng"
      ],
      "year": "2024",
      "venue": "Reinforcement learning with token-level feedback for controllable text generation",
      "doi": ""
    },
    {
      "id": "b85",
      "title": "Embracing change: Continual learning in deep neural networks",
      "authors": [
        "R Hadsell",
        "D Rao",
        "A A Rusu",
        "R Pascanu"
      ],
      "year": "2020",
      "venue": "Trends in cognitive sciences",
      "doi": ""
    },
    {
      "id": "b86",
      "title": "A practitioner's guide to continual multimodal pretraining",
      "authors": [
        "K Roth",
        "V Udandarao",
        "S Dziadzio",
        "A Prabhu",
        "M Cherti",
        "O Vinyals",
        "O Hénaff",
        "S Albanie",
        "M Bethge",
        "Z Akata"
      ],
      "year": "2024",
      "venue": "A practitioner's guide to continual multimodal pretraining",
      "doi": ""
    },
    {
      "id": "b87",
      "title": "Citb: A benchmark for continual instruction tuning",
      "authors": [
        "Z Zhang",
        "M Fang",
        "L Chen",
        "M.-R Namazi-Rad"
      ],
      "year": "2023",
      "venue": "Citb: A benchmark for continual instruction tuning",
      "doi": ""
    },
    {
      "id": "b88",
      "title": "X-instructblip: A framework for aligning x-modal instruction-aware representations to llms and emergent cross-modal reasoning",
      "authors": [
        "A Panagopoulou",
        "L Xue",
        "N Yu",
        "J Li",
        "D Li",
        "S Joty",
        "R Xu",
        "S Savarese",
        "C Xiong",
        "J C Niebles"
      ],
      "year": "2023",
      "venue": "X-instructblip: A framework for aligning x-modal instruction-aware representations to llms and emergent cross-modal reasoning",
      "doi": ""
    },
    {
      "id": "b89",
      "title": "Mixture of experts meets prompt-based continual learning",
      "authors": [
        "M Le",
        "A Nguyen",
        "H Nguyen",
        "T Nguyen",
        "T Pham",
        "L Van Ngo",
        "N Ho"
      ],
      "year": "2024",
      "venue": "Mixture of experts meets prompt-based continual learning",
      "doi": ""
    },
    {
      "id": "b90",
      "title": "Clap4clip: Continual learning with probabilistic finetuning for vision-language models",
      "authors": [
        "S Jha",
        "D Gong",
        "L Yao"
      ],
      "year": "2024",
      "venue": "Clap4clip: Continual learning with probabilistic finetuning for vision-language models",
      "doi": ""
    },
    {
      "id": "b91",
      "title": "Dual low-rank adaptation for continual learning with pre-trained models",
      "authors": [
        "H Chen",
        "J Li",
        "N Gazagnadou",
        "W Zhuang",
        "C Chen",
        "L Lyu"
      ],
      "year": "2024",
      "venue": "Dual low-rank adaptation for continual learning with pre-trained models",
      "doi": ""
    },
    {
      "id": "b92",
      "title": "Hvclip: High-dimensional vector in clip for unsupervised domain adaptation",
      "authors": [
        "N Vesdapunt",
        "K K Fu",
        "Y Wu",
        "X Zhang",
        "P Natarajan"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b93",
      "title": "Coin: A benchmark of continual instruction tuning for multimodel large language model",
      "authors": [
        "C Chen",
        "J Zhu",
        "X Luo",
        "H Shen",
        "L Gao",
        "J Song"
      ],
      "year": "2024",
      "venue": "Coin: A benchmark of continual instruction tuning for multimodel large language model",
      "doi": ""
    },
    {
      "id": "b94",
      "title": "Climb: A continual learning benchmark for vision-and-language tasks",
      "authors": [
        "T Srinivasan",
        "T.-Y Chang",
        "L Pinto Alva",
        "G Chochlakis",
        "M Rostami",
        "J Thomason"
      ],
      "year": "2022",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b95",
      "title": "Continual llava: Continual instruction tuning in large vision-language models",
      "authors": [
        "M Cao",
        "Y Liu",
        "Y Liu",
        "T Wang",
        "J Dong",
        "H Ding",
        "X Zhang",
        "I Reid",
        "X Liang"
      ],
      "year": "2024",
      "venue": "Continual llava: Continual instruction tuning in large vision-language models",
      "doi": ""
    },
    {
      "id": "b96",
      "title": "Vilcobench: Video language continual learning benchmark",
      "authors": [
        "T Tang",
        "S Deldari",
        "H Xue",
        "C De Melo",
        "F D Salim"
      ],
      "year": "2024",
      "venue": "Vilcobench: Video language continual learning benchmark",
      "doi": ""
    },
    {
      "id": "b97",
      "title": "Towards lifelong scene graph generation with knowledge-ware in-context prompt learning",
      "authors": [
        "T He",
        "T Wu",
        "D Zhang",
        "G Duan",
        "K Qin",
        "Y.-F Li"
      ],
      "year": "2024",
      "venue": "Towards lifelong scene graph generation with knowledge-ware in-context prompt learning",
      "doi": ""
    },
    {
      "id": "b98",
      "title": "Freezeomni: A smart and low latency speech-to-speech dialogue model with frozen llm",
      "authors": [
        "X Wang",
        "Y Li",
        "C Fu",
        "L Xie",
        "K Li",
        "X Sun",
        "L Ma"
      ],
      "year": "2024",
      "venue": "Freezeomni: A smart and low latency speech-to-speech dialogue model with frozen llm",
      "doi": ""
    },
    {
      "id": "b99",
      "title": "Interactive continual learning: Fast and slow thinking",
      "authors": [
        "B Qi",
        "X Chen",
        "J Gao",
        "D Li",
        "J Liu",
        "L Wu",
        "B Zhou"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b100",
      "title": "Maven: An effective multi-granularity hybrid visual encoding framework for multimodal large language model",
      "authors": [
        "C Jiang",
        "J Hongrui",
        "H Xu",
        "W Ye",
        "M Dong",
        "M Yan",
        "J Zhang",
        "F Huang",
        "S Zhang"
      ],
      "year": "2024",
      "venue": "Maven: An effective multi-granularity hybrid visual encoding framework for multimodal large language model",
      "doi": ""
    },
    {
      "id": "b101",
      "title": "Mova: Adapting mixture of vision experts to multimodal context",
      "authors": [
        "Z Zong",
        "B Ma",
        "D Shen",
        "G Song",
        "H Shao",
        "D Jiang",
        "H Li",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "Mova: Adapting mixture of vision experts to multimodal context",
      "doi": ""
    },
    {
      "id": "b102",
      "title": "Mome: Mixture of multimodal experts for generalist multimodal large language models",
      "authors": [
        "L Shen",
        "G Chen",
        "R Shao",
        "W Guan",
        "L Nie"
      ],
      "year": "2024",
      "venue": "Mome: Mixture of multimodal experts for generalist multimodal large language models",
      "doi": ""
    },
    {
      "id": "b103",
      "title": "Meteor: Mambabased traversal of rationale for large language and vision models",
      "authors": [
        "B.-K Lee",
        "C W Kim",
        "B Park",
        "Y M Ro"
      ],
      "year": "2024",
      "venue": "Meteor: Mambabased traversal of rationale for large language and vision models",
      "doi": ""
    },
    {
      "id": "b104",
      "title": "Coevolving with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning",
      "authors": [
        "H Ma",
        "T Hu",
        "Z Pu",
        "B Liu",
        "X Ai",
        "Y Liang",
        "M Chen"
      ],
      "year": "2024",
      "venue": "Coevolving with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning",
      "doi": ""
    },
    {
      "id": "b105",
      "title": "Lumen: Unleashing versatile vision-centric capabilities of large multimodal models",
      "authors": [
        "Y Jiao",
        "S Chen",
        "Z Jie",
        "J Chen",
        "L Ma",
        "Y.-G Jiang"
      ],
      "year": "2024",
      "venue": "Lumen: Unleashing versatile vision-centric capabilities of large multimodal models",
      "doi": ""
    },
    {
      "id": "b106",
      "title": "Octopus: A multi-modal llm with parallel recognition and sequential understanding",
      "authors": [
        "C Zhao",
        "Y Song",
        "J Chen",
        "K Rong",
        "H Feng",
        "G Zhang",
        "S Ji",
        "J Wang",
        "E Ding",
        "Y Sun"
      ],
      "year": "",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b107",
      "title": "Wings: Learning multimodal llms without text-only forgetting",
      "authors": [
        "Y.-K Zhang",
        "S Lu",
        "Y Li",
        "Y Ma",
        "Q.-G Chen",
        "Z Xu",
        "W Luo",
        "K Zhang",
        "D.-C Zhan",
        "H.-J Ye"
      ],
      "year": "2024",
      "venue": "Wings: Learning multimodal llms without text-only forgetting",
      "doi": ""
    },
    {
      "id": "b108",
      "title": "Cantor: Inspiring multimodal chain-of-thought of mllm",
      "authors": [
        "T Gao",
        "P Chen",
        "M Zhang",
        "C Fu",
        "Y Shen",
        "Y Zhang",
        "S Zhang",
        "X Zheng",
        "X Sun",
        "L Cao"
      ],
      "year": "",
      "venue": "Cantor: Inspiring multimodal chain-of-thought of mllm",
      "doi": ""
    },
    {
      "id": "b109",
      "title": "Autom3l: An automated multimodal machine learning framework with large language models",
      "authors": [
        "D Luo",
        "C Feng",
        "Y Nong",
        "Y Shen"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b110",
      "title": "Detached and interactive multimodal learning",
      "authors": [
        "Y Fan",
        "W Xu",
        "H Wang",
        "J Liu",
        "S Guo"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b111",
      "title": "Multimodal unlearnable examples: Protecting data against multimodal contrastive learning",
      "authors": [
        "X Liu",
        "X Jia",
        "Y Xun",
        "S Liang",
        "X Cao"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b112",
      "title": "Cream: Coarse-to-fine retrieval and multi-modal efficient tuning for document vqa",
      "authors": [
        "J Zhang",
        "Y Yu",
        "Y Zhang"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b113",
      "title": "Self-adaptive fine-grained multi-modal data augmentation for semi-supervised muti-modal coreference resolution",
      "authors": [
        "L Zheng",
        "B Chen",
        "H Fei",
        "F Li",
        "S Wu",
        "L Liao",
        "D Ji",
        "C Teng"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b114",
      "title": "Semantic alignment for multimodal large language models",
      "authors": [
        "T Wu",
        "M Li",
        "J Chen",
        "W Ji",
        "W Lin",
        "J Gao",
        "K Kuang",
        "Z Zhao",
        "F Wu"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b115",
      "title": "Collaborative training of tiny-large vision language models",
      "authors": [
        "S Lu",
        "L Guo",
        "W Wang",
        "Z Zhao",
        "T Yue",
        "J Liu",
        "S Liu"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b116",
      "title": "Efficient training for multilingual visual speech recognition: Pre-training with discretized visual speech representation",
      "authors": [
        "M Kim",
        "J Yeo",
        "S J Park",
        "H Rha",
        "Y M Ro"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b117",
      "title": "Large multi-modality model assisted ai-generated image quality assessment",
      "authors": [
        "P Wang",
        "W Sun",
        "Z Zhang",
        "J Jia",
        "Y Jiang",
        "Z Zhang",
        "X Min",
        "G Zhai"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b118",
      "title": "Worldgpt: Empowering llm as multimodal world model",
      "authors": [
        "Z Ge",
        "H Huang",
        "M Zhou",
        "J Li",
        "G Wang",
        "S Tang",
        "Y Zhuang"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b119",
      "title": "Q-align: Teaching lmms for visual scoring via discrete text-defined levels",
      "authors": [
        "H Wu",
        "Z Zhang",
        "W Zhang",
        "C Chen",
        "L Liao",
        "C Li",
        "Y Gao",
        "A Wang",
        "E Zhang",
        "W Sun"
      ],
      "year": "2023",
      "venue": "Q-align: Teaching lmms for visual scoring via discrete text-defined levels",
      "doi": ""
    },
    {
      "id": "b120",
      "title": "Flextron: Many-in-one flexible large language model",
      "authors": [
        "R Cai",
        "S Muralidharan",
        "G Heinrich",
        "H Yin",
        "Z Wang",
        "J Kautz",
        "P Molchanov"
      ],
      "year": "2024",
      "venue": "Flextron: Many-in-one flexible large language model",
      "doi": ""
    },
    {
      "id": "b121",
      "title": "Next-gpt: Any-to-any multimodal llm",
      "authors": [
        "S Wu",
        "H Fei",
        "L Qu",
        "W Ji",
        "T.-S Chua"
      ],
      "year": "2023",
      "venue": "Next-gpt: Any-to-any multimodal llm",
      "doi": ""
    },
    {
      "id": "b122",
      "title": "Densefusion-1m: Merging vision experts for comprehensive multimodal perception",
      "authors": [
        "X Li",
        "F Zhang",
        "H Diao",
        "Y Wang",
        "X Wang",
        "L.-Y Duan"
      ],
      "year": "2024",
      "venue": "Densefusion-1m: Merging vision experts for comprehensive multimodal perception",
      "doi": ""
    },
    {
      "id": "b123",
      "title": "E2e-mfd: Towards end-to-end synchronous multimodal fusion detection",
      "authors": [
        "J Zhang",
        "M Cao",
        "X Yang",
        "W Xie",
        "J Lei",
        "D Li",
        "W Huang",
        "Y Li"
      ],
      "year": "2024",
      "venue": "E2e-mfd: Towards end-to-end synchronous multimodal fusion detection",
      "doi": ""
    },
    {
      "id": "b124",
      "title": "Towards neuron attributions in multi-modal large language models",
      "authors": [
        "J Fang",
        "Z Bi",
        "R Wang",
        "H Jiang",
        "Y Gao",
        "K Wang",
        "A Zhang",
        "J Shi",
        "X Wang",
        "T.-S Chua"
      ],
      "year": "",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b125",
      "title": "Code: Contrasting selfgenerated description to combat hallucination in large multimodal models",
      "authors": [
        "J Kim",
        "H Kim",
        "Y Kim",
        "Y M Ro"
      ],
      "year": "2024",
      "venue": "Code: Contrasting selfgenerated description to combat hallucination in large multimodal models",
      "doi": ""
    },
    {
      "id": "b126",
      "title": "Understanding information storage and transfer in multi-modal large language models",
      "authors": [
        "S Basu",
        "M Grayson",
        "C Morrison",
        "B Nushi",
        "S Feizi",
        "D Massiceti"
      ],
      "year": "2024",
      "venue": "Understanding information storage and transfer in multi-modal large language models",
      "doi": ""
    },
    {
      "id": "b127",
      "title": "Advancing multimodal large language models with quantization-aware scale learning for efficient adaptation",
      "authors": [
        "J Xie",
        "Y Zhang",
        "M Lin",
        "L Cao",
        "R Ji"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b128",
      "title": "Multimodal llm enhanced cross-lingual cross-modal retrieval",
      "authors": [
        "Y Wang",
        "L Wang",
        "Q Zhou",
        "Z Wang",
        "H Li",
        "G Hua",
        "W Tang"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b129",
      "title": "Erl-mr: Harnessing the power of euler feature representations for balanced multi-modal learning",
      "authors": [
        "W Han",
        "C Cai",
        "Y Guo",
        "J Peng"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b130",
      "title": "Adaptive multimodality prompt learning",
      "authors": [
        "Z Wu",
        "Y Liu",
        "M Zhan",
        "P Hu",
        "X Zhu"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b131",
      "title": "Enhancing cross-modal fine-tuning with gradually intermediate modality generation",
      "authors": [
        "L Cai",
        "S Li",
        "W Ma",
        "J Kang",
        "B Xie",
        "Z Sun",
        "C Zhu"
      ],
      "year": "2024",
      "venue": "Enhancing cross-modal fine-tuning with gradually intermediate modality generation",
      "doi": ""
    },
    {
      "id": "b132",
      "title": "Improving context understanding in multimodal large language models via multimodal composition learning",
      "authors": [
        "W Li",
        "H Fan",
        "Y Wong",
        "Y Yang",
        "M Kankanhalli"
      ],
      "year": "",
      "venue": "Improving context understanding in multimodal large language models via multimodal composition learning",
      "doi": ""
    },
    {
      "id": "b133",
      "title": "Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models",
      "authors": [
        "C Schlarmann",
        "N D Singh",
        "F Croce",
        "M Hein"
      ],
      "year": "2024",
      "venue": "Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models",
      "doi": ""
    },
    {
      "id": "b134",
      "title": "Machine vision therapy: Multimodal large language models can enhance visual robustness via denoising in-context learning",
      "authors": [
        "Z Huang",
        "C Liu",
        "Y Dong",
        "H Su",
        "S Zheng",
        "T Liu"
      ],
      "year": "2023",
      "venue": "Forty-first ICML",
      "doi": ""
    },
    {
      "id": "b135",
      "title": "Attention prompting on image for large vision-language models",
      "authors": [
        "R Yu",
        "W Yu",
        "X Wang"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b136",
      "title": "Ivtp: Instruction-guided visual token pruning for large vision-language models",
      "authors": [
        "K Huang",
        "H Zou",
        "Y Xi",
        "B Wang",
        "Z Xie",
        "L Yu"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b137",
      "title": "Chattracker: Enhancing visual tracking performance via chatting with multimodal large language model",
      "authors": [
        "Y Sun",
        "F Yu",
        "S Chen",
        "Y Zhang",
        "J Huang",
        "C Li",
        "Y Li",
        "C Wang"
      ],
      "year": "2024",
      "venue": "Chattracker: Enhancing visual tracking performance via chatting with multimodal large language model",
      "doi": ""
    },
    {
      "id": "b138",
      "title": "Optimus-1: Hybrid multimodal memory empowered agents excel in longhorizon tasks",
      "authors": [
        "Z Li",
        "Y Xie",
        "R Shao",
        "G Chen",
        "D Jiang",
        "L Nie"
      ],
      "year": "2024",
      "venue": "Optimus-1: Hybrid multimodal memory empowered agents excel in longhorizon tasks",
      "doi": ""
    },
    {
      "id": "b139",
      "title": "Cumo: Scaling multimodal llm with co-upcycled mixture-of-experts",
      "authors": [
        "J Li",
        "X Wang",
        "S Zhu",
        "C.-W Kuo",
        "L Xu",
        "F Chen",
        "J Jain",
        "H Shi",
        "L Wen"
      ],
      "year": "2024",
      "venue": "Cumo: Scaling multimodal llm with co-upcycled mixture-of-experts",
      "doi": ""
    },
    {
      "id": "b140",
      "title": "Visual anchors are strong information aggregators for multimodal large language model",
      "authors": [
        "H Liu",
        "Q You",
        "X Han",
        "Y Liu",
        "H Huang",
        "R He",
        "H Yang"
      ],
      "year": "2024",
      "venue": "Visual anchors are strong information aggregators for multimodal large language model",
      "doi": ""
    },
    {
      "id": "b141",
      "title": "Accelerating pre-training of multimodal llms via chain-of-sight",
      "authors": [
        "Z Huang",
        "K Ji",
        "B Gong",
        "Z Qing",
        "Q Zhang",
        "K Zheng",
        "J Wang",
        "J Chen",
        "M Yang"
      ],
      "year": "2024",
      "venue": "Accelerating pre-training of multimodal llms via chain-of-sight",
      "doi": ""
    },
    {
      "id": "b142",
      "title": "Dense connector for mllms",
      "authors": [
        "H Yao",
        "W Wu",
        "T Yang",
        "Y Song",
        "M Zhang",
        "H Feng",
        "Y Sun",
        "Z Li",
        "W Ouyang",
        "J Wang"
      ],
      "year": "2024",
      "venue": "Dense connector for mllms",
      "doi": ""
    },
    {
      "id": "b143",
      "title": "Weakly supervised gaussian contrastive grounding with large multimodal models for video question answering",
      "authors": [
        "H Wang",
        "C Lai",
        "Y Sun",
        "W Ge"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b144",
      "title": "Q-moe: Connector for mllms with text-driven routing",
      "authors": [
        "H Wang",
        "J Ren",
        "Y Ding",
        "L Ren",
        "H Jiang",
        "W Chen",
        "F Feng",
        "X Wang"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b145",
      "title": "Continual learning with the neural tangent ensemble",
      "authors": [
        "A S Benjamin",
        "C Pehle",
        "K Daruwalla"
      ],
      "year": "2024",
      "venue": "Continual learning with the neural tangent ensemble",
      "doi": ""
    },
    {
      "id": "b146",
      "title": "Incremental learning of retrievable skills for efficient continual task adaptation",
      "authors": [
        "D Lee",
        "M Yoo",
        "W K Kim",
        "W Choi",
        "H Woo"
      ],
      "year": "2024",
      "venue": "Incremental learning of retrievable skills for efficient continual task adaptation",
      "doi": ""
    },
    {
      "id": "b147",
      "title": "Mitigate catastrophic remembering via continual knowledge purification for noisy lifelong person re-identification",
      "authors": [
        "K Xu",
        "H Zhang",
        "Y Li",
        "Y Peng",
        "J Zhou"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b148",
      "title": "Prior-free balanced replay: Uncertaintyguided reservoir sampling for long-tailed continual learning",
      "authors": [
        "L Liu",
        "L Liu",
        "Y Cui"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b149",
      "title": "Taskaware orthogonal sparse network for exploring shared knowledge in continual learning",
      "authors": [
        "Y Hu",
        "D Cheng",
        "D Zhang",
        "N Wang",
        "T Liu",
        "X Gao"
      ],
      "year": "",
      "venue": "Taskaware orthogonal sparse network for exploring shared knowledge in continual learning",
      "doi": ""
    },
    {
      "id": "b150",
      "title": "Learning to continually learn with the bayesian principle",
      "authors": [
        "S Lee",
        "H Jeon",
        "J Son",
        "G Kim"
      ],
      "year": "2024",
      "venue": "Learning to continually learn with the bayesian principle",
      "doi": ""
    },
    {
      "id": "b151",
      "title": "Regularizing with pseudo-negatives for continual self-supervised learning",
      "authors": [
        "S Cha",
        "K Cho",
        "T Moon"
      ],
      "year": "",
      "venue": "Forty-first ICML",
      "doi": ""
    },
    {
      "id": "b152",
      "title": "Self-composing policies for scalable continual reinforcement learning",
      "authors": [
        "M Malagon",
        "J Ceberio",
        "J A Lozano"
      ],
      "year": "",
      "venue": "Forty-first ICML",
      "doi": ""
    },
    {
      "id": "b153",
      "title": "Rapid learning without catastrophic forgetting in the morris water maze",
      "authors": [
        "R Wang",
        "J Hwang",
        "A Boopathy",
        "I R Fiete"
      ],
      "year": "",
      "venue": "Rapid learning without catastrophic forgetting in the morris water maze",
      "doi": ""
    },
    {
      "id": "b154",
      "title": "Diffusion-driven data replay: A novel approach to combat forgetting in federated class continual learning",
      "authors": [
        "J Liang",
        "J Zhong",
        "H Gu",
        "Z Lu",
        "X Tang",
        "G Dai",
        "S Huang",
        "L Fan",
        "Q Yang"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b155",
      "title": "Promptccd: Learning gaussian mixture prompt pool for continual category discovery",
      "authors": [
        "F J Cendra",
        "B Zhao",
        "K Han"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b156",
      "title": "An efficient memory module for graph few-shot class-incremental learning",
      "authors": [
        "D Li",
        "A Zhang",
        "J Gao",
        "B Qi"
      ],
      "year": "2024",
      "venue": "An efficient memory module for graph few-shot class-incremental learning",
      "doi": ""
    },
    {
      "id": "b157",
      "title": "Incremental learning via robust parameter posterior fusion",
      "authors": [
        "W Sun",
        "Q Li",
        "S Zhang",
        "W Wang",
        "Y Geng"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b158",
      "title": "Hierarchical multi-label learning for incremental multilingual text recognition",
      "authors": [
        "X.-Q Liu",
        "M.-H Liu",
        "Z.-D Chen",
        "X Luo",
        "X.-S Xu"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b159",
      "title": "Overcoming spatial-temporal catastrophic forgetting for federated class-incremental learning",
      "authors": [
        "H Yu",
        "X Yang",
        "X Gao",
        "Y Feng",
        "H Wang",
        "Y Kang",
        "T Li"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b160",
      "title": "Ts-ilm: Class incremental learning for online action detection",
      "authors": [
        "L Xiaochen",
        "J Cheng",
        "Z Xia",
        "Z Chen",
        "J Shi",
        "Z Dong",
        "N Tashi"
      ],
      "year": "2024",
      "venue": "ACM Multimedia",
      "doi": ""
    },
    {
      "id": "b161",
      "title": "Harnessing neural unit dynamics for effective and scalable class-incremental learning",
      "authors": [
        "D Li",
        "T Wang",
        "J Chen",
        "W Dai",
        "Z Zeng"
      ],
      "year": "2024",
      "venue": "Harnessing neural unit dynamics for effective and scalable class-incremental learning",
      "doi": ""
    },
    {
      "id": "b162",
      "title": "inemo: Incremental neural mesh models for robust class-incremental learning",
      "authors": [
        "T Fischer",
        "Y Liu",
        "A Jesslen",
        "N Ahmed",
        "P Kaushik",
        "A Wang",
        "A L Yuille",
        "A Kortylewski",
        "E Ilg"
      ],
      "year": "2024",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b163",
      "title": "A topology-aware graph coarsening framework for continual graph learning",
      "authors": [
        "X Han",
        "Z Feng",
        "Y Ning"
      ],
      "year": "2024",
      "venue": "A topology-aware graph coarsening framework for continual graph learning",
      "doi": ""
    },
    {
      "id": "b164",
      "title": "Gacl: Exemplar-free generalized analytic continual learning",
      "authors": [
        "H Zhuang",
        "Y Chen",
        "D Fang",
        "R He",
        "K Tong",
        "H Wei",
        "Z Zeng",
        "C Chen"
      ],
      "year": "2024",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b165",
      "title": "Dynamic sub-graph distillation for robust semi-supervised continual learning",
      "authors": [
        "Y Fan",
        "Y Wang",
        "P Zhu",
        "Q Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b166",
      "title": "Vector quantization prompting for continual learning",
      "authors": [
        "L Jiao",
        "Q Lai",
        "Y Li",
        "Q Xu"
      ],
      "year": "2024",
      "venue": "Vector quantization prompting for continual learning",
      "doi": ""
    },
    {
      "id": "b167",
      "title": "Random representations outperform online continually learned representations",
      "authors": [
        "A Prabhu",
        "S Sinha",
        "P Kumaraguru",
        "P Torr",
        "O Sener",
        "P K Dokania"
      ],
      "year": "",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b168",
      "title": "Label delay in online continual learning",
      "authors": [
        "B Csaba",
        "W Zhang",
        "M Üller",
        "S.-N Lim",
        "P Torr",
        "A Bibi"
      ],
      "year": "",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b169",
      "title": "Progressive prototype evolving for dual-forgetting mitigation in non-exemplar online continual learning",
      "authors": [
        "Q Li",
        "Y Peng",
        "J Zhou"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b170",
      "title": "Introducing common null space of gradients for gradient projection methods in continual learning",
      "authors": [
        "C Yang",
        "M Dong",
        "X Zhang",
        "J Qi",
        "A Zhou"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b171",
      "title": "Provable contrastive continual learning",
      "authors": [
        "Y Wen",
        "Z Tan",
        "K Zheng",
        "C Xie",
        "W Huang"
      ],
      "year": "2024",
      "venue": "Provable contrastive continual learning",
      "doi": ""
    },
    {
      "id": "b172",
      "title": "Mitigating catastrophic forgetting in online continual learning by modeling previous task interrelations via pareto optimization",
      "authors": [
        "Y Wu",
        "H Wang",
        "P Zhao",
        "Y Zheng",
        "Y Wei",
        "L.-K Huang"
      ],
      "year": "",
      "venue": "Mitigating catastrophic forgetting in online continual learning by modeling previous task interrelations via pareto optimization",
      "doi": ""
    },
    {
      "id": "b173",
      "title": "Federated continual learning via prompt-based dual knowledge transfer",
      "authors": [
        "H Piao",
        "Y Wu",
        "D Wu",
        "Y Wei"
      ],
      "year": "",
      "venue": "Forty-first ICML",
      "doi": ""
    },
    {
      "id": "b174",
      "title": "One size fits all for semantic shifts: Adaptive prompt tuning for continual learning",
      "authors": [
        "D Kim",
        "S Yoon",
        "D Park",
        "Y Lee",
        "H Song",
        "J Bang",
        "J.-G Lee"
      ],
      "year": "2023",
      "venue": "One size fits all for semantic shifts: Adaptive prompt tuning for continual learning",
      "doi": ""
    },
    {
      "id": "b175",
      "title": "Inflora: Interference-free low-rank adaptation for continual learning",
      "authors": [
        "Y.-S Liang",
        "W.-J Li"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b176",
      "title": "F-oal: Forward-only online analytic learning with fast training and low memory footprint in class incremental learning",
      "authors": [
        "H Zhuang",
        "Y Liu",
        "R He",
        "K Tong",
        "Z Zeng",
        "C Chen",
        "Y Wang",
        "L.-P Chau"
      ],
      "year": "",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b177",
      "title": "Prospective representation learning for nonexemplar class-incremental learning",
      "authors": [
        "W Shi",
        "M Ye"
      ],
      "year": "",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b178",
      "title": "Addressing imbalance for class incremental learning in medical image classification",
      "authors": [
        "X Hao",
        "W Ni",
        "X Jiang",
        "W Tan",
        "B Yan"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b179",
      "title": "Domain shared and specific prompt learning for incremental monocular depth estimation",
      "authors": [
        "Z Yang",
        "L Li",
        "J Zhang",
        "T Wang",
        "Y Sun",
        "C Yan"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b180",
      "title": "Multi-layer rehearsal feature augmentation for class-incremental learning",
      "authors": [
        "B Zheng",
        "D.-W Zhou",
        "H.-J Ye",
        "D.-C Zhan"
      ],
      "year": "",
      "venue": "Multi-layer rehearsal feature augmentation for class-incremental learning",
      "doi": ""
    },
    {
      "id": "b181",
      "title": "Gradual divergence for seamless adaptation: A novel domain incremental learning method",
      "authors": [
        "K Jeeveswaran",
        "E Arani",
        "B Zonooz"
      ],
      "year": "2024",
      "venue": "Gradual divergence for seamless adaptation: A novel domain incremental learning method",
      "doi": ""
    },
    {
      "id": "b182",
      "title": "Expandable subspace ensemble for pre-trained model-based class-incremental learning",
      "authors": [
        "D.-W Zhou",
        "H.-L Sun",
        "H.-J Ye",
        "D.-C Zhan"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b183",
      "title": "Visual instruction tuning towards general-purpose multimodal model: A survey",
      "authors": [
        "J Huang",
        "J Zhang",
        "K Jiang",
        "H Qiu",
        "S Lu"
      ],
      "year": "2023",
      "venue": "Visual instruction tuning towards general-purpose multimodal model: A survey",
      "doi": ""
    },
    {
      "id": "b184",
      "title": "Large multimodal models: Notes on cvpr 2023 tutorial",
      "authors": [
        "C Li"
      ],
      "year": "2023",
      "venue": "Large multimodal models: Notes on cvpr 2023 tutorial",
      "doi": ""
    },
    {
      "id": "b185",
      "title": "Improved baselines with visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Y Li",
        "Y J Lee"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b186",
      "title": "Generative agents: Interactive simulacra of human behavior",
      "authors": [
        "J S Park",
        "J O'brien",
        "C J Cai",
        "M R Morris",
        "P Liang",
        "M S Bernstein"
      ],
      "year": "2023",
      "venue": "Proceedings of the 36th annual acm symposium on user interface software and technology",
      "doi": ""
    },
    {
      "id": "b187",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J W Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b188",
      "title": "Vision-language models are zero-shot reward models for reinforcement learning",
      "authors": [
        "J Rocamonde",
        "V Montesinos",
        "E Nava",
        "E Perez",
        "D Lindner"
      ],
      "year": "2023",
      "venue": "Vision-language models are zero-shot reward models for reinforcement learning",
      "doi": ""
    },
    {
      "id": "b189",
      "title": "Aligning large multimodal models with factually augmented rlhf",
      "authors": [
        "Z Sun",
        "S Shen",
        "S Cao",
        "H Liu",
        "C Li",
        "Y Shen",
        "C Gan",
        "L.-Y Gui",
        "Y.-X Wang",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "Aligning large multimodal models with factually augmented rlhf",
      "doi": ""
    },
    {
      "id": "b190",
      "title": "Continual panoptic perception: Towards multi-modal incremental interpretation of remote sensing images",
      "authors": [
        "B Yuan",
        "D Zhao",
        "Z Liu",
        "W Li",
        "T Li"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b191",
      "title": "Cp-prompt: Composition-based cross-modal prompting for domain-incremental continual learning",
      "authors": [
        "Y Feng",
        "Z Tian",
        "Y Zhu",
        "Z Han",
        "H Luo",
        "G Zhang",
        "M Song"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b192",
      "title": "Mmal: Multi-modal analytic learning for exemplarfree audio-visual class incremental tasks",
      "authors": [
        "X Yue",
        "X Zhang",
        "Y Chen",
        "C Zhang",
        "M Lao",
        "H Zhuang",
        "X Qian",
        "H Li"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b193",
      "title": "Continual multimodal knowledge graph construction",
      "authors": [
        "X Chen",
        "J Zhang",
        "X Wang",
        "N Zhang",
        "T Wu",
        "Y Wang",
        "Y Wang",
        "H Chen"
      ],
      "year": "2023",
      "venue": "Continual multimodal knowledge graph construction",
      "doi": ""
    },
    {
      "id": "b194",
      "title": "Continual self-supervised learning: Towards universal multi-modal medical data representation learning",
      "authors": [
        "Y Ye",
        "Y Xie",
        "J Zhang",
        "Z Chen",
        "Q Wu",
        "Y Xia"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b195",
      "title": "Zeroshot generalizable incremental learning for vision-language object detection",
      "authors": [
        "J Deng",
        "H Zhang",
        "K Ding",
        "J Hu",
        "X Zhang",
        "Y Wang"
      ],
      "year": "2024",
      "venue": "Zeroshot generalizable incremental learning for vision-language object detection",
      "doi": ""
    },
    {
      "id": "b196",
      "title": "Stella: Continual audio-video pre-training with spatiotemporal localized alignment",
      "authors": [
        "J Lee",
        "J Yoon",
        "W Kim",
        "Y Kim",
        "S J Hwang"
      ],
      "year": "",
      "venue": "Stella: Continual audio-video pre-training with spatiotemporal localized alignment",
      "doi": ""
    },
    {
      "id": "b197",
      "title": "Rcs-prompt: Learning prompt to rearrange class space for prompt-based continual learning",
      "authors": [
        "L Yang",
        "H Zhao",
        "Y Yu",
        "X Zeng",
        "X Li"
      ],
      "year": "2024",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b198",
      "title": "Preventing zero-shot transfer degradation in continual learning of visionlanguage models",
      "authors": [
        "Z Zheng",
        "M Ma",
        "K Wang",
        "Z Qin",
        "X Yue",
        "Y You"
      ],
      "year": "2023",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b199",
      "title": "Conditional prompt learning for vision-language models",
      "authors": [
        "K Zhou",
        "J Yang",
        "C C Loy",
        "Z Liu"
      ],
      "year": "2022",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b200",
      "title": "Advancing cross-domain discriminability in continual learning of vision-language models",
      "authors": [
        "Y Xu",
        "Y Chen",
        "J Nie",
        "Y Wang",
        "H Zhuang",
        "M Okumura"
      ],
      "year": "2024",
      "venue": "Advancing cross-domain discriminability in continual learning of vision-language models",
      "doi": ""
    },
    {
      "id": "b201",
      "title": "Learning without forgetting",
      "authors": [
        "Z Li",
        "D Hoiem"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "doi": ""
    },
    {
      "id": "b202",
      "title": "Generalized variational continual learning",
      "authors": [
        "N Loo",
        "S Swaroop",
        "R E Turner"
      ],
      "year": "2020",
      "venue": "Generalized variational continual learning",
      "doi": ""
    },
    {
      "id": "b203",
      "title": "Continual learning at the edge: Real-time training on smartphone devices",
      "authors": [
        "L Pellegrini",
        "V Lomonaco",
        "G Graffieti",
        "D Maltoni"
      ],
      "year": "2021",
      "venue": "Continual learning at the edge: Real-time training on smartphone devices",
      "doi": ""
    },
    {
      "id": "b204",
      "title": "Sparse coding in a dual memory system for lifelong learning",
      "authors": [
        "F Sarfraz",
        "E Arani",
        "B Zonooz"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b205",
      "title": "Sparsity and heterogeneous dropout for continual learning in the null space of neural activations",
      "authors": [
        "A Abbasi",
        "P Nooralinejad",
        "V Braverman",
        "H Pirsiavash",
        "S Kolouri"
      ],
      "year": "2022",
      "venue": "Conference on Lifelong Learning Agents",
      "doi": ""
    },
    {
      "id": "b206",
      "title": "etag: Class-incremental learning via embedding distillation and taskoriented generation",
      "authors": [
        "L Huang",
        "Y Zeng",
        "C Yang",
        "Z An",
        "B Diao",
        "Y Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b207",
      "title": "Kfc: Knowledge reconstruction and feedback consolidation enable efficient and effective continual generative learning",
      "authors": [
        "L Huang",
        "Z An",
        "Y Zeng",
        "Y Xu"
      ],
      "year": "2024",
      "venue": "Kfc: Knowledge reconstruction and feedback consolidation enable efficient and effective continual generative learning",
      "doi": ""
    },
    {
      "id": "b208",
      "title": "Continual learning of a mixed sequence of similar and dissimilar tasks",
      "authors": [
        "Z Ke",
        "B Liu",
        "X Huang"
      ],
      "year": "2020",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b209",
      "title": "Semantic drift compensation for classincremental learning",
      "authors": [
        "L Yu",
        "B Twardowski",
        "X Liu",
        "L Herranz",
        "K Wang",
        "Y Cheng",
        "S Jui",
        "J V D Weijer"
      ],
      "year": "2020",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b210",
      "title": "Efficient lifelong learning with a-gem",
      "authors": [
        "A Chaudhry",
        "M Ranzato",
        "M Rohrbach",
        "M Elhoseiny"
      ],
      "year": "2018",
      "venue": "Efficient lifelong learning with a-gem",
      "doi": ""
    },
    {
      "id": "b211",
      "title": "Mitigating forgetting in online continual learning via instanceaware parameterization",
      "authors": [
        "H.-J Chen",
        "A.-C Cheng",
        "D.-C Juan",
        "W Wei",
        "M Sun"
      ],
      "year": "2020",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b212",
      "title": "A continual learning survey: Defying forgetting in classification tasks",
      "authors": [
        "M De Lange",
        "R Aljundi",
        "M Masana",
        "S Parisot",
        "X Jia",
        "A Leonardis",
        "G Slabaugh",
        "T Tuytelaars"
      ],
      "year": "2021",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "doi": ""
    },
    {
      "id": "b213",
      "title": "Continual learning with filter atom swapping",
      "authors": [
        "Z Miao",
        "Z Wang",
        "W Chen",
        "Q Qiu"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b214",
      "title": "Dualnet: Continual learning, fast and slow",
      "authors": [
        "Q Pham",
        "C Liu",
        "S Hoi"
      ],
      "year": "2021",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b215",
      "title": "Parameter-level soft-masking for continual learning",
      "authors": [
        "T Konishi",
        "M Kurokawa",
        "C Ono",
        "Z Ke",
        "G Kim",
        "B Liu"
      ],
      "year": "2023",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b216",
      "title": "Memory efficient data-free distillation for continual learning",
      "authors": [
        "X Li",
        "S Wang",
        "J Sun",
        "Z Xu"
      ],
      "year": "2023",
      "venue": "Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b217",
      "title": "Variational data-free knowledge distillation for continual learning",
      "authors": [],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b218",
      "title": "Continual learning with deep generative replay",
      "authors": [
        "H Shin",
        "J K Lee",
        "J Kim",
        "J Kim"
      ],
      "year": "2017",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b219",
      "title": "Topologypreserving class-incremental learning",
      "authors": [
        "X Tao",
        "X Chang",
        "X Hong",
        "X Wei",
        "Y Gong"
      ],
      "year": "2020",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b220",
      "title": "Training networks in null space of feature covariance for continual learning",
      "authors": [
        "S Wang",
        "X Li",
        "J Sun",
        "Z Xu"
      ],
      "year": "2021",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b221",
      "title": "Decoupling learning and remembering: A bilevel memory framework with knowledge projection for task-incremental learning",
      "authors": [
        "W Sun",
        "Q Li",
        "J Zhang",
        "W Wang",
        "Y.-A Geng"
      ],
      "year": "2023",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b222",
      "title": "Ilcoc: An incremental learning framework based on contrastive one-class classifiers",
      "authors": [
        "W Sun",
        "J Zhang",
        "D Wang",
        "Y.-A Geng",
        "Q Li"
      ],
      "year": "2021",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b223",
      "title": "Divide and not forget: Ensemble of selec-tively trained experts in continual learning",
      "authors": [
        "G Rypeść",
        "S Cygert",
        "V Khan",
        "T Trzci Ński",
        "B Zieli",
        "B Twardowski"
      ],
      "year": "2024",
      "venue": "Divide and not forget: Ensemble of selec-tively trained experts in continual learning",
      "doi": ""
    },
    {
      "id": "b224",
      "title": "Prototype reminiscence and augmented asymmetric knowledge aggregation for non-exemplar class-incremental learning",
      "authors": [
        "W Shi",
        "M Ye"
      ],
      "year": "2023",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b225",
      "title": "Overcoming catastrophic forgetting in neural networks",
      "authors": [
        "J Kirkpatrick",
        "R Pascanu",
        "N Rabinowitz",
        "J Veness",
        "G Desjardins",
        "A A Rusu",
        "K Milan",
        "J Quan",
        "T Ramalho",
        "A Grabska-Barwinska"
      ],
      "year": "2017",
      "venue": "Proceedings of the national academy of sciences",
      "doi": ""
    },
    {
      "id": "b226",
      "title": "icarl: Incremental classifier and representation learning",
      "authors": [
        "S.-A Rebuffi",
        "A Kolesnikov",
        "G Sperl",
        "C H Lampert"
      ],
      "year": "2017",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b227",
      "title": "Uncertainty-based continual learning with adaptive regularization",
      "authors": [
        "H Ahn",
        "S Cha",
        "D Lee",
        "T Moon"
      ],
      "year": "2019",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b228",
      "title": "Continual learning through synaptic intelligence",
      "authors": [
        "F Zenke",
        "B Poole",
        "S Ganguli"
      ],
      "year": "2017",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b229",
      "title": "Lifelong learning with dynamically expandable networks",
      "authors": [
        "J Yoon",
        "E Yang",
        "J Lee",
        "S J Hwang"
      ],
      "year": "2017",
      "venue": "Lifelong learning with dynamically expandable networks",
      "doi": ""
    },
    {
      "id": "b230",
      "title": "A neural dirichlet process mixture model for task-free continual learning",
      "authors": [
        "S Lee",
        "J Ha",
        "D Zhang",
        "G Kim"
      ],
      "year": "2020",
      "venue": "A neural dirichlet process mixture model for task-free continual learning",
      "doi": ""
    },
    {
      "id": "b231",
      "title": "Representational continuity for unsupervised continual learning",
      "authors": [
        "D Madaan",
        "J Yoon",
        "Y Li",
        "Y Liu",
        "S J Hwang"
      ],
      "year": "2021",
      "venue": "Representational continuity for unsupervised continual learning",
      "doi": ""
    },
    {
      "id": "b232",
      "title": "Self-supervised models are continual learners",
      "authors": [
        "E Fini",
        "V G T Da Costa",
        "X Alameda-Pineda",
        "E Ricci",
        "K Alahari",
        "J Mairal"
      ],
      "year": "2022",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b233",
      "title": "Continual learners are incremental model generalizers",
      "authors": [
        "J Yoon",
        "S J Hwang",
        "Y Cao"
      ],
      "year": "2023",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b234",
      "title": "Generative negative text replay for continual vision-language pretraining",
      "authors": [
        "S Yan",
        "L Hong",
        "H Xu",
        "J Han",
        "T Tuytelaars",
        "Z Li",
        "X He"
      ],
      "year": "2022",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b235",
      "title": "Audio-visual classincremental learning",
      "authors": [
        "W Pian",
        "S Mo",
        "Y Guo",
        "Y Tian"
      ],
      "year": "2023",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b236",
      "title": "Class-incremental grouping network for continual audio-visual learning",
      "authors": [
        "S Mo",
        "W Pian",
        "Y Tian"
      ],
      "year": "2023",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b237",
      "title": "Contintin: Continual learning from task instructions",
      "authors": [
        "W Yin",
        "J Li",
        "C Xiong"
      ],
      "year": "2022",
      "venue": "Contintin: Continual learning from task instructions",
      "doi": ""
    },
    {
      "id": "b238",
      "title": "Orthogonal subspace learning for language model continual learning",
      "authors": [
        "X Wang",
        "T Chen",
        "Q Ge",
        "H Xia",
        "R Bao",
        "R Zheng",
        "Q Zhang",
        "T Gui",
        "X Huang"
      ],
      "year": "2023",
      "venue": "Orthogonal subspace learning for language model continual learning",
      "doi": ""
    },
    {
      "id": "b239",
      "title": "Dapt: A dual attention framework for parameter-efficient continual learning of large language models",
      "authors": [
        "W Zhao",
        "S Wang",
        "Y Hu",
        "Y Zhao",
        "B Qin",
        "X Zhang",
        "Q Yang",
        "D Xu",
        "W Che"
      ],
      "year": "2024",
      "venue": "Dapt: A dual attention framework for parameter-efficient continual learning of large language models",
      "doi": ""
    },
    {
      "id": "b240",
      "title": "Exploring the benefits of training expert language models over instruction tuning",
      "authors": [
        "J Jang",
        "S Kim",
        "S Ye",
        "D Kim",
        "L Logeswaran",
        "M Lee",
        "K Lee",
        "M Seo"
      ],
      "year": "2023",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b241",
      "title": "Llama pro: Progressive llama with block expansion",
      "authors": [
        "C Wu",
        "Y Gan",
        "Y Ge",
        "Z Lu",
        "J Wang",
        "Y Feng",
        "P Luo",
        "Y Shan"
      ],
      "year": "2024",
      "venue": "Llama pro: Progressive llama with block expansion",
      "doi": ""
    },
    {
      "id": "b242",
      "title": "Adapting large language models via reading comprehension",
      "authors": [
        "D Cheng",
        "S Huang",
        "F Wei"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b243",
      "title": "Large-scale lifelong learning of in-context instructions and how to tackle it",
      "authors": [
        "J Mok",
        "J Do",
        "S Lee",
        "T Taghavi",
        "S Yu",
        "S Yoon"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b244",
      "title": "Train-attention: Meta-learning where to focus in continual knowledge learning",
      "authors": [
        "Y Seo",
        "D Lee",
        "J Yeo"
      ],
      "year": "2024",
      "venue": "Train-attention: Meta-learning where to focus in continual knowledge learning",
      "doi": ""
    },
    {
      "id": "b245",
      "title": "D-cpt law: Domain-specific continual pre-training scaling law for large language models",
      "authors": [
        "H Que",
        "J Liu",
        "G Zhang",
        "C Zhang",
        "X Qu",
        "Y Ma",
        "F Duan",
        "Z Bai",
        "J Wang",
        "Y Zhang"
      ],
      "year": "2024",
      "venue": "D-cpt law: Domain-specific continual pre-training scaling law for large language models",
      "doi": ""
    },
    {
      "id": "b246",
      "title": "Copal: Continual pruning in large language generative models",
      "authors": [
        "S Malla",
        "J H Choi",
        "C Choi"
      ],
      "year": "2024",
      "venue": "Copal: Continual pruning in large language generative models",
      "doi": ""
    },
    {
      "id": "b247",
      "title": "Magmax: Leveraging model merging for seamless continual learning",
      "authors": [
        "D Marczak",
        "B Twardowski",
        "T Trzci",
        "S Cygert"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b248",
      "title": "Sapt: A shared attention framework for parameter-efficient continual learning of large language models",
      "authors": [
        "W Zhao",
        "S Wang",
        "Y Hu",
        "Y Zhao",
        "B Qin",
        "X Zhang",
        "Q Yang",
        "D Xu",
        "W Che"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b249",
      "title": "Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal",
      "authors": [
        "J Huang",
        "L Cui",
        "A Wang",
        "C Yang",
        "X Liao",
        "L Song",
        "J Yao",
        "J Su"
      ],
      "year": "2024",
      "venue": "Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal",
      "doi": ""
    },
    {
      "id": "b250",
      "title": "Loramoe: Alleviating world knowledge forgetting in large language models via moe-style plugin",
      "authors": [
        "S Dou",
        "E Zhou",
        "Y Liu",
        "S Gao",
        "W Shen",
        "L Xiong",
        "Y Zhou",
        "X Wang",
        "Z Xi",
        "X Fan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b251",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b252",
      "title": "Chinese tiny llm: Pretraining a chinesecentric large language model",
      "authors": [
        "X Du",
        "Z Yu",
        "S Gao",
        "D Pan",
        "Y Cheng",
        "Z Ma",
        "R Yuan",
        "X Qu",
        "J Liu",
        "T Zheng"
      ],
      "year": "2024",
      "venue": "Chinese tiny llm: Pretraining a chinesecentric large language model",
      "doi": ""
    },
    {
      "id": "b253",
      "title": "Gpts are gpts: An early look at the labor market impact potential of large language models",
      "authors": [
        "T Eloundou",
        "S Manning",
        "P Mishkin",
        "D Rock"
      ],
      "year": "2023",
      "venue": "Gpts are gpts: An early look at the labor market impact potential of large language models",
      "doi": ""
    },
    {
      "id": "b254",
      "title": "A literature survey on open source large language models",
      "authors": [
        "S Kukreja",
        "T Kumar",
        "A Purohit",
        "A Dasgupta",
        "D Guha"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 7th International Conference on Computers in Management and Business",
      "doi": ""
    },
    {
      "id": "b255",
      "title": "Chatgpt for good? on opportunities and challenges of large language models for education",
      "authors": [
        "E Kasneci",
        "K Seßler",
        "S K Üchemann",
        "M Bannert",
        "D Dementieva",
        "F Fischer",
        "U Gasser",
        "G Groh",
        "S Ünnemann",
        "E H Üllermeier"
      ],
      "year": "2023",
      "venue": "Learning and individual differences",
      "doi": ""
    },
    {
      "id": "b256",
      "title": "A survey of large language models",
      "authors": [
        "W X Zhao",
        "K Zhou",
        "J Li",
        "T Tang",
        "X Wang",
        "Y Hou",
        "Y Min",
        "B Zhang",
        "J Zhang",
        "Z Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "doi": ""
    },
    {
      "id": "b257",
      "title": "A comprehensive overview of large language models",
      "authors": [
        "H Naveed",
        "A U Khan",
        "S Qiu",
        "M Saqib",
        "S Anwar",
        "M Usman",
        "N Akhtar",
        "N Barnes",
        "A Mian"
      ],
      "year": "2023",
      "venue": "A comprehensive overview of large language models",
      "doi": ""
    },
    {
      "id": "b258",
      "title": "A survey on evaluation of large language models",
      "authors": [
        "Y Chang",
        "X Wang",
        "J Wang",
        "Y Wu",
        "L Yang",
        "K Zhu",
        "H Chen",
        "X Yi",
        "C Wang",
        "Y Wang"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "doi": ""
    },
    {
      "id": "b259",
      "title": "Evaluating large language models trained on code",
      "authors": [
        "M Chen",
        "J Tworek",
        "H Jun",
        "Q Yuan",
        "H P D O Pinto",
        "J Kaplan",
        "H Edwards",
        "Y Burda",
        "N Joseph",
        "G Brockman"
      ],
      "year": "2021",
      "venue": "Evaluating large language models trained on code",
      "doi": ""
    },
    {
      "id": "b260",
      "title": "Interpretutor: Using large language models for interpreter assessment",
      "authors": [
        "C Ünl Ü"
      ],
      "year": "2023",
      "venue": "HiT-IT 2023",
      "doi": ""
    },
    {
      "id": "b261",
      "title": "A survey on large language models for recommendation",
      "authors": [
        "L Wu",
        "Z Zheng",
        "Z Qiu",
        "H Wang",
        "H Gu",
        "T Shen",
        "C Qin",
        "C Zhu",
        "H Zhu",
        "Q Liu"
      ],
      "year": "2024",
      "venue": "World Wide Web",
      "doi": ""
    },
    {
      "id": "b262",
      "title": "Instruction tuning for large language models: A survey",
      "authors": [
        "S Zhang",
        "L Dong",
        "X Li",
        "S Zhang",
        "X Sun",
        "S Wang",
        "J Li",
        "R Hu",
        "T Zhang",
        "F Wu"
      ],
      "year": "2023",
      "venue": "Instruction tuning for large language models: A survey",
      "doi": ""
    },
    {
      "id": "b263",
      "title": "Llms can evolve continually on modality for x-modal reasoning",
      "authors": [
        "J Yu",
        "H Xiong",
        "L Zhang",
        "H Diao",
        "Y Zhuge",
        "L Hong",
        "D Wang",
        "H Lu",
        "Y He",
        "L Chen"
      ],
      "year": "2024",
      "venue": "Llms can evolve continually on modality for x-modal reasoning",
      "doi": ""
    },
    {
      "id": "b264",
      "title": "Mind the interference: Retaining pre-trained knowledge in parameter efficient continual learning of vision-language models",
      "authors": [
        "L Tang",
        "Z Tian",
        "K Li",
        "C He",
        "H Zhou",
        "H Zhao",
        "X Li",
        "J Jia"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b265",
      "title": "Generative multi-modal models are good class incremental learners",
      "authors": [
        "X Cao",
        "H Lu",
        "L Huang",
        "X Liu",
        "M.-M Cheng"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b266",
      "title": "Pre-trained vision and language transformers are few-shot incremental learners",
      "authors": [
        "K.-H Park",
        "K Song",
        "G.-M Park"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b267",
      "title": "Modalprompt: Dual-modality guided prompt for continual learning of large multimodal models",
      "authors": [
        "F Zeng",
        "F Zhu",
        "H Guo",
        "X.-Y Zhang",
        "C.-L Liu"
      ],
      "year": "2024",
      "venue": "Modalprompt: Dual-modality guided prompt for continual learning of large multimodal models",
      "doi": ""
    },
    {
      "id": "b268",
      "title": "Clip with generative latent replay: a strong baseline for incremental learning",
      "authors": [
        "E Frascaroli",
        "A Panariello",
        "P Buzzega",
        "L Bonicelli",
        "A Porrello",
        "S Calderara"
      ],
      "year": "2024",
      "venue": "Clip with generative latent replay: a strong baseline for incremental learning",
      "doi": ""
    },
    {
      "id": "b269",
      "title": "Coleclip: Open-domain continual learning via joint task prompt and vocabulary learning",
      "authors": [
        "Y Li",
        "G Pang",
        "W Suo",
        "C Jing",
        "Y Xi",
        "L Liu",
        "H Chen",
        "G Liang",
        "P Wang"
      ],
      "year": "2024",
      "venue": "Coleclip: Open-domain continual learning via joint task prompt and vocabulary learning",
      "doi": ""
    },
    {
      "id": "b270",
      "title": "Investigating the catastrophic forgetting in multimodal large language models",
      "authors": [
        "Y Zhai",
        "S Tong",
        "X Li",
        "M Cai",
        "Q Qu",
        "Y J Lee",
        "Y Ma"
      ],
      "year": "2023",
      "venue": "Investigating the catastrophic forgetting in multimodal large language models",
      "doi": ""
    },
    {
      "id": "b271",
      "title": "Adapt-inf ty: Scalable lifelong multimodal instruction tuning via dynamic data selection",
      "authors": [
        "A Maharana",
        "J Yoon",
        "T Chen",
        "M Bansal"
      ],
      "year": "2024",
      "venue": "Adapt-inf ty: Scalable lifelong multimodal instruction tuning via dynamic data selection",
      "doi": ""
    },
    {
      "id": "b272",
      "title": "Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training",
      "authors": [
        "G Luo",
        "X Yang",
        "W Dou",
        "Z Wang",
        "J Dai",
        "Y Qiao",
        "X Zhu"
      ],
      "year": "2024",
      "venue": "Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training",
      "doi": ""
    },
    {
      "id": "b273",
      "title": "Moextend: Tuning new experts for modality and task extension",
      "authors": [
        "S Zhong",
        "S Gao",
        "Z Huang",
        "W Wen",
        "M Zitnik",
        "P Zhou"
      ],
      "year": "2024",
      "venue": "Moextend: Tuning new experts for modality and task extension",
      "doi": ""
    },
    {
      "id": "b274",
      "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "authors": [
        "P Young",
        "A Lai",
        "M Hodosh",
        "J Hockenmaier"
      ],
      "year": "2014",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b275",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F L Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b276",
      "title": "Palm 2 technical report",
      "authors": [
        "R Anil",
        "A M Dai",
        "O Firat",
        "M Johnson",
        "D Lepikhin",
        "A Passos",
        "S Shakeri",
        "E Taropa",
        "P Bailey",
        "Z Chen"
      ],
      "year": "2023",
      "venue": "Palm 2 technical report",
      "doi": ""
    },
    {
      "id": "b277",
      "title": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "authors": [
        "J Bai",
        "S Bai",
        "S Yang",
        "S Wang",
        "S Tan",
        "P Wang",
        "J Lin",
        "C Zhou",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "doi": ""
    },
    {
      "id": "b278",
      "title": "Microsoft coco captions: Data collection and evaluation server",
      "authors": [
        "X Chen",
        "H Fang",
        "T.-Y Lin",
        "R Vedantam",
        "S Gupta",
        "P Dollár",
        "C L Zitnick"
      ],
      "year": "2015",
      "venue": "Microsoft coco captions: Data collection and evaluation server",
      "doi": ""
    },
    {
      "id": "b279",
      "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
      "authors": [
        "Z Chen",
        "J Wu",
        "W Wang",
        "W Su",
        "G Chen",
        "S Xing",
        "M Zhong",
        "Q Zhang",
        "X Zhu",
        "L Lu"
      ],
      "year": "0198",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b280",
      "title": "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis",
      "authors": [
        "C Fu",
        "Y Dai",
        "Y Luo",
        "L Li",
        "S Ren",
        "R Zhang",
        "Z Wang",
        "C Zhou",
        "Y Shen",
        "M Zhang"
      ],
      "year": "2024",
      "venue": "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis",
      "doi": ""
    },
    {
      "id": "b281",
      "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "authors": [
        "Y Goyal",
        "T Khot",
        "D Summers-Stay",
        "D Batra",
        "D Parikh"
      ],
      "year": "2017",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b282",
      "title": "Vizwiz grand challenge: Answering visual questions from blind people",
      "authors": [
        "D Gurari",
        "Q Li",
        "A J Stangl",
        "A Guo",
        "C Lin",
        "K Grauman",
        "J Luo",
        "J P Bigham"
      ],
      "year": "2018",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b283",
      "title": "Llavanext: Improved reasoning, ocr, and world knowledge",
      "authors": [
        "H Liu",
        "C Li",
        "Y Li",
        "B Li",
        "Y Zhang",
        "S Shen",
        "Y J Lee"
      ],
      "year": "2024",
      "venue": "Llavanext: Improved reasoning, ocr, and world knowledge",
      "doi": ""
    },
    {
      "id": "b284",
      "title": "Mmbench: Is your multi-modal model an all-around player?",
      "authors": [
        "Y Liu",
        "H Duan",
        "Y Zhang",
        "B Li",
        "S Zhang",
        "W Zhao",
        "Y Yuan",
        "J Wang",
        "C He",
        "Z Liu"
      ],
      "year": "2025",
      "venue": "Mmbench: Is your multi-modal model an all-around player?",
      "doi": ""
    },
    {
      "id": "b285",
      "title": "Cheap and quick: Efficient vision-language instruction tuning for large language models",
      "authors": [
        "G Luo",
        "Y Zhou",
        "T Ren",
        "S Chen",
        "X Sun",
        "R Ji"
      ],
      "year": "2024",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b286",
      "title": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "authors": [
        "Z Yang",
        "L Li",
        "K Lin",
        "J Wang",
        "C.-C Lin",
        "Z Liu",
        "L Wang"
      ],
      "year": "2023",
      "venue": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "doi": ""
    },
    {
      "id": "b287",
      "title": "Gemini: a family of highly capable multimodal models",
      "authors": [
        "G Team",
        "R Anil",
        "S Borgeaud",
        "J.-B Alayrac",
        "J Yu",
        "R Soricut",
        "J Schalkwyk",
        "A M Dai",
        "A Hauth",
        "K Millican"
      ],
      "year": "2023",
      "venue": "Gemini: a family of highly capable multimodal models",
      "doi": ""
    },
    {
      "id": "b288",
      "title": "Internlm: A multilingual language model with progressively enhanced capabilities",
      "authors": [
        "I Team"
      ],
      "year": "2023",
      "venue": "Internlm: A multilingual language model with progressively enhanced capabilities",
      "doi": ""
    },
    {
      "id": "b289",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b290",
      "title": "Cogvlm: Visual expert for pretrained language models",
      "authors": [
        "W Wang",
        "Q Lv",
        "W Yu",
        "W Hong",
        "J Qi",
        "Y Wang",
        "J Ji",
        "Z Yang",
        "L Zhao",
        "X Song"
      ],
      "year": "2023",
      "venue": "Cogvlm: Visual expert for pretrained language models",
      "doi": ""
    },
    {
      "id": "b291",
      "title": "Parameter and computation efficient transfer learning for vision-language pre-trained models",
      "authors": [
        "Q Wu",
        "W Yu",
        "Y Zhou",
        "S Huang",
        "X Sun",
        "R Ji"
      ],
      "year": "2024",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b292",
      "title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
      "authors": [
        "X Yue",
        "Y Ni",
        "K Zhang",
        "T Zheng",
        "R Liu",
        "G Zhang",
        "S Stevens",
        "D Jiang",
        "W Ren",
        "Y Sun"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b293",
      "title": "The power of scale for parameter-efficient prompt tuning",
      "authors": [
        "B Lester",
        "R Al-Rfou",
        "N Constant"
      ],
      "year": "2021",
      "venue": "The power of scale for parameter-efficient prompt tuning",
      "doi": ""
    },
    {
      "id": "b294",
      "title": "Pivot: Prompting for video continual learning",
      "authors": [
        "A Villa",
        "J L Alcázar",
        "M Alfarra",
        "K Alhamoud",
        "J Hurtado",
        "F C Heilbron",
        "A Soto",
        "B Ghanem"
      ],
      "year": "2023",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b295",
      "title": "Stabilizing zero-shot prediction: A novel antidote to forgetting in continual vision-language tasks",
      "authors": [
        "Z Gao",
        "X Zhang",
        "K Xu",
        "X Mao",
        "H Wang"
      ],
      "year": "",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b296",
      "title": "Low-rank prompt interaction for continual vision-language retrieval",
      "authors": [
        "W Yan",
        "Y Wang",
        "W Lin",
        "Z Guo",
        "Z Zhao",
        "T Jin"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b297",
      "title": "Model tailor: Mitigating catastrophic forgetting in multimodal large language models",
      "authors": [
        "D Zhu",
        "Z Sun",
        "Z Li",
        "T Shen",
        "K Yan",
        "S Ding",
        "K Kuang",
        "C Wu"
      ],
      "year": "2024",
      "venue": "Model tailor: Mitigating catastrophic forgetting in multimodal large language models",
      "doi": ""
    },
    {
      "id": "b298",
      "title": "Llaca: Multimodal large language continual assistant",
      "authors": [
        "J Qiao",
        "Z Zhang",
        "X Tan",
        "Y Qu",
        "S Ding",
        "Y Xie"
      ],
      "year": "2024",
      "venue": "Llaca: Multimodal large language continual assistant",
      "doi": ""
    },
    {
      "id": "b299",
      "title": "Continually learn to map visual concepts to large language models in resource-constrained environments",
      "authors": [
        "C Rebillard",
        "J Hurtado",
        "A Krutsylo",
        "L Passaro",
        "V Lomonaco"
      ],
      "year": "2024",
      "venue": "Continually learn to map visual concepts to large language models in resource-constrained environments",
      "doi": ""
    },
    {
      "id": "b300",
      "title": "Re-tune: Incremental fine tuning of biomedical vision-language models for multi-label chest x-ray classification",
      "authors": [
        "M Mistretta",
        "A D Bagdanov"
      ],
      "year": "2024",
      "venue": "Re-tune: Incremental fine tuning of biomedical vision-language models for multi-label chest x-ray classification",
      "doi": ""
    },
    {
      "id": "b301",
      "title": "Clumo: Cluster-based modality fusion prompt for continual learning in visual question answering",
      "authors": [
        "Y Cai",
        "M Rostami"
      ],
      "year": "2024",
      "venue": "Clumo: Cluster-based modality fusion prompt for continual learning in visual question answering",
      "doi": ""
    },
    {
      "id": "b302",
      "title": "Beyond antiforgetting: Multimodal continual instruction tuning with positive forward transfer",
      "authors": [
        "J Zheng",
        "Q Ma",
        "Z Liu",
        "B Wu",
        "H Feng"
      ],
      "year": "2024",
      "venue": "Beyond antiforgetting: Multimodal continual instruction tuning with positive forward transfer",
      "doi": ""
    },
    {
      "id": "b303",
      "title": "Multimodal parameter-efficient few-shot class incremental learning",
      "authors": [
        "M D'alessandro",
        "A Alonso",
        "E Calabrés",
        "M Galar"
      ],
      "year": "2023",
      "venue": "ICCV",
      "doi": ""
    },
    {
      "id": "b304",
      "title": "Preserving knowledge in large language model with modelagnostic self-decompression",
      "authors": [
        "Z Zhang",
        "Y Sun",
        "T Zhao",
        "L Sha",
        "R Xu",
        "K Lee",
        "J Yin"
      ],
      "year": "2024",
      "venue": "Preserving knowledge in large language model with modelagnostic self-decompression",
      "doi": ""
    },
    {
      "id": "b305",
      "title": "Lines: Post-training layer scaling prevents forgetting and enhances model merging",
      "authors": [
        "K Wang",
        "N Dimitriadis",
        "A Favero",
        "G Ortiz-Jimenez",
        "F Fleuret",
        "P Frossard"
      ],
      "year": "2024",
      "venue": "Lines: Post-training layer scaling prevents forgetting and enhances model merging",
      "doi": ""
    },
    {
      "id": "b306",
      "title": "Attriclip: A non-incremental learner for incremental knowledge learning",
      "authors": [
        "R Wang",
        "X Duan",
        "G Kang",
        "J Liu",
        "S Lin",
        "S Xu",
        "J",
        "B Zhang"
      ],
      "year": "2023",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b307",
      "title": "Continual diffusion: Continual customization of text-to-image diffusion with c-lora",
      "authors": [
        "J S Smith",
        "Y.-C Hsu",
        "L Zhang",
        "T Hua",
        "Z Kira",
        "Y Shen",
        "H Jin"
      ],
      "year": "2023",
      "venue": "Continual diffusion: Continual customization of text-to-image diffusion with c-lora",
      "doi": ""
    },
    {
      "id": "b308",
      "title": "Cirp: Cross-item relational pre-training for multimodal product bundling",
      "authors": [
        "Y Ma",
        "Y He",
        "W Zhong",
        "X Wang",
        "R Zimmermann",
        "T.-S Chua"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b309",
      "title": "Enhancing storage and computational efficiency in federated multimodal learning for large-scale models",
      "authors": [
        "Z Zhang",
        "F Qi",
        "C Xu"
      ],
      "year": "",
      "venue": "Forty-first ICML",
      "doi": ""
    },
    {
      "id": "b310",
      "title": "A concept-based explainability framework for large multimodal models",
      "authors": [
        "J Parekh",
        "P Khayatan",
        "M Shukor",
        "A Newson",
        "M Cord"
      ],
      "year": "2024",
      "venue": "A concept-based explainability framework for large multimodal models",
      "doi": ""
    },
    {
      "id": "b311",
      "title": "Adaptively building a video-language model for video captioning and retrieval without massive video pretraining",
      "authors": [
        "Z Liu",
        "X Wu",
        "S Wang",
        "J Qian"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b312",
      "title": "Bilateral adaptive cross-modal fusion prompt learning for clip",
      "authors": [
        "Q Wang",
        "K Yan",
        "S Ding"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b313",
      "title": "Fact: Teaching mllms with faithful, concise and transferable rationales",
      "authors": [
        "M Gao",
        "S Chen",
        "L Pang",
        "Y Yao",
        "J Dang",
        "W Zhang",
        "J Li",
        "S Tang",
        "Y Zhuang",
        "T.-S Chua"
      ],
      "year": "",
      "venue": "Fact: Teaching mllms with faithful, concise and transferable rationales",
      "doi": ""
    },
    {
      "id": "b314",
      "title": "Object hallucination in image captioning",
      "authors": [
        "A Rohrbach",
        "L A Hendricks",
        "K Burns",
        "T Darrell",
        "K Saenko"
      ],
      "year": "2018",
      "venue": "Object hallucination in image captioning",
      "doi": ""
    },
    {
      "id": "b315",
      "title": "Plausible may not be faithful: Probing object hallucination in vision-language pretraining",
      "authors": [
        "W Dai",
        "Z Liu",
        "Z Ji",
        "D Su",
        "P Fung"
      ],
      "year": "2022",
      "venue": "Plausible may not be faithful: Probing object hallucination in vision-language pretraining",
      "doi": ""
    },
    {
      "id": "b316",
      "title": "Evaluating object hallucination in large vision-language models",
      "authors": [
        "Y Li",
        "Y Du",
        "K Zhou",
        "J Wang",
        "W X Zhao",
        "J.-R Wen"
      ],
      "year": "2023",
      "venue": "Evaluating object hallucination in large vision-language models",
      "doi": ""
    },
    {
      "id": "b317",
      "title": "Groundhog: Grounding large language models to holistic segmentation",
      "authors": [
        "Y Zhang",
        "Z Ma",
        "X Gao",
        "S Shakiah",
        "Q Gao",
        "J Chai"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b318",
      "title": "Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption",
      "authors": [
        "B Zhai",
        "S Yang",
        "X Zhao",
        "C Xu",
        "S Shen",
        "D Zhao",
        "K Keutzer",
        "M Li",
        "T Yan",
        "X Fan"
      ],
      "year": "2023",
      "venue": "Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption",
      "doi": ""
    },
    {
      "id": "b319",
      "title": "Mitigating hallucination in large multi-modal models via robust instruction tuning",
      "authors": [
        "F Liu",
        "K Lin",
        "L Li",
        "J Wang",
        "Y Yacoob",
        "L Wang"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b320",
      "title": "Ferret: Refer and ground anything anywhere at any granularity",
      "authors": [
        "H You",
        "H Zhang",
        "Z Gan",
        "X Du",
        "B Zhang",
        "Z Wang",
        "L Cao",
        "S.-F Chang",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "Ferret: Refer and ground anything anywhere at any granularity",
      "doi": ""
    },
    {
      "id": "b321",
      "title": "Analyzing and mitigating object hallucination in large vision-language models",
      "authors": [
        "Y Zhou",
        "C Cui",
        "J Yoon",
        "L Zhang",
        "Z Deng",
        "C Finn",
        "M Bansal",
        "H Yao"
      ],
      "year": "2023",
      "venue": "Analyzing and mitigating object hallucination in large vision-language models",
      "doi": ""
    },
    {
      "id": "b322",
      "title": "An llm-free multi-dimensional benchmark for mllms hallucination evaluation",
      "authors": [
        "J Wang",
        "Y Wang",
        "G Xu",
        "J Zhang",
        "Y Gu",
        "H Jia",
        "M Yan",
        "J Zhang",
        "J Sang"
      ],
      "year": "2023",
      "venue": "An llm-free multi-dimensional benchmark for mllms hallucination evaluation",
      "doi": ""
    },
    {
      "id": "b323",
      "title": "Multi-object hallucination in vision-language models",
      "authors": [
        "X Chen",
        "Z Ma",
        "X Zhang",
        "S Xu",
        "S Qian",
        "J Yang",
        "D F Fouhey",
        "J Chai"
      ],
      "year": "2024",
      "venue": "Multi-object hallucination in vision-language models",
      "doi": ""
    },
    {
      "id": "b324",
      "title": "Yi: Open foundation models by 01",
      "authors": [
        "A Young",
        "B Chen",
        "C Li",
        "C Huang",
        "G Zhang",
        "G Zhang",
        "H Li",
        "J Zhu",
        "J Chen",
        "J Chang"
      ],
      "year": "2024",
      "venue": "Yi: Open foundation models by 01",
      "doi": ""
    },
    {
      "id": "b325",
      "title": "Glamm: Pixel grounding large multimodal model",
      "authors": [
        "H Rasheed",
        "M Maaz",
        "S Shaji",
        "A Shaker",
        "S Khan",
        "H Cholakkal",
        "R M Anwer",
        "E Xing",
        "M.-H Yang",
        "F S Khan"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b326",
      "title": "Obelics: An open web-scale filtered dataset of interleaved imagetext documents",
      "authors": [
        "H Laurenc ¸on",
        "L Saulnier",
        "L Tronchon",
        "S Bekman",
        "A Singh",
        "A Lozhkov",
        "T Wang",
        "S Karamcheti",
        "A Rush",
        "D Kiela"
      ],
      "year": "2024",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b327",
      "title": "Minicpm: Unveiling the potential of small language models with scalable training strategies",
      "authors": [
        "S Hu",
        "Y Tu",
        "X Han",
        "C He",
        "G Cui",
        "X Long",
        "Z Zheng",
        "Y Fang",
        "Y Huang",
        "W Zhao"
      ],
      "year": "2024",
      "venue": "Minicpm: Unveiling the potential of small language models with scalable training strategies",
      "doi": ""
    },
    {
      "id": "b328",
      "title": "Gpt-4o system card",
      "authors": [
        "A Hurst",
        "A Lerer",
        "A P Goucher",
        "A Perelman",
        "A Ramesh",
        "A Clark",
        "A Ostrow",
        "A Welihinda",
        "A Hayes",
        "A Radford"
      ],
      "year": "2024",
      "venue": "Gpt-4o system card",
      "doi": ""
    },
    {
      "id": "b329",
      "title": "Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation",
      "authors": [
        "Q Huang",
        "X Dong",
        "P Zhang",
        "B Wang",
        "C He",
        "J Wang",
        "D Lin",
        "W Zhang",
        "N Yu"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b330",
      "title": "mclip: Multilingual clip via cross-lingual transfer",
      "authors": [
        "G Chen",
        "L Hou",
        "Y Chen",
        "W Dai",
        "L Shang",
        "X Jiang",
        "Q Liu",
        "J Pan",
        "W Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b331",
      "title": "mblip: Efficient bootstrapping of multilingual vision-llms",
      "authors": [
        "G Geigle",
        "A Jain",
        "R Timofte",
        "G Glavaš"
      ],
      "year": "2023",
      "venue": "mblip: Efficient bootstrapping of multilingual vision-llms",
      "doi": ""
    },
    {
      "id": "b332",
      "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
      "authors": [
        "W Dai",
        "J Li",
        "D Li",
        "A M H Tiong",
        "J Zhao",
        "W Wang",
        "B Li",
        "P Fung",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
      "doi": ""
    },
    {
      "id": "b333",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "authors": [
        "G Team",
        "P Georgiev",
        "V I Lei",
        "R Burnell",
        "L Bai",
        "A Gulati",
        "G Tanzer",
        "D Vincent",
        "Z Pan",
        "S Wang"
      ],
      "year": "2024",
      "venue": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "doi": ""
    },
    {
      "id": "b334",
      "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "authors": [
        "S Bubeck",
        "V Chandrasekaran",
        "R Eldan",
        "J Gehrke",
        "E Horvitz",
        "E Kamar",
        "P Lee",
        "Y T Lee",
        "Y Li",
        "S Lundberg"
      ],
      "year": "2023",
      "venue": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "doi": ""
    },
    {
      "id": "b335",
      "title": "Does gpt-3 grasp metaphors? identifying metaphor mappings with generative language models",
      "authors": [
        "L Wachowiak",
        "D Gromann"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b336",
      "title": "Ii-bench: An image implication understanding benchmark for multimodal large language models",
      "authors": [
        "Z Liu",
        "F Fang",
        "X Feng",
        "X Du",
        "C Zhang",
        "Z Wang",
        "Y Bai",
        "Q Zhao",
        "L Fan",
        "C Gan"
      ],
      "year": "2024",
      "venue": "Ii-bench: An image implication understanding benchmark for multimodal large language models",
      "doi": ""
    },
    {
      "id": "b337",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "doi": ""
    },
    {
      "id": "b338",
      "title": "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration",
      "authors": [
        "Q Ye",
        "H Xu",
        "J Ye",
        "M Yan",
        "A Hu",
        "H Liu",
        "Q Qian",
        "J Zhang",
        "F Huang",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration",
      "doi": ""
    },
    {
      "id": "b339",
      "title": "Deepseek-vl: Towards real-world vision-language understanding",
      "authors": [
        "H Lu",
        "W Liu",
        "B Zhang",
        "B Wang",
        "K Dong",
        "B Liu",
        "J Sun",
        "T Ren",
        "Z Li",
        "H Yang",
        "Y Sun",
        "C Deng",
        "H Xu",
        "Z Xie",
        "C Ruan"
      ],
      "year": "2024",
      "venue": "Deepseek-vl: Towards real-world vision-language understanding",
      "doi": ""
    },
    {
      "id": "b340",
      "title": "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
      "authors": [
        "X Dong",
        "P Zhang",
        "Y Zang",
        "Y Cao",
        "B Wang",
        "L Ouyang",
        "X Wei",
        "S Zhang",
        "H Duan",
        "M Cao",
        "W Zhang",
        "Y Li",
        "H Yan",
        "Y Gao",
        "X Zhang",
        "W Li",
        "J Li",
        "K Chen",
        "C He",
        "X Zhang",
        "Y Qiao",
        "D Lin",
        "J Wang"
      ],
      "year": "2024",
      "venue": "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
      "doi": ""
    },
    {
      "id": "b341",
      "title": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites",
      "authors": [
        "Z Chen",
        "W Wang",
        "H Tian",
        "S Ye",
        "Z Gao",
        "E Cui",
        "W Tong",
        "K Hu",
        "J Luo",
        "Z Ma"
      ],
      "year": "2024",
      "venue": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites",
      "doi": ""
    },
    {
      "id": "b342",
      "title": "Cogvlm2: Visual language models for image and video understanding",
      "authors": [
        "W Hong",
        "W Wang",
        "M Ding",
        "W Yu",
        "Q Lv",
        "Y Wang",
        "Y Cheng",
        "S Huang",
        "J Ji",
        "Z Xue",
        "L Zhao",
        "Z Yang",
        "X Gu",
        "X Zhang",
        "G Feng",
        "D Yin",
        "Z Wang",
        "J Qi",
        "X Song",
        "P Zhang",
        "D Liu",
        "B Xu",
        "J Li",
        "Y Dong",
        "J Tang"
      ],
      "year": "2024",
      "venue": "Cogvlm2: Visual language models for image and video understanding",
      "doi": ""
    },
    {
      "id": "b343",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "authors": [
        "G Team"
      ],
      "year": "2024",
      "venue": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "doi": ""
    },
    {
      "id": "b344",
      "title": "Benchmarking and improving generator-validator consistency of language models",
      "authors": [
        "X L Li",
        "V Shrivastava",
        "S Li",
        "T Hashimoto",
        "P Liang"
      ],
      "year": "2023",
      "venue": "Benchmarking and improving generator-validator consistency of language models",
      "doi": ""
    },
    {
      "id": "b345",
      "title": "Generating with confidence: Uncertainty quantification for black-box large language models",
      "authors": [
        "Z Lin",
        "S Trivedi",
        "J Sun"
      ],
      "year": "2023",
      "venue": "Generating with confidence: Uncertainty quantification for black-box large language models",
      "doi": ""
    },
    {
      "id": "b346",
      "title": "Unveiling the tapestry of consistency in large vision-language models",
      "authors": [
        "Y Zhang",
        "F Xiao",
        "T Huang",
        "C.-K Fan",
        "H Dong",
        "J Li",
        "J Wang",
        "K Cheng",
        "S Zhang",
        "H Guo"
      ],
      "year": "2024",
      "venue": "Unveiling the tapestry of consistency in large vision-language models",
      "doi": ""
    },
    {
      "id": "b347",
      "title": "Gemini: A family of highly capable multimodal models",
      "authors": [
        "G Team"
      ],
      "year": "2024",
      "venue": "Gemini: A family of highly capable multimodal models",
      "doi": ""
    },
    {
      "id": "b348",
      "title": "Mini-gemini: Mining the potential of multimodality vision language models",
      "authors": [
        "Y Li",
        "Y Zhang",
        "C Wang",
        "Z Zhong",
        "Y Chen",
        "R Chu",
        "S Liu",
        "J Jia"
      ],
      "year": "2024",
      "venue": "Mini-gemini: Mining the potential of multimodality vision language models",
      "doi": ""
    },
    {
      "id": "b349",
      "title": "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models",
      "authors": [
        "F Li",
        "R Zhang",
        "H Zhang",
        "Y Zhang",
        "B Li",
        "W Li",
        "Z Ma",
        "C Li"
      ],
      "year": "2024",
      "venue": "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models",
      "doi": ""
    },
    {
      "id": "b350",
      "title": "Gqa: A new dataset for realworld visual reasoning and compositional question answering",
      "authors": [
        "D A Hudson",
        "C D Manning"
      ],
      "year": "2019",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b351",
      "title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
      "authors": [
        "P Lu",
        "S Mishra",
        "T Xia",
        "L Qiu",
        "K.-W Chang",
        "S.-C Zhu",
        "O Tafjord",
        "P Clark",
        "A Kalyan"
      ],
      "year": "2022",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b352",
      "title": "Compbench: A comparative reasoning benchmark for multimodal llms",
      "authors": [
        "J Kil",
        "Z Mai",
        "J Lee",
        "Z Wang",
        "K Cheng",
        "L Wang",
        "Y Liu",
        "A Chowdhury",
        "W.-L Chao"
      ],
      "year": "2024",
      "venue": "Compbench: A comparative reasoning benchmark for multimodal llms",
      "doi": ""
    },
    {
      "id": "b353",
      "title": "Vila: On pre-training for visual language models",
      "authors": [
        "J Lin",
        "H Yin",
        "W Ping",
        "P Molchanov",
        "M Shoeybi",
        "S Han"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b354",
      "title": "Hallu-pi: Evaluating hallucination in multi-modal large language models within perturbed inputs",
      "authors": [
        "P Ding",
        "J Wu",
        "J Kuang",
        "D Ma",
        "X Cao",
        "X Cai",
        "S Chen",
        "J Chen",
        "S Huang"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b355",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "D Zhu",
        "J Chen",
        "X Shen",
        "X Li",
        "M Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "doi": ""
    },
    {
      "id": "b356",
      "title": "Minigpt-v2: large language model as a unified interface for vision-language multitask learning",
      "authors": [
        "J Chen",
        "D Zhu",
        "X Shen",
        "X Li",
        "Z Liu",
        "P Zhang",
        "R Krishnamoorthi",
        "V Chandra",
        "Y Xiong",
        "M Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-v2: large language model as a unified interface for vision-language multitask learning",
      "doi": ""
    },
    {
      "id": "b357",
      "title": "mplug-2: A modularized multi-modal foundation model across text, image and video",
      "authors": [
        "H Xu",
        "Q Ye",
        "M Yan",
        "Y Shi",
        "J Ye",
        "Y Xu",
        "C Li",
        "B Bi",
        "Q Qian",
        "W Wang"
      ],
      "year": "2023",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b358",
      "title": "Internlm2 technical report",
      "authors": [
        "Z Cai",
        "M Cao",
        "H Chen",
        "K Chen",
        "K Chen",
        "X Chen",
        "X Chen",
        "Z Chen",
        "Z Chen",
        "P Chu"
      ],
      "year": "2024",
      "venue": "Internlm2 technical report",
      "doi": ""
    },
    {
      "id": "b359",
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Z Du",
        "Y Qian",
        "X Liu",
        "M Ding",
        "J Qiu",
        "Z Yang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b360",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Q Ye",
        "H Xu",
        "G Xu",
        "J Ye",
        "M Yan",
        "Y Zhou",
        "J Wang",
        "A Hu",
        "P Shi",
        "Y Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "doi": ""
    },
    {
      "id": "b361",
      "title": "Shikra: Unleashing multimodal llm's referential dialogue magic",
      "authors": [
        "K Chen",
        "Z Zhang",
        "W Zeng",
        "R Zhang",
        "F Zhu",
        "R Zhao"
      ],
      "year": "2023",
      "venue": "Shikra: Unleashing multimodal llm's referential dialogue magic",
      "doi": ""
    },
    {
      "id": "b362",
      "title": "Reform-eval: Evaluating large vision language models via unified re-formulation of task-oriented benchmarks",
      "authors": [
        "Z Li",
        "Y Wang",
        "M Du",
        "Q Liu",
        "B Wu",
        "J Zhang",
        "C Zhou",
        "Z Fan",
        "J Fu",
        "J Chen"
      ],
      "year": "",
      "venue": "Reform-eval: Evaluating large vision language models via unified re-formulation of task-oriented benchmarks",
      "doi": ""
    },
    {
      "id": "b363",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b364",
      "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
      "authors": [
        "W Dai",
        "J Li",
        "D Li",
        "A M H Tiong",
        "J Zhao",
        "W Wang",
        "B Li",
        "P Fung",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
      "doi": ""
    },
    {
      "id": "b365",
      "title": "Pandagpt: One model to instruction-follow them all",
      "authors": [
        "Y Su",
        "T Lan",
        "H Li",
        "J Xu",
        "Y Wang",
        "D Cai"
      ],
      "year": "2023",
      "venue": "Pandagpt: One model to instruction-follow them all",
      "doi": ""
    },
    {
      "id": "b366",
      "title": "Imagebind-llm: Multi-modality instruction tuning",
      "authors": [
        "J Han",
        "R Zhang",
        "W Shao",
        "P Gao",
        "P Xu",
        "H Xiao",
        "K Zhang",
        "C Liu",
        "S Wen",
        "Z Guo"
      ],
      "year": "2023",
      "venue": "Imagebind-llm: Multi-modality instruction tuning",
      "doi": ""
    },
    {
      "id": "b367",
      "title": "Parameter-efficient visual instruction model",
      "authors": [
        "P Gao",
        "J Han",
        "R Zhang",
        "Z Lin",
        "S Geng",
        "A Zhou",
        "W Zhang",
        "P Lu",
        "C He",
        "X Yue"
      ],
      "year": "2023",
      "venue": "Parameter-efficient visual instruction model",
      "doi": ""
    },
    {
      "id": "b368",
      "title": "Multimodal-gpt: A vision and language model for dialogue with humans",
      "authors": [
        "T Gong",
        "C Lyu",
        "S Zhang",
        "Y Wang",
        "M Zheng",
        "Q Zhao",
        "K Liu",
        "W Zhang",
        "P Luo",
        "K Chen"
      ],
      "year": "2023",
      "venue": "Multimodal-gpt: A vision and language model for dialogue with humans",
      "doi": ""
    },
    {
      "id": "b369",
      "title": "What matters in training a gpt4-style language model with multimodal inputs",
      "authors": [
        "Y Zeng",
        "H Zhang",
        "J Zheng",
        "J Xia",
        "G Wei",
        "Y Wei",
        "Y Zhang",
        "T Kong",
        "R Song"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference of the North American Chapter",
      "doi": ""
    },
    {
      "id": "b370",
      "title": "Empowering vision-language models to follow interleaved vision-language instructions",
      "authors": [
        "J Li",
        "K Pan",
        "Z Ge",
        "M Gao",
        "H Zhang",
        "W Ji",
        "W Zhang",
        "T.-S Chua",
        "S Tang",
        "Y Zhuang"
      ],
      "year": "2023",
      "venue": "Empowering vision-language models to follow interleaved vision-language instructions",
      "doi": ""
    },
    {
      "id": "b371",
      "title": "Bliva: A simple multimodal llm for better handling of text-rich visual questions",
      "authors": [
        "W Hu",
        "Y Xu",
        "Y Li",
        "W Li",
        "Z Chen",
        "Z Tu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b372",
      "title": "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
      "authors": [
        "H Luo",
        "Q Sun",
        "C Xu",
        "P Zhao",
        "J Lou",
        "C Tao",
        "X Geng",
        "Q Lin",
        "S Chen",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
      "doi": ""
    },
    {
      "id": "b373",
      "title": "Can language models solve graph problems in natural language",
      "authors": [
        "H Wang",
        "S Feng",
        "T He",
        "Z Tan",
        "X Han",
        "Y Tsvetkov"
      ],
      "year": "2024",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b374",
      "title": "Visiongraph: Leveraging large multimodal models for graph theory problems in visual context",
      "authors": [
        "Y Li",
        "B Hu",
        "H Shi",
        "W Wang",
        "L Wang",
        "M Zhang"
      ],
      "year": "2024",
      "venue": "Visiongraph: Leveraging large multimodal models for graph theory problems in visual context",
      "doi": ""
    },
    {
      "id": "b375",
      "title": "Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models",
      "authors": [
        "Z Lin",
        "C Liu",
        "R Zhang",
        "P Gao",
        "L Qiu",
        "H Xiao",
        "H Qiu",
        "C Lin",
        "W Shao",
        "K Chen"
      ],
      "year": "2023",
      "venue": "Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models",
      "doi": ""
    },
    {
      "id": "b376",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Z Cheng",
        "Z.-Q Cheng",
        "J.-Y He",
        "J Sun",
        "K Wang",
        "Y Lin",
        "Z Lian",
        "X Peng",
        "A Hauptmann"
      ],
      "year": "2024",
      "venue": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "doi": ""
    },
    {
      "id": "b377",
      "title": "Multimodal large language models make text-to-image generative models align better",
      "authors": [
        "X Wu",
        "S Huang",
        "G Wang",
        "J Xiong",
        "F Wei"
      ],
      "year": "",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b378",
      "title": "Genartist: Multimodal llm as an agent for unified image generation and editing",
      "authors": [
        "Z Wang",
        "A Li",
        "Z Li",
        "X Liu"
      ],
      "year": "2024",
      "venue": "Genartist: Multimodal llm as an agent for unified image generation and editing",
      "doi": ""
    },
    {
      "id": "b379",
      "title": "Visual sketchpad: Sketching as a visual chain of thought for multimodal language models",
      "authors": [
        "Y Hu",
        "W Shi",
        "X Fu",
        "D Roth",
        "M Ostendorf",
        "L Zettlemoyer",
        "N A Smith",
        "R Krishna"
      ],
      "year": "2024",
      "venue": "Visual sketchpad: Sketching as a visual chain of thought for multimodal language models",
      "doi": ""
    },
    {
      "id": "b380",
      "title": "Restoreagent: Autonomous image restoration agent via multimodal large language models",
      "authors": [
        "H Chen",
        "W Li",
        "J Gu",
        "J Ren",
        "S Chen",
        "T Ye",
        "R Pei",
        "K Zhou",
        "F Song",
        "L Zhu"
      ],
      "year": "2024",
      "venue": "Restoreagent: Autonomous image restoration agent via multimodal large language models",
      "doi": ""
    },
    {
      "id": "b381",
      "title": "Finecliper: Multi-modal fine-grained clip for dynamic facial expression recognition with adapters",
      "authors": [
        "H Chen",
        "H Huang",
        "J Dong",
        "M Zheng",
        "D Shao"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b382",
      "title": "Sleepmg: Multimodal generalizable sleep staging with inter-modal balance of classification and domain discrimination",
      "authors": [
        "S Ma",
        "Y Zhang",
        "Q Zhang",
        "Y Chen",
        "H Wang",
        "Z Jia"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b383",
      "title": "Moba: Mixture of bi-directional adapter for multi-modal sarcasm detection",
      "authors": [
        "Y Xie",
        "Z Zhu",
        "X Chen",
        "Z Chen",
        "Z Huang"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b384",
      "title": "X-prompt: Multi-modal visual prompt for video object segmentation",
      "authors": [
        "P Guo",
        "W Li",
        "H Huang",
        "L Hong",
        "X Zhou",
        "Z Chen",
        "J Li",
        "K Jiang",
        "W Zhang",
        "W Zhang"
      ],
      "year": "",
      "venue": "X-prompt: Multi-modal visual prompt for video object segmentation",
      "doi": ""
    },
    {
      "id": "b385",
      "title": "Robust multimodal sentiment analysis of image-text pairs by distribution-based feature recovery and fusion",
      "authors": [
        "D Wu",
        "D Yang",
        "Y Zhou",
        "C Ma"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b386",
      "title": "Chain of visual perception: Harnessing multimodal large language models for zero-shot camouflaged object detection",
      "authors": [
        "L Tang",
        "P.-T Jiang",
        "Z.-H Shen",
        "H Zhang",
        "J.-W Chen",
        "B Li"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b387",
      "title": "Autograph: Enabling visual context via graph alignment in open domain multi-modal dialogue generation",
      "authors": [
        "D Zhao",
        "D Han",
        "Y Yuan",
        "B Ning",
        "M Li",
        "Z He",
        "S Song"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b388",
      "title": "Dysarl: Dynamic structureaware representation learning for multimodal knowledge graph reasoning",
      "authors": [
        "K Liu",
        "F Zhao",
        "Y Yang",
        "G Xu"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b389",
      "title": "Ct2c-qa: Multimodal question answering over chinese text, table and chart",
      "authors": [
        "B Zhao",
        "T Cheng",
        "Y Zhang",
        "Y Cheng",
        "R Feng",
        "X Zhang"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b390",
      "title": "Hivg: Hierarchical multimodal fine-grained modulation for visual grounding",
      "authors": [
        "L Xiao",
        "X Yang",
        "F Peng",
        "Y Wang",
        "C Xu"
      ],
      "year": "",
      "venue": "ACM MM, 2024",
      "doi": ""
    },
    {
      "id": "b391",
      "title": "White-box multimodal jailbreaks against large vision-language models",
      "authors": [
        "R Wang",
        "X Ma",
        "H Zhou",
        "C Ji",
        "G Ye",
        "Y.-G Jiang"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b392",
      "title": "Miko: multimodal intention knowledge distillation from large language models for social-media commonsense discovery",
      "authors": [
        "F Lu",
        "W Wang",
        "Y Luo",
        "Z Zhu",
        "Q Sun",
        "B Xu",
        "H Shi",
        "S Gao",
        "Q Li",
        "Y Song"
      ],
      "year": "",
      "venue": "Miko: multimodal intention knowledge distillation from large language models for social-media commonsense discovery",
      "doi": ""
    },
    {
      "id": "b393",
      "title": "Como-nas: Core-structures-guided multi-objective neural architecture search for multi-modal classification",
      "authors": [
        "P Fu",
        "X Liang",
        "Y Qian",
        "Q Guo",
        "Z Wei",
        "W Li"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b394",
      "title": "Hawkeye: Discovering and grounding implicit anomalous sentiment in recon-videos via scene-enhanced video large language model",
      "authors": [
        "J Zhao",
        "J Wang",
        "Y Jin",
        "J Luo",
        "G Zhou"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b395",
      "title": "Differentialperceptive and retrieval-augmented mllm for change captioning",
      "authors": [
        "X Zhang",
        "H Wen",
        "J Wu",
        "P Qin",
        "H Xue",
        "'",
        "L Nie"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b396",
      "title": "Gpt4video: A unified multimodal large language model for lnstruction-followed understanding and safety-aware generation",
      "authors": [
        "Z Wang",
        "L Wang",
        "Z Zhao",
        "M Wu",
        "C Lyu",
        "H Li",
        "D Cai",
        "L Zhou",
        "S Shi",
        "Z Tu"
      ],
      "year": "",
      "venue": "Gpt4video: A unified multimodal large language model for lnstruction-followed understanding and safety-aware generation",
      "doi": ""
    },
    {
      "id": "b397",
      "title": "Reason-and-execute prompting: Enhancing multi-modal large language models for solving geometry questions",
      "authors": [
        "X Duan",
        "D Tan",
        "L Fang",
        "Y Zhou",
        "C He",
        "Z Chen",
        "L Wu",
        "G Chen",
        "Z Gong",
        "W Luo"
      ],
      "year": "",
      "venue": "Reason-and-execute prompting: Enhancing multi-modal large language models for solving geometry questions",
      "doi": ""
    },
    {
      "id": "b398",
      "title": "Gallerygpt: Analyzing paintings with large multimodal models",
      "authors": [
        "Y Bin",
        "W Shi",
        "Y Ding",
        "Z Hu",
        "Z Wang",
        "Y Yang",
        "S.-K Ng",
        "H T Shen"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b399",
      "title": "Videopoet: A large language model for zero-shot video generation",
      "authors": [
        "D Kondratyuk",
        "L Yu",
        "X Gu",
        "J Lezama",
        "J Huang",
        "G Schindler",
        "R Hornung",
        "V Birodkar",
        "J Yan",
        "M.-C Chiu"
      ],
      "year": "2023",
      "venue": "Videopoet: A large language model for zero-shot video generation",
      "doi": ""
    },
    {
      "id": "b400",
      "title": "Safety finetuning at (almost) no cost: A baseline for vision large language models",
      "authors": [
        "Y Zong",
        "O Bohdal",
        "T Yu",
        "Y Yang",
        "T Hospedales"
      ],
      "year": "2024",
      "venue": "Safety finetuning at (almost) no cost: A baseline for vision large language models",
      "doi": ""
    },
    {
      "id": "b401",
      "title": "Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization",
      "authors": [
        "Y Jin",
        "Z Sun",
        "K Xu",
        "L Chen",
        "H Jiang",
        "Q Huang",
        "C Song",
        "Y Liu",
        "D Zhang",
        "Y Song"
      ],
      "year": "2024",
      "venue": "Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization",
      "doi": ""
    },
    {
      "id": "b402",
      "title": "Momentor: Advancing video large language model with fine-grained temporal reasoning",
      "authors": [
        "L Qian",
        "J Li",
        "Y Wu",
        "Y Ye",
        "H Fei",
        "T.-S Chua",
        "Y Zhuang",
        "S Tang"
      ],
      "year": "2024",
      "venue": "Momentor: Advancing video large language model with fine-grained temporal reasoning",
      "doi": ""
    },
    {
      "id": "b403",
      "title": "Bat: Learning to reason about spatial sounds with large language models",
      "authors": [
        "Z Zheng",
        "P Peng",
        "Z Ma",
        "X Chen",
        "E Choi",
        "D Harwath"
      ],
      "year": "2024",
      "venue": "Bat: Learning to reason about spatial sounds with large language models",
      "doi": ""
    },
    {
      "id": "b404",
      "title": "video-salmonn: Speech-enhanced audiovisual large language models",
      "authors": [
        "G Sun",
        "W Yu",
        "C Tang",
        "X Chen",
        "T Tan",
        "W Li",
        "L Lu",
        "Z Ma",
        "Y Wang",
        "C Zhang"
      ],
      "year": "2024",
      "venue": "video-salmonn: Speech-enhanced audiovisual large language models",
      "doi": ""
    },
    {
      "id": "b405",
      "title": "Georeasoner: Geo-localization with reasoning in street views using a large vision-language model",
      "authors": [
        "L Li",
        "Y Ye",
        "B Jiang",
        "W Zeng"
      ],
      "year": "",
      "venue": "Georeasoner: Geo-localization with reasoning in street views using a large vision-language model",
      "doi": ""
    },
    {
      "id": "b406",
      "title": "Cascade-clip: Cascaded vision-language embeddings alignment for zero-shot semantic segmentation",
      "authors": [
        "Y Li",
        "Z Li",
        "Q Zeng",
        "Q Hou",
        "M.-M Cheng"
      ],
      "year": "2024",
      "venue": "Cascade-clip: Cascaded vision-language embeddings alignment for zero-shot semantic segmentation",
      "doi": ""
    },
    {
      "id": "b407",
      "title": "Understanding forgetting in continual learning with linear regression",
      "authors": [
        "M Ding",
        "K Ji",
        "D Wang",
        "J Xu"
      ],
      "year": "2024",
      "venue": "Understanding forgetting in continual learning with linear regression",
      "doi": ""
    },
    {
      "id": "b408",
      "title": "Safe: Slow and fast parameter-efficient tuning for continual learning with pre-trained models",
      "authors": [
        "L Zhao",
        "X Zhang",
        "K Yan",
        "S Ding",
        "W Huang"
      ],
      "year": "2024",
      "venue": "Safe: Slow and fast parameter-efficient tuning for continual learning with pre-trained models",
      "doi": ""
    },
    {
      "id": "b409",
      "title": "Make continual learning stronger via c-flat",
      "authors": [
        "A Bian",
        "W Li",
        "H Yuan",
        "C Yu",
        "Z Zhao",
        "M Wang",
        "A Lu",
        "T Feng"
      ],
      "year": "2024",
      "venue": "Make continual learning stronger via c-flat",
      "doi": ""
    },
    {
      "id": "b410",
      "title": "Visual prompt tuning in null space for continual learning",
      "authors": [
        "Y Lu",
        "S Zhang",
        "D Cheng",
        "Y Xing",
        "N Wang",
        "P Wang",
        "Y Zhang"
      ],
      "year": "2024",
      "venue": "Visual prompt tuning in null space for continual learning",
      "doi": ""
    },
    {
      "id": "b411",
      "title": "Disentangled continual graph neural architecture search with invariant modular supernet",
      "authors": [
        "Z Zhang",
        "X Wang",
        "Y Qin",
        "H Chen",
        "Z Zhang",
        "X Chu",
        "W Zhu"
      ],
      "year": "",
      "venue": "Disentangled continual graph neural architecture search with invariant modular supernet",
      "doi": ""
    },
    {
      "id": "b412",
      "title": "Bayesian adaptation of network depth and width for continual learning",
      "authors": [
        "J Thapa",
        "R Li"
      ],
      "year": "",
      "venue": "Forty-first ICML",
      "doi": ""
    },
    {
      "id": "b413",
      "title": "Rethinking momentum knowledge distillation in online continual learning",
      "authors": [
        "N Michel",
        "M Wang",
        "L Xiao",
        "T Yamasaki"
      ],
      "year": "2023",
      "venue": "Rethinking momentum knowledge distillation in online continual learning",
      "doi": ""
    },
    {
      "id": "b414",
      "title": "An effective dynamic gradient calibration method for continual learning",
      "authors": [
        "W Lin",
        "J Chen",
        "R Huang",
        "H Ding"
      ],
      "year": "2024",
      "venue": "An effective dynamic gradient calibration method for continual learning",
      "doi": ""
    },
    {
      "id": "b415",
      "title": "Anytime continual learning for open vocabulary classification",
      "authors": [
        "Z Zhu",
        "Y Gong",
        "D Hoiem"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b416",
      "title": "Replay-and-forget-free graph class-incremental learning: A task profiling and prompting approach",
      "authors": [
        "C Niu",
        "G Pang",
        "L Chen",
        "B Liu"
      ],
      "year": "2024",
      "venue": "Replay-and-forget-free graph class-incremental learning: A task profiling and prompting approach",
      "doi": ""
    },
    {
      "id": "b417",
      "title": "Class balance matters to active class-incremental learning",
      "authors": [
        "Z Huang",
        "Z Chen",
        "Y Li",
        "B Dong",
        "E Zhou",
        "Y Liu",
        "R S M Goh",
        "C.-M Feng",
        "W Zuo"
      ],
      "year": "",
      "venue": "Class balance matters to active class-incremental learning",
      "doi": ""
    },
    {
      "id": "b418",
      "title": "Importance-aware shared parameter subspace learning for domain incremental learning",
      "authors": [
        "S Wang",
        "C Li",
        "J Tang",
        "X Gong",
        "Y Yuan",
        "G Wang"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b419",
      "title": "Select and distill: Selective dual-teacher knowledge transfer for continual learning on vision-language models",
      "authors": [
        "Y.-C Yu",
        "C.-P Huang",
        "J.-J Chen",
        "K.-P Chang",
        "Y.-H Lai",
        "F.-E Yang",
        "Y.-C F Wang"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b420",
      "title": "Boosting continual learning of vision-language models via mixture-ofexperts adapters",
      "authors": [
        "J Yu",
        "Y Zhuge",
        "L Zhang",
        "P Hu",
        "D Wang",
        "H Lu",
        "Y He"
      ],
      "year": "2024",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b421",
      "title": "Calibrating prompt from history for continual vision-language retrieval and grounding",
      "authors": [
        "T Jin",
        "W Yan",
        "Y Wang",
        "S Cai",
        "Q Shuai",
        "Z Zhao"
      ],
      "year": "2024",
      "venue": "ACM MM",
      "doi": ""
    },
    {
      "id": "b422",
      "title": "Semantic residual prompts for continual learning",
      "authors": [
        "M Menabue",
        "E Frascaroli",
        "M Boschini",
        "E Sangineto",
        "L Bonicelli",
        "A Porrello",
        "S Calderara"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b423",
      "title": "Class-incremental learning with clip: Adaptive representation adjustment and parameter fusion",
      "authors": [
        "L Huang",
        "X Cao",
        "H Lu",
        "X Liu"
      ],
      "year": "2025",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b424",
      "title": "Forgetting before learning: Utilizing parametric arithmetic for knowledge updating in large language models",
      "authors": [
        "S Ni",
        "D Chen",
        "C Li",
        "X Hu",
        "R Xu",
        "M Yang"
      ],
      "year": "2023",
      "venue": "Forgetting before learning: Utilizing parametric arithmetic for knowledge updating in large language models",
      "doi": ""
    },
    {
      "id": "b425",
      "title": "Learn or recall? revisiting incremental learning with pre-trained language models",
      "authors": [
        "J Zheng",
        "S Qiu",
        "Q Ma"
      ],
      "year": "2023",
      "venue": "Learn or recall? revisiting incremental learning with pre-trained language models",
      "doi": ""
    },
    {
      "id": "b426",
      "title": "Fine-tuned language models are continual learners",
      "authors": [
        "T Scialom",
        "T Chakrabarty",
        "S Muresan"
      ],
      "year": "2022",
      "venue": "Fine-tuned language models are continual learners",
      "doi": ""
    },
    {
      "id": "b427",
      "title": "Reformulating domain adaptation of large language models as adapt-retrieverevise",
      "authors": [
        "Y Zhang",
        "Y Wang",
        "F Cheng",
        "S Kurohashi"
      ],
      "year": "2023",
      "venue": "Reformulating domain adaptation of large language models as adapt-retrieverevise",
      "doi": ""
    },
    {
      "id": "b428",
      "title": "How abilities in large language models are affected by supervised fine-tuning data composition",
      "authors": [
        "G Dong",
        "H Yuan",
        "K Lu",
        "C Li",
        "M Xue",
        "D Liu",
        "W Wang",
        "Z Yuan",
        "C Zhou",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "How abilities in large language models are affected by supervised fine-tuning data composition",
      "doi": ""
    },
    {
      "id": "b429",
      "title": "Enhancing multiple-choice question answering through sequential fine-tuning and curriculum learning strategies",
      "authors": [
        "G Yigit",
        "M F Amasyali"
      ],
      "year": "2023",
      "venue": "Knowledge and Information Systems",
      "doi": ""
    },
    {
      "id": "b430",
      "title": "On tiny episodic memories in continual learning",
      "authors": [
        "A Chaudhry",
        "M Rohrbach",
        "M Elhoseiny",
        "T Ajanthan",
        "P K Dokania",
        "P H Torr",
        "M Ranzato"
      ],
      "year": "2019",
      "venue": "On tiny episodic memories in continual learning",
      "doi": ""
    },
    {
      "id": "b431",
      "title": "Coda-prompt: Continual decomposed attention-based prompting for rehearsalfree continual learning",
      "authors": [
        "J S Smith",
        "L Karlinsky",
        "V Gutta",
        "P Cascante-Bonilla",
        "D Kim",
        "A Arbelle",
        "R Panda",
        "R Feris",
        "Z Kira"
      ],
      "year": "2023",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b432",
      "title": "Dualprompt: Complementary prompting for rehearsal-free continual learning",
      "authors": [
        "Z Wang",
        "Z Zhang",
        "S Ebrahimi",
        "R Sun",
        "H Zhang",
        "C.-Y Lee",
        "X Ren",
        "G Su",
        "V Perot",
        "J Dy"
      ],
      "year": "2022",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b433",
      "title": "On the effectiveness of lipschitz-driven rehearsal in continual learning",
      "authors": [
        "L Bonicelli",
        "M Boschini",
        "A Porrello",
        "C Spampinato",
        "S Calderara"
      ],
      "year": "2022",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b434",
      "title": "Memory aware synapses: Learning what (not) to forget",
      "authors": [
        "R Aljundi",
        "F Babiloni",
        "M Elhoseiny",
        "M Rohrbach",
        "T Tuytelaars"
      ],
      "year": "2018",
      "venue": "ECCV",
      "doi": ""
    },
    {
      "id": "b435",
      "title": "Large scale incremental learning",
      "authors": [
        "Y Wu",
        "Y Chen",
        "L Wang",
        "Y Ye",
        "Z Liu",
        "Y Guo",
        "Y Fu"
      ],
      "year": "2019",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b436",
      "title": "Quantized prompt for efficient generalization of vision-language models",
      "authors": [
        "H Chen",
        "G Ding"
      ],
      "year": "2024",
      "venue": "Quantized prompt for efficient generalization of vision-language models",
      "doi": ""
    },
    {
      "id": "b437",
      "title": "Improving multimodal large language models using continual learning",
      "authors": [
        "S Srivastava",
        "M Y Harun",
        "R Shrestha",
        "C Kanan"
      ],
      "year": "2024",
      "venue": "Improving multimodal large language models using continual learning",
      "doi": ""
    },
    {
      "id": "b438",
      "title": "Efficient continual pre-training by mitigating the stability gap",
      "authors": [
        "Y Guo",
        "J Fu",
        "H Zhang",
        "D Zhao",
        "Y Shen"
      ],
      "year": "2024",
      "venue": "Efficient continual pre-training by mitigating the stability gap",
      "doi": ""
    },
    {
      "id": "b439",
      "title": "Continual instruction tuning for large multimodal models",
      "authors": [
        "J He",
        "H Guo",
        "M Tang",
        "J Wang"
      ],
      "year": "2023",
      "venue": "Continual instruction tuning for large multimodal models",
      "doi": ""
    },
    {
      "id": "b440",
      "title": "TABLE 20: The results of generative task on image concatenation, cropping, and prompt misleading",
      "authors": [
        "Y Cai",
        "M Rostami"
      ],
      "year": "2024",
      "venue": "MLLMs Image Concatenation Image Cropping Prompt Misleading CHAIR Cover Hal Cog Hal Hal Before After Before After Before After Before After Before After Before After CogVLM",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "When Continue Learning Meets Multimodal Large Language Model: A Survey",
      "text": "Yukang Huo Hao Tang1 Yukang Huo is with the School of College of Information and Electrical Engineering, China Agricultural University, Beijing 100193, China. E-mail: yukanghuo.at@gmail.com Hao Tang is with the School of Computer Science, Peking University, Beijing 100871, China. E-mail: haotang@pku.edu.cn"
    },
    {
      "title": "Abstract",
      "text": "In recent years, significant progress has been made in the field of Artificial Intelligence with the development of Multimodal Large Language Models (MLLMs). However, adapting static, pre-trained MLLMs to dynamic data distributions and various tasks in an accurate and efficient manner remains a major challenge. When fine-tuning pre-trained MLLMs for specific tasks, a noticeable performance degradation often occurs in the modest's prior knowledge domain -- a phenomenon known as \"Catastrophic Forgetting\". While this issue has been extensively studied within the Continual Learning (CL) community, it presents new challenges in the context of MLLMs. As the first review paper in the field of continual learning for multimodal large models, this paper provides a comprehensive overview and detailed analysis of the 440 research papers on MLLM continual learning. Beyond introducing the fundamental concepts, the review is structured into four main sections. Firstly, it provides an overview of the latest research on MLLMs, including various model innovation strategies, benchmarks, and applications across diverse fields. Secondly, it presents a detailed categorization and overview of the latest research on continual learning, divided into three key areas: non-large language models(LLMs) unimodal contain learning (Non-LLM Unimodal CL), non-large language models multimodal continual learning (Non-LLM Multimodal CL), and continual learning in large language models (CL in LLM). In-depth and extensive research in both the MLLM and CL domains has laid a solid foundation for research on MLLM continual learning. In the fourth section, we conduct an in-depth analysis of the current research status of MLLM continual learning, examining common benchmark evaluations, innovative improvements in model architectures and methods, and systematically summarizing and reviewing existing theoretical and empirical studies. This review aims to connect the basic setup, theoretical foundations, method innovations, and practical applications of continual learning in multimodal large models, shedding light on the research progress and challenges in the field. Finally, this paper offers a forward-looking discussion on the challenges and future development trends of continual learning in multimodal large models, aiming to inspire researchers in the field and promote the advancement of related technologies. Multimodal Large Language Model, Continual Learning, Benchmark Evaluations, Model Innovation, Catastrophic Forgetting"
    },
    {
      "title": "1 Introduction",
      "text": "Research on Multimodal Large Language Models (MLLMs) has rapidly advanced in recent years, becoming a significant direction in the field of artificial intelligence [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. By integrating multimodal information such as language, vision, and audio, these models demonstrate powerful cross-modal understanding and generation capabilities, providing innovative solutions to complex real-world problems [11, 12, 13, 14, 15]. To enhance the performance of MLLMs, researchers have proposed various improvement strategies. Firstly, for cross-modal information fusion, more efficient architectural designs have been introduced [16, 17, 18], such as Transformer-based multimodal joint encoders and decoders, as well as lightweight cross-modal attention modules [19, 20, 21]. Secondly, pre-training techniques have been further developed, significantly improving the model's generalization ability and robustness through the introduction of multimodal contrastive learning, cross-modal consistency constraints, and self-supervised learning objectives [22, 23, 24, 25]. In addition, fine-tuning techniques have become increasingly refined [26], including efficient parameter adjustment methods (such as LoRA [27]) and task-specific adaptation layer designs. These approaches enable MLLMs to adapt to diverse task scenarios with lower computational costs [28, 29, 30, 31]. As shown in Figure 1, the performance evaluation of MLLMs is based on multimodal benchmarks that cover a wide range of task categories. For example, benchmarks in the vision and language domain include Visual Question Answering (VQA) [32, 33, 34, 35, 36], Image Captioning [37, 38, 39, 40, 41, 42], and Visual Grounding [43, 44, 45, 46]; in the audio and language domain, benchmarks include Audio-Text Alignment and Audio Generation [47, 48, 49]; there are also more complex cross-modal reasoning tasks, among others [50, 51]. Moreover, MLLMs are also showing great potential in real-world applications. They are playing an increasingly important role in fields such as healthcare, education, robotics, and autonomous driving [52, 53, 54]. Continual learning aims to address the challenge of how models can effectively learn new tasks while retaining prior knowledge when faced with dynamically changing data streams, thus mitigating the problem of catastrophic forgetting [55, 56, 57]. In recent years, research in the field of continuous learning has been deepened, particularly with significant developments in its application across models of various scales and multimodal learning scenarios [58]. [59, 60, 61, 62, 63]. In unimodal settings, the focus has mainly been on the design of algorithms to alleviate the problem of catastrophic forgetting, enabling models to maintain performance in previous tasks while incorporating new ones [65, 66, 67, 68, 69, 64]. Research in multimodal continual learning is more challenging than in unimodal settings, as models must simultaneously handle the characteristics of different modalities and their cross-modal interactions [70, 71, 72, 61]. Researchers have primarily focused on techniques for cross-modal feature extraction, alignment, and processing, aiming to reduce cross-modal interference, enhance inter-modal consistency, and improve the model's generalization ability [73, 74, 75, 76]. With the widespread application of large language models (LLMs) in natural language processing, research on their continual learning has become a new hotspot [77, 78, 79, 80, 81, 82]. Due to the massive parameter scale of LLMs and their reliance on vast amounts of pre-trained data, traditional continual learning strategies face challenges such as high computational costs and limited adaptability. To address these challenges, researchers have proposed several optimization directions: Parameter-Efficient Fine-Tuning (PEFT) methods (such as LoRA, Prefix Tuning, etc.) [27, 28, 29, 30, 31], prompt learning methods, and so on. These approaches have shown tremendous potential in tasks such as open-domain question answering, continual dialogue systems, and cross-domain text generation [83, 84, 85]. The rapid development of MLLMs and the in-depth integration of CL research have provided new perspectives for the exploration of the frontier in the field of artificial intelligence [9, 14, 17, 24, 52, 65, 69, 79, 86]. A key research challenge in this domain is how to efficiently retain knowledge from previous tasks while learning new ones while maintaining cross-modal collaboration capabilities [87, 88, 89]. This has become a central research question in the field. Building on existing research, this paper provides a systematic review and summary of the research on continual learning in multimodal large models. It delves into the innovations in model structure and methods, including the design of various model frameworks, dynamic parameter adjustment mechanisms, and modules that support task adaptation [90, 91, 92, 93]. These techniques not only significantly mitigate the problem of catastrophic forgetting, but also effectively enhance the task adaptability and generalization ability of MLLMs. In addition, this paper also introduces existing benchmarks for evaluating continual learning in multimodal large models, which provide important support for assessing model performance in continual learning tasks [94, 95, 96, 97]. Research on continual learning in multimodal large models not only provides new technological means for the dynamic adaptation of cross-modal tasks, but also offers innovative solutions for complex tasks in real-world domains such as intelligent education, healthcare, and robotic interaction [89, 98, 99, 100]. Finally, this paper offers a forward-looking discussion on the challenges and future development trends of continual learning in multimodal large models, covering aspects such as catastrophic forgetting, the improvement and standardization of evaluation benchmarks, and the enhancement of interpretability and transparency in multimodal large model continual learning. Through these discussions, the paper aims to provide valuable research insights for scholars in the field and promote the further development and application of continual learning technologies in multimodal large models."
    },
    {
      "title": "2 Multimodal Large Language Model",
      "text": ""
    },
    {
      "title": "_Preliminary_",
      "text": "In this section, we provide an overview of the latest research on MLLMs, including various model innovation strategies, a range of benchmarks, and the application of MLLMs in diverse domains. Fig. 1: Timeline of Multimodal Large Model Development."
    },
    {
      "title": "_Model Innovation_",
      "text": "With the continuous development of MLLMs, researchers have made various innovations in their structure, methods, and functional modules to enhance model performance, generalization ability, and adaptability. This section reviews the main innovations, which focus on three core directions: framework design, method optimization, and functional module improvements. These innovations collectively drive the performance of MLLMs in complex multimodal tasks. This section will explore the latest research advancements in these areas."
    },
    {
      "title": "2.2.1 Framework Innovation",
      "text": "Framework innovation is the foundation of MLLM development, aiming to achieve efficient fusion and processing of cross-modal information by improving the overall architectural design. In recent years, researchers have proposed many efficient framework designs. As shown in Table I, researchers have proposed several efficient framework designs, such as MaVEn, MoVA, AutoM3L, DI-MML and et. These framework innovations provide more efficient tools and methods for MLLMs to handle multimodal tasks involving language, vision, and hearing. They enable MLLMs to achieve more precise reasoning and decision-making in the interaction of \\begin{table} \\begin{tabular}{p{113.8pt}|p{113.8pt}|p{113.8pt}} \\hline MLLMs & \\multicolumn{2}{c|}{Starting point of the problem} & \\multicolumn{1}{p{113.8pt}}{How to solve} \\\\ \\hline **MaVEn**[101] & Enhancing the image visual understanding of MLLMs. & MaVEn proposes an effective multi-granularity hybrid visual encoding framework. \\\\ \\hline **MoVA**[102] & No single visual encoder can dominate the understanding of various image contents. & MoVA incorporates coarse-grained context-aware expert routing and fine-grained expert fusion. \\\\ \\hline **MoME**[103] & The performance of general-purpose MLLMs is typically inferior to that of expert MLLMs. & MoME combines the MoVE and the MoLE to reduce task interference. \\\\ \\hline **Meteor**[104] & The performance gap of MLLMs in understanding and answering complex questions. & Meteor introduced the new concept of ”traversal of rationales.” \\\\ \\hline **CORY**[105] & The stability and performance issues MLLMs encounter in fl fine-tuning. & CORY leverages the inherent cooperative evolution and emergence capabilities of multi-agent systems. \\\\ \\hline **Lumen**[106] & MLMs overlook the intrinsic characteristics of different visual tasks. & Lumen enhances multimodal understanding by separating task-agnostic and task-specific learning. \\\\ \\hline **Octopus**[107] & MLLMs combine visual recognition and understanding sequentially at the LLM, which is suboptimal. & Octopus proposed the ”Parallel Recognition \\(\\rightarrow\\) Sequential Understanding” MLLM framework. \\\\ \\hline **Wings**[108] & MLLMs tend to forget knowledge acquired from text-only instructions during training. & Wings introduces additional modules and mechanisms to compensate for attention shifts. \\\\ \\hline **Cantor**[109] & The ”hallucination” problem in decision-making is caused by insufficient visual information. & Cantor inspires a multimodal chain-of-thought of MLLM. \\\\ \\hline **AutoM3L**[110] & The limitations of automation in multimodal machine learning. & AutoM3L proposes an automated multimodal machine learning framework with MLLMs. \\\\ \\hline **DI-MML**[111] & The modality competition issue in multimodal learning. & DI-MML proposes detached and interactive multimodal learning. \\\\ \\hline **MEM**[112] & Data scraped from networks may leak personal privacy. & MEM optimizes by combining image noise and text triggers to mislead the model into learning shortcuts. \\\\ \\hline **CREAM**[113] & The lack of cross-page interaction support in document visual question answering. & CREAM proposes Coarse-to-Fine retrieval and multimodal efficient tuning for document VQA. \\\\ \\hline **SLUDA**[114] & Insufficient labeled data and the underutilization of unlabeled data. & SLUDA generates fine-grained data, optimizes unlabeled data usage, and employs adaptive selection and dynamic threshold strategies. \\\\ \\hline **SAM**[115] & The semantic alignment issue in MLLMs when processing multi-image instructions. & SAM enhances image-semantic associations through a bidirectional semantic guidance mechanism. \\\\ \\hline **CTVLMs**[116] & Improving performance and reducing computational resource demands in MLLMs for multimodal tasks. & CTVLMs use knowledge distillation and multimodal alignment to transfer knowledge from large models to smaller ones. \\\\ \\hline **Bloom**[117] & Reducing the high computational cost of large-scale multilingual visual data modeling. & Bloom proposes pre-training with discretized visual speech representation. \\\\ \\hline **MA-AGIQA**[118] & The quality evaluation issue of AI-generated images (AGIs). & MA-AGIQA combines multimodal models and traditional DNNs, utilizing semantic information extraction and the mixture of experts (MoE) structure to dynamically integrate quality-aware features. \\\\ \\hline **WorldGPT**[119] & Enhancing the applicability and generalization ability of MLLMs. & WorldGPT includes memory offloading, knowledge retrieval, and a Context Reflector. \\\\ \\hline **Q-ALIGN**[120] & Enhancing the applicability and generalization ability of MLLMs. & Q-ALIGN unifies IQA, IAA, and VQA tasks to enhance the model’s cross-task generalization ability. \\\\ \\hline **Flextron**[121] & The deployment challenges of MLLMs in resource-constrained environments. & Flextron selects different sub-models or sub-networks by using routers. \\\\ \\hline **NEXT-GPT**[122] & Existing MLLMs can only understand the input modality. & NEXT-GPT proposes lightweight alignment techniques and modality-switching instruction tuning. \\\\ \\hline \\end{tabular} \\end{table} TABLE I: Innovations in MLL Frameworks. multimodal data, thereby offering strong support for solving complex problems in practical applications. More details of the innovation of MLLMs frameworks are provided in Section 7.1 of the Appendix."
    },
    {
      "title": "2.2.2 Method Innovation",
      "text": "Method innovation is the core driving force behind the performance improvement of MLLMs. By designing more efficient training methods and optimization objectives, it helps models better adapt to dynamic task environments. As shown in Table II, in recent years, researchers have proposed numerous novel and efficient methods to enhance the accuracy and robustness of MLLMs. These method research has explored cutting-edge techniques such as multimodal contrastive learning, self-supervised learning objectives, and multimodal alignment mechanisms. These methods not only enhance the model's generalization ability but also significantly improve the accuracy and robustness of cross-modal tasks. More details of the innovation of MLLMs methods are provided in Section 7.1 of the Appendix."
    },
    {
      "title": "_Benchmarks_",
      "text": "As MLLMs continue to achieve breakthroughs in multimodal tasks such as vision, language, and speech, comprehensive benchmarks have become crucial for systematically evaluating and comparing model performance. These benchmarks not only provide standardized datasets and tasks, but also \\begin{table} \\begin{tabular}{p{113.8pt}|p{113.8pt}|p{113.8pt}} \\hline Method & Starting point of the problem & How to solve \\\\ \\hline **DenseFusion**[123] & Enhancing the visual perception ability of MLLMs. & DenseFusion proposes a multimodal perception fusion method that integrates visual experts. \\\\ \\hline **E2E-MFD[124]** & The complex training process hinders the broader application of MLLMs. & E2E-MFD proposes a novel end-to-end algorithm for multimodal fusion detection. \\\\ \\hline **NAM[125]** & Neuron attribution in MLLMs has not been fully explored yet. & NAM proposes a neuron attribution method tailored for MLLMs. \\\\ \\hline **CODE[126]** & Addressing the hallucination problem in MLLMs when generating visual content. & CODE utilizes self-generated descriptions as contrastive references to adjust the information flow. \\\\ \\hline **MULTEDIT[127]** & To correct errors and insert new information. & MULTEDIT introduces a multimodal causal tracking method. \\\\ \\hline **QSAAW[128]** & Tackling the resource consumption issue faced by MLLMs in visual-language instruction tuning. & QSAAW learns group scale factors of quantized weights and adopts multimodal pretraining method. \\\\ \\hline **LECR[129]** & To improve the quality of cross-modal alignment. & LECR proposes the MLLM-enhanced cross-lingual, cross-modal retrieval method. \\\\ \\hline **ERL-MR[130]** & To address the modality imbalance problem in MLLMs. & ERL-MR uses Euler transformations and multimodal constraint loss. \\\\ \\hline **AMMPL[131]** & Enhancing the model’s performance and reasoning ability. & AMMPL proposes an adaptive multimodal prompt learning method. \\\\ \\hline **PaRe[132]** & Enhancing the model’s performance and reasoning ability. & PaRe progressively generates intermediate modalities and replaces modality-agnostic fragments. \\\\ \\hline **MCL[133]** & Addressing the insufficient interaction problem when handling complex multimodal scenarios. & MCL proposes the multimodal combination learning (MCL) method. \\\\ \\hline **FARE[134]** & MLLMs are vulnerable to adversarial attacks in the visual modality. & FARE proposes the unsupervised adversarial fine-tuning scheme. \\\\ \\hline **DICL[135]** & Reducing the reliance on manual annotations. & DICL leverages MLLMs knowledge to enhance the robustness of visual models. \\\\ \\hline **API[136]** & Addressing the limitations of traditional visual prompting techniques. & API enhances model perception through attention heatmaps guided by text queries. \\\\ \\hline **IVTP[137]** & Addressing the high computational cost problem in MLLMs. & IVTP proposeS the instruction-guided visual token pruning method. \\\\ \\hline **ChatTracker[138]** & Enhancing the tracking performance of MLLM trackers. & ChatTracker proposes a novel reflection-based prompt optimization module. \\\\ \\hline **Optimus-1[139]** & Current general agents lack the necessary world knowledge and multimodal experience. & Optimus-1 proposes a hybrid multimodal memory module. \\\\ \\hline **CuMo[140]** & Improving the performance of MLLMs on multimodal tasks. & CuMo integrates sparse gated Top-K MoE blocks in the visual encoder and MLP connectors. \\\\ \\hline **AcFormer[141]** & The connection between visual encoders and LLMs has limitations. & AcFormer identified visual anchors and proposed a novel vision-language connector \\\\ \\hline **Chain-of-Sight[142]** & Accelerating the pretraining process and improving model performance. & Chain-of-Sight captures visual details at different spatial scales through a multi-scale visual resampler. \\\\ \\hline **Dense Connector[143]** & Existing MLLMs underutilise the visual encoder while overly emphasising the language modality. & Dense Connector enhances the visual perception ability by integrating multi-layer visual features. \\\\ \\hline **GCG[144]** & In video question answering, MLLMs overlook visually relevant cues related to the question. & GCG learns to represent the temporal structure of videos and selects key frames. \\\\ \\hline **Q-MoE[145]** & Connection structure struggles with filtering visual information according to task requirements. & Q-MoE proposes a query-based hybrid expert connector. \\\\ \\hline \\end{tabular} \\end{table} TABLE II: Innovations in MLL Methods. define metrics for assessing models' abilities in cross-modal reasoning, generation, classification, and other areas. They play a key role in guiding research directions, identifying model limitations, and advancing technological progress. More details of the overview of MLLM benchmarks are provided in Section 7.2 of the Appendix. Section 7.2 in the Appendix introduces some of the recent representative benchmarks, covering a wide range of scenarios from academic research to practical applications, reflecting the diverse needs and challenges in the multimodal field."
    },
    {
      "title": "_Applications Of Mllms_",
      "text": "Multimodal large models (MLLMs) have emerged as a significant direction in artificial intelligence research in recent years [184, 185, 186, 187, 1, 2, 3, 10]. With the rapid development of technologies such as natural language processing, computer vision, and speech recognition, single-modal intelligent systems can no longer meet the increasingly complex requirements of real-world applications [187, 188, 189, 190]. Multimodal learning, by integrating different types of data inputs, simulates the diversity and complexity of human information processing, offering more comprehensive and flexible intelligent services. At the same time, with the deepening of interdisciplinary research, MLLMs will not only play a role in traditional AI tasks but will also expand into more edge domains, driving artificial intelligence from closed systems to a more open and intelligent ecosystem. More details of the applications of MLLMs are provided in Section 7.3 of the Appendix. In summary, the application prospects of multimodal large models are vast. However, to fully unleash their potential, this requires the combined advancement of technological innovation and theoretical breakthroughs. In the future, with ongoing progress in algorithms, hardware, and cross-domain collaboration, it is expected that MLLMs will achieve more efficient and intelligent performance in a wider range of \\begin{table} \\begin{tabular}{c|p{142.3pt}|p{142.3pt}} \\hline Framework & Starting point of the problem & How to solve \\\\ \\hline **NTE**[146] & Addressing the catastrophic forgetting problem in graph neural networks. & NTE views a neural network as an ensemble of fixed experts. \\\\ \\hline **IsCiL**[147] & To address the issue of new data lacking labels due to annotation delays in continual learning. & IsCiL improves sample efficiency and task adaptability by incrementally learning shared skills. \\\\ \\hline **CKP**[148] & To address the performance degradation caused by incorrect labels in the Lifelong Person Relation task. & CKP purifies data through the CDP and ILR modules, and filters out erroneous knowledge using the EKF algorithm. \\\\ \\hline **PBR**[149] & To reduce forgetting and enhances long-tail continual learning performance. & PBR proposes an uncertainty-guided sampling strategy and two prior-free constraints. \\\\ \\hline **OSN**[150] & Reducing the interference of new tasks on old tasks. & OSN explores shared knowledge between old and new tasks through parameter sharing. \\\\ \\hline **MoDE**[67] & Improving adaptation to new domains while preserving old knowledge. & MoDE includes domain-adaptive routing and domain-expert collaborative loss. \\\\ \\hline **SB-MCL**[151] & To address the catastrophic forgetting problem in continual learning. & SB-MCL achieves continual learning through sequential Bayesian updates. \\\\ \\hline **PNR**[152] & Addressing the knowledge transfer and catastrophic forgetting issues. & PNR Generates pseudo-negative samples and optimizing knowledge transfer. \\\\ \\hline **CompoNet**[153] & Addressing the issue of old task forgetting caused in continual reinforcement learning. & CompoNet proposes a modular neural network with linearly growing parameters. \\\\ \\hline **Vector-HaSH**[154] & To enable fast learning and continual memory. & Vector-HaSH combines hetero-associative memory and spatially invariant CNNs. \\\\ \\hline **DDDR**[155] & Addressing the issue of catastrophic forgetting in federated continual learning. & DDDR uses diffusion models to generate historical data and employs contrastive learning. \\\\ \\hline **PromptCCD**[156] & Mitigating catastrophic forgetting. & PromptCCD introduces the GMP, which dynamically generates prompts to adapt to new classes. \\\\ \\hline **Mecoin**[157] & To reduce parameter fine-tuning, lower the forgetting rate. & Mecoin employs SMU and a McCo for efficient storage and updating of class prototypes. \\\\ \\hline **RP2F**[158] & Enabling effective knowledge sharing and backward knowledge transfer. & RP2F uses perturbation methods to approximate the Hessian matrix and introduces a prior. \\\\ \\hline **HAMMER**[159] & To address the catastrophic forgetting issue in multilingual text recognition. & HAMMER proposes online knowledge analysis and a hierarchical language evaluation mechanism. \\\\ \\hline **FedCBC**[160] & Mitigating catastrophic forgetting. & FedCBC proposes category-specific binary classifiers and selective knowledge fusion. \\\\ \\hline **TS-ILM**[161] & Reducing information redundancy and enhancing memory retention. & TS-ILM proposes a task-level temporal pattern extractor and a time-sensitive example selector. \\\\ \\hline **AutoActivator**[162] & To address the issue of model forgetting old classes when continuously learning new classes. & AutoActivator dynamically adapts neural units to new tasks, enabling on-demand network expansion. \\\\ \\hline **iNeMo**[163] & To achieve efficient class-incremental learning. & iNeMo proposes latent space initialization and position regularization. \\\\ \\hline **TACO**[164] & Offering a novel perspective for understanding and mitigating catastrophic forgetting. & TACO combines graph coarsening and continual learning to dynamically store information from previous tasks. \\\\ \\hline \\end{tabular} \\end{table} TABLE III: Innovations in Non-LLM Unimodal CL Frameworks. practical applications, further advancing the development of artificial intelligence."
    },
    {
      "title": "3 Continue Learning",
      "text": ""
    },
    {
      "title": "_Preliminary_",
      "text": "Continual Learning (CL) has become a central focus in AI research due to the rapid growth of deep learning and LLMs [202, 203, 204, 205, 206, 207, 208, 209, 210]. The challenge is to enable models to retain and enhance learning capabilities when faced with continuously changing data and tasks. Traditional methods assume that models can learn all tasks at once and maintain a fixed knowledge base, but in reality, data and tasks evolve, often leading to \"Catastrophic Forgetting\" [211, 212, 213, 214, 215, 216, 217, 218]. Therefore, CL, as a learning paradigm that better aligns with real-world application needs, aims to enable models to effectively accumulate and update knowledge across multiple stages, thereby better adapting to dynamic and evolving environments. This section will provide a detailed classification and overview of the latest innovative research in continual learning. The specific content is divided into three parts: 1) Exploring non-LLMs unimodal continual learning and focusing on traditional models' continual learning research in unimodal data; 2) Analyzing non-LLMs multimodal continual learning and discussing the challenges and research in continual learning across multi-modal data; 3) Analyzing and summarizing the latest advancements in continual learning for LLMs and examining the unique challenges and solutions they face when handling large-scale textual data."
    },
    {
      "title": "_Non-Llm Unimodal Cl_",
      "text": "In traditional unimodal learning, research on continual learning primarily focuses on how to prevent models from \\begin{table} \\begin{tabular}{c|p{142.3pt}|p{142.3pt}} \\hline Method & Starting point of the problem & How to solve \\\\ \\hline **GACL**[165] & Addressing the catastrophic forgetting problem of models in class-incremental learning. & GACL establishes the equivalence between incremental learning and joint training. \\\\ \\hline **C-Flat**[165] & Addressing the balance between new task training sensitivity and memory retention. & C-Flat optimizes the flatness of the loss landscape. \\\\ \\hline **DSGD**[166] & Addressing the practical deployment challenge. & DSGD uses structural and semantic information for stable knowledge distillation. \\\\ \\hline **VQ-Prompt**[167] & To improve continual learning performance. & VQ-Prompt utilizes vector quantization to achieve end-to-end optimization of discrete prompt selection. \\\\ \\hline **RanDumb**[168] & Exploring whether the representations generated by continual learning algorithms are truly effective. & RanDumb uses random transformations and linear classifiers to address. \\\\ \\hline **IWMS**[169] & The label delay issue in online continual learning. & IWMS prioritizes the memory of samples similar to new data. \\\\ \\hline **PPE**[170] & To address the catastrophic forgetting problem in non-sample online continual learning. & PPE learns class prototypes during the online learning phase. \\\\ \\hline **GPCNS**[171] & Improving the performance of continual learning. & GPCNS enhances plasticity by utilizing gradient information from old tasks. \\\\ \\hline **CILA**[172] & Improving the performance of continual learning. & CILA proposes an adaptive distillation coefficient and theoretical performance guarantees. \\\\ \\hline **POCL**[173] & Existing methods fail to fully leverage the inter-task dependencies. & POCL models task relationships through Pareto optimization and dynamically adjusts weights. \\\\ \\hline **Powder**[174] & Addressing the cross-task and cross-client knowledge transfer in federated continual learning. & Powder enables prompt-based dual knowledge transfer. \\\\ \\hline **AdaPromptCL**[175] & Addressing the challenge of task-specific semantic variations. & AdaPromptCL proposes dynamic semantic grouping and prompt adjustment. \\\\ \\hline **LPR**[175] & To reduce catastrophic forgetting and underfitting. & LPR adjusts the optimization geometry to balance the learning of new and old data. \\\\ \\hline **InfLoRA**[176] & To address the issue of forgetting old tasks when adapting to new tasks. & InfLoRA microsets parameter reparameterization into pre-trained weights. \\\\ \\hline **F-OAL**[177] & To alleviate the issue of catastrophic forgetting in online class-incremental learning. & F-OAL proposes a forward online analytical learning method. \\\\ \\hline **PRL**[178] & Improving performance in non-sample class-incremental learning. & PRL aligns reserved space and latent space to adapt new class features to the reserved space. \\\\ \\hline **CIL**[179] & To address the issue of catastrophic forgetting. & CIL proposes the CIL-balanced classification loss and distribution margin loss. \\\\ \\hline **DSSP**[180] & To eliminate the need for sample replay. & DSSP leverages domain sharing and task-specific prompt learning. \\\\ \\hline **MRFA**[181] & To reduce catastrophic forgetting. & MRFA optimizes the entire layer margin by enhancing the features of review samples. \\\\ \\hline **DARE**[182] & Improving the model’s performance on old tasks. & DARE reduces representation drift through a three-stage training process. \\\\ \\hline **EASE**[183] & To reduce catastrophic forgetting. & EASE constructs task-specific subspaces using lightweight adapters. \\\\ \\hline \\end{tabular} \\end{table} TABLE IV: Innovations in Non-LLM Unimodal CL Methods. forgetting previously learned knowledge when learning new tasks. Many researchers have proposed solutions to this problem, including strategies based on knowledge retention, incremental learning methods, and improvements to neural network architectures [205, 219, 220, 221, 222, 223, 224, 225]. For non-large models, the challenges of continual learning are particularly pronounced due to limitations in computational resources. Furthermore, the unimodal continual learning for non-large models primarily focuses on individual modalities such as vision, speech, and text. As show in Tables III and IV, to address the specific characteristics of these tasks, researchers have proposed a variety of innovative frameworks and methods. Overall, unimodal continual learning with non-large models has made significant progress in scenarios with limited computational resources. Many innovative frameworks and methods have been developed to effectively mitigate catastrophic forgetting. However, how to scale these approaches to multimodal and large-scale data remains an important direction for future research. More details of the non-LLM unimodal continual learning are provided in Section 8.1 of the Appendix."
    },
    {
      "title": "_Non-Llm Multimodal Cl_",
      "text": "Compared to unimodal continual learning, multimodal continual learning presents more complex challenges. Data from different modalities often exhibit heterogeneity, and the key difficulty in multimodal continual learning for non-large models lies in how to effectively fuse information across modalities while retaining previously acquired knowledge during the process of learning new modalities. In recent years, researchers have proposed various methods to address these challenges, including inter-modal collaborative learning, shared and independent representations for each modality, and others [226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237]. As shown in Table V, these innovative methods enable non-large models to perform continual learning in multimodal environments, while minimizing knowledge conflicts between different modalities. More details of the non-LLM multimodal continual learning are provided in Section 8.2 of the Appendix."
    },
    {
      "title": "_Cl In Llm_",
      "text": "LLMs such as GPT and BERT, with their powerful language understanding and generation capabilities, have achieved remarkable results on various natural language processing tasks [252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263]. However, LLMs still face unique challenges in continual learning. Particularly in the context of increasing data volume and task diversity, how to effectively update models, avoid catastrophic forgetting, and maintain efficient computational capabilities are key focuses in the research of LLMs for continual learning. As shown in Table VI, researchers have proposed a variety of instruction fine-tuning methods. Through model improvements and methods such as instruction fine-tuning, LLMs are able to expand their knowledge while effectively addressing the issue of catastrophic forgetting. However, as model sizes continue to grow, core challenges in the field of continual learning for LLMs remain, such as how to handle updates and learning with large-scale data, and how to maintain good adaptability in multi-task and cross-modal environments. These remain critical issues that need to be addressed. More details of the LLM continual learning are provided in Section 8.3 of the Appendix. Continual learning is a multidimensional and complex research field, characterized by both challenges and opportunities. From unimodal to multimodal, and then to continual learning in LLMs, each category of methods and strategies presents its own unique challenges and innovations. Future research will not only need to deepen the understanding of existing methods, but also explore how to achieve more \\begin{table} \\begin{tabular}{c|p{142.3pt}|p{142.3pt}} \\hline Method & Starting point of the problem & How to solve \\\\ \\hline **CPP**[191] & Improving the performance of continual learning. & CPP incorporates the CCE, TKD, and TPL mechanisms to achieve multimodal vision perception. \\\\ \\hline **CP-Prompt**[192] & To reduce catastrophic forgetting. & CP-Prompt utilizes a dual-prompt strategy and parameter-efficient adjustments. \\\\ \\hline **MMAL**[193] & Reducing forgetting and enhancing incremental learning performance. & MMAL proposes the modality fusion module and MSKC module. \\\\ \\hline **MSPT**[194] & To reduce catastrophic forgetting. & MSP optimizes multimodal learning through gradient modulation and attention distillation. \\\\ \\hline **MedCoSS**[195] & To reduce catastrophic forgetting. & MSP propose a staged multimodal self-supervised learning framework that avoids modality conflicts. \\\\ \\hline **ZiRa**[196] & Retaining zero-shot generalization ability. & ZiRa proposes zero-interference loss and a reparameterized dual-branch structure. \\\\ \\hline **STELLA**[197] & To reduce forgetting of previously learned knowledge. & STELLA proposes a localized patch importance scoring method. \\\\ \\hline **RCS-Prompt**[198] & To address the issue of overlap between old and new category spaces. & RCS-Prompt proposes bidirectional prompt optimization and prompt magnitude normalization. \\\\ \\hline **ZSCL**[199] & To reduce catastrophic forgetting. & ZSCL proposes feature space distillation and parameter space weight integration. \\\\ \\hline **CoCoOp**[200] & To address the issue of pretrained models lacking generalization ability to unseen classes when adapting to new tasks. & CoCoOp generates dynamic prompts using a lightweight neural network. \\\\ \\hline **RAIL**[201] & Improving cross-domain classification capabilities during continual learning. & RAIL uses recursive ridge regression and a no-training fusion module. \\\\ \\hline \\end{tabular} \\end{table} TABLE V: Innovations in Non-LLM Multimodal CL Methods. efficient and robust continual learning in environments with large-scale, multimodal data and tasks. As computational power and data scale continue to expand, research in continual learning will provide a more solid theoretical and technological foundation for the adaptability, robustness, and sustainability of intelligent systems."
    },
    {
      "title": "4 Continual Learning In Mllms",
      "text": ""
    },
    {
      "title": "_Preliminary_",
      "text": "Recent advancements in MLLMs have shown remarkable capabilities across various domains. However, as their scale grows, maintaining long-term effectiveness in dynamic environments is a critical challenge [27, 27, 28, 29, 28, 27, 26, 29, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]. CL addresses this by enabling models to learn new tasks without forgetting previously acquired knowledge in evolving data and task contexts. For MLLMs, continual learning is more complex due to the vast data and complex computations involved, requiring significant computational resources and storage. Although existing research provides valuable theoretical and experimental insights [285, 286, 287, 288, 289, 290, 291, 292, 293], applying MLLMs to continual learning still faces many challenges. This section explores innovations in multimodal large model continual learning and the related evaluation benchmarks."
    },
    {
      "title": "_Model Innovation_",
      "text": "As shown in Tables VII and VIII, to achieve multi-task CL in multimodal large models and avoid catastrophic forgetting, researchers have proposed numerous innovative frameworks and methods [235, 270, 272, 273, 294, 295, 100, 235]. These innovations not only facilitate knowledge sharing and transfer between multiple tasks but also effectively address challenges such as catastrophic forgetting, modality conflicts, and computational resource constraints. These efforts collectively advance the continual learning capabilities of multimodal large models in dynamic environments. More details of the model innovation in the continual learning of MLLMs are provided in Section 9 of the Appendix."
    },
    {
      "title": "_Benchmarks_",
      "text": "As the application of multimodal large models in continual learning increases, evaluating their CL capability has become a key issue. To comprehensively assess the continual learning performance of multimodal large models, benchmarks and evaluation frameworks have emerged. However, benchmarks specifically designed for continual learning in multimodal large models are still relatively scarce, and the relevant evaluation standards are still in the process of development. Section 9.1 in the Appendix analyzes and lists the few existing benchmarks to evaluate the continual learning capability of multimodal large models, exploring their design concepts, evaluation metrics, and applicability in different application scenarios. \\begin{table} \\begin{tabular}{p{113.8pt}|p{113.8pt}|p{113.8pt}} \\hline Method & Starting point of the problem & How to solve \\\\ \\hline **ConTin Tin**[238] & To reduce catastrophic forgetting. & InstructionSpeak learns from negative outputs and revisites the instructions of previous tasks. \\\\ \\hline **OLORA**[239] & Improving the performance of continual learning. & OLoRA introduces orthogonal low-rank adaptation for CIT. \\\\ \\hline **DAPT**[240] & To reduce catastrophic forgetting. & DAPT proposes a dual-attention learning and selection module. \\\\ \\hline **ELM**[241] & To reduce catastrophic forgetting. & ELM trains a small expert adapter for each task on top of the LLM. \\\\ \\hline **LLAMA PRO**[242] & Retaining the initial functionality through post-training. & LLaMA PRO introduces an innovative block expansion technique. \\\\ \\hline **AdaptILM**[243] & To help the model leverage domain-specific knowledge while enhancing prompt performance. & AdaptILM adapts the LLM to different domains by enriching the original training corpus with a series of content-related reading comprehension tasks. \\\\ \\hline **DynInst**[244] & To enhance the generalization of the LLM. & DynaInst combines dynamic instruction replay with a local minima-inducing regularizer. \\\\ \\hline **TAALM**[245] & Enabling targeted knowledge updates and reducing forgetting. & TAALM uses meta-learning to dynamically predict token importance. \\\\ \\hline **D-CPT Law**[246] & To reduce GPU resource consumption and improve domain adaptability. & D-CPT Law predicts the optimal training ratio. \\\\ \\hline **COPAL**[247] & High computational demands and model adaptability limitations. & COPAL enables continual pruning without the need for retraining. \\\\ \\hline **MagMax**[248] & To reduce catastrophic forgetting. & MagMax proposes sequential fine-tuning and maximum magnitude weight selection. \\\\ \\hline **SAPT**[249] & Enabling effective knowledge retention and transfer. & SAPT aligns the learning and selection of PET blocks through a shared attention mechanism. \\\\ \\hline **SSR**[250] & To reduce catastrophic forgetting. & SSR utilizes LLM-generated synthetic instances for rehearsal. \\\\ \\hline **LoRAMoE**[251] & Enhancing multi-task handling capabilities. & LoRAMoE integrates LoRA and router networks, and introduces local balance constraints. \\\\ \\hline **F-Learning paradigm**[251] & Improving the performance of continual learning. & F-Learning paradigm first forgets old knowledge before learning new knowledge. \\\\ \\hline \\end{tabular} \\end{table} TABLE VI: Innovations in LLM Instruction Fine-tuning Methods. Existing benchmarks for multimodal large model continual learning provide some reference value for assessing a model's learning ability. However, due to the scarcity of such benchmarks, with only a few available for use, many issues and limitations remain to be addressed. In the future, there is a need to design more comprehensive, flexible, and scalable evaluation benchmarks to meet the evolving demands of multimodal large model continual learning technologies."
    },
    {
      "title": "5 Challenges And Future Trends In Multimodal Large Model Continual Learning",
      "text": ""
    },
    {
      "title": "_Catastrophic Forgetting_",
      "text": ""
    },
    {
      "title": "5.1.1 Challenges Encountered",
      "text": "Catastrophic forgetting has long been a classic problem in continual learning tasks, and its presence significantly limits the adaptability and generalization ability of models in real-world dynamic environments. For multimodal large models, this issue becomes even more complex due to the need for training on large-scale data, as well as the immense computational resources and storage space required."
    },
    {
      "title": "5.1.2 Future Trends",
      "text": "Balancing forgetting management with learning efficiency, especially as tasks increase, is a complex optimization challenge. The goal is to prevent catastrophic forgetting while maintaining learning efficiency. Future research should focus on strategies to mitigate forgetting, such as frameworks or algorithms that preserve old knowledge while learning new information, or mechanisms for periodic knowledge consolidation. In addition, techniques such as self-supervised learning and transfer learning can be utilized. By sharing latent features or representations across different modalities, these methods can reduce interference between tasks, thereby alleviating the impact of catastrophic forgetting."
    },
    {
      "title": "_Improvement And Standardization Of Evaluation Benchmarks_",
      "text": ""
    },
    {
      "title": "5.2.1 Challenges Encountered",
      "text": "Evaluation benchmarks should not only consider a model's performance in learning new tasks but also assess its ability to retain knowledge across different modalities, the effectiveness of cross-task transfer, and its stability over long-term learning. Currently, benchmarks for evaluating continual learning in multimodal large models are still relatively scarce. As multimodal large models become increasingly complex in real-world applications, developing comprehensive and systematic evaluation benchmarks for their continual learning capabilities is an urgent problem that needs to be addressed."
    },
    {
      "title": "5.2.2 Future Trends",
      "text": "Future research should focus on designing more comprehensive and flexible evaluation benchmarks that support the assessment of continual learning in multimodal large models within multi-task environments. Researchers need to develop evaluation metrics capable of measuring a model's performance in multi-task learning, knowledge transfer, catastrophic forgetting, and cross-modal consistency. Furthermore, the standardization of evaluation benchmarks will \\begin{table} \\begin{tabular}{p{113.8pt}|p{113.8pt}|p{113.8pt}} \\hline Framework & Starting point of the problem & How to solve \\\\ \\hline **PathWeave**[264] & To reduce the dependency on large-scale joint pre-training. & PathWeave enhances modality alignment and collaboration. \\\\ \\hline **CLAP**[91] & To enhance the model’s uncertainty estimation capabilities. & CLAP is compatible with various prompt methods. \\\\ \\hline **DIKI**[265] & To reduce catastrophic forgetting. & DIKI proposes a residual mechanism and distribution-aware calibration. \\\\ \\hline **GMM**[266] & To reduce catastrophic forgetting. & GMM implements incremental learning through generated label text and feature matching. \\\\ \\hline **PriViElege**[267] & To address catastrophic forgetting and overfitting in MLMs. & PriViElege proposes prompt functionality and knowledge distillation. \\\\ \\hline **ModalPrompt**[268] & To address catastrophic forgetting and overfitting in MLMs. & ModalPrompt proposes bi-modal guided prototype prompts and knowledge transfer. \\\\ \\hline **CGIL**[269] & To reduce catastrophic forgetting. & CGIL uses VAEs to learn class-conditioned distributions and generate synthetic samples. \\\\ \\hline **CoLeCLIP**[270] & To reduce interference between tasks. & CoLeCLIP proposes joint learning of task prompts and cross-domain vocabularies. \\\\ \\hline **ICL**[100] & To enhance the efficiency of continual learning in MLMs. & ICL enables interaction between a fast intuition model and a slow deep thinking model. \\\\ \\hline **EMT**[271] & To evaluate catastrophic forgetting in MLLMs. & EMT offers a new perspective for improving fine-tuning strategies in MLMs. \\\\ \\hline **Freeze-Omni**[99] & To reduce catastrophic forgetting. & Freeze-Omni implements a three-stage training strategy. \\\\ \\hline **Adapt-\\(\\infty\\)[272] & To reduce catastrophic forgetting. & Adapt-\\(\\infty\\) proposes dynamic data selection and a clustering-based permanent pruning strategy. \\\\ \\hline **Mono-InternVL**[273] & To address the performance degradation and catastrophic forgetting issues that arise when expanding the visual and language capabilities of MLLMs. & Mono-InternVL integrates visual experts using a MOE structure and introduces endogenous visual pretraining. \\\\ \\hline **MoExtend**[274] & To address the issues of catastrophic forgetting and high training costs. & MoExtend designes a three-stage training process, including alignment, extension, and fine-tuning. \\\\ \\hline \\end{tabular} \\end{table} TABLE VII: Innovations in MLModel CL Frameworks. be a key direction for future development. By establishing unified evaluation frameworks, it will be possible to more effectively compare the strengths and weaknesses of different models, thereby advancing research in this field."
    },
    {
      "title": "_Improving The Interpretability And Transparency Of Continual Learning In Multimodal Large Models_",
      "text": ""
    },
    {
      "title": "5.3.1 Challenges Encountered",
      "text": "In multimodal learning tasks, models need to integrate information from different modalities (such as images, text, audio, etc.), which makes their decision-making process more complex and harder to trace. In particular in continual learning environments, the model must continuously learn new tasks while retaining knowledge from previous tasks. The integration and transfer of information across different modalities during this learning process make the model's decision mechanism even more challenging to interpret. Enhancing the interpretability of multimodal large models in continual learning not only helps increase the model's trustworthiness but also provides effective debugging and error diagnosis mechanisms during the learning process."
    },
    {
      "title": "5.3.2 Future Trends",
      "text": "In future research on continual learning for multimodal large models, to enhance model interpretability, researchers can design more transparent and traceable architectures that allow for clear tracking and analysis of the model's decision-making rationale when handling different tasks. At the model design level, researchers can integrate the latest advances in explainable AI (XAI) to incorporate highly interpretable model structures, thus improving transparency in the decision-making process. Furthermore, by combining techniques such as cross-modal learning and transfer learning, researchers can effectively facilitate the transfer and retention of cross-task knowledge during continual learning, while also enhancing the understanding and explainability of the knowledge transfer mechanisms."
    },
    {
      "title": "6 Conclusion",
      "text": "In this review, we systematically discuss the latest advancements and challenges in the continual learning of multimodal large models (MLM). First, we review the innovative strategies of multimodal large models and their applications \\begin{table} \\begin{tabular}{p{142.3pt}|p{142.3pt}|p{142.3pt}} \\hline Method & Starting point of the problem & How to solve \\\\ \\hline \\multirow{2}{*}{**NoRGa**[264] & To enhance the continual learning performance of multimodal large language models. & NoRGa proposes the non-linear residual gate. \\\\ \\hline \\multirow{2}{*}{**ZAF**[296]} & To reduce catastrophic forgetting. & ZAF preserves knowledge through zero-shot stability regularization. \\\\ \\hline \\multirow{2}{*}{**DualLoRA**[92]} & Improving the efficiency and effectiveness of continual learning in multimodal large language models. & DualLoRA utilizes orthogonal and residual low-rank adapters along with a dynamic memory mechanism to balance model stability and plasticity. \\\\ \\hline \\multirow{2}{*}{**LPI**[297]} & To address the insufficient interaction between modalities and tasks. & LPI enhances inter-modal and inter-task interactions through low-rank decomposition and contrastive learning. \\\\ \\hline \\multirow{2}{*}{**Model Tailor**[298]} & To reduce catastrophic forgetting. & Retaining most of the pre-trained parameters and replacing a small number of fine-tuned parameters. \\\\ \\hline \\multirow{2}{*}{**HVCLIP**[93]} & Enhancing the model’s ability to retain critical information while adapting to new tasks or domains. & HVCLIP uses strategies such as forgetting reduction, discrepancy reduction, and feature enhancement. \\\\ \\hline \\multirow{2}{*}{**Continual**} & Enhancing the ability to preserve knowledge from previous tasks while accommodating new ones.. & Continual LLaVA proposes a parameter-efficient tuning method that does not require rehearsal. \\\\ \\hline \\multirow{2}{*}{**LLaCA**[299]} & To reduce forgetting and lower computational costs. & LLaCA dynamically adjusts the EMA weights and introduces an approximation mechanism. \\\\ \\hline \\multirow{2}{*}{**CVM**[300]} & To reduce forgetting and improve generalization. & CVM maps the representations of small visual models to the knowledge space of a fixed LLM. \\\\ \\hline \\multirow{2}{*}{**RE-tune**[301]} & Addressing challenges related to computational resources, data privacy, and catastrophic forgetting. & RE-tune freezes the backbone of the model and trains adapters, using text prompts to guide training. \\\\ \\hline \\multirow{2}{*}{**CluMo**[302]} & Enhancing the performance of MLLMs in CL and improving their ability to retain old knowledge. & CluMo employs a two-stage training and modality fusion prompt strategy. \\\\ \\hline \\multirow{2}{*}{**Fwd-Prompt**[303]**} & To achieve anti-forgetting and positive transfer. & Fwd-Prompt utilizes gradient projection techniques and proposes a multimodal prompt pool. \\\\ \\hline \\multirow{2}{*}{**CPE-CLIP**[304]} & Enhancing the performance of few-shot class incremental learning in MLLMs. & CPE-CLIP using learnable prompts and regularization strategies. \\\\ \\hline \\multirow{2}{*}{**TG**[305]} & To reduce catastrophic forgetting. & TG proposes the model-agnostic self-uncompression method. \\\\ \\hline \\multirow{2}{*}{**LiNeS**[306]} & Preserving the generalization ability of pretraining while improving fine-tuning task performance. & LiNeS proposes parameter updates with differentiated layer depth. \\\\ \\hline \\multirow{2}{*}{**AttriCLIP**[307]} & Enhancing the generalization and continual learning capabilities of MLLMs in multimodal tasks. & AttriCLIP adapts to new tasks using an attribute lexicon and textual prompts. \\\\ \\hline \\multirow{2}{*}{**AttriCLIP**[307]} & Enhancing the generalization and continual learning capabilities of MLLMs in multimodal tasks. & AttriCLIP adapts to new tasks using an attribute lexicon and textual prompts. \\\\ \\hline \\multirow{2}{*}{**C-LoRA**[308]} & To reduce catastrophic forgetting. & C-LoRA performs continual adaptive low-rank adjustments in the cross-attention layers of MLLMs. \\\\ \\hline \\end{tabular} \\end{table} TABLE VIII: Innovations in MLLModel CL Methods. across different fields, highlighting their advantages in handling diverse data sources. We also introduce the most commonly used benchmark testing methods and provide application examples in various domains such as natural language processing and computer vision. Next, we provide a detailed overview of the latest research in continual learning, offering a classification of unimodal and multimodal continual learning in non-large models, and delving into the current state of research on large language models (LLMs) in continual learning. By comparing research across these different areas, we further clarify their approaches and limitations in dealing with data distribution changes. The extensive and in-depth research in both the multimodal large model and continual learning domains has laid a solid foundation for research in multimodal large model continual learning. We conduct a thorough analysis of the current state of research in this area, discussing aspects such as benchmark evaluation, model structures, and innovations in methods, revealing both the potential and the challenges faced by MLLM in continual learning. Finally, we provide a forward-looking discussion on the challenges and future development trends in the continual learning of multimodal large models. Our goal is to inspire researchers in the field and provide valuable insights for future research directions, aiming to promote the advancement and innovation of technologies related to the continual learning of multimodal large models. [MISSING_PAGE_POST] * [35] K. Kafle, B. Price, S. Cohen, and C. Kanan, \"Dvqa: Understanding data visualizations via question answering,\" in _CVPR_, 2018, pp. 5648-5656. * [36] J. Chen, J. Tang, J. Qin, X. Liang, L. Liu, E. P. Xing, and L. Lin, \"Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning,\" _arXiv preprint arXiv:2105.14517_, 2021. * [37] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, \"Microsoft coco: Common objects in context,\" in _ECCV_. Springer, 2014, pp. 740-755. * [38] P. Sharma, N. Ding, S. Goodman, and R. Soricut, \"Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning,\" in _ACL_, 2018, pp. 2556-2565. * [39] O. Sidorov, R. Hu, M. Rohrbach, and A. Singh, \"Textcaps: a dataset for image captioning with reading comprehension,\" in _ECCV_. Springer, 2020, pp. 742-758. * [40] D. Gurari, Y. Zhao, M. Zhang, and N. Bhattacharya, \"Captioning images taken by people who are blind,\" in _ECCV_. Springer, 2020, pp. 417-434. * [41] J. Pont-Tuset, J. Uijlings, S. Changpinyo, R. Soricut, and V. Ferrari, \"Connecting vision and language with localized narratives,\" in _ECCV_. Springer, 2020, pp. 647-664. * [42] H. Agrawal, K. Desai, Y. Wang, X. Chen, R. Jain, M. Johnson, D. Batra, D. Parikh, S. Lee, and P. Anderson, \"Nocaps: Novel object captioning at scale,\" in _ICCV_, 2019, pp. 8948-8957. * [43] Y. Cui, A. Khandelwal, Y. Artzi, N. Snavely, and H. Averbuch-Elor, \"Whc 8's waldo: linking people across text and images,\" in _ICCV_, 2021, pp. 1374-1384. * [44] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, \"Modeling context in referring expressions,\" in _ECCV_. Springer, 2016, pp. 69-85. * [45] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy, \"Generation and comprehension of unambiguous object descriptions,\" in _CVPR_, 2016, pp. 11-20. * [46] M. Tanaka, T. Itamochi, K. Narioka, I. Sato, Y. Ushiku, and T. Harada, \"Generating easy-to-understand referring expressions for target identifications,\" in _ICCV_, 2019, pp. 5794-5803. * [47] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \"Autoio set: An ontology and human-labeled dataset for audio events,\" in _ICASSP_. IEEE, 2017, pp. 776-780. * [48] F. Hernandez, V. Nguyen, S. Ghannay, N. Tomashenko, and Y. Esteve, \"Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation,\" in _Speech and Computer: 20th International Conference, SPECOM 2018, Leipzig, Germany, September 18-22, 2018, Proceedings 20_. Springer, 2018, pp. 198-208. * [49] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, \"Common voice: A massively-multilingual speech corpus,\" _arXiv preprint arXiv:1912.06670_, 2019. * [50] R. Wadhawan, H. Bansal, K.-W. Chang, and N. Peng, \"Contextual: Evaluating context-sensitive text-rich visual reasoning in large multimodal models,\" _arXiv preprint arXiv:2401.13311_, 2024. * [51] T.-H. Wu, G. Biamby, D. Chan, L. Dunlap, R. Gupta, X. Wang, J. E. Gonzalez, and T. Darrell, \"See say and segment: Teaching Imms to overcome false premises,\" in _CVPR_, 2024, pp. 13 459-13 469. * [52] X. Guo, W. Chai, S.-Y. Li, and G. Wang, \"Llava-ultra: Large chinese language and vision assistant for ultrasound,\" in _ACM MM_, 2024, pp. 8845-8854. * [53] K. Chen, Y. Du, T. You, M. Islam, Z. Guo, Y. Jin, G. Chen, and P.-A. Heng, \"Llm-assisted multi-teacher continual learning for visual question answering in robotic surgery,\" _arXiv preprint arXiv:2402.16664_, 2024. * [54] Z. Huang, T. Tang, S. Chen, S. Lin, Z. Jie, L. Ma, G. Wang, and X. Liang, \"Making large language models better planners with reasoning-decision alignment,\" in _ECCV_. Springer, 2025, pp. 73-90. * [55] A. Cossu, A. Carta, L. Passaro, V. Lomonaco, T. Tuytelaars, and D. Baccin, \"Continual pre-training mitigates forgetting in language and vision,\" _Neural Networks_, vol. 179, p. 106492, 2024. * [56] Y. Qin, J. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, \"EllE: Efficient lifelong pre-training for emerging data,\" _arXiv preprint arXiv:2203.06311_, 2022. * [57] Y. Sun, S. Wang, Y. Li, S. Feng, H. Tian, H. Wu, and H. Wang, \"Ernie 2.0: A continual pre-training framework for language understanding,\" in _Proceedings of the AAAI conference on artificial intelligence_, vol. 34, no. 05, 2020, pp. 8968-8975. * [58] Z. Wang, Z. Zhang, C.-Y. Lee, H. Zhang, R. Sun, X. Ren, G. Su, V. Perot, J. Dy, and T. Pfister, \"Learning to prompt for continual learning,\" in _CVPR_, 2022, pp. 139-149. * [59] Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng, \"Data engineering for scaling language models to 128k context,\" _arXiv preprint arXiv:2402.10171_, 2024. * [60] H. Guo, F. Zhu, W. Liu, X.-Y. Zhang, and C.-L. Liu, \"Pilora: Prototype guided incremental for federated class-incremental learning,\" in _ECCV_. Springer, 2025, pp. 141-159. * [61] J. Pang, C. Lin, X. Hao, R. Yin, Z. Wang, Z. Zhang, J. He, and H. Tai Sheng, \"Ftf-er: Feature-topology fusion-based experience replay method for continual graph learning,\" in _ACM MM_, 2024, pp. 8336-8344. * [62] Z. Hu, Y. Li, J. Lyu, D. Gao, and N. Vasconcelos, \"Dense network expansion for class incremental learning,\" in _CVPR_, 2023, pp. 11 858-11867. * [63] D. Abati, J. Tomczak, T. Blankevoort, S. Calderara, R. Cucchiara, and B. E. Bejnordi, \"Conditional channel gated networks for task-aware continual learning,\" in _CVPR_, 2020, pp. 3931-3940. * [64] C. Yang, W. Liu, S. Chen, J. Qi, and A. Zhou, \"Generating prompts in latent space for rehearsal-free continual learning,\" in _ACM MM_, 2024, pp. 8913-8922. * [65] E. Guha and V. Lakshman, \"On the diminishing returns of width for continual learning,\" _arXiv preprint arXiv:2403.06398_, 2024. * [66] S. Cha, K. Cho, and T. Moon, \"Regularizing with pseudo-negatives for continual self-supervised learning,\" 2024. [Online]. Available: [https://arxiv.org/abs/2306.05101](https://arxiv.org/abs/2306.05101) * [67] D. Lee, J. Yoon, and S. J. Hwang, \"Becotta: Input-dependent online blending of experts for continual test-time adaptation,\" _arXiv preprint arXiv:2402.08712_, 2024. * [68] X. Zhao, H. Wang, W. Huang, and W. Lin, \"A statistical theory of regularization-based continual learning,\" _arXiv preprint arXiv:2406.06213_, 2024. * [69] P. Garg, K. Joseph, V. N. Balasubramanian, N. C. Camgoz, C. Wan, K. Kin, W. Si, S. Ma, and F. De La Torre, \"Poet: Prompt offset tuning for continual human action adaptation,\" in _ECCV_. Springer, 2025, pp. 346-455. * [70] Y. Kim, Y. Li, and P. Panda, \"One-stage prompt-based continual learning,\" in _ECCV_. Springer, 2025, pp. 163-179. * [71] Z. Yang, S. Qian, D. Xue, J. Wu, F. Yang, W. Dong, and C. Xu, \"Semantic editing increment benefits zero-shot composed image retrieval,\" in _ACM MM_, 2024, pp. 1245-1254. * [72] M. K. Nori and I.-M. Kim, \"Task confusion and catastrophic forgetting in class-incremental learning: A mathematical framework for discriminative and generative modelings,\" _arXiv preprint arXiv:2410.20768_, 2024. * [73] Y. Kim, J. Fang, Q. Zhang, Z. Cai, Y. Shen, R. Duggal, D. S Raychaudhuri, Z. Tu, Y. Xing, and O. Dabeer, \"Open-world dynamic prompt and continual visual representation learning,\" in _ECCV_. Springer, 2025, pp. 357-374. * [74] D. Marczak, S. Cygert, T. Trzcinski, and B. Twardowski, \"Revisiting supervision for continual representation learning,\" in _ECCV_. Springer, 2025, pp. 181-197. * [75] B. Li, Z. Yan, D. Wu, H. Jiang, and H. Zha, \"Learn to memorize and to forget: A continual learning perspective of dynamic slam,\" in _ECCV_. Springer, 2024, pp. 41-57. * [76] H. Chen, Z. Wu, X. Han, M. Jia, and Y.-G. Jiang, \"Promptfusion: Decoupling stability and plasticity for continual learning,\" _arXiv preprint arXiv:2303.07223_, 2023. * [77] Z. Guo and Y. Hua, \"Continuous training and fine-tuning for domain-specific language models in medical question answering,\" _arXiv preprint arXiv:2311.00204_, 2023. * [78] P. Colombo, T. P. Pires, M. Boudid, D. Culver, R. Melo, C. Corro, A. F. Martins, F. Esposito, V. L. Raposo, S. Morgado _et al._, \"Saullm-7b: A pioneering large language model for law,\" _arXiv preprint arXiv:2403.03883_, 2024. * [79] C. Deng, T. Zhang, Z. He, Q. Chen, Y. * [81] R. Han, X. Ren, and N. Peng, \"Econet: Effective continual pretraining of language models for event temporal reasoning,\" _arXiv preprint arXiv:2012.15283_, 2020. * [82] S. Ma, S. Huang, S. Huang, X. Wang, Y. Li, H.-T. Zheng, P. Xie, F. Huang, and Y. Jiang, \"Ecomgpt-ct: Continual pre-training of e-commerce large language models with semi-structured data,\" _arXiv preprint arXiv:2312.15696_, 2023. * [83] S. Wang, M. Yu, X. Guo, Z. Wang, T. Klinger, W. Zhang, S. Chang, G. Tesauro, B. Zhou, and J. Jiang, \"R\\(\\theta\\)3: Reinforced reader-ranker for open-domain question answering,\" _arXiv preprint arXiv:1709.00023_, 2017. * [84] W. Yang, Y. Xie, A. Lin, X. Li, L. Tan, K. Xiong, M. Li, and J. Lin, \"End-to-end open-domain question answering with bertserini,\" _arXiv preprint arXiv:2012.01718_, 2019. * [85] W. Li, W. Wei, K. Xu, W. Xie, D. Chen, and Y. Cheng, \"Reinforcement learning with token-level feedback for controllable text generation,\" _arXiv preprint arXiv:2403.11558_, 2024. * [86] R. Hadsell, D. Rao, A. A. Rusu, and R. Pascanu, \"Embracing change: Continual learning in deep neural networks,\" _Trends in cognitive sciences_, vol. 24, no. 12, pp. 1028-1040, 2020. * [87] K. Roth, V. Udandarao, S. Dladziolaro, A. Prabhu, M. Cherti, O. Vinyals, O. Henaff, S. Albanie, M. Bethge, and Z. Akata, \"A practitioner's guide to continual multimodal pretraining,\" _arXiv preprint arXiv:2408.14471_, 2024. * [88] Z. Zhang, M. Fang, L. Chen, and M.-R. Namazi-Rad, \"Citb: A benchmark for continual instruction tuning,\" _arXiv preprint arXiv:2310.14510_, 2023. * [89] A. Panagopoulou, L. Xue, N. Yu, J. Li, D. Li, S. Joty, R. Xu, S. Savarese, C. Xiong, and J. C. Niebles, \"X-instructblip: A framework for aligning x-modal instruction-aware representations to l1ms and emergent cross-modal reasoning,\" _arXiv preprint arXiv:2311.18799_, 2023. * [90] M. Le, A. Nguyen, H. Nguyen, T. Nguyen, T. Pham, L. Van Ngo, and N. Ho, \"Mixture of experts meets prompt-based continual learning,\" _arXiv preprint arXiv:2405.14124_, 2024. * [91] S. Jha, D. Gong, and L. Yao, \"Clap4clip: Continual learning with probabilistic finetuning for vision-language models,\" _arXiv preprint arXiv:2403.19137_, 2024. * [92] H. Chen, J. Li, N. Kazagnadou, W. Zhuang, C. Chen, and L. Lyu, \"Dual low-rank adaptation for continual learning with pre-trained models,\" _arXiv preprint arXiv:2411.00623_, 2024. * [93] N. Vesdapunt, K. K. Fu, Y. Wu, X. Zhang, and P. Natarajan, \"Hvclip: High-dimensional vector in clip for unsupervised domain adaptation,\" in _ECCV_. Springer, 2025, pp. 36-54. * [94] C. Chen, J. Zhu, X. Luo, H. Shen, L. Gao, and J. Song, \"Coin: A benchmark of continual instruction tuning for multimodel large language model,\" _arXiv preprint arXiv:2403.08350_, 2024. * [95] T. Srinivasan, T.-Y. Chang, L. Pinto Alva, G. Chochalkis, M. Rostami, and J. Thomson, \"Climb: A continual learning benchmark for vision-and-language tasks,\" _in NeurIPS_, vol. 35, pp. 29 440-29 453, 2022. * [96] M. Cao, Y. Liu, Y. Liu, T. Wang, J. Dong, H. Ding, X. Zhang, I. Reid, and X. Liang, \"Continual llava: Continual instruction tuning in large vision-language models,\" _arXiv preprint arXiv:2411.02564_, 2024. * [97] T. Tang, S. Deldari, H. Xue, C. De Melo, and F. D. Salim, \"Vilocbench: Video language continual learning benchmark,\" _arXiv preprint arXiv:2406.13123_, 2024. * [98] T. He, T. Wu, D. Zhang, G. Duan, K. Qin, and Y.-F. Li, \"Towards lifelong scene graph generation with knowledge-ware in-context prompt learning,\" _arXiv preprint arXiv:2401.14626_, 2024. * [99] X. Wang, Y. Li, C. Fu, L. Xie, K. Li, X. Sun, and L. Ma, \"Freeze-omn: A smart and low latency speech-to-speech dialogue model with frozen llm,\" _arXiv preprint arXiv:2411.00774_, 2024. * [100] B. Qi, X. Chen, J. Gao, D. Li, J. Liu, L. Wu, and B. Zhou, \"Interactive continual learning: Fast and slow thinking,\" in _CVPR_, 2024, pp. 12 882-12 892. * [101] C. Jiang, J. Hongrui, H. Xu, W. Ye, M. Dong, M. Yan, J. Zhang, F. Huang, and S. Zhang, \"Maven: An effective multi-granularity hybrid visual encoding framework for multimodal large language model,\" _arXiv preprint arXiv:2408.12321_, 2024. * [102] Z. Zong, B. Ma, D. Shen, G. Song, H. Shao, D. Jiang, H. Li, and Y. Liu, \"Mova: Adapting mixture of vision experts to multimodal context,\" _arXiv preprint arXiv:2404.13046_, 2024. * [103] L. Shen, G. Chen, R. Shao, W. Guan, and L. Nie, \"Mome: Mixture of multimodal experts for generalist multimodal large language models,\" _arXiv preprint arXiv:2407.12709_, 2024. * [104] B.-K. Lee, C. W. Kim, B. Park, and Y. M. Ro, \"Meteor: Mambas-based traversal of rationale for large language and vision models,\" _arXiv preprint arXiv:2405.15574_, 2024. * [105] H. Ma, T. Hu, Z. Pu, B. Liu, X. Ai, Y. Liang, and M. Chen, \"Coevolving with the other your: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning,\" _arXiv preprint arXiv:2410.06101_, 2024. * [106] Y. Jiao, S. Chen, Z. Jie, J. Chen, L. Ma, and Y.-G. Jiang, \"Lumen: Unleashing versatile vision-centric capabilities of large multimodal models,\" _arXiv preprint arXiv:2403.07304_, 2024. * [107] C. Zhao, Y. Song, J. Chen, K. Rong, H. Feng, G. Zhang, S. Ji, J. Wang, E. Ding, and Y. Sun, \"Octopus: A multi-modal llm with parallel recognition and sequential understanding,\" in _The Thirty-eighth Annual Conference on Neural Information Processing Systems_. * [108] Y.-K. Zhang, S. Lu, Y. Li, Y. Ma, Q.-G. Chen, Z. Xu, W. Luo, K. Zhang, D.-C. Zhan, and H.-J. Ye, \"Wings: Learning multimodal lms without text-only forgetting,\" _arXiv preprint arXiv:2406.03496_, 2024. * [109] T. Gao, P. Chen, M. Zhang, C. Fu, Y. Shen, Y. Zhang, S. Zhang, X. Zheng, X. Sun, L. Cao _et al._, \"Cantor: Inspiring multimodal chain-of-thoughst of rmlml,\" in _ACM MM_, 2024, pp. 9096-9105. * [110] D. Luo, C. Feng, Y. Nong, and Y. Shen, \"Autom3l: An automated multimodal machine learning framework with large language models,\" in _ACM MM_, 2024, pp. 8586-8594. * [111] Y. Fan, W. Xu, H. Wang, J. Liu, and S. Guo, \"Detached and interactive multimodal learning,\" in _ACM MM_, 2024, pp. 5470-5478. * [112] X. Liu, X. Jia, Y. Xun, S. Liang, and X. Cao, \"Multimodal unlearnable examples: Protecting data against multimodal contrastive learning,\" in _ACM MM_, 2024, pp. 8024-8033. * [113] J. Zhang, Y. Yu, and Y. Zhang, \"Cream: Coarse-to-fine retrieval and multi-modal efficient tuning for document vqa,\" in _ACM MM_, 2024, pp. 925-934. * [114] L. Zheng, B. Chen, H. Fei, F. Li, S. Wu, L. Liao, D. Ji, and C. Teng, \"Self-adaptive fine-grained multi-modal data augmentation for semi-supervised multi-modal coreference resolution,\" in _ACM MM_, 2024, pp. 8576-8585. * [115] T. Wu, M. Li, J. Chen, W. Ji, W. Lin, J. Gao, K. Kuang, Z. Zhao, and F. Wu, \"Semantic alignment for multimodal large language models,\" in _ACM MM_, 2024, pp. 3489-3498. * [116] S. Lu, L. Guo, W. Wang, Z. Zhao, T. Yue, J. Liu, and S. Liu, \"Collaborative training of tiny-large vision language models,\" in _ACM MM_, 2024, pp. 4928-4937. * [117] M. Kim, J. Yeo, S. J. Park, H. Rha, and Y. M. Ro, \"Efficient training for multilingual visual speech recognition: Pre-training with discretized visual speech representation,\" in _ACM MM_, 2024, pp. 1311-1320. * [118] P. Wang, W. Sun, Z. Zhang, J. Jia, Y. Jiang, Z. Zhang, X. Min, and G. Zhai, \"Large multi-modality model assisted ai-generated image quality assessment,\" in _ACM MM_, 2024, pp. 7803-7812. * [119] Z. Ge, H. Huang, M. Zhou, J. Li, G. Wang, S. Tang, and Y. Zhuang, \"Worldgpt: Empowering llm as multimodal world model,\" in _ACM MM_, 2024, pp. 7346-7355. * [120] H. Wu, Z. Zhang, W. Zhang, C. Chen, L. Liao, C. Li, Y. Gao, A. Wang, E. Zhang, W. Sun _et al._, \"Q-align: Teaching lms for visual scoring via discrete text-defined levels,\" _arXiv preprint arXiv:2312.17090_, 2023. * [121] R. Cai, S. Muralitharan, G. Heinrich, H. Yin, Z. Wang, J. Kautz, and P. Molchanov, \"Flextron: Many-in-one flexible large language model,\" _arXiv preprint arXiv:2406.10260_, 2024. * [122] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, \"Next-gppt: Any-to-any multimodal llm,\" _arXiv preprint arXiv:2309.05519_, 2023. * [123] X. Li, F. Zhang, H. Diao, Y. Wang, X. Wang, and L.-Y. Duan, \"Densefusion-1m: Merging vision experts for comprehensive multimodal perception,modal large language models,\" _arXiv preprint arXiv:2406.04236_, 2024. * [128] J. Xie, Y. Zhang, M. Lin, L. Cao, and R. Ji, \"Advancing multimodal large language models with quantization-aware scale learning for efficient adaptation,\" in _ACM MM_, 2024, pp. 10 582-10 591. * [129] Y. Wang, L. Wang, Q. Zhou, Z. Wang, H. Li, G. Hua, and W. Tang, \"Multimodal llm enhanced cross-lingual cross-modal retrieval,\" in _ACM MM_, 2024, pp. 8296-8305. * [130] W. Han, C. Cai, Y. Guo, and J. Peng, \"Erl-mr: Harnessing the power of euler feature representations for balanced multi-modal learning,\" in _ACM MM_, 2024, pp. 4591-4600. * [131] Z. Wu, Y. Liu, M. Zhan, P. Hu, and X. Zhu, \"Adaptive multi-modality prompt learning,\" in _ACM MM_, 2024, pp. 8672-8680. * [132] L. Cai, S. Li, W. Ma, J. Kang, B. Xie, Z. Sun, and C. Zhu, \"Enhancing cross-modal fine-tuning with gradually intermediate modality generation,\" _arXiv preprint arXiv:2406.09003_, 2024. * [133] W. Li, H. Fan, Y. Wong, Y. Yang, and M. Kankanhalli, \"Improving context understanding in multimodal large language models via multimodal composition learning,\" in _Forty-first ICML_. * [134] C. Schalermann, N. D. Singh, F. Croce, and M. Hein, \"Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models,\" _arXiv preprint arXiv:2402.12336_, 2024. * [135] Z. Huang, C. Liu, Y. Dong, H. Su, S. Zheng, and T. Liu, \"Machine vision therapy: Multimodal large language models can enhance visual robustness via denoising in-context learning,\" in _Forty-first ICML_, 2023. * [136] R. Yu, W. Yu, and X. Wang, \"Attention prompting on image for large vision-language models,\" in _ECCV_. Springer, 2025, pp. 251-268. * [137] K. Huang, H. Zou, Y. Xi, B. Wang, Z. Xie, and L. Yu, \"Ivtp: Instruction-guided visual token pruning for large vision-language models,\" in _ECCV_. Springer, 2025, pp. 214-230. * [138] Y. Sun, F. Yu, S. Chen, Y. Zhang, J. Huang, C. Li, Y. Li, and C. Wang, \"Chattracker: Enhancing visual tracking performance via chutting with multimodal large language model,\" _arXiv preprint arXiv:2411.01756_, 2024. * [139] Z. Li, Y. Xie, R. Shao, G. Chen, D. Jiang, and L. Nie, \"Optimus-1: Hybrid multimodal memory empowered agents excel in long-horizon tasks,\" _arXiv preprint arXiv:2408.03615_, 2024. * [140] J. Li, X. Wang, S. Zhu, C.-W. Kuo, L. Xu, F. Chen, J. Jain, H. Shi, and L. Wen, \"Cumo: Scaling multimodal llm with co-uycedl mixture-of-experts,\" _arXiv preprint arXiv:2405.05949_, 2024. * [141] H. Liu, Q. You, X. Han, Y. Liu, H. Huang, R. He, and H. Yang, \"Visual anchors are strong information aggregators for multimodal large language model,\" _arXiv preprint arXiv:2405.17815_, 2024. * [142] Z. Huang, K. Ji, B. Gong, Z. Qing, Q. Zhang, K. Zheng, J. Wang, J. Chen, and M. Yang, \"Accelerating pre-training of multimodal lms via chain-of-sight,\" _arXiv preprint arXiv:2407.15819_, 2024. * [143] H. Yao, W. Wu, T. Yang, Y. Song, M. Zhang, H. Feng, Y. Sun, Z. Li, W. Ouyang, and J. Wang, \"Dense connector for mllms,\" _arXiv preprint arXiv:2405.13800_, 2024. * [144] H. Wang, C. Lai, Y. Sun, and W. Ge, \"Weakly supervised gaussian contrastive grounding with large multimodal models for video question answering,\" in _ACM MM_, 2024, pp. 5289-5298. * [145] H. Wang, J. Ren, Y. Ding, L. Ren, H. Jiang, W. Chen, F. Feng, and X. Wang, \"Q-moe: Connector for mllms with text-driven routing,\" in _ACM MM_, 2024, pp. 817-825. * [146] A. S. Benjamin, C. Pehle, and K. Daruwalla, \"Continual learning with the neural tangent ensemble,\" _arXiv preprint arXiv:2408.17384_, 2024. * [147] D. Lee, M. Yoo, W. K. Kim, W. Choi, and H. Woo, \"Incremental learning of retrievable skills for efficient continual task adaptation,\" _arXiv preprint arXiv:2410.22658_, 2024. * [148] K. Xu, H. Zhang, Y. Li, Y. Peng, and J. Zhou, \"Mitigate catastrophic remembering via continual knowledge purification for noisy lifelong person re-identification,\" in _ACM MM_, 2024, pp. 5790-5799. * [149] L. Liu, L. Liu, and Y. Cui, \"Prior-free balanced replay: Uncertainty-guided reservoir sampling for long-tailed continual learning,\" in _ACM MM_, 2024, pp. 2888-2897. * [150] Y. Hu, D. Cheng, D. Zhang, N. Wang, T. Liu, and X. Gao, \"Task-aware orthogonal sparse network for exploring shared knowledge in continual learning,\" in _Forty-first ICML_. * [151] S. Lee, H. Jeon, J. Son, and G. Kim, \"Learning to continually learn with the bayesian principle,\" _arXiv preprint arXiv:2405.18758_, 2024. * [152] S. Cha, K. Cho, and T. Moon, \"Regularizing with pseudo-negatives for continual self-supervised learning,\" in _Forty-first ICML_. * [153] M. Malagon, J. Ceberio, and J. A. Lozano, \"Self-compposing policies for scalable continual reinforcement learning,\" in _Forty-first ICML_. * [154] R. Wang, J. Hwang, A. Boopathy, and I. R. Fiete, \"Rapid learning without catastrophic forgetting in the morris water maze,\" in _Forty-first ICML_. * [155] J. Liang, J. Zhong, H. Gu, Z. Lu, X. Tang, G. Dai, S. Huang, L. Fan, and Q. Yang, \"Diffusion-driven data replay: A novel approach to combat forgetting in federated class continual learning,\" in _ECCV_. Springer, 2025, pp. 303-319. * [156] F. J. Cendra, B. Zhao, and K. Han, \"Promptcd: Learning gaussian mixture prompt pool for continual category discovery,\" in _ECCV_. Springer, 2025, pp. 188-205. * [157] D. Li, A. Zhang, J. Gao, and B. Qi, \"An efficient memory module for graph few-shot class-incremental learning,\" _arXiv preprint arXiv:2411.06659_, 2024. * [158] W. Sun, Q. Li, S. Zhang, W. Wang, and Y. Geng, \"Incremental learning via robust parameter posterior fusion,\" in _ACM MM_, 2024, pp. 4292-4301. * [159] X.-Q. Liu, M.-H. Liu, Z.-D. Chen, X. Luo, and X.-S. Xu, \"Hierarchical multi-label learning for incremental multilingual text recognition,\" in _ACM MM_, 2024, pp. 8750-8758. * [160] H. Yu, X. Yang, X. Gao, Y. Feng, H. Wang, Y. Kang, and T. Li, \"Overcoming spatial temporal catastrophic forgetting for federated class-incremental learning,\" in _ACM MM_, 2024, pp. 5280-5288. * [161] L. Xiaochen, J. Cheng, Z. Xia, Z. Chen, J. Shi, Z. Dong, and N. Ishai, \"Ts-llm: Class incremental learning for online action detection,\" in _ACM Multimedia 2024_. * [162] D. Li, T. Wang, J. Chen, W. Dai, and Z. Zeng, \"Harnessing neural unit dynamics for effective and scalable class-incremental learning,\" _arXiv preprint arXiv:2406.02428_, 2024. * [163] T. Fischer, Y. Liu, A. Jesslen, N. Ahmed, P. Kaushik, A. Wang, A. L. Yuille, A. Kortylewski, and E. Ilg, \"inemo: Incremental neural mesh models for robust class-incremental learning,\" in _ECCV_. Springer, 2024, pp. 357-374. * [164] X. Han, Z. Feng, and Y. Ning, \"A topology-aware graph coarsening framework for continual graph learning,\" _arXiv preprint arXiv:2401.03077_, 2024. * [165] H. Zhuang, Y. Chen, D. Fang, R. He, K. Tong, H. Wei, Z. Zeng, and C. Chen, \"Gacl: Exemplar-free generalized analytic continual learning,\" in _The Thirty-eighth Annual Conference on Neural Information Processing Systems_, 2024. * [166] Y. Fan, Y. Wang, P. Zhu, and Q. Hu, \"Dynamic sub-graph distillation for robust semi-supervised continual learning,\" in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 38, no. 11, 2024, pp. 11 927-11 935. * [167] L. Jiao, Q. Lai, Y. Li, and Q. Xu, \"Vector quantization prompting for continual learning,\" _arXiv preprint arXiv:2410.20444_, 2024. * [168] A. Prabhu, S. Sinha, P. Kumaraguru, P. Torr, O. Sener, and P. K. Dokania, \"Random representations outperform online continually learned representations,\" in _The Thirty-eighth Annual Conference on Neural Information Processing Systems_. * [169] B. Csaba, W. Zhang, M. Muller, S.-N. Lim, P. Torr, and A. Bibi, \"Label delay in online continual learning,\" in _The Thirty-eighth Annual Conference on Neural Information Processing Systems_. * [170] Q. Li, Y. Feng, and J. Zhou, \"Progressive prototype evolving for dual-forgetting mitigation in non-exemplar online continual learning,\" in _ACM MM_, 2024, pp. 2477-2486. * [171] C. Yang, M. Dong, X. Zhang, J. Qi, and A. Zhou, \"Introducing common null space of gradients for gradient projection methods in continual learning,\" in _ACM MM_, 2024, pp. 5498-5497. * [172] Y. Wen, Z. Tan, K. Zheng, C. Xie, and W. Huang, \"Provable contrastive continual learning,\" _arXiv preprint arXiv:2405.18756_, 2024. * [173] Y. Wu, H. Wang, P. Zhao, Y. Zheng, Y. Wei, and L.-K. Huang, \"Mitigating catastrophic forgetting in online continual learning by modeling previous task interrelations via pareto optimization,\" in _Forty-first ICML_. * [174] H. Piao, Y. Wu, D. Wu, and Y. Wei, \"Federated continual learning via prompt-based dual knowledge transfer,\" in _Forty-first ICML_. * [175] D. Kim, S. Yoon, D. Park, Y. Lee, H. Song, J. Bang, and J.-G. Lee, \"One size fits all for semantic shifts: Adaptive prompt tuning for continual learning,\" _arXiv preprint arXiv:2311.12048_, 2023. * [176] Y.-S. Liang and W * [177] H. Zhuang, Y. Liu, R. He, K. Tong, Z. Zeng, C. Chen, Y. Wang, L.-P. Chau, \"F-oal: Forward-only online analytic learning with fast training and low memory footprint in class incremental learning,\" in _The Thirty-eighth Annual Conference on Neural Information Processing Systems_. * [178] W. Shi and M. Ye, \"Perspective representation learning for non-exemplar class-incremental learning,\" in _The Thirty-eighth Annual Conference on Neural Information Processing Systems_. * [179] X. Hao, W. Ni, X. Jiang, W. Tan, and B. Yan, \"Addressing imbalance for class incremental learning in medical image classification,\" in _ACM MM_, 2024, pp. 2467-2476. * [180] Z. Yang, L. Li, J. Zhang, T. Wang, Y. Sun, and C. Yan, \"Domain shared and specific prompt learning for incremental monocular depth estimation,\" in _ACM MM_, 2024, pp. 8306-8315. * [181] B. Zheng, D.-W. Zhou, H.-J. Ye, and D.-C. Zhan, \"Multi-layer rehearsal feature augmentation for class-incremental learning,\" in _Forty-first ICML_. * [182] K. Jeeveswaran, E. Arani, and B. Zonooz, \"Gradual divergence for seamless adaptation: A novel domain incremental learning method,\" _arXiv preprint arXiv:2406.16231_, 2024. * [183] D.-W. Zhou, H.-L. Sun, H.-J. Ye, and D.-C. Zhan, \"Expandable subspace ensemble for pre-trained model-based class-incremental learning,\" in _CVPR_, 2024, pp. 23554-23564. * [184] J. Huang, J. Zhang, K. Jiang, H. Qiu, and S. Lu, \"Visual instruction tuning towards general-purpose multimodal model: A survey,\" _arXiv preprint arXiv:2312.16602_, 2023. * [185] C. Li, \"Large multimodal models: Notes on cvpr 2023 tutorial,\" _arXiv preprint arXiv:2306.14985_, 2023. * [186] H. Liu, C. Li, Y. Li, and Y. J. Lee, \"Improved baselines with visual instruction tuning,\" in _CVPR_, 2024, pp. 26296-26306. * [187] J. S. Park, J. O'Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein, \"Generative agents: Interactive simulators of human behavior,\" in _Proceedings of the 36th annual acm symposium on user interface software and technology_, 2023, pp. 1-22. * [188] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark _et al._, \"Learning transferable visual models from natural language supervision,\" in _ICML_. PMLR, 2021, pp. 8748-8763. * [189] J. Rocamonde, V. Montesinos, E. Nava, E. Perez, and D. Lindner, \"Vision-language models are zero-shot reward models for reinforcement learning,\" _arXiv preprint arXiv:2310.12921_, 2023. * [190] Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L.-Y. Gui, Y.-X. Wang, Y. Yang _et al._, \"Aligning large multimodal models with factually augmented rbf,\" _arXiv preprint arXiv:2309.14525_, 2023. * [191] B. Yuan, D. Zhao, Z. Liu, W. Li, and T. Li, \"Continual panoptic perception: Towards multi-modal incremental interpretation of remote sensing images,\" in _ACM MM_, 2024, pp. 2117-2126. * [192] Y. Feng, Z. Tian, Y. Zhu, Z. Han, H. Luo, G. Zhang, and M. Song, \"Cp-prompt: Composition-based cross-modal prompting for domain-incremental continual learning,\" in _ACM MM_, 2024, pp. 2729-2738. * [193] X. Yue, X. Zhang, Y. Chen, C. Zhang, M. Lao, H. Zhuang, X. Qian, and H. Li, \"Mmal: Multi-modal analytic learning for exemplar-free audio-visual class incremental tasks,\" in _ACM MM_, 2024, pp. 2428-2437. * [194] X. Chen, J. Zhang, X. Wang, N. Zhang, T. Wu, Y. Wang, Y. Wang, and H. Chen, \"Continual multimodal knowledge graph construction,\" _arXiv preprint arXiv:2305.08698_, 2023. * [195] Y. Ye, Y. Xie, J. Zhang, Z. Chen, Q. Wu, and Y. Xia, \"Continual self-supervised learning: Towards universal multi-modal medical data representation learning,\" in _CVPR_, 2024, pp. 11 114-1124. * [196] J. Deng, H. Zhang, K. Ding, J. Hu, X. Zhang, and Y. Wang, \"Zero-shot generalizable incremental learning for vision-language object detection,\" _arXiv preprint arXiv:2403.01680_, 2024. * [197] J. Lee, J. Yoon, W. Kim, Y. Kim, and S. J. Hwang, \"Stella: Continual audio-video pre-training with spatiotemporal localized alignment,\" in _Forty-first ICML_. * [198] L. Yang, H. Zhao, Y. Yu, X. Zeng, and X. Li, \"Rcs-prompt: Learning prompt to rearrange class space for prompt-based continual learning,\" in _ECCV_, 2024. * [200] Z. Zheng, M. Ma, K. Wang, Z. Qin, X. Yue, and Y. You, \"Preventing zero-shot transfer degradation in continual learning of vision-language models,\" in _ICCV_, 2023, pp. 19 125-19 136. * [201] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, \"Conditional prompt learning for vision-language models,\" in _CVPR_, 2022, pp. 16 816-16 825. * [202] Y. Xu, Y. Chen, J. Nie, Y. Wang, H. Zhuang, and M. Okumura, \"Advancing cross-domain discriminability in continual learning of vision-language models,\" _arXiv preprint arXiv:2406.18688_, 2024. * [203] Z. Li and D. Hoiem, \"Learning without forgetting,\" _IEEE transactions on pattern analysis and machine intelligence_, vol. 40, no. 12, pp. 2935-2947, 2017. * [204] N. Loo, S. Swaroop, and R. E. Turner, \"Generalized variational continual learning,\" _arXiv preprint arXiv:2011.12328_, 2020. * [205] L. Pellegrini, V. Lomonaco, G. Graffietti, and D. Maltoni, \"Continual learning at the edge: Real-time training on smartphone devices,\" _arXiv preprint arXiv:2105.13127_, 2021. * [206] F. Sarfraz, E. Arani, and B. Zonooz, \"Sparse coding in a dual memory system for lifelong learning,\" in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 37, no. 8, 2023, pp. 9714-9722. * [207] A. Abbasi, P. Nooralinejad, V. Braverman, H. Pirsiavash, and S. Kolouri, \"Sparsity and heterogeneous dropout for continual learning in the null space of neural activations,\" in _Conference on Lifelong Learning Agents_. PMLR, 2022, pp. 617-628. * [208] L. Huang, Y. Zeng, C. Yang, Z. An, B. Diao, and Y. Xu, \"etag: Class-incremental learning via embedding distillation and task-oriented generation,\" in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 38, no. 11, 2024, pp. 12 591-12 599. * [209] L. Huang, Z. An, Y. Zeng, Y. Xu _et al._, \"Kfc: Knowledge reconstruction and feedback consolidation enable efficient and effective continual generative learning,\" in _The Second Tiny Papers Track at ICLR 2024_, 2024. * [210] Z. Ke, B. Liu, and X. Huang, \"Continual learning of a mixed sequence of similar and dissimilar tasks,\" in _NeurIPS_, vol. 33, pp. 18 493-18 504, 2020. * [211] L. Yu, B. Tuwardowski, X. Liu, L. Herranz, K. Wang, Y. Cheng, S. Jui, and J. v. de Weijer, \"Semantic drift compensation for class-incremental learning,\" in _CVPR_, 2020, pp. 6982-6991. * [212] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elboseiny, \"Efficient lifelong learning with a-gem,\" _arXiv preprint arXiv:1812.00420_, 2018. * [213] H.-J. Chen, A.-C. Cheng, D.-C. Juan, W. Wei, and M. Sun, \"Mitigating forgetting in online continual learning via instance-aware parameterization,\" in _NeurIPS_, vol. 33, pp. 17 466-17 477, 2020. * [214] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, and T. Tuytelaars, \"A continual learning survey: Derlying forgetting in classification tasks,\" _IEEE transactions on pattern analysis and machine intelligence_, vol. 44, no. 7, pp. 3366-3385, 2021. * [215] Z. Miao, Z. Wang, W. Chen, and Q. Qiu, \"Continual learning with filter atom swapping,\" in _International Conference on Learning Representations_, 2021. * [216] Q. Pham, C. Liu, and S. Hoi, \"Dualnet: Continual learning, fast and slow,\" in _NeurIPS_, vol. 34, pp. 16 131-16 144, 2021. * [217] T. Konishi, M. Kurokawa, C. Ono, Z. Ke, G. Kim, and B. Liu, \"Parameter-level soft-masking for continual learning,\" in _ICML_. PMLR, 2023, pp. 17 492-17 505. * [218] X. Li, S. Wang, J. Sun, and Z. Xu, \"Memory efficient data-free distillation for continual learning,\" _Pattern Recognition_, vol. 144, p. 109857, 2023. * [219] ----, \"Variational data-free knowledge distillation for continual learning,\" _IEEE Transactions on Pattern Analysis and Machine Intelligence_, vol. 45, no. 10, pp. 12618-12 634, 2023. * [220] H. Shin, J. K. Lee, J. Kim, and J. Kim, \"Continual learning with deep generative replay,\" in _NeurIPS_, vol. 30, 2017. * [221] X. Tao, X. Chang, X. Hong, X. Wei, and Y. Gong, \"Topology-preserving class-incremental learning,\" in _ECCV_. Springer, 2020, pp. 254-270. * [222] S. Wang, X. Li, J. Sun, and Z. Xu, \"Training networks in null space of feature covariance for continual learning,\" in _CVPR_, 2021, pp. 184-193. * [223] W. Sun, Q. Li, J. Zhang, W. Wang, and Y.-a. Geng, \"Decoupling learning and remembering: A bilevel memory framework with knowledge projection for task-incremental learning,\" in _CVPR_, 2023, pp. 20 186-20 195. * [224] W. Sun, J. Zhang, D. Wang,tively trained experts in continual learning,\" _arXiv preprint arXiv:2401.10191_, 2024. * [25] W. Shi and M. Ye, \"Prototype reminiscence and augmented asymmetric knowledge aggregation for non-exemplar class-incremental learning,\" in _ICCV_, 2023, pp. 1772-1781. * [26] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska _et al._, \"Overcoming catastrophic forgetting in neural networks,\" _Proceedings of the national academy of sciences_, vol. 114, no. 13, pp. 3521-3526, 2017. * [27] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, \"icarl: Incremental classifier and representation learning,\" in _CVPR_, 2017, pp. 2001-2010. * [28] H. Ahn, S. Cha, D. Lee, and T. Moon, \"Uncertainty-based continual learning with adaptive regularization,\" _in NeurIPS_, vol. 32, 2019. * [29] F. Zenke, B. Poole, and S. Ganguli, \"Continual learning through synaptic intelligence,\" in _ICML_. PMLR, 2017, pp. 3987-3995. * [30] J. Yoon, E. Yang, J. Lee, and S. J. Hwang, \"Lifelong learning with dynamically expandable networks,\" _arXiv preprint arXiv:1708.01547_, 2017. * [31] S. Lee, J. Ha, D. Zhang, and G. Kim, \"A neural dirichlet process mixture model for task-free continual learning,\" _arXiv preprint arXiv:2001.00689_, 2020. * [32] D. Madan, J. Yoon, Y. Li, Y. Liu, and S. J. Hwang, \"Representational continuity for unsupervised continual learning,\" _arXiv preprint arXiv:2110.06976_, 2021. * [33] E. Fini, V. G. T. Da Costa, X. Alameda-Pineda, E. Ricci, K. Alahari, and J. Mairal, \"Self-supervised models are continual learners,\" in _CVPR_, 2022, pp. 9621-9630. * [34] J. Yoon, S. J. Hwang, and Y. Cao, \"Continual learners are incremental model generalizers,\" in _ICML_. PMLR, 2023, pp. 40 129-40 146. * [35] S. Yan, L. Hong, H. Xu, J. Han, T. Tuytelaars, Z. Li, and X. He, \"Generative negative text replay for continual vision-language pretraining,\" in _ECCV_. Springer, 2022, pp. 2-38. * [36] W. Pian, S. Mo, Y. Guo, and Y. Tian, \"Audio-visual class-incremental learning,\" in _ICCV_, 2023, pp. 7799-7811. * [37] S. Mo, W. Pian, and Y. Tian, \"Class-incremental grouping network for continual audio-visual learning,\" in _ICCV_, 2023, pp. 7788-7798. * [38] W. Yin, J. Li, and C. Xiong, \"Continual: Continual learning from task instructions,\" _arXiv preprint arXiv:2203.08512_, 2022. * [39] X. Wang, T. Chen, Q. Ge, H. Xia, R. Bao, R. Zheng, Q. Zhang, T. Gui, and X. Huang, \"Orthogonal subspace learning for language model continual learning,\" _arXiv preprint arXiv:2310.14152_, 2023. * [40] W. Zhao, S. Wang, Y. Hu, Y. Zhao, B. Qin, X. Zhang, Q. Yang, D. Xu, and W. Che, \"Daprl: A dual attention framework for parameter-efficient continual learning of large language models,\" _arXiv preprint arXiv:2401.08295_, 2024. * [41] J. Jang, S. Kim, Y. Se, D. Kim, L. Logeswaran, M. Lee, K. Lee, and M. Seo, \"Exploring the benefits of training expert language models over instruction tuning,\" in _ICML_. PMLR, 2023, pp. 14 702-14 729. * [42] C. Wu, Y. Gan, Y. Ge, Z. Lu, J. Wang, Y. Feng, P. Luo, and Y. Shan, \"Llama pro: Progressive llama with block expansion,\" _arXiv preprint arXiv:2401.02415_, 2024. * [43] D. Cheng, S. Huang, and F. Wei, \"Adapting large language models via reading comprehension,\" in _The Twelfth International Conference on Learning Representations_, 2023. * [44] J. Mok, J. Do, S. Lee, T. Taghavi, S. Yu, and S. Yoon, \"Large-scale lifelong learning of in-context instructions and how to tackle it,\" in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2023, pp. 12 573-12 589. * [45] Y. Seo, D. Lee, and J. Yeo, \"Train-attention: Meta-learning where to focus in continual knowledge learning,\" _arXiv preprint arXiv:2407.16920_, 2024. * [46] H. Que, J. Liu, G. Zhang, C. Zhang, X. Qu, Y. Ma, F. Duan, Z. Bai, J. Wang, Y. Zhang _et al._, \"D-cpt law: Domain-specific continual pre-training scaling law for large language models,\" _arXiv preprint arXiv:2406.01375_, 2024. * [47] S. Malla, J. H. Choi, and C. Choi, \"Copal: Continual pruning in large language generative models,\" _arXiv preprint arXiv:2405.02347_, 2024. * [48] D. Marczak, B. Wardowski, T. Trzcinski, and S. Cygert, \"Magnac: Leveraging model merging for seamless continual learning,\" in _ECCV_. Springer, 2025, pp. 379-395. * [49] W. Zhao, S. Wang, Y. Hu, Y. Zhao, B. Qin, X. Zhang, Q. Yang, D. Xu, and W. Che, \"Sapt: A shared attention framework for parameter-efficient continual learning of large language models,\" in _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2024, pp. 11 641-11 661. * [50] J. Huang, L. Cui, A. Wang, C. Yang, X. Liao, L. Song, J. Yao, and J. Su, \"Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal,\" _arXiv preprint arXiv:2403.01244_, 2024. * [51] S. Dou, E. Zhou, Y. Liu, S. Gao, W. Shen, L. Xiong, Y. Zhou, X. Wang, Z. Xi, X. Fan _et al._, \"Loramoe: Alleviating world knowledge forgetting in large language models via one-style plugin,\" in _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2024, pp. 1932-1945. * [52] J. Devlin, \"Bert: Pre-training of deep bidirectional transformers for language understanding,\" _arXiv preprint arXiv:1810.04805_, 2018. * [53] X. Du, Z. Yu, S. Gao, D. Pan, Y. Cheng, Z. Ma, R. Yuan, X. Qu, J. Liu, T. Zheng _et al._, \"Chinese tiny llm: Pretraining a chinese-centric large language model,\" _arXiv preprint arXiv:2404.04167_, 2024. * [54] T. Eloundou, S. Manning, P. Mishkin, and D. Rock, \"Gpts are gpts: An early look at the labor market impact potential of large language models,\" _arXiv preprint arXiv:2303.10130_, 2023. * [55] S. Kukreja, T. Kumar, A. Purohit, A. Dasgupta, and D. Guha, \"A literature survey on open source large language models,\" in _Proceedings of the 2024 7th International Conference on Computers in Management and Business_, 2024, pp. 133-143. * [56] E. Kasneel, K. Sessler, S. Kuchenmann, M. Bannert, D. Dementieva, F. Fischer, D. Gasser, G. Groh, S. Gunnemann, E. Hullermeier _et al._, \"Chatepf for good? on opportunities and challenges of large language models for education,\" _Learning and individual differences_, vol. 103, p. 102274, 2023. * [57] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong _et al._, \"A survey of large language models,\" _arXiv preprint arXiv:2303.18223_, 2023. * [58] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar, N. Barnes, and A. Mian, \"A comprehensive overview of large language models,\" _arXiv preprint arXiv:2307.06435_, 2023. * [59] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang _et al._, \"A survey on evaluation of large language models,\" _ACM Transactions on Intelligent Systems and Technology_, vol. 15, no. 3, pp. 1-45, 2024. * [60] M. Chen, T. Tworke, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman _et al._, \"Evaluating large language models trained on code,\" _arXiv preprint arXiv:2107.03374_, 2021. * [61] C. UNLU, \"Interprettority: Using large language models for interpreter assessment,\" _HiT-IT 2023_, pp. 78-96, 2023. * [62] L. Wu, Z. Zheng, Z. Qiu, H. Wu, H. Gu, T. Shen, C. Qin, C. Zhu, H. Zhu, Q. Liu _et al._, \"A survey on large language models for recommendation,\" _World Wide Web_, vol. 27, no. 5, p. 60, 2024. * [63] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T. Zhang, F. Wu _et al._, \"Instruction tuning for large language models: A survey,\" _arXiv preprint arXiv:2308.10792_, 2023. * [64] J. Yu, H. Xiong, L. Zhang, H. Diao, Y. Zhuge, L. Hong, D. Wang, H. Lu, Y. He, and L. Chen, \"Llms can evolve continually on mortality for x-modal reasoning,\" _arXiv preprint arXiv:2410.20178_, 2024. * [65] L. Tang, Z. Tian, K. Li, C. He, H. Zhou, H. Zhao, X. Li, and J. Jia, \"Mind the interference: Retaining pre-trained knowledge in parameter efficient continual learning of vision-language models,\" in _ECCV_. Springer, 2025, pp. 346-365. * [66] X. Cao, H. Lu, L. Huang, X. Liu, and M.-M. Cheng, \"Generative multi-modal models are good class incremental learners,\" in _CVPR_, 2024, pp. 28 706-28 717. * [67] K.-H. Park, K. Song, and G.-M. Park, \"Pre-trained vision and language transformers are few-shot incremental learners,\" in _CVPR_, 2024, pp. 23 881-23 890. * [68]* [270] Y. Li, G. Pang, W. Suo, C. Jing, Y. Xi, L. Liu, H. Chen, G. Liang, and P. Wang, \"Coleclip: Open-domain continual learning via joint task prompt and vocabulary learning,\" _arXiv preprint arXiv:2403.10245_, 2024. * [271] Y. Zhai, S. Tong, X. Li, M. Cai, Q. Qu, Y. J. Lee, and Y. Ma, \"Investigating the catastrophic forgetting in multimodal large language models,\" _arXiv preprint arXiv:2309.10313_, 2023. * [272] A. Maharana, J. Yoon, T. Chen, and M. Bansal, \"Adapt-\\(\\mathrm{inf}\\)_fty_: Scalable lifelong multimodal instruction tuning via dynamic data selection,\" _arXiv preprint arXiv:2410.10636_, 2024. * [273] G. Luo, X. Yang, W. Dou, Z. Wang, J. Dai, Y. Qiao, and X. Zhu, \"Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training,\" _arXiv preprint arXiv:2410.08202_, 2024. * [274] S. Zhong, S. Gao, Z. Huang, W. Wen, M. Zitnik, and P. Zhou, \"Moerated: Tuning new experts for modality and task extension,\" _arXiv preprint arXiv:2408.03511_, 2024. * [275] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, \"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,\" _Transactions of the Association for Computational Linguistics_, vol. 2, pp. 67-78, 2014. * [276] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Alterschmidt, S. Altman, S. Andakat _et al._, \"Gpt-4 technical report,\" _arXiv preprint arXiv:2303.08774_, 2023. * [277] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen _et al._, \"Palm 2 technical report,\" _arXiv preprint arXiv:2305.10403_, 2023. * [278] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, \"Qwen-vl: A frontier large vision-language model with versatile abilities,\" _arXiv preprint arXiv:2308.12966_, 2023. * [279] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollar, and C. L. Zitnick, \"Microsoft coco captions: Data collection and evaluation server,\" _arXiv preprint arXiv:1504.00325_, 2015. * [280] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu _et al._, \"Interrvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks,\" in _CVPR_, 2024, pp. 2185-24198. * [281] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang _et al._, \"Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal l1ms in video analysis,\" _arXiv preprint arXiv:2405.21075_, 2024. * [282] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, \"Making the v in vqa matter: Cleaving the role of image understanding in visual question answering,\" in _CVPR_, 2017, pp. 6904-6913. * [283] D. Gurari, Q. Li, A. J. Stang, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham, \"Vizwiz grand challenge: Answering visual questions from blind people,\" in _CVPR_, 2018, pp. 3608-3617. * [284] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee, \"Llava-next: Improved reasoning, ccr, and world knowledge,\" 2024. * [285] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu _et al._, \"Mmbench: Is your multi-modal model an all-around player?\" in _ECCV_. Springer, 2025, pp. 216-233. * [286] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, and R. Ji, \"Cheap and quick: Efficient vision-language instruction tuning for large language models,\" in _NeurIPS_, vol. 36, 2024. * [287] Z. Yang, L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, and L. Wang, \"The dawn of l1ms: Preliminary explorations with gpt-4v (sion),\" _arXiv preprint arXiv:2309.17421_, vol. 9, no. 1, p. 1, 2023. * [288] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican _et al._, \"Gemini: a family of highly capable multimodal models,\" _arXiv preprint arXiv:2312.11805_, 2023. * [289] I. Team, \"Interlmm: A multilingual language model with progressively enhanced capabilities,\" 2023. * [290] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozice, N. Goyal, E. Hambro, F. Azhar _et al._, \"Llama: Open and efficient foundation language models,\" _arXiv preprint arXiv:2302.13971_, 2023. * [291] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song _et al._, \"Cogylm: Visual expert for pretrained language models,\" _arXiv preprint arXiv:2311.03079_, 2023. * [292] Q. Wu, W. Yu, Y. Zhou, S. Huang, X. Sun, and R. Ji, \"Parameter and computation efficient transfer learning for vision-language pre-trained models,\" _NeurIPS_, vol. 36, 2024. * [293] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun _et al._, \"Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi,\" in _CVPR_, 2024, pp. 9556-9567. * [294] B. Lester, R. Al-Rfou, and N. Constant, \"The power of scale for parameter-efficient prompt tuning,\" _arXiv preprint arXiv:2104.08691_, 2021. * [295] A. Villa, J. L. Alcazar, M. Alfarra, K. Alhamoud, J. Hurtado, F. C. Heilbron, A. Soto, and B. Ghanem, \"Pivot: Prompting for video continual learning,\" in _CVPR_, 2023, pp. 24 214-24 223. * [296] Z. Gao, X. Zhang, K. Xu, X. Mao, and H. Wang, \"Stabilizing zero-shot prediction: A novel middle to forgetting in continual vision-language tasks,\" in _The Thirty-eighth Annual Conference on Neural Information Processing Systems_. * [297] W. Yan, Y. Wang, W. Lin, Z. Guo, Z. Zhao, and T. Jin, \"Low-rank prompt interaction for continual vision-language retrieval,\" in _ACM MM_, 2024, pp. 8257-8266. * [298] D. Zhu, Z. Sun, Z. Li, T. Shen, K. Yan, S. Ding, K. Kuang, and C. Wu, \"Model tailor: Mitigating catastrophic forgetting in multimodal large language models,\" _arXiv preprint arXiv:2402.12048_, 2024. * [299] J. Qiao, Z. Zhang, X. Tan, Y. Qu, S. Ding, and Y. Xie, \"Llaca: Multimodal large language continual assistant,\" _arXiv preprint arXiv:2410.10868_, 2024. * [300] C. Rebillard, J. Hurtado, A. Krutsylo, L. Passaro, and V. Lomonaco, \"Continually learn to map visual concepts to large language models in resource-constrained environments,\" _arXiv preprint arXiv:2407.08279_, 2024. * [301] M. Mistretta and A. D. Bagdanov, \"Re-tune: Incremental fine tuning of biomedical vision-language models for multi-label chest x-ray classification,\" _arXiv preprint arXiv:2410.17827_, 2024. * [302] Y. Cai and M. Rostami, \"Clumo: Cluster-based modality fusion prompt for continual learning in visual question answering,\" _arXiv preprint arXiv:2408.11742_, 2024. * [303] J. Zheng, Q. Ma, Z. Liu, B. Wu, and H. Feng, \"Beyond anti-forgetting: Multimodal continual instruction tuning with positive forward transfer,\" _arXiv preprint arXiv:2401.09181_, 2024. * [304] M. D'Alessandro, A. Alonso, E. Calabres, and M. Galar, \"Multimodal parameter-efficient few-shot class incremental learning,\" in _ICCV_, 2023, pp. 3393-3403. * [305] Z. Zhang, Y. Sun, T. Zhao, L. Sha, R. Xu, K. Lee, and J. Yin, \"Preserving knowledge in large language model with model-agnostic self-decompression,\" _arXiv e-prints_, pp. arXiv-2406, 2024. * [306] K. Wang, N. Dimitrizadis, A. Favero, G. Ortiz-Jimenez, F. Fleuret, and P. Frossard, \"Lines: Post-training layer scaling prevents forgetting and enhances model merging,\" _arXiv preprint arXiv:2410.17146_, 2024. * [307] R. Wang, X. Duan, G. Kang, J. Liu, S. Lin, S. Xu, J. Lu, and B. Zhang, \"Attretilp: A non-incremental learner for incremental knowledge learning,\" in _CVPR_, 2023, pp. 3654-3663. * [308] J. S. Smith, Y.-C. Hsu, L. Zhang, T. Hua, Z. Kira, Y. Shen, and H. Jin, \"Continual diffusion: Continual customization of text-to-image diffusion with c-lora,\" _arXiv preprint arXiv:2304.06027_, 2023. * [309] Y. Ma, Y. He, W. Zhong, X. Wang, R. Zimmermann, and T.-S. Chua, \"Cirp: Cross-item relational pre-training for multimodal product bundling,\" in _ACM MM_, 2024, pp. 9641-9649. * [310] Z. Zhang, F. Qi, and C. Xu, \"Enhancing storage and computational efficiency in federated multimodal learning for large-scale models,\" in _Forty-first ICML_. * [311] J. Parekh, P. Khayatan, M. Shukor, A. Newson, and M. Cord, \"A concept-based explainability framework for large multimodal models,\" _arXiv preprint arXiv:2406.08074_, 2024. * [312] Z. Liu, X. Wu, S. Wang, and J. Qian, \"Adaptively building a video-language model for video captioning and retrieval without massive video pretraining,\" in _ACM MM_, 2* [317] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen, \"Evaluating object hallucination in large vision-language models,\" _arXiv preprint arXiv:2305.10355_, 2023. * [318] Y. Zhang, Z. Ma, X. Gao, S. Shakahi, Q. Gao, and J. Chai, \"Ground-hog: Grounding large language models to holistic segmentation,\" in _CVPR_, 2024, pp. 14227-14238. * [319] B. Zhai, S. Yang, X. Zhao, C. Xu, S. Shen, D. Zhao, K. Keutzer, M. Li, T. Yan, and X. Fan, \"Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption,\" _arXiv preprint arXiv:2310.01779_, 2023. * [320] F. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang, \"Mitigating hallucination in large multi-modal models via robust instruction tuning,\" in _The Twelfth International Conference on Learning Representations_, 2023. * [321] H. You, H. Zhang, Z. Gan, X. Du, B. Zhang, Z. Wang, L. Cao, S.-F. Chang, and Y. Yang, \"Ferret: Refer and ground anything anywhere at any granularity,\" _arXiv preprint arXiv:2310.07704_, 2023. * [322] Y. Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn, M. Bansal, and H. Yao, \"Analyzing and mitigating object hallucination in large vision-language models,\" _arXiv preprint arXiv:2310.00754_, 2023. * [323] J. Wang, Y. Wang, G. Xu, J. Zhang, Y. Gu, H. Jia, M. Yan, J. Zhang, and J. Sang, \"An llm-free multi-dimensional benchmark for mllms hallucination evaluation,\" _arXiv preprint arXiv:2311.07397_, 2023. * [324] X. Chen, Z. Ma, X. Zhang, S. Xu, S. Qian, J. Yang, D. F. Fouhey, and J. Chai, \"Multi-object hallucination in vision-language models,\" _arXiv preprint arXiv:2407.06192_, 2024. * [325] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang _et al._, \"Yi: Open foundation models by 01. ai,\" _arXiv preprint arXiv:2403.04652_, 2024. * [326] H. Rasheed, M. Maaz, S. Shaji, A. Shaker, S. Khan, H. Cholakkal, R. M. Anwer, E. Xing, M.-H. Yang, and F. S. Khan, \"Glamm: Pixel grounding large multimodal model,\" in _CVPR_, 2024, pp. 13 009-13 018. * [327] H. Laurenon, E. Saulnier, L. Tonchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karamcheti, A. Rush, D. Kicla _et al._, \"Obelics: An open web-scale filtered dataset of interleaved image-text documents,\" in _NeurIPS_, vol. 36, 2024. * [328] S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng, Y. Fang, Y. Huang, W. Zhao _et al._, \"Minipcm: Unveiling the potential of small language models with scalable training strategies,\" _arXiv preprint arXiv:2404.06395_, 2024. * [329] A. Hurst, A. Lerner, A. P. Coucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford _et al._, \"Gpt-4o system card,\" _arXiv preprint arXiv:2410.21276_, 2024. * [330] Q. Huang, X. Dong, P. Zhang, B. Wang, C. He, J. Wang, D. Lin, W. Zhang, and N. Yu, \"Opera: Alleviting hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation,\" in _CVPR_, 2024, pp. 13 418-13 427. * [331] G. Chen, L. Hou, Y. Chen, W. Dai, L. Shang, X. Jiang, Q. Liu, J. Pan, and W. Wang, \"mclip: Multilingual clip via cross-lingual transfer,\" in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2023, pp. 13 028-13 043. * [332] G. Geigle, A. Jain, R. Timofte, and G. Glavas, \"mblip: Efficient bootstrapping of multilingual vision-llms,\" _arXiv preprint arXiv:2307.06930_, 2023. * [333] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, \"Instructblip: Towards general-purpose vision-language models with instruction tuning,\" 2023. [Online]. Available: [https://arxiv.org/abs/2305.06500](https://arxiv.org/abs/2305.06500) * [334] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang _et al._, \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,\" _arXiv preprint arXiv:2403.05530_, 2024. * [335] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg _et al._, \"Sparks of artificial general intelligence: Early experiments with gpt-4,\" _arXiv preprint arXiv:2303.12712_, 2023. * [336] L. Wachowiak and D. Gromann, \"Does gpt-3 grasp metaphors? identifying metaphor mappings with generative language models,\" in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2023, pp. 1018-1032. * [337] Z. Liu, F. Fang, X. Feng, X. Du, C. Zhang, Z. Wang, Y. Bai, Q. Zhao, L. Fan, C. Gan _et al._, \"li-bench: An image implication understanding benchmark for multimodal large language models,\" _arXiv preprint arXiv:2406.05862_, 2024. * [338] J. Li, D. Li, S. Savarese, and S. Hoi, \"Blip-2: Bootstrap-language pre-training with frozen image encoders and large language models,\" 2023. [Online]. Available: [https://arxiv.org/abs/2301.12597](https://arxiv.org/abs/2301.12597) * [339] Q. Ye, H. Xu, J. Ye, M. Yan, A. Hu, H. Liu, Q. Qian, J. Zhang, F. Huang, and J. Zhou, \"mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration,\" 2023. [Online]. Available: [https://arxiv.org/abs/2311.04257](https://arxiv.org/abs/2311.04257) * [340] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, H. Yang, Y. Sun, C. Deng, H. Xu, Z. Xie, and C. Ruan, \"Deepsevl-7 towards real-world vision-language understanding,\" 2024. [Online]. Available: [https://arxiv.org/abs/2403.05525](https://arxiv.org/abs/2403.05525) * [341] X. Dong, P. Zhang, Y. Zang, Y. Cao, B. Wang, L. Ouyang, X. Wei, S. Zhang, H. Duan, M. Cao, W. Zhang, Y. Li, H. Yan, Y. Gao, X. Zhang, W. Li, J. Li, K. Chen, C. He, X. Zhang, Y. Qiao, D. Lin, and J. Wang, \"Intermlm-acomposer2: Mastering free-form text-image composition and comprehension in vision-language large model,\" 2024. [Online]. Available: [https://arxiv.org/abs/2401.16420](https://arxiv.org/abs/2401.16420) * [342] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma _et al._, \"How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites,\" _arXiv preprint arXiv:2404.16821_, 2024. * [343] W. Hong, W. Wang, M. Ding, W. Yu, Q. Lv, Y. Wang, Y. Cheng, S. Huang, J. Ji, Z. Xue, L. Zhao, Z. Yang, X. Gu, X. Zhang, G. Feng, D. Yin, Z. Wang, J. Qi, X. Song, P. Zhang, D. Liu, B. Xu, J. Li, Y. Dong, and J. Tang, \"Cogvlm2: Visual language models for image and video understanding,\" 2024. [Online]. Available: [https://arxiv.org/abs/2408.16500](https://arxiv.org/abs/2408.16500) * [344] G. Team, \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,\" 2024. [Online]. Available: [https://arxiv.org/abs/2403.05530](https://arxiv.org/abs/2403.05530) * [345] X. L. Li, V. Shrivastava, S. Li, T. Hashimoto, and P. Liang, \"Benchmarking and improving generator-validator consistency of language models,\" _arXiv preprint arXiv:2310.01846_, 2023. * [346] Z. Lin, S. Trivedi, and J. Sun, \"Generating with confidence: Uncertainty quantification for black-box large language models,\" _arXiv preprint arXiv:2305.19187_, 2023. * [347] Y. Zhang, F. Xiao, T. Huang, C.-K. Fan, H. Dong, J. Li, J. Wang, K. Cheng, S. Zhang, and H. Guo, \"Unveiling the tapestry of consistency in large vision-language models,\" _arXiv preprint arXiv:2405.11456_, 2024. * [348] G. Team, \"Gemini: A family of highly capable multimodal models,\" 2024. [Online]. Available: [https://arxiv.org/abs/2312.11805](https://arxiv.org/abs/2312.11805) * [349] Y. Li, Y. Zhang, C. Wang, Z. Zhong, Y. Chen, R. Chu, S. Liu, and J. Jia, \"Mini-gemini: Mining the potential of multimodality vision language models,\" 2024. [Online]. Available: [https://arxiv.org/abs/2403.18814](https://arxiv.org/abs/2403.18814) * [350] F. Li, R. Zhang, H. Zhang, Y. Zhang, B. Li, W. Li, Z. Ma, and C. Li, \"Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models,\" 2024. [Online]. Available: [https://arxiv.org/abs/2407.07895](https://arxiv.org/abs/2407.07895) * [351] D. A. Hudson and C. D. Manning, \"Gqa: A new dataset for real-world visual reasoning and compositional question answering,\" in _CVPR_, 2019, pp. 6700-6709. * [352] P. Liu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Taford, P. Clark, and A. Kalyan, \"Learn to explain: Multimodal reasoning via thought chains for science question answering,\" _in NeurIPS_, vol. 35, pp. 2507-2521, 2022. * [353] J. Kil, Z. Mai, J. Lee, Z. Wang, K. Cheng, L. Wang, Y. Liu, A. Chowdhury, and W.-L. Chao, \"Compbench: A comparative reasoning benchmark for multimodal lms,\" _arXiv preprint arXiv:2407.16837_, 2024. language model as a unified interface for vision-language multi-task learning,\" _arXiv preprint arXiv:2310.09478_, 2023. * [39] H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Qi, Qian, W. Wang _et al._, \"mplug-2: A modularized multi-modal foundation model across text, image and video,\" in _ICML_. PMLR, 2023, pp. 38728-38748. * [40] Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen, Z. Chen, Z. Chen, P. Chu _et al._, \"Internlm2 technical report,\" _arXiv preprint arXiv:2403.17297_, 2024. * [41] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang, \"Glm: General language model pretraining with autoregressive blank infilling,\" in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 320-335. * [42] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi _et al._, \"mplug-owl: Modularization empowers large language models with multimodality,\" _arXiv preprint arXiv:2304.14178_, 2023. * [43] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao, \"Shixe: Unleashing multimodal llm's reretinal dialogue magic,\" _arXiv preprint arXiv:2306.15195_, 2023. * [44] Z. Li, Y. Wang, M. Du, Q. Liu, B. Wu, J. Zhang, C. Zhou, Z. Fan, J. Fu, J. Chen _et al._, \"Reform-eval: Evaluating large vision language models via unified re-formulation of task-oriented benchmarks,\" in _ACM MM_, 2024, pp. 1971-1980. * [45] J. Li, D. Li, S. Savarese, and S. Hoi, \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,\" in _ICML_. PMLR, 2023, pp. 19730-19742. * [46] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, \"Instructblip: Towards general-purpose vision-language models with instruction tuning,\" 2023. [Online]. Available: [https://arxiv.org/abs/2305.06500](https://arxiv.org/abs/2305.06500) * [47] Y. Su, T. Lan, H. Li, J. Xu, Y. Wang, and D. Cai, \"Pandagrp: One model to instruction-follow them all,\" _arXiv preprint arXiv:2305.16355_, 2023. * [48] J. Han, R. Zhang, W. Shao, P. Gao, P. Xu, H. Xiao, K. Zhang, C. Liu, S. Wen, Z. Guo _et al._, \"Imagebird-mlml: Multi-modality instruction tuning,\" _arXiv preprint arXiv:2309.03905_, 2023. * [49] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu, C. He, X. Yue _et al._, \"Llama-adapter v2: Parameter-efficient visual instruction model,\" _arXiv preprint arXiv:2304.15010_, 2023. * [50] T. Gong, C. Lyu, S. Zhang, Y. Wang, M. Zheng, Q. Zhao, K. Liu, W. Zhang, P. Luo, and K. Chen, \"Multimodal-gpt: A vision and language model for dialogue with humans,\" _arXiv preprint arXiv:2305.04790_, 2023. * [51] Y. Zeng, H. Zhang, J. Zheng, J. Xia, G. Wei, Y. Wei, Y. Zhang, T. Kong, and R. Song, \"What matters in training a gpt4-style language model with multimodal inputs?\" in _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_, 2024, pp. 7930-7957. * [52] J. Li, K. Pan, Z. Ge, M. Gao, H. Zhang, W. Ji, W. Zhang, T.-S. Chua, S. Tang, and Y. Zhuang, \"Empowering vision-language models to follow interleaved vision-language instructions,\" _arXiv preprint arXiv:2308.04152_, 2023. * [53] W. Hu, Y. Xu, Y. Li, W. Li, Z. Chen, and Z. Tu, \"Bliva: A simple multimodal llm for better handling of text-rich visual questions,\" in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 38, no. 3, 2024, pp. 2256-2264. * [54] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang, \"Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct,\" _arXiv preprint arXiv:2308.09583_, 2023. * [55] H. Wang, S. Feng, T. He, Z. Tan, X. Han, and Y. Tsvetkov, \"Can language models solve graph problems in natural language?\" _in NeurIPS_, vol. 36, 2024. * [56] Y. Li, B. Hu, H. Shi, W. Wang, L. Wang, and M. Zhang, \"Vision-graph: Leveraging large multimodal models for graph theory problems in visual context,\" _arXiv preprint arXiv:2405.04950_, 2024. * [57] Z. Lin, C. Liu, R. Zhang, P. Gao, L. Qiu, H. Xiao, H. Qiu, C. Lin, W. Shao, K. Chen _et al._, \"Spihmc: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models,\" _arXiv preprint arXiv:2311.07575_, 2023. * [58] Z. Cheng, Z.-Q. Cheng, J.-Y. He, J. Sun, K. Wang, Y. Lin, Z. Lian, X. Peng, and A. Hauptmann, \"Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning,\" _arXiv preprint arXiv:2406.11161_, 2024. * [59] X. Wu, S. Huang, G. Wang, J. Xiong, and F. Wei, \"Multimodal large language models make text-to-image generative models align better,\" in _The Thirty-eighth Annual Conference on Neural Information Processing Systems_. * [60] Z. Wang, A. Li, Z. Li, and X. Liu, \"Genartist: Multimodal llm as an agent for unified image generation and editing,\" _arXiv preprint arXiv:2407.05060_, 2024. * [61] Y. Hu, W. Shi, X. Fu, D. Roth, M. Ostendorf, L. Zettlemoyer, N. A. Smith, and R. Krishna, \"Visual sketchpad: Sketching as a visual chain of thought for multimodal language models,\" _arXiv preprint arXiv:2406.09403_, 2024. * [62] H. Chen, W. Li, J. Gu, J. Ren, S. Chen, T. Ye, R. Pei, K. Zhou, F. Song, and L. Zhu, \"Restoreagent: Autonomous image restoration agent via multimodal large language models,\" _arXiv preprint arXiv:2407.18035_, 2024. * [63] H. Chen, H. Huang, J. Dong, M. Zheng, and D. Shao, \"Finecliper: Multi-modal fine-grained clip for dynamic facial expression recognition with adapters,\" in _ACM MM_, 2024, pp. 2301-2310. * [64] S. Ma, Y. Zhang, Q. Zhang, Y. Chen, H. Wang, and Z. Jia, \"Sleepmg: Multimodal generalizable sleep staging with inter-modal balance of classification and domain discrimination,\" in _ACM MM_, 2024, pp. 4004-4013. * [65] Y. Xie, Z. Zhu, X. Chen, Z. Chen, and Z. Huang, \"Moba: Mixture of bi-directional adapter for multi-modal sarcasm detection,\" in _ACM MM_, 2024, pp. 4264-4272. * [66] P. Guo, W. Li, H. Huang, L. Hong, X. Zhou, Z. Chen, J. Li, K. Jiang, W. Zhang, and W. Zhang, \"X-prompt: Multi-modal visual prompt for video object segmentation,\" in _ACM MM_, 2024, pp. 5151-5160. * [67] D. Wu, D. Yang, Y. Zhou, and C. Ma, \"Robust multimodal sentiment analysis of image-text pairs by distribution-based feature recovery and fusion,\" in _ACM MM_, 2024, pp. 5780-5789. * [68] L. Tang, P.-T. Jiang, Z.-H. Shen, H. Zhang, J.-W. Chen, and B. Li, \"Chain of visual perception: Harnessing multimodal large language models for zero-shot camouflaged object detection,\" in _ACM MM_, 2024, pp. 8805-8814. * [69] D. Zhao, D. Han, Y. Yuan, B. Ning, M. Li, Z. He, and S. Song, \"Autograph: Enabling visual context via graph alignment in open domain multi-modal dialogue generation,\" in _ACM MM_, 2024, pp. 2079-2088. * [70] K. Liu, F. Zhao, Y. Yang, and G. Xu, \"Dysarl: Dynamic structure-aware representation learning for multimodal knowledge graph reasoning,\" in _ACM MM_, 2024, pp. 8247-8256. * [71] B. Zhao, T. Cheng, Y. Zhang, Y. Cheng, R. Feng, and X. Zhang, \"Ct2c-qa: Multimodal question answering over chinese text, table and chart,\" in _ACM MM_, 2024, pp. 3897-3906. * [72] L. Xiao, X. Yang, F. Peng, Y. Wang, and C. Xu, \"Hive: Hierarchical multimodal fine-grained modulation for visual grounding,\" in _ACM MM_, 2024, pp. 5460-5469. * [73] R. Wang, X. Ma, H. Zhou, C. Ji, G. Ye, and Y.-G. Jiang, \"White-box multimodal siplbreaks against large vision-language models,\" in _ACM MM_, 2024, pp. 6920-6928. * [74] F. Lu, W. Wang, Y. Luo, Z. Zhu, Q. Sun, B. Xu, H. Shi, S. Gao, Q. Li, Y. Song _et al._, \"Mixo: multimodal intention knowledge distillation from large language models for social-media commonsense discovery,\" in _ACM MM_, 2024, pp. 3303-3312. * [75] P. Fu, X. Liang, Y. Qian, Q. Guo, Z. Wei, and W. Li, \"Como-nas: Core-structures-guided multi-objective neural architecture search for multi-modal classification,\" in _ACM MM_, 2024, pp. 9126-9135. * [76] J. Zhao, J. Wang, Y. Jin, J. Luo, and G. Zhou, \"Hawkeye: Discovering and grounding implicit anomalous sentiment in recon-videos via scene-enhanced video large language model,\" in _ACM MM_, 2024, pp. 592-601. * [77] X. Zhang, H. Wen, J. Wu, P. Qin, H. Xue, and L. Nie, \"Differential-perceptive and retrieval-augmented mllm for change captioning,\" in _ACM MM_, 2024, pp. 4148-4157. * [78] Z. Wang, L. Wang, Z. Zhao, M. Wu, C. Lyu, H. Li, D. Cai, L. Zhou, S. Shi, and Z. Tu, \"Gpt4vid: A unified multimodal large language model for instruction-followed understanding and safety-aware generation,\" in _ACM MM_, 2024, pp. 3907-3916. * [79] X * [400] D. Kondratyuk, L. Yu, X. Gu, J. Lezama, J. Huang, G. Schindler, R. Hornung, V. Birodkar, J. Yan, M.-C. Chiu _et al._, \"Videoopect: A large language model for zero-shot video generation,\" _arXiv preprint arXiv:2312.14125_, 2023. * [401] Y. Zong, O. Bohdal, T. Yu, Y. Yang, and T. Hospedales, \"Safety fine-tuning at (almost) no cost: A baseline for vision large language models,\" _arXiv preprint arXiv:2104.02207_, 2024. * [402] Y. Jin, Z. Sun, K. Xu, L. Chen, H. Jiang, Q. Huang, C. Song, Y. Liu, D. Zhang, Y. Song _et al._, \"Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization,\" _arXiv preprint arXiv:2402.03161_, 2024. * [403] L. Qian, J. Li, Y. Wu, Y. Ye, H. Fei, T.-S. Chua, Y. Zhuang, and S. Tang, \"Momentor: Advancing video large language model with fine-grained temporal reasoning,\" _arXiv preprint arXiv:2402.11435_, 2024. * [404] Z. Zheng, P. Peng, Z. Ma, X. Chen, E. Choi, and D. Harwath, \"Bat Learning to reason about spatial sounds with large language models,\" _arXiv preprint arXiv:2402.01591_, 2024. * [405] G. Sun, W. Yu, C. Tang, X. Chen, T. Tan, W. Li, L. Lu, Z. Ma, Y. Wang, and C. Zhang, \"video-salmonn: Speech-enhanced audio-visual large language models,\" _arXiv preprint arXiv:2406.15704_, 2024. * [406] L. Li, Y. Ye, B. Jiang, and W. Zeng, \"Georeasoner: Geo-localization with reasoning in street views using a large vision-language model,\" in _Forty-first ICML_. * [407] Y. Li, Z. Li, Q. Zeng, Q. Hou, and M.-M. Cheng, \"Cascade-clip: Cascade vision-language embeddings alignment for zero-shot semantic segmentation,\" _arXiv preprint arXiv:2406.00670_, 2024. * [408] M. Ding, K. Ji, D. Wang, and J. Xu, \"Understanding forgetting in continual learning with linear regression,\" _arXiv preprint arXiv:2405.17583_, 2024. * [409] L. Zhao, X. Zhang, K. Yan, S. Ding, and W. Huang, \"Safe: Slow and fast parameter-efficient tuning for continual learning with pre-trained models,\" _arXiv preprint arXiv:2411.02175_, 2024. * [410] A. Bian, W. Li, H. Yuan, C. Yu, Z. Zhao, M. Wang, A. Lu, and T. Feng, \"Make robust continual learning stronger via c-flat,\" _arXiv preprint arXiv:2404.00986_, 2024. * [411] Y. Lu, S. Zhang, D. Cheng, Y. Xing, N. Wang, P. Wang, and Y. Zhang, \"Visual prompt tuning in null space for continual learning,\" _arXiv preprint arXiv:2406.05683_, 2024. * [412] Z. Zhang, X. Wang, Y. Qin, H. Chen, Z. Zhang, X. Chu, and W. Zhu, \"Disentangled continual graph neural architecture search with invariant modular supernet,\" in _Forty-first ICML_. * [413] J. Thapa and R. Li, \"Bayesian adaptation of network depth and width for continual learning,\" in _Forty-first ICML_. * [414] N. Michel, M. Wang, L. Xiao, and T. Yamasaki, \"Rethinking momentum knowledge distillation in online continual learning,\" _arXiv preprint arXiv:2309.02870_, 2023. * [415] W. Lin, J. Chen, R. Huang, and H. Ding, \"An effective dynamic gradient calibration method for continual learning,\" _arXiv preprint arXiv:2407.20956_, 2024. * [416] Z. Zhu, Y. Gong, and D. Hoiem, \"Anytime continual learning for open vocabulary classification,\" in _ECCV_. Springer, 2025, pp. 269-285. * [417] C. Niu, G. Pang, L. Chen, and B. Liu, \"Replay-and-forget-free graph class-incremental learning: A task profiling and prompting approach,\" _arXiv preprint arXiv:2410.10341_, 2024. * [418] Z. Huang, Z. Chen, Y. Li, B. Dong, E. Zhou, Y. Liu, R. S. M. Goh, C.-M. Feng, and W. Zuo, \"Class balance matters to active class-incremental learning,\" in _ACM MM_, 2024, pp. 9445-9454. * [419] S. Wang, C. Li, J. Tang, X. Gong, Y. Yuan, and G. Wang, \"Importance-aware shared parameter subspace learning for domain incremental learning,\" in _ACM MM_, 2024, pp. 8874-8883. * [420] Y.-C. Yu, C.-P. Huang, J.-J. Chen, K.-P. Chang, Y.-H. Lai, F.-E. Yang, and Y.-C. F. Wang, \"Select and distill: Selective dual-teacher knowledge transfer for continual learning on vision-language models,\" in _ECCV_. Springer, 2025, pp. 219-236. * [421] J. Yu, Y. Zhuge, L. Zhang, P. Hu, D. Wang, H. Lu, and Y. He, \"Boosting continual learning of vision-language models via mixture-of-experts adapters,\" in _CVPR_, 2024, pp. 23 219-23 230. * [422] T. Jin, W. Yan, Y. Wang, S. Cai, Q. Shuai, and Z. Zhao, \"Calibrating prompt from history for continual vision-language retrieval and grounding,\" in _ACM MM_, 2024, pp. 4302-4311. * [423] M. Menabue, E. Frascarodi, M. Boschini, E. Sangineto, L. Bonicelli, A. Porrello, and S. Calderara, \"Semantic residual prompts for continual learning,\" in _ECCV_. Springer, 2025, pp. 1-18. * [424] L. Huang, X. Cao, H. Lu, and X. Liu, \"Class-incremental learning with clip: Adaptive representation adjustment and parameter fusion,\" in _ECCV_. Springer, 2025, pp. 214-231. * [425] S. Ni, D. Chen, C. Li, X. Hu, R. Xu, and M. Yang, \"Forgetting before learning: Utilizing parametric arithmetic for knowledge updating in large language models,\" _arXiv preprint arXiv:2311.08011_, 2023. * [426] J. Zheng, S. Qiu, and Q. Ma, \"Learn or recall? revisiting incremental learning with pre-trained language models,\" _arXiv preprint arXiv:2312.07887_, 2023. * [427] T. Sciamion, T. Chakrabarty, and S. Muresan, \"Fine-tuned language models are continual learners,\" _arXiv preprint arXiv:2205.12333_, 2022. * [428] Y. Zhang, Y. Wang, F. Cheng, S. Kurohashi _et al._, \"Reformulating domain adaptation of large language models as adapt-retrieve-review,\" _arXiv preprint arXiv:2310.03328_, 2023. * [429] G. Dong, H. Yuan, K. Lu, C. Li, M. Xue, D. Liu, W. Wang, Z. Yuan, C. Zhou, and J. Zhou, \"How abilities in large language models are affected by supervised fine-tuning data composition,\" _arXiv preprint arXiv:2310.05492_, 2023. * [430] G. Yigit and M. F. Amasayali, \"Enhancing multiple-choice question answering through sequential fine-tuning and curriculum learning strategies,\" _Knowledge and Information Systems_, vol. 65, no. 11, pp. 5025-5042, 2023. * [431] A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato, \"On tiny episodic memories in continual learning,\" _arXiv preprint arXiv:1902.10486_, 2019. * [432] J. S. Smith, L. Karlinsky, V. Gutta, P. Cascante-Bonilla, D. Kim, A. Arbelle, R. Panda, R. Feris, and Z. Kira, \"Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning,\" in _CVPR_, 2023, pp. 11 908-11919. * [433] Z. Wang, Z. Zhang, S. Ebrahimi, R. Sun, H. Zhang, C.-Y. Lee, X. Ren, G. Su, V. Perot, J. Dy _et al._, \"Dualprompt: Complementary prompting for rehearsal-free continual learning,\" in _ECCV_. Springer, 2022, pp. 631-648. * [434] L. Bonicelli, M. Boschini, A. Porrello, C. Spampinato, and S. Calderara, \"On the effectiveness of lipschitz-driven rehearsal in continual learning,\" in _NeurIPS_, vol. 35, pp. 31886-31 901, 2022. * [435] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars, \"Memory aware synapses: Learning what (not) to forget,\" in _ECCV_, 2018, pp. 139-154. * [436] Y. Wu, Y. Chen, L. Wang, Y. Ye, Z. Liu, Y. Guo, and Y. Fu, \"Large scale incremental learning,\" in _CVPR_, 2019, pp. 374-382. * [437] H. Chen and G. Ding, \"Quantized prompt for efficient generalization of vision-language models,\" 2024. * [438] S. Srivastava, M. Y. Harun, R. Shrestha, and C. Kanan, \"Improving multimodal large language models using continual learning,\" _arXiv preprint arXiv:2410.15295_, 2024. * [439] Y. Guo, J. Fu, H. Zhang, D. Zhao, and Y. Shen, \"Efficient continual pre-training by mitigating the stability gap,\" _arXiv preprint arXiv:2406.14833_, 2024. * [44] J. He, H. Guo, M. Tang, and J. Wang, \"Continual instruction tuning for large multimodal models,\" _arXiv preprint arXiv:2311.16206_, 2023. * [45] Y. Cai and M. Rostami, \"Dynamic transformer architecture for continual learning of multimodal tasks,\" _arXiv preprint arXiv:2401.15275_, 2024."
    },
    {
      "title": "7 Multimodal Large Language Model",
      "text": ""
    },
    {
      "title": "_Model Innovation_",
      "text": ""
    },
    {
      "title": "7.1.1 Framework Innovation",
      "text": "Chaoya Jiang et al. [101] introduced the multi-granularity hybrid visual encoding framework MaVEn, which combines discrete visual symbol sequences representing abstract, coarse-grained semantic concepts with traditional continuous representation sequences that simulate fine-grained features. This combination enhances the model's ability to understand visual information in images. Zhuofan Zong et al. [102] proposed the MoVA framework, which incorporates coarse-grained context-aware expert routing and fine-grained expert fusion. This framework adaptively routes and fuses visual experts for specific tasks through a coarse-to-fine mechanism, thereby mitigating the bias of the CLIP visual encoder and enhancing the model's ability to understand and process diverse image content. Leyang Shen et al. [103] proposed a multimodal expert mixing framework, MoME, which combines the visual expert mixture model (MoVE) and the language expert mixture model (MoLE) to reduce task interference. Byung-Kwan Lee et al. [104] proposed the Meteor model, based on the Mamba architecture, which enhances the comprehension and response capabilities of large language and vision models through multifaceted reasoning. Hao Ma et al. [105] proposed the sequential cooperative multi-agent reinforcement learning framework, CORY, which enhances the stability and performance of multimodal large models in reinforcement learning fine-tuning by leveraging the inherent collaborative evolution and emergent capabilities of multi-agent systems. Yang Jiao et al. [106] proposed a vision-centric multimodal large model framework, Lumen, which strengthens multimodal content understanding by decoupling task-agnostic and task-specific learning. This framework enables flexible adaptation to various vision tasks, enhancing the LMM's capabilities in visual perception and instruction following. Chuyang Zhao et al. [107] proposed the \"Parallel Recognition \\(\\rightarrow\\) Sequential Understanding\" MLLM framework, Octopus. This framework achieves parallel recognition of object queries at the lower LLM layers and passes the results to the top LLM layers for sequential understanding, thereby improving the efficiency and accuracy of MLLMs. Yikai Zhang et al. [108] proposed the Wings framework, which introduces additional modules and mechanisms to compensate for attention shifts. This allows the model to effectively process visual information while maintaining focus on textual information. Timin Gao et al. [109] proposed the Cantor framework, which integrates visual inputs with logical reasoning and leverages the advanced cognitive functions of MLLMs. By acting as a multifaceted expert, it directly acquires higher-level information, thereby improving decision-making quality. Daqin Luo et al. [110] proposed the AutoM3L framework, based on the AutoML architecture, which automates the construction of multimodal training pipelines, feature engineering, and model selection using LLMs, thereby reducing manual intervention. Yunfeng Fan et al. [111] proposed the DI-MML framework, which addresses modality competition in multimodal learning by independently training modality encoders. They introduced a shared classifier and DUC loss to facilitate cross-modal interaction and knowledge transfer, thereby mitigating the modality competition issue in multimodal learning. Xinwei Liu et al. [112] proposed the multi-step error minimization framework, MEM, which optimizes by combining image noise and text triggers. This approach misleads the model into learning shortcuts, thereby protecting data privacy. Jinxu Zhang et al. [113] proposed the CREAM framework, which integrates high-performance retrieval enhancement, multi-image and multimodal processing, and efficient instruction tuning. This effectively addresses the challenges in document-based VQA tasks. Li Zheng et al. [114] proposed the Adaptive Multimodal Data Augmentation framework, SLUDA, which generates fine-grained data, optimizes the utilization of unlabeled data, and employs adaptive selection strategies and dynamic threshold adjustments. This approach addresses the issues of insufficient labeled data and the underutilization of unlabeled data. Tao Wu et al. [115] proposed the SAM model, which enhances semantic associations between images by introducing a bidirectional semantic guidance mechanism. This improves the semantic alignment ability of multimodal instructions. Shichen Lu et al. [116] proposed the Tiny-Large collaborative training framework, CTVLMs, which leverages knowledge distillation and multimodal alignment to enable large models to transfer knowledge to smaller models. This approach achieves a dual improvement in both performance and efficiency. Minsu Kim et al. [117] proposed the Bloom framework, which uses bidirectional modality transformation and adaptive cross-modal fusion. It pertains a VSR (Visual Speech Recognition) model with visual and speech units and introduces a curriculum learning strategy to enhance training efficiency and multilingual recognition performance. Yunshan Ma et al. [90] proposed the CIRP framework, which uses a multimodal encoder and cross-item contrastive loss to learn individual item semantics and relationships. By introducing a relationship pruning module, this framework enhances the ability to align cross-modal information and capture cross-item relationships in cold-start items. Puyi Wang et al. [118] proposed the multimodal large model-assisted artificial intelligence-generated image quality assessment framework, MA-AGIQA. By combining multimodal models with traditional DNNs, and utilizing semantic information extraction and a mixture of experts (MoE) structure, the framework dynamically integrates quality perception features. This significantly improves the quality assessment performance of AGIs, particularly excelling in reducing the false-negative rate. Zhiqi Ge et al. [119] proposed a novel cognitive framework, WorldGPT, which includes memory offloading, knowledge retrieval, and a Context Reflector to enhance the model's performance in specific scenarios and long-term tasks. Haoning Wu et al. [120] proposed the ONEALIGN model, which unifies IQA, IAA, and VQA tasks, thereby enhancing the model's cross-task generalization ability. Zixin Zhang et al. [310] proposed the M2FEDSA framework, which combines segmentation learning and multimodal federated learning. By introducing dual-adaptive fine-tuning and dual knowledge transfer strategies, the framework improves both computational and storage efficiency, as well as performance, when deploying large-scale multimodal models in federated learning settings. Ruisi Cai et al. [121] proposed an elastic architecture called Flextron, which supports adaptive subnetwork selection. By using routers to choose different sub-models or subnetworks, Flextron addresses the deployment challenges of multimodal large models in resource-constrained environments. Shengqiong Wu et al. [122] proposed an end-to-end Any-to-Any multimodal large model framework, which achieves efficient cross-modal understanding and generation through lightweight alignment techniques and modality-switching instruction tuning."
    },
    {
      "title": "7.1.2 Method Innovation",
      "text": "Xiaotong Li et al. [123] proposed a comprehensive multimodal perception fusion method that integrates visual experts, thereby enhancing the visual perception capability of MLLMs. Jiaqing Zhang et al. [124] proposed a novel end-to-end algorithm for multimodal fusion detection, achieving high performance through a single training phase and simplifying the overall process. Junfeng Fang et al. [125] proposed a neuron attribution method tailored for MLLMs, called NAM. NAM reveals the modality-specific semantic knowledge learned by neurons in MLLMs and highlights certain neuron characteristics that collectively elucidate the internal workings of MLLMs. Jayneel Parekh et al. [311] proposed a concept extraction method based on dictionary learning to interpret the internal representations of large multimodal models. They innovatively defined multimodal concepts and validated their effectiveness in interpreting models and understanding test sample representations. Junho Kim et al. [126] proposed CODE, which utilizes self-generated descriptions as contrastive references to dynamically adjust the information flow, enhancing the coherence and informativeness of responses. This approach addresses the hallucination problem in MLLMs when generating visual content. Samvadeep Basu et al. [127] proposed the model editing algorithm MULTEDIT, which can correct errors and insert new information. They also introduced a multimodal causal tracking method, extending the research on information storage to other domains. Jinging Xie et al. [128] proposed the Quantized Scale Learning Method (QSAAW), which effectively reduces quantization errors, prevents overfitting, and improves model adaptability and efficiency by learning the group scale factors of quantized weights and employing a multimodal pretraining strategy. Yabing Wang et al. [129] proposed the MLLM-enhanced cross-lingual, cross-modal retrieval method LECCCR. This approach leverages MLLMs to generate visual descriptions, which are then aggregated into multi-view semantic slots to enhance the semantic richness of visual features. By incorporating English feature guidance, it improves the quality of cross-modal alignment. Zihao Liu et al. [312] proposed a visual perception adapter and fine-grained tri-modal contrastive learning method. By aligning tokens across modalities, they reduce semantic gaps, thereby improving the performance of multimodal video tasks. Weixiang Han et al. [130] proposed the ERL-MR strategy, which uses Euler transformations and multimodal constraint loss to transform inter-modal competition into cooperation, thereby achieving performance improvement. Qiang Wang et al. [313] proposed a bilateral adaptive cross-modal fusion prompt learning paradigm, Bloom, which achieves more flexible cross-modal interaction and alignment through bidirectional modal transformation and adaptive fusion functions. This significantly enhances the performance of CLIP on a variety of generalization tasks. Zongqian Wu et al. [131] proposed an adaptive multimodal prompt learning method, AMMPL, which effectively handles meaningless image patches and enhances the model's generalization ability through image prompts and cross-modal interaction learning. Minghe Gao et al. [314] proposed the Fact paradigm, which teaches MLLMs by generating Faithful, Concise, and Transferable multimodal rationales, enhancing the model's performance and reasoning ability across various visual tasks. Lincan Cai et al. [132] proposed the PaRe method, which enhances the stability and transferability of cross-modal fine-tuning by progressively generating intermediate modalities and replacing modality-agnostic fragments. Wei Li et al. [133] proposed the Multimodal Combination Learning (MCL) method, which strengthens the mapping between visual and language modalities. By leveraging LLMs to automatically generate multimodal learning samples, they introduced a stacked retrieval mechanism to extract diverse multimodal information. Christian Schlarmann et al. [134] proposed the FARE unsupervised adversarial fine-tuning scheme, which enhances the robustness of the CLIP model while preserving its original performance, without the need for retraining on downstream tasks. Zhuo Huang et al. [135] proposed the DICL strategy, which leverages MLLM knowledge to enhance the robustness of visual models and align MLLMs with visual tasks. This approach enables unsupervised fine-tuning, improving performance in out-of-distribution (OOD) scenarios. Rumpeng Yu et al. [136] proposed the API technique, which enhances model perception through attention heatmaps guided by text queries. This approach enables model self-reflection and integration, improving performance on visual-linguistic tasks and addressing the limitations of traditional visual prompting techniques. Kai Huang et al. [137] proposed the Instruction-guided Visual Token Pruning method (IVTP), which includes an intra-group Token Pruning (GTP) module and cross-modal instruction-guided pruning. This approach effectively reduces the number of visual tokens and lowers computational complexity, while maintaining model performance."
    },
    {
      "title": "7.1.3 Module Innovation",
      "text": "Wenfang Yao et al. [138] proposed a novel reflection-based prompt optimization module, leveraging multimodal large language models to generate high-quality language descriptions to improve tracking performance. By iteratively refining the vague and inaccurate descriptions of targets through tracking feedback, this approach addresses the issue of frequent ambiguous language descriptions in annotations. Zaijing Li et al. [139] proposed a hybrid multimodal memory module that transforms knowledge into a hierarchical directed knowledge graph, enabling agents to explicitly represent and learn world knowledge. Additionally, historical information is summarized into an abstract multimodal experience pool, providing agents with rich contextual learning references. This approach addresses the challenge of general agents struggling to complete long-term tasks in open-world environments. Jaachen Li et al. [140] enhanced model capabilities by integrating sparse gated Top-K MoE (Mixture-of-Experts) blocks in the visual encoder and MLP connectors, and by introducing MoE blocks during the visual instruction fine-tuning phase. This approach improves the performance of MLLMs on multimodal tasks. Haogeng Liu et al. [141] innovatively identified visual anchors and proposed a novel vision-language connector, AcFormer. By utilizing visual anchors to aggregate information, this approach significantly enhances the accuracy and computational efficiency of MLLMs. Ziyuan Huang et al. [142] proposed the Chain-of-Sight module, which captures visual details at different spatial scales through a multi-scale visual resampler. This module enables flexible expansion of the number of visual tokens after retraining, accelerating the pretraining process while maintaining or improving model performance. Huanjin Yao et al. [143] proposed a new connector, the Dense Connector, which enhances the visual perception ability of MLLMs by integrating multi-layer visual features. It is characterized by high computational efficiency and ease of integration, addressing the issue of existing MLLMs underutilizing the visual encoder while overly emphasizing the language modality. Haibo Wang et al. [144] designed the Gaussian Contrastive Localization (GCG) module, which learns to represent the temporal structure of videos and selects key frames relevant to the question. This approach addresses the issue in video question answering where large multimodal models neglect question-related visual cues and lack key timestamp annotations. Hanzi Wang et al. [145] proposed a query-based hybrid expert connector, Q-MoE, which utilizes text-driven routing and an optimal expert path training strategy to achieve precise extraction and processing of task-specific visual information. This approach addresses the issue in MLLMs where the connection structure struggles to filter visual information according to task requirements in vision-language tasks."
    },
    {
      "title": "_Benchmarks_",
      "text": ""
    },
    {
      "title": "7.2.1 Rope: Recognition-Based Object Probing Evaluation Benchmark",
      "text": "Despite the impressive performance of MLLMs in various downstream applications, they often encounter the issue of object hallucination [315, 316, 317, 318, 319, 320, 321, 322, 323], where the model erroneously generates objects that do not exist in the image. Current benchmarks for evaluating object hallucination mainly focus on the presence of a single object category, rather than individual entities. Xuwievi Chen et al. [324] conducted a systematic study of the multi-object hallucination problem, examining how models misidentify objects when attending to multiple objects simultaneously (e.g., inventing non-existent objects or being distracted). They introduced an automated evaluation protocol called Recognition-based Object Probing Evaluation (ROPE), which considers the distribution of object categories within a single image during testing. By using visual reference to disambiguate, the protocol systematically analyzes multi-object hallucination, revealing the hallucination behaviors and influencing factors when models process multiple objects. In addition, ROPE designs multiple task prompts, including Default Multi-Object, Student-Forcing, Teacher-Forcing, and Single-Object. The dataset is divided into four subsets, each considering different object category distributions: 1) Homogeneous: All test objects belong to the same category. 2) Heterogeneous: All test objects belong to different categories. 3) In-the-Wild: A mixed object category distribution, with test objects randomly selected and ordered. 4) Adversarial: After multiple repetitions of the same category, a different category object is introduced. The dataset is further divided into Seen and Unseen based on whether the model has encountered these images during instruction tuning. More details of the overview of MLLM performance on the ROPE are provided in table IX."
    },
    {
      "title": "7.2.2 Cvqa: Culturally-Diverse Multilingual Visual Question Answering Benchmark",
      "text": "Visual Question Answering (VQA) is a crucial task in MLLMs, designed to test their understanding and reasoning capabilities across visual and textual data [32, 33, 34, 35, 36]. However, most existing VQA datasets primarily focus on English and a few major world languages, with images often being Western-centric. While recent efforts have expanded the linguistic coverage of VQA datasets, they still lack diversity in low-resource languages. Moreover, these datasets typically extend their language range through translation or other methods while keeping the images unchanged, leading to limited cultural representation. To address these limitations, David Romero et al. [323] developed a new benchmark, CVQA, which aims to encompass rich linguistic and cultural diversity. This benchmark involves native speakers and cultural experts in the data collection process to ensure authenticity and inclusivity. Figure 2 illustrates the scale and diversity of the CVQA benchmark, which includes 10,374 questions and languages from 30 different countries. This demonstrates how it covers a wide range of languages and cultures. Figure 3 shows the performance of different models across various country-language pairs, including question-option pairs in both English and local languages. The blue line in the figure represents performance separated by continents. Despite differences in scale, it highlights the similar behavior of all models in most cases. This figure reveals the challenges models face when handling questions in local languages, as well as the performance variations across different regions and languages. Table X shows the average performance of different MLLMs on the CVQA dataset using English prompts (EN) and local language prompts (LOC). These results indicate that even the best-performing open models, such as LLAVA-1.5-7B, significantly lag behind closed models on CVQA. Furthermore, their performance is poorer with local language prompts, highlighting the challenges models face when processing non-English prompts. Table X compares the performance of LLAVA-1.5-7B and InstructBLIP on CVQA and other established English VQA benchmarks. The results show that while LLAVA-1.5-7B performs better on other English VQA benchmarks, it still faces challenges on CVQA, highlighting the difficulty of culturally specific questions in CVQA. Table X presents the performance of models across 10 categories in CVQA. It shows that models achieve the highest accuracy in the \"People\" category, while the accuracy in the \"Food\" and \"Pop Culture\" categories is lower with local language prompts. This indicates that the diversity of food and pop culture across different cultures presents a challenge for the generalization of MLLMs."
    },
    {
      "title": "7.2.3 Ii-Bench: Image Implication Understanding Benchmark",
      "text": "Images often contain rich emotional and cultural narratives, and understanding their meaning and exploring the human emotions and cultural context they reflect requires attention to detail [335, 336, 276]. While MLLMs have made significant progress in understanding and generating cross-modal content, achieving new breakthroughs in benchmarks like image captioning [37, 38, 39, 40, 41, 42] and visual question answering [32, 33, 34, 35, 36], there has been insufficient exploration of their higher-order perceptual abilities. Ziqiang Liu et al. [337] introduced a new benchmark, II-Bench, designed to evaluate MLLMs' ability to understand and reason about the complex implicit meanings in images, addressing the gap in existing benchmarks for assessing higher-order perceptual abilities in MLLMs. II-Bench includes 1,222 images across six different domains: life, art, society, psychology, environment, and others. The images consist of various types, including illustrations, memes, posters, comics, logos, and paintings. Each image is accompanied by one to three multiple-choice questions, totaling 1,434 questions. Of these, 1,399 questions are used to construct the test set, and 35 questions are used for the development and validation sets. Table X presents the overall results of different MLLMs and human participants on the II-Bench benchmark. It shows model performance across various domains, such as life, art, society, psychology, and environment, as well as across different emotional categories (positive, neutral, and negative). The table lists the average and best accuracies for multiple open-source and closed-source MLLMs, alongside the performance of human participants."
    },
    {
      "title": "7.2.4 Conbench: Mllms Answer Consistency Evaluation Benchmark",
      "text": "MLLMs have made rapid progress in visual information perception and reasoning. Although MLLMs are capable of generating high-quality task prompt responses, simply modifying the prompt can lead to contradictory answers, even when the correct answer is provided. Specifically, under different prompt space sizes, these models lack consistency in answers to the same knowledge point, which significantly undermines trust in these models [345, 346]. To ensure that MLLMs can predict correct and consistent answers when faced with various query formats, Yuan Zhang et al. [347] proposed a multimodal benchmark tool, ConBench, designed to comprehensively assess the consistency of MLLMs--specifically, their ability to provide the same answer to the same knowledge point across different query formats. ConBench evaluates MLLMs by offering a diverse set of question formats, including true/false questions, multiple-choice questions, and limited visual question answering (VQA) problems. It also introduces two multidimensional evaluation metrics: 1)Discriminative Domain Evaluation Metric (ConScore[D]): Assesses consistency based on the accuracy of the model's answers to discriminative questions. 2)Generative Domain Evaluation Metric (ConScore[C]): Evaluates consistency by comparing the coherence between the model-generated captions and the discriminative answers. The specific structure of ConBench is shown in figure 4, providing an overview of the 19 evaluation categories in ConBench. These categories are distributed across three core capabilities: Sensation, Cognition, and Knowledge. The benchmark comprehensively covers tasks of varying difficulty levels, thereby assessing the performance of MLLMs across different aspects. Table XIV presents the performance evaluation results of different MLLMs on ConBench. These results are based on ConScore[D], which evaluates the correctness of the model's answers to discriminative questions. The table includes three \\begin{table} \\begin{tabular}{l c c c c c c c c c c c c c} \\hline \\hline **L1aVA-1.5-7B**[141] & **M-CLIP**[331] & **CLIP**[188] & **mBLIP-mTo**[332] & **mBLIP-BLOOMZ**[332] & **InstructBLIP**[333] & **Cemini-1.5-Flash**[334] & **CPT-4o**[329] \\\\ \\hline **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** \\\\ \\hline 49.6 & 35.5 & 88.0 & 33.7 & 42.7 & 30.6 & 31.3 & 30.9 & 39.3 & 32.7 & 49.0 & 31.9 & 66.9 & 68.5 & 75.4 & 74.3 \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE XIV: Average performance of MLLMs on our CVQA dataset with English prompts (EN) and local language prompts (LOC). [33] \\begin{table} \\begin{tabular}{l|c c c c|c c c|c c c|c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multicolumn{2}{c|}{Default Multi-Object} & \\multicolumn{4}{c|}{Student-Forcing} & \\multicolumn{4}{c|}{Teacher-Forcing} & \\multicolumn{4}{c}{Single-Object} \\\\ \\cline{2-13} & Wild & Hom. & Het. & Wild & Hom. & Het. & Wild & Hom. & Het. & Wild & Hom. & Het. \\\\ \\hline \\multicolumn{13}{c}{_Seen_} \\\\ \\hline Yi-VL-6B [325] & 2.95 & 5.65 & 1.99 & 3.44 & 6.80 & 3.78 & 5.45 & 26.25 & 4.36 & 0.19 & 0.30 & 0.13 \\\\ Yi-VL-34B [325] & 8.50 & 15.35 & 3.33 & 8.97 & 16.30 & 4.23 & 10.09 & 19.75 & 4.94 & 0.22 & 2.60 & 0.13 \\\\ LaVA-7B [141] & 31.29 & 67.50 & 8.00 & 31.28 & 67.25 & 11.22 & 31.49 & 92.15 & 12.37 & 35.32 & 62.35 & 17.37 \\\\ LaVA-13B [141] & 31.54 & 67.63 & 12.64 & 31.49 & 73.25 & 11.54 & 34.97 & 94.25 & 16.03 & 43.13 & 80.60 & 23.91 \\\\ LLaVA-34B [141] & 39.95 & 85.75 & 18.85 & **52.75** & **85.20** & **33.91** & **56.41** & **95.81** & **25.31** & 55.05 & **86.50** & 18.97 \\\\ \\hline \\multicolumn{13}{c}{_Qwen VL_ [278]} & 2.73 & 6.60 & 1.03 & 6.25 & 16.00 & 3.65 & 18.74 & 71.50 & 5.45 & 8.73 & 16.05 & 5.58 \\\\ \\multicolumn{13}{c}{_Qwen VL-C_ [278]} & 8.72 & 16.90 & 6.67 & 5.26 & 8.60 & 4.10 & 12.11 & 47.75 & 8.08 & 25.99 & 43.40 & 13.21 \\\\ \\multicolumn{13}{c}{_CogVLM_ [291]} & 0.04 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.10 & 0.95 & 0.00 & 0.00 & 0.00 & 0.00 \\\\ \\multicolumn{13}{c}{_CogVLM_-G [291]} & 0.00 & 0.00 & 0.00 & 9.86 & 13.50 & 6.79 & 22.64 & 75.45 & 0.45 & 11.25 & 22.65 & 7.12 \\\\ \\multicolumn{13}{c}{_CogVLM_-C [291]} & 12.89 & 22.75 & 7.18 & 25.37 & 43.63 & 12.03 & 28.25 & 72.80 & 17.50 & 30.16 & 56.00 & 16.35 \\\\ \\hline \\multicolumn{13}{c}{_LAVA-7B_ [141]} & - & - & - & 9.16 & 14.60 & 5.51 & - & - & - & - & 11.68 & 23.55 & 9.36 \\\\ \\multicolumn{13}{c}{_GLAMM_ [326]} & - & - & - & 27.11 & 53.35 & 13.01 & - & - & - & - & **63.81** & 81.75 & 53.40 \\\\ GroundHOG [318] & - & - & - & - & 23.57 & 30.80 & 24.23 & - & - & - & - & 44.80 & 43.10 & 38.97 \\\\ \\hline IDEFICS [327] & 0.00 & 1.45 & 0.13 & 6.25 & 18.70 & 0.64 & 17.37 & 76.15 & 10.06 & 4.62 & 0.00 & 0.32 \\\\ \\multicolumn{13}{c}{_CogVLM_-2 [291]} & 21.51 & 37.55 & 17.31 & 37.02 & 70.85 & 12.69 & 37.10 & 73.50 & 17.44 & 21.16 & 38.75 & 13.65 \\\\ \\multicolumn{13}{c}{_MiniCPM_-V [328]} & 34.75 & 59.91 & 17.37 & 31.62 & 62.80 & 13.65 & 32.16 & 68.05 & 16.79 & 27.42 & 55.35 & 16.92 \\\\ \\multicolumn{13}{c}{_GPT_-4V [276]} & 53.80 & 77.55 & 40.83 & - & - & - & - & - & - & 55.89 & 78.25 & 41.03 \\\\ \\multicolumn{13}{c}{_GPT_-4O [329]} & **71.27** & **89.25** & **66.03** & - & - & - & - & - & - & 60.77 & 73.92 & **54.31** \\\\ \\hline \\multicolumn{13}{c}{_LLaVA-7B_ [141]} & 21.26 & 52.40 & 7.69 & - & - & - & - & - & - & - & 30.59 & 60.85 & 12.69 \\\\ \\multicolumn{13}{c}{_+OPERA_ [330]} & 24.07 & 58.65 & 7.35 & - & - & - & - & - & - & - & 30.44 & 60.85 & 13.27 \\\\ \\hline \\multicolumn{13}{c}{_Unseen_} \\\\ \\hline Yi-VL-6B [325] & 2.74 & 3.88 & 1.14 & 3.18 & 4.24 & 5.20 & 4.04 & 10.90 & 10.57 & 0.14 & 0.45 & 0.08 \\\\ Yi-VL-34B [325] & 7.77 & 15.63 & 4.23 & 10.28 & 18.04 & 7.97 & 11.24 & 22.49 & 12.03 & 0.46 & 2.37 & 0.41 \\\\ LLaVA-7B [141] & 30.56 & 68.12 & 10.33 & 30.55 & 68.16 & 10.24 & 31.89 & 90.33 & 13.25 & 34.88 & 64.41 & 16.18 \\\\ LLaVA-13B [141] & 27.56 & 63.10 & 8.37 & 27.41 & 63.10 & 8.37 & 35.65 & 31.09 & 14.80 & 42.66 &types of questions: True/False (T), Multiple-Choice (C), and Limited Visual Question Answering (VQA) (V). It also shows the models' performance across the three core capabilities: Sensation, Cognition, and Knowledge. Table XVII further evaluates the consistency between the captions generated by MLLMs and the discriminative answers (ConScore[C]). This includes the overall ConScore[C], as well as consistency scores for the three question types: True/False (T), Multiple-Choice (C), and Limited Visual Question Answering (VQA) (V)."
    },
    {
      "title": "7.2.5 Compbench: Comparative Reasoning Benchmark",
      "text": "The ability to compare objects, scenes, or situations is crucial for decision-making and problem-solving in everyday life [35, 34, 352]. Although this ability is widespread in human cognition, it has not been fully explored in the field of Artificial General Intelligence (AGI). Jihyung Kil et al. [353] proposed a benchmark, COMPBENCH, designed to evaluate the comparative reasoning ability of MLLMs. As show in table XVII. COMPBENCH questions are carefully crafted to distinguish relative features between two images, testing the models' performance across eight different comparative dimensions by providing image pairs and related questions. Table XVII presents the performance of recent MLLMs on the COMPBENCH benchmark. 2.6 Hallu-Pli: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs Similarly, in the context of the hallucination problem faced by MLLMs in visual-language understanding and generation Fig. 3: Model performance per Country-Language pair. The blue lines indicate separation by continent. All models show similar behaviour in the majority of cases, despite having different sizes. [33] Fig. 2: Statistics of the CVQA Benchmark. [33]tasks [315, 316, 317, 318, 322, 323, 321, Peng Ding et al. [355] pointed out that previous studies have mainly focused on evaluating hallucinations on standard, undisturbed benchmarks, neglecting the prevalent interference inputs in the real world. This is crucial for a comprehensive evaluation of hallucinations in MLLMs. They proposed the first benchmark designed to evaluate hallucinations in MLLMs under disturbed inputs, called Hall-Pl, which includes seven types of disturbed scenarios: noise, blur, weather, digits, image stitching, image cropping, and prompt misdirection. Table 18 presents the performance of MLLMs under four basic disturbance types (noise, blur, weather, and digits). The \"Before/After\" columns compare the performance before and after the perturbation, using the ACC+ (Accuracy+) and CHAIR (Hallucinated Object Occurrence Rate) metrics to measure the level of hallucinations in the models. Table 19 focuses on the performance of MLLMs under three additional disturbance types in Hallu-Pl: Concat, Cropping, and Prompt Mislead. The PI-Score (a comprehensive evaluation \\begin{table} \\begin{tabular}{l|c|c|c|c|c|c|c|c|c} \\hline \\hline **Models** & **Overall** & **Life** & **Art** & **Society** & **Fsy.** & **Env.** & **Others** & **Positive** & **Neutral** & **Negative** \\\\ & (1,399) & (585) & (85) & (461) & (152) & (51) & (65) & (196) & (789) & (414) \\\\ \\hline \\multicolumn{9}{c}{_Open-source Models_} \\\\ \\hline InstructRILP-T5-XL [333] & 47.3 & 45.6 & 48.2 & 48.8 & 44.7 & 52.9 & 50.8 & 46.9 & 48.3 & 45.4 \\\\ BLIP-2 FLAN-T5-XL [338] & 52.8 & 53.0 & 58.8 & 52.5 & 42.8 & 64.7 & 58.5 & 56.1 & 52.9 & 51.0 \\\\ mPLUCs-OW2L [339] & 53.2 & 54.0 & 56.5 & 50.5 & 52.0 & 60.8 & 56.9 & 55.6 & 52.6 & 53.1 \\\\ Qvern-VL-Chat [278] & 53.4 & 53.2 & 49.4 & 52.1 & 50.0 & 60.8 & 72.3 & 56.1 & 52.6 & 53.6 \\\\ InstructRILP-T5-XL [333] & 56.7 & 56.2 & 58.8 & 58.6 & 45.4 & 64.7 & 64.6 & 63.3 & 56.1 & 54.6 \\\\ Mantis-8B-siglip-Llama3 & 57.5 & 56.8 & 61.2 & 57.5 & 53.9 & 64.7 & 61.5 & 59.2 & 58.0 & 55.6 \\\\ BLIP-2 FLAN-T5-XXL [338] & 57.8 & 57.1 & 63.5 & 57.0 & 53.3 & 66.7 & 66.2 & 67.9 & 57.2 & 54.3 \\\\ DeepSeek-VL-Chat-7B [340] & 60.3 & 59.0 & 58.8 & 58.4 & 61.8 & 68.6 & 76.9 & 65.8 & 60.1 & 58.0 \\\\ Yi-VL-6B-Chat [325] & 61.3 & 60.9 & 63.5 & 60.7 & 56.6 & 66.7 & 72.3 & 61.7 & 61.7 & 60.1 \\\\ InterMN-XComposer-2VL [341] & 62.1 & 61.7 & 62.4 & 62.3 & 58.6 & 70.6 & 66.2 & 65.8 & 63.0 & 58.7 \\\\ InterVL-Chat-15 [342] & 66.3 & 63.6 & 65.9 & 68.5 & 65.8 & 64.7 & 76.9 & 73.5 & 65.4 & 64.5 \\\\ Idefics2-8B [327] & 67.7 & 67.2 & **74.1** & 67.7 & 62.5 & 74.5 & 70.8 & 68.9 & 67.0 & 68.4 \\\\ Yi-VL-34B-Chat [325] & 67.9 & 67.5 & 70.6 & 67.7 & 63.8 & 70.6 & 76.9 & 74.0 & 68.2 & 64.5 \\\\ MiniCPM-Llama3-2.5 [328] & 69.4 & 68.4 & 71.8 & 69.4 & 64.5 & **80.4** & 78.5 & 75.0 & 69.3 & 66.9 \\\\ CogVLM2-Llama3-Chat [343] & 70.3 & 68.9 & 68.2 & 70.9 & 67.8 & 72.5 & **86.2** & 69.9 & 71.1 & 69.1 \\\\ LLVA-1.6-34B [141] & **73.8** & **73.8** & 71.8 & **73.3** & **71.1** & 78.4 & 81.5 & **79.1** & **72.9** & **72.9** \\\\ \\hline \\multicolumn{9}{c}{_Class-source Models_} \\\\ \\hline GPT-4V [276] & 65.9 & 65.0 & 69.4 & 65.3 & 59.9 & 76.5 & 80.0 & 69.4 & 66.0 & 64.0 \\\\ GPT-4o [329] & 72.6 & 72.5 & 72.9 & 73.3 & 68.4 & 76.5 & 75.4 & 78.6 & 71.2 & 72.5 \\\\ Gemini-1.5 Pro [344] & 73.9 & 73.7 & **74.1** & 74.4 & 63.2 & **80.4** & 83.1 & **80.1** & 70.8 & **75.4** \\\\ Qwen-VL-MAX [278] & **74.8** & **74.7** & 71.8 & **74.6** & **73.0** & 76.5 & **84.6** & **80.1** & **74.5** & 72.9 \\\\ \\hline \\multicolumn{9}{c}{_Humans_} \\\\ \\hline Human\\_avg [337] & 90.3 & 90.0 & 88.2 & 91.4 & 86.6 & 96.1 & 92.3 & 84.7 & 89.1 & 92.2 \\\\ Human\\_best [337] & **98.2** & **97.9** & **98.8** & **98.3** & **97.4** & **100.0** & **100.0** & **98.0** & **98.0** & **98.8** \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE XIII: Devent results of different MLLMs and humans on different domains and emotions. [337] Fig. 4: Overview of 19 evaluation detailed categories in ConBench. [347] \\begin{table} \\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c} \\hline \\hline **Model** & **VQAv2** & **GQA** & **VizWiz** & **SciQA-IMG** & **TextVQA** & **CVQA (EN)** & **CVQA (LOC)** \\\\ \\hline LLaVA-1.5-7B [141] & 78.5 & 62.0 & 50.0 & 66.8 & 58.2 & 48.9 & 36.5 \\\\ InstructBLIP [333] & - & 49.2 & 34.5 & 60.5 & 50.1 & 47.8 & 32.7 \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE XII: LLaVA-1.5-7B and InstructBLIP results on various VQA datasets. [33]metric) is used to assess the overall performance of the models under these specific disturbance scenarios. Table IV provides the performance details of MLLMs in generation tasks under the Concat, Cropping, and Prompt Mislead disturbances. The metrics CHAIR, Cover, Hal, and Cog are used to evaluate the models' performance in generation tasks. These metrics help us understand the models' accuracy and hallucination tendencies when generating descriptions that are consistent with the image content. Table IV presents the performance of MLLMs in discriminative tasks under image stitching, cropping, and prompt misdirection disturbances. The metrics ACC, ACC+, and F1 are used to measure the models' accuracy in discriminative tasks. These data provide insights into the models' ability to handle disturbed inputs in discriminative tasks."
    },
    {
      "title": "7.2.7 Reform-Eval: Evaluating Mllms Via Unified Reformulation Of Task-Oriented Benchmarks",
      "text": "MLLMs have made significant progress in understanding and reasoning about visual information [361, 362, 141, 356, 141], However, this has posed challenges for the automatic evaluation of free-form text outputs from MLLMs. To leverage annotations from existing benchmarks and reduce the manual effort required to construct new benchmarks, Zejun Li et al. [363] proposed a method for reformatting existing benchmarks into a unified format compatible with MLLMs. Through systematic data collection and reformatting, they introduced the ReForm-Eval benchmark, which is designed to comprehensively and \\begin{table} \\begin{tabular}{l|c|c c|c c c|c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{ConScore[D]} & \\multicolumn{3}{c|}{Sensation} & \\multicolumn{3}{c|}{Cognition} & \\multicolumn{3}{c}{Knowledge} \\\\ \\cline{3-10} & & T & C & V & Con & T & C & V & Con & T & C & V & Con \\\\ \\hline \\hline \\multicolumn{10}{c}{_Tosed-source Vision Language Models_} \\\\ \\hline GPT-4V [276] & 29.20 & 80.4 & 79.9 & 61.7 & 48.3 & 68.8 & 53.2 & 39.9 & 20.4 & 63.1 & 57.2 & 30.0 & 14.2 \\\\ GPT-4-Ommi [329] & 35.70 & 89.2 & 79.4 & 64.4 & 55.0 & 71.8 & 62.8 & 44.9 & 27.8 & 64.7 & 61.7 & 39.7 & 23.3 \\\\ Gemini-Pro-Vision [348] & 25.00 & 85.2 & 60.7 & 63.4 & 39.3 & 71.8 & 45.0 & 44.2 & 15.1 & 65.0 & 51.4 & 39.7 & 15.8 \\\\ Gemini-Ultra-Vision [348] & 33.10 & 78.9 & 78.6 & 66.3 & 50.3 & 68.1 & 58.5 & 47.9 & 28.5 & 62.9 & 62.2 & 44.7 & 19.7 \\\\ Qwen-VL-Plus [278] & 28.10 & 82.7 & 74.9 & 60.4 & 45.0 & 64.2 & 41.7 & 30.8 & 16.3 & 63.6 & 54.2 & 33.3 & 15.8 \\\\ Qwen-VL-Max [278] & **37.00** & 86.4 & 80.7 & 65.4 & **56.3** & **72.9** & 51.4 & 51.3 & 28.1 & 68.3 & 58.6 & 38.9 & **24.2** \\\\ \\hline \\multicolumn{10}{c}{_T / Vision Language Models_} \\\\ \\hline ILVA-v1.5-7B [141] & 16.60 & 79.3 & 56.8 & 44.3 & 28.3 & 51.4 & 33.5 & 15.8 & 4.7 & 61.7 & 44.4 & 16.9 & 7.8 \\\\ Qwen-VL-Chat [278] & 26.40 & 81.0 & 79.6 & 54.2 & 39.0 & 55.0 & 46.3 & 33.2 & 13.5 & 60.3 & 54.2 & 28.9 & 14.7 \\\\ \\hline \\hline \\multicolumn{10}{c}{_\\(\\sim\\) 13B Vision Language Models_} \\\\ \\hline ILVA-v1.5-13B [141] & 24.00 & 82.9 & 77.1 & 49.6 & 39.5 & 53.6 & 37.8 & 20.1 & 10.4 & 65.6 & 50.3 & 17.2 & 9.7 \\\\ MiniGemini-13B [349] & 21.80 & 81.9 & 69.7 & 52.8 & 39.3 & 51.9 & 38.2 & 21.1 & 6.9 & 52.8 & 36.7 & 17.5 & 9.2 \\\\ InternVL-v1.5-26B [342] & 31.40 & 85.6 & 84.8 & 65.0 & 54.3 & 59.7 & 58.6 & 44.4 & 19.4 & 58.1 & 55.8 & 25.3 & 12.2 \\\\ \\hline \\multicolumn{10}{c}{_\\(\\sim\\) 34B Vision Language Models_} \\\\ \\hline ILVA-NeXT-34B [350] & 27.70 & 82.4 & 81.7 & 55.6 & 43.6 & 50.7 & 47.5 & 25.6 & 9.9 & 60.4 & 56.1 & 27.8 & 12.8 \\\\ MiniGemini-34B [349] & 23.00 & 80.8 & 76.8 & 48.2 & 39.7 & 36.9 & 30.7 & 18.9 & 6.0 & 58.1 & 42.3 & 20.8 & 8.2 \\\\ InternVL-v1.2P-40B [280] & 34.70 & 83.7 & 83.2 & 66.6 & 53.4 & 74.2 & 67.6 & 57.1 & **34.9** & 72.2 & 58.3 & 28.6 & 13.6 \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE IV: **Evaluation[D] of mainstreams series of MLLMs on ConBench. The detailed results of the Sensation, Cognition, and Knowledge core capabilities are listed below. T, C, and V represent true-false, multiple-choice, and limited VQA questions, respectively. The ranking can be found below the respective numbers. \\(\\uparrow\\): Due to safety considerations, GPT-4V declined to answer the celebrity category. [347]** \\begin{table} \\begin{tabular}{l|c|c|c|c|c} \\hline \\hline Method & ConScore[C] & Con[T] & Con[C] & Con[V] & Ordered \\\\ \\hline \\hline \\multicolumn{10}{c}{_Tosed-source Vision Language Models_} \\\\ \\hline GPT-4V [276] & 55.6 & \\(51.20\\) & \\(56.50\\) & \\(59.10\\) & Y \\\\ GPT-4-Ommi [329] & \\(\\mathbf{62.2}\\) & \\(58.00\\) & \\(62.50\\) & \\(66.10\\) & Y \\\\ Gemini-Pro-Vision [348] & \\(48.4\\) & \\(43.30\\) & \\(45.20\\) & \\(56.80\\) & Y \\\\ Gemini-Ultra-Vision [348] & \\(54.6\\) & \\(47.80\\) & \\(55.20\\) & \\(60.70\\) & Y \\\\ Qwen-VL-Plus [278] & \\(50.2\\) & \\(47.10\\) & \\(49.10\\) & \\(54.30\\) & Y \\\\ Qwen-VL-Max [278] & \\(58.4\\) & \\(54.30\\) & \\(58.00\\) & \\(62.90\\) & Y \\\\ \\hline \\hline \\multicolumn{10}{c}{_T /R Vision Language Models_} \\\\ \\hline ILaVA-v1.5-7B [141] & 38.4 & \\(39.20\\) & \\(36.60\\) & \\(39.50\\) & N \\\\ Qwen-VL-Chat [278] & 48.0 & \\(42.00\\) & \\(50.80\\) & \\(51.30\\) & Y \\\\ \\hline \\hline \\multicolumn{10}{c}{_\\(\\sim\\) 13B Vision Language Models_} \\\\ \\hline ILaVA-v1.5-13B [141] & \\(44.4\\) & \\(41.50\\) & \\(45.80\\) & \\(46.00\\) & Y \\\\ MiniGemini-13B [349] & \\(41.7\\) & \\(38.80\\) & \\(42.90\\) & \\(43.30\\) & Y \\\\ InternVL-v1.5-26B [342] & \\(50.9\\) & \\(44.50\\) & \\(53.90\\) & \\(54.20\\) & Y \\\\ \\hline \\hline \\multicolumn{10}{c}{_\\(\\sim\\) 34B Vision Language Models_} \\\\ \\hline ILaVA-NeXT-34B & \\(48.3\\) & \\(46.00\\) & \\(52.20\\) & \\(46.80\\) & N \\\\ MiniGemini-34B [349] & \\(49.6\\) & \\(56.80\\) & \\(48.00\\) & \\(44.10\\) & N \\\\ InternVL-v1.2P-40B [280] & \\(53.7\\) & \\(49.80\\) & \\(55.50\\) & \\(55.80\\) & Y \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE IV: **Evaluation[D] of mainstreams series of MLLMs on ConBench. The detailed results of the Sensation, Cognition, and Knowledge core capabilities are listed below. T, C, and V represent true-false, multiple-choice, and limited VQA questions, respectively. The ranking can be found below the respective numbers. \\(\\uparrow\\): Due to safety considerations, GPT-4V declined to answer the celebrity category. [347]**quantifatively assess the capabilities of MLLMs. This approach overcomes the structural differences between existing task-oriented multimodal benchmarks and MLLMs. Figure 5 illustrates the capabilities and task dimensions of the ReForm-Eval benchmark. It categorizes the evaluation dimensions into two major categories with eight subcategories: 1)Visual Perception Tasks: Coarse-Grained Perception (CG), Fine-Grained Perception (FG), Scene Text Perception (STP). 2)Visual Cognition Tasks: Visually Grounded Reasoning (VGR), Spatial Understanding (Spatial), Cross-Modal Inference (CMI), Visual Description (Desc), Multi-Turn Dialogue (Dialog). These categories and subcategories comprehensively cover different aspects of MLLMs' visual understanding and reasoning capabilities, providing a comprehensive benchmark for evaluating model performance. Table XII shows a comprehensive performance evaluation of 16 open-source MLLMs across different capability dimensions, based on the ReForm-Eval benchmark."
    },
    {
      "title": "7.2.8 Visiongraph: Graph Theory Problems Benchmark In Visual Context",
      "text": "MLLMs have achieved significant success in visual understanding and reasoning [373, 362, 141, 276], but multimodal graph reasoning remains a challenging task [374]. It requires MLLMs to accurately understand graph structures and perform multi-step reasoning on visual graphs. To explore the ability of advanced MLLMs to address multimodal graph reasoning tasks, Yunxin Li et al. [375] designed a benchmark called VisionGraph, which includes a series of graph reasoning problems aimed at testing MLLMs' understanding of graph structures and their multi-step reasoning capabilities. Table XII presents the performance of different MLLMs on the VisionGraph benchmark, including evaluation metrics such as node recognition accuracy, edge recognition accuracy, and solution accuracy for specific graph theory problems. These results provide valuable insights for researchers into the models' abilities to understand and reason about graph structures. Table XII shows the performance improvements of models on three representative graph theory problems (Connectivity, Cycle, and Shortest Path) after applying the Description-Program-Reasoning (DPR) method. The DPR approach enhances MLLMs' multi-step reasoning abilities by combining natural language processing and programming logic."
    },
    {
      "title": "_Applications Of Mllms_",
      "text": "Zebang Cheng et al. [377] proposed Emotion-LLaMA, which integrates audio, visual, and text inputs through an emotion-specific encoder, and significantly improves emotion recognition and reasoning accuracy through instruction tuning. This approach enhances the model's ability to understand and reason about emotional content across different modalities. Xun Wu et al. [378] created the VisionPrefer dataset, which includes fine-grained human preference annotations. They then trained the VP-Score reward model on this dataset to guide the training of image generation models, improving the alignment between images and text prompts. Finally, they fine-tuned the model using reinforcement learning to make the generated images more aligned with human aesthetics and preferences. Zhenyu Wang et al. [379] proposed the GenArtist system, which enables unified image generation and editing coordinated by a multimodal large language model. The system introduces location-aware tool execution and integrates tool libraries, enhancing the model's flexibility and applicability. Yushi Hu et al. [380] proposed the Visual SKETCHPAD framework, enabling multimodal language models to draw sketches and perform reasoning based on visual artifacts. This significantly enhances the model's performance in mathematical and visual tasks. Haoyu Chen et al. [381] proposed an MLLM-based intelligent image restoration system, RestoreAgent, which can automatically assess degradation, determine tasks, select models, and perform restoration. Haodong Chen et al. [382] proposed the FineCLIPER framework, which enhances facial expression recognition performance by incorporating text description augmentation, hierarchical information mining, and parameter-efficient fine-tuning to achieve multimodal feature fusion and cross-modal contrastive learning. Shuo Ma et al. [383] proposed SleepMG, which addresses the classification and domain-discrepancy performance issues in sleep staging by quantifying modal performance differences and adaptively adjusting gradients to achieve multimodal balance. This method specifically tackles the challenges posed by the classification of multimodal physiological signals, such as EEG, EOG, EMG, and ECG. Yifeng Xie et al. [384] proposed the MoBA model, which employs bidirectional adapters and a mixture of experts system to achieve efficient cross-modal interaction with a low parameter Fig. 5: Assessed capability dimensions and tasks in ReForm-Eval. “Desc” and “Classif” are respectively short for description and classification. [363] \\begin{table} \\begin{tabular}{c c c c} \\hline \\hline \\multirow{2}{*}{Relativity} & Dataset & Domain & samples \\\\ \\hline \\multirow{4}{*}{Attribute} & MIT-States & Open & 0.2K \\\\ & Fashionpedia & Fashion & 2.4K \\\\ & VAW & Open & 0.9K \\\\ & CUB-200-2011 & Bird & 0.9K \\\\ & Wildfish++ & Fish & 0.9K \\\\ \\hline \\multirow{2}{*}{Existence} & MagicBrush & Open & 0.9K \\\\ & Spot-the-diff & Outdoor Scene & 1.2K \\\\ \\hline \\multirow{2}{*}{State} & MIT-States & Open & 0.6K \\\\ & VAW & Open & 0.5K \\\\ \\hline \\multirow{2}{*}{Emotion} & CelebA & Face & 1.5K \\\\ & FER-2013 & Face & 3.8K \\\\ \\hline \\multirow{2}{*}{Temporality} & SoccerNet & Sport & 8.3K \\\\ & CompCars & Car & 5K \\\\ \\hline \\multirow{2}{*}{Spatiality} & NYU-Depth V2 & Indoor Scene & 1.9K \\\\ \\hline \\multirow{2}{*}{Quantity} & VQAw2 & Open & 9.8K \\\\ \\hline \\multirow{2}{*}{Quality} & Q-Bench2 & Open & 1K \\\\ \\hline \\multirow{2}{*}{Total} & \\multirow{2}{*}{-} & \\multirow{2}{*}{-} & 39.8K \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE XII: Overall statistics of COMPBENCH. [353] [MISSING_PAGE_FAIL:29] models in video understanding and generation, enabling them to better handle multimodal inputs and efficiently generate video content. Xiuliang Duan et al. [398] proposed the Reason-and-Execute prompting method, which enhances the model's ability to solve geometric problems by combining reasoning templates and execution templates. Xuechen Guo et al. [52] proposed the LLaVA-Ultra model, which introduces a fine-grained visual encoder and an adaptive sampling module through architecture improvements, addressing the performance limitations of current multimodal large language models in medical visual question answering (Med-VQA). Yi Bin et al. [399] constructed the large-scale painting analysis dataset, PaintingForm, and proposed the GalleryGPT model. By fine-tuning for tasks focused on visual feature analysis, the model significantly improved the performance and generalization ability of art analysis. Dan Kondratyuk et al. [400] proposed VideoPoet, a zero-shot video generation model based on LLMs. It uses a decoder architecture to process multimodal inputs and enables high-quality video synthesis, demonstrating the ability to generate complex dynamic scenes. Yongshuo Zong et al. [401] proposed post hoc and hybrid fine-tuning strategies to effectively enhance the safety of MLMs, addressing the issues of harmful content generation and susceptibility to attacks in MLMs. Yang Jin et al. [402] proposed the Video-LaVIT framework, which achieves efficient video decomposition using keyframes and motion vectors. This approach enables unified pretraining for video, image, and text, improving the safety and efficiency of MLMs. Long Qian et al. [403] proposed the Momentor model, which incorporates a time-aware module and event-based sequence modeling to achieve fine-grained temporal understanding and video segment-level reasoning. Zhisheng Zheng et al. [404] designed the SPATIAL-AST encoder, which jointly performs sound event detection, spatial localization, and distance estimation. By integrating SPATIAL-AST with LLaMA-2, they constructed the BAT model, capable of answering questions about sound source relationships in 3D environments. The model utilizes a multi-stage training strategy to progressively enhance its spatial audio perception and reasoning capabilities. Guangzhi Sun et al. [405] proposed Video-SALMONN, the first unified model to simultaneously process video, speech, and music. They designed the MRC Q-Former structure to achieve multi-resolution information extraction, enhancing the ability of AV-LLMs to integrate speech information for comprehensive video content understanding. Ling Li et al. [406] introduced the concept of \"localizability\" to quantify street view images and filter high-quality data. They proposed the GeoReasoner model, which combines human reasoning knowledge and employs a two-stage fine-tuning approach to achieve geographic localization and reasoning, addressing the challenges of geographic localization in street view images. Yunheng Li et al. [407] proposed the Cascade-CLIP frame \\begin{table} \\begin{tabular}{c|c c|c c c|c c|c c|c c|c c} \\hline \\multirow{3}{*}{MLLMs} & \\multicolumn{6}{c|}{Image Concatenation} & \\multicolumn{6}{c|}{Image Cropping} & \\multicolumn{2}{c}{Prompt Misleading} \\\\ \\cline{2-13} & \\multicolumn{3}{c|}{CIFAIR} & \\multicolumn{3}{c|}{Coyer} & \\multicolumn{3}{c|}{Hal} & \\multicolumn{3}{c}{Cog} & \\multicolumn{3}{c}{Hal} & \\multicolumn{3}{c}{Hal} \\\\ \\cline{2-13} & Before & After & Before & After & Before & After & Before & After & Before & After & Before & After \\\\ \\hline CogVLM [291] & 62.0 & 69.0 & 55.3 & 48.3 & 58.3 & 97.1 & 4.3 & 5.9 & 80.0 & 90.0 & 36.7 & **93.3** \\\\ Multi-GPT [276] & 73.5 & **97.5** & 22.5 & 2.0 & 96.7 & 86.3 & **30.8** & **77.1** & 76.7 & **100.0** & 63.3 & **93.3** \\\\ LLaVA [141] & 68.5 & 92.3 & 38.8 & 7.4 & 93.3 & 96.7 & 4.3 & 14.9 & **93.3** & 86.7 & **66.7** & **93.3** \\\\ LLaVA1.5 [141] & 68.9 & 76.1 & 43.8 & 25.0 & 78.3 & 96.3 & 3.4 & 5.7 & 86.7 & 90.0 & 63.3 & 90.0 \\\\ MiniGPT-4 [356] & 72.4 & 89.3 & 46.5 & 24.8 & 98.3 & 95.8 & 5.1 & 8.2 & 80.0 & 83.3 & 63.3 & **93.3** \\\\ MiniGPT-v2 [357] & 72.1 & 88.9 & 49.6 & 32.5 & **100.0** & 96.7 & 4.0 & 7.1 & **93.3** & 93.3 & 53.3 & **93.3** \\\\ mPLUG2 [358] & 65.0 & 82.3 & 44.6 & 14.3 & 86.7 & 89.6 & 6.2 & 6.4 & **93.3** & 96.7 & 46.7 & 80.0 \\\\ InternLM [359] & 58.4 & 79.2 & 16.3 & 9.5 & 71.7 & 62.5 & 18.8 & 16.7 & 86.7 & 86.7 & 43.3 & 63.3 \\\\ Qwen-VL [278] & 58.2 & 56.3 & 35.8 & 32.3 & 46.7 & 79.2 & 9.8 & 11.1 & 83.3 & 93.3 & 6.7 & 16.7 \\\\ VisualGLM [360] & **76.9** & 89.1 & 45.0 & 29.6 & **100.0** & **99.2** & 4.4 & 9.2 & **93.3** & **100.0** & 46.7 & 66.7 \\\\ Gemini [288] & 57.3 & 63.4 & 50.2 & 43.7 & 56.7 & 90.8 & 3.6 & 4.5 & 26.7 & 56.7 & 12.1 & 30.0 \\\\ GPT-4V [276] & 66.1 & 63.6 & **66.6** & **53.6** & 63.3 & 98.3 & 1.6 & 1.9 & 33.3 & 73.3 & 1.1 & 3.3 \\\\ \\hline \\end{tabular} \\end{table} TABLE 20: The results of generative task on image concatenation, cropping, and prompt misleading. [355] \\begin{table} \\begin{tabular}{c|c c c|c c c|c c c|c c c|c c} \\hline \\multirow{3}{*}{MLLMs} & \\multicolumn{6}{c|}{Image Concatenation} & \\multicolumn{6}{c|}{Image Cropping} & \\multicolumn{2}{c}{Prompt Misleading} \\\\ \\cline{2-13} & \\multicolumn{3}{c|}{Before} & \\multicolumn{3}{c|}{After} & \\multicolumn{3}{c|}{Before} & \\multicolumn{3}{c|}{After} & \\multicolumn{3}{c}{After} \\\\ \\cline{2-13} & ACC & ACC+ F1 & ACC & ACC+ F1 & ACC & ACC+ F1 & ACC & ACC+ F1 & ACC & ACC+ F1 & ACC & ACC+ F1 \\\\ \\hline CogVLM [291] & 69.9 & **49.0** & **74.4** & **67.2** & **42.0** & **73.1** & 50.0 & 0.0 & 66.7 & 50.0 & 0.0 & **66.7** & 56.7 & 33.3 & 51.9 \\\\ Multi-GPT [276] & 46.8 & 13.3 & 52.4 & 41.8 & 16.3 & 48.9 & 48.3 & 0.0 & 65.2 & 45.0 & 0.0 & 62.1 & 28.3 & 6.7 & 41.1 \\\\ LLava [141] & 51.5 & 6.3 & 57.2 & 50.3 & 1.0 & 54.0 & 50.0 & 0.0 & 66.7 & 50.0 & 0.0 & **66.7** & 1.7 & 0.0 & 3.2 \\\\ LLava1.5 [141] & **70.5** & 43.0 & **76.1** & 51.7 & 8.0 & 61.7 & 51.7 & 6.7 & 56.7 & 48.3 & 6.7 & 45.6 & 40.0 & 3.3 & 5.2 \\\\ MiniGPT-4 [356] & 43.0 & 16.0 & 47.6 & 30.2 & 7.7 & 25.4 & 38.3 & 0.0 & 55.4 & 30.0 & 0.0 & 46.2 & 20.0 & 0.0 & 33.4 \\\\ MiniGPT-v2 [357] & 55.8 & 28.3 & 56.4 & 48.2 & 21.3 & 41.3 & 55.0 & 26.7 & 62.0 & 48.3 & **23.3** & 47.5 & 88.3 & 80.0 & 88.8 \\\\ mPLUG2 [358] & 62.3 & 80.0 & 68.3 & 51.5 & 27.3 & 54.5 & 50.0 & 13.3 & 62.5 & 48.3 & 13.3 & 59.7 & 43.3 & 13.3 & 34.6 \\\\ InterLM [359] & 68.2 & 48.3 & 70.8 & 61.2 & 37.0 & 55.9 & 50.0 & 3.3 & 60.5 & 51.7 & 67.6 & 61.3 & 75.0 & 50.0 & 68.1 \\\\ Qwen-VL [278] & 62.5 & 39.3 & 62.0 & 55.7 & 18.3 & 52.4 & 58.3 & 23.3 & 65.7 & 48.3 & 16.7 & 53.7 & 93.3 & 86.7 & 92.9 \\\\ VisualGLM [360] & 46.3 & 5.3 & 50.9 & 43.3 & 0.3 & 45.0 & 50.0 & 0.0 & 66.7work, which aligns multi-level visual features with text embeddings in a cascading manner. By introducing independent decoders to handle features at different levels, the framework enhances the transferability to new categories. This approach addresses the issue where the pre-trained model CLIP fails to fully leverage intermediate visual feature information in zero-shot semantic segmentation tasks. Zhijian Huang et al. [54] proposed the RDA-Driver model, which ensures the consistency between reasoning and decision-making in MLLMs through reasoning-decision alignment constraints and a redesigned Chain-of-Thought (CoT) framework. This approach enhances the interpretability and performance of autonomous driving systems."
    },
    {
      "title": "8 Continue Learning",
      "text": ""
    },
    {
      "title": "_Non-Large Language Model Unimodal Continual Learning_",
      "text": ""
    },
    {
      "title": "8.1.1 Framework Innovation",
      "text": "Xiaoxue Han et al. [164] proposed the TACO framework, which combines graph coarsening and continual learning to dynamically store information from previous tasks. They designed an efficient graph coarsening algorithm, RePro, based on node similarity, and introduced a node fidelity preservation strategy. The effectiveness of this approach in preventing the disappearance of minority classes was theoretically validated. Ari S. Benjamin et al. [146] proposed the Neural Tangent Ensemble (NTE) framework, which views a neural network as an ensemble of fixed experts. They derived its posterior update rule, which is equivalent to a specific form of stochastic gradient descent (SGD), offering a novel perspective for understanding and mitigating catastrophic forgetting. Daheeee Lee et al. [147] proposed the IsCiL framework, which improves sample efficiency and task adaptability by incrementally learning shared skills. They introduced prototype-based skill retrieval and adapter learning to enable effective knowledge sharing across different tasks. Kunlun Xu et al. [148] proposed the CKP framework, which purifies data through the CDF and ILR modules, and filters out erroneous knowledge using the EKF algorithm. This approach addresses the performance degradation issue caused by incorrect labels in the Lifelong Person Re-Identification task. Lei Liu et al. [149] proposed the PBR framework, which operates without prior knowledge. It reduces forgetting and enhances long-tail continual learning performance through an uncertainty-guided sampling strategy and two prior-free constraints. Yusong Hu et al. [150] proposed the Task-Aware Orthogonal Sparse Network (OSN), which explores shared knowledge between old and new tasks through parameter sharing. They introduced sharpness-aware orthogonal projections to optimize the update of shared parameters and reduce interference with old tasks. Daemu Lee et al. [67] proposed the Mixture-of-Domain Low-rank Experts (MoDE) framework, which includes domain-adaptive routing and domain-expert collaborative loss. This framework enables input-dependent online expert fusion, improving adaptation to new domains while preserving old knowledge. Meng Ding et al. [408] proposed a theoretical analysis framework for linear regression applicable to different parameterization scenarios. They revealed the impact of task sequences and algorithm parameters on forgetting and experimentally validated the theoretical findings. Soochan Lee et al. [151] proposed the SB-MCL framework, which achieves continual learning through sequential Bayesian updates. The neural network is fixed to prevent forgetting, and the framework is domain- and model-agnostic. Mikel et al. [153] proposed CompoNet, a modular neural network with linearly growing parameters. By combining strategies, it prevents forgetting while achieving efficient knowledge transfer and scalability. Raymond L. Wang et al. [154] proposed a Vector-HaSH-based neural model that combines hetero-associative memory and spatially invariant CNNs to enable fast learning and continual memory. They introduced the vHSN method, which utilizes attention mechanisms and grid encoding to prevent catastrophic forgetting and enhance generalization across different environments. Jinglin Liang et al. [155] proposed the DDDR framework, which utilizes diffusion models to generate historical data. By employing contrastive learning, the framework enhances the model's generalization ability on both generated and real data, addressing the issue of catastrophic forgetting in federated continual learning. Fernando Julio Cendra et al. [156] proposed the PromptCCD framework, which uses GMM as a prompting method to address the CCD problem. They introduced the GMP module, which dynamically generates prompts to adapt to new classes, \\begin{table} \\begin{tabular}{c|c c c|c c c c c|c c c|c c c|c c} \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{6}{c|}{**Generation Evaluation**} & \\multicolumn{6}{c}{****************} \\\\ \\cline{2-13} & \\multicolumn{3}{c|}{**Perception**} & \\multicolumn{3}{c|}{**Cognition**} & \\multicolumn{3}{c|}{\\(R\\)} & \\multicolumn{3}{c|}{**Perception**} & \\multicolumn{3}{c|}{**Cognition**} & \\multirow{2}{*}{\\(R\\)} \\\\ \\cline{2-13} \\cline{5-13} & CG & FG & STP & Spatial & VGR & Dialog & CMI & & & & CG & FG & Spatial & VGR & Dialog & CMI \\\\ \\hline BLIP-2\\({}_{F}\\)[364] & 69.4 & 76.6 & 38.1 & 43.2 & 73.3 & **61.8** & 66.9 & **74.3** & 2 & 60.7 & 74.4 & 51.1 & 69.8 & 62.6 & 58.9 & 4 \\\\ InstructGLIP\\({}_{F}\\)[365] & **71.2** & **78.1** & **41.2** & **46.1** & **73.9** & 60.6 & **71.4** & 43.8 & **2** & 60.4 & 75.6 & 51.2 & 71.0 & 67.2 & 55.5 & 4 \\\\ InstructGLIP\\({}_{V}\\)[366] & 69.1 & 70.8 & 40.7 & 44.4 & 63.0 & 48.6 & 53.8 & 27.3 & 4 & 58.5 & 77.8 & 52.3 & **73.5** & **68.7** & 55.4 & 3 \\\\ LLAvAv [141] & 28.7 & 34.4 & 18.4 & 28.7 & 44.0 & 35.6 & 47.3 & 36.6 & 11 & 61.0 & 70.3 & 42.4 & 58.9 & 52.3 & 48.0 & 8 \\\\ LLAvAv\\({}_{L2}\\)[141] & 48.3 & 59.8 & 21.5 & 41.2 & 59.7 & 46.3 & 49.9 & 39.5 & 6 & 49.9 & 65.6 & 47.4 & 56.7 & 48.6 & 49.7 & 11 \\\\ MiniGPT4 [356] & 46.2 & 53.2 & 33.0 & 34.6 & 45.6 & 39.5 & 45.4 & 47.5 & 7 & 54.9 & 70.6 & 49.2 & 57.3 & 54.1 & 50.9 & 8 \\\\ mPLUG-Owl [339] & 42.0 & 37.2 & 39.8 & 26.8 & 37.5 & 35.2 & 40.4 & 44.7 & 11 & 57.9 & 66.1 & 48.6 & 54.3 & 45.5 & 49.8 & 10 \\\\ PandGAPT [366] & 28.2 & 34.6 & 4.5 & 33.3 & 41.9 & 34.1 & 36.6 & 1.6 & 14 & 42.3 & 47.4 & 39.4 & 43.3 & 41.5 & 37.0 & 16 \\\\ IBLLM [367] & 29.2 & 32.7 & 8.2 & 35.6 & 36.7 & 35.3 & 36.6 & 27.6 & 13 & 49.6 & 54.4 & 46.1 & 50.3 & 39.5 & 45.6 & 15 \\\\ LA-V2 [368] & 33.2 & 30.8 & 24.2 & 23.8 & 36.3 & 35.4 & 41.1 & 36.0 & 13 & 42.7 & 61.4 & 48.6 & 54.1 & 43.4 & 49.9 & 12 \\\\ mmGPT [369] & 30.4 & 30.3 & 16.7 & 26.9 & 33.0 & 31.8 & 38.2 & 27.7 & 14 & 52.6 & 62.4 & 47.2 & 56.2 & 43.1 & 44.1 & 13 \\\\ Shikra [362] & 47.2 & 47.5 & 8.3 & 33.3 & 41.2 & 35.2 & 44.5 & 31.8 & 11 & 60.9 & 66.8 & 45.5 & 58.5 & 59.5 & **59.3** & 7 \\\\ Lynx [370] & 59.5 & 62.6 & 18.6 & 40.2 & 58.4 & 47.0 & 53.0 & 60.7 & 5 & **66.1** & 76.2 & **53.9** & 69.9 & 60.0 & 57.4 & 3 \\\\ Chectorv [371] & 52.0 & 50.3 & 25.9 & 30.6 & 49.9 & 40.3 & 47.4 & 61.6 & 7 & 56.1 & 69.0 & **48.4** & 58.7 & 57.6 & 50.6 & 8 \\\\ Chector\\({}_{L2}\\)[371] & 46.5 & 51.4 & 18.8 & 34.5 & 54.4 & 40.6 & 44.0 & 43.9 & 8 & 61.6 & 56.1 & 48.7 & 57.5 & 46.8 & 47.2 & 11 \\\\ BLIVA [372] & 41.7 & 43.4 & 40.8 & 33.3 & 42.4 & 39.8 & 45.2 & 52.5 & 8 & 64.9 & **78.2** & 51.7 & 72.9 & 68.1 & 53.7 & **2** \\\\ \\hline \\end{tabular} \\end{table} TABLE II: General evaluation results of MLLMs across different capability dimensions. “CG”, “FG”, “CMI”, and “Desc” are respectively short for coarse-grained perception, fine-grained perception, cross-modal inference, and description. “\\(\\bar{R}\\)” represents the average rank across dimensions. [363]thus solving the problem of automatically discovering new classes in continuous data streams while mitigating catastrophic forgetting. Dong Li et al. [157] proposed the Mecoin framework, which employs Structured Memory Units (SMU) and a Memory Construction Module (MeCo) for efficient storage and updating of class prototypes. They introduced the Memory Representation Adaptation Module (MRaM) and the Graph Knowledge Interchange Module (GKIM) to reduce parameter fine-tuning, lower the forgetting rate, and enhance the model's generalization ability. Linglan Zhao et al. [409] proposed the SAFE framework, which, in the first session, inherits the knowledge of the pre-trained model through knowledge transfer loss. In subsequent sessions, the framework balances model stability and adaptability by fixing slow parameters and updating fast parameters. It introduces an entropy-based aggregation strategy to dynamically fuse the advantages of two types of learners. This approach enables the efficient use of the rich knowledge from pre-trained models in continual learning while maintaining the model's adaptability and stability when facing new data. Wenju Sun et al. [158] proposed the RP2F framework, which directly combines the posterior parameters of new and old tasks. They introduced a parameter robustness prior and used perturbation methods to approximate the Hessian matrix, enabling effective knowledge sharing and backward knowledge transfer. Xiaoqian Liu et al. [159] proposed the HAMMER framework, which identifies shared knowledge and guides multilingual learning through online knowledge analysis and a hierarchical language evaluation mechanism, effectively alleviating the forgetting problem. Hao Yu et al. [160] proposed the FedCBC framework, which overcomes forgetting through category-specific binary classifiers and selective knowledge fusion. Xiaochen Li et al. [161] proposed the TS-ILM framework, which includes a task-level temporal pattern extractor and a time-sensitive example selector. This framework effectively captures cross-task temporal patterns, selects representative frames for replay, reduces information redundancy, and enhances memory retention. Depeng Li et al. [162] proposed the AutoActivator model, which dynamically adapts neural units to new tasks, enabling on-demand network expansion. This approach addresses the issue of forgetting old classes when learning new classes incrementally in class-incremental learning. Tom Fischer et al. [163] proposed iNeMo, an incremental neural grid model, which achieves efficient class-incremental learning through latent space initialization and position regularization."
    },
    {
      "title": "8.1.2 Method Innovation",
      "text": "Huiping Zhuang et al. [165] proposed the sample-free Generalized Analytical Continual Learning (GACL) technique, which avoids catastrophic forgetting through analytical learning. It establishes the equivalence between incremental learning and joint training, effectively addressing the challenges of handling mixed data categories. Ang Bian et al. [410] proposed the C-Flat method, which enhances continual learning (CL) performance by optimizing the \\begin{table} \\begin{tabular}{l|c c c c c c c c} \\hline Model\\(\\downarrow\\) Task Types \\(\\rightarrow\\) & Connect & Cycle & Topo. Sort & Shortest Path & Max Flow & Bipartite Graph & Hamilton Path & GNNs \\\\ \\hline \\multicolumn{8}{c}{_Node Recognition_} \\\\ \\hline MiniGFT-4 (Vicuna-7b) [356] & 19.14 & 12.04 & 42.96 & 42.19 & 32.76 & 8.33 & 60.34 & 53.85 \\\\ BLIP-2 (FianT5-x0) [604] & 37.74 & 52.88 & 47.41 & 81.25 & 67.24 & 22.62 & 62.07 & 61.54 \\\\ InstrucBLIP (FianT5-x0) [165] & 36.12 & 47.64 & 46.67 & 75.00 & 56.90 & 36.90 & 53.45 & 74.36 \\\\ InstrucBLIP (FianT5-x0) [165] & 35.31 & 52.88 & 61.48 & 85.94 & 77.59 & 17.86 & 65.52 & 61.54 \\\\ Sphirx [376] & 61.99 & 98.95 & 94.07 & 100.00 & 91.38 & 55.95 & **100.00** & 97.44 \\\\ Internnnn [59] & **67.92** & **100.00** & **97.78** & **100.00** & **98.25** & **77.38** & **100.00** & **100.00** \\\\ Lava-v1-5.5 [314] & 64.15 & 96.86 & 92.59 & 100.00 & 93.10 & 13.10 & **100.00** & 94.87 \\\\ Lava-v1-5-13b [141] & 62.26 & 97.91 & 91.11 & 100.00 & 96.55 & 11.9 & **100.00** & 97.44 \\\\ Quen-Plus (0-shot) [278] & 2.96 & 0.00 & 0.00 & 0.00 & 5.17 & 0.00 & 0.00 & 56.41 \\\\ Queen-max-(0-shot) [278] & 29.11 & 31.94 & 30.37 & 12.50 & 3.45 & 14.29 & 29.31 & 46.15 \\\\ Gemini (0-shot) [348] & 40.97 & 42.93 & 47.41 & 61.79 & 72.41 & 10.71 & 65.52 & 35.90 \\\\ GPT-4V (0-shot) [276] & 46.49 & 81.15 & 81.48 & 89.06 & 58.62 & 20.24 & 100.00 & 97.44 \\\\ \\hline MiniGFT-4 (Vicuna-7b) [356] & 11.78/31.78 & 0.68/1.59 & 12.54/58.89 & 4.78/87.82 & 0.61/61.15 & 14.45/47.53 & 28.48/34.69 & 37.48/53.05 \\\\ BLIP-2 (FianT5-x0) [364] & 12.49/84.03 & 15.11/84.69 & 0.08/2.14 & 1.75/96.84 & 0.00/0.00 & 9.92/75.89 & 11.73/45.55 & 17.26/**88.84** \\\\ Sphirx [376] & 44.76/66.69 & 22.13/79.69 & 37.84/73.07 & 39.88/70.62 & 20.68/86.57 & **83.93**/53.51 & 66.26/71.15 & 60.66/61.43 \\\\ Intermn [59] & 53.08/35.01 & 50.78/08.05 & **55.70**/50.58 & **57.43**/50.72 & **42.34**/50.87 & 71.21/42.34 & **73.86**/36.00 & **83.00**/19.69 \\\\ InstrucBLIP (FianT5-x0) [366] & 17.27/**87.62** & 26.02/**88.06** & 0.00/0.00 & 5.70/39.93 & 0.00/0.00 & 12.72/83.13 & 37.07/**82.85** & 49.18/81.28 \\\\ InstrucBLIP (FianT5-x0) [365] & 16.34/81.50 & 16.04/85.54 & 0.00/0.00 & 3.58/**98.31** & 0.00/0.00 & 13.26/**76.86** & 32.05/65.84 & 37.07/67.57 \\\\ Lava-v1-5.7b [141] & 46.81/81.83 & 23.27/77.63 & 36.56/72.97 & 38.76/66.47 & 9.08/91.56 & 63.01/54.70 & 80.14/48.66 & 69.85/32.92 \\\\ Lava-v1-5.13b [141] & 51.18/33.41 & 22.60/79.1 & 38.70/70.26 & 41.93/63.50 & 8.99/91.72 & 67.88/54.51 & 76.26/45.21 & 67.40/33.59 \\\\ Quen-Plus [278] & 30.46/46.78 & 27.42/82.37 & 10.59/8.46 & 6.16/16.10 & 1.32/64.62 & 7.593/58.46 & 46.83/50.41 & 33.71/60.56 \\\\ Quen-max [278] & 25.71/63.21 & 20.92/83.50 & 16.70/**76.00** & 1.63/95.70 & 1.12/**96.58** & 42.99/55.55 & 44.07/51.61 & 35.17/55.81 \\\\ Gemini (0-shot) [348] & 23.26/52.35 & 21.85/00.09 & 11.91/66.64 & 18.18/30.39 & **47.97/47.98** & 60.61/33.50 & 39.49/37.87 & 40.83/52.60 \\\\ GPT-4V (0-shot) [276] & 14.10/23.99 & 17.50/72.97 & 9.64/50.58 & 23.01/60.65 & 53.31/43.62 & 24.13/32.33 & 29.22/83.03 & 46.14/24.74 \\\\ GPT-4V (4-shot) [276] & 20.63/34.52 & 26.25/69.95 & 13.19/51.75 & 23.40/61.69 & 61.22/84.94 & 46.33/51.69 & 58.49/49.79 & 48.06/35.01 \\\\ \\hline \\multicolumn{8}{c}{_Accuracy on Specific Graph Theory Problems_} \\\\ \\hline MiniGFT-4 (Vicuna-7b) [356] & 50.67 & 48.69 & 0.00 & 0.00 & 0.00 & **5.95** & 0.00 & 0.00 \\\\ BLIP-2 (FianT5-x0) [64] & 46.63 & **61.26flatness of the loss landscape. The method is easy to integrate and outperforms traditional approaches comprehensively. Yan Fan et al. [166] proposed the Dynamic Subgraph Distillation (DSGD) method, which uses structural and semantic information for stable knowledge distillation. This approach enhances the model's robustness to distribution shifts and adapts to different supervision settings, addressing the practical deployment challenges in continual learning that arise from relying on a large number of labeled samples. Li Jiao et al. [167] proposed the VQ-Prompt method, which utilizes vector quantization to achieve end-to-end optimization of discrete prompt selection. They introduced gradient estimation, regularization terms, and representation statistics to stabilize task knowledge learning and improve continual learning performance. Ameya Prabhu et al. [168] proposed the RanDumb method, which uses random transformations and linear classifiers to investigate whether the representations produced by continual learning algorithms are truly effective in online continual learning. Yue Lu et al. [411] proposed two consistency conditions and an invariant prompt distribution constraint to reduce interference from new tasks on old tasks, overcoming catastrophic forgetting. Botos Csaba et al. [169] proposed the IWMS method, which addresses label delay by prioritizing the memory of samples similar to new data. This approach helps mitigate the label delay issue in online continual learning. Qiwei Li et al. [170] proposed the Progressive Prototype Evolution (PPE) method, which learns class prototypes during the online learning phase to alleviate forgetting. They introduced prototype similarity preservation and prototype-guided gradient constraint modules, effectively combating dual forgetting. Chengyi Yang et al. [171] proposed the Gradient Projection Common Null Space (GPCNS), which enhances plasticity by utilizing gradient information from old tasks. They integrated feature and gradient information through a collaborative framework, improving the performance of continual learning. Zeyang Zhang et al. [412] introduced a factor-based task-module router to optimize task routing and reduce forgetting. They designed an invariance-based architecture search mechanism to capture shared knowledge between tasks, enhancing knowledge sharing. This approach addresses the static assumptions and catastrophic forgetting issues in Graph Neural Architecture Search (GNAS) when handling continuous graph tasks. Jeevan Thapa et al. [413] proposed a non-parametric Bayesian method that infers network depth using a Beta process and adapts the width through a conjugate Bernoulli process. This approach enables joint inference of both network structure and weights, enhancing continual learning performance. Nicolas Michel et al. [414] proposed a new method based on momentum knowledge distillation, which dynamically updates the teacher model using exponential moving averages. This approach effectively overcomes the challenges of data stream processing and catastrophic forgetting in online continual learning. Yichen Wen et al. [172] proposed the CILA algorithm, which improves model performance in continual tasks through an adaptive distillation coefficient and theoretical performance guarantees. Yichen Wu et al. [173] proposed the POCL algorithm, which models task relationships through Pareto optimization and dynamically adjusts weights to reduce forgetting. Hongming Piao et al. [174] proposed the Powder algorithm, which enables prompt-based dual knowledge transfer. By selectively transferring knowledge based on task relevance, it reduces communication costs, addressing the challenge of cross-task and cross-client knowledge transfer in federated continual learning. Weichen Lin et al. [415] proposed the Dynamic Gradient Calibration (DGC) method, which effectively utilizes historical data to calibrate gradients. By combining it with existing continual learning methods, DGC helps alleviate the issue of catastrophic forgetting caused by data stream updates in continual learning. Doyoung Kim et al. [175] proposed an adaptive prompting method, AdaPromptCL, which effectively adapts to varying degrees of semantic change through dynamic semantic grouping and prompt adjustment. This approach addresses the challenge of task-specific semantic variations in continual learning that fixed prompting strategies face. Jason Yoo et al. [175] proposed the Layerwise Proximal Replay (LPR) method, which adjusts the optimization geometry to balance the learning of new and old data, enabling progressive changes. This approach reduces catastrophic forgetting and underfitting, improving the model's adaptability to both new and old data. Zhen Zhu et al. [416] proposed a dynamic weight prediction method and attention-weighted PCA feature compression, enabling efficient updates and storage compression in continual learning. This approach enhances model accuracy and flexibility. Yanshuo Liang et al. [176] proposed the InfLoRA method, which injects parameter reparameterization into pre-trained weights, effectively fine-tuning within a subspace. The method designs subspace elimination to prevent new tasks from interfering with old tasks, addressing the issue of forgetting old tasks when adapting to new tasks in continual learning. Chaoxi Niu et al. [417] proposed a Laplace smoothing-based graph task analysis and prompting method, which enables \\begin{table} \\begin{tabular}{l|c c c c|c c c c|c c c} \\hline Task Types \\(\\rightarrow\\) & \\multicolumn{4}{c|}{Connectivity} & \\multicolumn{4}{c|}{Cycle} & \\multicolumn{4}{c}{Shortest Path} \\\\ \\cline{2-13} Model & Easy & Medium & Hard & Avg. & Easy & Medium & Hard & Avg. & Easy & Hard & Avg. \\\\ \\hline MiniCPT-4 (Vicuna-7b) [356] & 60.71 & 53.57 & 52.94 & 54.45 & 36.00 & 51.40 & **59.32** & 51.83 & 0.00 & 0.00 & 0.00 \\\\ BLIP-2 (FanT5-x) [364] & 37.50 & 43.37 & **56.30** & 46.63 & **88.00** & **63.55** & 45.76 & **61.26** & 0.00 & 0.00 & 0.00 \\\\ InstruBLIP (FanT5-x) [365] & 46.43 & 46.43 & 53.78 & 48.79 & 36.00 & 50.47 & 45.76 & 47.12 & 0.00 & 0.00 & 0.00 \\\\ Sphins [376] & 39.29 & 45.41 & 52.10 & 46.63 & 64.00 & 49.53 & 54.24 & 52.88 & 6.00 & 0.00 & 3.12 \\\\ Intermm [359] & 78.57 & **66.33** & 52.10 & 52.94 & 52.00 & 55.14 & **59.32** & 56.02 & 0.00 & 0.00 & 0.00 \\\\ Llava-v1.5-7b [141] & 64.29 & 50.00 & 53.78 & 53.27 & 36.00 & 50.47 & 45.76 & 47.12 & 6.90 & 0.00 & 3.12 \\\\ Llava-v1.5-13b [141] & **71.43** & 44.94 & 49.58 & 52.83 & 36.00 & 50.47 & 45.76 & 47.12 & 10.34 & 0.00 & 4.69 \\\\ Gemini (ob-bb) [348] & 69.64 & 56.63 & 47.06 & 58.52 & 60.00 & _47.66_ & 45.76 & 48.69 & 0.00 & 0.00 & 0.00 \\\\ Gemini (DrB) [348] & 66.07 & 52.04 & 36.97 & 49.32 & 76.00 & 27.10 & 22.03 & 31.93 & 0.00 & 0.00 & 0.00 \\\\ Qwen-plus [278] & 62.50 & 56.63 & 47.06 & 54.45 & 64.00 & 49.53 & 54.24 & 52.88 & 0.00 & 0.00 & 0.00 \\\\ Qwen-max [278] & 62.50 & 56.63 & 46.22 & 54.18 & 64.00 & 49.53 & 54.24 & 52.88 & 0.00 & 0.00 & 0.00 \\\\ GPT-4V (ob-b) [276] & 69.64 & 42.86 & 17.65 & 38.81 & 60.00 & 48.60 & 45.76 & 49.21 & 6.90 & 0.00 & 3.12 \\\\ GPT-4V (2-ob) [276] & 67.86 & 56.12 & 47.06 & 54.98 & 64.00 & 48.60 & 54.24 & 52.35 & 13.79 & 0.00 & 6.25 \\\\ GPT-4V (0-COT) [276] & 64.29 & 34.69 & 7.56 & 30.45 & 64.00 & 47.66 & 49.15 & 50.26 & 17.24 & 0.00 & _7.69_ \\\\ GPT-4V (2-COT) [276] & 67.86 & 56.63 & 45.38 & 54.71 & 64.00 & 49.53 & 54.24 & 52.87 & 13.79 & 0.00 & 6.25 \\\\ GPT-4V (DPR) [276] & 92.86 & 58.67 & 36.97 & **56.87** & 76.00 & 48.60 & 45.76 & 51.30 & **24.14** & **2.86** & **12.50** \\\\ \\hline \\end{tabular} \\end{table} TABLE IV: Model performance on three common graph theory problems in VisionGraph. [375]accurate prediction of task IDs and learning of task-specific knowledge without the need for data replay. This approach effectively prevents forgetting and improves classification accuracy. Huiping Zhuang et al. [177] proposed a forward online analytical learning method, F-OAL, which does not rely on backpropagation. It updates the linear classifier using recursive least squares, helping to alleviate the issue of catastrophic forgetting in online class-incremental learning. Wuxuan Shi et al. [178] proposed Prospective Representation Learning (PRL), which aligns reserved space and latent space to adapt new class features to the reserved space. This method balances new and old classes, improving performance in non-sample class-incremental learning. Zitong Huang et al. [418] proposed the ACIL task and CBS strategy, which implement class balancing through clustering and greedy selection, enhancing performance in incremental learning. Xuze Hao et al. [179] proposed the CIL-balanced classification loss and distribution margin loss to reduce classifier bias and enhance class separability. This approach addresses the issue of catastrophic forgetting in class-incremental learning for medical image classification. Zhiwen Yang et al. [180] proposed the DSSP method, which leverages domain sharing and task-specific prompt learning, along with the S-Adapter to adapt to deep space variations. This approach eliminates the need for sample replay and effectively mitigates catastrophic forgetting. Shiye Wang et al. [419] proposed Shared Parameter Subspace Learning, which combines momentum updates and an importance-aware mechanism, along with cross-domain contrast and orthogonality constraints, to capture cross-domain shared information and reduce forgetting. Bowen Zheng et al. [181] proposed the MRFA method, which optimizes the entire layer margin by enhancing the features of review samples. By increasing the margin, this approach helps reduce catastrophic forgetting. Kishaan Jeeveswaran et al. [182] proposed the DARE method, which reduces representation drift through a three-stage training process. They introduced the IRS strategy to optimize buffer sampling, thereby improving the model's performance on old tasks. Dawei Zhou et al. [183] proposed the EASE method, which constructs task-specific subspaces using lightweight adapters and synthesizes new features for old classes by leveraging semantic information. This approach effectively alleviates catastrophic forgetting. Table II shows the results of Truth Alignment ability for different methods on the CoIN benchmark. These methods include multitask training, zero-shot learning, and fine-tuning. The table lists the performance of each method on individual tasks, as well as the average performance across all tasks, including metrics such as MAA, and BWT. Table II presents the results of Reasoning Capability for different methods on the CoIN benchmark. Similar to Table II, these results provide a comprehensive evaluation of the model's understanding and reasoning capabilities across different tasks. Table II explores the impact of different data volumes on MLLMs' instruction following ability on the CoIN benchmark. By randomly selecting varying proportions of samples from each dataset, Table II illustrates how the volume of data affects the model's performance."
    },
    {
      "title": "_Non-Large Language Model Multimodal Continual Learning_",
      "text": ""
    },
    {
      "title": "8.2.1 Framework Innovation",
      "text": "Bo Yuan et al. [191] proposed the CPP model for multi-task joint learning, which incorporates the CCE, TKD, and TPL mechanisms to achieve end-to-end multimodal general vision perception, significantly enhancing the efficiency of continual learning. Yu Feng et al. [192] proposed the CP-Prompt framework, which utilizes a dual-prompt strategy and parameter-efficient adjustments to achieve domain-specific knowledge extraction and inter-domain knowledge sharing, significantly reducing the forgetting rate. Xianghu Yue et al. [193] proposed the MMAL framework, which includes the modality fusion module and MSKC module. It effectively integrates audio-visual information without requiring samples, reducing forgetting and enhancing incremental learning performance. Yuchu Yu et al. [420] proposed a selective dual-teacher knowledge transfer framework, which utilizes unlabeled data to identify teacher networks, thereby ensuring knowledge retention and maintaining zero-shot capability. Xiang Chen et al. [194] proposed the MSPT framework, which optimizes multimodal learning through gradient modulation and attention distillation. It balances knowledge retention and new data integration, effectively mitigating catastrophic forgetting. Jiazuo Yu et al. [421] proposed a dynamic expansion framework based on MoE adapters and DDAS, enabling parameter-efficient and zero-shot continual learning. Yiwen Ye et al. [195] proposed MedCoSS, a staged multimodal self-supervised learning framework that avoids modality conflicts. It introduces rehearsal strategies and feature distillation, effectively preventing catastrophic forgetting and enhancing knowledge retention."
    },
    {
      "title": "8.2.2 Method Innovation",
      "text": "Jieren Deng et al. [196] proposed the ZiRa method, which effectively alleviates the challenge of adapting visual-language object detection models to new domains while retaining zero-shot generalization capabilities in incremental learning. This is achieved through zero-interference loss and a reparameterized dual-branch structure, without increasing memory burden. Tao Jin et al. [422] proposed a historical prompt calibration strategy, which includes intra-modal correlation estimation and inter-modal consistency alignment to calibrate prompts in pre-trained models. This enhances the task and modality relationships, addressing the issues of task unfamiliarity and modality heterogeneity in multimodal continual learning. Jaewo Lee et al. [197] proposed a localized patch importance scoring method, emphasizing the semantic interweaving of audio-visual patches. The replay-guided relevance assessment reduces forgetting of previously learned knowledge. Longrong Yang et al. [198] proposed the RCS-Prompt method, which reduces category space overlap and establishes clear boundaries between sessions through bidirectional prompt optimization and prompt magnitude normalization. This addresses the issue of overlap between old and new category spaces in continual learning. Zangwei Zheng et al. [199] proposed the ZSCL method, which mitigates forgetting through feature space distillation and parameter space weight integration. Kaiyang Zhou et al. [200] proposed the CoCoOp method, which generates dynamic prompts using a lightweight neural network to enhance model generalization. This addresses the issue of insufficient zero-shot generalization to unseen categories when pre-trained vision-language models adapt to new tasks. Martin Menabue et al. [423] proposed a dual-level prompt mechanism and semantic residual prompts, combined with multimodal generative replay, to enhance the stability and adaptability of models in continual learning. Yicheng Xu et al. [201] proposed the RAIL method, which uses recursive ridge regression and a no-training fusion module, along with the introduction of the X-TAIL setup, aiming to address the challenge of improving cross-domain classification capabilities in vision-language models during continual learning. Linlan Huang et al. [424] proposed an adaptive representation adjustment and parameter fusion method, which adjusts the representations of old categories affected by new categories using text features. Additionally, they employ a decomposition-based parameter fusion strategy to reduce forgetting. Through continuously innovative frameworks and methods, multimodal continual learning in non-large models has achieved a certain level of effective integration and learning across different modalities. However, with the diversification of data types and application scenarios, non-large model multimodal continual learning will face more complex tasks and dynamic environments, necessitating more flexible and efficient solutions."
    },
    {
      "title": "_Continual Learning In Large Language Model_",
      "text": ""
    },
    {
      "title": "8.3.1 Model Innovation",
      "text": "Yeongbin Seo et al. [245] proposed the TAALM method, which uses meta-learning to dynamically predict token importance, enabling targeted knowledge updates and reducing forgetting. Haoran Que et al. [246] proposed the D-CPT Law and Cross-Domain D-CPT Law, which predict the optimal training ratio to address the issue of selecting the mixed corpus ratio during continual pre-training of large language models. These methods reduce GPU resource consumption and improve domain adaptability. Srikanth Malla et al. [247] proposed the COPAL algorithm, which enables continual pruning without the need for retraining, thereby avoiding model retraining. This solution addresses the high computational demands and model adaptability limitations faced by large language models when adapting to new domains. Daniel Marczak et al. [248] proposed the MagMax method, which achieves effective cross-task knowledge integration through sequential fine-tuning and maximum magnitude weight selection. This approach mitigates the problem of catastrophic forgetting of old knowledge in large pre-trained models during continual learning, enabling adaptation to the continuously evolving data stream. Weixiang Zhao et al. [249] proposed the SAPT framework, which aligns the learning and selection of PET blocks through a shared attention mechanism. They introduced the ARM module to recall old tasks using pseudo-samples, enabling effective knowledge retention and transfer. Jianheng Huang et al. [250] proposed the SSR framework, which utilizes LLM-generated synthetic instances for rehearsal. This approach effectively mitigates forgetting, improves data \\begin{table} \\begin{tabular}{c|c|c c c c c c c|c c} \\hline \\hline \\multirow{2}{*}{**MLLM**} & \\multirow{2}{*}{**Method**} & \\multicolumn{8}{c|}{**Accuracy on Each Task**} & \\multicolumn{2}{c}{**Overall Results**} \\\\ \\cline{3-11} & & ScienceQA & TextVQA & ImageNet & GQA & VizWiz & Grounding & VQAV2 & OCR-VQA & MAA & BWT \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} LLAVA [141] \\\\ \\end{tabular} } & Multi-task & 56.77 & 49.35 & 95.55 & 56.65 & 53.90 & 30.09 & 59.50 & 55.65 & **57.18** & - \\\\ \\cline{2-11} & Zero-shot & 49.91 & 2.88 & 0.33 & 2.08 & 0.90 & 0.00 & 0.68 & 0.17 & 7.12 & - \\\\ \\cline{2-11} & Sequential & **82.45** & 49.99 & **96.05** & 56.40 & **55.45** & 31.27 & 62.20 & 57.08 & \\multirow{4}{*}{32.97} & \\multirow{4}{*}{-32.62} \\\\ & Finetune & 21.26 & 28.74 & 10.25 & 36.78 & 32.45 & 0.83 & 42.50 & 57.08 & \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} Qwen-VL [278] \\\\ \\end{tabular} } & Multi-task & 25.70 & 60.88 & 17.05 & 56.77 & 35.58 & 6.78 & 68.67 & **63.50** & 41.87 & - \\\\ \\cline{2-11} & Zero-shot & 64.56 & 48.15 & 11.82 & 44.50 & 5.97 & 0.00 & 64.10 & 27.50 & 33.78 & - \\\\ \\cline{2-11} & Sequential & 67.69 & 66.36 & 53.70 & **59.30** & 36.38 & **63.10** & **71.00** & 47.80 & & 43.35 & -16.94 \\\\ \\cline{2-11} & Finetune & 31.05 & 42.45 & 29.57 & 55.57 & 15.30 & 40.33 & 67.75 & 47.80 & & \\\\ \\hline \\multirow{4}{*}{ \\begin{tabular}{c} MiniGPT-v2 [357] \\\\ \\end{tabular} } & Multi-task & 43.55 & 19.24 & 10.57 & 28.43 & 41.62 & 0.00 & 27.12 & 1.45 & 21.50 & - \\\\ \\cline{2-11} & Zero-shot & 32.16 & 6.83 & 0.07 & 11.58 & 35.20 & 0.00 & 12.20 & 0.03 & 12.26 & - \\\\ \\cline{1-1} \\cline{2-11} & Sequential & 28.81 & 10.40 & 7.25 & 31.55 & 41.35 & 0.00 & 36.10 & 6.15 & \\multirow{4}{*}{25.45} & \\multirow{4}{*}{6.04} \\\\ \\cline{1-1} & Finetune & 44.35 & 29.89 & 11.90 & 36.95 & 42.58 & 0.00 & 38.10 & 6.15 & \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE II: The results evaluating the _Truth Alignment_ ability are presented below. The first line of **Sequential Finetune** are the results for each task evaluated when just tuned on the corresponding task, and the second line displays the final results of each task after fine-tuning on the last task. [94] \\begin{table} \\begin{tabular}{c|c|c|c|c} \\hline \\hline **Task** & **Dataset** & **Instruction** & **Train** & **Test** \\\\ \\hline \\multirow{2}{*}{**Grounding**} & RefCOCO & Please provide the bounding & \\multirow{2}{*}{55k} & \\multirow{2}{*}{31k} \\\\ & RefCOCO+ & box coordinate of the region & & \\\\ & RefCOCOg & this sentence describes & & \\\\ \\hline \\multirow{2}{*}{**Classification**} & ImageNet & What is the object in the image? & \\multirow{2}{*}{129k} & \\multirow{2}{*}{5k} \\\\ & & Answer the question using a & & single word or phrase & \\\\ \\hline **Image Question Answering (IQA)** & VQAv2 & Answer the question using a & & \\\\ & & single word or phrase & & \\\\ \\hline **Knowledge Grounded IQA** & ScienceQA & Answer with the option’s letter & \\multirow{2}{*}{12k} & \\multirow{2}{*}{4k} \\\\ & from the given choices directly & & \\\\ \\hline **Reading Comprehension IQA** & TextVQA & Answer the question using a & & \\\\ & & single word or phrase & & \\\\ \\hline **Visual Reasoning IQA** & GQA & Answer the question using a & & \\\\ & & single word or phrase & & \\\\ \\hline **Blind People** & VizWiz & Answer the question using a & & \\\\ & & single word or phrase & & \\\\ \\hline **OCR IQA** & OCR-VQA & Answer the question using a & & 165k & 100k \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE II: The results evaluating the _Truth Alignment_ ability are presented below. The first line of **Sequential Finetune** are the results for each task evaluated when just tuned on the corresponding task, and the second line displays the final results of each task after fine-tuning on the last task. [94]efficiency, and maintains the model's generalization ability. Shihan Dou et al. [251] proposed the LoRAMoE framework, which integrates LoRA and router networks, introducing local balance constraints to effectively mitigate the forgetting of world knowledge while enhancing multi-task handling capabilities. Shiwen Ni et al. [425] proposed the F-Learning paradigm, which first forgets old knowledge before learning new knowledge. Experiments show that it outperforms traditional fine-tuning, and the LoRA parameter reduction method achieves results comparable to full-parameter fine-tuning. Junhao Zheng et al. [426] proposed the SEQ method, which enhances the performance of LLMs in incremental learning through simple strategies, reducing both parameters and training time."
    },
    {
      "title": "8.3.2 Instruction Fine-Tuning",
      "text": "To mitigate catastrophic forgetting, Continual-T0 [427] uses a memory buffer for rehearsal [219], storing data from previous tasks and replaying them during training. ConTinTim [238] proposed InstructionSpeak, which includes two strategies that fully leverage task instructions to improve both forward and backward transfer. The first strategy involves learning from negative outputs, while the second focuses on revisiting the instructions of previous tasks. ELM [241] trains a small expert adapter for each task on top of the LLM. It then adopts a retrieval-based approach to select the most relevant LLM for each new task. Based on the parameter-efficient tuning (PET) framework, OLoRA [239] introduces orthogonal low-rank adaptation for CIT O-LoRA gradually learns new tasks in orthogonal subspaces while preserving the LoRA parameters learned from past tasks, thereby minimizing catastrophic forgetting. DAPT [240] introduces an innovative dual-attention framework, which coordinates the learning and selection of LoRA parameters through a dual-attention learning and selection module. LLMA PRO [242] introduces an innovative block expansion technique that allows new knowledge to be injected into the LLM while efficiently retaining the initial functionality through post-training. AdaptLLM [243] adapts the LLM to different domains by enriching the original training corpus with a series of content-related reading comprehension tasks. These tasks are designed to help the model leverage domain-specific knowledge while enhancing prompt performance. [428] designed an adapt-retrieve-revise process to enable the LLM to adapt to new domains. [429] analyzed LLMs that continuously adapt to different domains and found that the order of training data has a significant impact on the performance of LLMs. DynInst [244] proposes a hybrid approach that combines dynamic instruction replay with a local minima-inducing regularizer. These two components enhance the generalization of the LLM while reducing memory and computational usage in the replay module. \\begin{table} \\begin{tabular}{c|c c c c c c c c|c c} \\hline \\multirow{2}{*}{**MLLM**} & \\multirow{2}{*}{**Method**} & \\multicolumn{8}{c|}{**Accuracy on Each Task**} & \\multicolumn{2}{c}{**Overall Results**} \\\\ \\cline{3-11} & & ScienceQA & TextVQA & ImageNet & GQA & VizWiz & Grounding & VQAV2 & OCR-VQA & MAA & BWT \\\\ \\hline \\multirow{4}{*}{LLaVA [141]} & Multi-task & 80 & 75 & **97** & 72 & 42 & 86 & 73 & 79 & 75.50 & - \\\\ \\cline{2-11} & Zero-shot & 93 & **83** & 69 & 64 & 48 & 35 & 64 & 66 & 65.25 & - \\\\ \\cline{2-11} & Sequential & 92 & 75 & **97** & 72 & 42 & 58 & 75 & 78 & 71.28 & -10.88 \\\\ \\cline{2-11} & Finetune & 82 & 74 & 55 & 56 & 47 & 52 & 58 & 78 & 74.50 & - \\\\ \\hline \\multirow{4}{*}{Qwen-VL [278]} & Multi-task & **98** & 82 & 68 & 77 & 50 & 51 & **82** & **88** & 74.50 & - \\\\ \\cline{2-11} & Zero-shot & 97 & 81 & 78 & 74 & **54** & 58 & 81 & 74 & 74.63 & - \\\\ \\cline{2-11} & Sequential & 96 & **83** & 86 & **78** & 51 & 82 & **82** & 75 & **80.97** & -3.25 \\\\ \\cline{2-11} & Finetune & 95 & 78 & 77 & 77 & 47 & 76 & **82** & 75 & & \\\\ \\hline \\multirow{4}{*}{MiniGPT-v2 [357]} & Multi-task & 96 & 76 & 58 & 62 & 44 & 89 & 63 & 59 & 68.38 & - \\\\ \\cline{2-11} & Zero-shot & **98** & 72 & 48 & 63 & 48 & 80 & 64 & 61 & 66.75 & - \\\\ \\cline{1-1} \\cline{2-11} & Sequential & 97 & 71 & 55 & 61 & 44 & 91 & 63 & 52 & & \\\\ \\cline{1-1} \\cline{2-11} & Finetune & 89 & 73 & 59 & 60 & 44 & **94** & 63 & 52 & & \\\\ \\hline \\end{tabular} \\end{table} TABLE II: The evaluation results of _Reasoning Capability_ are presented below. [94] \\begin{table} \\begin{tabular}{c|c c c c c c c c|c c} \\hline \\multirow{2}{*}{**Volume**} & \\multicolumn{8}{c|}{**Accuracy on Each Task**} & \\multicolumn{2}{c}{**Overall Results**} \\\\ \\cline{2-11} & ScienceQA & TextVQA & ImageNet & GQA & VizWiz & Grounding & VQAV2 & OCR-VQA & MAA & BWT \\\\ \\hline \\multirow{2}{*}{0.1} & 70.00 & 42.88 & 93.45 & 36.93 & 43.7 & 3.73 & 40.48 & 45.62 & \\multirow{2}{*}{30.27} & \\multirow{2}{*}{-16.17} \\\\ & 53.71 & 32.62 & 5.38 & 33.50 & 36.98 & 2.85 & 36.77 & 45.62 & \\multirow{2}{*}{30.33} & \\multirow{2}{*}{-19.89} \\\\ \\hline \\multirow{2}{*}{0.2} & 69.86 & 46.86 & 94.38 & 44.98 & 44.15 & 4.81 & 32.55 & 52.10 & \\multirow{2}{*}{30.33} & \\multirow{2}{*}{-19.89} \\\\ & 41.12 & 33.25 & 5.53 & 33.80 & 25.85 & 1.77 & 37.10 & 45.62 & \\multirow{2}{*}{33.18} & \\multirow{2}{*}{-24.85} \\\\ \\hline \\multirow{2}{*}{0.4} & 75.33 & 47.06 & 94.95 & 52.95 & 50.77 & 10.25 & 56.73 & 55.33 & \\multirow{2}{*}{33.18} & \\multirow{2}{*}{-24.85} \\\\ & 49.96 & 23.60 & 7.22 & 36.12 & 33.05 & 0.09 & 39.20 & 55.33 & \\multirow{2}{*}{33.18} & \\multirow{2}{*}{-24.85} \\\\ \\hline \\multirow{2}{*}{0.6} & 78.09 & 47.65 & 95.85 & 55.93 & 53.08 & 10.00 & 59.17 & 46.33 & \\multirow{2}{*}{31.47} & \\multirow{2}{*}{-32.57} \\\\ & 27.42 & 19.54 & 7.03 & 33.52 & 13.15 & 0.05 & 38.48 & 46.33 & \\multirow{2}{*}{31.47} & \\multirow{2}{*}{-32.57} \\\\ \\hline \\multirow{2}{*}{0.8} & 80.02 & 48.13 & 95.45 & 54.00 & 49.85 & 28.33 & 58.35 & 56.67 & \\multirow{2}{*}{30.00} & \\multirow{2}{*}{-33.60} \\\\ & 11.74 & 16.94 & 8.85 & 32.62 & 35.50 & 0.00 & 39.67 & 56.67 & \\multirow{2}{*}{30.00} & \\multirow{2}{*}{-33.60} \\\\ \\hline \\multirow{2}{*}{1.0} & **82.45** & **49.99** & **96.05** & **56.40** & **55.45** & **31.27** & **62.20** & **57.08** & \\multirow{2}{*}{**32.97**} & \\multirow{2}{*}{-32.62} \\\\ & 21.26 & 28.74 & 10.25 & 36.78 & 32.45 & 0.83 & 42.50 & **57.08** & & \\\\ \\hline \\end{tabular} \\end{table} TABLE II: The results of LLaVA about **different data volumes** are presented below. [94]"
    },
    {
      "title": "9 Continual Learning In Multimodal Large Language Model",
      "text": ""
    },
    {
      "title": "_Benchmark_",
      "text": ""
    },
    {
      "title": "9.1.1 Coin: Continual Instruction Tuning Benchmark",
      "text": "MLLMs adapt to new tasks and users' evolving needs through instruction tuning. However, these models face challenges in adapting to the constantly changing knowledge requirements of users. To address this, Cheng Chen et al. [94] proposed the CoIN benchmark to evaluate MLLMs' performance under the sequential instruction tuning paradigm. They also introduced the MoELoRA method to help MLLMs retain previous instruction alignment, reducing catastrophic forgetting. CoIN consists of 10 commonly used datasets, covering 8 different task categories, ensuring diversity in both instructions and tasks. Table II provides a detailed list of the datasets included in the CoIN benchmark, along with their corresponding instruction types, training sample sizes, and test sample sizes. The datasets cover a variety of task types, including Referring Expression Comprehension (REC), Classification, Image Question Answering (IQA), and Knowledge Grounded IQA, among others. Each task has two versions of instructions, Type1 and Type2, to ensure the diversity and comprehensiveness of the evaluation. Furthermore, CoIN evaluates MLLMs from two perspectives: 1) Truth Alignment. The ability to generate the correct result in the desired format to follow task instrucuc- tion is the basic requirement for instruction tuning. 2) Reasoning Capability. The performance of MLLMs depends not only on the instruction following but also on the knowledge maintained in MLLMs. Three metrics are used to measure the performance of MLLMs: 1) Backward Transfer (BWT): Measures the catastrophic forgetting that occurs after learning all tasks. 2) Mean Average Accuracy (MAA): Assesses the model's performance throughout the entire training process."
    },
    {
      "title": "9.1.2 Climb: The Continual Learning In Multimodality Benchmark",
      "text": "Existing multimodal large language models are typically fine-tuned separately for each downstream task, requiring a new model to be fine-tuned and stored for each task. In contrast, multitask learning involves training on a fixed set of tasks, but it cannot dynamically learn new tasks. To address this, Tejas Srinivasan et al. [95] proposed the CLiMB benchmark, designed to study the continual learning challenges faced by multimodal large models in multimodal tasks and to systematically evaluate how upstream continual learning can quickly generalize to new multimodal and unimodal tasks. The CLiMB benchmark includes vision-and-language input tasks, such as VQAv2, NLVR2, SNLV-VE, and VCR. Additionally, the evaluation phase of the CLiMB benchmark includes: 1) Upstream Continual Learning: The model is trained on a series of vision-language tasks, and its ability to forget old tasks and transfer knowledge to new tasks is evaluated after each task. 2) Downstream Low-Shot Transfer: After training on upstream tasks, the model's adaptability to new multimodal and unimodal tasks with limited samples is assessed. Table II presents the results of different continual learning algorithms for multimodal large models in upstream multimodal task learning. It compares the upstream knowledge transfer (\\(\\text{T}_{\\text{L}\\text{U}\\text{E}}(i)\\)) relative to direct fine-tuning, along with the task scores \\([S_{i}^{s}]\\). Table III presents the Forgetting Transfer results for six continual learning algorithms applied to multimodal large models. It shows the performance degradation on previous tasks after training on subsequent tasks, indicating the extent of catastrophic forgetting. Table III illustrates the impact of different upstream task sequences on the upstream knowledge forgetting of multimodal large models."
    },
    {
      "title": "9.1.3 Coast: Continual Instruction Tuning Benchmark",
      "text": "An ideal MLLM should be able to continuously adjust to new tasks in the face of task flow distributions across different domains, new capabilities, and new datasets, while minimizing forgetting of prior knowledge. However, most existing MLLMs are limited to single-task adaptation and lack performance evaluation standards for continual learning of new tasks. To comprehensively assess MLLMs' continual learning performance across different domains, capabilities, and datasets, Meng Cao et al. [96] proposed the COAST benchmark. COAST includes three incremental learning settings: 1) Domain-incremental: Simulates scenarios where MLLMs continuously adapt to different domains. Capability-incremental: Evaluates the ability of MLLMs to progressively acquire and integrate new capabilities. 2) Dataset-incremental: Assesses the ability of MLLMs to adapt to and generalize across varying dataset distributions. 3) By chaining and reusing existing benchmark tests, the COAST benchmark creates a streaming task distribution to evaluate the performance of MLLMs when continually learning new tasks. Table III presents the average accuracy (Avg.\\(\\uparrow\\)) and average forgetting rate (Fgt.\\(\\downarrow\\)) of different continual learning methods under the COAST-domain setting. These results reflect the performance of multimodal large models on new tasks and their ability to retain performance on previous tasks while learning new ones. Table III presents the performance of different methods on the continual instruction tuning tasks under the COAST-capability setting, focusing on the ability of MLLMs to acquire and integrate new capabilities. The table categorizes tasks into Conv. (Conversation), Desc. (Detail Description), Reason (Complex Reasoning), and Ref. (Referring qa). Table III presents the performance of various methods on the continual instruction tuning task under the COAST-dataset setting, evaluating the ability of MLLMs to adapt to and generalize across dataset distributions. The terms \"SciQA,\" \"Text,\" \"ImgNet,\" \"GQA,\" \"Viz,\" \"REC,\" \"VQA,\" and \"OCR\" in the table represent different visual question answering datasets."
    },
    {
      "title": "9.1.4 Vilco-Bench: Video Language Continual Learning Benchmark",
      "text": "Multimodal large models in the domain of video-language continual learning involve the continuous adaptation to information from both video and text inputs, enhancing the model's ability to handle new tasks while retaining previous knowledge. This is a relatively under-explored field, and establishing appropriate benchmarks is crucial to promoting communication and research in this area. To address this, Tianqi Tang et al. [97] proposed the first benchmark specifically designed for video-language continual learning in multimodal large models, called ViLCo-Bench. This benchmark aims to evaluate continual learning models across a range of video-text tasks. ViLCo-Bench includes three unique video-language tasks: 1) Moment Queries (MQ). 2) Natural Language Queries (NLQ). 3) Visual Queries (VQ). These tasks require the model to understand video content and retrieve relevant segments of the video based on language queries. Table III presents the results of different continual learning methods on the MQ task. The evaluation used Average Recall, including R@1 and R@5 (IoU=0.3 and IoU=0.5), to measure the model's performance at different Intersection over Union (IoU) thresholds. Table III presents the results of various continual learning methods on the NLQ task. The NLQ task is more complex than the MQ task, as language queries are not limited to human activities but involve open-vocabulary descriptions. Table III presents the results of various continual learning methods on the VQ task. The VQ task requires the system to understand the visual content of the queried image. tAP (temporal Average Precision) is used as the performance metric, whichmeasures the distance between predicted and true locations in continuous tasks."
    },
    {
      "title": "_Framework Innovation_",
      "text": "Jiazuo Yu et al. [264] introduced the Adapter-in-Adapter framework to enhance modality alignment and collaboration. They also proposed a flexible and scalable framework, PathWeave, which incorporates modality path switching and expansion capabilities. This allows MLLMs to continuously evolve on the modality used for X-modality reasoning, addressing the high computational burden when expanding to new modalities and reducing the dependency on large-scale joint pre-training. Saurav Jha et al. [91] proposed the CLAP framework, which enhances the model's generalization ability and reduces forgetting through probabilistic fine-tuning. It is compatible with various prompt methods and strengthens the model's uncertainty estimation capabilities. Longxiang Tang et al. [265] proposed the DIKI framework, which efficiently preserves pre-trained knowledge through a residual mechanism and distribution-aware calibration. This approach addresses the problem of forgetting pre-trained knowledge in MLLMs during domain-category incremental learning, maintaining a balance between the model's adaptability to new tasks and the retention of old knowledge. Xusheng Cao et al. [266] proposed the GMM framework based on multimodal large models, which implements incremental learning through generated label text and feature matching. This approach reduces bias toward the current task and effectively minimizes forgetting. Keon-Hee Park et al. [267] proposed the PriViLege framework, which effectively addresses catastrophic forgetting and overfitting in MLLMs through prompt functionality and knowledge distillation. Fanhu Zeng et al. [268] proposed the ModalPrompt framework, which implements continuous learning without data replay through bi-modal guided prototype prompts and knowledge transfer. This approach addresses the issue of forgetting old tasks when large multimodal models sequentially learn new tasks. Emanuele Frascaroli et al. [269] proposed the CGIL framework, which combines prompt learning and latent generative replay. It uses VAEs to learn class-conditioned distributions and generate synthetic samples, effectively addressing the issue of catastrophic forgetting in multimodal large models during continual learning. Yukun Li et al. [270] proposed the CoLeCLIP framework, which enhances the performance of multimodal large models in open-domain continual learning through joint learning of task prompts and cross-domain vocabularies. It achieves cross-domain vocabulary learning, maintaining a unified semantic space for multimodal large models, and reduces interference between tasks. The framework introduces task prompt learning, addressing domain differences and category associations, thereby improving the model's adaptability and discriminative ability for new tasks. Biqing Qi et al. [100] proposed the ICL framework, which combines Vision Transformers (ViT) and MLLMs. By enabling interaction between a fast intuition model and a slow deep thinking model, the framework enhances the efficiency of continual learning in multimodal large language models. Yuexiang Zhai et al. [271] proposed the EMT framework to evaluate catastrophic forgetting in MLLMs. They found that moderate fine-tuning can improve continual learning performance, but excessive fine-tuning leads to a decline in performance and the emergence of hallucinations. This offers a new perspective for improving fine-tuning strategies in MLLMs. Xiong Wang et al. [99] proposed the Freeze-Omni model, which implements a three-stage training strategy to enable speech input-output capabilities without unfreezing the LLM parameters. This approach addresses the issue of catastrophic forgetting when integrating the speech modality into multimodal LLMs, preserving the LLM's intelligence level and enabling low-latency speech-to-speech conversations. Adyasha Maharana et al. [272] proposed the Adapt-\\(infty\\) framework, which optimizes model learning efficiency and reduces computational burden through dynamic data selection and a clustering-based permanent pruning strategy. This approach effectively mitigates catastrophic forgetting in multimodal large models. Gen Luo et al. [273] proposed Mono-InternVL, which integrates visual experts using a mixture-of-experts structure without altering the pre-trained language model. By introducing endogenous visual pretraining, it enables progressive learning of visual knowledge from noise to high-quality data through incremental learning, effectively preventing forgetting. This approach addresses the performance degradation and catastrophic forgetting issues that arise when expanding the visual and language capabilities of multimodal large language models. Shanshan Zhong et al. [274] proposed the MoExtend framework, which expands modality capabilities without adjusting the pre-trained model by integrating new experts. They designed a three-stage training process, including alignment, extension, and fine-tuning, to enable rapid modality adaptation. Additionally, they introduced an image localization score as a new scoring function to optimize multimodal sample selection. This approach addresses the issues of catastrophic forgetting and high training costs that arise when large language models are extended to multimodal tasks, particularly in the visual-language understanding domain. Artemis Panagopoulou et al. [89] addressed the challenges faced by multimodal large language models in continual learning, particularly in self-supervised pretraining environments. They focused on how to effectively integrate and reason across knowledge from different modalities to overcome the performance limitations of traditional methods when handling multimodal data. They proposed the HiDe-Prompt framework, which is an scalable solution designed to align multiple modal \\begin{table} \\begin{tabular}{l|r|r|r|r|r} \\hline \\hline \\multirow{2}{*}{Alg \\(\\mathcal{A}\\)} & \\multirow{2}{*}{Params} & Task 1 & Task 2 & Task 3 & Task 4 \\\\ & Trained & VQAv2 & NLVR2 & SNLI-VE & VCR \\\\ \\hline Direct FT & 100\\% & [67.70] & [73.07] & [76.31] & [61.31] \\\\ \\hline SeqFT [430] & 100\\% & 0.13\\% [67.79] & -1.80\\% [72.66] & -3.33\\% [74.89] & -5.09\\% [59.47] \\\\ Frozen Enc [95] & 7.88\\% & -14.10\\% [58.15] & -40.78\\% [63.66] & -15.98\\% [69.45] & -53.47\\% [41.90] \\\\ Frozen B9 [95] & 25.92\\% & -0.58\\% [67.30] & -0.58\\% [72.94] & -3.31\\% [74.90] & -15.49\\% [55.69] \\\\ ER [431] & 100\\% & 0.26\\% [67.87] & 0.56\\% [73.20] & -2.89\\% [75.08] & -4.45\\% [59.70] \\\\ EWC [226] & 100\\% & 0.20\\% [67.84] & -2.79\\% [72.39] & -4.52\\% [74.38] & -4.86\\% [59.55] \\\\ Adapters [28] & 13.02\\% & **0.59\\% [68.10]** & **2.55\\% [73.66]** & **-0.56\\% [76.08]** & **-0.36\\% [61.18]** \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE II: Upstream Knowledge Transfer \\(\\mathbb{T}_{\\mathit{U}k}(i)\\) relative to direct fine-tuning on each task, along with task score \\([S^{i}_{\\mathcal{A}}]\\) (%) for different CL algorithms \\(\\mathcal{A}\\) applied to ViLT. No CL algorithms achieve notable positive Knowledge Transfer, while the majority in fact _hurt_ learning of new tasks. [95]ities (such as images, 3D, audio, and video) with frozen large language models and enable cross-modal reasoning without joint optimization."
    },
    {
      "title": "_Method Innovation_",
      "text": "Minh Le et al. [90] revealed the connection between self-attention and mixture-of-experts, proposing the Non-linear Residual Gate (NoRGa) to enhance the continual learning performance of multimodal large language models. Zangwei Zheng et al. [296] proposed the ZAF method, which preserves knowledge through zero-shot stability regularization. They introduced the EMA-based parameter-efficient EMA-LoRA architecture, achieving the decoupling of learning and forgetting. Huancheng Chen et al. [92] proposed DualLoRA, which utilizes orthogonal and residual low-rank adapters along with a dynamic memory mechanism to balance model stability and plasticity, thereby improving the efficiency and effectiveness of continual learning in multimodal large language models. Weicai Yan et al. [297] proposed the Low-Rank Prompt Interaction (LPI) method, which enhances inter-modal and inter-task interactions through low-rank decomposition and contrastive learning. They introduced task semantic distance to guide prompt learning, addressing the insufficient interaction between modalities and tasks in continual learning of multimodal large language models (MLLMs), thereby reducing catastrophic forgetting. Didi Zhu et al. [298] proposed the Model Tailor method, which alleviates catastrophic forgetting during fine-tuning by retaining most of the pre-trained parameters and only replacing a small number of fine-tuned parameters. This approach helps to mitigate the forgetting problem while improving performance on new tasks. Tianxiang Hao et al. [437] proposed a quantized prompt technique, which uses quantization errors as a form of regularization. They designed an efficient quantization-aware training algorithm that enhances the model's generalization ability \\begin{table} \\begin{tabular}{l|c|c|c} \\hline \\hline Checkpoint Evaluated on & Task 1 & Task 2 & Task 3 \\\\ & VQAv2 & NLVR2 & SNLI-VE \\\\ \\hline After training on that task & [67.79] & [72.66] & [74.89] \\\\ \\hline Task 2: NLVR2 & 40.97\\% [40.02] & - & - \\\\ Task 3: SNLI-VE & 39.25\\% [41.18] & 43.81\\% [62.73] & - \\\\ Task 4: VCR & 63.90\\% [24.47] & 93.74\\% [51.24] & 89.93\\% [37.52] \\\\ \\hline \\hline \\end{tabular} \\begin{tabular}{l|c|c|c} \\hline \\hline Checkpoint Evaluated on & Task 1 & Task 2 & Task 3 \\\\ & VQAv2 & NLVR2 & SNLI-VE \\\\ \\hline After training on that task & [58.15] & [63.66] & [69.45] \\\\ \\hline Task 2: NLVR2 & -0.38\\% [58.37] & - & - \\\\ Task 3: SNLI-VE & -0.38\\% [58.37] & -0.31\\% [63.70] & - \\\\ Task 4: VCR & -0.38\\% [58.37] & -0.42\\% [63.72] & 0.00\\% [69.45] \\\\ \\hline \\hline \\end{tabular} \\begin{tabular}{l|c|c|c} \\hline \\hline Checkpoint Evaluated on & Task 1 & Task 2 & Task 3 \\\\ & VQAv2 & NLVR2 & SNLI-VE \\\\ \\hline After training on that task & [67.30] & [72.94] & [74.90] \\\\ \\hline Task 2: NLVR2 & 16.97\\% [55.90] & - & - \\\\ Task 3: SNLI-VE & 21.36\\% [52.93] & 29.32\\% [66.21] & - \\\\ Task 4: VCR & 71.61\\% [19.11] & 78.52\\% [54.93] & 35.01\\% [60.34] \\\\ \\hline \\hline \\end{tabular} \\begin{tabular}{l|c|c|c} \\hline \\hline Checkpoint Evaluated on & Task 1 & Task 2 & Task 3 \\\\ & VQAv2 & NLVR2 & SNLI-VE \\\\ \\hline After training on that task & [67.87] & [73.20] & [75.08] \\\\ \\hline Task 2: NLVR2 & 12.88\\% [59.13] & - & - \\\\ Task 3: SNLI-VE & 12.96\\% [59.07] & 17.10\\% [69.23] & - \\\\ Task 4: VCR & 43.62\\% [38.27] & 78.27\\% [55.04] & 33.45\\% [61.11] \\\\ \\hline \\hline \\end{tabular} \\begin{tabular}{l|c|c|c} \\hline \\hline Checkpoint Evaluated on & Task 1 & Task 2 & Task 3 \\\\ & VQAv2 & NLVR2 & SNLI-VE \\\\ \\hline After training on that task & [67.84] & [72.39] & [74.38] \\\\ \\hline Task 2: NLVR2 & 39.81\\% [40.83] & - & - \\\\ Task 3: SNLI-VE & 31.52\\% [46.46] & 25.73\\% [66.66] & - \\\\ Task 4: VCR & 65.25\\% [23.58] & 81.03\\% [54.25] & 73.61\\% [43.34] \\\\ \\hline \\hline \\end{tabular} \\begin{tabular}{l|c|c|c} \\hline \\hline Checkpoint Evaluated on & Task 1 & Task 2 & Task 3 \\\\ & VQAv2 & NLVR2 & SNLI-VE \\\\ \\hline After training on that task & [68.10] & [73.66] & [76.08] \\\\ \\hline Task 2: NLVR2 & -0.01\\% [68.11] & - & - \\\\ Task 3: SNLI-VE & 0.04\\% [68.07] & 3.51\\% [72.83] & - \\\\ Task 4: VCR & 0.67\\% [67.64] & 6.48\\% [72.13] & 0.89\\% [75.70] \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE III: Full numbers for forgetting transfer \\(\\mathbb{T}_{F}(j\\gets i)\\) of previously seen tasks for each CL algorithm. We also show the transfer score \\([S^{j\\gets i}_{\\mathcal{A}}]\\) when evaluated on that task after training on future task \\(i\\). The first row contains task score \\([S^{j}_{\\mathcal{A}}]\\) after originally training on \\(j^{th}\\) task. [95]while reducing its size. This approach addresses the issues of overfitting and catastrophic forgetting in MLLMs during downstream tasks, as well as the high storage and inference costs associated with large models. Noranart Vesdapunt et al. [93] proposed HVCLIP, which transforms CLIP features into a high-dimensional vector space. Through strategies such as forgetting reduction, discrepancy reduction, and feature enhancement, HVCLIP addresses the catastrophic forgetting issue encountered during fine-tuning of MLLM pre-trained models like CLIP in unsupervised domain adaptation. This approach helps mitigate the loss of pre-trained knowledge, enhancing the model's ability to retain critical information while adapting to new tasks or domains. Meng Cao et al. [96] proposed a parameter-efficient tuning method that does not require rehearsal. This approach constructs intrinsic and contextual incremental embeddings to encode task-specific features and inter-task dependencies. By doing so, the model can continuously adapt to new tasks while retaining prior knowledge. This significantly alleviates the catastrophic forgetting problem in MLLMs, enhancing their ability to preserve knowledge from previous tasks while accommodating new ones. Shikhar Srivastava et al. [438] proposed and evaluated five MLLM continual learning methods aimed at mitigating linguistic forgetting. Their findings revealed that the best-performing method significantly enhanced both language and vision task performance while maintaining multimodal accuracy. Jingyang Qiao et al. [299] proposed the LLACA method, which dynamically adjusts the EMA weights to reduce forgetting and introduces an approximation mechanism to lower computational costs, thereby addressing the issue of catastrophic forgetting in MLLMs when learning new tasks. \\begin{table} \\begin{tabular}{l|c c c c c c} \\hline \\hline **Methods** & **Params** & **Avg.** & **Fgt.** & **ChartQA** & **DocVQA** & **IconQA** & **MedicalQA** \\\\ \\hline Joint [96] & 6.76B & **42.79** & **—** & **21.99** & **20.08** & **64.37** & **64.73** \\\\ CODA [432] & 0.75M & 36.06 & **2.72** & 15.03 & 16.93 & 58.96 & 53.33 \\\\ Dual [433] & 0.75M & 35.80 & 2.79 & 14.92 & 16.77 & 58.60 & 52.92 \\\\ L2P [58] & 0.75M & 35.06 & 2.91 & 14.77 & 16.73 & 57.55 & 51.20 \\\\ LWF [202] & 6.76B & 27.06 & 15.05 & 14.07 & 13.19 & 37.93 & 43.05 \\\\ EVC [226] & 6.76B & 25.82 & 15.23 & 13.73 & 11.89 & 35.12 & 42.53 \\\\ Reh. [343] & 6.76B & 24.92 & 15.61 & 13.10 & 11.20 & 34.83 & 40.53 \\\\ Seq. [96] & 6.76B & 24.02 & 15.83 & 11.77 & 11.29 & 33.73 & 39.27 \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE III: Full forgetting results with different task orders. [95] Clea Rebillard et al. [300] proposed the Continual Visual Mapping (CVM) method, which reduces forgetting and improves generalization by mapping the representations of small visual models to the knowledge space of a fixed large language model. Marco Mistretta et al. [301] proposed the RE-tune method, which freezes the backbone of the model and trains adapters, using text prompts to guide training. This approach enables privacy-preserving, computationally efficient, and anti-forgetting incremental learning. It optimizes pre-trained multimodal biomedical models for incremental learning scenarios in chest X-ray multi-label classification, addressing challenges related to computational resources, data privacy, and catastrophic forgetting. Yuliang Cai et al. [302] proposed the CluMo method, which employs a two-stage training and modality fusion prompt strategy to combine visual and textual modalities, thereby enhancing the performance of multimodal large models in continual learning and improving their ability to retain old knowledge. Yiduo Guo et al. [439] proposed three strategies to overcome the stability gap, including multi-round pretraining on small-scale high-quality datasets, selecting high-quality sub-corpora for pretraining, and employing a data-mixing strategy using data similar to pretraining data. These strategies effectively enhanced the performance and adaptability of multimodal large language models in new domains. Jinghan He et al. [440] proposed a task similarity-guided regularization and model expansion method, which effectively enhances the continual learning capability of multimodal large models. Junhao Zheng et al. [303] proposed the Fwd-Prompt method, which utilizes gradient projection techniques and a multimodal prompt pool to achieve anti-forgetting and positive transfer, without requiring old samples and with minimal parameter updates. This approach improves the performance of multimodal large models in multimodal continual learning tasks. Yuliang Cai et al. [441] proposed dynamic model expansion and task attention layers to adapt to different tasks, while employing knowledge distillation and experience replay to mitigate catastrophic forgetting in multimodal large models. [304] proposed an incremental learning strategy for multimodal large language models, the CPE-CLIP method. By using learnable prompts and regularization strategies, it achieves parameter-efficient transfer learning for multimodal large language models, reducing the parameter size and training costs, while enhancing the performance of few-shot class incremental learning in multimodal large models. Zilun Zhang et al. [305] proposed the model-agnostic self-uncompression method, TG, which decompresses knowledge into the training corpus to reduce forgetting. They also designed the TG-SFT strategy for supervised fine-tuning of MLLMs, addressing the common issue of catastrophic forgetting encountered during post-training or supervised fine-tuning (SFT) on domain-specific data for multimodal large models. Ke Wang et al. [306] proposed the LiNeSt technique, which performs parameter updates with layer-specific depth differentiation, preserving the generalization ability of pretraining while improving fine-tuning task performance. This approach addresses the issue of forgetting prior knowledge during the fine-tuning of multimodal pre-trained models. Brian Lester et al. [294] proposed an end-to-end learning \\begin{table} \\begin{tabular}{l|c c|c c|c c c|c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Num. Task} & \\multirow{2}{*}{Mem. Capacity} & \\multirow{2}{*}{BwF\\(\\downarrow\\)} & \\multicolumn{3}{c|}{Avg R@1 (\\%)\\(\\uparrow\\)} & \\multicolumn{3}{c}{Avg R@5 (\\%)\\(\\uparrow\\)} \\\\ & & & IoU=0.3 & IoU=0.5 & mean & IoU=0.3 & IoU=0.5 & mean \\\\ \\hline Upper-Bound & None & None & None & 13.82 & 9.20 & 11.51 & 33.59 & 23.18 & 28.39 \\\\ \\hline Naive & 13 & None & 48.76 & 6.05 & 3.61 & 4.83 & 16.77 & 10.07 & 13.42 \\\\ EWC [226] & 13 & None & 50.05 & 6.34 & 4.05 & 5.20 & 19.50 & 12.08 & 15.79 \\\\ MAS [435] & 13 & None & 35.92 & 7.04 & 4.22 & 5.63 & 21.56 & 12.63 & 17.10 \\\\ VILCo [97] & 13 & 1010 & **10.60** & **9.49** & **6.21** & **7.85** & **25.52** & **16.36** & **20.94** \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE III: Results of Methods on Natural Language query. [97] \\begin{table} \\begin{tabular}{l|c c|c c c c c c c c} \\hline \\hline **Methods** & **Avg.1** & **Fgt.1** & **SciQA** & **Text** & **ImgNet** & **GQA** & **Viz** & **REC** & **VQA** & **OCR** \\\\ \\hline Joint [96] & 57.03 & — & 61.74 & 52.14 & 60.93 & 65.56 & 47.46 & 21.86 & 67.54 & 79.04 \\\\ CODA [432] & 50.27 & 9.70 & 54.80 & 44.55 & 53.64 & 58.43 & 39.07 & 14.97 & 62.63 & 74.08 \\\\ Dual [433] & 49.40 & 12.03 & 53.82 & 41.88 & 52.21 & 59.24 & 39.13 & 14.05 & 62.80 & 72.14 \\\\ L2P [58] & 49.01 & 12.12 & 53.13 & 41.64 & 51.69 & 58.96 & 38.90 & 13.78 & 62.22 & 71.78 \\\\ LIVE [202] & 26.41 & 36.94 & 52.40 & 30.02 & 23.99 & 27.30 & 14.65 & 3.43 & 35.13 & 24.32 \\\\ EWC [226] & 27.24 & 32.52 & 52.93 & 31.84 & 25.13 & 28.61 & 15.25 & 5.03 & 35.21 & 23.91 \\\\ Reh. [434] & 26.49 & 33.17 & 52.02 & 31.29 & 24.44 & 28.03 & 14.80 & 4.14 & 34.14 & 23.03 \\\\ Seq. [96] & 25.35 & 35.82 & 51.57 & 30.19 & 23.27 & 26.08 & 14.19 & 1.32 & 33.49 & 22.67 \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE III: **Evaluation results (%) of continual instruction tuning on COAST-dataset. \"Reh.\", \"Seq.\" and \"Joint\" denote rehearsal, sequential, and joint training. [96]** \\begin{table} \\begin{tabular}{l|c c c|c c c c|c c c} \\hline \\hline Method & Num. Task & Mem. Capacity & BwF\\(\\downarrow\\) & \\multicolumn{3}{c|}{Avg R@1 (\\%)\\(\\uparrow\\)} & \\multicolumn{3}{c}{Avg R@5 (\\%)\\(\\uparrow\\)} \\\\ & & & IoU=0.3 & IoU=0.5 & mean & IoU=0.3 & IoU=0.5 & mean \\\\ \\hline Upper-Bound & None & None & None & 19.62\\(\\pm\\)0.09 & 38.71\\(\\pm\\)0.02 & 43.39 & 67.30\\(\\pm\\)0.03 & 56.87\\(\\pm\\)0.05 & 62.09 \\\\ Lower-Bound & None & None & None & 19.62\\(\\pm\\)0.25 & 10.87\\(\\pm\\)0.06 & 15.25 & 31.61\\(\\pm\\)0.76 & 19.11\\(\\pm\\)0.41 & 25.36 \\\\ \\hline EWC [226] & 5 & None & 24.2\\(\\pm\\)0.03 & 17.61\\(\\pm\\)0.57 & 12.51\\(\\pm\\)0.14 & 15.06 & 28.13\\(\\pm\\)0.03 & 22.33\\(\\pm\\)0.51 & 25.23 \\\\ MAS [435] & 5 & None & 11.5\\(\\pm\\)0.01 & 14.45\\(\\pm\\)0.01 & 9.88\\(\\pm\\)0.033 & 12.17 & 22.50\\(\\pm\\)0.06 & 16.89\\(\\pm\\)0.07 & 19.70 \\\\ iCARI [227] & 5 & 1010 & 4.6\\(\\pm\\)0.01 & 32.01\\(\\pm\\)0.14 & 23.66\\(\\pm\\)0.30 & 27.84 & 50.59\\(\\pm\\)0.12 & 39.68\\(\\pm\\)0.03 & 45.14 \\\\ BiC [436] & 5 & 1010 & 1.4\\(\\pm\\)0.001 & 5.28\\(\\pm\\)0.42 & 3.39\\(\\pm\\)0.09 & 4.34 & 6.90\\(\\pm\\)0.30 & 4.53\\(\\pm\\)0.003 & 5.72 \\\\ ViCo [97] & 5 & 1010 & 2.9\\(\\pm\\)0.09 & **33.58\\(\\pm\\)0.06** & **26.24\\(\\pm\\)0.04** & **29.91** & **53.75\\(\\pm\\)0.33** & **42.70\\(\\pm\\)0.006** & **48.23** \\\\ \\hline \\hline \\end{tabular} \\end{table} TABLE III: Results of Methods on Visual Query. [97]soft prompt method, which adapts to new tasks by adjusting input prompts rather than the entire model parameters. This approach enhances the performance and domain adaptability of multimodal large language models in continual learning. Runqi Wang et al. [307] proposed an non-incremental learning method based on CLIP, called AttriCLIP. This method adapts to new tasks using an attribute lexicon and textual prompts, without the need for additional memory data, thereby enhancing the generalization and continual learning capabilities of multimodal large models in multimodal tasks. Shipeng Yan et al. [235] introduced pseudo-text replay and multimodal knowledge distillation to enhance negative sample diversity, align predictions between old and new models, and improve the performance of multimodal large models in multimodal continual learning tasks. Andrea Cossu et al. [55] explored how multimodal large language models can reduce catastrophic forgetting in continual learning environments through continuous pretraining, while maintaining adaptability to new knowledge. They demonstrated the advantages of self-supervised pretraining in preserving old knowledge and proposed effective pretraining strategies. James Seale Smith et al. [308] proposed the C-LoRA method, which effectively mitigates catastrophic forgetting by performing continual adaptive low-rank adjustments in the cross-attention layers of multimodal large models. This approach adapts to new concepts through a self-regulating mechanism while preserving knowledge of old concepts. Tao He et al. [98] introduced a lifelong scene graph generation task and a knowledge-aware contextual prompt learning strategy, enabling the model to effectively retain old knowledge in incremental learning. This approach addresses the issue of updating and forgetting old and new knowledge in multimodal large models during scene graph generation tasks."
    }
  ]
}