{
  "title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models",
  "authors": [
    "Stefan Hegselmann",
    "Alejandro Buendia",
    "Hunter Lang",
    "Monica Agrawal",
    "Xiaoyi Jiang",
    "David Sontag"
  ],
  "abstract": "\n We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting. \n",
  "references": [
    {
      "id": null,
      "title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models",
      "authors": [
        "Stefan Hegselmann",
        "Alejandro Buendia",
        "Hunter Lang",
        "Monica Agrawal",
        "Xiaoyi Jiang",
        "David Sontag"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Large Language Models are Zero-Shot Clinical Information Extractors",
      "authors": [
        "M References Agrawal",
        "S Hegselmann",
        "H Lang",
        "Y Kim",
        "D Sontag"
      ],
      "year": "2022",
      "venue": "Large Language Models are Zero-Shot Clinical Information Extractors",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Tabnet: Attentive interpretable tabular learning",
      "authors": [
        "S Ö Arik",
        "T Pfister"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Improving palliative care with deep learning",
      "authors": [
        "A Avati",
        "K Jung",
        "S Harman",
        "L Downing",
        "A Ng",
        "N H Shah"
      ],
      "year": "2018",
      "venue": "BMC medical informatics and decision making",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "PromptSource: An integrated development environment and repository for natural language prompts",
      "authors": [
        "S Bach",
        "V Sanh",
        "Z X Yong",
        "A Webson",
        "C Raffel",
        "N V Nayak",
        "A Sharma",
        "T Kim",
        "M S Bari",
        "T Fevry",
        "Z Alyafeai",
        "M Dey",
        "A Santilli",
        "Z Sun",
        "S Bendavid",
        "C Xu",
        "G Chhablani",
        "H Wang",
        "J Fries",
        "M Al-Shaibani",
        "S Sharma",
        "U Thakker",
        "K Almubarak",
        "X Tang",
        "D Radev",
        "M T Jiang",
        ".-J",
        "A Rush"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Scarf: Self-supervised contrastive learning using random feature corruption",
      "authors": [
        "D Bahri",
        "H Jiang",
        "Y Tay",
        "D Metzler"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Tabtext: a systematic approach to aggregate knowledge across tabular data structures",
      "authors": [
        "D Bertsimas",
        "K V Carballo",
        "Y Ma",
        "L Na",
        "L Boussioux",
        "C Zeng",
        "L R Soenksen",
        "I Fuentes"
      ],
      "year": "2022",
      "venue": "Tabtext: a systematic approach to aggregate knowledge across tabular data structures",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Deep Neural Networks and Tabular Data: A Survey",
      "authors": [
        "V Borisov",
        "T Leemann",
        "K Seßler",
        "J Haug",
        "M Pawelczyk",
        "G Kasneci"
      ],
      "year": "2022",
      "venue": "Deep Neural Networks and Tabular Data: A Survey",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Language models are realistic tabular data generators",
      "authors": [
        "V Borisov",
        "K Seßler",
        "T Leemann",
        "M Pawelczyk",
        "G Kasneci"
      ],
      "year": "2022",
      "venue": "Language models are realistic tabular data generators",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J D Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "XGBoost: A Scalable Tree Boosting System",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Few-shot tabular data enrichment using fine-tuned transformer architectures",
      "authors": [
        "A Harari",
        "G Katz"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Tabpfn: A transformer that solves small tabular classification problems in a second",
      "authors": [
        "N Hollmann",
        "S Müller",
        "K Eggensperger",
        "F Hutter"
      ],
      "year": "2022",
      "venue": "Tabpfn: A transformer that solves small tabular classification problems in a second",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Machine Learning Core",
      "authors": [
        "S Horng"
      ],
      "year": "2022",
      "venue": "Machine Learning Core",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "TabTransformer: Tabular Data Modeling Using Contextual Embeddings",
      "authors": [
        "X Huang",
        "A Khetan",
        "M Cvitkovic",
        "Z Karnin"
      ],
      "year": "2020",
      "venue": "TabTransformer: Tabular Data Modeling Using Contextual Embeddings",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "A good prompt is worth millions of parameters? low-resource prompt-based learning for vision-language models",
      "authors": [
        "W Jin",
        "Y Cheng",
        "Y Shen",
        "W Chen",
        "X Ren"
      ],
      "year": "2022",
      "venue": "ACL 2022",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Well-tuned simple nets excel on tabular datasets",
      "authors": [
        "A Kadra",
        "M Lindauer",
        "F Hutter",
        "J Grabocka"
      ],
      "year": "2021",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "LightGBM: A Highly Efficient Gradient Boosting Decision Tree",
      "authors": [
        "G Ke",
        "Q Meng",
        "T Finley",
        "T Wang",
        "W Chen",
        "W Ma",
        "Q Ye",
        "T.-Y Liu"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Deep contextual clinical prediction with reverse distillation",
      "authors": [
        "R Kodialam",
        "R Boiarsky",
        "J Lim",
        "A Sai",
        "N Dixit",
        "D Sontag"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Deep Neural Decision Forests",
      "authors": [
        "P Kontschieder",
        "M Fiterau",
        "A Criminisi",
        "S R Bulo"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision (ICCV)",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Co-training improves prompt-based learning for large language models",
      "authors": [
        "H Lang",
        "M N Agrawal",
        "Y Kim",
        "D Sontag",
        "K Chaudhuri",
        "S Jegelka",
        "L Song",
        "C Szepesvari",
        "G Niu",
        "S Sabato"
      ],
      "year": "2022",
      "venue": "Proceedings of the 39th International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Transfer Learning with Deep Tabular Models",
      "authors": [
        "R Levin",
        "V Cherepanova",
        "A Schwarzschild",
        "A Bansal",
        "C B Bruss",
        "T Goldstein",
        "A G Wilson",
        "M Goldblum"
      ],
      "year": "2022",
      "venue": "Transfer Learning with Deep Tabular Models",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Deep entity matching with pre-trained language models",
      "authors": [
        "Y Li",
        "J Li",
        "Y Suhara",
        "A Doan",
        "W.-C Tan"
      ],
      "year": "2020",
      "venue": "Proc. VLDB Endow",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning",
      "authors": [
        "H Liu",
        "D Tam",
        "M Muqeeth",
        "J Mohta",
        "T Huang",
        "M Bansal",
        "C Raffel"
      ],
      "year": "2022",
      "venue": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "GPT Understands, Too",
      "authors": [
        "X Liu",
        "Y Zheng",
        "Z Du",
        "M Ding",
        "Y Qian",
        "Z Yang",
        "J Tang"
      ],
      "year": "2021",
      "venue": "GPT Understands, Too",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint",
      "authors": [
        "S Min",
        "X Lyu",
        "A Holtzman",
        "M Artetxe",
        "M Lewis",
        "H Hajishirzi",
        "L Zettlemoyer"
      ],
      "year": "2022",
      "venue": "Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Can Foundation Models Wrangle Your Data?",
      "authors": [
        "A Narayan",
        "I Chami",
        "L Orr",
        "C Ré"
      ],
      "year": "2022",
      "venue": "Can Foundation Models Wrangle Your Data?",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C L Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray",
        "J Schulman",
        "J Hilton",
        "F Kelton",
        "L Miller",
        "M Simens",
        "A Askell",
        "P Welinder",
        "P Christiano",
        "J Leike",
        "R Lowe"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "True few-shot learning with language models",
      "authors": [
        "E Perez",
        "D Kiela",
        "K Cho"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Neural oblivious decision ensembles for deep learning on tabular data",
      "authors": [
        "S Popov",
        "S Morozov",
        "A Babenko"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Prompt programming for large language models: Beyond the few-shot paradigm",
      "authors": [
        "L Reynolds",
        "K Mcdonell"
      ],
      "year": "2021",
      "venue": "Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Explainable artificial intelligence for tabular data: A survey",
      "authors": [
        "M Sahakyan",
        "Z Aung",
        "T Rahwan"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Multitask prompted training enables zero-shot task generalization",
      "authors": [
        "V Sanh",
        "A Webson",
        "C Raffel",
        "S Bach",
        "L Sutawika",
        "Z Alyafeai",
        "A Chaffin",
        "A Stiegler",
        "A Raja",
        "M Dey",
        "M S Bari",
        "C Xu",
        "U Thakker",
        "S S Sharma",
        "E Szczechla",
        "T Kim",
        "G Chhablani",
        "N Nayak",
        "D Datta",
        "J Chang",
        "M T Jiang",
        "-J Wang",
        "H Manica",
        "M Shen",
        "S Yong",
        "Z X Pandey",
        "H Bawden",
        "R Wang",
        "T Neeraj",
        "T Rozen",
        "J Sharma",
        "A Santilli",
        "A Fevry",
        "T Fries",
        "J A Teehan",
        "R Scao",
        "T L Biderman",
        "S Gao",
        "L Wolf",
        "T Rush",
        "A M"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
      "authors": [
        "T Schick",
        "H Schütze"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter",
      "doi": ""
    },
    {
      "id": "b33",
      "title": ") benign essential hypertension 14095 -0.245 1.86 (1.72-2.01) finding of frequency of urination 14096 -0.255 1.48 (1.34-1.64) benign essential microscopic he",
      "authors": [],
      "year": "",
      "venue": ") benign essential hypertension 14095 -0.245 1.86 (1.72-2.01) finding of frequency of urination 14096 -0.255 1.48 (1.34-1.64) benign essential microscopic he",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Improving palliative care with deep learning",
      "authors": [
        "A Avati",
        "K Jung",
        "S Harman",
        "L Downing",
        "A Ng",
        "N H Shah"
      ],
      "year": "2018",
      "venue": "BMC medical informatics and decision making",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Deep Neural Networks and Tabular Data: A Survey",
      "authors": [
        "V Borisov",
        "T Leemann",
        "K Seßler",
        "J Haug",
        "M Pawelczyk",
        "G Kasneci"
      ],
      "year": "2022",
      "venue": "Deep Neural Networks and Tabular Data: A Survey",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J D Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Randomized trial of communication facilitators to reduce family distress and intensity of end-of-life care",
      "authors": [
        "J R Curtis",
        "P D Treece",
        "E L Nielsen",
        "J Gold",
        "P S Ciechanowski",
        "S E Shannon",
        "N Khandelwal",
        "J P Young",
        "R A Engelberg"
      ],
      "year": "2016",
      "venue": "American journal of respiratory and critical care medicine",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "International application of a new probability algorithm for the diagnosis of coronary artery disease",
      "authors": [
        "R Detrano",
        "A Janosi",
        "W Steinbrunn",
        "M Pfisterer",
        "J.-J Schmid",
        "S Sandhu",
        "K H Guppy",
        "S Lee",
        "V Froelicher"
      ],
      "year": "1989",
      "venue": "The American journal of cardiology",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "UCI machine learning repository",
      "authors": [
        "D Dua",
        "C Graff"
      ],
      "year": "2017",
      "venue": "UCI machine learning repository",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Why do tree-based models still outperform deep learning on typical tabular data?",
      "authors": [
        "L Grinsztajn",
        "E Oyallon",
        "G Varoquaux"
      ],
      "year": "2022",
      "venue": "Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Diabetes Prediction Using Ensembling of Different Machine Learning Classifiers",
      "authors": [
        "M K Hasan",
        "M A Alam",
        "D Das",
        "E Hossain",
        "M Hasan"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Observational Health Data Sciences and Informatics (OHDSI): Opportunities for Observational Researchers",
      "authors": [
        "G Hripcsak",
        "J D Duke",
        "N H Shah",
        "C G Reich",
        "V Huser",
        "M J Schuemie",
        "M A Suchard",
        "R W Park",
        "I C K Wong",
        "P R Rijnbeek",
        "J Van Der Lei",
        "N Pratt",
        "G N Li",
        "Y.-C Stang",
        "P E Madigan",
        "D Ryan",
        "P B"
      ],
      "year": "2015",
      "venue": "Observational Health Data Sciences and Informatics (OHDSI): Opportunities for Observational Researchers",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Well-tuned simple nets excel on tabular datasets",
      "authors": [
        "A Kadra",
        "M Lindauer",
        "F Hutter",
        "J Grabocka"
      ],
      "year": "2021",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid",
      "authors": [
        "R Kohavi"
      ],
      "year": "1996",
      "venue": "Kdd",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "A data-driven approach to predict the success of bank telemarketing",
      "authors": [
        "S Moro",
        "P Cortez",
        "Rita",
        "P"
      ],
      "year": "2014",
      "venue": "Decision Support Systems",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Early and accurate detection and diagnosis of heart disease using intelligent computational model",
      "authors": [
        "Y Muhammad",
        "M Tahir",
        "M Hayat",
        "K T Chong"
      ],
      "year": "2020",
      "venue": "Scientific Reports",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C L Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray",
        "J Schulman",
        "J Hilton",
        "F Kelton",
        "L Miller",
        "M Simens",
        "A Askell",
        "P Welinder",
        "P Christiano",
        "J Leike",
        "R Lowe"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Sparse spatial autoregressions",
      "authors": [
        "R K Pace",
        "R Barry"
      ],
      "year": "1997",
      "venue": "Sparse spatial autoregressions",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Scikitlearn: Machine learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "E Duchesnay"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Multitask prompted training enables zero-shot task generalization",
      "authors": [
        "V Sanh",
        "A Webson",
        "C Raffel",
        "S Bach",
        "L Sutawika",
        "Z Alyafeai",
        "A Chaffin",
        "A Stiegler",
        "A Raja",
        "M Dey",
        "M S Bari",
        "C Xu",
        "U Thakker",
        "S S Sharma",
        "E Szczechla",
        "T Kim",
        "G Chhablani",
        "N Nayak",
        "D Datta",
        "J Chang",
        "M T Jiang",
        "-J Wang",
        "H Manica",
        "M Shen",
        "S Yong",
        "Z X Pandey",
        "H Bawden",
        "R Wang",
        "T Neeraj",
        "T Rozen",
        "J Sharma",
        "A Santilli",
        "A Fevry",
        "T Fries",
        "J A Teehan",
        "R Scao",
        "T L Biderman",
        "S Gao",
        "L Wolf",
        "T Rush",
        "A M"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Tabllm: Few-Shot Classification Of Tabular Data With Large Language Models",
      "text": "Stefan Hegselmann\\({}^{1,2}\\) Alejandro Buendia\\({}^{1}\\) Hunter Lang\\({}^{1}\\) Monica Agrawal\\({}^{1}\\) Xiaoyi Jiang\\({}^{2}\\) David Sontag\\({}^{1}\\) \\({}^{1}\\) MIT CSAIL \\({}^{2}\\) University of Munster"
    },
    {
      "title": "Abstract",
      "text": "We study the application of large language models to zero-shot and few-shot classification of _tabular data_. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even _zero_-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting."
    },
    {
      "title": "1 Introduction",
      "text": "Many real world applications generate _tabular data_ as a natural byproduct of relational databases (Shwartz-Ziv and Armon, 2022). It is ubiquitous in domains ranging from healthcare to climate and finance (Sahakyan et al., 2021). Obtaining enough labeled data to train supervised learning algorithms for classification can be difficult. For example, in healthcare, there are 10,000 rare diseases (Haendel et al., 2020) affecting very few patients, which hampers the development of risk stratification models. Thus, we seek to develop methods that can exploit prior knowledge (e.g., from medical articles) to improve predictive performance in settings with a small number of training examples, i.e. the _few-shot_ setting. While deep learning has led to breakthroughs in computer vision and natural language processing, this success has not yet been extended to the tabular domain. For example, self-supervised deep learning methods have been introduced for tabular data (Yin et al., 2020; Arik and Pfister, 2021), but Grinsztajn et al. (2022) showed that these deep techniques still underperform ensembles of gradient boosted trees in the fully supervised setting. This disparity in performance can be attributed to the differences between tabular data and text or images; tabular data lacks locality, contains mixed data types, and the number of columns is usually fairly small compared to the number of features in text or image data (Borisov et al., 2022). Recently, large language models (LLMs) such as GPT-3, which are pre-trained on enormous corpora of text, have shown incredible performance on few-shot text classification and generation tasks (Brown et al., 2020; Sanh et al., 2022; Ouyang et al., 2022). These LLMs perform well on a variety of tasks and domains, including fact retrieval (Liu et al., 2021), mathematical reasoning (Wei et al., 2022), medical information extraction (Agrawal et al., 2022), and tabular data cleaning tasks (Narayan et al., 2022). Most importantly, because of all the knowledge encoded in their parameters, LLMs require little or no _labeled_ training data to obtain this good performance. In this work we introduce _TabLLM_, which is a general framework to leverage LLMs for few-shot _classification_ of tabular data. We prompt the LLM with a serialization of a row to a natural-language representation and a short description of the classification problem. For risk stratification, for instance, this serialization could list relevant patient attributes and combine it with, \"Will this patient be hospitalized?\". We experiment with nine different serializations and the T0 language model of different sizes (Sanh et al., 2022). We use the parameter-efficient fine-tuning method T-Few (Liu et al., 2022) to update the LLM's parameters using some labeled examples. We also evaluate GPT-3 in the zero-shot setting (Brown et al., 2020). To the best of our knowledge, this is one of the widest evaluations of LLMs for zero- and few-shot tabular classification. Despite its simplicity, we find that TabLLM outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. By using information from the natural-language column names and feature values, it often enables effective _zero_-shot classification of tabular data. Unlike many deep learning methods on tabular data, this approach is also competitive with gradient-boosted tree baselines and outperforms them or is on par until 256 shots. In the very-few-shot setting it outperforms them by a considerable margin. The main contributions of this work are: * We introduce TabLLM, a novel framework leveraging LLMs for data-efficient tabular classification * We study nine serialization techniques and explore their performance across ten different datasets * We show that TabLLM instantiated with a simple text serialization and the T0 LLM can outperform state-of-the-art neural models and tree ensembles in the zero- and few-shot setting * We investigate the application of TabLLM to a large real-world healthcare claims dataset and introduce serialization methods that deal with many input features"
    },
    {
      "title": "2 Related Work",
      "text": ""
    },
    {
      "title": "Machine Learning On Tabular Data",
      "text": "Due to the success of deep learning in other domains, there have been many recent attempts at representation learning for tabular data. Self-supervised objectives have largely revolved around the prediction of masked cells, the identification or correction of corrupted cells, and contrastive losses over augmentations (Bahri et al., 2022; Somepalli et al., 2021; Yoon et al., 2020; Arik and Pfister, 2021; Huang et al., 2020). Additional efforts have included differentiable trees, which combine advantages of tree ensembles with gradient based optimization of neural networks (Kontschieder et al., 2015; Popov et al., 2020). However, several recent comprehensive reviews (Shwartz-Ziv and Armon, 2022; Borisov et al., 2022; Grinsztajn et al., 2022) found that gradient-boosted tree ensembles like XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017) systematically outperform these novel deep learning architectures, even with proper fine-tuning and regularization (Kadra et al., 2021). Levin et al. (2022) found utility in transfer learning in the semi-supervised setting, but required a set of additional supervised tasks on the same table, which can be a nontrivial limitation. They investigate few-shot classification for medical diagnosis using 4 to 200 labeled examples, but do not exploit the power of large pre-trained models, as we do in this work. Hollmann et al. (2022) recently introduced TabPFN, a Bayesian neural network pre-trained on synthetic tabular data, outperforming gradient boosted trees in a comprehensive evaluation."
    },
    {
      "title": "Large Language Models For Tabular Data",
      "text": "Another approach has been to leverage the natural language capabilities of language models. Yin et al. (2020) use a language model for semantic parsing of natural language queries over tabular data. Li et al. (2020) investigate the ability of language models to perform entity matching on tabular data, i.e. determining if two rows refer to the same object. Harari and Katz (2022) study data enrichment by linking each table row with additional unstructured text (e.g., from Wikipedia) from which they generated addi Figure 1: Overview of TabLLM. We first serialize the feature names and values into a natural language string. We evaluate different strategies. This string is then combined with a task-specific prompt. To get predictions, we obtain output probabilities from the LLM for each of a pre-specified set of verbalizer tokens (e.g., “Yes”, “No”), which map to class labels (e.g., \\(1,-1\\)). If \\(k>0\\), we use the \\(k\\) labeled examples to fine-tune the large language model using T-Few (Liu et al., 2022). Finally, we use the (possibly tuned) large language model to obtain predictions on unlabeled examples. tional features using a language model. However, this setup requires named entities (e.g., celebrities, universities, etc.), which is quite limiting. Bertsimas et al. (2022) studied two healthcare datasets and used a language model to generate feature embeddings, which they fed into classifiers like gradient boosted trees. All these studies use a BERT-style language model (Devlin et al., 2019). Narayan et al. (2022) recently assessed in-context learning with the autoregressive language model GPT-3 for tabular data cleaning tasks. They found that it often outperforms state-of-the-art approaches with ten labeled examples. Borisov et al. (2022) introduced an LLM-agnostic method to generate realistic tabular data and found that it achieved better results than existing approaches. In contrast, here we study classification tasks of tabular data and investigate parameter-efficient fine-tuning of LLMs. To use an LLM for tabular data, the table must be serialized into a natural text representation. All aforementioned works relied on simple list or sentence serializations; Yin et al. (2020) also included the column data type in the serialized string. Only Bertsimas et al. (2022) studied different serialization variants, but this was in a different context of deriving feature embeddings from BERT-style language models. The LIFT method introduced by Dinh et al. (2022) comes closest to our work. The authors evaluated the capabilities of fine-tuned GPT-3 and GPT-J models for regression and classification on synthetic, tabular, and vision data. They also studied the sample efficiency and considered different static serialization templates assessing the effect of including column names in the input. In this work, we focus on the publicly available T0 model and perform a broader analysis of nine serialization techniques including automatic approaches and ablations evaluating the importance of feature values. Particularly, we are interested in leveraging prior knowledge encoded in LLMs and we do a more fine-grained analysis of the sample efficiency including zero-shot experiments on ten different datasets."
    },
    {
      "title": "3 Methods",
      "text": ""
    },
    {
      "title": "Tabllm For Tabular Data Classification",
      "text": "**Problem Formalization.** Suppose we have a tabular dataset with \\(n\\) rows and \\(d\\) columns or features. We can formalize this as \\(D=\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}\\), where each \\(\\mathbf{x}_{i}\\) is a \\(d\\)-dimensional feature vector. Since we consider classification, \\(y_{i}\\in C\\) for a set of classes \\(C\\). We define the column names or feature names as \\(F=\\{f_{1},...,f_{d}\\}\\). We assume the \\(f_{i}\\)'s are natural-language strings such as \"age\" or \"education\" (see Figure 1). For our \\(k\\)-shot classification experiments, we only use a subset \\(D_{k}\\) of size \\(k\\)--sampled from \\(D\\) with replacement--for fine-tuning or training. **Serialization of Tabular Data.** To use an LLM for tabular data, the table must be transformed into a natural text representation. Typically, when prompting an LLM, there is a template used to both serialize the inputs into one natural-language string, and to provide the prompt itself (e.g., the string _\"Does this person make more than 50,000 dollars? Yes or no?\"_), which is usually located after the serialized input. In this work, we break these pieces up into a _serialization_ and a _prompt_. We define a function \\(\\texttt{serialize}(F,\\mathbf{x})\\) that takes the column names \\(F\\) and feature values \\(\\mathbf{x}\\) for a row as inputs and creates a textual representation of the input. Combining this serialization with a task-specific prompt \\(p\\) will then form the LLM input (\\(\\texttt{serialize}(F,\\mathbf{x}),p\\)). This is illustrated in Figure 1. We primarily study the serialization, since that is the biggest difference compared to existing applications of prompting. Previous work has usually considered a simple concatenation of feature names and values as a serialization of tabular data (Li et al., 2020; Narayan et al., 2022). In our work, this function can be arbitrarily complex. For instance, we explore serializations that include (i) incorporating another LLM and (ii) employing feature selection as a substep. **Large Language Models For Classification** TabLLM can be used with different LLMs that generate text based on a natural-language input. Let LLM be an LLM with vocabulary \\(V\\). Then, \\(\\texttt{LLM}((\\texttt{serialize}(F,\\mathbf{x}),p))\\in V^{*}\\) is the prompted output of the LLM. In our few-shot setting, \\(\\{(\\texttt{serialize}(F,\\mathbf{x}),p)\\mid(\\mathbf{x},y)\\in D_{k}\\}\\) can be used as training examples for fine-tuning the LLM. The LLM generates text in the vocabulary space \\(V^{*}\\) that has to be mapped to a valid class in \\(C\\). Several approaches already exist for this problem. For example, the verbalizer (Schick and Schutze, 2021) defines a mapping between LLM output tokens and the discrete label space. Verbalizers can be manually specified or automatically learned; see Cui et al. (2022) for an overview of different verbalizer-learning approaches. In this work, we assume for simplicity that the verbalizer mapping is manually specified (see answer _choices in the templates in Sec. 8 in the Supplement)."
    },
    {
      "title": "Our Instantiation Of Tabllm",
      "text": "**Serialization Approaches for TabLLM.** The performance of LLMs is very sensitive to the precise details of the natural-language input (Zhao et al., 2021; Webson and Pavlick, 2022). In this work, we focus on the serialization of the tabular data. For the prompt, we use a simple description of the classification task and perform no further prompt engineering. We study nine different serialization formats varying in complexity. All serialization methods require minimal human effort to apply to new classification tasks. We evaluate several methods that generate natural text to create inputs that are closer to the training distribution of the LLM, thereby improving zero and very-few-shot performance. Additional details and examples for the serializations are given in Sec. 1.2.1 and 9 in the Supplement. * **List Template**: A list of column names and feature values. We fixed an arbitrary ordering of the columns. * **Text Template**: An textual enumeration of all features as \"The _column name_ is _value_.\" (see Figure 1). * **Table-To-Text**: We use an LLM fine-tuned on a table-to-text generation task from HuggingFace (Narrativaiai/bloom-560m-finetuned-totto -table-to-text). To ensure that the serialization includes all data we hand each column-value tuple to the model separately and concatenate the outputs. * **Text T0**: We use the LLM T0 with 11B parameters (bigscience/T0pp) (Sanh et al., 2022). We split up a row into pairs of two column-value tuples. We send them to LLM separately with the prompt \"Write this information as a sentence:\" and combine the outputs. * **Text GPT-3**: We use GPT-3 (engine _text-davinci-002_) accessible through an API (Ouyang et al., 2022). GPT-3 was able to serialize all features at once, so we use a list of all features with the prompt \"Rewrite all list items in the input as a natural text.\" as input. We guide the output with \"The {person, car, patient} is\". We consider the following serializations as ablations: * **List Only Values**: _List Template_ for feature values only. We want to evaluate whether column names aid the classification performance. * **List Permuted Names**: _List Template_ with permuted column names. Hence, the wrong column name is associated with each feature value. The permutation is the same across all examples. We perform this ablation to study the relevance of the correct association between column names and feature values. * **List Permuted Values**: _List Template_ with consistently permuted values across all examples. We generate one permutation for each column and apply this mapping to all column values. For continuous values, we use ten uniform bins. This tests whether the LLM uses the fine-grained information encoded by the feature _values_ for zero-shot and few-shot classification. * **List Short**: _List Template_ with at most ten features. We only consider this for the healthcare dataset where the number of features exceeds the input limit of the LLM. We want to study the effect of less information. Large Language Models for TabLLMAnother crucial component of TabLLM is the LLM. TabLLM is both agnostic to the LLM and the specific fine-tuning method that is used. We only consider a single LLM for most of our experiments. We employ the T0 encoder-decoder model with 11 billion parameters as the LLM for TabLLM (Sanh et al., 2022). It was trained on a large variety of task-specific prompts, making it a suitable candidate for our experiments (Sanh et al., 2022). This model has a token limit of 1024, which roughly corresponds to 400 words. We also evaluate the effect of a smaller version of the T0 model (T0 3B). We fine-tuned on the few-shot data \\(\\mathcal{D}_{k}\\) using the recent T-Few recipe, which outperforms other parameter-efficient tuning methods such as soft prompt tuning (Liu et al., 2022). In addition, we perform zero-shot experiments with the LLM GPT-3 (engine _text-davinci-002_) (Ouyang et al., 2022)."
    },
    {
      "title": "4 Experimental Setup",
      "text": ""
    },
    {
      "title": "Datasets",
      "text": "We studied TabLLM in two experimental settings. First, we considered nine medium-sized tabular datasets for binary and multi-class classification. We systematically identified datasets from Kadra et al. (2021), Grinsztajn et al. (2022), and Borisov et al. (2022). We included datasets with at most 50,000 rows to keep the fine-tuning costs manageable and at most 30 columns to stay within T0's token limit. We also required textual feature names to make the serializations more meaningful and we excluded datasets with derived feature values (e.g., mean pixel values). This lead to inclusion of **Bank** (45,211 rows, 16 feats), **Blood** (748, 4), **California** (20,640, 8), **Car** (1,728, 8), **Credit-g** (1,000, 20), **Income** (48,842, 14), and **Jungle** (44,819, 6). We added two additional datasets from Kaggle that fulfilled our inclusion criteria: **Diabetes** (768, 8) and **Heart** (918, 11). Second, we evaluated TabLLM for risk stratification on three binary classification tasks, following prior work by Kodialam et al. (2021) and similarly using a de-identified health claims dataset from a U.S. health insurer. We predicted the end-of-life (**EoL**) of all patients older than 70 years, which can be used to inform care in a palliative setting (Avati et al., 2018). We also considered the need for any surgical procedure (**Surgery**) and the likelihood of hospitalization (**LoH**), which can help with determining health care needs and estimating future costs. Additional details on all datasets can be found in Sec. 1 in the Supplement. We release the code for our experiments on Github.1 Footnote 1: [https://github.com/clinicalml/TabLLM](https://github.com/clinicalml/TabLLM)"
    },
    {
      "title": "Llm And Fine-Tuning",
      "text": "We used the HuggingFace implementation of the T0 model (bigscience/{T0pp,T0_3B}). Prompts for the LLM were designed following Sanh et al. (2022) using the PromptSource framework (Bach et al., 2022). Each class in our classification tasks was manually encoded in a textual response, e.g., \"Yes\" and \"No\" for true and false (Sanh et al., 2022). The prediction probability for each class corresponds to the probability of the LLM generating its token sequence normalized across all classes. All templates used in this work are given in Sec. 8 in the Supplement. For fine-tuning, we adopted the default hyperparameters of the T-Few method without any additional parameter tuning (Liu et al., 2022). The authors used a setup of \\(k=32\\) shots and 1,000 training steps for most of their experiments, which corresponds to 31.25 epochs. Hence, we fixed 30 training epochs for all few-shot experiments on the public tabular datasets. We used 20% of the data as a test set. For the large healthcare claims dataset, we used 10 epochs for up to 256 shots and 3 epochs for 1,024, 4,096 and 16,384 to reduce the runtime and prevent overfitting for many training examples. We used a test set of 10,000 examples for the three healthcare tasks. All experiments were evaluated with the area under the receiver operating characteristic curve (AUC). We used macro-AUC one-versus-rest for the multiclass setting. Estimates for the runtime are given in Sec. 2 in the Supplement."
    },
    {
      "title": "Baseline Models",
      "text": "We compared TabLLM to several baselines. For the simplest baseline, we used a logistic regression (LR) model. Since previous work showed the superiority of gradient boosted tree ensembles (Borisov et al., 2022), we included the most common models XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017). We also evaluated several state-of-the-art deep learning baselines. TabNet is a widely used neural model for tabular data that uses attention over columns (Arik and Pfister, 2021). SAINT is a more recent approach that uses attention over rows and columns (Somepalli et al., 2021). SAINT performed best in a comprehensive review on tabular data (Borisov et al., 2022). NODE is a differentiable tree ensemble method that performed best in the evaluation of Shwartz-Ziv and Armon (2022). Lastly, we include TabPFN, a Bayesian neural network that was pre-trained on synthetic tabular data (Hollmann et al., 2022). In contrast to TabLLM, we performed hyperparameter tuning for all baselines except TabPFN (see Sec. 3 in the Supplement), which requires no tuning by design. We adopted the parameter ranges from previous reviews (Borisov et al., 2022; Grinsztajn et al., 2022). Since no validation set exists in the few-shot setting, we used 4-fold cross validation on the \\(k\\)-shots. In particular, we _did not_ use a large validation set for hyperparameter tuning, unlike some few-shot learning works as highlighted by Perez et al. (2021). We encoded categorical values as one-hot vectors. We also tested ordinal encoding for LR, XGBoost, LightGBM, and TabPFN, but it showed worse results (see Table 12, 13, and 14 in the Supplement). In addition, we give results for GPT-3 (text-davinci-002) without fine-tuning, i.e. in the zero-shot setting using the _Text Template_ serialization. For the three health claims tasks, we used the same experimental setup for the baselines. However, we only included LR and LightGBM due to runtime limitations. Following Kodialam et al. (2021), each patient's input was a one-hot encoded vector. For each medical concept, there were three indicator variables of whether that concept occurred within 30 days, 1 year, and anytime before prediction time."
    },
    {
      "title": "Serializations",
      "text": "For the public datasets, some column names and feature values were manually mapped to human-readable forms, based on the provided documentation. For instance, for the **Income** dataset, the feature name _hours_per_week_ was mapped to _work hours per week_ and the feature value _private_ for working class was mapped to _private sector employee_. Numerical values were not changed. Serialization was more complex for the healthcare claims data. Each patient record is a time series of visits, with each visit consisting of a list of medical conditions and procedures. We only considered the manual serializations _List Template_ and _Text Template_. We tried to mimic the style of a medical professional to tap potential prior knowledge of the LLM. To this end, the serialization starts with an intro sentence containing the patient's gender, age, and race. It then describes each visit, stating its date, the type of doctor the patient saw (e.g., dermatology) if an outpatient visit or length of hospitalization if an inpatient visit, the primary complaint of the associated visit, and procedures performed. Since there are no feature values in this dataset, we omit _List Only Values_ and _List Permuted Values_. We also performed experiments for concept selection and different names for the medical concepts. Details for these additional experiments and examples of the serializations are given in Sec. 1.2.2, 1.2.3, and 9 in the Supplement."
    },
    {
      "title": "5 Results",
      "text": ""
    },
    {
      "title": "Effects Of Serialization",
      "text": "Figure 2 shows the performance of different serialization methods for TabLLM averaged over the nine public datasets. The _Text Template_ serialization performed very well across all experiments. In the zero-shot setting, the _Text Template_ showed improvements over _List Template_, indicating the benefit of a serialization that is closer to the training distribution of T0. However, these differences already vanished for 8 training examples. Hence, very few training examples might already suffice to adjust for different templates. This suggests that sophisticated serializations might be unnecessary when some training data exists. Using LLMs for serialization showed mixed results. The ordering is according to the complexity of the LLM used for serialization. GPT-3 has 175B, T0 11B, and the BLOOM table-to-text model 0.56B parameters. Different reasons might be responsible for the worse performance overall. The models tended to hallucinate information for some examples, leading to biased predictions of TabLLM. [MISSING_PAGE_FAIL:6] was on par or very close to the best baseline models on the full datasets, indicating that there is little performance lost due to the serialization and the choice of model family. **Introspecting TabLLM--What Prior Knowledge Does it Use?** Given the strong zero-shot performance of TabLLM on the **Income** dataset, we next sought to understand which features it based its predictions on in order to shed light on the prior knowledge used by the LLM. To determine the feature importance for TabLLM, we fit a LR model to the zero-shot prediction using the original features as covariates as described in Sec. 6 in the Supplement. Highly weighted features (see Table 2) for zero-shot TabLLM include the individual's occupation (with e.g., 'Farming-fishing' having a large negative weight), highest education level ('Masters' and 'Doctorate' have positive weights; 'Preschool' grade has a negative weight), and workclass ('Without-pay' has a negative weight). TabLLM also seems to be able to correctly interpret the numerically encoded capital gain value. For comparison, we also show the feature weights for a LR model trained on all data. We see a strong concordance between both models; TabLLM's top five features are all among the top seven of the LR model. However, TabLLM scores the highest education \\begin{table} \\begin{tabular}{l l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Dataset**} & \\multirow{2}{*}{**Method**} & \\multirow{2}{*}{**0**} & \\multicolumn{6}{c}{**Number of Shots**} \\\\ & & & **4** & **8** & **16** & **32** & **64** & **128** & **256** & **512** & **all** \\\\ \\hline \\multirow{3}{*}{Bank} & XGBoost & — & 0.50\\({}_{0.00}\\) & 0.56\\({}_{0.09}\\) & 0.68\\({}_{0.04}\\) & 0.76\\({}_{0.03}\\) & **0.83\\({}_{0.02}\\)** & 0.85\\({}_{0.03}\\) & 0.88\\({}_{0.01}\\) & 0.90\\({}_{0.01}\\) & **0.94\\({}_{0.00}\\)** \\\\ & TabPFN & — & 0.59\\({}_{1.4}\\) & **0.66\\({}_{0.08}\\)** & **0.69\\({}_{0.02}\\)** & 0.76\\({}_{0.03}\\) & 0.82\\({}_{0.03}\\) & **0.86\\({}_{0.02}\\)** & **0.89\\({}_{0.00}\\)** & 0.90\\({}_{0.00}\\) & 0.91\\({}_{0.00}\\) \\\\ & TabLLM & **0.63\\({}_{0.01}\\)** & 0.59\\({}_{1.0}\\) & 0.64\\({}_{0.05}\\) & 0.65\\({}_{0.05}\\) & 0.64\\({}_{0.06}\\) & 0.69\\({}_{0.03}\\) & 0.82\\({}_{0.05}\\) & 0.87\\({}_{0.01}\\) & 0.88\\({}_{0.01}\\) & 0.92 \\(\\uparrow\\) \\\\ \\hline \\multirow{3}{*}{Blood} & XGBoost & — & 0.50\\({}_{0.00}\\) & 0.58\\({}_{0.07}\\) & 0.66\\({}_{0.04}\\) & 0.67\\({}_{0.06}\\) & 0.68\\({}_{0.05}\\) & 0.71\\({}_{0.06}\\) & 0.70\\({}_{0.07}\\) & 0.67\\({}_{0.06}\\) & 0.71\\({}_{0.04}\\) \\\\ & TabPFN & — & 0.52\\({}_{0.08}\\) & 0.64\\({}_{0.04}\\) & **0.67\\({}_{0.01}\\)** & **0.70\\({}_{0.04}\\)** & **0.73\\({}_{0.04}\\)** & **0.75\\({}_{0.04}\\)** & **0.76\\({}_{0.04}\\)** & **0.76\\({}_{0.03}\\)** & **0.74\\({}_{0.03}\\)** \\\\ & TabLLM & **0.61\\({}_{0.04}\\)** & **0.58\\({}_{0.09}\\)** & **0.66\\({}_{0.03}\\)** & 0.66\\({}_{0.07}\\) & 0.68\\({}_{0.04}\\) & 0.68\\({}_{0.04}\\) & 0.68\\({}_{0.06}\\) & 0.70\\({}_{0.08}\\) & 0.68\\({}_{0.04}\\) & 0.70\\({}_{0.04}\\) \\\\ \\hline \\multirow{3}{*}{Calhousing} & XGBoost & — & 0.50\\({}_{0.00}\\) & 0.62\\({}_{1.0}\\) & 0.74\\({}_{0.03}\\) & 0.79\\({}_{0.04}\\) & 0.82\\({}_{0.04}\\) & 0.87\\({}_{0.01}\\) & 0.90\\({}_{0.01}\\) & 0.92\\({}_{0.01}\\) & **0.97\\({}_{0.00}\\)** \\\\ & TabPFN & — & 0.63\\({}_{1.13}\\) & **0.63\\({}_{1.1}\\)** & **0.80\\({}_{0.03}\\)** & **0.85\\({}_{0.03}\\)** & **0.89\\({}_{0.01}\\)** & **0.91\\({}_{0.01}\\)** & **0.92\\({}_{0.00}\\)** & **0.93\\({}_{0.00}\\)** & 0.94\\({}_{0.00}\\) \\\\ & TabLLM & **0.61\\({}_{0.01}\\)** & 0.63\\({}_{0.05}\\) & 0.60\\({}_{0.07}\\) & 0.70\\({}_{0.08}\\) & 0.77\\({}_{0.08}\\) & 0.77\\({}_{0.04}\\) & 0.81\\({}_{0.02}\\) & 0.83\\({}_{0.01}\\) & 0.86\\({}_{0.02}\\) & 0.95\\({}_{0.00}\\) \\\\ \\hline \\multirow{3}{*}{Car} & XGBoost & — & 0.50\\({}_{0.00}\\) & 0.59\\({}_{0.04}\\) & 0.70\\({}_{0.08}\\) & 0.82\\({}_{0.03}\\) & 0.91\\({}_{0.02}\\) & 0.95\\({}_{0.01}\\) & 0.98\\({}_{0.01}\\) & 0.99\\({}_{0.01}\\) & 1.00\\({}_{0.00}\\) \\\\ & TabPFN & — & 0.64\\({}_{0.06}\\) & 0.75\\({}_{0.05}\\) & **0.87\\({}_{0.04}\\)** & **0.92\\({}_{0.02}\\)** & **0.97\\({}_{0.00}\\)** & **0.99\\({}_{0.01}\\)** & **1.00\\({}_{0.00}\\)** & 1.00\\({}_{0.00}\\) \\\\ & TabLLM & **0.82\\({}_{0.02}\\)** & **0.83\\({}_{0.03}\\)** & **0.85\\({}_{0.03}\\)** & 0.86\\({}_{0.03}\\) & 0.91\\({}_{0.02}\\) & 0.96\\({}_{0.02}\\) & 0.98\\({}_{0.01}\\) & 0.99\\({}_{0.00}\\) & 1.00\\({}_{0.00}\\) & 1.00\\({}_{0.00}\\) \\\\degrees in the opposite order. Table 16 in the Supplement shows the importance of all 106 features."
    },
    {
      "title": "Large Healthcare Claims Dataset",
      "text": "Table 3 shows the results for TabLLM with the _List Template_ serialization on **EoL**, **Surgery**, and **LoH**, the three prediction tasks for the healthcare claims dataset. TabLLM showed very considerable zero-shot performance, ranging from 0.67 AUC for **Surgery** to 0.71 for **LoH**. The performance improves with higher number of training examples. However, the performance jumps happen at different steps and to a different extent. TabLLM outperformed LR for up to 16 (**Surgery** and **LoH**) to 64 (**EoL**) training examples and LightGBM for up to 64 (**LoH**) and 256 (**EoL**) examples. For more examples, LR and LightGBM performed slightly better. This could suggest that the information lost from our concept selection procedure, needed because of the token limits of the LLM, eventually starts costing TabLLM performance. We also evaluated TabLLM and LR in an unbalanced setting (see Table 15 in the Supplement). In this case, TabLLM outperforms LR up to 64 training examples on all datasets emphasizing its utility in a real world setting with limited access to labeled data. Introspecting TabLLM--What Prior Knowledge Does it Use?We also performed a feature analysis to study the strong zero-shot performance on **EoL**. However, we did not compare to a LR model trained on all data due to the vast amount of features and potential colinearites in the data. Instead, we compared to the relative risk (RR) with a 95% confidence interval (CI). Table 4 shows the five highest and lowest weighted features of zero-shot TabLLM and their relative risk for **EoL**. All top five features have a significantly increased relative risk demonstrating the capabilities of TabLLM to identify relevant features even without any training examples. For the five lowest weighted features, only'sex_female' has a significantly decreased risk. A list of 100 features is given in Table 17 in the Supplement."
    },
    {
      "title": "6 Discussion",
      "text": "For all datasets except **Credit-g** and **Heart**, the _List Template_ and _Text Template_ serializations showed nontrivial zero-shot performance, indicating that TabLLM is able to effectively utilize prior knowledge in the LLM for classification. Serializations with LLMs proved suboptimal due to their noisy outputs suggesting that simple templates are preferable for TabLLM. The performance drops observed when we removed or permuted the column names indicate that the LLM actually makes use of feature names and their relationships to the correct values, especially in the few-shot setting. These findings are partly consistent with Dinh et al. (2022) who used GPT-3 and tested serializations with removed or permuted column names. When using all training examples, they showed that using the correct column names led to the best performance on four classification tasks. In contrast to our results, however, they could not confirm these findings when using only a fraction (0.2, 0.4, 0.6, 0.8) of the training data. A reason for this could be that we tested much fewer number of training examples. In addition to that, we found a very strong drop in performance for permuted values showing that the LLM relies more on the correct values than feature names. Surprisingly, how \\begin{table} \\begin{tabular}{l l l} \\hline \\hline **Feature** & **TabLLM** & **RR (95\\% CI)** \\\\ \\hline atrial fibrillation & 0.633 & 2.72 (2.51-2.95) \\\\ atherosclerosis of coronary att... & 0.530 & 2.10 (1.94-2.27) \\\\ atherosclerosis of aorta & 0.473 & 1.99 (1.81-2.19) \\\\ exudative age-related macular d... & 0.452 & 2.38 (2.06-2.75) \\\\ sex\\_male & 0.442 & 1.23 (1.14-1.33) \\\\ open angle with borderline int... & -0.338 & 1.20 (1.03-1.40) \\\\ primary localized osteoarthritis... & -0.366 & 1.08 (0.82-1.43) \\\\ localized, primary osteoarthritis & -0.393 & 1.23 (1.07-1.40) \\\\ sex\\_female & -0.441 & 0.81 (0.75-0.88) \\\\ open-angle glaucoma - borderline & -0.495 & 0.97 (0.85-1.10) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Five highest and lowest weighted features for zero-shot TabLLM for **EoL** and their relative risk (RR) with confidence intervals (CI). The top five features show a significant increase of the relative risk. \\begin{table} \\begin{tabular}{l l l l l l l l l} \\hline \\hline \\multirow{2}{*}{**Dataset**} & \\multirow{2}{*}{**Method**} & \\multicolumn{6}{c}{**Number of Shots**} \\\\ & & **16** & **64** & **256** & **1,024** & **4,096** & **16,384** & **all** \\\\ \\hline \\multirow{2}{*}{EoL} & LR & — & 0.65.07 & 0.77.02 & **0.80.02** & **0.83.01** & **0.83.01** & **0.84.01** & **0.84.01** \\\\ & LightGBM & — & 0.50.00 & 0.71.01 & 0.76.02 & 0.80.01 & 0.82.01 & 0.83.01 & 0.82 \\(\\dagger\\) \\\\ & TabLLM & **0.70** & **0.74** & **0.78** & 0.78 & 0.79 & 0.81 & 0.81 & — \\\\ \\hline \\multirow{2}{*}{Surgery} & LR & — & 0.72.04 & **0.75.05** & 0.77.01 & 0.79.01 & 0.80.01 & 0.80.00 & 0.81.00 \\\\ & LightGBM & — & 0.50.00 & 0.73.02 & 0.77.01 & 0.79.01 & 0.80.00 & 0.81.01 & **0.82 \\(\\dagger\\)** \\\\ & TabLLM & **0.67** & **0.73** & 0.72 & 0.73 & 0.75 & 0.78 & 0.79 & — \\\\ \\hline \\multirow{2}{*}{LoH} & LR & — & 0.72.04 & **0.76.03** & **0.80.01** & **0.82.01** & 0.83.01 & 0.83.01 & 0.84.01 \\\\ & LightGBM & — & 0.50.00 & 0.72.02 & 0.76.03 & 0.81.01 & 0.83.00 & 0.83.01 & **0.85 \\(\\dagger\\)** \\\\ & TabLLM & **0.71** & **0.73** & 0.73 & 0.76 & 0.78 & 0.81 & 0.82 & — \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Test AUC on the healthcare claims dataset. TabLLM outperforms logistic regression (LR) for up to 64 and LightGBM for up 256 training examples on End of Life (**EoL**). Standard deviations are given across five random seeds. ever, all serializations with less information came close to the best serialization for 256 (tabular datasets) to 1024 training examples (insurance dataset). Hence, when hundreds of training examples are available, the input format proved less relevant, and the LLM was able to adapt (Jin et al., 2022). Like our results, Bertsimas et al. (2022) found that natural language representation of healthcare data gave little-to-no improvement (in their different setup) compared to a more straightforward serialization in the medium-shot setting. Our findings also support prior work showing that irrelevant and even misleading inputs can lead to similar few-shot performance (Min et al., 2022; Webson and Pavlick, 2022; Reynolds and McDonell, 2021). For instance, permuting the column names only showed a difference for up to 16 training examples (see Figure 2). We found clear performance improvements for TabLLM when using additional training examples. It often outperformed strong baseline models in the very-few-shot setting. This emphasizes the value of leveraging LLMs when only little labeled data is available. Surprisingly, Dinh et al. (2022) could not confirm these findings for GPT-3. On two binary classification tasks a fine-tuned GPT-3 model performed worse than LR for up to 250 training examples. Our results indicate that the sample efficiency of TabLLM is highly task-dependent. The performance on **Blood**, **Credit-g**, **Diabetes**, and **Heart** is worse than the performance on **Income** and **Car**. Most features of the latter datasets have semantically meaningful textual values likely boosting TabLLM's performance. However, TabLLM also achieved reasonable results on numerical datasets (**Blood**, **California**, **Diabetes**, and **Jungle**). In addition, **Diabetes** and **Heart** have somewhat specialized feature names and values, such as \"ventricular hypertrophy\" and \"Plasma glucose concentration,\" whereas **Income** and **Car** are more general-domain knowledge. This indicates that T0, the language model we used in TabLLM, seems to have less prior knowledge about medicine than about general-domain concepts. Indeed, the training tasks for T0 do not contain any tasks with medical data (Sanh et al., 2022). Our findings on the three insurance claims datasets partly reinforce this hypothesis. Zero-shot performance depends on the concept selection strategy and the LLM seems to have little knowledge about medical procedures. Prior work has shown that medical-domain-specific language models, such as PubMedBERT, and general-domain models with medical data in their training sets, such as GPT-3, perform well at downstream prediction tasks on medical data even with fairly few samples (Gu et al., 2021; Agrawal et al., 2022). Substituting T0 with one of these models in TabLLM to study medical predictions tasks is an interesting direction for future work. Our results on the public **Blood**, **Diabetes**, and **Heart** datasets are very similar to our results for **EoL**, **Surgery**, and **LoH**, which are practically relevant but rely on private data. Except for the zero-shot and very few-shot regime, other baselines tend to outperform TabLLM on these datasets. This suggests that **Blood**, **Diabetes**, and **Heart** datasets could be good proxies for the community to further study medical-domain tabular classification with LLMs without needing access to large private datasets."
    },
    {
      "title": "7 Limitations And Conclusion",
      "text": "TabLLM has a much larger computational footprint compared to traditional algorithms. It still requires fairly large GPUs to fine-tune the LLM, and inference with T0 requires far more FLOPs than inference with XGBoost or LR. Our results indicate that TabLLM trades off this computational efficiency for improved sample efficiency. Further, as we saw with the three healthcare claims tasks, performance may suffer if the dense feature set for a given row cannot fit within the token limit for a given LLM. Since the gains from TabLLM stem from its ability to use existing domain knowledge, the semantics of the column names and feature values need to have been observed during the LLM's original pre-training. For example, if the columns represent genes, we may not expect a vanilla LLM to have strong representations for gene names. Finally, due to dataset shift, the pre-training data for a given LLM may not necessarily reflect the settings under which a given table was aggregated, e.g., due to inflation and a changing value of money (see Sec. 5 in the Supplement). Despite these limitations, our empirical results show that TabLLM enjoys strong performance at tabular classification, outperforming state-of-the-art baseline algorithms like XGBoost and SAINT by over 5 AUC points in the very-few-shot regime, all while staying competitive with these methods when a large number of samples is available. Currently, TabLLM does not use any _unlabeled_ data; a fruitful direction could involve leveraging unlabeled data, e.g., using the techniques from Lang et al. (2022) to combine the few-shot performance of TabLLM with the ultimate performance of tree-based baselines by co-training the models together. Other improvements could include more faithful LLM serializations as well as numeric-specific encoding methods (Gorishniy et al., 2022)."
    },
    {
      "title": "8 Societal Impact",
      "text": "Similar to other ML systems that were trained on historic data, LLMs are prone to replicate existing biases and stereotypes. Hence, when applying TabLLM for sensitive tasks such as income or a health trajectory, predictions should be considered with great care and further analyses (e.g., for subgroups) are mandatory. In addition, LLMs require a lot of computing resources. This bears the risk of creating an exclusive research environment. Also, the environmental impact of LLMs can be significant."
    },
    {
      "title": "9 Acknowledgements",
      "text": "SH was supported by the German Academic Exchange Service, HL by NSF AiTF award CCF-1723344, MA by a Takeda Fellowship, and DS, HL, AB, and SH in part by Independence Blue Cross. Thanks to Dr. Steven Horng for generously donating GPU-time on the BIDMC computing cluster (Horng, 2022) and to NVIDIA Corporation for their donation of two NVIDIA A100 GPUs used in this work."
    },
    {
      "title": "References",
      "text": "* M. Agrawal, S. Hegselmann, H. Lang, Y. Kim, and D. Sontag (2022)Large Language Models are Zero-Shot Clinical Information Extractors. Technical Report arXiv:2205.12689, arXiv. Cited by: SS1. * S. O. Arik and T. Pfister (2021)Tabnet: attentive interpretable tabular learning. Proceedings of the AAAI Conference on Artificial Intelligence35 (8), pp. 6679-6687. Cited by: SS1. * A. Avati, K. Jung, S. Harman, L. Downing, A. Ng, and N. H. Shah (2018)Improving palliative care with deep learning. BMC medical informatics and decision making18 (4), pp. 55-64. Cited by: SS1. * S. Bach, V. Sanh, Z. X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry, Z. Alyafeai, M. S. Dey, A. Santilli, Z. Sun, S. Bendavid, C. Xu, G. Chhablani, H. Wang, J. Fries, M. Al-shaibani, S. Sharma, U. Thakker, A. Almubarak, X. Tang, D. Radev, M. T. Jiang, and A. Rush (2022)PromptSource: an integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Dublin, Ireland, pp. 93-104. Cited by: SS1. * D. Bahri, H. Jiang, Y. Tay, and D. Metzler (2022)Scarf: self-supervised contrastive learning using random feature corruption. In International Conference on Learning Representations, Cited by: SS1. * D. Bertsimas, K. V. Carballo, Y. Ma, L. Na, L. Boussioux, C. Zeng, L. R. Soenksen, and I. Fuentes (2022)Tabtext: a systematic approach to aggregate knowledge across tabular data structures. arXiv preprint arXiv:2206.10381. Cited by: SS1. * V. Borisov, T. Leemann, K. Sessler, J. Haug, M. Pawelczyk, and G. Kasneci (2022)Deep neural networks and tabular data: a survey. Technical report arXiv:2110.01889, arXiv. Cited by: SS1. * V. Borisov, K. Sessler, T. Leemann, M. Pawelczyk, and G. Kasneci (2022)Language models are realistic tabular data generators. arXiv preprint arXiv:2210.06280. Cited by: SS1. * T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020)Language models are few-shot learners. In Advances in Neural Information Processing Systems, Vol. 33, pp. 1877-1901. Cited by: SS1. * T. Chen and C. Guestrin (2016)XGBoost: a scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, New York, NY, USA, pp. 785-794. Cited by: SS1. * G. Cui, S. Hu, N. Ding, L. Huang, and Z. Liu (2022)Prototypical verbalizer for prompt-based few-shot tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, pp. 7014-7024. Cited by: SS1. * J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019)BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota, pp. 4171-4186. External Links: Link, Document Cited by: SS1. * T. Dinh, Y. Zeng, R. Zhang, Z. Lin, M. Gira, S. Rajput, S. yong Sohn, D. Papailiopoulos, and K. Lee (2022)LIFT: language-interfaced fine-tuning for non-language machine learning tasks. In Advances in Neural Information Processing Systems, H. Oh, A. H. Agarwal, D. Belgrave, and K. Cho (Eds.), pp.. External Links: Link Cited by: SS1. * Y. Gorishniy, I. Rubachev, and A. Babenko (2022)On embeddings for numerical features in tabular deep learning. arXiv preprint arXiv:2203.05556. Cited by: SS1. * L. Grinsztajn, E. Oyallon, and G. Varoquaux (2022)Why do tree-based models still outperform deep learning on typical tabular data?. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, Cited by: SS1. * Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon (2021)Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH)3 (1), pp. 1-23. Cited by: SS1. * M. Haendel, N. Vasilevsky, D. Unni, C. Bologa, N. Harris, H. Rehm, A. Hamosh, G. Baynam, T. Groza, J. McMurry, et al. (2020)How many rare diseases are there?. Nature Reviews Drug Discovery19 (2), pp. 77-78. Cited by: SS1. * Harari and Katz (2022) Harari, A. and Katz, G. (2022). Few-shot tabular data enrichment using fine-tuned transformer architectures. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1577-1591. * Hollmann et al. (2022) Hollmann, N., Muller, S., Eggensperger, K., and Hutter, F. (2022). Tabpfn: A transformer that solves small tabular classification problems in a second. _arXiv preprint arXiv:2207.01848_. * Horng (2022) Horng, S. (2022). Machine Learning Core. * Huang et al. (2020) Huang, X., Khetan, A., Cvitkovic, M., and Karnin, Z. (2020). TabTransformer: Tabular Data Modeling Using Contextual Embeddings. Technical Report arXiv:2012.06678, arXiv. * Jin et al. (2022) Jin, W., Cheng, Y., Shen, Y., Chen, W., and Ren, X. (2022). A good prompt is worth millions of parameters? low-resource prompt-based learning for vision-language models. In _ACL 2022_. * Kadra et al. (2021) Kadra, A., Lindauer, M., Hutter, F., and Grabocka, J. (2021). Well-tuned simple nets excel on tabular datasets. _Advances in neural information processing systems_, 34:23928-23941. * Ke et al. (2017) Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., and Liu, T.-Y. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc. * Kodialam et al. (2021) Kodialam, R., Boiarsky, R., Lim, J., Sai, A., Dixit, N., and Sontag, D. (2021). Deep contextual clinical prediction with reverse distillation. _Proceedings of the AAAI Conference on Artificial Intelligence_, 35(1):249-258. * Kontschieder et al. (2015) Kontschieder, P., Fiterau, M., Criminisi, A., and Bulo, S. R. (2015). Deep Neural Decision Forests. In _2015 IEEE International Conference on Computer Vision (ICCV)_, pages 1467-1475, Santiago, Chile. IEEE. * Lang et al. (2022) Lang, H., Agrawal, M. N., Kim, Y., and Sontag, D. (2022). Co-training improves prompt-based learning for large language models. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S., editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 11985-12003. PMLR. * Levin et al. (2022) Levin, R., Cherepanova, V., Schwarzschild, A., Bansal, A., Bruss, C. B., Goldstein, T., Wilson, A. G., and Goldblum, M. (2022). Transfer Learning with Deep Tabular Models. Technical Report arXiv:2206.15306, arXiv. * Li et al. (2020) Li, Y., Li, J., Suhara, Y., Doan, A., and Tan, W.-C. (2020). Deep entity matching with pre-trained language models. _Proc. VLDB Endow._, 14(1):50-60. * Liu et al. (2022) Liu, H., Tam, D., Muqeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. (2022). Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than InContext Learning. _arXiv:2205.05638 [cs]_. arXiv: 2205.05638. * Liu et al. (2021) Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J. (2021). GPT Understands, Too. Technical Report arXiv:2103.10385, arXiv. * Min et al. (2022) Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. (2022). Rethinking the role of demonstrations: What makes in-context learning work? _arXiv preprint arXiv:2202.12837_. * Narayan et al. (2022) Narayan, A., Chami, I., Orr, L., and Re, C. (2022). Can Foundation Models Wrangle Your Data? Technical Report arXiv:2205.09911, arXiv. * Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training language models to follow instructions with human feedback. _arXiv:2203.02155 [cs]_. arXiv: 2203.02155. * Perez et al. (2021) Perez, E., Kiela, D., and Cho, K. (2021). True few-shot learning with language models. _Advances in Neural Information Processing Systems_, 34:11054-11070. * Popov et al. (2020) Popov, S., Morozov, S., and Babenko, A. (2020). Neural oblivious decision ensembles for deep learning on tabular data. In _International Conference on Learning Representations_. * Reynolds and McDonell (2021) Reynolds, L. and McDonell, K. (2021). Prompt programming for large language models: Beyond the few-shot paradigm. In _Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems_, pages 1-7. * Sahakyan et al. (2021) Sahakyan, M., Aung, Z., and Rahwan, T. (2021). Explainable artificial intelligence for tabular data: A survey. _IEEE Access_, 9:135392-135422. * Sanh et al. (2022) Sanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Scao, T. L., Biderman, S., Gao, L., Wolf, T., and Rush, A. M. (2022). Multi-task prompted training enables zero-shot task generalization. In _International Conference on Learning Representations_. * Schick and Schutze (2021) Schick, T. and Schutze, H. (2021). Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference. In _Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume_, pages 255-269, Online. Association for Computational Linguistics. * Shwartz-Ziv and Armon (2022) Shwartz-Ziv, R. and Armon, A. (2022). Tabular data: Deep learning is not all you need. _Information Fusion_, 81. * Sompealli et al. (2021) Sompealli, G., Goldblum, M., Schwarzschild, A., Bruss, C. B., and Goldstein, T. (2021). SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training. Technical Report arXiv:2106.01342, arXiv. * Webson and Pavlick (2022) Webson, A. and Pavlick, E. (2022). Do Prompt-Based Models Really Understand the Meaning of Their Prompts? In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2300-2344, Seattle, United States. Association for Computational Linguistics. * Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models. _arXiv:2201.11903 [cs]_. arXiv: 2201.11903. * Yin et al. (2020) Yin, P., Neubig, G., Yih, W.-t., and Riedel, S. (2020). TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 8413-8426, Online. Association for Computational Linguistics. * Yoon et al. (2020) Yoon, J., Zhang, Y., Jordon, J., and van der Schaar, M. (2020). VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain. In _Advances in Neural Information Processing Systems_, volume 33, pages 11033-11043. Curran Associates, Inc. * Zhao et al. (2021) Zhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S. (2021). Calibrate Before Use: Improving Few-shot Performance of Language Models. In _Proceedings of the 38th International Conference on Machine Learning_, pages 12697-12706. PMLR. ISSN: 2640-3498. **Supplementary Materials:** **TabLLM: Few-shot Classification of Tabular Data with Large Language Models**"
    },
    {
      "title": "1 Additional Dataset Details",
      "text": ""
    },
    {
      "title": "Public Tabular Datasets",
      "text": "We systematically identified datasets for classification from Kadra et al. (2021), Grinsztajn et al. (2022), Borisov et al. (2022a), and from Kaggle. Each dataset was separated into \\(80/20\\) train-test splits. The \\(k\\) labeled examples \\(\\mathcal{D}_{k}\\) were sampled in a class-balanced manner from the training set. We performed experiments for different numbers of trainings examples (shots) ranging from 0 to 512 and the entire dataset (all). To characterize the sensitivity of models to the choice of \\(k\\) labeled examples, we repeated the dataset splitting and sampling procedures for five different seeds and report the mean AUC and standard deviation (SD) across seeds. No hyperparameter tuning was conducted for TabLLM; for baselines, internal cross validation was conducted to choose optimal hyperparameters, and the model was then retrained on all data. We analyzed the following datasets: * **Bank**(Kadra et al., 2021) contains information of a direct marketing campaign from a Portuguese banking institution (Moro et al., 2014). The goal is to predict whether a customer subscribed to a term deposit or not. It consists of 45,211 rows and 16 features; 5,289 labels are positive. * **Blood**(Kadra et al., 2021) consists of data of a blood transfusion service from Taiwan (Yeh et al., 2009). It contains 4 attributes of 748 donors and the label is representing whether they returned for another donation (178 positive). * **California**(Grinsztajn et al., 2022) contains eight attributes of 20,640 districts in California and the goal is to predict the median house value in each district (Pace and Barry, 1997). Analogously to Grinsztajn et al. (2022), we created a balanced classification task by predicting whether the house value is below or above the median (10,317 positive). * **Car**(Kadra et al., 2021) has entries for different cars that are characterized by six attributes; the task is a multiclass classification problem evaluating the state of each car. The dataset contains 1,728 rows, and the four classes have a distribution of 1210, 384, 65, and 69 examples. * **Credit-g**(Kadra et al., 2021) describes 1,000 people from Germany that want to receive a credit using 20 attributes. The label is to predict whether they have good or bad risk; 700 are classified as good. * **Diabetes** (from Kaggle2) was collected by the National Institute of Diabetes and Digestive and Kidney Diseases (Smith et al., 1988) and contains 768 rows, each corresponding to women of Pima Indian heritage with eight clinical variables. The task is binary classification of whether a person has diabetes; 268 cases are positive. Footnote 2: [https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database) (06/28/2022) * **Heart** (from Kaggle3) contains data of four different hospitals (Detrano et al., 1989). Each row contains 11 clinical variables of a patient. The task is binary classification of coronary artery disease. Of the 918 patients, 508 are positive. * **Income**(Kadra et al., 2021; Borisov et al., 2022a) also called Adult contains rows for 48,842 individuals with twelve attributes collected in the 1994 U.S. Census (Kohavi et al., 1996; Dua and Graff, 2017). The task is to predict whether each person has an annual income over 550,000. The dataset has 11,687 positive labels. * **Jungle**(Kadra et al., 2021) is a collection of 44,819 end game positions of Jungle Chess (van Rijn and Vis, 2014). Each game is described with 6 attributes and the goal is to predict whether the white player will win (23,062 positive)."
    },
    {
      "title": "Large Healthcare Claims Dataset",
      "text": "The de-identified health claims data set was provided by a large U.S. health insurer. The data is stored in the Observational Medical Outcomes Partnership (OMOP) Common Data Model version 6.0 (Hripcsak et al., 2015). It contains an entry for every encounter a patient has with the health system. Each entry is associated with a date, a visit type (5 total), a medical specialty (216 total), present conditions (14,095 total), and performed procedures (21,184 total). We additionally used the static concepts age, sex, and race at time of prediction. We studied three different tasks on this dataset with distinct cohorts. For all tasks, we used a six month outcome period and a gap of three months between time of prediction and the outcome window to prevent data leakage. We required patients to have at least one medical visit and to have been actively enrolled in an insurance plan for at least 95% of the last year and the six month outcome window. We used 10% of the data as a holdout set and sampled the \\(k\\) balanced shots with replacement from the remaining data. We chose larger shot sizes, as the tasks are more complex. We only ran the experiments for a single seed due to runtime limitations. We considered the following tasks: * **End of Life (EoL)**: We predicted the mortality of all patients older than 70 years. This is often used as a surrogate task. For instance, it can improve initiation of palliative care (Avati et al., 2018) and can help to inform close relatives to reduce family distress (Curtis et al., 2016). The final cohort contained 94,972 individuals; 2,424 were positive. * **Surgical Procedure (Surgery)**: We predicted the need for any surgical procedure. The task is important in determining health care needs and estimating costs. The cohort included 620,382 people of which 243,349 were positive. * **Likelihood of Hospitalization (LoH)**: We also predicted the likelihood of being hospitalized. Again, this information can help identify needs and estimate costs. The cohort included 612,656 individuals; 22,427 were positive."
    },
    {
      "title": "1.2.1 More Details On The Serialization",
      "text": "Each serialization begins with the patient's age, sex, and race. For each concept entry that we included, we also added information of the associated visit. This included its date, the type of doctor the patient saw (e.g., dermatology), if an outpatient visit or length of hospitalization if an inpatient visit, and the primary complaint of the associated visit. If a visit was already added to the serialization, we just added the concept to the existing visit entry. For the _List Template_ and _Text Template_ serializations approximately 40 medical concepts could be added until the token limit of T0 was reached. To explore the effect of fewer information in the input, we also tested the _List Short_ serializations were we added only 10 medical concepts to the serialization. Hence, not the entire token limit of the LLM was used. Examples of the _List Template_, _Text Template_ and _List Permuted Names_ serializations illustrating this structure are given in Sec. 9.1 at the end of the Supplement. \\begin{table} \\begin{tabular}{l c c c} \\hline \\hline **Method** & **EoL** & **Surgery** & **LoH** \\\\ \\hline Age, sex, and race & 0.59 & 0.57 & 0.65 \\\\ \\hline Least frequent conditions & 0.57 & 0.64 & 0.67 \\\\ Least frequent procedures & 0.59 & 0.59 & 0.65 \\\\ Least frequent concepts (cond. + proc.) & 0.55 & 0.55 & 0.66 \\\\ Most frequent conditions & 0.67 & 0.66 & 0.69 \\\\ Most frequent procedures & 0.59 & 0.58 & 0.65 \\\\ Most frequent concepts (cond. + proc.) & 0.62 & 0.61 & 0.65 \\\\ Oldest conditions & 0.65 & 0.66 & 0.69 \\\\ Oldest procedures & 0.59 & 0.58 & 0.65 \\\\ Oldest concepts (cond. + proc.) & 0.60 & 0.60 & 0.67 \\\\ Most recent conditions & 0.65 & 0.66 & 0.69 \\\\ Most recent procedures & 0.55 & 0.59 & 0.65 \\\\ Most recent concepts (cond. + proc.) & 0.59 & 0.60 & 0.66 \\\\ \\hline Most relevant concepts based on 256 shots* & 0.60 & 0.58 & 0.69 \\\\ Most relevant concepts based on 4096 shots* & 0.65 & 0.57 & 0.68 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: Evaluation of different concept selection methods for the healthcare claims dataset in the zero-shot setting. The last two rows show the performance when concepts where selected based on the lasso path of logistic regression weights, which violates the zero-shot assumption (*)."
    },
    {
      "title": "1.2.2 Concept Selection",
      "text": "For the healthcare claims dataset, the number of recorded medical concepts per patients usually exceeded T0's token limit. Hence, we had to determine which concepts of a patient should be included during the serialization. We evaluated four different concept selection strategies in the zero-shot setting for the _List Template_ serialization. Choosing the least frequent, most frequent, oldest, or most recent concepts per patient. We tested these for all concepts (conditions and procedures), only conditions, or only procedures. For each patient, we ranked all concepts according to one of the above methods and added concepts until the token limit of the LLM was reached. For least frequent and most frequent, we used the earliest visits associated with the selected medical concepts. We used a simple serialization that only contained the patient's age, sex, and race as a baseline for our experiments. We also tested concept selection based on the lasso path of a logistic regression model determined on 256 and 4,096 shots. This violates the few-shot assumption, but we considered it an interesting comparison with the other strategies that select concepts per patient. The results are given in Table 5. Using the most frequent conditions per patient consistently outperformed all other selection strategies. Frequent conditions might be useful since they reveal the most relevant condition of a patient. Also, they are usually more common allowing more prior knowledge of the LLM. Across all strategies conditions were usually more useful than procedures. This suggests more prior knowledge of conditions. Interestingly, selecting the most frequent conditions is even better than using the concept weights of a LR model trained on 256 or 4,096 shots."
    },
    {
      "title": "1.2.3 Alternative Concept Names",
      "text": "The healthcare claims dataset used SNOMED concept names for conditions and SNOMED, Healthcare Common Procedure Coding System (HCPCS), International Classification of Diseases (ICD), and Current Procedural Terminology (CPT) concept names for procedures. We tested different concept names to assess their effect on the performance. We used a zero \\begin{table} \\begin{tabular}{p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}} \\hline \\hline **Original name** & **ICD** & **MEDCIN** & **CHV** & **Simplify (GPT-3)** & **Jargon (GPT-3)** \\\\ \\hline Seasonal allergic rhinitis & Allergic rhinitis due to pollen & hay fever & hay fever & Allergies & Seasonal allergic rhinitis \\\\ \\hline Disturbance in speech & Unspecified speech disturbances & speech difficulties & speech impairment & Speech problems & Dysarthria \\\\ \\hline Congenital duplication of cervix & — & — & double cervix & Double cervix & Congenital duplication of the cervix \\\\ \\hline Hypertensive retinopathy & Hypertensive retinopathy & hypertensive retinopathy & hypertensive retinopathy & High blood pressure affecting the retina & Retinopathy h-tensa \\\\ \\hline Malignant neoplasm of liver unspecified & Malignant neoplasm of liver & of liver & liver & Liver cancer & Hepato-ca \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: Five examples of different concept names for conditions. The first column shows the original name in the healthcare claims dataset using SNOMED codes. A dash illustrates that no mapping was available. \\begin{table} \\begin{tabular}{p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}} \\hline \\hline **Method** & **EoL** & **Surgery** & **LoH** \\\\ \\hline Original concept names (SNOMED) & 0.67 & 0.66 & 0.69 \\\\ \\hline Map to ICD concept names & 0.67 & 0.67 & 0.68 \\\\ Map to MEDCIN concept names & 0.67 & 0.66 & 0.69 \\\\ Map to CHV concept names & 0.66 & 0.66 & 0.69 \\\\ Shorten longs concepts with GPT-3 & 0.67 & 0.66 & 0.69 \\\\ Simplify concepts with GPT-3 & 0.67 & 0.66 & 0.70 \\\\ Medical jargon with GPT-3 & 0.68 & 0.67 & 0.70 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 7: Evaluation of alternative condition concepts names. International Classification of Diseases (ICD), MEDCIN and the Consumer Health Vocabulary (CHV) are alternative medical terminologies. We also tested shortening, simplifying, and rewriting concepts as medical jargon via GPT-3. None of the alternative concept names showed consistent performance improvement. shot setting with the _List Template_ serialization and the most frequent conditions per patient as the best selection strategy determined as described above. Since the selection method only considered conditions, we only used different condition names. We considered three alternative vocabularies in the Unified Medical Language System (UMLS) that covered at least 20% of the condition concepts and offered different names. ICD is a very common medical terminology offering alternative names for conditions. MEDCIN and the Consumer Health Vocabulary (CHV) offer concept names specifically targeted at clinicians or consumers. We mapped the concept via their UMLS identifier. For ICD we were able to map 7,372, for MEDCIN 9,370 and for CHV 3,700 of the 14,095 condition concepts. Alternatively, we explored concept names generated by GPT-3 (Brown et al., 2020). To do so, we used the publicly accessible GPT-3 API (engine _text-davinci-002_) (Ouyang et al., 2022). We considered shortened names for concepts with more than sixty character (\"Rewrite this medical condition with at most six words.\"), simplified concept names (\"Write this medical condition in a short form in lay language.\") and medical jargon (\"Write this medical condition in medical jargon.\"). For the simplified names and the medical jargon, we provided GPT-3 with a single example for in-context learning. Examples for all alternative concept names except the shortening are given in Table 6. The results of this experiment are given in Table 7. We used the most frequent concept as a concept selection methods. Based on the best concept selection, we performed additional experiments for alternative concept names. We found no consistent performance difference even though there were considerable differences in the concept names (see Table 6). Surprisingly, TabLLM performs better for **EoL** and **Surgery** using medical jargon to encode concepts."
    },
    {
      "title": "2 Runtime Estimates For Tabllm",
      "text": "The TabLLM training time on the **Income** dataset for 64 training examples and 30 epochs with a batch size of 8 was less than 3 minutes. The average inference time for the test set of 10,000 examples with a batch size of 16 was 2 minutes, around 12 ms per example. The training and inference times for the other public datasets were comparable. Due to the larger size of the healthcare claims dataset, it took nearly 4 minutes to train for 64 examples and 10 epochs for **EoL** and was similar for the other two tasks. Inference took approximately 14 minutes for 10,000 examples with a batch size of 16, i.e. around 84 ms per example. The training times scaled linearly in the shot size."
    },
    {
      "title": "3 Parameter Tuning For Baselines",
      "text": "We used the scikit-learn framework to perform cross-validation and parameter tuning for the LR and the tree-based models (Pedregosa et al., 2011). For LR we tried common parameters for the penalty term and regularization strength (see Table 8). We used the same LR parameters for the public tabular datasets and the healthcare claims dataset. For the tree-based models we adopted the hyperparameter ranges from Borisov et al. (2022) and Grinsztajn et al. (2022). We discretized the \\begin{table} \\begin{tabular}{l l} \\hline \\hline **Parameter** & **Values** \\\\ \\hline max\\_depth & 2, 4, 6, 8, 10, 12 \\\\ lambda\\_11 & 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1. \\\\ lambda\\_12 & 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1. \\\\ eta & 0.01, 0.03, 0.1, 0.3 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 10: Hyperparameters for XGBoost model. \\begin{table} \\begin{tabular}{l l} \\hline \\hline **Parameter** & **Values** \\\\ \\hline penalty & ‘11’, ‘12’ \\\\ C & 100, 10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 8: Hyperparameters for LR model. \\begin{table} \\begin{tabular}{l l} \\hline \\hline **Parameter** & **Values** \\\\ \\hline num\\_leaves & 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096 \\\\ lambda\\_11 & 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1., 10. \\\\ lambda\\_12 & 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1., 10. \\\\ learning\\_rate & 0.01, 0.03, 0.1, 0.3 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 9: Hyperparameters for LightGBM model. parameter ranges and performed a complete grid search (see Tables 9 and 10). For the neural baselines SAINT, TabNet, and NODE, we used the setup and suggested hyperparameter ranges in Borisov et al. (2022). We modified the open-source implementation of these methods4 to support ingestion of the nine public tabular datasets. We used the hyperparameter-tuning framework Optuna5 and selected parameters that maximize AUC-ROC across folds. Note that for the 4-shot setting of the **Car** dataset, AUC may not be defined if the selected validation set includes only one label; in this case we used accuracy as our validation metric but report AUC-ROC on the holdout test set. Each neural baseline model was run for 20 trials with Optuna and trained for 100 epochs per hyperparameter settings. Footnote 4: [https://github.com/kathrinse/TabSurvey](https://github.com/kathrinse/TabSurvey) Footnote 5: [https://github.com/optuna/optuna](https://github.com/optuna/optuna)"
    },
    {
      "title": "4 Comparing Baseline Results To The Literature",
      "text": "To assess whether our baseline results match results reported in the literature, we report studies that used the same models. Bank Dataset.Kadra et al. (2021) trained a XGBoost, TabNet, and NODE baseline on this dataset and achieved a balanced accuracy of 72.7, 70.6, and 74.6. Our experiments for a set of 512 balanced training examples (512 shots) show a better performance for XGBoost than NODE. Blood Dataset.The XGBoost, TabNet, and NODE baselines trained in Kadra et al. (2021) achieved a balanced accuracy of 62.3, 64.3, 50. Our results for a set of 512 balanced training examples (512 shots) also show a better performance for TabNet than XGBoost. However, in our experiments NODE performs better than XGBoost and not worse. California Dataset.Borisov et al. (2022) trained a Linear Model, XGBoost, LightGBM, TabNet, NODE, and SAINT baseline on a regression version of the dataset. They achieved a mean squared error of 0.53, 0.21, 0.20, 0.35, 0.28, and 0.23. Our experiments for a set of 512 balanced training examples (512 shots) show a better performance for XGBoost than LightGBM and the same performance for TabNet and NODE. Also, our linear model performs much better which is probably due to more extensive hyperparameter tuning. Car Dataset.The XGBoost, TabNet, and NODE models in Kadra et al. (2021) showed a balanced accuracy of 92.4, 98.7, and 46.1. In our experiments, XGBoost and TabNet performed very similar for many training examples and NODE was only slightly inferior. Credit-g Dataset.The XGBoost, TabNet, and NODE baselines trained in Kadra et al. (2021) achieved a balanced accuracy of 68.9, 61.2, and 73.1. Our AUC results cannot easily be compared but our experiments for 512 balanced training examples (512 shots) follow the same trend. Diabetes Dataset.Hasan et al. (2020) reported an AUC of 0.828 (0.030) for XGBoost on the diabetes dataset, which matches our findings. With additional feature selection and preprocessing methods they reached an AUC of 0.946 (0.020) with XGBoost, but this was out of the scope of our work. XGBoost was the most performant model that they included in their experiments. Heart Dataset.Muhammad et al. (2020) used only the 303 instances from the Cleveland cohort, while we combined all four sub-cohorts. They achieved an AUC of 0.923 with LR, which is close to our results on all sub-cohorts. They also tested several models that outperformed LR. Income Dataset.Many studies used the Income or Adult dataset. The review Borisov et al. (2022) included several of our baselines. They reported an AUC of 0.854 (0.002) for a linear model, 0.928 (0.001) for XGBoost, 0.928 (0.001) for LightGBM, 0.916 (0.002) for SAINT, 0.911 (0.001) for TabNet, and 0.911 (0.002) for NODE. These are in accordance with our results. We reckon the better performance of our LR model is due to more extensive parameter tuning. Jungle Dataset.The XGBoost and TabNet baselines trained in Kadra et al. (2021) achieved a balanced accuracy of 87.3 and 73.4. They did not train a NODE moel for this dataset. The results follows the same trend as our experiments for a set of 512 balanced training examples (512 shots)."
    },
    {
      "title": "5 Adjusting Income Dataset For Inflation",
      "text": "We wanted to investigate how a distribution shift caused by inflation affects the zero-shot performance of TabLLM. The **Income** dataset was collected in 1994, and the label and two features (capital gain/loss in last year) contain dollar values. T0 was trained in 2021 (Sanh et al., 2022), and we assumed that the training data is much more recent than the **Income** dataset. The inflation rate from 1994 to 2021 is 1.796. Without inflation correction the zero-shot results were 0.80 (0.01). Correcting the two features, correcting only the prompt, and correcting both all yielded the same performance as the uncorrected one. The accuracy values also remained the same with the inflation correction. Footnote 6: U.S. Bureau of Labor Statistics, CPI Inflation Calculator: [https://www.bls.gov/data/inflation_calculator.htm](https://www.bls.gov/data/inflation_calculator.htm)"
    },
    {
      "title": "6 Feature Importance Analysis Of Tabllm",
      "text": "We wanted to understand which features were most important for the zero-shot performance of TabLLM on **Income** and **EoL**. To this end, we used zero-shot TabLLM with the _List Template_ serialization to predict the label probability of all examples in the dataset. We then used 4-fold cross validation to fit a L2-regularized LR model to the predicted label using the features in the serialization as covariates. For **EoL**, we used age, sex, race, and the conditions as inputs, which summed up to 14,105 features. For **Income** we compared these approximated importance scores to the feature coefficients of a LR model trained on all data for a single seed (Table 16). We used the same setup for the LR model as for our main experiments. We did 4-fold cross validation on an 80% training split to choose hyperparameters, and then refit the model using all training data. The best parameters of the LR model for **Income** were a '11' penalty and a regularization constant of 1. For **EoL**, we decided that the LR model coefficients did not provide a good estimate of the ground truth due to the vast amount of features and possible collinearities in the data. Instead, we provide the relative risk (RR) with 95% confidence intervals (CI) treating the occurrence of a feature as an intervention. We report the 50 most and least important features of TabLLM in Table 17."
    },
    {
      "title": "7 Effect Of Using Different Prompts",
      "text": "To evaluate the effect of using a different prompt we considered the zero-shot setting, since even few training examples mostly cancel the effect. For all datasets we constructed five different prompts that contained the same question, e.g., \"Does this person earn a lot of money?\" instead of \"Does this person earn more than 50000 dollars per year?\" for the **Income** dataset. The results are summarized in Table 11. The effects were relative small ranging from a standard deviation of 0.00 for **Jungle** to 0.04 for **Heart** across the five prompts. This suggests that TabLLM is not very sensitive to using different prompts. \\begin{table} \\begin{tabular}{l c c c c c c c c c} \\hline \\hline **Dataset** & **Bank** & **Blood** & **California** & **Car** & **Credit-g** & **Diabetes** & **Heart** & **Income** & **Jungle** \\\\ \\hline TabLLM 0-shot: 1 prompt (ours) & 0.63 & 0.61 & 0.61 & 0.81 & 0.53 & 0.68 & 0.54 & 0.84 & 0.60 \\\\ \\hline TabLLM 0-shot: avg. 5 prompts & 0.64\\({}_{.01}\\) & 0.60\\({}_{.02}\\) & 0.59\\({}_{.01}\\) & 0.80\\({}_{.01}\\) & 0.52\\({}_{.01}\\) & 0.67\\({}_{.01}\\) & 0.55\\({}_{.04}\\) & 0.84\\({}_{.01}\\) & 0.60\\({}_{.00}\\) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 11: The mean performance for one prompt (ours, SD over five seed omitted) and the mean performance and SD across five different prompts (each again over five seeds). \\begin{table} \\begin{tabular}{l c c c c c c c c c c} \\hline \\hline \\multicolumn{10}{c}{**Number of Shots**} \\\\ **Method** & **0** & **4** & **8** & **16** & **32** & **64** & **128** & **256** & **512** & **all** \\\\ \\hline \\hline **Bank Dataset** & & & & & & & & & & & \\\\ \\hline Logistic regression & — & 0.55 \\({}_{\\it 00}\\) & 0.66 \\({}_{\\it 00}\\) & 0.75 \\({}_{\\it 00}\\) & 0.81 \\({}_{\\it 02}\\) & 0.84 \\({}_{\\it 02}\\) & 0.86 \\({}_{\\it 02}\\) & 0.88 \\({}_{\\it 01}\\) & 0.89 \\({}_{\\it 00}\\) & 0.91 \\({}_{\\it 00}\\) \\\\ Logistic regression (ordinal) & — & 0.51 \\({}_{\\it 02}\\) & 0.60 \\({}_{\\it 12}\\) & 0.68 \\({}_{\\it 09}\\) & 0.78 \\({}_{\\it 04}\\) & 0.82 \\({}_{\\it 01}\\) & 0.84 \\({}_{\\it 03}\\) & 0.86 \\({}_{\\it 01}\\) & 0.87 \\({}_{\\it 00}\\) & 0.88 \\({}_{\\it 00}\\) \\\\ LightGBM & — & 0.50 \\({}_{\\it 00}\\) & 0.50 \\({}_{\\it 00}\\) & 0.50 \\({}_{\\it 00}\\) & 0.77 \\({}_{\\it 03}\\) & 0.84 \\({}_{\\it 03}\\) & 0.88 \\({}_{\\it 01}\\) & 0.89 \\({}_{\\it 00}\\) & 0.94 \\({}_{\\it 00}\\) \\\\ LightGBM (ordinal) & — & 0.50 \\({}_{\\it 00}\\) & 0.50 \\({}_{\\it 00}\\) & 0.50 \\({}_{\\it 00}\\) & 0.50 \\({}_{\\it 00}\\) & 0.78 \\({}_{\\it 03}\\) & 0.84 \\({}_{\\it 02}\\) & 0.87 \\({}_{\\it 01}\\) & 0.89 \\({}_{\\it 00}\\) & 0.94 \\({}_{\\it 00}\\) \\\\ XGBoost (ordinal) & — & 0.50 \\({}_{\\it 00}\\) & 0.50 \\({}_{\\it 00}\\) & 0.50 \\({}_{\\it 00}\\) & 0.50 \\({}_{\\it 00}\\) & 0.78 \\({}_{\\it 03}\\) & 0.84 \\({}_{\\it 02}\\) & 0.87 \\({}_{\\it 01}\\) & 0.89 \\({}_{\\it 00}\\) & 0.94 \\({}_{\\it 00}\\) \\\\ XGBoost & — & 0.50 \\({}_{\\it 00}\\) & 0.56 \\({}_{\\it 00}\\) & 0.68 \\({}_{\\it 04}\\) & 0.76 \\({}_{\\it 03}\\) & 0.83 \\({}_{\\it 02}\\) & 0.85 \\({}_{\\it 03}\\) & 0.88 \\({}_{\\it 01}\\) & 0.90 \\({}_{\\it 01}\\) & 0.94 \\({}_{\\it 00}\\) \\\\ XGBoost (ordinal) & — & 0.50 \\({}_{\\it 00}\\) & 0.56 \\({}_{\\it 00}\\) & 0.69 \\({}_{\\it 05}\\) & 0.75 \\({}_{\\it 04}\\) & 0.82 \\({}_{\\it 02}\\) & 0.84 \\({}_{\\it 03}\\) & 0.87 \\({}_{\\it 01}\\) & 0.89 \\({}_{\\it 00}\\) & 0.93 \\({}_{\\it 00}\\) \\\\ \\hline SAINT & — & 0.51 \\({}_{\\it 00}\\) & 0.61 \\({}_{\\it 11}\\) & 0.70 \\({}_{\\it 00}\\) & 0.77 \\({}_{\\it 03}\\) & 0.81 \\({}_{\\it 03}\\) & 0.85 \\({}_{\\it 02}\\) & 0.88 \\({}_{\\it 01}\\) & 0.88 \\({}_{\\it 01}\\) & 0.93 \\({}_{\\it 00}\\) \\\\ TabNet & — & 0.51 \\({}_{\\it 00}\\) & 0.58 \\({}_{\\it 02}\\) & 0.64 \\({}_{\\it 01}\\) & 0.62 \\({}_{\\it 04}\\) & 0.71 \\({}_{\\it 05}\\) & 0.73 \\({}_{\\it 03}\\) & 0.80 \\({}_{\\it 01}\\) & 0.83 \\({}_{\\it 01}\\) & 0.93 \\({}_{\\it 00}\\) \\\\ NODE & — & 0.52 \\({}_{\\it 02}\\) & 0.55 \\({}_{\\it 06}\\) & 0.64 \\({}_{\\it 06}\\) & 0.73 \\({}_{\\it 06}\\) & 0.78 \\({}_{\\it 02}\\) & 0.83 \\({}_{\\it 03}\\) & 0.85 \\({}_{\\it 01}\\) & 0.86 \\({}_{\\it 01}\\) & 0.76 \\({}_{\\it 02}\\) \\\\ TabPFN (ordinal) & — & 0.59 \\({}_{\\it 14}\\) & 0.66 \\({}_{\\it 08}\\) & 0.69 \\({}_{\\it 02}\\) & 0.76 \\({}_{\\it 03}\\) & 0.82 \\({}_{\\it 03}\\) & 0.86 \\({}_{\\it 02}\\) & 0.89 \\({}_{\\it 00}\\) & 0.90 \\({}_{\\it 00}\\) & 0.91 \\({}_{\\it 00}\\) \\\\ TabPFN (ordinal) & — & 0.57 \\({}_{\\it 10}\\) & 0.67 \\({}_{\\it 03}\\) & 0.71 \\({}_{\\it 05}\\) & 0.78 \\({}_{\\it 04}\\) & 0.83 \\({}_{\\it 01}\\) & 0.86 \\({}_{\\it 02}\\) & 0.87 \\({}_{\\it 00}\\) & 0.88 \\({}_{\\it 01}\\) & 0.89 \\({}_{\\it 01}\\) \\\\ \\hline TabLLM (T0 + Text GPT-3) & 0.63 \\({}_{\\it 01}\\) & 0.61 \\({}_{\\it 01}\\) & 0.62 \\({}_{\\it 02}\\) & 0.63 \\({}_{\\it 03}\\) & 0.64 \\({}_{\\it 02}\\) & 0.66 \\({}_{\\it 04}\\) & 0.76 \\({}_{\\it 04}\\) & 0.81 \\({}_{\\it 02}\\) & 0.82 \\({}_{\\it 01}\\) & 0.82 \\({}_{\\it 01}\\) \\\\ TabLLM (T0 + Text T0) & 0.54 \\({}_{\\it 01}\\) & 0.56 \\({}_{\\it 08}\\) & 0.60 \\({}_{\\it 00}\\) & 0.59 \\({}_{\\it 06}\\) & 0.60 \\({}_{\\it 04}\\) & 0.62 \\({}_{\\it 04}\\) & 0.67 \\({}_{\\it 04}\\) & 0.79 \\({}_{\\it 03}\\) & 0.85 \\({}_{\\it 01}\\) & 0.89 \\({}_{\\it 01}\\) & 0.89 \\({}_{\\it 01}\\) \\\\ TabLLM (T0 + Table-To-Text) & 0.42 \\({}_{\\it 01}\\) & 0.48 \\({}_{\\it 07}\\) & 0.50 \\({}_{\\it 05}\\) & 0.56 \\({}_{\\it 03}\\) & 0.57 \\({}_{\\it 04}\\) & 0.59 \\({}_{\\it 05}\\) & 0.63 \\({}_{\\it 03}\\) & 0.68 \\({}_{\\it 02}\\) & 0.74 \\({}_{\\it 01}\\) & 0.89 \\({}_{\\it 01}\\) & 0.92 \\({}_{\\it 01}\\) \\\\ TabLLM (T0 + Text Template) & 0.63 \\({}_{\\it 01}\\) & 0.59 \\({}_{\\it 10}\\) & 0.64 \\({}_{\\it 05}\\) & 0.65 \\({}_{\\it 04}\\) & 0.64 \\({}_{\\it 06}\\) & 0.69 \\({}_{\\it 04}\\) & 0.82 \\({}_{\\it 02}\\) & 0.87 \\({}_{\\ \\begin{table} \\begin{tabular}{l c c c c c c c c c c} \\hline \\hline \\multicolumn{11}{c}{**Number of Shots**} \\\\ **Method** & **0** & **4** & **8** & **16** & **32** & **64** & **128** & **256** & **512** & **all** \\\\ \\hline \\hline **Car Dataset** & & & & & & & & & & & \\\\ \\hline Logistic regression & — & 0.61 & 0.65 & 10 & 0.74 & 0.83 & 0.93 & 0.92 & 0.96 & 0.97 & 0.91 & 0.98 & 0.90 \\\\ Logistic regression (ordinal) & — & 0.62 & 0.63 & 0.64 & 0.79 & 0.75 & 0.73 & 0.73 & 0.73 & 0.74 & 0.76 & 0.78 & 0.03 \\\\ LightGBM & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.85 & 0.93 & 0.91 & 0.99 & 0.91 & 1.00 \\\\ LightGBM (ordinal) & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.54 & 0.91 & 0.98 & 0.99 & 0.90 & 1.00 \\\\ XGBoost & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.54 & 0.91 & 0.95 & 0.91 & 0.99 \\\\ XGBoost & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.54 & 0.91 & 0.98 & 0.99 & 0.99 & 1.00 \\\\ XGBoost & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.54 & 0.91 & 0.98 & 0.99 & 0.99 & 1.00 \\\\ XGBoost (ordinal) & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.54 & 0.91 & 0.98 & 0.99 & 0.99 & 1.00 \\\\ \\hline SAINT & — & 0.56 & 0.64 & 0.76 & 0.85 & 0.53 & 0.92 & 0.96 & 0.98 & 0.99 & 0.99 & 1.00 \\\\ TabNet & — & 0.54 & 0.64 & 0.45 & 0.66 & 0.73 & 0.81 & 0.93 & 0.92 & 0.98 & 1.00 \\\\ NODE & — & 0.51 & 1.0 & 0.57 & 0.66 & 0.69 & 0.74 & 0.80 & 0.82 & 0.91 & 0.91 & 0.96 & 0.93 \\\\ TabPhFN & — & 0.64 & 0.75 & 0.87 & 0.94 & 0.92 & 0.97 & 0.90 & 0.99 & 1.00 & 1.00 & 1.00 \\\\ TabPhFN (ordinal) & — & 0.59 & 0.65 & 0.68 & 0.75 & 0.84 & 0.82 & 0.98 & 0.91 & 0.93 & 0.98 & 0.99 & 1.00 \\\\ \\hline TabLLM (T0 + Text GPT-3) & 0.72 & 0.75 & 0.75 & 0.72 & 0.78 & 0.81 & 0.83 & 0.87 & 0.90 & 0.93 & 0.93 & 0.99 & 0.99 & 1.00 \\\\ TabLLM (T0 + Text T0) & 0.85 & 0.85 & 0.82 & 0.84 & 0.86 & 0.89 & 0.92 & 0.92 & 0.94 & 0.98 & 0.99 & 0.99 & 1.00 \\\\ TabLLM (T0 + Table-To-Text) & 0.61 & 0.69 & 0.04 & 0.74 & 0.79 & 0.88 & 0.91 & 0.92 & 0.94 & 0.96 & 0.95 & 0.91 & 0.96 \\\\ TabLLM (T0 + Text Template) & 0.82 & 0.83 & 0.85 & 0.86 & 0.86 & 0.91 & 0.92 & 0.96 & 0.98 & 0.99 & 1.00 & 1.00 \\\\ TabLLM (T0 + List Template) & 0.79 & 0.84 & 0.85 & 0.82 & 0.86 & 0.91 & 0.92 & 0.95 & 0.91 & 0.98 & 0.99 & 1.00 & 1.00 \\\\ TabLLM (T0 + List Only Values) & 0.48 & 0.62 & 0.67 & 0.70 & 0.73 & 0.75 & 0.87 & 0.94 & 0.98 & 0.91 & 0.99 & 1.00 & 1.00 \\\\ TabLLM (T0 + List Perm. Names) & 0.39 & 0.24 & 0.54 & 1.05 & 0.88 & 0.70 & 0.86 & 0.94 & 0.97 & 0.99 & 0.99 & 1.00 & 1.00 \\\\ TabLLM (T0 + List Perm. Values) & 0.38 & 0.2 & 0.48 & 0.85 & 0.54 & 0.63 & 0.94 & 0.69 & 0.93 & 0.98 & 0.99 & 1.00 & 1.00 \\\\ TabLLM (T0 3B + Text Template) & 0.78 & 0.80 & 0.84 & 0.84 & 0.84 & 0.89 & 0.91 & 0.91 & 0.96 & 0.98 & 0.99 & 1.00 & 1.00 \\\\ \\hline \\hline **Credit-g Dataset** & & & & & & & & & & & & \\\\ \\hline Logistic regression & — & 0.50 & 0.56 & 0.58 & 0.68 & 0.68 & 0.66 & 0.71 & 0.75 & 0.76 & 0.62 & 0.79 & 0.79 \\\\ Logistic regression (ordinal) & — & 0.56 & 0.54 & 0.65 & 0.55 & 0.61 & 0.68 & 0.65 & 0.68 & 0.68 & 0.64 & 0.71 & 0.72 & 0.72 \\\\ LightGBM & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.61 & 0.69 & 0.68 & 0.72 & 0.75 & 0.78 & 0.76 \\\\ LightGBM (ordinal) & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.68 & 0.66 & 0.74 & 0.72 & 0.75 & 0.76 \\\\ XGBoost & — & 0.50 & 0.50 & 0.51 & 0.57 & 0.50 & 0.66 & 0.68 & 0.66 & 0.73 & 0.72 & 0.75 & 0.78 & 0.74 \\\\ XGBoost (ordinal) & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.68 & 0.64 & 0.74 & 0.76 & 0.76 & 0.76 & 0.76 \\\\ \\hline SAINT & — & 0.56 & 0.53 & 0.60 & 0.66 & 0.66 & 0.66 & 0.66 & 0.68 & 0.72 & 0.73 & 0.73 & 0.77 & 0.74 \\\\ TabNet & — & 0.48 & 0.52 & 0.49 & 0.52 & 0.52 & 0.56 & 0.68 & 0.68 & 0.61 & 0.66 & 0.66 & 0.64 & 0.63 \\\\ NODE & — & 0.54 & 0.59 & 0.54 & 0.54 & 0.59 & 0.57 & 0.63 & 0.64 & 0.68 & 0.68 & 0.70 & 0.70 & 0.65 & 0.65 \\\\ TabPN (ordinal) & — & 0.58 & 0.59 & 0.63 & 0.64 & 0.66 & 0.69 & 0.70 & 0.70 & 0.72 & 0.76 & 0.75 & 0.75 & 0.75 & 0.73 \\\\ \\hline TabLM (T0 + Text GPT-3) & 0.52 & 0.4 & 0.53 & 0.56 & 0.56 & 0.55 & 0.55 & 0.57 & 0.68 & 0.60 & 0.61 & 0.63 & 0.63 & 0.62 & 0.62 \\\\ TabLLM (T0 + Text T0) & 0.49 & 0.50 & 0.56 & 0.54 & 0.55 & 0.54 & 0.60 & 0.66 & 0.61 & 0.62 & 0.61 & 0.62 & 0.63 & 0.65 & 0.62 & \\begin{table} \\begin{tabular}{l c c c c c c c c c c} \\hline \\hline \\multicolumn{11}{c}{**Number of Shots**} \\\\ **Method** & **0** & **4** & **8** & **16** & **32** & **64** & **128** & **256** & **512** & **all** \\\\ \\hline \\hline **Heart Dataset** & & & & & & & & & & & \\\\ \\hline Logistic regression & — & 0.69 & 0.75 & 0.75 & 0.82 & 0.66 & 0.87 & 0.91 & 0.90 & 0.92 & 0.92 & 0.93 & 0.93 \\\\ Logistic regression (ordinal) & — & 0.70 & 0.75 & 0.73 & 0.84 & 0.80 & 0.83 & 0.89 & 0.88 & 0.90 & 0.92 & 0.92 & 0.92 \\\\ LightGBM & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.91 & 0.91 & 0.91 & 0.93 & 0.90 & 0.94 \\\\ LightGBM & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.91 & 0.91 & 0.91 & 0.92 & 0.91 & 0.94 \\\\ XGBoost & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.90 & 0.91 & 0.91 & 0.90 & 0.92 & 0.91 & 0.94 \\\\ XGBoost & — & 0.50 & 0.55 & 0.54 & 0.84 & 0.47 & 0.88 & 0.91 & 0.91 & 0.91 & 0.90 & 0.92 & 0.91 & 0.94 \\\\ XGBoost (ordinal) & — & 0.50 & 0.56 & 1.55 & 0.84 & 0.90 & 0.93 & 0.91 & 0.90 & 0.90 & 0.90 & 0.92 & 0.91 & 0.93 \\\\ \\hline SAINT & — & 0.80 & 0.12 & 0.83 & 0.88 & 0.90 & 0.90 & 0.90 & 0.90 & 0.90 & 0.92 & 0.91 & 0.93 \\\\ TabNet & — & 0.56 & 1.02 & 0.70 & 0.73 & 0.80 & 0.84 & 0.83 & 0.84 & 0.88 & 0.82 & 0.83 & 0.89 \\\\ NODE & — & 0.52 & 1.0 & 0.78 & 0.83 & 0.83 & 0.86 & 0.82 & 0.88 & 0.91 & 0.92 & 0.92 & 0.92 \\\\ TabPhFN & — & 0.84 & 0.86 & 0.88 & 0.87 & 0.96 & 0.92 & 0.92 & 0.92 & 0.92 & 0.92 & 0.92 & 0.92 \\\\ TabPhFN (ordinal) & — & 0.79 & 0.85 & 0.87 & 0.86 & 0.90 & 0.92 & 0.92 & 0.91 & 0.92 & 0.92 & 0.92 & 0.92 \\\\ \\hline TabLLM (T0 + Text GPT-3) & 0.51 & 0.72 & 0.82 & 0.83 & 0.85 & 0.88 & 0.91 & 0.89 & 0.92 & 0.91 & 0.91 & 0.93 \\\\ TabLLM (T0 + Text T0) & 0.44 & 0.74 & 0.82 & 1.10 & 0.87 & 0.88 & 0.82 & 0.89 & 0.94 & 0.90 & 0.89 & 0.89 & 0.93 \\\\ TabLLM (T0 + Table-To-Text) & 0.56 & 0.73 & 0.79 & 0.78 & 0.86 & 0.88 & 0.91 & 0.92 & 0.91 & 0.90 & 0.91 & 0.92 \\\\ TabLLM (T0 + Text Template) & 0.54 & 0.76 & 1.4 & 0.83 & 0.87 & 0.84 & 0.91 & 0.90 & 0.90 & 0.92 & 0.92 & 0.91 & 0.94 \\\\ TabLLM (T0 + List Template) & 0.52 & 0.73 & 0.73 & 1.21 & 0.83 & 0.85 & 0.87 & 0.94 & 0.88 & 0.91 & 0.91 & 0.92 & 0.91 & 0.94 \\\\ TabLLM (T0 + List Only Values) & 0.40 & 0.67 & 1.66 & 0.83 & 0.84 & 0.88 & 0.89 & 0.93 & 0.92 & 0.90 & 0.90 & 0.91 & 0.92 \\\\ TabLLM (T0 + List Perm. Names) & 0.57 & 0.78 & 0.85 & 0.82 & 0.82 & 0.87 & 0.95 & 0.92 & 0.92 & 0.91 & 0.91 & 0.91 & 0.93 \\\\ TabLLM (T0 + List Perm. Values) & 0.23 & 0.63 & 2.00 & 0.79 & 1.92 & 0.83 & 0.88 & 0.94 & 0.90 & 0.90 & 0.91 & 0.91 & 0.93 \\\\ TabLLM (T0 3B + Text Template) & 0.56 & 0.63 & 0.82 & 0.84 & 0.85 & 0.86 & 0.93 & 0.91 & 0.91 & 0.93 & 0.91 & 0.94 \\\\ \\hline \\hline **Income Dataset** & & & & & & & & & & & & \\\\ \\hline Logistic regression & — & 0.68 & 0.72 & 0.13 & 0.80 & 0.82 & 0.83 & 0.85 & 0.87 & 0.87 & 0.88 & 0.90 & 0.90 \\\\ Logistic regression (ordinal) & — & 0.55 & 0.44 & 0.56 & 0.58 & 0.70 & 0.70 & 0.76 & 0.73 & 0.80 & 0.80 & 0.80 & 0.81 \\\\ LightGBM & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.78 & 0.81 & 0.87 & 0.88 & 0.93 & 0.93 \\\\ LightGBM (ordinal) & — & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.78 & 0.81 & 0.81 & 0.86 & 0.89 & 0.90 & 0.93 \\\\ XGBoost & — & 0.50 & 0.50 & 0.50 & 0.70 & 0.79 & 0.82 & 0.84 & 0.87 & 0.81 & 0.88 & 0.93 & 0.90 \\\\ XGBoost (ordinal) & — & 0.50 & 0.50 & 0.59 & 0.77 & 0.79 & 0.82 & 0.84 & 0.84 & 0.87 & 0.81 & 0.88 & 0.93 \\\\ XGBoost (ordinal) & — & 0.50 & 0.63 & 0.04 & 0.74 & 0.76 & 0.79 & 0.84 & 0.82 & 0.86 & 0.88 & 0.93 & 0.90 \\\\ \\hline SAINT & — & 0.74 & 0.65 & 1.55 & 0.79 & 0.81 & 0.83 & 0.84 & 0.84 & 0.87 & 0.81 & 0.88 & 0.91 & 0.90 \\\\ TabNet & — & 0.56 & 0.59 & 0.62 & 0.61 & 0.64 & 0.71 & 0.73 & 0.78 & 0.80 & 0.83 & 0.92 & 0.92 & 0.90 \\\\ NODE & — & 0.54 & 0.54 & 0.65 & 0.67 & 0.73 & 0.75 & 0.78 & 0.78 & 0.81 & 0.89 & 0.82 & 0.82 \\\\ TabPN (ordinal) & — & 0.73 & 0.78 & 0.79 & 0.69 & 0.80 & 0.84 & 0.82 & 0.84 & 0.84 & 0.86 & 0.87 & 0.81 & 0.89 & 0.89 \\\\ TabPLM (T0 + Text GPT-3) & 0.64 & 1.11 & 0.64 & 0.72 & 0.74 & 0.77 & 0.80 & 0.82 & 0.81 & 0.83 & 0.84 & 0.84 & 0.85 & 0.87 & 0.89 & 0.89 \\\\ \\hline \\hline **TablLM** (T0 + Text GPT-3) & 0.75 & 0.79 & 0.80 & 0.80 & 0.82 & 0.81 & 0.84 & 0.84 & 0.84 & 0.85 & 0.86 & 0.87 & 0.89 & 0.89 & 0.89 \\begin{table} \\begin{tabular}{l c c c c c c c c} \\hline \\hline & \\multicolumn{8}{c}{**Number of Shots**} \\\\ **Method** & **0** & **16** & **64** & **256** & **1,024** & **4,096** & **16,384** & **all** \\\\ \\hline \\hline **End of Life (EoL)** & & & & & & & & \\\\ \\hline TabLLM (T0 + List Template) & 0.70 & 0.74 & 0.78 & 0.78 & 0.79 & 0.81 & 0.81 & — \\\\ TabLLM (T0 + Text Template) & 0.63 & 0.71 & 0.74 & 0.76 & 0.78 & 0.79 & 0.80 & — \\\\ TabLLM (T0 + List Short) & 0.68 & 0.71 & 0.76 & 0.79 & 0.80 & 0.81 & 0.82 & — \\\\ TabLLM (T0 + List Perm. Names) & 0.62 & 0.66 & 0.70 & 0.74 & 0.75 & 0.77 & 0.79 & — \\\\ \\hline Logistic Regression & — & \\(0.65_{,07}\\) & \\(0.77_{,02}\\) & \\(0.80_{,02}\\) & \\(0.83_{,01}\\) & \\(0.83_{,01}\\) & \\(0.84_{,01}\\) & \\(0.84_{,01}\\) \\\\ LightGBM & — & \\(0.50_{,00}\\) & \\(0.71_{,01}\\) & \\(0.76_{,02}\\) & \\(0.80_{,01}\\) & \\(0.82_{,01}\\) & \\(0.83_{,01}\\) & \\(0.82\\) * \\\\ \\hline TabLLM (T0 + List Template) unbalanced & 0.70 & 0.64 & 0.69 & 0.74 & 0.74 & 0.77 & 0.79 & — \\\\ Logistic Regression unbalanced & — & \\(0.44_{,04}\\) & \\(0.53_{,12}\\) & \\(0.75_{,03}\\) & \\(0.77_{,03}\\) & \\(0.80_{,02}\\) & \\(0.82_{,02}\\) & \\(0.84_{,01}\\) \\\\ \\hline \\hline **Surgical Procedure (Surgery)** & & & & & & & \\\\ \\hline TabLLM (T0 + List Template) & 0.67 & 0.73 & 0.72 & 0.73 & 0.75 & 0.78 & 0.79 & — \\\\ TabLLM (T0 + Text Template) & 0.62 & 0.71 & 0.69 & 0.72 & 0.74 & 0.77 & 0.78 & — \\\\ TabLLM (T0 + List Short) & 0.66 & 0.70 & 0.69 & 0.72 & 0.73 & 0.76 & 0.78 & — \\\\ TabLLM (T0 + List Perm. Names) & 0.60 & 0.68 & 0.70 & 0.72 & 0.74 & 0.77 & — \\\\ \\hline Logistic Regression & — & \\(0.72_{,04}\\) & \\(0.75_{,05}\\) & \\(0.77_{,01}\\) & \\(0.79_{,01}\\) & \\(0.80_{,01}\\) & \\(0.80_{,00}\\) & \\(0.81_{,00}\\) \\\\ LightGBM & — & \\(0.50_{,00}\\) & \\(0.73_{,02}\\) & \\(0.77_{,01}\\) & \\(0.79_{,01}\\) & \\(0.80_{,00}\\) & \\(0.81_{,01}\\) & \\(0.82\\) * \\\\ \\hline TabLLM (T0 + List Template) unbalanced & 0.67 & 0.68 & 0.73 & 0.74 & 0.75 & 0.77 & 0.79 & — \\\\ Logistic Regression unbalanced & — & \\(0.61_{,15}\\) & \\(0.77_{,01}\\) & \\(0.77_{,02}\\) & \\(0.78_{,01}\\) & \\(0.80_{,01}\\) & \\(0.80_{,00}\\) & \\(0.81_{,00}\\) \\\\ \\hline \\hline \\multicolumn{8}{l}{**Likelihood of Hospitalization (LoH)**} \\\\ TabLLM (T0 + List Template) & 0.71 & 0.73 & 0.73 & 0.76 & 0.78 & 0.81 & 0.82 & — \\\\ TabLLM (T0 + Text Template) & 0.65 & 0.74 & 0.72 & 0.74 & 0.78 & 0.80 & 0.81 & — \\\\ TabLLM (T0 + List Short) & 0.70 & 0.73 & 0.75 & 0.78 & 0.79 & 0.80 & 0.82 & — \\\\ TabLLM (T0 + List Perm. Names) & 0.62 & 0.71 & 0.72 & 0.75 & 0.75 & 0.78 & 0.80 & — \\\\ \\hline Logistic Regression & — & \\(0.72_{,04}\\) & \\(0.76_{,03}\\) & \\(0.80_{,01}\\) & \\(0.82_{,01}\\) & \\(0.83_{,01}\\) & \\(0.83_{,01}\\) & \\(0.84_{,01}\\) \\\\ LightGBM & — & \\(0.50_{,00}\\) & \\(0.72_{,02}\\) & \\(0.76_{,03}\\) & \\(0.81_{,01}\\) & \\(0.83_{,00}\\) & \\(0.83_{,01}\\) & \\(0.85\\)* \\\\ \\hline TabLLM (T0 + List Template) unbalanced & 0.71 & 0.66 & 0.72 & 0.75 & 0.75 & 0.78 & 0.80 & — \\\\ Logistic Regression unbalanced & — & \\(0.53_{,06}\\) & \\(0.54_{,09}\\) & \\(0.73_{,06}\\) & \\(0.79_{,01}\\) & \\(0.81_{,01}\\) & \\(0.82_{,01}\\) & \\(0.84_{,01}\\) \\\\ \\hline \\hline \\end{tabular} * These experiments were only performed for a single run due to runtime limitations on the full dataset. \\end{table} Table 15: Full results on healthcare claims dataset. The best concept selection method (most frequent concepts) and concept names (original concept names) were used as determined in prior zero-shot experiments. A fix number of 10 epochs was used for up to 256 shots and 3 epochs for more shots to decrease the runtime and prevent overfitting. \\begin{table} \\begin{tabular}{l r r r r} \\hline \\hline **Feature** & \\multicolumn{1}{c}{**TabLLM**} & \\multicolumn{1}{c}{**LR**} \\\\ & rank & weight & rank & weight \\\\ \\hline capital\\_gain & 1 & 5.310 & 2 & 2.393 \\\\ education\\_Masters & 2 & 4.623 & 6 & 1.455 \\\\ education\\_Doctorate & 3 & 3.410 & 4 & 2.066 \\\\ education\\_Bachelors & 4 & 2.995 & 7 & 1.135 \\\\ education\\_Prof-school & 5 & 2.949 & 5 & 1.900 \\\\ occupation\\_Machine-op-insp. & 6 & 2.589 & 75 & -0.325 \\\\ workclass\\_Private & 7 & 2.275 & 37 & 0.102 \\\\ relationship\\_Wife & 8 & 2.109 & 8 & 0.955 \\\\ native\\_country\\_China & 9 & 2.086 & 94 & -0.839 \\\\ native\\_country\\_United-States & 10 & 2.045 & 38 & 0.087 \\\\ native\\_country\\_Taivan & 11 & 1.965 & 54 & 0.000 \\\\ workclass\\_Federal-gov & 12 & 1.784 & 14 & 0.574 \\\\ race\\_White & 13 & 1.685 & 61 & 0.000 \\\\ education\\_Assoc-acdm & 14 & 1.621 & 13 & 0.574 \\\\ native\\_country\\_nan & 15 & 1.565 & 63 & -0.056 \\\\ marital\\_status\\_Married-civ-sp. & 16 & 1.487 & 3 & 2.214 \\\\ occupation\\_Protective-serv & 17 & 1.434 & 17 & 0.535 \\\\ sex\\_Male & 18 & 1.335 & 42 & 0.000 \\\\ occupation\\_Armed-Forces & 19 & 1.290 & 60 & 0.000 \\\\ occupation\\_Adm-clerical & 20 & 1.245 & 52 & 0.000 \\\\ hours\\_per\\_week & 21 & 1.240 & 20 & 0.424 \\\\ native\\_country\\_Pora & 22 & 1.227 & 86 & -0.749 \\\\ occupation\\_Tech-support & 23 & 1.164 & 18 & 0.526 \\\\ relationship\\_Husband & 24 & 1.087 & 72 & -0.212 \\\\ occupation\\_Sales & 25 & 0.857 & 28 & 0.298 \\\\ native\\_country\\_Vietnam & 26 & 0.803 & 95 & -0.898 \\\\ marital\\_status\\_Married-AF-sp. & 27 & 0.792 & 1 & 2.571 \\\\ native\\_country\\_Philippines & 28 & 0.711 & 40 & 0.011 \\\\ age & 29 & 0.710 & 22 & 0.411 \\\\ native\\_country\\_Poland & 30 & 0.698 & 53 & 0.000 \\\\ occupation\\_Prof-speciality & 31 & 0.684 & 12 & 0.620 \\\\ race\\_Asian-Pac-Islander & 32 & 0.651 & 32 & 0.254 \\\\ native\\_country\\_Ceuador & 85 & -0.952 & 49 & 0.000 \\\\ native\\_country\\_Outlying-US & 33 & 0.591 & 92 & -0.836 \\\\ workclass\\_Self-emp-not-inc & 34 & 0.582 & 76 & -0.344 \\\\ native\\_country\\_Haiti & 87 & -1.062 & 35 & 0.137 \\\\ native\\_country\\_Italy & 35 & 0.534 & 24 & 0.400 \\\\ marital\\_status\\_Separated & 36 & 0.523 & 70 & -0.181 \\\\ workclass\\_nan & 37 & 0.515 & 59 & 0.000 \\\\ occupation\\_7th-8th & & 90 & -1.151 & 103 & -1.303 \\\\ occupation\\_Exec-managerial & 38 & 0.503 & 10 & 0.773 \\\\ native\\_country\\_Scotland & 39 & 0.491 & 81 & -0.626 \\\\ native\\_country\\_Laos & 40 & 0.475 & 44 & 0.000 \\\\ native\\_country\\_Cranbodia & 41 & 0.328 & 11 & 0.642 \\\\ native\\_country\\_Southeast & 42 & 0.276 & 55 & 0.000 \\\\ workclass\\_State-gov & 43 & 0.267 & 73 & -0.223 \\\\ native\\_country\\_Germany & 44 & 0.262 & 39 & 0.043 \\\\ native\\_country\\_Puerto-Rico & 45 & 0.241 & 67 & -0.128 \\\\ native\\_country\\_Hungary & 46 & 0.177 & 34 & 0.191 \\\\ native\\_country\\_Mexico & 47 & 0.123 & 80 & -0.579 \\\\ native\\_country\\_Ireland & 48 & 0.116 & 9 & 0.954 \\\\ education\\_HS-grad & 49 & 0.092 & 43 & 0.000 \\\\ occupation\\_Transport-moving & 50 & 0.090 & 62 & -0.048 \\\\ native\\_country\\_El-Salvador & 51 & 0.027 & 90 & -0.803 \\\\ native\\_country\\_Canada & 52 & 0.027 & 23 & 0.407 \\\\ workclass\\_Self-emp-inc & 53 & 0.001 & 30 & 0.255 \\\\ \\hline \\hline \\end{tabular} \\begin{tabular}{l r r r r} \\hline **Feature** & \\multicolumn{1}{c}{**TabLLM**} & \\multicolumn{1}{c}{**LR**} \\\\ & rank & weight & rank & weight \\\\ \\hline relationship\\_Other-relative & 54 & -0.010 & 88 & -0.759 \\\\ native\\_country\\_Trinadad\\&Tob. & 55 & -0.028 & 66 & -0.097 \\\\ race\\_Black & 56 & -0.044 & 74 & -0.291 \\\\ native\\_country\\_England & 57 & -0.088 & 16 & 0.551 \\\\ native\\_country\\_Honduras & 58 & -0.105 & 58 & 0.000 \\\\ relationship\\_Not-in-family & 59 & -0.153 & 29 & 0.257 \\\\ native\\_country\\_Holand-Neth. & 60 & -0.154 & 57 & 0.000 \\\\ occupation\\_Craft-repair & 61 & -0.161 & 36 & 0.108 \\\\ capital\\_loss & 62 & -0.182 & 31 & 0.255 \\\\ race\\_Other & 63 & -0.202 & 65 & -0.085 \\\\ native\\_country\\_Yugoslavia & 64 & -0.204 & 27 & 0.357 \\\\ workclass\\_Local-gov & 65 & -0.230 & 47 & 0.000 \\\\ occupation\\_nan & 66 & -0.248 & 82 & -0.653 \\\\ native\\_Neuver-married & 67 & -0.292 & 77 & -0.443 \\\\ native\\_country\\_Jran & 68 & -0.330 & 41 & 0.000 \\\\ native\\_country\\_Dominant-Rep. & 69 & -0.332 & 85 & -0.731 \\\\ core\\_Other & 63 & -0.379 & 51 & 0.000 \\\\ native\\_Country\\_Yamicaa & 71 & -0.416 & 25 & 0.392 \\\\ active\\_country\\_Nicaragua & 72 & -0.425 & 45 & 0.000 \\\\ active\\_country\\_Thailand & 73 & -0.451 & 100 & -1.116 \\\\ native\\_Country\\_Pera & 74 & -0.522 & 93 & -0.837 \\\\ native\\_country\\_Japan & 75 & -0.617 & 56 & 0.000 \\\\ occupation\\_Tech-support & 23 & 1.164 & 18 & 0.526 \\\\ relationship\\_Husband & 24 & 1.087 & 72 & -0.212 \\\\ occupation\\_Sales & 25 & 0.857 & 28 & 0.298 \\\\ native\\_country\\_Vietnam & 26 & 0.803 & 95 & -0.898 \\\\ marital\\_status\\_Married-AF-sp. & 27 & 0.792 & 1 & 2.571 \\\\ native\\_country\\_Philippines & 28 & 0.711 & 40 & 0.011 \\\\ native\\_country\\_Columbia & 81 & -0.836 & 104 & -1.855 \\\\ age & 29 & 0.710 & 22 & 0.411 \\\\ native\\_country\\_Poland & 30 & 0.698 & 53 & 0.000 \\\\ occupational\\_Prof-speciality & 31 & 0.684 & 12 & 0.620 \\\\ race\\_Asian-Pac-Islander & 32 & 0.651 & 32 & 0.254 \\\\ native\\_country\\_Ecuador & 85 & -0.952 & 49 & 0.000 \\\\ native\\_country\\_Outlying-US & 33 & 0.591 & 92 & -0.836 \\\\ workclass\\_Self-emp-not-inc & 34 & 0.582 & 76 & -0.344 \\\\ native\\_country\\_Haiti & 87 & -1.062 & 35 & 0.137 \\\\ native\\_country\\_Italy & 35 & 0.534 & 24 \\begin{table} \\begin{tabular}{l c c c c} \\hline **Feature** & **TabLLM** & **RR (95\\% CI)** & **Feature** & **TabLLM** & **RR (95\\% CI)** \\\\ & rank weight & & & rank weight & \\\\ \\hline atrial fibrillation & 1 & 0.633 & 2.72 (2.51-2.95) & open wound of forehead without... & 14056 -0.152 & 1.80 (1.18-2.74) \\\\ atherosclerosis of coronary art... & 2 & 0.530 & 2.10 (1.94-2.27) & prediabetes & 14057 -0.157 & 0.81 (0.68-0.96) \\\\ atherosclerosis of aorta & 3 & 0.473 & 1.99 (1.81-2.19) & prediabetes & 1.63 (1.03-2.56) \\\\ exudative age-related macular d... & 4 & 0.452 & 2.38 (2.06-2.75) & prediabetes & 1.4059 -0.157 & 0.87 (0.73-1.04) \\\\ sex\\_male & 5 & 0.442 & 1.23 (1.14-1.33) & prediabetes & 1.14 (0.94-1.40) \\\\ non-hodgkin’s lymphoma (clinical) & 6 & 0.440 & 1.36 (0.94-1.96) & prediabetes & 1.4059 -0.157 & 1.14 (0.91-1.42) \\\\ chronic atrial fibrillation & 7 & 0.436 & 3.36 (3.05-3.70) & prediabetes & 14062 -0.160 & 0.98 (0.82-1.16) \\\\ chronic kidney disease stage 3 & 8 & 0.430 & 2.75 (2.53-2.98) & prediabetes & 14063 -0.161 & 1.22 (1.06-1.42) \\\\ atherosclerosis of arteries of... & 9 & 0.404 & 2.76 (2.42-3.15) & prediabetes & 14064 -0.161 & 2.50 (2.11-2.97) \\\\ bartert’s esophagus & 10 & 0.402 & 1.07 (0.84-1.37) & prediabetes & 14065 -0.162 & 1.04 (0.63-1.72) \\\\ chronic obstructive lung disease & 11 & 0.401 & 2.39 (2.19-2.60) & prediabetes & 14066 -0.166 & 1.12 (1.01-1.25) \\\\ paroxysmal atrial fibrillation & 12 & 0.395 & 2.58 (2.37-2.81) & localized, primary osteoarthritis & 14067 -0.167 & 1.50 (1.33-1.70) \\\\ systemic lupus erythematosusous & 13 & 0.395 & 1.51 (0.99-2.29) & prehens neoplasm of skin of low... & 14068 -0.167 & 0.68 (0.53-0.89) \\\\ atherosclerosis of artery of lo... & 14 & 0.394 & 2.45 (2.20-2.72) & cyst of ovary & 14069 -0.171 & 0.90 (0.64-1.26) \\\\ coronary atherosclerosis & 15 & 0.381 & 2.15 (1.95-2.36) & microscopic hematuria & 14070 -0.171 & 1.18 (1.01-1.37) \\\\ nonexudative age-related macular... & 16 & 0.377 & 2.15 (1.95-2.37) & problem related to lifestyle & 14071 -0.172 & 0.96 (0.48-1.91) \\\\ age related macular degeneration & 17 & 0.371 & 2.18 (1.76-2.71) & acquired hypothyroidism & 14072 -0.172 & 1.47 (1.34-1.62) \\\\ pseudoexfoliation glaucoma & 18 & 0.360 & 1.13 (0.72-1.76) & abnormal findings on diagnostic... & 14073 -0.176 & 0.63 (0.54-0.73) \\\\ degenerative joint disease invo... & 19 & 0.359 & 1.77 (1.52-2.06) & increased frequency of urination & 14074 -0.177 & 1.41 (1.22-1.64) \\\\ coronary arteriosclerosis & 20 & 0.357 & 2.00 (1.82-2.20) & disorder of skin & 14075 -0.178 & 1.18 (0.95-1.48) \\\\ coronary artery graft present & 21 & 0.346 & 1.64 (1.41-1.91) & thyroiditis & 14076 -0.180 & 0.87 (0.49-1.57) \\\\ aotcoroornary bypass graft present & 22 & 0.335 & 2.24 (1.98-2.54) & race\\_hispanic\\_or\\_latino & 14077 -0.186 & 0.96 (0.60-1.51) \\\\ dehydration & 23 & 0.332 & 2.94 (2.68-3.22) & herpes zoster without complication & 14078 -0.187 & 1.14 (0.96-1.35) \\\\ primary malignant neoplasm of f... & 24 & 0.327 & 1.19 (1.01-1.40) & altered sensation of skin & 14079 -0.191 & 1.00 (0.82-1.22) \\\\ malignant lymphoma & 25 & 0.322 & 1.54 (0.96-2.46) & generalized hyperhridrosis & 14080 -0.194 & 1.37 (1.07-1.76) \\\\ cerebral infarction due to thro... & 26 & 0.316 & 2.86 (2.46-3.32) & primary open angle glaucoma & 14081 -0.194 & 1.35 (1.20-1.52) \\\\ congestive heart failure & 27 & 0.313 & 3.67 (3.38-3.99) & stool finding & 14082 -0.195 & 1.48 (1.26-1.73) \\\\ old myocardial infarction & 28 & 0.299 & 2.04 (1.81-2.30) & primary gout & 14083 -0.196 & 1.80 (1.51-2.15) \\\\ sleep apnea & 29 & 0.294 & 1.16 (0.98-1.37) & localized, primary osteoarthritis & 14084 -0.199 & 1.10 (0.92-1.30) \\\\ acute hyposemic respiratory fa... & 30 & 0.292 & 4.02 (3.62-4.46) & diarrhea & 14085 -0.200 & 1.73 (1.57-1.90) \\\\ obstructive sleep apnea syndrome & 31 & 0.287 & 1.09 (0.96-1.24) & benign neoplasm of skin of uppe... & 14086 -0.204 & 0.78 (0.58-1.03) \\\\ primary malignant neoplasm of e... & 32 & 0.284 & 0.92 (0.56-1.53) & prostatitis & 14087 -0.204 & 1.20 (0.89-1.62) \\\\ sensorinaural hearing loss & 33 & 0.281 & 1.26 (1.09-1.47) & eruption & 14088 -0.205 & 1.25 (1.11-1.41) \\\\ retention of urine & 34 & 0.280 & 2.19 (1.97-2.44) & scar conditions and fibrosis of... & 14089 -0.206 & 1.00 (0.86-1.15) \\\\ atrial future & 35 & 0.280 & 2.14 (1.85-2.47) & hashimoto thyroiditis & 14090 -0.215 & 0.91 (0.49-1.68) \\\\ abdominal aortic aneurysm wino... & 36 & 0.275 & 1.85 (1.58-2.18) & acquired deformity of toe & 14091 -0.227 & 1.25 (0.94-1.65) \\\\ chronic kidney disease due to h... & 37 & 0.274 & 2.65 (2.42-2.29) & race\\_asian & 14092 -0.228 & 0.70 (0.50-0.99) \\\\ non-rheumatic aortic sclerosis & 38 & 0.271 & 2.64 (2.38-2.93) & localized swelling, mass and lu... & 14093 -0.242 & 1.48 (1.15-1.91) \\\\ type 2 diabetes mellitus & 39 & 0.267 & 2.14 (1.96-2.33) & benign neoplasm of skin of trunk & 14094 -0.245 & 0.91 (0.79-1.05) \\\\ introlatocal carcinoma in situ 0... & 40 & 0.265 & 0.62 (0.30-1.29) & benign essential hypertension & 14095 -0.245 & 1.86 (1.72-2.01) \\\\ chronic kidney disease stage 2 & 41 & 0.264 & 1.77 (1.55-2.03) & finding of frequency of urination & 14096 -0.255 & 1.48 (1.34-1.64) \\\\ degenerative disorder of macula & 42 & 0.263 & 2.23 (1.88-2.65) & benign essential microscopic he... & 14097 -0.258 & 1.10 (0.76-1.59) \\\\ sensorinaural hearing loss, bil... & 43 & 0.262 & 1.30 (1.17-1.43) & localized swelling, mass and lu... & 14098 -0.262 & 1.93 (1.67-2.23) \\\\ race\\_white & 44 & 0.262 & 1.25 (1.14-1.37) & digestive symptom & 14099 -0.267 & 0.91 (0.68-1.21) \\\\ metabolic encephalopathy & 45 & 0.259 & 4.42 (3.86-5.07) & type 1 diabetes mellitus wino... &"
    },
    {
      "title": "8 Task Templates",
      "text": "Bank Dataset: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this client subscrib to a term deposit? Yes or no? Answer: || {answer_choices[label] }}' Blood Dataset: answer_choices: 'No || Yes' jinja: '{{serialization}} Did the person donate blood? Yes or no? Answer: || {answer_choices[label] }}' California Dataset: answer_choices: 'No || Yes' jinja: '{{serialization}} Is this house block valuable? Yes or no? Answer: || {answer_choices[label] }}' Car Dataset: answer_choices: 'Unacceptable || | Acceptable || Good || Very good' jinja: '{{serialization}} How would you rate the decision to buy this car? Unacceptable, acceptable, good or very good? Answer: || {answer_choices[label] }}' Credit-g Dataset: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this person receive a credit? Yes or no? Answer: || {answer_choices[label] }}' Diabetes Dataset: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient have diabetes? Yes or no? Answer: || {answer_choices[label] }}' Heart Dataset: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient have diabetes? Yes or no? Answer: || {answer_choices[label] }}' Heart Dataset: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this personearn more than 50000 dollars per year? Yes or no? Answer: || {answer_choices[label] }}' **Jungle Dataset**: answer_choices: 'No || Yes' jinja: '{{serialization}} Does the white player win this two pieces endgame of Jungle Chess? Yes or no? Answer: || {answer_choices[label] }}' **End Of Life Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient die in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' Surgical Procedure Task: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient need a surgery in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Likelihood of Hospitalization Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Is this patient admitted to the hospital in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **End Of Life Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient die in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Surgical Procedure Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient need a surgery in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Likelihood of Hospitalization Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Is this patient admitted to the hospital in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **End Of Life Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient die in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Surgical Procedure Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient need a surgery in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Likelihood of Hospitalization Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Is this patient admitted to the hospital in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Surgical Procedure Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient need a surgery in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Likelihood of Hospitalization Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Is this patient admitted to the hospital in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Surgical Procedure Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient need a surgery in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Likelihood of Hospitalization Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Is this patient admitted to the hospital in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Surgical Procedure Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient need a surgery in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Likelihood of Hospitalization Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Is this patient admitted to the hospital in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Surgical Procedure Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient need a surgery in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Likelihood of Hospitalization Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Is this patient admitted to the hospital in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Surgical Procedure Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Does this patient need a surgery in the next nine months? Yes or no? Answer: || {answer_choices[label] }}' **Likelihood of Hospitalization Task**: answer_choices: 'No || Yes' jinja: '{{serialization}} Is this patient admitted to the hospital in the next nine months? Yes or no? Answer: || {answer_choices[label] }}'"
    },
    {
      "title": "9 Example Serializations",
      "text": "Bank Dataset (List Template): ``` - age:69 - type of job: retired - marital status: single - education: tertiary - has credit in default?: no - average yearly balance, in euros: 2144 - has housing loan?: no - has personal loan?: no - contact communication type: cellular - last contact day of the month: 29 - last contact month of year: jul - last contact duration, in seconds: 417 - number of contacts performed during this campaign and for this client: - number of days that passed by after the client was last contacted from a previous campaign: 184 - number of contacts performed before this campaign and for this client: 4 - outcome of the previous marketing campaign: success ``` **Bank Dataset (Text Template):** ``` The age is69. The type of job is retired. The marital status is single. The education is tertiary. The has credit in default? is no. The average yearly balance, in euros is 2144. The has housing loan? is no. The has personal loan? is no. The contact communication type is cellular. The last contact day of the month is 29. The last contact month of year is jul. The last contact duration, in seconds is 417. The number of contacts performed during this campaign and for this client is. The number of days that passed by after the client was last contacted from a previous campaign is 184. The number of contacts performed before this campaign and for this client is 4. The outcome of the previous marketing campaign is success. ``` Bank Dataset (Table-To-Text): ``` theageof69was69years.theretiredretired.themaritalstatusissinglewiththesinglename.theschoolhasaschooloffourstudents.thehasa creditof$500,000.Theaverageyearlybalanceineurosis2144.thehasatotalof2,000+housingunits.thehasanofficialloanof$500million.thestandarddefinitionhasbeenupdatedtothestandarddefinition.thecurrentrecordofthemonthis29.thefirstcontactmonthwasonDecember20,2005,andthenonMarch22,2006,thenextmonthwasonMarch22,2006.thefirstcontactdurationwas417seconds.theDVBhasaselectionofDVB.Theyear,inwhichtheclientwasfirstcontactedbyaformerairlineoperator,wasbyaformerairlineoperator,andbyaformerairlineoperator,hewasthefirsttoenterthepostoftheoffice.the4isa4-purposecycle.thefirstofthefirst20MBofthehistorytotheusethe20MB. ``` **Bank Dataset (Text T0):** ``` aretiredsoldiershowsoffhistattoos.acityisacitywithapopulationofsinglesandtertiaryeducation.no,theaverageyearlybalanceis2144euros.nohehasnopersonalloanorhousingloanamamiscontactingawomanonhercellphoneonthe29thdayoftthemonth.lastcontactmonthofyearwasjuly,lastcontactdurationwas417seconds.184daysaftertheclientwaslastcontactedfromapreviouscampaign.thepreviousmarketingcampaignforthisclientresultedinsuccesswith4contacts ``` **Bank Dataset (Text GPT-3):** ``` Thepersonis69yearsold,retired,single,andhasasteriaryeducation.Theyhavenoecreditindefault,andtheiraverageyearlybalanceis2144euros.Theyhavenohousingloanorpersonalloan.Thecontactcommunicationtypeiscellular,andthelastcontactwasonthe29thdayofthemonthandlasted417seconds.Theyhavebeencontacted4timesbeforethiscampaign,andtheoutcomeofthepreviousmarketingcampaignwassuccess. ``` **Blood Dataset (List Template):** ``` -Recency-monthssineclastdonation:23 -Frequency-totalnumberofdonation:1 -Monetary-totalblooddonatedinc.c.:250 -Time-monthssinecfirstdonation:23 ``` [MISSING_PAGE_EMPTY:27] Credit-g Dataset (List Template): ``` -Statusofexistingcheckingaccount: 0<=...<200DM -Durationinmonth:11 -Credithistory:existingcredits paidbackdulytillnow -Purpose:furniture/equipment -Creditamount:1577 -Savingsaccount/bonds:...>=1000 DM -Presentemploymentsince:<1 -Installmentrateinpercentageof disposableincome:4 -Personalstatusandsex:female:divored/separated/married -Otherdebtors/guarantors:none -Presentresidencesince:1 -Property:realestate -Ageinyears:20 -Otherinstallmentplans:none -Housing:own -Numberofexistingcreditsatthis bank:1 -Job:skilledemployee/official -Numberofpeoplebeingliableto provide maintenancefor:1.0 -Telephone:none -foreignworker:yes ``` **Credit-g Dataset (Text Template):** ``` TheStatusofexistingcheckingaccountis0<=...<200DM.TheDurationinmonthis11.TheCredithistoryisexistingcredits paidbackdulytillnow.ThePurposeis furniture/equipment.TheCreditamountis1577.TheSavingsaccount/bondsis...>=1000DM.ThePresentemploymentsinceis<1.TheInstallmentrateinpercentageof disposableincomeis4.ThePersonalstatusandsexisfemale:divored/separated/married.TheOtherdebtors/guarantorsisnone.ThePresentresidencesinceis1.ThePropertyisrealestate.TheAgeinyearsis20.TheOtherinstallmentplansisnone.TheHousingisown.TheNumberofexistingcreditsatthisbank is1.TheJobisskilledemployee/official.TheNumberofpeoplebeingliabletoprovidemaintenanceforis1.0.TheTelephoneisnone.Theforeignworkerisyes. ``` **Credit-g Dataset (Table-To-Text):** ``` the0.2(0.2)isatypeof00.2.Theaverageannualprecipitationis11.5millimetres(4.5in).theCredithistoryhasbeenpaidbacktoafewyears.thestandard_cellisa standard_cell.theamountwas1577.theSavingsaccount/bondswerefromtheSavingsaccount/bonds.therewere1,000employees.therewere4,000peopleinthecity.Themalehasamalescoreofthefemale.thedebtors$12.5million ($9.5million2013).thecurrentresidencehasa1,000feet(460)long.thestandardestateisastandardestate.Ithasaageof20years.thefirstinstallmentwasthefirstinstallmentintheyear2005.TheHousingisapublictransportsystemthatisanetworkofpublic.thecompanyhasanumberofexistingandexistingworks.thecavingsandhasanumberofexistingandexistingworks.thecampany'sjobisjobwiththejobnameas\"Success\".thenetworkhasanetworkofover800MT/s.theforeignworkerhasnoforeignworker. ``` **Credit-g Dataset (Text T0):** ``` Thecheckingaccounthasabalancedof0DM.Amanispayingforfurnitureandequipmentwithacreditcard.Thecreditamountis1577,thesavingsaccount/bondsare>=1000DM.Thepresentemployeehasbeenintthisjobforayear,andtheinstallmentrateis4.%ofdisposableincome.Afemalewhoisdivored/separated/marriedisrequestingaloan.Thepropertyislocatedinafagatedcommunityandhasbeenonthemarketsince.Themanis20yearsoldandhasnootherinstallmentplans.Thenumberofexistingcreditsatthisbankis1.Askilledemployeeisliabletoprovidaintenancefor1.0.Aforeignworkeriswithoutatelephone. ``` **Credit-g Dataset (Text GPT-3):** ``` Thepersonisa20-year-oldfemalewithacheckingaccountstatusof0-200DM.Shehasbeenemployedforlessthanayearandherinstallmentrateis4%ofherdisposableincome.Sheisdivored/separated/marriedandhasnootherdebtorsorguarantors.Shehasbeenlivinginhercurrentresidencefor1yearandownsexalestate.Shehas1creditatthisbankandisskilledemployee/official.Sheisliableformaintenancefor1person.Shehasnotelephone.Sheisaforeignworker. ```Diabetes Dataset (List Template): - Age: 30 years - Number of times pregnant: 1 - Diastolic blood pressure: 64 mmHg - Triceps skin fold thickness: 32 mm - Plasma glucose concentration at 2 hours in an oral glucose tolerance test (GTT): 122 mg/dl - 2-hour serum insulin: 156 uU/ml - Body mass index: 35.1 - Diabetes pedigree function: 0.692 Diabetes Dataset (Text Template): The Age is 30. The Number of times pregnant is 1. The Diastolic blood pressure is 64. The Triceps skin fold thickness is 32. The Plasma glucose concentration at 2 hours in an oral glucose tolerance test (GTT) is 122. The 2-hour serum insulin is 156. The Body mass index is 35.1. The Diabetes pedigree function is 0.692. Diabetes Dataset (Table-To-Text): The age was 30 years, and was the youngest ever to enter the age. the number of children is 1. The Diastolic blood pressure is 64. the Triceps can run up to 32. the 2 hours of the glucose is 122. the 2-hour cycle peaked to 156. The mass index was 35.1. The 0.692 is a fast and pathos. Diabetes Dataset (Text T0): The woman is 30 years old and has been pregnant once. The doctor checks the blood pressure and triceps skin fold thickness of the patient. The glucose concentration at 2 hours in an oral glucose tolerance test (GTT) was 122 and the 2-hour serum insulin was 156. The pedigree function of this family is 0.692. Diabetes Dataset (Text GPT-3): This patient 30 years old, has been pregnant once, has a diastolic blood pressure of 64 mmHg, and has a triceps skin fold thickness of 32 mm. The patient's plasma glucose concentration at 2 hours in an oral glucose tolerance test (GTT) is 122 mg/dl, and the patient's 2-hour serum insulin is 156 uU/ml. The patient's body mass index is 35.1, and the patient's diabetes pedigree function is 0.692. Heart Dataset (List Template): - Age of the patient: 43 years - Sex of the patient: male - Chest pain type: asymptomatic - Resting blood pressure: 132 - Serum cholesterol: 247 - Fasting blood sugar > 120 mg/dl: yes - Resting electrocardiogram results: probable or definite left ventricular hypertrophy - Maximum heart rate achieved: 143 - Exercise-induced angina: yes - ST depression induced by exercise relative to rest: 0.1 - Slope of the peak exercise ST segment: flat Heart Dataset (Text Template): The Age of the patient is 43. The Sex of the patient is male. The Chest pain type is asymptomatic. The Resting blood pressure is 132. The Serum cholesterol is 247. The Fasting blood sugar > 120 mg/dl is yes. The Resting electrocardiogram results is probable or definite left ventricular hypertrophy. The Maximum heart rate achieved is 143. The Exercise-induced angina is yes. The ST depression induced by exercise relative to rest is 0.1. The Slope of the peak exercise ST segment is flat. Heart Dataset (Table-To-Text): The male patient was the 43rd of the Age of the patient. The male is a male of the same class. The blood pressure was 132. The Serum cave has a cave of 247. the sugar has a low of 120 mg/dl. the type of the group is the type of the group that has a group of the group. The highest heart rate achieved is 143. the Exercise angina has a yes value. The ST depression has ranged from 0.1 to 0.1. the first segment was a flat of the ST. Heart Dataset (Text T0): The patient is a 43-year-old male. The chest pain is asymptomatic and resting blood pressure is 132. The doctor checks the fasting blood sugar and finds it is above 120 mg/dl. The resting ECG results showed probable or definite left ventricular hypertrophy, with maximum heart rate of 143 beats per minute. The patient had exercise-induced angina, with ST depression induced by exercise relative to rest of 0.1. The slope of the peak exercise segment is flat. [MISSING_PAGE_FAIL:30]"
    },
    {
      "title": "Large Healthcare Claims Dataset",
      "text": "End Of Life Task anonymized (List Template): ``` Summary:Thepatientisa73yearoldhispanicorlatinoman. ``` May30,2014:sawadoctorfordermatologyConditions:-chroniccholeystitis-applasticanemiaducetoddrugs April21,2017:visitedthehospitalfor12daysConditions:-chroniccholeystitis[...] ``` **End Of Life Task anonymized (Text Template)**: ``` Summary:Thepatientisa73yearoldhispanicorlatinoman. ``` OnMay30,2014thepatientsawadoctorfordermatologywithaprimarycomplaintofchroniccholeystitis.Hewasalsotreatedforplasticanemiaducetoddrugs. OnApril21,2017thepatientvisitedthehospitalfor12dayswithaprimarycomplaintofchroniccholeystitis.[...] ``` **End Of Life Task anonymized (List Permuted Names)**: ``` Summary:Thepatientisa73yearoldhispanicorlatinoman. ``` May30,2014:sawadoctorfordermatologyConditions:-onychomycosisduetodermatophyte-chronickidneydisease April21,2017:visitedthehospitalfor12daysConditions:-onychomycosisduetodermatophyte[...]"
    },
    {
      "title": "Supplementary Materials References",
      "text": "* A. Avati, K. Jung, S. Harman, L. Downing, A. Ng, and N. H. Shah (2018)Improving palliative care with deep learning. BMC medical informatics and decision making18 (4), pp. 55-64. Cited by: SS1. * V. Borisov, T. Leemann, K. Sessler, J. Haug, M. Pawelczyk, and G. Kasneci (2022)Deep neural networks and tabular data: a survey. Technical report arXiv:2110.01889. External Links: Link Cited by: SS1. * T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, G. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Shen, M. Sigler, S. Litwin, B. Gray, J. Chess, C. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020)Language models are few-shot learners. In Advances in Neural Information Processing Systems, Vol. 33, pp. 1877-1901. Cited by: SS1. * J. R. Curtis, P. D. Treece, E. L. Nielsen, J. Gold, P. S. Ciechanowski, S. E. Shannon, N. Khandelwal, and R. A. Young (2016)Randomized trial of communication facilitators to reduce family distress and intensity of end-of-life care. American journal of respiratory and critical care medicine193 (2), pp. 154-162. Cited by: SS1. * R. Detrano, A. Janosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. H. Guppy, S. Lee, and V. Froelicher (1989)International application of a new probability algorithm for the diagnosis of coronary artery disease. The American journal of cardiology64 (5), pp. 304-310. Cited by: SS1. * D. Dua and C. Graff (2017)UCI machine learning repository. Cited by: SS1. * L. Grinsztajn, E. Oyallon, and G. Varoquaux (2022)Why do tree-based models still outperform deep learning on typical tabular data?. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, Cited by: SS1. * M. K. Hasan, M. A. Alam, D. Das, E. Hossain, and M. Hasan (2020)Diabetes prediction using ensembling of different machine learning classifiers. IEEE Access8, pp. 76516-76531. External Links: Link, Document Cited by: SS1. * G. Hripcsak, J. D. Duke, N. H. Shah, C. G. Reich, V. Huser, M. J. Schuerme, M. A. Suchard, R. W. Park, I. K. Wong, P. R. Rijnbeek, N. van der Lei, N. Pratt, N. Nor&#233, N. G. N. Nor, Y. Li, P. E. Stang, D. Madigan, and P. B. Ryan (2015)Observational health data sciences and informatics (OHDSI): opportunities for observational researchers. MEDINFO 2015: eHealth-enabled health, pp. 574-578. External Links: Link, Document Cited by: SS1. * A. Kadra, M. Lindauer, F. Hutter, and J. Grabocka (2021)Well-tuned simple nets excel on tabular datasets. Advances in neural information processing systems34, pp. 23928-23941. Cited by: SS1. * R. Kohavi (1996)Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid. In Kdd, Vol. 96, pp. 202-207. Cited by: SS1. * S. Moro, P. Cortez, and P. Rita (2014)A data-driven approach to predict the success of bank telemarketing. Decision Support Systems62, pp. 22-31. Cited by: SS1. * Y. Muhammad, M. Tahir, M. Hayat, and K. T. Chong (2020)Early and accurate detection and diagnosis of heart disease using intelligent computational model. Scientific Reports10 (1), pp. 19747. Cited by: SS1. * L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, K. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, J. Christiano, J. Leike, and R. Lowe (2022)Training language models to follow instructions with human feedback. arXiv:2203.02155. External Links: Link, 2203.02155 Cited by: SS1. * R. K. Pace and R. Barry (1997)Sparse spatial autoregressions. Statistics & Probability Letters33 (3), pp. 291-297. Cited by: SS1. * F. Pedregosa, A. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay (2011)Scikit-learn: machine learning in Python. Journal of Machine Learning Research12, pp. 2825-2830. Cited by: SS1. * V. Sanh, A. Webson, C. Raffel, S. Bach, Z. Sucha, A. Alyafeai, A. Shaegler, M. A. Raja, M. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Kim, T. Chhablani, N. Nayak, D. Datta, J. Chang, M. T. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Song, H. Banden, R. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fevrier, R. Frehan, T. Kehan, S. Scao, L. Biderman, L. Gao, T. Wolf, and A. M. Rush (2022)Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, Cited by: SS1. * M. S. Sanker, A. A. K. * Smith et al. (1988) Smith, J. W., Everhart, J., Dickson, W., Knowler, W., and Johannes, R. (1988). Using the ADAP Learning Algorithm to Forecast the Onset of Diabetes Mellitus. _Proceedings of the Annual Symposium on Computer Application in Medical Care_, pages 261-265. * van Rijn and Vis (2014) van Rijn, J. N. and Vis, J. K. (2014). Endgame analysis of dou shou qi. _ICGA Journal_, 37(2):120-124. * Yeh et al. (2009) Yeh, I.-C., Yang, K.-J., and Ting, T.-M. (2009). Knowledge discovery on rfm model using bernoulli sequence. _Expert Systems with Applications_, 36(3):5866-5871."
    }
  ]
}