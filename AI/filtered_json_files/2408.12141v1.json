{
  "title": "TRRG: Towards Truthful Radiology Report Generation With Cross-modal Disease Clue Enhanced Large Language Models",
  "authors": [
    "Yuhao Wang",
    "Chao Hao",
    "Yawen Cui",
    "Xinqi Su",
    "Weicheng Xie",
    "Tao Tan",
    "Zitong Yu"
  ],
  "abstract": "\n The vision-language modeling capability of multi-modal large language models has attracted wide attention from the community. However, in medical domain, radiology report generation using vision-language models still faces significant challenges due to the imbalanced data distribution caused by numerous negated descriptions in radiology reports and issues such as rough alignment between radiology reports and radiography. In this paper, we propose a truthful radiology report generation framework, namely TRRG, based on stage-wise training for cross-modal disease clue injection into large language models. In pre-training stage, During the pre-training phase, contrastive learning is employed to enhance the visual encoder's ability to perceive fine-grained disease details. In fine-tuning stage, the clue injection module we proposed significantly enhances the disease-oriented perception capability of the large language model by effectively incorporating the robust zero-shot disease perception. Finally, through the cross-modal clue interaction module, our model effectively achieves the multi-granular interaction of visual embeddings and an arbitrary number of disease clue embeddings. This significantly enhances the report generation capability and clinical effectiveness of multi-modal large language models in the field of radiology reportgeneration. Experimental results demonstrate that our proposed pre-training and fine-tuning framework achieves state-of-the-art performance in radiology report generation on datasets such as IU-Xray and MIMIC-CXR. Further analysis indicates that our proposed method can effectively enhance the model's ability to perceive diseases and improve its clinical effectiveness. \n",
  "references": [
    {
      "id": null,
      "title": "TRRG: Towards Truthful Radiology Report Generation With Cross-modal Disease Clue Enhanced Large Language Models",
      "authors": [
        "Yuhao Wang",
        "Chao Hao",
        "Yawen Cui",
        "Xinqi Su",
        "Weicheng Xie",
        "Tao Tan",
        "Zitong Yu"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "J.-B Alayrac",
        "J Donahue",
        "P Luc",
        "A Miech",
        "I Barr",
        "Y Hasson",
        "K Lenc",
        "A Mensch",
        "K Millican",
        "M Reynolds"
      ],
      "year": "2022",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Publicly available clinical BERT embeddings",
      "authors": [
        "E Alsentzer",
        "J R Murphy",
        "W Boag",
        "W.-H Weng",
        "D Jin",
        "T Naumann",
        "M Mcdermott"
      ],
      "year": "2019",
      "venue": "Publicly available clinical BERT embeddings",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "2024a. LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge",
      "authors": [
        "G Chen",
        "L Shen",
        "R Shao",
        "X Deng",
        "L Nie"
      ],
      "year": "",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "2024b. Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation",
      "authors": [
        "W Chen",
        "L Shen",
        "J Lin",
        "J Luo",
        "X Li",
        "Y Yuan"
      ],
      "year": "",
      "venue": "62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Align, reason and learn: Enhancing medical vision-and-language pre-training with knowledge",
      "authors": [
        "Z Chen",
        "G Li",
        "X Wan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Crossmodal memory networks for radiology report generation",
      "authors": [
        "Z Chen",
        "Y Shen",
        "Y Song",
        "X Wan"
      ],
      "year": "2022",
      "venue": "Crossmodal memory networks for radiology report generation",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Generating radiology reports via memory-driven transformer",
      "authors": [
        "Z Chen",
        "Y Song",
        "T.-H Chang",
        "X Wan"
      ],
      "year": "2020",
      "venue": "EMNLP",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Meshed-Memory Transformer for Image Captioning",
      "authors": [
        "M Cornia",
        "M Stefanini",
        "L Baraldi",
        "R Cucchiara"
      ],
      "year": "2020",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Meshed-Memory Transformer for Image Captioning",
      "authors": [
        "M Cornia",
        "M Stefanini",
        "L Baraldi",
        "R Cucchiara"
      ],
      "year": "2020",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "authors": [
        "W Dai",
        "J Li",
        "D Li",
        "A M H Tiong",
        "J Zhao",
        "W Wang",
        "B Li",
        "P Fung",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Preparing a collection of radiology examinations for distribution and retrieval",
      "authors": [
        "D F Dina",
        "M D Kohli",
        "M B Rosenman",
        "S E Shooshan",
        "R Laritza",
        "A Sameer",
        "G R Thoma",
        "C J Mcdonald"
      ],
      "year": "2015",
      "venue": "Journal of the American Medical Informatics Association Jamia",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition",
      "authors": [
        "S.-C Huang",
        "L Shen",
        "M P Lungren",
        "S Yeung"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Mistral 7B",
      "authors": [
        "A Q Jiang",
        "A Sablayrolles",
        "A Mensch",
        "C Bamford",
        "D S Chaplot",
        "D D Casas",
        "F Bressand",
        "G Lengyel",
        "G Lample",
        "L Saulnier"
      ],
      "year": "2023",
      "venue": "Mistral 7B",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Promptmrg: Diagnosis-driven prompts for medical report generation",
      "authors": [
        "H Jin",
        "H Che",
        "Y Lin",
        "H Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "MIMIC-CXR: A large publicly available database of labeled chest radiographs",
      "authors": [
        "A E W Johnson",
        "T J Pollard",
        "N R Greenbaum",
        "M P Lungren",
        "C Y Deng",
        "Y Peng",
        "Z Lu",
        "R G Mark",
        "S J Berkowitz",
        "S Horng"
      ],
      "year": "2019",
      "venue": "MIMIC-CXR: A large publicly available database of labeled chest radiographs",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning",
      "authors": [
        "J Johnson",
        "A Karpathy",
        "L Fei-Fei"
      ],
      "year": "2016",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "a. Knowledge-driven Encode, Retrieve",
      "authors": [
        "C Y Li",
        "X Liang",
        "Z Hu",
        "E P Xing"
      ],
      "year": "2019",
      "venue": "a. Knowledge-driven Encode, Retrieve",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Knowledge-Driven Encode, Retrieve, Paraphrase for Medical Image Report Generation",
      "authors": [
        "C Y Li",
        "X Liang",
        "Z Hu",
        "E P Xing"
      ],
      "year": "2019",
      "venue": "AAAI",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems",
      "authors": [
        "J Li",
        "R Selvaraju",
        "A Gotmare",
        "S Joty",
        "C Xiong",
        "S C H Hoi"
      ],
      "year": "2021",
      "venue": "Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Dynamic Graph Enhanced Contrastive Learning for Chest X-ray Report Generation",
      "authors": [
        "M Li",
        "B Lin",
        "Z Chen",
        "H Lin",
        "X Liang",
        "X Chang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation",
      "authors": [
        "Y Li",
        "X Liang",
        "Z Hu",
        "E P Xing"
      ],
      "year": "2018",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "ROUGE: A Package for Automatic Evaluation of Summaries",
      "authors": [
        "C.-Y Lin"
      ],
      "year": "2004",
      "venue": "ACL",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "2021a. Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation",
      "authors": [
        "F Liu",
        "X Wu",
        "S Ge",
        "W Fan",
        "Y Zou"
      ],
      "year": "",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning",
      "authors": [
        "J Lu",
        "C Xiong",
        "D Parikh",
        "R Socher"
      ],
      "year": "2017",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Representation Learning with Contrastive Predictive Coding. Cornell University -arXiv",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation Learning with Contrastive Predictive Coding. Cornell University -arXiv",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "X-Linear Attention Networks for Image Captioning",
      "authors": [
        "Y Pan",
        "T Yao",
        "Y Li",
        "T Mei"
      ],
      "year": "2020",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "K Papineni",
        "S Roukos",
        "T Ward",
        "W.-J Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Amanda",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Learning Transferable Visual Models From Natural Language Supervision",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Self-Critical Sequence Training for Image Captioning",
      "authors": [
        "S J Rennie",
        "E Marcheret",
        "Y Mroueh",
        "J Ross",
        "V Goel"
      ],
      "year": "2017",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Automatic Radiology Reports Generation via Memory Alignment Network",
      "authors": [
        "H Shen",
        "M Pei",
        "J Liu",
        "Z Tian"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "CheXbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT",
      "authors": [
        "A Smit",
        "S Jain",
        "P Rajpurkar",
        "A Pareek",
        "A Y Ng",
        "M P Lungren"
      ],
      "year": "2020",
      "venue": "CheXbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Clip4caption: Clip for video caption",
      "authors": [
        "M Tang",
        "Z Wang",
        "Z Liu",
        "F Rao",
        "D Li",
        "X Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning",
      "authors": [
        "E Tiu",
        "E Talius",
        "P Patel",
        "C P Langlotz",
        "A Y Ng",
        "P Rajpurkar"
      ],
      "year": "2022",
      "venue": "Nature Biomedical Engineering",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozi√®re",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Attention is All you Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A N Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "NIPS",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "CIDEr: Consensus-based image description evaluation",
      "authors": [
        "R Vedantam",
        "C L Zitnick",
        "D Parikh"
      ],
      "year": "2015",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Cross-modal prototype driven network for radiology report generation",
      "authors": [
        "J Wang",
        "A Bhalerao",
        "Y He"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "2022a. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-tosequence learning framework",
      "authors": [
        "P Wang",
        "A Yang",
        "R Men",
        "J Lin",
        "S Bai",
        "Z Li",
        "J Ma",
        "C Zhou",
        "J Zhou",
        "H Yang"
      ],
      "year": "",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "2023a. Self adaptive global-local feature enhancement for radiology report generation",
      "authors": [
        "Y Wang",
        "K Wang",
        "X Liu",
        "T Gao",
        "J Zhang",
        "G Wang",
        "Ieee",
        "Z Wang",
        "L Liu",
        "L Wang",
        "L Zhou"
      ],
      "year": "2023",
      "venue": "METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "2023c. R2gengpt: Radiology report generation with frozen llms",
      "authors": [
        "Z Wang",
        "L Liu",
        "L Wang",
        "L Zhou"
      ],
      "year": "",
      "venue": "Meta-Radiology",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "2022b. A medical semantic-assisted transformer for radiographic report generation",
      "authors": [
        "Z Wang",
        "M Tang",
        "L Wang",
        "X Li",
        "L Zhou"
      ],
      "year": "",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "",
      "authors": [
        "Z Wang",
        "Z Wu",
        "D Agarwal",
        "J Sun"
      ],
      "year": "2022",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "A Self-Boosting Framework for Automated Radiographic Report Generation",
      "authors": [
        "Z Wang",
        "L Zhou",
        "L Wang",
        "X Li"
      ],
      "year": "2021",
      "venue": "CVPR",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Multimodal Recurrent Model with Attention for Automated Radiology Report Generation",
      "authors": [
        "Y Xue",
        "T Xu",
        "L R Long",
        "Z Xue",
        "S Antani",
        "G R Thoma",
        "X Huang"
      ],
      "year": "2018",
      "venue": "Multimodal Recurrent Model with Attention for Automated Radiology Report Generation",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Knowledge Matters: Radiology Report Generation with General and Specific Knowledge",
      "authors": [
        "S Yang",
        "X Wu",
        "S Ge",
        "S K Zhou",
        "L Xiao"
      ],
      "year": "2021",
      "venue": "Medical Image Analysis",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Knowledge matters: Chest radiology report generation with general and specific knowledge",
      "authors": [
        "S Yang",
        "X Wu",
        "S Ge",
        "S K Zhou",
        "L Xiao"
      ],
      "year": "2022",
      "venue": "Medical image analysis",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Automatic Generation of Medical Imaging Diagnostic Report with Hierarchical Recurrent Neural Network",
      "authors": [
        "C Yin",
        "B Qian",
        "J Wei",
        "X Li",
        "X Zhang",
        "Y Li",
        "Q Zheng"
      ],
      "year": "2020",
      "venue": "ICDM",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Multimodal transformer with multi-view visual representation for image captioning",
      "authors": [
        "J Yu",
        "J Li",
        "Z Yu",
        "Q Huang"
      ],
      "year": "2019",
      "venue": "IEEE transactions on circuits and systems for video technology",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Automatic Radiology Report Generation Based on Multi-view Image Fusion and Medical Concept Enrichment",
      "authors": [
        "J Yuan",
        "H Liao",
        "R Luo",
        "J Luo",
        "Miccai",
        "Y Zhang",
        "X Wang",
        "Z Xu",
        "Q Yu",
        "A Yuille",
        "D Xu"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Abstract",
      "text": "The vision-language modeling capability of multi-modal large language models has attracted wide attention from the community. However, in medical domain, radiology report generation using vision-language models still faces significant challenges due to the imbalanced data distribution caused by numerous negated descriptions in radiology reports and issues such as rough alignment between radiology reports and radiography. In this paper, we propose a truthful radiology report generation framework, namely TRRG, based on stage-wise training for cross-modal disease clue injection into large language models. In pre-training stage, During the pre-training phase, contrastive learning is employed to enhance the visual encoder's ability to perceive fine-grained disease details. In fine-tuning stage, the clue injection module we proposed significantly enhances the disease-oriented perception capability of the large language model by effectively incorporating the robust zero-shot disease perception. Finally, through the cross-modal clue interaction module, our model effectively achieves the multi-granular interaction of visual embeddings and an arbitrary number of disease clue embeddings. This significantly enhances the report generation capability and clinical effectiveness of multi-modal large language models in the field of radiology reportgeneration. Experimental results demonstrate that our proposed pre-training and fine-tuning framework achieves state-of-the-art performance in radiology report generation on datasets such as IU-Xray and MIMIC-CXR. Further analysis indicates that our proposed method can effectively enhance the model's ability to perceive diseases and improve its clinical effectiveness."
    },
    {
      "title": "Introduction",
      "text": "Radiology report generation aims to automatically generate radiology reports for medical images, such as X-rays, MRI, and CT scans, etc. Radiology report generation can effectively alleviate the workload of clinical practitioners, minimize misdiagnosis and missed diagnoses resulting from human judgment errors, and expedite clinical workflows. A radiology report typically consists of a paragraph with multiple sentences. Some sentences describe normal organ appearances in radiography, while others delineate the findings of diseases reflected in the images, including the location of lesions, types of diseases, severity, etc. With the development of visual-language models, encoder-decoder-based text generation models have become the most prevalent architecture. However, unlike natural domain image captioning[12, 13, 14, 15, 16], radiology reports contain a significant amount of normal descriptions. Abnormal descriptions related to diseases are crucial for radiology reports. This leads to sparse supervision signals regarding diseases. Many pure encoder-decoder-based models[13, 14, 15, 16, 17, 18, 19, 20, 11] for image-text coarse-grained alignment lack fine-grained perception of diseases. Consequently, these models fail to effectively focus on the correct image regions, resulting in insufficient clinical effectiveness of the generated radiology reports. Multi-modal large language models[13, 12, 14], as powerful infrastructure adaptable to various downstream tasks, perform well in tasks such as image captioning and Visual Question Answering (VQA). Through techniques such as low-parameter fine-tuning, these models can efficiently enhance multimodal large language models for radiology report generation. To further enhance the clinical effectiveness of radiology report generation (RRG), we propose a stage-wise, cross-modal clue-injection approach based on large language models. Firstly, we perform pretraining using image-text contrastive learning [1] through sampling sentences from radiology reports to enhance the vision encoder's ability to perceive diseases effectively. Moreover, by harnessing the powerful zero-shot inference capability of the vision encoder, our model can effectively acquire clues about various diseases. In the fine-tuning stage, we conduct clue injection through visual disease tokens and several disease clues embeddings. Subsequently, we propose a cross-modal clue interaction module that effectively integrates visual embeddings and disease clue embeddings to enhance the disease perception capability of the large language model. Finally, we propose a disease-aware consistency loss to assist large language models in training for radiology report generation tasks. The disease-aware consistency loss effectively enhances textual semantic supervision signals, promoting the model to acquire disease-awareness capability while generating radiology reports. Our model achieves optimal performance in generating radiology re ports. We conduct experiments on the IU-Xray [4] and MIMIC-CXR [13] datasets. Compared with previous studies, our method achieves state-of-the-art results, significantly improving both the quality of language generation and clinical effectiveness. Further ablation experiments validate the effectiveness of our proposed disease clue injection module, cross-modal clue interaction module, and disease-aware consistency loss. Qualitative results offer intuitive interpretations of the generated radiology reports. * We propose the TRRG for truthful radiology report generation using an disease clue injection enhanced large language model. TRRG alleviates the problem of coarse-grained alignment between radiography and report, enabling the model acquire fine-grained disease-aware perception. * We introduce a cross-modal disease clue interaction module, which effectively integrates visual embeddings and disease clue embeddings to guide large language models in producing higher-quality radiology reports. * Experimental results demonstrate that our proposed method outperforms previous approaches in terms of both language generation quality and clinical effectiveness on two datasets, IU-Xray [4] and MIMIC-CXR [13]."
    },
    {
      "title": "Related Work",
      "text": "Radiology Report GenerationCurrently, most research on radiology report generation adopts an encoder-decoder architecture. Some studies employ CNN-RNN architecture [21, 14, 20]. Through convolution neural network encoders, these models encode images into vectors, which are then decoded token by token using recurrent neural networks. Some studies utilize a hierarchical generation process [20], in which the model initially generates relevant topics as keywords. Subsequently, it employs LSTM architecture to generate sentences for each keyword. With the advancement of transformers [21], which benefit from parallel training, are suitable for modeling long sequences, and have the capacity to integrate various modalities of visual and language data, radiology report generation is gradually transitioning towards architectures based on transformers [15, 20, 21, 22, 23]. Some studies utilize memory networks [22, 23, 24] to help the model learn more effective patterns of medical knowledge. For example, use a memory matrix to simulate memory information and continuously update this matrix during training to generate higher-quality radiology reports. Others adopt prototype learning[20], storing prototype vectors and updating them through cross-modal queries and responses to drive the model to store pattern information in memory. Furthermore, some research enhances the quality of generated radiology reports by incorporating external knowledge embeddings [14, 20]. This involves constructing knowledge graphs of anatomical structures and disease-related relationships as auxiliary features for embedding, which are then combined with the model for the final text generation. Historical radiography-report pairs [20] are also utilized as domain knowledge, mimicking the process of doctors generating textual reports, thereby improving the performance of the respective models during decoding. With the advancement of large language models. However, all these coarse-grained alignment methods lead to limitations in the clinical effectiveness of the generated radiology reports. Multi modal Large Language ModelMulti-modal Large Models: With the advancement of large models, many lightweight large models such as LLAMA [20] and Mistral [21] have provided possibilities for usage and deployment in low-resource scenarios. The combination of large language models and vision encoders enables these models to effectively process tasks that involve multimodal data. Some studies [14, 20, 21, 22], utilize visual mappers, such as basic linear layers, to map the features from vision encoders to token-level features of large language models with matching dimensions. Through this simple mapping technique, large models demonstrate exceptional performance in various multimodal tasks, such as image captioning and visual question answering. This suggests that multimodal large models can achieve excellent visual perception capabilities with only minor parameter fine-tuning. Fine-tuning large language models through instructions [22, 23] to become task-driven for various multimodal tasks is a widely adopted approach. By crafting specific instructions, large language models can generate relevant results tailored to the specific task type. Some studies[23] have injected disease labels as prior information into radiology report generation frameworks; however, this approach limits the scalability of such frameworks Vision Language PretrainingVision language models aim to align vision and language features through cross-modal interaction [14, 20, 21]. One of the most influential studies is CLIP [15], which utilizes contrastive learning to align paired image embeddings and text embeddings. CLIP achieves a significant performance improvement across several downstream tasks, including zero-shot classification and cross-modal retrieval. Some medical multimodal pretraining methods [20, 21, 22] have demonstrated strong performance on zero-shot disease classification tasks. Meanwhile, some studies[22] have effectively enhanced the alignment and reasoning capabilities of multimodal pre-trained models by incorporating external medical knowledge. These approaches achieve inference for common diseases and the diagnosis of rare diseases without the need for extensive structured labels. This provides insights for our model to generate radiology reports."
    },
    {
      "title": "Methods",
      "text": "The proposed TRRG consists of a two-stage training process. In the pretraining stage, we focus on disease-aware cross-modal fine-grained alignment between radiographs and corresponding radiology reports. Large language models typically facilitate the alignment between images and text through supervised signals at the token level. By utilizing sentence-level contrastive learning, our approach enhances the detailed capture of disease information by the vision encoder. Furthermore, in conjunction with our proposed clue injection module and cross-modal clue interaction module in the fine-tuning stage, our model demonstrates superior performance in both language generation and clinical effectiveness. At the same time, our proposed cross-modal clue interaction module effectively facilitates alignment between visual embeddings and disease clue embeddings. The details of TRRG are shown in Fig. 4."
    },
    {
      "title": "Stage 1: Disease-Aware Cross-Modal Fine Gained Alignment",
      "text": "Studies [22, 13] demonstrate that pre-trained CLIP models, trained on large-scale medical image-text pairs, exhibit accuracy comparable to human performance in zero-shot disease classification tasks. Inspired by this, we decompose our pipeline into stage-wise training. During the pre-training stage, we utilize random sampling of sentences from radiology reports for training. This approach enhances the representation ability of vision encoders and also accommodates the robust clue prompting of disease clue injection modules. Given an Image-Text Pair \\((I,T)\\), where \\(T\\) : \\(\\{T_{1},T_{2},...,T_{t}\\}\\), \\(T_{i}\\) represents the sentence-level component of the radiology report. The images are encoded into \\(\\mathbf{v}=\\{\\mathbf{v}_{cls},\\mathbf{v}_{1},...,\\mathbf{v}_{n}\\}\\in\\mathbb{R }^{(n+1)*d}\\) by the transformer-based vision encoder, where \\(n\\) is the number of patches in the images and \"cls\" represents the pooling token of the outputs. \\[\\mathbf{v}=\\mathbf{E}_{img}(I). \\tag{1}\\] For the sentence randomly sampled from the corresponding radiology report, a BERT-based model encodes it as \\(\\mathbf{t}=\\{\\mathbf{t}_{cls},\\mathbf{t}_{1},...,\\mathbf{t}_{m}\\}\\in\\mathbb{R }^{(m+1)*d}\\), where \\(m\\) is the maximum length of a sentence in the training corpus. We extract their global representations \\(\\mathbf{t}\\in\\mathbb{R}^{1*d}\\) by utilizing the \"CLS\" token of a BERT-like model. \\[\\mathbf{t}=\\mathbf{E}_{txt}(T_{r}), \\tag{2}\\] where \\(T_{r}\\) is a randomly selected sentence from \\(T\\). Finally, we compute the text-to-image contrastive loss and image-to-text contrastive loss to enhance our vision encoder's ability to learn better disease-oriented representations. We only utilize pooled visual tokens \\(\\mathbf{v}_{cls}\\) and textual tokens \\(\\mathbf{v}_{cls}\\). For an image embedding and text embedding \\(\\{\\mathbf{v}^{\\prime}_{i},\\mathbf{t}_{i}\\}_{i=1}^{N}\\), the optimizing objective is InfoNCE loss [11], which can be formulated as: \\[\\mathcal{L}_{v2t}=-log(\\frac{exp(\\sigma(\\mathbf{t}_{i},\\mathbf{v}^{\\prime}_{i })/\\tau)}{\\sum_{j=1}^{N}exp(\\sigma(\\mathbf{t}_{i},\\mathbf{v}^{\\prime}_{j})/ \\tau)}), \\tag{3}\\] \\[\\mathcal{L}_{t2v}=-log(\\frac{exp(\\sigma(\\mathbf{v}^{\\prime}_{i},\\mathbf{t}_{i })/\\tau)}{\\sum_{j=1}^{N}exp(\\sigma(\\mathbf{v}^{\\prime}_{i},\\mathbf{t}_{j})/ \\tau)}). \\tag{4}\\] The total loss of the pretraining stage is calculated as follows, where \\(N\\) represents the batch size, and \\(\\tau\\) is a temperature factor: \\[\\mathcal{L}=\\mathcal{L}_{v2t}+\\mathcal{L}_{t2v}. \\tag{5}\\]"
    },
    {
      "title": "Stage 2: Clue Enhanced Instruct Tuning On Radiology Report Generation",
      "text": "Visual EmbeddingGiven an image \\(\\mathbf{x}\\in\\mathbb{R}^{H\\times W\\times C}\\), we reshape it into a sequence of flattened 2D patches \\(\\mathbf{v}\\in\\mathbb{R}^{n\\times d}\\). The transformer-based frozen vision encoder encodes the input patch tokens into an encoded patch embedding \\(\\mathbf{x}_{e}=\\{\\mathbf{v}_{cls},\\mathbf{v}_{1},...,\\mathbf{v}_{n}\\}\\in \\mathbb{R}^{(n+1)\\times d}\\). Subsequently, the patch embedding will be fed into a visual mapper layer composed of linear layers, which transform the patch embedding \\(\\mathbf{v}_{d}\\in\\mathbb{R}^{n\\times d}\\) to the same dimension as the LLM's word embedding. Thus, we obtain generative vision embedding \\(\\mathbf{v}_{d}\\in\\mathbb{R}^{n\\times d}\\) and visual disease expert tokens \\(\\mathbf{v}_{cls}\\in\\mathbb{R}^{1\\times d}\\) that have been encoded only by the frozen vision encoder. The visual embedding process can be expressed as follows: \\[\\mathbf{v}_{e}=E_{img}(x), \\tag{6}\\] \\[\\mathbf{v}_{d}=W\\mathbf{v}_{e}+b, \\tag{7}\\] where \\(W\\) is the trainable weight of the visual mapper. \\[\\mathbf{v}_{cls}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{v}_{i}\\quad\\mathbf{v}_{i} \\in\\mathbb{R}^{1\\times d}. \\tag{8}\\] \\(\\mathbf{v}_{i}\\) is an element of the path token sequences \\(\\mathbf{v}_{e}\\). After visual encoding, we obtain the disease visual embedding \\(\\mathbf{v}_{d}\\in\\mathbb{R}^{n\\times d}\\) and the disease visual expert token \\(\\mathbf{v}_{cls}\\in\\mathbb{R}^{1\\times d}\\). Clue Injection ModuleFollowing Gloria's work [13], we utilize a variety of language descriptions to construct disease clue prompts. By extracting multiple descriptive phrases from the dataset related to the severity and location of a specific disease, we randomly combined phrases associated with disease type, severity level, and location to create disease clue prompts. Specifically, our prompt construction template is as follows: \\[Clue:<severity><disease>at<location>\\] We created manual templates for \\(m\\) common diseases, such as opacity, pneumonia, and pneumothorax, to act as potential prompts for identifying diseases. Then, the frozen Figure 1: The training strategy of our proposed TRRGtext encoder encoded disease clue prompts into multiple disease clue embeddings \\(\\mathbf{c}:\\{\\mathbf{c}^{i}\\}_{i=1}^{m}\\), where \\(\\mathbf{c}^{i}=\\{\\mathbf{c_{cls}},\\mathbf{c_{1}},....,\\mathbf{c_{r}}\\}\\), and \\(\\mathbf{c}^{i}\\in\\mathbb{R}^{(r+1)\\times d}\\) is the i-th disease clue embedding. Here, \\(m\\) represents the number of diseases defined, and \\(r\\) is the maximum length of disease clue prompts. Subsequently, we compute clue weights, which represent the importance of disease clue expert token \\(\\mathbf{c}^{i}_{cls}\\in\\mathbb{R}^{1*d}\\) to visual disease expert tokens \\(\\mathbf{v}_{cls}\\in\\mathbb{R}^{1*d}\\) through matrix multiplication. The computing process of clue weight can be formulated as follows: \\[w_{i}=\\text{softmax}\\left(\\mathbf{v}_{cls}\\cdot\\mathbf{c}^{i}_{cls}\\right) \\quad i\\in 1,.....m, \\tag{9}\\] where clue weight \\(w:\\)\\(\\{w_{1},w_{2},\\cdots,w_{m}\\}\\in\\mathbb{R}^{1\\times m}\\), where \\(m\\) represents the number of different disease clues. We assign weights to each clue to represent the importance of this disease relative to the images. After that, we utilize the Hadamard product to obtain the weighted disease clue embedding. The weighted clue embeddings are computed as follows: \\[\\mathbf{c}^{i}=w_{i}\\odot\\mathbf{c}^{i}. \\tag{10}\\] Due to some clues being irrelevant to the corresponding images, we only select the top-k important clues as disease expert clues for the final model input. Our final injection clues are: \\[\\mathbf{c}_{s}:\\{\\mathbf{c}^{i}\\}\\quad i\\in\\mathbf{topk}(w,k), \\tag{11}\\] where \\(\\mathbf{topk}(w,k)\\) presents the indices of the top-k highest values in the set \\(w\\). Finally, we consider the multiple disease clue embedding \\(\\mathbf{c}_{s}\\in\\mathbb{R}^{k\\times r\\times d}\\) as our injection clues. Cross Modal Clue InteractionFor multi-disease clues \\(\\mathbf{c}_{s}\\in\\mathbb{R}^{k\\times r\\times d}\\), these clues are highly aligned with visual representations obtained through the frozen vision encoder but do not fully interact with the generated visual features obtained from the visual mapper. Therefore, we propose a Cross-Modal Clue Interaction Module to simultaneously enhance generative representations and facilitate cross-modal interaction of disease clues. Typically, there is often a significantly larger number of disease clue tokens compared to visual tokens. To address the excessive disease clue token input, we define a set of learnable queries for cross-modal interaction. Finally, we propose a Disease Clue Consistency Loss to maintain sufficient attention to disease clue embeddings in cross-modal interaction and provide disease-oriented supervision signals for fine-tuning with large language models. Attention mechanisms are adopted in the multimodal interaction module. We adopt a two-stream architecture for cross-modal feature interaction. Each structure comprises a self-attention module, a cross-attention module, and a Feed Forward layer. The self-attention layer facilitates intra-modal interaction, enhancing feature representation within each modality. Conversely, the cross-attention mechanism ensures alignment between textual clues and visual representations, facilitating cross-modal interaction by enforcing consistency across different modalities. The attention is de Figure 3: Architecture of Clue Injection Module, HP and FL represent Hadamard Product and Flatten, respectively. Figure 2: During the fine-tuning stage, the visual encoder and the clue encoder are frozen, and disease clues are injected simultaneously through the clue injection module. In this stage, visual embeddings processed by the visual mapper interact with disease clue embeddings through cross-modal clue interaction. Finally, the frozen large language model is fine-tuned through instruction-based fine-tuning to achieve medical image report generation. [MISSING_PAGE_FAIL:5] the model on the MIMIC-CXR dataset for 5 epochs and on the IU-Xray dataset for 20 epochs. The batch size was set to 8, and the learning rate was 1e-4. We compare the performance of our model with a wide range of state-of-the-art models in image captioning and radiology report generation. Table 3 presents the comparison results for both Natural Language Generation (NLG) metrics and CE metrics. The models we compare include R2Gen [3], R2GenCMN [3], PPKED [11], R2GenGPT[22], FGIRG[3] and R2GMMN[3]. It can be observed that our model outperforms the current state-of-the-art methods across various language generation metrics, especially on MIMIC-CXR. Our model's performance on the IU-Xray dataset is acceptable but not exceptional. This could be attributed to the limited size of the IU-Xray dataset, which consists of only 2.8K image-text pairs. Consequently, the scarcity of training data may hinder the effective learning of text generation capabilities during the fine-tuning of large language models.The larger sample size of the MIMIC-CXR dataset allows for more comprehensive training of the vision encoder during the pretraining stage, thereby facilitating more consistent cross-modal alignment. We achieved a significant improvement across all Natural Language Generation (NLG) metrics except for CIDER[10]. METransformer[22] employed a specialized optimization strategy involving a voting strategy for the CIDER metric, thereby achieving significant superiority in performance according to the CIDER metric. Our model acquire a significant ability to maintain language consistency and rich semantics when generating medical image reports. This suggests that our approach can accurately capture keywords in radiology reports. Further evaluation of clinical efficacy metrics demonstrates the significant potential of our proposed method. Compared to other methods that fine-tuning large language models for radiology report generation, such as R2GenGPT[22], our model achieves significant improvements in multiple clinical efficacy metrics without relying on any external disease annotations. We observe that our proposed method achieves an average accuracy, recall, and F1 score of 0.403, 0.399, and 0.393, respectively, across various diseases. This represents a significant improvement over the current state-of-the-art method R2GMMN[3], demonstrating the effectiveness of our approach. Furthermore, it indicates that our proposed model has stronger disease perception capabilities, enabling the generation of more truthful radiology reports."
    },
    {
      "title": "Ablation Study",
      "text": "**Effectiveness of each component.** We constructed our baseline model, called the \"BASE\" model, by solely fine-tuning the visual mapper during training using visual features. Meanwhile, we introduced a disease clue injection module, a cross-modal clue interaction module, and a disease-aware consistency loss function, abbreviated as \"DCI,\" \"CMCI\" and \"DAL\" respectively. The symbol \"+\" denotes the experimental effects of adding different components to the base model, and specific results are presented in Tab. 2. It can be observed that the three components we proposed all have a positive impact on performance. Despite some randomness in the experiments conducted in the deep learning laboratory, the results demonstrate credible comparability due to the systematic training partition applied to the MIMIC-CXR [12] dataset. Our proposed three modules have achieved significant improvements of 14.5%, 15.8%, and 13.6%, 11.7%, 9.9% in BLEU-4, ROUGE-L, METEOR, CIDEr, and F1 from the \"Base\" to our TRRG. The incorporation of different components resulted in varying degrees of improvement on the base model, thus validating the effectiveness of our approach. **Impact of clue numbers.** The number of disease clue injections determines the richness of disease information perceived by the model during the fine-tuning stage. We set the value of \\(k\\) to 1, 2, 3, 4, and 5, and verified the corresponding experimental results on MIMIC-CXR [12]. The experiments demonstrate that when our parameter \\(k\\) set to 3, the model achieves superior performance across multiple evaluation metrics. This aligns with our intuitive understanding that, typically, a chest X-ray may exhibit concurrent manifestations of 3-4 diseases at most. As \\(k\\) gradually \\begin{table} \\begin{tabular}{c|c|c c c c c c c|c c c} \\hline \\hline \\multirow{2}{*}{Dataset} & \\multirow{2}{*}{Model} & \\multicolumn{6}{c|}{NLG Metrics} & \\multicolumn{3}{c}{CE Metrics} \\\\ & & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & ROUGE & METEOR & CIDEr & Precision & Recall & F1 \\\\ \\hline \\multirow{8}{*}{IU X-Ray} & HGRO-Agent [11] & 0.438 & 0.298 & 0.208 & 0.151 & 0.322 & - & 0.343 & - & - & - \\\\ & KERP [11] & 0.482 & 0.325 & 0.226 & 0.162 & 0.339 & - & 0.280 & - & - & - \\\\ & R2Gen [3] & 0.470 & 0.304 & 0.219 & 0.165 & 0.371 & 0.187 & - & - & - & - \\\\ & PPKED [11] & 0.483 & 0.315 & 0.224 & 0.168 & 0.376 & 0.187 & 0.351 & - & - & - \\\\ & GSK [22] & **0.496** & **0.327** & **0.238** & **0.178** & **0.381** & - & 0.382 & - & - & - \\\\ & R2GenCMN [3] & 0.475 & 0.309 & 0.222 & 0.170 & 0.375 & 0.191 & - & - & - & - \\\\ & METransformer [22] & 0.483 & 0.322 & 0.228 & 0.172 & 0.380 & 0.192 & **0.435** & - & - & - \\\\ \\cline{2-11} & **TRG**(**0.0urs**) & 0.482 & 0.302 & 0.217 & 0.151 & 0.377 & **0.209** & **0.405** & - & - & - \\\\ \\hline \\multirow{8}{*}{MIMIC-CXR} & M2Transformer [1] & 0.332 & 0.210 & 0.142 & 0.101 & 0.264 & 0.134 & 0.142 & - & - & - \\\\ & R2Gen [3] & 0.353 & 0.218 & 0.145 & 0.103 & 0.277 & 0.142 & - & 0.333 & 0.273 & 0.276 \\\\ & PPKED [11] & 0.36 & 0.224 & 0.149 & 0.106 & 0.284 & 0.149 & 0.237 & - & - & - \\\\ & GSK [22] & 0.363 & 0.228 & 0.156 & 0.115 & 0.284 & & 0.203 & - & - & - \\\\ & R2GenCMN [3] & 0.353 & 0.218 & 0.148 & 0.106 & 0.278 & 0.142 & - & 0.334 & 0.275 & 0.278 \\\\ & MSAT [22] & 0.373 & 0.235 & 0.162 & 0.120 & 0.282 & 0.143 & 0.299 & - & - & - \\\\ & METransformer [22] & 0.386 & 0.250 & 0.169 & 0.124 & 0.291 & 0.152 & **0.362** & 0.364 & 0.309 & 0.311 \\\\ & DCL [11] & - & - & - & - & 0.107 & 0.284 & 0.150 & 0.281 & **0.471** & 0.352 & 0.373 \\\\ & R2GenGPT [22] & 0.365 & 0.237 & 0.163 & 0.117 & 0.277 & 0.136 & 0.145 & 0.341 & 0.312 & 0.325 \\\\ & FGIRG [3] & 0.379 & 0.234 & 0.154 & 0.106 & 0.285 & 0.162 & & & & \\\\ & R2GMMN [3] & 0.396 & 0.244 & 0.162 & 0.115 & 0.274 & 0.151 & - & 0.411 & 0.398 & 0.389 \\\\ \\cline{2-11} & **TRG**(**0urs**) & **0.436** & **0.298** & **0.213** & **0.157** & **0.336** & **0.167** & 0.219 & 0.403 & **0.399** & **0.393** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Comparisons of the proposed TRRG with previous studies on the IU X-Ray and MIMIC-CXR test set with respect to language generation (NLG) and clinical efficacy (CE) metrics. Best results are in **bold**. increases from 1 to 3, the model's performance steadily improves. When the parameter k exceeds 3, there is a decline in model performance. This phenomenon may be attributed to an excessive injection of irrelevant disease cues, which leads the model to inadequately attend to crucial regions within the images and pertinent disease cues. **Impact of the length of learnable queries.** We conducted an ablation study on the length of learnable queries to explore the impact of different levels of compression of disease cues on the final model performance. We set the length \\(L\\) to 4, 8, 16, and 32, respectively, and found that the model achieved the best performance when the length of the learnable queries was set to 16. The appropriate length of learnable queries can ensure effective detection and compression of disease cues by the model."
    },
    {
      "title": "Qualitative Analysis",
      "text": "We conduct a qualitative analysis to validate the effectiveness of the proposed model. As depicted in Fig 5, our disease clues and probabilities are highlighted using colored fonts. Compared to the base model, our proposed model tends to include more disease-related content with injected clues during the process of generating radiology reports. This observation confirms that our model has higher clinical reliability. In the second example, the conventional model hinted at the presence of auxiliary devices in the image, but our model failed to provide relevant descriptions. This indicates some limitations of our proposed approach, which may lead to an overemphasis on disease-related content in certain cases, thereby compromising findings and obscuring the expression of basic descriptions."
    },
    {
      "title": "Conclusion",
      "text": "In this paper, we propose the TRGG for truthful radiology report generation based on fine-tuning large language models with injected disease cues. Our proposed stage-wise training strategy effectively promotes cross-modal alignment between radiography and reports. The clue injection \\begin{table} \\begin{tabular}{c|c c c c} \\hline \\(L\\) & BLEU-4 & ROUGE & METEOR & F1 \\\\ \\hline 4 & **0.161** & 0.322 & 0.161 & 0.358 \\\\ 8 & 0.155 & 0.332 & 0.162 & 0.377 \\\\ **16** & 0.157 & **0.336** & **0.167** & **0.393** \\\\ 32 & 0.142 & 0.329 & 0.156 & 0.386 \\\\ \\hline \\end{tabular} \\end{table} Table 4: Model with different learnable queries numbers \\(L\\). \\begin{table} \\begin{tabular}{|l|l|} \\hline **Ground-Truth:** There is no clear & **Baseline:** Frontal and & **Ours:** The cardiomediasinal \\\\ radiographic change over the past & lateral views of the chest. & silhouette and pulmonary \\\\ 11 days. **Bilateral pleural** & The pulmonary vasculature \\\\ **effusions moderate** on the right & is normal. The lungs are \\\\ small on the left and callus & clear without consolidation, \\\\ pulmonary nodules are & effusion or pneumothorax. \\\\ unchanged. Confluent & The cardiomediasinal \\\\ **opacification** at the base of the & silhouette is normal. \\\\ right lung is probably ratelensis, & \\\\ pleural mild pneumonia is & \\\\ difficult to exclude. & \\\\ \\hline **Ground-Truth:** Since the prior & **Baseline:** Frontal views of \\\\ study, there is no change in **large** & the chest. Left chest wall \\\\ **right pleural effusion** and & pacing device seen with \\\\ associated nickeless. Heart size & leads in the right atrium. The \\\\ and mediastium are unchanged & lungs are clear, and the \\\\ including cardiomegaly. & cardiomediasinal silhouette \\\\ **Biventricular pacer is & and hila are normal. pleural** \\\\ redemonstrated** & **effusions show in in right** \\\\ **ellung zone.** & **lung zone.** \\\\ \\hline \\end{tabular} \\end{table} Table 2: Ablation study of different componet we proposed, ‚ÄùDCI,‚Äù ‚ÄùCMCI,‚Äù and ‚ÄùDAL‚Äù respectively denote the Disease Clue Injection module, Cross-Modal Clue Interaction module, and Disease-Aware Loss function. \\begin{table} \\begin{tabular}{l l|c c c c|c c c c c} \\hline \\multirow{2}{*}{\\#} & \\multirow{2}{*}{Models} & \\multicolumn{6}{c}{IU-Xray} & \\multicolumn{6}{c}{MIMIC-CXR} \\\\ \\cline{2-10} & & BLEU-4 & ROUGE & METEOR & CIDEr & F1 & BLEU\\_4 & ROUGE & METEOR & CIDEr & F1 \\\\ \\hline 1 & BASE & **0.156** & 0.370 & 0.194 & 0.387 & - & 0.137 & 0.290 & 0.147 & 0.196 & 0.354 \\\\ 2 & +DCI & 0.152 & 0.365 & 0.197 & 0.390 & - & 0.142 & 0.311 & 0.156 & 0.207 & 0.384 \\\\ 3 & +DCI+CMCI & 0.147 & 0.372 & 0.205 & 0.402 & - & **0.159** & 0.324 & 0.162 & 0.211 & 0.387 \\\\ 4 & +DCI+CMCI+DAL & 0.151 & **0.377** & **0.209** & **0.405** & - & 0.157 & **0.336** & **0.167** & **0.219** & **0.393** \\\\ \\hline \\end{tabular} \\end{table} Table 3: Model with different disease clue numbers \\(k\\), all experiments were conducted concerning varying quantities of disease clue injections on MIMI-CXR. Figure 5: We compare the generated results of the base model and the TRRG (Ours) with the ground truth, highlighting key information using colored fonts. Our model effectively generated specific descriptions tailored to diseases. [MISSING_PAGE_FAIL:8] Oord, A.; Li, Y.; and Vinyals, O. 2018. Representation Learning with Contrastive Predictive Coding. _Cornell University - arXiv_. * Pan et al. (2020) Pan, Y.; Yao, T.; Li, Y.; and Mei, T. 2020. X-Linear Attention Networks for Image Captioning. In _CVPR_. * Papineni et al. (2002) Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, 311-318. * Rennie et al. (2017) Rennie, S. J.; Marcheret, E.; Mroueh, Y.; Ross, J.; and Goel, V. 2017. Self-Critical Sequence Training for Image Captioning. In _CVPR_. * Shen et al. (2024) Shen, H.; Pei, M.; Liu, J.; and Tian, Z. 2024. Automatic Radiology Reports Generation via Memory Alignment Network. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, 4776-4783. * Smit et al. (2020) Smit, A.; Jain, S.; Rajpurkar, P.; Pareek, A.; Ng, A. Y.; and Lungren, M. P. 2020. CheXbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT. arXiv:2004.09167. * Tang et al. (2021) Tang, M.; Wang, Z.; Liu, Z.; Rao, F.; Li, D.; and Li, X. 2021. Clip4caption: Clip for video caption. In _Proceedings of the 29th ACM International Conference on Multimedia_, 4858-4862. * Tu et al. (2022) Tu, E.; Talius, E.; Patel, P.; Langlotz, C. P.; Ng, A. Y.; and Rajpurkar, P. 2022. Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning. _Nature Biomedical Engineering_, 6(12): 1399-1406. * Touvron et al. (2023) Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Roziere, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_. * Vaswani et al. (2017) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is All you Need. In _NIPS_. * Vedantam et al. (2015) Vedantam, R.; Zitnick, C. L.; and Parikh, D. 2015. CIDEr: Consensus-based image description evaluation. In _CVPR_. * Wang, Bhalerao, and He (2022) Wang, J.; Bhalerao, A.; and He, Y. 2022. Cross-modal prototype driven network for radiology report generation. In _European Conference on Computer Vision_, 563-579. Springer. * Wang et al. (2022a) Wang, P.; Yang, A.; Men, R.; Lin, J.; Bai, S.; Li, Z.; Ma, J.; Zhou, C.; Zhou, J.; and Yang, H. 2022a. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In _International Conference on Machine Learning_, 23318-23340. PMLR. * Wang et al. (2023a) Wang, Y.; Wang, K.; Liu, X.; Gao, T.; Zhang, J.; and Wang, G. 2023a. Self adaptive global-local feature enhancement for radiology report generation. In _2023 IEEE International Conference on Image Processing (ICIP)_, 2275-2279. IEEE. * Wang et al. (2023b) Wang, Z.; Liu, L.; Wang, L.; and Zhou, L. 2023b. METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens. In _CVPR_, 11558-11567. * Wang et al. (2023c) Wang, Z.; Liu, L.; Wang, L.; and Zhou, L. 2023c. R2gengpt: Radiology report generation with frozen llms. _Meta-Radiology_, 1(3): 100033. * Wang et al. (2022b) Wang, Z.; Tang, M.; Wang, L.; Li, X.; and Zhou, L. 2022b. A medical semantic-assisted transformer for radiographic report generation. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, 655-664. Springer. * Wang et al. (2022c) Wang, Z.; Wu, Z.; Agarwal, D.; and Sun, J. 2022c. MedCLIP: Contrastive Learning from Unpaired Medical Images and Text. * Wang et al. (2021) Wang, Z.; Zhou, L.; Wang, L.; and Li, X. 2021. A Self-Boosting Framework for Automated Radiographic Report Generation. In _CVPR_. * Xue et al. (2018) Xue, Y.; Xu, T.; Long, L. R.; Xue, Z.; Antani, S.; Thoma, G. R.; and Huang, X. 2018. Multimodal Recurrent Model with Attention for Automated Radiology Report Generation. In _MICCAI_. * Yang et al. (2021) Yang, S.; Wu, X.; Ge, S.; Zhou, S. K.; and Xiao, L. 2021. Knowledge Matters: Radiology Report Generation with General and Specific Knowledge. _Medical Image Analysis_. * Yang et al. (2022) Yang, S.; Wu, X.; Ge, S.; Zhou, S. K.; and Xiao, L. 2022. Knowledge matters: Chest radiology report generation with general and specific knowledge. _Medical image analysis_, 80: 102510. * Yin et al. (2020) Yin, C.; Qian, B.; Wei, J.; Li, X.; Zhang, X.; Li, Y.; and Zheng, Q. 2020. Automatic Generation of Medical Imaging Diagnostic Report with Hierarchical Recurrent Neural Network. In _ICDM_. * Yu et al. (2019) Yu, J.; Li, J.; Yu, Z.; and Huang, Q. 2019. Multimodal transformer with multi-view visual representation for image captioning. _IEEE transactions on circuits and systems for video technology_, 30(12): 4467-4480. * Yuan et al. (2019) Yuan, J.; Liao, H.; Luo, R.; and Luo, J. 2019. Automatic Radiology Report Generation Based on Multi-view Image Fusion and Medical Concept Enrichment. In _MICCAI_. * Zhang et al. (2020) Zhang, Y.; Wang, X.; Xu, Z.; Yu, Q.; Yuille, A.; and Xu, D. 2020. When radiology report generation meets knowledge graph. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, 12910-12917."
    },
    {
      "title": "Reproducibility Checklist",
      "text": "[YES] [NO] [NA] 1. This paper: 1. Includes a conceptual outline and/or pseudocode description of AI methods introduced. [YES] 2. Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results. [YES] 3. Provides well marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper. [YES] 2. Does this paper make theoretical contributions? (yes/no) 1. All assumptions and restrictions are stated clearly and formally. [YES] 2. All novel claims are stated formally (_e.g._, in theorem statements). [YES] 3. Proofs of all novel claims are included. [YES] 4. Proof sketches or intuitions are given for complex and/or novel results. [YES] 5. Appropriate citations to theoretical tools used are given.[YES] 6. All theoretical claims are demonstrated empirically to hold. [YES] 7. All experimental code used to eliminate or disprove claims is included.[YES] 3. Does this paper rely on one or more datasets? [YES] 1. A motivation is given for why the experiments are conducted on the selected datasets. [YES] 2. All novel datasets introduced in this paper are included in a data appendix. [NA] 3. All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. [NA] 4. All datasets drawn from the existing literature (potentially including authors' own previously published work) are accompanied by appropriate citations.[YES] 5. All datasets drawn from the existing literature (potentially including authors' own previously published work) are publicly available. [YES] 6. All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. [NA] 4. Does this paper include computational experiments? (yes/no) 1. Any code required for pre-processing data is included in the appendix. [NA] 2. All source code required for conducting and analyzing the experiments is included in a code appendix. [YES] 3. All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. [YES] 4. All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from. [YES] 5. If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. [YES] 6. This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. [YES] 7. This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. [YES] 8. This paper states the number of algorithm runs used to compute each reported result. [NO] 9. Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. [NO] 10. The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). [NO] 11. This paper lists all final (hyper-)parameters used for each model/algorithm in the paper's experiments. [YES] 12. This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting. [NO]"
    }
  ]
}