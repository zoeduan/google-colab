{
  "title": "Embracing Large Language Models in Traffic Flow Forecasting",
  "authors": [
    "Yusheng Zhao",
    "Xiao Luo",
    "Zhiping Xiao",
    "Wei Ju",
    "Ming Zhang"
  ],
  "abstract": "\n Traffic flow forecasting aims to predict future traffic flows based on the historical traffic conditions and the road network. It is an important problem in intelligent transportation systems, with a plethora of methods been proposed. Existing efforts mainly focus on capturing and utilizing spatio-temporal dependencies to predict future traffic flows. Though promising, they fall short in adapting to test-time environmental changes of traffic conditions. To tackle this challenge, we propose to introduce large language models (LLMs) to help traffic flow forecasting and design a novel method named Large Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two branches, capturing different spatio-temporal relations using graph and hypergraph structures respectively. The two branches are first pre-trained individually, and during test-time, they yield different predictions. Based on these predictions, a large language model is used to select the most likely result. Then, a ranking loss is applied as the learning objective to enhance the prediction ability of the two branches. Extensive experiments on several datasets demonstrate the effectiveness of the proposed LEAF. \n",
  "references": [
    {
      "id": null,
      "title": "Embracing Large Language Models in Traffic Flow Forecasting",
      "authors": [
        "Yusheng Zhao",
        "Xiao Luo",
        "Zhiping Xiao",
        "Wei Ju",
        "Ming Zhang"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Llama 3 model card",
      "authors": [
        "Ai@meta"
      ],
      "year": "2024",
      "venue": "Llama 3 model card",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "A new hybrid for improvement of autoregressive integrated moving average models applying particle swarm optimization",
      "authors": [
        "Shahrokh Asadi",
        "Akbar Tavakoli",
        "Reza Seyed",
        "Hejazi"
      ],
      "year": "2012",
      "venue": "Expert Systems with Applications",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Large language models for recommendation: Past, present, and future",
      "authors": [
        "Keqin Bao",
        "Jizhi Zhang",
        "Xinyu Lin",
        "Yang Zhang",
        "Wenjie Wang",
        "Fuli Feng"
      ],
      "year": "2024",
      "venue": "Proceedings of the 47th International ACM SI-GIR Conference on Research and Development in Information Retrieval",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Remember this event that year? assessing temporal information and understanding in large language models",
      "authors": [
        "Himanshu Beniwal",
        "Dishant Patel",
        "D Kowsik"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Artificial intelligence-based vehicular traffic flow prediction methods for supporting intelligent transportation systems",
      "authors": [
        "Azzedine Boukerche",
        "Yanjie Tao",
        "Peng Sun"
      ],
      "year": "2020",
      "venue": "Computer networks",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Traffic transformer: Capturing the continuity and periodicity of time series for traffic forecasting",
      "authors": [
        "Ling Cai",
        "Krzysztof Janowicz",
        "Gengchen Mai",
        "Bo Yan",
        "Rui Zhu"
      ],
      "year": "2020",
      "venue": "Transactions in GIS",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "A survey on evaluation of large language models",
      "authors": [
        "Yupeng Chang",
        "Xu Wang",
        "Jindong Wang",
        "Yuan Wu",
        "Linyi Yang",
        "Kaijie Zhu",
        "Hao Chen",
        "Xiaoyuan Yi",
        "Cunxiang Wang",
        "Yidong Wang"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "2022a. Bidirectional spatial-temporal adaptive transformer for urban traffic flow forecasting",
      "authors": [
        "Changlu Chen",
        "Yanbin Liu",
        "Ling Chen",
        "Chengqi Zhang"
      ],
      "year": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Hongxing Zhang, and Xiping Hu. 2024a. Traffic flow matrixbased graph neural network with attention mechanism for traffic flow prediction",
      "authors": [
        "Jian Chen",
        "Li Zheng",
        "Yuzhu Hu",
        "Wei Wang"
      ],
      "year": "",
      "venue": "Information Fusion",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "2024b. When large language models meet personalization: Perspectives of challenges and opportunities",
      "authors": [
        "Jin Chen",
        "Zheng Liu",
        "Xu Huang",
        "Chenwang Wu",
        "Qi Liu",
        "Gangwei Jiang",
        "Yuanhao Pu",
        "Yuxuan Lei",
        "Xiaolong Chen",
        "Xingmei Wang"
      ],
      "year": "",
      "venue": "World Wide Web",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Aargnn: An attentive attributed recurrent graph neural network for traffic flow prediction considering multiple dynamic factors",
      "authors": [
        "Ling Chen",
        "Wei Shao",
        "Mingqi Lv",
        "Weiqi Chen",
        "Youdong Zhang",
        "Chenghu Yang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Intelligent Transportation Systems",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "2024c. Calibration of timeseries forecasting: Detecting and adapting contextdriven distribution shift",
      "authors": [
        "Mouxiang Chen",
        "Lefei Shen",
        "Han Fu",
        "Zhuo Li",
        "Jianling Sun",
        "Chenghao Liu"
      ],
      "year": "",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "Junyoung Chung",
        "Caglar Gulcehre",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "St-innet: Deep spatio-temporal inception networks for traffic flow prediction in smart cities",
      "authors": [
        "Fei Dai",
        "Penggui Huang",
        "Qi Mo",
        "Xiaolong Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Intelligent Transportation Systems",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Intelligent transportation systems",
      "authors": [
        "George Dimitrakopoulos",
        "Panagiotis Demestichas"
      ],
      "year": "2010",
      "venue": "IEEE Vehicular Technology Magazine",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Scalable deep traffic flow neural networks for urban traffic congestion prediction",
      "authors": [
        "Mohammadhani Fouladgar",
        "Mostafa Parchami",
        "Ramez Elmasri",
        "Amir Ghaderi"
      ],
      "year": "2017",
      "venue": "2017 international joint conference on neural networks (IJCNN)",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "",
      "authors": [
        "Jean-Christophe Gagnon-Audet",
        "Kartik Ahuja",
        "Mohammad-Javad Darvishi-Bayazi"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Woods: Benchmarks for out-of-distribution generalization in time series",
      "authors": [
        "Guillaume Mousavi",
        "Irina Dumas",
        "Rish"
      ],
      "year": "2022",
      "venue": "Woods: Benchmarks for out-of-distribution generalization in time series",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Multivariate short-term traffic flow forecasting using time-series analysis",
      "authors": [
        "Bidisha Ghosh",
        "Biswajit Basu",
        "Margaret O' Mahony"
      ],
      "year": "2009",
      "venue": "IEEE transactions on intelligent transportation systems",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Large language models are zero-shot time series forecasters",
      "authors": [
        "Nate Gruver",
        "Marc Finzi",
        "Shikai Qiu",
        "Andrew G Wilson"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Attention based spatialtemporal graph convolutional networks for traffic flow forecasting",
      "authors": [
        "Shengnan Guo",
        "Youfang Lin",
        "Ning Feng",
        "Chao Song",
        "Huaiyu Wan"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "2024a. Towards explainable traffic flow prediction with large language models",
      "authors": [
        "Xusen Guo",
        "Qiming Zhang",
        "Junyue Jiang",
        "Mingxing Peng",
        "Meixin Zhu",
        "Hao Frank",
        "Yang"
      ],
      "year": "",
      "venue": "Communications in Transportation Research",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Explainable traffic flow prediction with large language models",
      "authors": [
        "Xusen Guo",
        "Qiming Zhang",
        "Mingxing Peng",
        "Meixin Zhua"
      ],
      "year": "2024",
      "venue": "Explainable traffic flow prediction with large language models",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Inductive representation learning on large graphs. Advances in neural information processing systems",
      "authors": [
        "Will Hamilton",
        "Zhitao Ying",
        "Jure Leskovec"
      ],
      "year": "2017",
      "venue": "Inductive representation learning on large graphs. Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Event traffic forecasting with sparse multimodal data",
      "authors": [
        "Xiao Han",
        "Zhenduo Zhang",
        "Yiling Wu",
        "Xinfeng Zhang",
        "Zhe Wu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Long short-term memory",
      "authors": [
        "Hochreiter"
      ],
      "year": "1997",
      "venue": "Neural Computation",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Hybrid evolutionary algorithms in a svr traffic flow forecasting model",
      "authors": [
        "Wei-Chiang Hong",
        "Yucheng Dong",
        "Feifeng Zheng",
        "Shih Yung",
        "Wei"
      ],
      "year": "2011",
      "venue": "Applied Mathematics and Computation",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "From moments to milestones: Incremental timeline summarization leveraging large language models",
      "authors": [
        "Qisheng Hu",
        "Geonsik Moon",
        "Hwee Tou Ng"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Trilevel navigator: Llm-empowered tri-level learning for time series ood generalization",
      "authors": [
        "Chengtao Jian",
        "Kai Yang",
        "Yang Jiao"
      ],
      "year": "2024",
      "venue": "Trilevel navigator: Llm-empowered tri-level learning for time series ood generalization",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "An urban traffic signal control system based on traffic flow prediction",
      "authors": [
        "Chun-Yao Jiang",
        "Xiao-Min Hu",
        "Wei-Neng Chen"
      ],
      "year": "2021",
      "venue": "2021 13th international conference on advanced computational intelligence (ICACI)",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Pdformer: Propagation delayaware dynamic long-range transformer for traffic flow prediction",
      "authors": [
        "Jiawei Jiang",
        "Chengkai Han",
        "Wayne Xin Zhao",
        "Jingyuan Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI conference on artificial intelligence",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Cool: a conjoint perspective on spatiotemporal graph neural network for traffic forecasting",
      "authors": [
        "Wei Ju",
        "Yusheng Zhao",
        "Yifang Qin",
        "Siyu Yi",
        "Jingyang Yuan",
        "Zhiping Xiao",
        "Xiao Luo",
        "Xiting Yan",
        "Ming Zhang"
      ],
      "year": "2024",
      "venue": "Information Fusion",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "When model meets new normals: Test-time adaptation for unsupervised time-series anomaly detection",
      "authors": [
        "Dongmin Kim",
        "Sunghyun Park",
        "Jaegul Choo"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Semisupervised classification with graph convolutional networks",
      "authors": [
        "N Thomas",
        "Max Kipf",
        "Welling"
      ],
      "year": "2016",
      "venue": "Semisupervised classification with graph convolutional networks",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Financial forecasting from textual and tabular time series",
      "authors": [
        "Ross Koval",
        "Nicholas Andrews",
        "Xifeng Yan"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Efficient memory management for large language model serving with pagedattention",
      "authors": [
        "Woosuk Kwon",
        "Zhuohan Li",
        "Siyuan Zhuang",
        "Ying Sheng",
        "Lianmin Zheng",
        "Cody Hao Yu",
        "Joseph E Gonzalez",
        "Hao Zhang",
        "Ion Stoica"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Biomistral: A collection of opensource pretrained large language models for medical domains",
      "authors": [
        "Yanis Labrak",
        "Adrien Bazoge",
        "Emmanuel Morin",
        "Pierre-Antoine Gourraud",
        "Mickael Rouvier",
        "Richard Dufour"
      ],
      "year": "2024",
      "venue": "Biomistral: A collection of opensource pretrained large language models for medical domains",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Large language models in law: A survey",
      "authors": [
        "Jinqi Lai",
        "Wensheng Gan",
        "Jiayang Wu",
        "Zhenlian Qi",
        "S Yu",
        "Philip"
      ],
      "year": "2024",
      "venue": "Large language models in law: A survey",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Dstagnn: Dynamic spatial-temporal aware graph neural network for traffic flow forecasting",
      "authors": [
        "Shiyong Lan",
        "Yitong Ma",
        "Weikang Huang",
        "Wenwu Wang",
        "Hongyu Yang",
        "Pyang Li"
      ],
      "year": "2022",
      "venue": "International conference on machine learning",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Spatialtemporal fusion graph neural networks for traffic flow forecasting",
      "authors": [
        "Mengzhang Li",
        "Zhanxing Zhu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Econagent: large language modelempowered agents for simulating macroeconomic activities",
      "authors": [
        "Nian Li",
        "Chen Gao",
        "Mingyu Li",
        "Yong Li",
        "Qingmin Liao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "A k-nearest neighbor locally weighted regression method for short-term traffic flow forecasting",
      "authors": [
        "Shuangshuang Li",
        "Zhen Shen",
        "Gang Xiong"
      ],
      "year": "2012",
      "venue": "2012 15th International IEEE Conference on Intelligent Transportation Systems",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Diffusion convolutional recurrent neural network: Data-driven traffic forecasting",
      "authors": [
        "Yaguang Li",
        "Rose Yu",
        "Cyrus Shahabi",
        "Yan Liu"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "2024b. Urbangpt: Spatio-temporal large language models",
      "authors": [
        "Zhonghang Li",
        "Lianghao Xia",
        "Jiabin Tang",
        "Yong Xu",
        "Lei Shi",
        "Long Xia",
        "Dawei Yin",
        "Chao Huang"
      ],
      "year": "",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Exploring large language models for human mobility prediction under public events. Computers, Environment and Urban Systems",
      "authors": [
        "Yuebing Liang",
        "Yichao Liu",
        "Xiaohan Wang",
        "Zhan Zhao"
      ],
      "year": "2024",
      "venue": "Exploring large language models for human mobility prediction under public events. Computers, Environment and Urban Systems",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Dynamic route planning with real-time traffic predictions",
      "authors": [
        "Thomas Liebig",
        "Nico Piatkowski",
        "Christian Bockermann",
        "Katharina Morik"
      ],
      "year": "2017",
      "venue": "Information Systems",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Can large language models reason about medical questions?",
      "authors": [
        "Valentin Liévin",
        "Egeberg Christoffer",
        "Andreas Geert Hother",
        "Ole Motzfeldt",
        "Winther"
      ],
      "year": "2024",
      "venue": "Patterns",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Spatialtemporal large language model for traffic prediction",
      "authors": [
        "Chenxi Liu",
        "Sun Yang",
        "Qianxiong Xu",
        "Zhishuai Li",
        "Cheng Long",
        "Ziyue Li",
        "Rui Zhao"
      ],
      "year": "2024",
      "venue": "Spatialtemporal large language model for traffic prediction",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Spatio-temporal adaptive embedding makes vanilla transformer sota for traffic forecasting",
      "authors": [
        "Hangchen Liu",
        "Zheng Dong",
        "Renhe Jiang",
        "Jiewen Deng",
        "Jinliang Deng",
        "Quanjun Chen",
        "Xuan Song"
      ],
      "year": "2023",
      "venue": "Proceedings of the 32nd ACM international conference on information and knowledge management",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Out-of-distribution representation learning for time series classification",
      "authors": [
        "Wang Lu",
        "Jindong Wang",
        "Xinwei Sun",
        "Yiqiang Chen",
        "Xing Xie"
      ],
      "year": "2022",
      "venue": "Out-of-distribution representation learning for time series classification",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Directed hypergraph attention network for traffic forecasting",
      "authors": [
        "Xiaoyi Luo",
        "Jiaheng Peng",
        "Jun Liang"
      ],
      "year": "2022",
      "venue": "IET Intelligent Transport Systems",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Spatio-temporal fusion graph convolutional network for traffic flow forecasting",
      "authors": [
        "Ying Ma",
        "Haijie Lou",
        "Ming Yan",
        "Fanghui Sun",
        "Guoqi Li"
      ],
      "year": "2024",
      "venue": "Information Fusion",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Large language models challenge the future of higher education",
      "authors": [
        "Silvia Milano",
        "Joshua A Mcgrane",
        "Sabina Leonelli"
      ],
      "year": "2023",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Large language models: A survey",
      "authors": [
        "Shervin Minaee",
        "Tomas Mikolov",
        "Narjes Nikzad",
        "Meysam Chenaghlu",
        "Richard Socher",
        "Xavier Amatriain",
        "Jianfeng Gao"
      ],
      "year": "2024",
      "venue": "Large language models: A survey",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Towards greener smart cities and road traffic forecasting using air pollution data",
      "authors": [
        "Yilong Ren",
        "Yue Chen",
        "Shuai Liu",
        "Boyue Wang",
        "Haiyang Yu",
        "Zhiyong Cui"
      ],
      "year": "2021",
      "venue": "Sustainable Cities and Society",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Traffic flow forecasting: comparison of modeling approaches",
      "authors": [
        "L Brian",
        "Michael J Smith",
        "Demetsky"
      ],
      "year": "1997",
      "venue": "Journal of transportation engineering",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Spatial-temporal synchronous graph convolutional networks: A new framework for spatialtemporal network data forecasting",
      "authors": [
        "Chao Song",
        "Youfang Lin",
        "Shengnan Guo",
        "Huaiyu Wan"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "A bayesian network approach to traffic flow forecasting",
      "authors": [
        "Shiliang Sun",
        "Changshui Zhang",
        "Guoqiang Yu"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on intelligent transportation systems",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Real-time traffic flow forecasting using spectral analysis",
      "authors": [
        "Biswajit Tigran T Tchrakian",
        "Margaret O' Basu",
        "Mahony"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Intelligent Transportation Systems",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Adapted large language models can outperform medical experts in clinical text summarization",
      "authors": [
        "Dave Van Veen",
        "Cara Van Uden",
        "Louis Blankemeier",
        "Jean-Benoit Delbrouck",
        "Asad Aali",
        "Christian Bluethgen",
        "Anuj Pareek",
        "Malgorzata Polacin"
      ],
      "year": "",
      "venue": "Nature medicine",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "Attention is all you need",
      "authors": [
        "Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Graph attention networks",
      "authors": [
        "Petar Veličković",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Liò",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Large-scale traffic prediction with hierarchical hypergraph message passing networks",
      "authors": [
        "Jingcheng Wang",
        "Yong Zhang",
        "Yongli Hu",
        "Baocai Yin"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "Multitask hypergraph convolutional networks: A heterogeneous traffic prediction framework",
      "authors": [
        "Jingcheng Wang",
        "Yong Zhang",
        "Lixun Wang",
        "Yongli Hu",
        "Xinglin Piao",
        "Baocai Yin"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Intelligent Transportation Systems",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Traffic flow prediction via spatial temporal graph neural network",
      "authors": [
        "Xiaoyang Wang",
        "Yao Ma",
        "Yiqi Wang",
        "Wei Jin",
        "Xin Wang",
        "Jiliang Tang",
        "Caiyan Jia",
        "Jian Yu"
      ],
      "year": "2020",
      "venue": "Proceedings of the web conference 2020",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "Shgcn: a hypergraph-based deep learning model for spatiotemporal traffic flow prediction",
      "authors": [
        "Yi Wang",
        "Di Zhu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 5th ACM SIGSPA-TIAL International Workshop on AI for Geographic Knowledge Discovery",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "Chain-of-history reasoning for temporal knowledge graph forecasting",
      "authors": [
        "Yuwei Xia",
        "Ding Wang",
        "Qiang Liu",
        "Liang Wang",
        "Shu Wu",
        "Xiao-Yu Zhang"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics ACL 2024",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "How powerful are graph neural networks?",
      "authors": [
        "Keyulu Xu",
        "Weihua Hu",
        "Jure Leskovec",
        "Stefanie Jegelka"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "Spatial-temporal transformer networks for traffic flow forecasting",
      "authors": [
        "Mingxing Xu",
        "Wenrui Dai",
        "Chunmiao Liu",
        "Xing Gao",
        "Weiyao Lin",
        "Guo-Jun Qi",
        "Hongkai Xiong"
      ],
      "year": "2020",
      "venue": "Spatial-temporal transformer networks for traffic flow forecasting",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "Optimized structure of the traffic flow forecasting model with a deep learning approach",
      "authors": [
        "Hao-Fan",
        "Tharam S Yang",
        "Yi-Ping Phoebe Dillon",
        "Chen"
      ],
      "year": "2016",
      "venue": "IEEE transactions on neural networks and learning systems",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "A survey on multimodal large language models",
      "authors": [
        "Shukang Yin",
        "Chaoyou Fu",
        "Sirui Zhao",
        "Ke Li",
        "Xing Sun",
        "Tong Xu",
        "Enhong Chen"
      ],
      "year": "2024",
      "venue": "National Science Review",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "Spatiotemporal graph convolutional networks: a deep learning framework for traffic forecasting",
      "authors": [
        "Bing Yu",
        "Haoteng Yin",
        "Zhanxing Zhu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Joint Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "Harnessing llms for temporal data-a study on explainable financial time series forecasting",
      "authors": [
        "Xinli Yu",
        "Zheng Chen",
        "Yanbin Lu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
      "doi": ""
    },
    {
      "id": "b73",
      "title": "Unist: a prompt-empowered universal model for urban spatio-temporal prediction",
      "authors": [
        "Yuan Yuan",
        "Jingtao Ding",
        "Jie Feng",
        "Depeng Jin",
        "Yong Li"
      ],
      "year": "2024",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "Datadriven intelligent transportation systems: A survey",
      "authors": [
        "Junping Zhang",
        "Fei-Yue Wang",
        "Kunfeng Wang",
        "Wei-Hua Lin",
        "Xin Xu",
        "Cheng Chen"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Intelligent Transportation Systems",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "Traffic flow forecasting with spatial-temporal graph diffusion network",
      "authors": [
        "Xiyue Zhang",
        "Chao Huang",
        "Yong Xu",
        "Lianghao Xia",
        "Peng Dai",
        "Liefeng Bo",
        "Junbo Zhang",
        "Yu Zheng"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence",
      "doi": ""
    },
    {
      "id": "b76",
      "title": "Dynamic hypergraph structure learning for traffic flow forecasting",
      "authors": [
        "Yusheng Zhao",
        "Xiao Luo",
        "Wei Ju",
        "Chong Chen",
        "Xian-Sheng Hua",
        "Ming Zhang"
      ],
      "year": "2023",
      "venue": "2023 IEEE 39th International Conference on Data Engineering (ICDE)",
      "doi": ""
    },
    {
      "id": "b77",
      "title": "2023a. Spatio-temporal joint graph convolutional networks for traffic forecasting",
      "authors": [
        "Chuanpan Zheng",
        "Xiaoliang Fan",
        "Shirui Pan",
        "Haibing Jin",
        "Zhaopeng Peng",
        "Zonghan Wu",
        "Cheng Wang",
        "S Yu",
        "Philip"
      ],
      "year": "",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "doi": ""
    },
    {
      "id": "b78",
      "title": "Zijin Wang, and Shengxuan Ding. 2023b. Chatgpt is on the horizon: could a large language model be suitable for intelligent traffic safety research and applications? arXiv preprint",
      "authors": [
        "Ou Zheng",
        "Mohamed Abdel-Aty",
        "Dongdong Wang"
      ],
      "year": "",
      "venue": "Zijin Wang, and Shengxuan Ding. 2023b. Chatgpt is on the horizon: could a large language model be suitable for intelligent traffic safety research and applications? arXiv preprint",
      "doi": ""
    },
    {
      "id": "b79",
      "title": "Multispans: A multi-range spatial-temporal transformer network for traffic forecast via structural entropy optimization",
      "authors": [
        "Dongcheng Zou",
        "Senzhang Wang",
        "Xuefeng Li",
        "Hao Peng",
        "Yuandong Wang",
        "Chunyang Liu",
        "Kehua Sheng",
        "Bo Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 17th ACM International Conference on Web Search and Data Mining",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Embracing Large Language Models In Traffic Flow Forecasting",
      "text": "Yusheng Zhao\\({}^{\\heartsuit}\\), Xiao Luo\\({}^{\\clubsuit}\\), Haomin Wen\\({}^{\\diamondsuit}\\), Zhiping Xiao\\({}^{\\clubsuit}\\), Wei Ju\\({}^{\\heartsuit}\\), Ming Zhang\\({}^{\\heartsuit}\\) \\({}^{\\heartsuit}\\) Peking University \\({}^{\\clubsuit}\\) University of California, Los Angeles \\({}^{\\diamondsuit}\\) Carnegie Mellon University \\({}^{\\clubsuit}\\) University of Washington yusheng.zhao@stu.pku.edu.cn, xiaoluo@cs.ucla.edu, wenhaomin.whm@gmail.com, patxiao@uw.edu, {juwei, mzhang_cs}@pku.edu.cn"
    },
    {
      "title": "Abstract",
      "text": "Traffic flow forecasting aims to predict future traffic flows based on the historical traffic conditions and the road network. It is an important problem in intelligent transportation systems, with a plethora of methods been proposed. Existing efforts mainly focus on capturing and utilizing spatio-temporal dependencies to predict future traffic flows. Though promising, they fall short in adapting to test-time environmental changes of traffic conditions. To tackle this challenge, we propose to introduce large language models (LLMs) to help traffic flow forecasting and design a novel method named Large Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two branches, capturing different spatio-temporal relations using graph and hypergraph structures respectively. The two branches are first pre-trained individually, and during test-time, they yield different predictions. Based on these predictions, a large language model is used to select the most likely result. Then, a ranking loss is applied as the learning objective to enhance the prediction ability of the two branches. Extensive experiments on several datasets demonstrate the effectiveness of the proposed LEAF."
    },
    {
      "title": "1 Introduction",
      "text": "Traffic flow forecasting is an integral part of intelligent transportation systems Dimitrakopoulos and Demestichas (2010); Zhang et al. (2011) and smart cities Shahid et al. (2021); Dai et al. (2022). The goal of traffic flow forecasting is to predict future traffic flows using the historical data and the spatial information (_i.e._ the road network), which has a wide range of applications including traffic signal control Jiang et al. (2021), route planning Liebig et al. (2017), and congestion management Fouladgar et al. (2017). Due to its value in real-world applications, great efforts have been made to resolve the problem of traffic flow forecasting Smith and Demetsky (1997); Sun et al. (2006); Guo et al. (2019); Li and Zhu (2021). Early works mainly model the traffic systems using physical rules or shallow models Ghosh et al. (2009); Tchrakian et al. (2011); Hong et al. (2011); Li et al. (2012). With the advent of deep learning, the main-stream of traffic flow forecasting methods utilizes graph neural networks Kipf and Welling (2016); Hamilton et al. (2017); Xu et al. (2018); Velickovic et al. (2018), recurrent neural networks Hochreiter (1997); Chung et al. (2014), and transformers Vaswani (2017); Jiang et al. (2023) to capture the rich spatio-temporal relations Yu et al. (2018); Li et al. (2018); Guo et al. (2019); Li and Zhu (2021); Zhang et al. (2021); Chen et al. (2022a); Liu et al. (2023); Ma et al. (2024). Despite their success, existing traffic flow forecasting methods have two limitations, which hinder their applications in the real world. (1) _Unable to adapt to environmental changes of traffic conditions during test time._ Most existing methods make the assumption that test data follow the same (or a very similar) distribution as the training data, which may fail to hold true in real-world scenarios Kim et al. (2024); Chen et al. (2024c), especially for time series data Lu et al. (2022); Gagnon-Audet et al. (2022); Jian et al. (2024). In the real world, Figure 1: To use LLMs for traffic flow forecasting, a naive solution is to utilize their _generative_ ability (a), which is hard to incorporate spatial relations. By comparison, LEAF utilizes the discriminative ability of LLMs (b), making it easier for LLMs to predict. traffic condition changes over time due to a variety of factors like special events, the change of weather, or the shift of eras. While it is difficult for existing methods to adequately adapt to all these changes, the assistance of large language models (LLMs) can make a difference, as LLMs have the ability to understand these changes Chang et al. (2024); Minaee et al. (2024); Beniwal et al. (2024). A naive solution is to utilize the _generative_ ability of LLMs to make direct predictions Li et al. (2024); Ren et al. (2024); Liang et al. (2024), as shown in Figure 1. However, directly generating future traffic flows could be too challenging for language models, as accurate forecasting relies on both historical data and complex spatio-temporal relations. (2) _Weak in capturing the rich structure of spatio-temporal relations in traffic data._ The traffic network is complicated and the temporal dimension adds another layer of complexity. A large number of prior works focus on capturing the complex spatio-temporal relations, using graph structures Song et al. (2020); Zheng et al. (2023) or hypergraph structures Wang et al. (2022); Wang and Zhu (2022); Zhao et al. (2023). Graphs capture pair-wise relations, while hypergraphs model non-pair-wise relations. Adopting only one of them is not enough, as the spatio-temporal relations in traffic data is rich by nature. For example, traffic congestion at one vertex affects adjacent vertices (pair-wise relations), whereas road closures affect a large set of vertices (non-pair-wise relations). Modeling the rich structure of spatio-temporal relations is a challenging aspect of predicting future traffic flows. Towards this end, we propose a novel method termed Large Language Model Enhanced Traffic Flow Predictor (LEAF) for adaptive and structure-perspective traffic flow forecasting. LEAF consists of a predictor and a selector, where the predictor generates options of predictions and the selector chooses the most likely result. To enhance adaptability, we build an LLM-based selector that selects from a range of possible future traffic flows using the discriminative ability of a large language model, as shown in Figure 1. The selection results are used to guide the predictor with a ranking loss. The large language model is good at understanding the changing traffic conditions and is open to further information provided by humans, making it an adaptable predictor. To better capture the rich structures of spatio-temporal relations, we build a dual-branch predictor composed of a graph branch which captures pair-wise relations of spatio-temporal traffic data, and a hypergraph branch, which captures non-pair-wise relations. During test time, the dual-branch traffic flow predictor generates different forecasting results, and subsequently, a set of transformations is applied to obtain a wealth of choices of future traffic flows. Our contribution is summarized as follows: * We propose an LLM-enhanced traffic flow forecasting framework that introduces large language models in test time to enhance the adaptability of traffic flow forecasting models. * We propose a dual-branch predictor that captures both pair-wise and non-pair-wise relations of spatio-temporal traffic data, and an LLM-based selector that chooses from possible prediction results generated by the predictor. The selection results further guide the adaptation of the predictor with a ranking loss. * Extensive experiments on several benchmark datasets verify the effectiveness of the proposed method."
    },
    {
      "title": "2 Related Works",
      "text": ""
    },
    {
      "title": "Traffic Flow Forecasting",
      "text": "Traffic flow forecasting is a topic that has been studied for several decades Smith and Demetsky (1997); Sun et al. (2006); Yang et al. (2016); Song et al. (2020); Li et al. (2024). Early efforts mainly focus traditional models Smith and Demetsky (1997); Asadi et al. (2012). With the success of deep learning, deep neural networks have become the mainstream in this field. One line of research adopt recurrent neural networks (RNNs) Hochreiter (1997) and graph neural networks (GNNs) Kipf and Welling (2016), where the GNNs and RNNs capture the spatial and temporal relations respectively Li et al. (2018); Wang et al. (2020); Chen et al. (2022). To jointly model spatial and temporal relations, another line of research utilize GNNs in both dimensions Song et al. (2020); Li and Zhu (2021); Lan et al. (2022); Chen et al. (2024). As simple graphs only capture pair-wise relations, some works make a step further, introducing hypergraph neural networks (HGNNs) to capture non-pair-wise spatio-temporal relations Luo et al. (2022); Wang and Zhu (2022); Zhao et al. (2023); Wang et al. (2024). In this work, we take the benefits from both sides, with a dual-branch predictor, capturing rich structures of spatio-temporal relations."
    },
    {
      "title": "Large Language Models",
      "text": "In recent years, large language models has drawn increased attention, both within the community of natural language processing (Chen et al., 2024; Yin et al., 2024) and beyond. Efforts have been made to utilize the power of LLMs in other fields, including healthcare (Lievin et al., 2024; Van Veen et al., 2024; Labrak et al., 2024), education (Milano et al., 2023), legal technology (Lai et al., 2024), economics (Li et al., 2024), recommendation (Chen et al., 2024; Bao et al., 2024), and transportation (Liu et al., 2024; Guo et al., 2024; Ren et al., 2024). Among these applications, traffic flow forecasting is an important one, being the foundation of smart cities (Boukerche et al., 2020). The success of LLMs in language has inspired a plethora of works in this task. Some works adopt the architecture of LLMs for building transformer-based traffic flow predictors (Cai et al., 2020; Xu et al., 2020; Chen et al., 2022; Liu et al., 2023; Jiang et al., 2023; Zou et al., 2024). However, they typically require a large amount of data for training. Another line of research attempts to equip LLMs with the ability to predict future traffic flows based on the history and specific situations (Zheng et al., 2023; Guo et al., 2024; Li et al., 2024; Yuan et al., 2024; Han et al., 2024). Although LLMs have shown promising results in understanding time series (Yu et al., 2023; Koval et al., 2024; Gruver et al., 2024) or temporal events (Xia et al., 2024; Hu et al., 2024), the traffic data involves complex spatio-temporal relations challenging LLMs' _generative_ ability. In this work, we show that one can instead utilize their _discriminative_ ability to enhance existing traffic flow forecasting models."
    },
    {
      "title": "3 Methodology",
      "text": ""
    },
    {
      "title": "Problem Setup And Overview",
      "text": "**Problem Setup**. We follow the standard setup for traffic flow forecasting (Song et al., 2020), where there is a road network denoted as \\(\\mathcal{G}=\\langle\\mathcal{V},\\mathcal{E},\\mathbf{A}\\rangle\\). \\(\\mathcal{V}\\) is the set of \\(N\\) vertices (_i.e._ the sensors in the city), \\(\\mathcal{E}\\) is the set of edges, and \\(\\mathbf{A}\\in\\mathbb{R}^{N\\times N}\\) is the adjacency matrix. In this road network, historical traffic flows can be represented as \\(\\mathbf{X}=(\\mathbf{X}_{1},\\mathbf{X}_{2},\\cdots,\\mathbf{X}_{T})\\in\\mathbb{R}^{T\\times N\\times F}\\), where \\(T\\) is the length of historical observation and \\(F\\) is the dimension of input features. The goal is to predict the future of traffic flows with the length of \\(T^{\\prime}\\), denoted as \\(\\mathbf{X}^{\\prime}=(\\mathbf{X}_{1}^{\\prime},\\mathbf{X}_{2}^{\\prime},\\cdots,\\mathbf{X}_{T^{ \\prime}}^{\\prime})\\in\\mathbb{R}^{T^{\\prime}\\times N\\times F}\\). **Framework Overview**. To solve the aforementioned problem with the help of large language models, we propose a novel framework termed Large Language Model Enhanced Traffic Flow Predictor (LEAF). The overview of the framework Figure 2: The framework of the proposed LEAF. LEAF consists of a dual-branch traffic flow predictor and a large language model based selector. The predictor generates forecasts of future traffic flows through the graph branch and the hypergraph branch. The selector constructs choice set and then selects the best option using (frozen) LLMs. The results of selection is used to supervise the predictor. is illustrated in Figure 2. Specifically, to achieve LLM-enhanced prediction (Section 3.4), we design a dual-branch traffic flow predictor (Section 3.2) and an LLM-based selector (Section 3.3). The predictor consists of a graph neural network branch capturing pair-wise spatio-temporal relations, and a hypergraph neural network branch capturing non-pair-wise relations. During training, the predictor is first pretrained using the training data. During test time, we apply transformations to the forecasting results of the predictor to obtain a variety of choices, among which the selector chooses the best one with a frozen LLM. The selection results are then used to fine-tune the dual-branch predictor with a ranking loss during test time so as to improve the adaptability of the model."
    },
    {
      "title": "Dual-Branch Traffic Flow Predictor",
      "text": "Previous works on traffic flow forecasting adopt _the graph perspective_(Song et al., 2020; Zheng et al., 2023) or _the hypergraph perspective_(Wang et al., 2022; Zhao et al., 2023). The graph perspective propagates messages between pairs of nodes, which makes them adept in modeling pair-wise spatio-temporal relations (_e.g._ an accident affects adjacent locations). On the other hand, the hypergraph perspective propagates messages among groups of nodes, making them proficient in modeling non-pair-wise spatio-temporal relations (_e.g._ people move from the residential area to the business area). For LEAF, we aim to take the benefits from both sides by constructing a dual-branch predictor and letting the LLM to select. **Spatio-temporal Graph Construction**. To utilize graph neural networks and hypergraph neural networks, we first construct a spatio-temporal graph corresponding to the input tensor \\(\\mathbf{X}\\in\\mathbb{R}^{T\\times N\\times F}\\). Particularly, the length of historical data \\(T\\) yields a set of \\(TN\\) spatio-temporal nodes, denoted as: \\[\\mathcal{V}^{ST}=\\left\\{v_{i}^{t}\\mid i=1,\\dots,N,t=1,\\dots,T\\right\\}, \\tag{1}\\] and we add temporal edges in addition to spatial edges \\(\\mathcal{E}\\) to obtain the edge set \\(\\mathcal{E}^{ST}\\): \\[\\mathcal{E}^{ST}=\\big{\\{}\\langle v_{i}^{t_{1}},v_{j}^{t_{2}}\\rangle \\mid(|t_{1}-t_{2}|=1\\wedge i=j)\\\\ \\vee\\left(t_{1}=t_{2}\\wedge\\langle i,j\\rangle\\in\\mathcal{E} \\right)\\big{\\}}. \\tag{2}\\] The spatio-temporal graph can then be represented as \\(\\mathcal{G}^{ST}=\\langle\\mathcal{V}^{ST},\\mathcal{E}^{ST},\\mathbf{A}^{ST}\\rangle\\), where \\(\\mathbf{A}^{ST}\\in\\mathbb{R}^{TN\\times TN}\\). The spatio-temporal features \\(\\mathbf{X}^{(0)}\\in\\mathbb{R}^{TN\\times d}\\) is derived from \\(\\mathbf{X}\\in\\mathbb{R}^{T\\times N\\times F}\\) with a linear mapping and a reshape operation, where \\(d\\) is the dimension of the hidden space. **The Graph Branch**. Based on the constructed spatio-temporal graph, we first adopt a graph neural network to model pair-wise spatio-temporal relations. Concretely, given the spatio-temporal feature inputs \\(\\mathbf{X}^{(0)}\\in\\mathbb{R}^{TN\\times d}\\), we adopt convolution layers to process the features, which is \\[\\mathbf{X}^{(l)}=\\sigma\\left(\\widehat{\\mathbf{A}^{ST}}\\mathbf{X}^{(l-1)}\\mathbf{W}_{G}^{(l)} \\right), \\tag{3}\\] where \\(\\sigma(\\cdot)\\) is the activation layer and \\(\\mathbf{W}_{G}^{(l)}\\in\\mathbb{R}^{d\\times d}\\) is the weight matrix. \\(\\widehat{\\mathbf{A}^{ST}}=\\mathbf{D}^{-1/2}\\mathbf{A}^{ST}\\mathbf{D}^{-1/2}\\) is the normalized version of the adjacency matrix, where \\(\\mathbf{D}\\) is the degree matrix (Kipf and Welling, 2016; Song et al., 2020). By adopting graph convolutions in Eq. 3, the information from one spatio-temporal vertex can be propagated to its neighbors as defined in \\(\\mathcal{E}^{ST}\\) in Eq. 2, and thus this branch models pair-wise spatio-temporal relations. **The Hypergraph Branch**. Although the graph branch is adept in capturing pair-wise relations, the complex traffic patterns contain non-pair-wise relations. For example, in the morning rush hours, people move from the residential area (which is a set of vertices in a hyperedge) to the business area (which is another hyperedge). The vertices in one hyperedge share common patterns and the hyperedges affect each other. To model non-pair-wise relations, hypergraphs are adopted. For a hypergraph, its incidence matrix \\(\\mathbf{I}_{H}\\in\\mathbb{R}^{NT\\times m}\\) describes the assignment of \\(NT\\) vertices to \\(m\\) hyperedges. As the incidence matrix is not given as input, we resort to a learnable one with low-rank decomposition (Zhao et al., 2023): \\[\\mathbf{I}_{H}=\\mathrm{softmax}(\\mathbf{X}_{H}^{(l-1)}\\mathbf{W}_{H}), \\tag{4}\\] where \\(\\mathbf{X}_{H}^{(l-1)}\\in\\mathbb{R}^{NT\\times d}\\) is the hidden features, and \\(\\mathbf{W}_{H}\\in\\mathbb{R}^{d\\times m}\\) is the weight matrix. \\(\\mathrm{softmax}(\\cdot)\\) is applied for normalization. Subsequently, the output features can be computed as: \\[\\mathbf{X}_{H}^{(l)}=\\mathbf{I}_{H}\\left(\\mathbf{I}_{H}^{T}\\mathbf{X}_{H}^{(l-1)}+\\sigma\\left( \\mathbf{W}_{E}\\mathbf{I}_{H}^{T}\\mathbf{X}_{H}^{(l-1)}\\right)\\right), \\tag{5}\\] where \\(\\mathbf{W}_{E}\\in\\mathbb{R}^{m\\times m}\\) models the interactions of the hyperedges. In this way, the hypergraph branch considers both (_a_) the interactions within a set of vertices (within a hyperedge) through the first term of Eq. 5, and (_b_) the interactions among groups of vertices (among the hyperedges) through the second term of Eq. 5."
    },
    {
      "title": "Large Language Model Based Selector",
      "text": "In the Section 3.2, we obtain different prediction results from different branches, denoted as \\(\\mathbf{Y}^{G}\\) and \\(\\mathbf{Y}^{H}\\). Most previous works [11, 10, 12] directly generate these predictions, which is challenging since the complex spatio-temporal relations are hard to express in texts. By comparison, the LLM-based selector aims to choose the best prediction, using the internal knowledge of a frozen LLM and the constructed prompts. As the traffic networks are usually large, we break the result \\(\\mathbf{Y}\\in\\{\\mathbf{Y}^{G},\\mathbf{Y}^{H}\\}\\subset\\mathbb{R}^{T^{\\prime}\\times N}\\) for individual vertices, _i.e._\\(\\mathbf{Y}=[\\mathbf{y}_{1},\\mathbf{y}_{2},\\dots,\\mathbf{y}_{N}]\\), where \\(\\mathbf{y}_{i}\\in\\mathbb{R}^{T^{\\prime}}\\). **Choice Set Construction**. In practice, we want to give the LLM-based selector more choices so that it has the potential to make better predictions. Therefore, we introduce several transformations: smoothing, upward trend, downward trend, overestimate, and underestimate. The set of transformations is denoted as \\(\\mathcal{T}\\). For vertex \\(i\\in\\mathcal{V}\\), the choice set is determined as follows: \\[\\mathcal{C}_{i}=\\big{\\{}\\tau(\\mathbf{y}_{i})|\\tau\\in\\mathcal{T},\\mathbf{y}_{i}\\in\\{ \\mathbf{y}_{i}^{G},\\mathbf{y}_{i}^{H}\\}\\}\\cup\\{\\mathbf{y}_{i}^{G},\\mathbf{y}_{i}^{H}\\}. \\tag{6}\\] By adopting transformations, the choice set is expanded and the selector has the potential to deal with more complex situations. For instance, if it believes that both of the branches underestimate the traffic flow on a Monday morning, the selector can choose an option with an upward trend. **Prompt Construction**. When constructing the prompt, we consider the following aspects. (1) General information about the data, including the meaning of the numbers, the way traffic data are selected, etc. (2) The spatial information of the vertex, including the sensor id, and the geometric location information. (3) The temporal information, including the time historical data are collected, the time we want to forecast, and whether there are special events. (4) The historical data. (5) The task instructions. (6) The choice set constructed in Eq. 6, including the names of specific branches and the description of augmentations. An illustration of this is shown in Figure 3."
    },
    {
      "title": "Large Language Model Enhanced Prediction",
      "text": "The LEAF framework consists of a predictor and a selector. The predictor (the graph branch and the hypergraph branch) is pre-trained on the training set. During test time, the predictor first predicts, and the selector then selects. The selection results are used to supervise the predictor, and thus the two modules benefit from each other, achieving large language model enhanced prediction. Concretely, given the input data \\(\\mathbf{X}\\in\\mathbb{R}^{T\\times N\\times F}\\), the predictor generates two forecasts, _i.e._\\(\\mathbf{Y}^{G}\\) and \\(\\mathbf{Y}^{H}\\). Subsequently, the selector constructs choice sets and use the LLM to find the best option for each individual vertex. The selection results is denoted as \\(\\hat{\\mathbf{y}}_{i}\\), \\(i\\in\\mathcal{V}\\), which are then used to supervise the predictor. Conceivably, \\(\\hat{\\mathbf{y}}_{i}\\) may not be the optimal choice in the choice set, and therefore, we adopt a ranking loss described as follows: \\[\\mathcal{L}^{G}=[\\Delta(\\mathbf{y}_{i}^{G},\\hat{\\mathbf{y}}_{i})-\\inf_{\\mathbf{y}_{i}^{G} \\in\\mathcal{C}_{i}\\setminus\\{\\hat{\\mathbf{y}}_{i}\\}}\\Delta(\\mathbf{y}_{i}^{G},\\mathbf{y} _{i}^{\\prime})+\\epsilon]_{+}, \\tag{7}\\] where \\([\\cdot]_{+}\\) is the hinge function, \\(\\Delta(\\cdot,\\cdot)\\) is a distance measure, and \\(\\epsilon\\) is the margin. Similarly, we can define the loss function \\(\\mathcal{L}^{H}\\) using \\(\\mathbf{y}_{i}^{H}\\). The final objective is written as: \\[\\mathcal{L}=\\mathcal{L}^{G}+\\mathcal{L}^{H}. \\tag{8}\\] In Eq. 7 and Eq. 8, we encourage the forecasts of the predictor (_i.e._\\(\\mathbf{y}_{i}^{G}\\) and \\(\\mathbf{y}_{i}^{H}\\)) to be closer to the selected prediction (_i.e._\\(\\hat{\\mathbf{y}}_{i}\\)) than the closest one in suboptimal predictions (_i.e._\\(\\mathcal{C}_{i}\\setminus\\{\\hat{\\mathbf{y}}_{i}\\}\\)). Since the ground truth may not be covered by the choice set, this objective is better than directly minimizing the distance between the predictions and the selected forecast, as it allows the model to learn from a better choice compared to suboptimal choices. By supervising the predictor, the two modules (_i.e._ the predictor and the selector) benefit from Figure 3: An illustration of the prompt template. each other. A better predictor yields better choices, which benefits the selector; better selection results provide better supervision signals for the predictor. Through the iteration of prediction and selection, we can achieve large language model enhanced prediction. The LEAF framework during inference is summarized in Algorithm 1. ``` 0: The road network \\(\\mathcal{G}=\\langle\\mathcal{V},\\mathcal{E},\\mathbf{A}\\rangle\\), historical data \\(\\mathbf{X}\\), the number of iterations \\(K\\), the graph branch \\(\\mathcal{B}^{G}\\), and the hypergraph branch \\(\\mathcal{B}^{H}\\). 0: The forecasting result \\(\\hat{\\mathbf{Y}}\\). 1: Construct spatio-temporal graph \\(\\mathcal{G}^{ST}\\). 2:for\\(j=1,2,...,K\\)do 3: Compute the prediction of the graph branch \\(\\mathcal{B}^{G}\\) as \\(\\mathbf{Y}^{G}\\). 4: Compute the prediction of the hypergraph branch \\(\\mathcal{B}^{H}\\) as \\(\\mathbf{Y}^{H}\\). 5: Construct the choice set using Eq. 6. 6: Use LLM to select the best option for each vertex as \\(\\hat{\\mathbf{y}}_{i}\\) where \\(i\\in\\mathcal{V}\\). 7: Use \\(\\hat{\\mathbf{y}}_{i}\\) to supervise the predictor (\\(\\mathcal{B}^{G}\\) and \\(\\mathcal{B}^{H}\\)) with Eq. 7 and Eq. 8. 8:endfor 9: Stack \\(\\hat{\\mathbf{y}}_{i},i\\in\\mathcal{V}\\) to obtain \\(\\hat{\\mathbf{Y}}\\) ``` **Algorithm 1** The Algorithm of LEAF"
    },
    {
      "title": "4 Experiments",
      "text": ""
    },
    {
      "title": "Experimental Setup",
      "text": "**Datasets**. We adopt three widely used datasets in traffic flow forecasting, including PEMS03, PEMS04, and PEMS08. The datasets are publicly available and collected by California Transportation Agencies (CalTrans) Performance Measurement Systems (PEMS). 1. The traffic data come from sensors on the road of various places in California, and they are counted every five minutes. Footnote 1: [https://pems.dot.ca.gov/](https://pems.dot.ca.gov/) **Evaluation Metrics**. We follow standard setting (Li et al., 2018) that use one-hour historical data (_i.e._\\(T=12\\) timesteps) to forecast one-hour future (_i.e._\\(T^{\\prime}=12\\) timesteps). For evaluation the prediction results, we use three standard metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) (Song et al., 2020). **Baseline Methods**. We compare LEAF with a variety of baseline methods, including DCRNN (Li et al., 2018), ASTGCN (Guo et al., 2019), STSGCN (Song et al., 2020), HGCN (Wang et al., 2022), DyHSL (Zhao et al., 2023), STAEformer (Liu et al., 2023), COOL (Ju et al., 2024), and LLM-MPE (Liang et al., 2024). Among these methods, ASTGCN, STSGCN, and COOL are based on graph neural networks, while HGCN and DyHSL are based on hypergraph neural networks. STAEformer directly takes the transformer architecture for the traffic data, while LLM-MPE use LLMs to understand and predict traffic data with textual guidance. **Implementation Details**. For the dual-branch predictor, the number of layers \\(L\\) for both branches is set to 7. We use linear mapping as the input embedding, and the hidden dimension \\(d\\) is set to 64, which is also shared across all baselines. We also use a two layer MLP to map the last hidden embedding to the output. In choice set construction, smoothing is implemented with an average filter, upward/downward trend increases/decreases the traffic flow by 1% to 12% (12 timesteps, linearly increasing), overestimate/underestimate increases/decreases the traffic flow by 5% (for all 12 timesteps). As for the loss function in Eq. 7, we adopt Huber distance for \\(\\Delta(\\cdot,\\cdot)\\) and the margin \\(\\epsilon\\) is set to 0. When training with this loss function, we update the parameters for \\(M\\) iterations, which is set to 5. For the prediction-selection loop, we set \\(K\\) to 2. For the LLM, we use LLaMA 3 70B (AI@Meta, 2024) and vLLM (Kwon et al., 2023) as the inference software."
    },
    {
      "title": "Main Results",
      "text": "The performance of LEAF in comparison with baselines are shown in Table 1. According to the results, we have several observations: Firstly, the proposed LEAF achieves a consistent lead in all three datasets, which demonstrates the effectiveness of the proposed framework that adopts a dual-branch traffic flow predictor and a LLM-based selector. Prior methods often fail to provide satisfactory predictions when there is test-time distribution changes, as they cannot learn adaptively, leaving the room for improvement. Secondly, the method that utilizes the generative ability of LLMs (_i.e._ LLM-MPE) does not perform well on all datasets. As we can see on PEMS03 and PEMS04 datasets, its predictions are generally worse or similar compared to simple methods using graph neural networks. Since LLMs are not very adept in capturing complex spatio-temporalrelations, it is reasonable that we see such results on datasets with larger networks like PEMS03. Thirdly, the proposed LEAF generally performs better than graph-based methods (_e.g._ ASTGCN, STSGCN) and hypergraph-based methods (_e.g._ HGCN, DyHSL), which shows that our method has the potential to take advantage of both complex spatio-temporal relations captured by the predictor and the knowledge of large language models, achieving LLM enhanced traffic flow forecasting."
    },
    {
      "title": "Ablation Study",
      "text": "We also perform ablation studies on the PEMS08 dataset, and the results are shown in Table 2. Specifically, we perform the following experiments. **E1** measures the performance of graph branch only without the LLM-based selector. **E2** measures the performance of the hypergraph branch without the selector. The vanilla version (E1 and E2) of both branches performs much worse than LEAF. **E3** uses the graph branch in conjunction with the selector, which leads to performance degradation compared to LEAF. This suggests that non-pair-wise relations are important in traffic flow forecasting. **E4** only uses the hypergraph branch together with the selector. Similarly, it performs worse than LEAF, which shows that pair-wise and non-pair-wise relations are both important. Moreover, E3 and E4 perform better than E1 and E2, which shows the effect of the LLM-based selector. **E5** removes the transformations, _i.e._\\(\\mathcal{T}=\\emptyset\\). This leads to slightly worse performance, showing the effectiveness of providing more choices. **E6** removes the ranking loss, which means that the predictor are not further trained with the results from the selector (reducing to \\(K=1\\)). This experiment demonstrates the effectiveness of supervising the predictor with a ranking loss using the selection results."
    },
    {
      "title": "Hyper-Parameter Analysis",
      "text": "We perform experiments with respect to two hyperparameters: **(a)**\\(M\\), _i.e._ the number of iterations when training with the ranking loss in Eq.7 and Eq.8, and **(b)** the number of \\(K\\), _i.e._ the number of prediction-selection iterations in Algorithm 1. The results on the PEMS08 dataset are shown in Figure 4. As can be seen from the figure, when \\(M\\) increases, both MAE and RMSE decreases, and then plateaus after around 5. This shows that the predictor converges and therefore, we set \\(M\\) to 5. For another hyper-parameter \\(K\\), the optimal performance is achieved when \\(K\\) is set to 2, and with more iter \\begin{table} \\begin{tabular}{l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{3}{c}{**PEMS03**} & \\multicolumn{3}{c}{**PEMS04**} & \\multicolumn{3}{c}{**PEMS08**} \\\\ \\cline{2-9} & MAE & RMSE & MAPE & MAE & RMSE & MAPE & MAE & RMSE & MAPE \\\\ \\hline DCRNN (Li et al., 2018) & 29.99 & 39.52 & 21.33 & 34.36 & 46.19 & 24.73 & 31.41 & 43.91 & 15.44 \\\\ ASTGCN (Guo et al., 2019) & 28.4 & 41.94 & 15.78 & 33.09 & 46.08 & 18.19 & 29.20 & 41.16 & 12.76 \\\\ STSGNN (Song et al., 2020) & 28.21 & 43.43 & 15.49 & 33.43 & 45.69 & 18.89 & 29.58 & 41.95 & 12.90 \\\\ HGCN (Wang et al., 2022) & 28.43 & 43.92 & 15.39 & 35.77 & 49.92 & 20.11 & 28.83 & 39.65 & 12.63 \\\\ DyHSL (Zhao et al., 2023) & 27.10 & 41.59 & 14.31 & 33.36 & 46.96 & 19.64 & 27.34 & 39.05 & 11.56 \\\\ STAEformer (Liu et al., 2023) & 27.87 & 37.31 & 16.49 & 33.77 & 45.50 & 18.36 & 27.43 & 38.16 & 11.36 \\\\ COOL (Ju et al., 2024) & 27.51 & 41.11 & 14.73 & 34.68 & 47.22 & 19.72 & 27.22 & 38.47 & 11.72 \\\\ LLM-MPE (Liang et al., 2024) & 33.82 & 47.06 & 20.40 & 35.63 & 51.41 & 18.19 & 26.42 & 40.02 & 10.61 \\\\ \\hline **LEAF** & **25.46** & **35.17** & **14.22** & **31.49** & **44.45** & **17.53** & **24.68** & **36.07** & **10.56** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Forecasting errors on PEMS03, PEMS04, and PEMS08 datasets. \\begin{table} \\begin{tabular}{l c c c} \\hline \\hline **Experiments** & MAE & RMSE & MAPE \\\\ \\hline E1: Graph branch & 29.12 & 41.36 & 13.54 \\\\ \\hline E2: Hypergraph branch & 27.94 & 39.11 & 11.82 \\\\ \\hline E3: _w/o_ hypergraph branch & 26.29 & 38.18 & 12.83 \\\\ \\hline E4: _w/o_ graph branch & 25.80 & 37.23 & 11.00 \\\\ \\hline E5: _w/o_ transformation & 25.47 & 36.47 & 11.01 \\\\ \\hline E6: _w/o_ ranking loss & 25.41 & 37.00 & 11.34 \\\\ \\hline LEAF & 24.68 & 36.07 & 10.56 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Ablation study on the PEMS08 dataset. Figure 4: The forecasting errors under different hyper-parameters, _i.e._\\(M\\)s (left) and \\(K\\)s (right). [MISSING_PAGE_EMPTY:8]"
    },
    {
      "title": "Limitations",
      "text": "One limitation of this work is that we only focus on the traffic flow forecasting domain, due to the scope of this paper and the limited computational resources. In the future, we plan to extend this framework to more generalized spatio-temporal forecasting problem. Besides, we do not evaluate our methods on other traffic flow forecasting datasets due to limited resources. For future works, we plan to evaluate our framework using other datasets."
    },
    {
      "title": "References",
      "text": "* A1@Meta (2024) Al@Meta. 2024. Llama 3 model card. * Asadi et al. (2012) Shahrokh Asadi, Akbar Tavakoli, and Seyed Reza Hejazi. 2012. A new hybrid for improvement of auto-regressive integrated moving average models applying particle swarm optimization. _Expert Systems with Applications_, 39(5):5332-5337. * Bao et al. (2024) Keqin Bao, Jizhi Zhang, Xinyu Lin, Yang Zhang, Wenjie Wang, and Fuli Feng. 2024. Large language models for recommendation: Past, present, and future. In _Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 2993-2996. * Beniwal et al. (2024) Himanshu Beniwal, Dishant Patel, D Kowsk, Hrtik Ladia, Ankit Yadav, and Mayank Singh. 2024. Remember this event that year? assessing temporal information and understanding in large language models. In _Findings of the Association for Computational Linguistics: EMNLP 2024_, pages 16239-16348. * Boukerche et al. (2020) Azzedine Boukerche, Yanjie Tao, and Peng Sun. 2020. Artificial intelligence-based vehicular traffic flow prediction methods for supporting intelligent transportation systems. _Computer networks_, 182:107484. * Cai et al. (2020) Ling Cai, Krzysztof Janowicz, Gengchen Mai, Bo Yan, and Rui Zhu. 2020. Traffic transformer: Capturing the continuity and periodicity of time series for traffic forecasting. _Transactions in GIS_, 24(3):736-755. * Chang et al. (2024) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A survey on evaluation of large language models. _ACM Transactions on Intelligent Systems and Technology_, 15(3):1-45. * Chen et al. (2022a) Changlu Chen, Yanbin Liu, Ling Chen, and Chengqi Zhang. 2022a. Bidirectional spatial-temporal adaptive transformer for urban traffic flow forecasting. _IEEE Transactions on Neural Networks and Learning Systems_, 34(10):6913-6925. * Chen et al. (2024a) Jian Chen, Li Zheng, Yuzhu Hu, Wei Wang, Hongxing Zhang, and Xiping Hu. 2024a. Traffic flow matrix-based graph neural network with attention mechanism for traffic flow prediction. _Information Fusion_, 104:102146. * Chen et al. (2024b) Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, et al. 2024b. When large language models meet personalization: Perspectives of challenges and opportunities. _World Wide Web_, 27(4):42. * Chen et al. (2022b) Ling Chen, Wei Shao, Mingqi Lv, Weiqi Chen, Youdong Zhang, and Chenghu Yang. 2022b. Aargm: An attentive attributed recurrent graph neural network for traffic flow prediction considering multiple dynamic factors. _IEEE Transactions on Intelligent Transportation Systems_, 23(10):17201-17211. * Chen et al. (2024c) Mouxiang Chen, Lefei Shen, Han Fu, Zhuo Li, Jianling Sun, and Chenghao Liu. 2024c. Calibration of time-series forecasting: Detecting and adapting context-driven distribution shift. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 341-352. * Chung et al. (2014) Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. _arXiv preprint arXiv:1412.3555_. * Dai et al. (2022) Fei Dai, Penggui Huang, Qi Mo, Xiaolong Xu, Muhammad Bilal, and Houbing Song. 2022. St-innet: Deep spatio-temporal inception networks for traffic flow prediction in smart cities. _IEEE Transactions on Intelligent Transportation Systems_, 23(10):19782-19794. * Dimitrakopoulos and Demestichas (2010) George Dimitrakopoulos and Panagiotis Demestichas. 2010. Intelligent transportation systems. _IEEE Vehicular Technology Magazine_, 5(1):77-84. * Fouladgar et al. (2017) Mohammadhani Fouladgar, Mostafa Parchami, Ramez Elmasri, and Amir Ghaderi. 2017. Scalable deep traffic flow neural networks for urban traffic congestion prediction. In _2017 international joint conference on neural networks (IJCNN)_, pages 2251-2258. IEEE. * Gagnon-Audet et al. (2022) Jean-Christophe Gagnon-Audet, Kartik Ahuja, Mohammad-Javad Darvishi-Bayazi, Pooneh Mousavi, Guillaume Dumas, and Irina Rish. 2022. Woods: Benchmarks for out-of-distribution generalization in time series. _arXiv preprint arXiv:2203.09978_. * Ghosh et al. (2009) Bidisha Ghosh, Biswajit Basu, and Margaret O'Mahony. 2009. Multivariate short-term traffic flow forecasting using time-series analysis. _IEEE transactions on intelligent transportation systems_, 10(2):246-254. * Gruver et al. (2024) Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew G Wilson. 2024. Large language models are zero-shot time series forecasters. _Advances in Neural Information Processing Systems_, 36. * Gruver et al. (2017)Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. 2019. Attention based spatial-temporal graph convolutional networks for traffic flow forecasting. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 922-929. * Guo et al. (2024a) Xusen Guo, Qiming Zhang, Junyue Jiang, Mingxing Peng, Meixin Zhu, and Hao Frank Yang. 2024a. Towards explainable traffic flow prediction with large language models. _Communications in Transportation Research_, 4:100150. * Guo et al. (2024b) Xusen Guo, Qiming Zhang, Mingxing Peng, Meixin Zhua, et al. 2024b. Explainable traffic flow prediction with large language models. _arXiv preprint arXiv:2404.02937_. * Hamilton et al. (2017) Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30. * Han et al. (2024) Xiao Han, Zhenduo Zhang, Yiling Wu, Xinfeng Zhang, and Zhe Wu. 2024. Event traffic forecasting with sparse multimodal data. In _Proceedings of the 32nd ACM International Conference on Multimedia_, pages 8855-8864. * Hochreiter (1997) S Hochreiter. 1997. Long short-term memory. _Neural Computation MIT-Press_. * Hong et al. (2011) Wei-Chiang Hong, Yucheng Dong, Feifeng Zheng, and Shih Yung Wei. 2011. Hybrid evolutionary algorithms in a svr traffic flow forecasting model. _Applied Mathematics and Computation_, 217(15):6733-6747. * Hu et al. (2024) Qisheng Hu, Geonsik Moon, and Hwee Tou Ng. 2024. From moments to milestones: Incremental timeline summarization leveraging large language models. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 7232-7246. * Jian et al. (2024) Chengtao Jian, Kai Yang, and Yang Jiao. 2024. Tri-level navigator: Llm-empowered tri-level learning for time series ood generalization. _arXiv preprint arXiv:2410.07018_. * Jiang et al. (2021) Chun-Yao Jiang, Xiao-Min Hu, and Wei-Neng Chen. 2021. An urban traffic signal control system based on traffic flow prediction. In _2021 13th international conference on advanced computational intelligence (ICACI)_, pages 259-265. IEEE. * Jiang et al. (2023) Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. 2023. Ploformer: Propagation delayer-aware dynamic long-range transformer for traffic flow prediction. In _Proceedings of the AAAI conference on artificial intelligence_, volume 37, pages 4365-4373. * Ju et al. (2024) Wei Ju, Yusheng Zhao, Yifang Qin, Siyu Yi, Jingyang Yuan, Zhiping Xiao, Xiao Luo, Xiting Yan, and Ming Zhang. 2024. Cool: a conjoint perspective on spatio-temporal graph neural network for traffic forecasting. _Information Fusion_, 107:102341. * Kim et al. (2024) Dongmin Kim, Sunghyun Park, and Jaegul Choo. 2024. When model meets new normals: Test-time adaptation for unsupervised time-series anomaly detection. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 13113-13121. * Kipf and Welling (2016) Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_. * Koval et al. (2024) Ross Koval, Nicholas Andrews, and Xifeng Yan. 2024. Financial forecasting from textual and tabular time series. In _Findings of the Association for Computational Linguistics: EMNLP 2024_, pages 8289-8300. * Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_. * Labrak et al. (2024) Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. 2024. Biomistral: A collection of open-source pretrained large language models for medical domains. _arXiv preprint arXiv:2402.10373_. * Lai et al. (2024) Jinqi Lai, Wensheng Gan, Jiayang Wu, Zhenlian Qi, and S Yu Philip. 2024. Large language models in law: A survey. _AI Open_. * Lan et al. (2022) Shiyong Lan, Yitong Ma, Weikang Huang, Wenwu Wang, Hongyu Yang, and Pyang Li. 2022. Dstagnn: Dynamic spatial-temporal aware graph neural network for traffic flow forecasting. In _International conference on machine learning_, pages 11906-11917. PMLR. * Li and Zhu (2021) Mengzhang Li and Zhanxing Zhu. 2021. Spatial-temporal fusion graph neural networks for traffic flow forecasting. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 4189-4196. * Li et al. (2024a) Nian Li, Chen Gao, Mingyu Li, Yong Li, and Qingmin Liao. 2024a. Econagent: large language model-empowered agents for simulating macroeconomic activities. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 15523-15536. * Li et al. (2012) Shuangshuang Li, Zhen Shen, and Gang Xiong. 2012. A k-nearest neighbor locally weighted regression method for short-term traffic flow forecasting. In _2012 15th International IEEE Conference on Intelligent Transportation Systems_, pages 1596-1601. IEEE. * Li et al. (2018) Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2018. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. In _International Conference on Learning Representations_. Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, and Chao Huang. 2024b. Urbangpt: Spatio-temporal large language models. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 5351-5362. * Liang et al. (2024) Yuebing Liang, Yichao Liu, Xiaohan Wang, and Zhan Zhao. 2024. Exploring large language models for human mobility prediction under public events. _Computers, Environment and Urban Systems_, 112:102153. * Liebig et al. (2017) Thomas Liebig, Nico Piatkowski, Christian Bockermann, and Katharina Morik. 2017. Dynamic route planning with real-time traffic predictions. _Information Systems_, 64:258-265. * Lievin et al. (2024) Valentin Lievin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther. 2024. Can large language models reason about medical questions? _Patterns_, 5(3). * Liu et al. (2024) Chenxi Liu, Sun Yang, Qianxiong Xu, Zhishuai Li, Cheng Long, Ziyue Li, and Rui Zhao. 2024. Spatial-temporal large language model for traffic prediction. _arXiv preprint arXiv:2401.10134_. * Liu et al. (2023) Hangchen Liu, Zheng Dong, Renhe Jiang, Jiewen Deng, Jinliang Deng, Quanjun Chen, and Xuan Song. 2023. Spatio-temporal adaptive embedding makes vanilla transformer sota for traffic forecasting. In _Proceedings of the 32nd ACM international conference on information and knowledge management_, pages 4125-4129. * Lu et al. (2022) Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, and Xing Xie. 2022. Out-of-distribution representation learning for time series classification. _arXiv preprint arXiv:2209.07027_. * Luo et al. (2022) Xiaoyi Luo, Jiaheng Peng, and Jun Liang. 2022. Directed hypergraph attention network for traffic forecasting. _IET Intelligent Transport Systems_, 16(1):85-98. * Ma et al. (2024) Ying Ma, Haijie Lou, Ming Yan, Fanghui Sun, and Guoqi Li. 2024. Spatio-temporal fusion graph convolutional network for traffic flow forecasting. _Information Fusion_, 104:102196. * Milano et al. (2023) Silvia Milano, Joshua A McGrane, and Sabina Leonelli. 2023. Large language models challenge the future of higher education. _Nature Machine Intelligence_, 5(4):333-334. * Minaee et al. (2024) Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2024. Large language models: A survey. _arXiv preprint arXiv:2402.06196_. * Ren et al. (2024) Yilong Ren, Yue Chen, Shuai Liu, Boyue Wang, Haiyang Yu, and Zhiyong Cui. 2024. Tpllm: A traffic prediction framework based on pretrained large language models. _arXiv preprint arXiv:2403.02221_. * Shahid et al. (2021) Nimra Shahid, Munam Ali Shah, Abid Khan, Carsten Maple, and Gwanggil Jeon. 2021. Towards greener smart cities and road traffic forecasting using air pollution data. _Sustainable Cities and Society_, 72:103062. * Smith and Demetsky (1997) Brian L Smith and Michael J Demetsky. 1997. Traffic flow forecasting: comparison of modeling approaches. _Journal of transportation engineering_, 123(4):261-266. * Song et al. (2020) Chao Song, Youfang Lin, Shengnan Guo, and Huaiyu Wan. 2020. Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 914-921. * Sun et al. (2006) Shiliang Sun, Changshui Zhang, and Guoqiang Yu. 2006. A bayesian network approach to traffic flow forecasting. _IEEE Transactions on intelligent transportation systems_, 7(1):124-132. * Tchrakian et al. (2011) Tigran T Tchrakian, Biswajit Basu, and Margaret O'Mahony. 2011. Real-time traffic flow forecasting using spectral analysis. _IEEE Transactions on Intelligent Transportation Systems_, 13(2):519-526. * Veen et al. (2024) Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna Seehofnerova, et al. 2024. Adapted large language models can outperform medical experts in clinical text summarization. _Nature medicine_, 30(4):1134-1142. * Vaswani (2017) A Vaswani. 2017. Attention is all you need. _Advances in Neural Information Processing Systems_. * Velickovic et al. (2018) Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph attention networks. In _International Conference on Learning Representations_. * Wang et al. (2024) Jingcheng Wang, Yong Zhang, Yongli Hu, and Baocai Yin. 2024. Large-scale traffic prediction with hierarchical hypergraph message passing networks. _IEEE Transactions on Computational Social Systems_. * Wang et al. (2022) Jingcheng Wang, Yong Zhang, Lixun Wang, Yongli Hu, Xinglin Piao, and Baocai Yin. 2022. Multitask hypergraph convolutional networks: A heterogeneous traffic prediction framework. _IEEE Transactions on Intelligent Transportation Systems_, 23(10):18557-18567. * Wang et al. (2020) Xiaoyang Wang, Yao Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang Tang, Caiyan Jia, and Jian Yu. 2020. Traffic flow prediction via spatial temporal graph neural network. In _Proceedings of the web conference 2020_, pages 1082-1092. * Wang and Zhu (2022) Yi Wang and Di Zhu. 2022. Shgcn: a hypergraph-based deep learning model for spatiotemporal traffic flow prediction. In _Proceedings of the 5th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery_, pages 30-39. Ytwei Xia, Ding Wang, Qiang Liu, Liang Wang, Shu Wu, and Xiao-Yu Zhang. 2024. Chain-of-history reasoning for temporal knowledge graph forecasting. In _Findings of the Association for Computational Linguistics ACL 2024_, pages 16144-16159. * Xu et al. (2018) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful are graph neural networks? In _International Conference on Learning Representations_. * Xu et al. (2020) Mingxing Xu, Wenrui Dai, Chunmiao Liu, Xing Gao, Weiyao Lin, Guo-Jun Qi, and Hongkai Xiong. 2020. Spatial-temporal transformer networks for traffic flow forecasting. _arXiv preprint arXiv:2001.02908_. * Yang et al. (2016) Hao-Fan Yang, Tharam S Dillon, and Yi-Ping Phoebe Chen. 2016. Optimized structure of the traffic flow forecasting model with a deep learning approach. _IEEE transactions on neural networks and learning systems_, 28(10):2371-2381. * Yin et al. (2024) Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2024. A survey on multimodal large language models. _National Science Review_, page nwae403. * Yu et al. (2018) Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting. In _Proceedings of the 27th International Joint Conference on Artificial Intelligence_, pages 3634-3640. * Yu et al. (2023) Xinli Yu, Zheng Chen, and Yanbin Lu. 2023. Harnessing llms for temporal data-a study on explainable financial time series forecasting. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track_, pages 739-753. * Yuan et al. (2024) Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, and Yong Li. 2024. Unist: a prompt-empowered universal model for urban spatio-temporal prediction. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 4095-4106. * Zhang et al. (2011) Junping Zhang, Fei-Yue Wang, Kunfeng Wang, Wei-Hua Lin, Xin Xu, and Cheng Chen. 2011. Data-driven intelligent transportation systems: A survey. _IEEE Transactions on Intelligent Transportation Systems_, 12(4):1624-1639. * Zhang et al. (2021) Xiyue Zhang, Chao Huang, Yong Xu, Lianghao Xia, Peng Dai, Liefeng Bo, Junbo Zhang, and Yu Zheng. 2021. Traffic flow forecasting with spatial-temporal graph diffusion network. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 15008-15015. * Zhao et al. (2023) Yusheng Zhao, Xiao Luo, Wei Ju, Chong Chen, Xian-Sheng Hua, and Ming Zhang. 2023. Dynamic hypergraph structure learning for traffic flow forecasting. In _2023 IEEE 39th International Conference on Data Engineering (ICDE)_, pages 2303-2316. IEEE. * Zheng et al. (2023) Chuanpan Zheng, Xiaoliang Fan, Shirui Pan, Haibing Jin, Chaopeng Peng, Zonghan Wu, Cheng Wang, and S Yu Philip. 2023a. Spatio-temporal joint graph convolutional networks for traffic forecasting. _IEEE Transactions on Knowledge and Data Engineering_, 36(1):372-385. * Zheng et al. (2023b) Ou Zheng, Mohamed Abdel-Aty, Dongdong Wang, Zijin Wang, and Shengxuan Ding. 2023b. Chatgpt is on the horizon: could a large language model be suitable for intelligent traffic safety research and applications? _arXiv preprint arXiv:2303.05382_. * Zou et al. (2024) Dongcheng Zou, Senzhang Wang, Xuefeng Li, Hao Peng, Yuandong Wang, Chunyang Liu, Kehua Sheng, and Bo Zhang. 2024. Multispans: A multi-range spatial-temporal transformer network for traffic forecast via structural entropy optimization. In _Proceedings of the 17th ACM International Conference on Web Search and Data Mining_, pages 1032-1041."
    }
  ]
}