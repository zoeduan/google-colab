{
  "title": "A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models",
  "authors": [
    "Peiqin Lin",
    "André F T Martins",
    "Hinrich Schütze"
  ],
  "abstract": "\n Recent studies have highlighted the potential of exploiting parallel corpora to enhance multilingual large language models in both bilingual tasks, e.g., machine translation, and generalpurpose tasks, e.g., text classification. Building upon these findings, our comprehensive study aims to identify the most effective strategies for leveraging parallel corpora. We investigate the impact of parallel corpus quality and quantity, training objectives, and model size on the performance of multilingual large language models enhanced with parallel corpora across diverse languages and tasks. Our analysis reveals several key insights: (i) filtering noisy translations is essential for exploiting parallel corpora, while language identification and short sentence filtering have little effect; (ii) even a corpus with just 10K parallel sentences can yield results comparable to those obtained from larger datasets; (iii) employing only the machine translation objective yields the best results among various training objectives and their combinations; (iv) larger multilingual language models benefit more from parallel corpora than smaller models. Our study offers valuable insights into the optimal utilization of parallel corpora to enhance multilingual large language models, extending the generalizability of previous findings from limited languages and tasks to a broader range of scenarios. \n",
  "references": [
    {
      "id": null,
      "title": "A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models",
      "authors": [
        "Peiqin Lin",
        "André F T Martins",
        "Hinrich Schütze"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "SIB-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects",
      "authors": [
        "David Ifeoluwa",
        "Adelani",
        "Hannah Liu",
        "Xiaoyu Shen",
        "Nikita Vassilyev",
        "Jesujoba O Alabi",
        "Yanke Mao",
        "Haonan Gao",
        "En-Shiun Annie Lee"
      ],
      "year": "2023",
      "venue": "SIB-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects",
      "doi": "10.48550/arXiv.2309.07445"
    },
    {
      "id": "b1",
      "title": "MEGA: multilingual evaluation of generative AI",
      "authors": [
        "Kabir Ahuja",
        "Rishav Hada",
        "Millicent Ochieng",
        "Prachi Jain",
        "Harshita Diddee",
        "Samuel Maina",
        "Tanuja Ganu",
        "Sameer Segal",
        "Maxamed Axmed",
        "Kalika Bali",
        "Sunayana Sitaram"
      ],
      "year": "2023",
      "venue": "MEGA: multilingual evaluation of generative AI",
      "doi": "10.48550/arXiv.2303.12528"
    },
    {
      "id": "b2",
      "title": "Steering large language models for machine translation with finetuning and in-context learning",
      "authors": [
        "M Duarte",
        "Nuno Alves",
        "João Miguel Guerreiro",
        "José Alves",
        "Ricardo Pombal",
        "José Rei",
        "Pierre Guilherme Camargo De Souza",
        "Colombo",
        "F T André",
        "Martins"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": "10.18653/V1/2023.FINDINGS-EMNLP.744"
    },
    {
      "id": "b3",
      "title": "Tower: An open multilingual large language model for translation-related tasks",
      "authors": [
        "M Duarte",
        "José Alves",
        "Pombal",
        "M Nuno",
        "Pedro Henrique Guerreiro",
        "João Martins",
        "M Alves",
        "Ben Amin Farajian",
        "Ricardo Peters",
        "Patrick Rei",
        "Sweta Fernandes",
        "Pierre Agrawal",
        "Colombo",
        "G C José",
        "De Souza",
        "F T André",
        "Martins"
      ],
      "year": "2024",
      "venue": "Tower: An open multilingual large language model for translation-related tasks",
      "doi": "10.48550/ARXIV.2402.17733"
    },
    {
      "id": "b4",
      "title": "Does corpus quality really matter for low-resource languages?",
      "authors": [
        "Mikel Artetxe",
        "Itziar Aldabe",
        "Rodrigo Agerri",
        "Olatz Perez-De-Viñaspre",
        "Aitor Soroa"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "On the cross-lingual transferability of monolingual representations",
      "authors": [
        "Mikel Artetxe",
        "Sebastian Ruder",
        "Dani Yogatama"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
      "doi": "10.18653/v1/2020.acl-main.421"
    },
    {
      "id": "b6",
      "title": "Instructalign: Teaching novel languages with to llms through alignment-based cross-lingual instruction",
      "authors": [
        "Samuel Cahyawijaya",
        "Holy Lovenia",
        "Tiezheng Yu",
        "Willy Chung",
        "Pascale Fung"
      ],
      "year": "2023",
      "venue": "Instructalign: Teaching novel languages with to llms through alignment-based cross-lingual instruction",
      "doi": "10.48550/arXiv.2305.13627"
    },
    {
      "id": "b7",
      "title": "Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning",
      "authors": [
        "Yiming Hao Chen",
        "Qi Zhang",
        "Hantao Zhang",
        "Xiaomeng Yang",
        "Xuetao Hu",
        "Yifan Ma",
        "Junbo Yanggong",
        "Zhao"
      ],
      "year": "2023",
      "venue": "Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning",
      "doi": "10.48550/ARXIV.2305.09246"
    },
    {
      "id": "b8",
      "title": "Infoxlm: An information-theoretic framework for cross-lingual language model pre-training",
      "authors": [
        "Zewen Chi",
        "Li Dong",
        "Furu Wei",
        "Nan Yang",
        "Saksham Singhal",
        "Wenhui Wang",
        "Xia Song",
        "Xian-Ling Mao",
        "Heyan Huang",
        "Ming Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online",
      "doi": "10.18653/v1/2021.naacl-main.280"
    },
    {
      "id": "b9",
      "title": "2021b. Improving pretrained cross-lingual language models via self-labeled word alignment",
      "authors": [
        "Zewen Chi",
        "Li Dong",
        "Bo Zheng",
        "Shaohan Huang",
        "Xian-Ling Mao",
        "Heyan Huang",
        "Furu Wei"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.265"
    },
    {
      "id": "b10",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "Alexis Conneau",
        "Kartikay Khandelwal",
        "Naman Goyal",
        "Vishrav Chaudhary",
        "Guillaume Wenzek",
        "Francisco Guzmán",
        "Edouard Grave",
        "Myle Ott",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online",
      "doi": "10.18653/v1/2020.acl-main.747"
    },
    {
      "id": "b11",
      "title": "Crosslingual language model pretraining",
      "authors": [
        "Alexis Conneau",
        "Guillaume Lample"
      ],
      "year": "2019",
      "venue": "Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "No language left behind: Scaling human-centered machine translation",
      "authors": [
        "Marta R Costa-Jussà",
        "James Cross",
        "Onur Çelebi",
        "Maha Elbayad",
        "Kenneth Heafield",
        "Kevin Heffernan",
        "Elahe Kalbassi",
        "Janice Lam",
        "Daniel Licht",
        "Jean Maillard",
        "Anna Sun",
        "Skyler Wang",
        "Guillaume Wenzek",
        "Al Youngblood",
        "Bapi Akula",
        "Loïc Barrault",
        "Gabriel Mejia Gonzalez",
        "Prangthip Hansanti",
        "John Hoffman",
        "Semarley Jarrett",
        "Ram Kaushik",
        "Dirk Sadagopan",
        "Shannon Rowe",
        "Chau Spruit",
        "Pierre Tran",
        "Necip Andrews",
        "Shruti Fazil Ayan",
        "Sergey Bhosale",
        "Angela Edunov",
        "Cynthia Fan",
        "Vedanuj Gao",
        "Francisco Goswami",
        "Philipp Guzmán",
        "Alexandre Koehn",
        "Christophe Mourachko",
        "Safiyyah Ropers",
        "Holger Saleem",
        "Jeff Schwenk",
        "Wang"
      ],
      "year": "2022",
      "venue": "No language left behind: Scaling human-centered machine translation",
      "doi": "10.48550/arXiv.2207.04672"
    },
    {
      "id": "b13",
      "title": "Instruction tuned models are quick learners",
      "authors": [
        "Himanshu Gupta",
        "Arjun Saurabh",
        "Swaroop Sawant",
        "Mutsumi Mishra",
        "Arindam Nakamura",
        "Mitra"
      ],
      "year": "2023",
      "venue": "Instruction tuned models are quick learners",
      "doi": "10.48550/ARXIV.2306.05539"
    },
    {
      "id": "b14",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "Edward J Hu",
        "Yelong Shen",
        "Phillip Wallis",
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li",
        "Shean Wang",
        "Lu Wang",
        "Weizhu Chen"
      ],
      "year": "2022",
      "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Explicit alignment objectives for multilingual bidirectional encoders",
      "authors": [
        "Junjie Hu",
        "Melvin Johnson",
        "Orhan Firat",
        "Aditya Siddhant",
        "Graham Neubig"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
      "doi": "10.18653/v1/2021.naacl-main.284"
    },
    {
      "id": "b16",
      "title": "Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks",
      "authors": [
        "Haoyang Huang",
        "Yaobo Liang",
        "Nan Duan",
        "Ming Gong",
        "Linjun Shou",
        "Daxin Jiang",
        "Ming Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/D19-1252"
    },
    {
      "id": "b17",
      "title": "Glot500: Scaling multilingual corpora and language models to 500 languages",
      "authors": [
        "Ayyoob Imani",
        "Peiqin Lin",
        "Amir Hossein Kargaran",
        "Silvia Severini",
        "Jalili Masoud",
        "Nora Sabet",
        "Chunlan Kassner",
        "Helmut Ma",
        "Schmid",
        "F T André",
        "François Martins",
        "Hinrich Yvon",
        "Schütze"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/V1/2023.ACL-LONG.61"
    },
    {
      "id": "b18",
      "title": "Glotlid: Language identification for low-resource languages",
      "authors": [
        "Ayyoob Amir Hossein Kargaran",
        "François Imani",
        "Hinrich Yvon",
        "Schütze"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": "10.18653/V1/2023.FINDINGS-EMNLP.410"
    },
    {
      "id": "b19",
      "title": "Quality at a glance: An audit of web-crawled multilingual datasets",
      "authors": [
        "Julia Kreutzer",
        "Isaac Caswell",
        "Lisa Wang",
        "Ahsan Wahab",
        "Daan Van Esch",
        "Nasanbayar Ulzii-Orshikh",
        "Allahsera Tapo",
        "Nishant Subramani",
        "Artem Sokolov",
        "Claytone Sikasote",
        "Monang Setyawan",
        "Supheakmungkol Sarin",
        "Sokhar Samb",
        "Benoît Sagot",
        "Clara Rivera",
        "Annette Rios",
        "Isabel Papadimitriou",
        "Salomey Osei",
        "Pedro",
        "Javier Ortiz Suárez",
        "Iroro Orife",
        "Kelechi Ogueji",
        "Andre Niyongabo Rubungo",
        "Toan Q Nguyen",
        "Mathias Müller",
        "André Müller",
        "Hassan Shamsuddeen",
        "Nanda Muhammad",
        "Ayanda Muhammad",
        "Jamshidbek Mnyakeni",
        "Tapiwanashe Mirzakhalov",
        "Colin Matangira",
        "Nze Leong",
        "Sneha Lawson",
        "Yacine Kudugunta",
        "Mathias Jernite",
        "Orhan Jenny",
        "Firat",
        "F P Bonaventure",
        "Sakhile Dossou",
        "Dlamini"
      ],
      "year": "2022",
      "venue": "Trans. Assoc. Comput. Linguistics",
      "doi": "10.1162/tacl_a_00447"
    },
    {
      "id": "b20",
      "title": "Word translation without parallel data",
      "authors": [
        "Guillaume Lample",
        "Alexis Conneau",
        "Marc'aurelio Ranzato",
        "Ludovic Denoyer",
        "Hervé Jégou"
      ],
      "year": "2018",
      "venue": "6th International Conference on Learning Representations, ICLR 2018",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "MLQA: evaluating cross-lingual extractive question answering",
      "authors": [
        "S H Patrick",
        "Barlas Lewis",
        "Ruty Oguz",
        "Sebastian Rinott",
        "Holger Riedel",
        "Schwenk"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
      "doi": "10.18653/v1/2020.acl-main.653"
    },
    {
      "id": "b22",
      "title": "Align after pre-train: Improving multilingual generative models with cross-lingual alignment",
      "authors": [
        "Chong Li",
        "Shaonan Wang",
        "Jiajun Zhang",
        "Chengqing Zong"
      ],
      "year": "2023",
      "venue": "Align after pre-train: Improving multilingual generative models with cross-lingual alignment",
      "doi": "10.48550/ARXIV.2311.08089"
    },
    {
      "id": "b23",
      "title": "2024a. mplm-sim: Better cross-lingual similarity and transfer in multilingual pretrained language models",
      "authors": [
        "Peiqin Lin",
        "Chengzhi Hu",
        "Zheyu Zhang",
        "F T André",
        "Hinrich Martins",
        "Schütze"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EACL 2024, St",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "2024b. Mala-500: Massive language adaptation of large language models",
      "authors": [
        "Peiqin Lin",
        "Shaoxiong Ji",
        "Jörg Tiedemann",
        "F T André",
        "Hinrich Martins",
        "Schütze"
      ],
      "year": "",
      "venue": "2024b. Mala-500: Massive language adaptation of large language models",
      "doi": "10.48550/ARXIV.2401.13303"
    },
    {
      "id": "b25",
      "title": "XAMPLER: learning to retrieve cross-lingual in-context examples",
      "authors": [
        "Peiqin Lin",
        "F T André",
        "Hinrich Martins",
        "Schütze"
      ],
      "year": "2024",
      "venue": "XAMPLER: learning to retrieve cross-lingual in-context examples",
      "doi": "10.48550/ARXIV.2405.05116"
    },
    {
      "id": "b26",
      "title": "OFA: A framework of initializing unseen subword embeddings for efficient largescale multilingual continued pretraining",
      "authors": [
        "Yihong Liu",
        "Peiqin Lin",
        "Mingyang Wang",
        "Hinrich Schütze"
      ],
      "year": "2023",
      "venue": "OFA: A framework of initializing unseen subword embeddings for efficient largescale multilingual continued pretraining",
      "doi": "10.48550/ARXIV.2311.08849"
    },
    {
      "id": "b27",
      "title": "Are emergent abilities in large language models just in-context learning?",
      "authors": [
        "Sheng Lu",
        "Irina Bigoulaeva"
      ],
      "year": "2023",
      "venue": "Are emergent abilities in large language models just in-context learning?",
      "doi": "10.48550/arXiv.2309.01809"
    },
    {
      "id": "b28",
      "title": "ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora",
      "authors": [
        "Xuan Ouyang",
        "Shuohuan Wang",
        "Chao Pang",
        "Yu Sun",
        "Hua Hao Tian",
        "Haifeng Wu",
        "Wang"
      ],
      "year": "2020",
      "venue": "ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Are pretrained multilingual models equally fair across languages?",
      "authors": [
        "Laura Cabello",
        "Piqueras",
        "Anders Søgaard"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Quality does matter: A detailed look at the quality and utility of web-mined parallel corpora",
      "authors": [
        "Surangika Ranathunga",
        "Menan Nisansa De Silva",
        "Aloka Velayuthan",
        "Charitha Fernando",
        "Rathnayake"
      ],
      "year": "2024",
      "venue": "Quality does matter: A detailed look at the quality and utility of web-mined parallel corpora",
      "doi": "10.48550/ARXIV.2402.07446"
    },
    {
      "id": "b31",
      "title": "Cometkiwi: Ist-unbabel 2022 submission for the quality estimation shared task",
      "authors": [
        "Ricardo Rei",
        "Marcos V Treviso",
        "Miguel Nuno",
        "Chrysoula Guerreiro",
        "Ana C Zerva",
        "Christine Farinha",
        "Maroti",
        "G C José",
        "Taisiya De Souza",
        "Glushkova",
        "M Duarte",
        "Luísa Alves",
        "Alon Coheur",
        "Lavie",
        "F T André",
        "Martins"
      ],
      "year": "2022",
      "venue": "Proceedings of the Seventh Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "2022a. On the role of parallel data in cross-lingual transfer learning",
      "authors": [
        "Machel Reid",
        "Mikel Artetxe"
      ],
      "year": "",
      "venue": "2022a. On the role of parallel data in cross-lingual transfer learning",
      "doi": "10.48550/arXiv.2212.10173"
    },
    {
      "id": "b33",
      "title": "PARADISE: exploiting parallel data for multilingual sequence-tosequence pretraining",
      "authors": [
        "Machel Reid",
        "Mikel Artetxe"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022",
      "doi": "10.18653/v1/2022.naacl-main.58"
    },
    {
      "id": "b34",
      "title": "",
      "authors": [
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilic",
        "Roman Hesslow",
        "Alexandra Castagné",
        "François Sasha Luccioni",
        "Matthias Yvon",
        "Jonathan Gallé",
        "Alexander M Tow",
        "Stella Rush",
        "Albert Biderman",
        "Pawan Webson",
        "Thomas Sasanka Ammanamanchi",
        "Benoît Wang",
        "Niklas Sagot",
        "Albert Muennighoff",
        "Olatunji Villanova Del Moral",
        "Rachel Ruwase",
        "Stas Bawden",
        "Angelina Bekman",
        "Iz Mcmillan-Major",
        "Huu Beltagy",
        "Lucile Nguyen",
        "Samson Saulnier",
        "Pedro Ortiz Tan",
        "Victor Suarez",
        "Hugo Sanh",
        "Yacine Laurençon",
        "Julien Jernite",
        "Margaret Launay",
        "Mitchell"
      ],
      "year": "",
      "venue": "",
      "doi": "10.48550/arXiv.2211.05100"
    },
    {
      "id": "b35",
      "title": "Aya model: An instruction finetuned open-access multilingual language model",
      "authors": [
        "Ahmet Üstün",
        "Viraat Aryabumi",
        "Xin Zheng",
        "Wei-Yin Yong",
        "Ko",
        "D' Daniel",
        "Gbemileke Souza",
        "Neel Onilude",
        "Shivalika Bhandari",
        "Hui-Lee Singh",
        "Amr Ooi",
        "Freddie Kayid",
        "Phil Vargus",
        "Shayne Blunsom",
        "Niklas Longpre",
        "Marzieh Muennighoff",
        "Julia Fadaee",
        "Sara Kreutzer",
        "Hooker"
      ],
      "year": "2024",
      "venue": "Aya model: An instruction finetuned open-access multilingual language model",
      "doi": "10.48550/ARXIV.2402.07827"
    },
    {
      "id": "b36",
      "title": "How far can camels go? exploring the state of instruction tuning on open resources",
      "authors": [
        "Yizhong Wang",
        "Hamish Ivison",
        "Pradeep Dasigi",
        "Jack Hessel",
        "Tushar Khot",
        "Raghavi Khyathi",
        "David Chandu",
        "Kelsey Wadden",
        "Noah A Macmillan",
        "Iz Smith",
        "Hannaneh Beltagy",
        "Hajishirzi"
      ],
      "year": "2023",
      "venue": "How far can camels go? exploring the state of instruction tuning on open resources",
      "doi": "10.48550/arXiv.2306.04751"
    },
    {
      "id": "b37",
      "title": "Emergent abilities of large language models",
      "authors": [
        "Jason Wei",
        "Yi Tay",
        "Rishi Bommasani",
        "Colin Raffel",
        "Barret Zoph",
        "Sebastian Borgeaud",
        "Dani Yogatama",
        "Maarten Bosma",
        "Denny Zhou",
        "Donald Metzler",
        "Ed H Chi",
        "Tatsunori Hashimoto",
        "Oriol Vinyals",
        "Percy Liang",
        "Jeff Dean",
        "William Fedus"
      ],
      "year": "2022",
      "venue": "Trans. Mach. Learn. Res",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Larger language models do in-context learning differently",
      "authors": [
        "Jerry W Wei",
        "Jason Wei",
        "Yi Tay",
        "Dustin Tran",
        "Albert Webson",
        "Yifeng Lu",
        "Xinyun Chen",
        "Hanxiao Liu",
        "Da Huang",
        "Denny Zhou",
        "Tengyu Ma"
      ],
      "year": "2023",
      "venue": "Larger language models do in-context learning differently",
      "doi": "10.48550/ARXIV.2303.03846"
    },
    {
      "id": "b39",
      "title": "On learning universal representations across languages",
      "authors": [
        "Xiangpeng Wei",
        "Rongxiang Weng",
        "Yue Hu",
        "Luxi Xing",
        "Heng Yu",
        "Weihua Luo"
      ],
      "year": "2021",
      "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "A paradigm shift in machine translation: Boosting translation performance of large language models",
      "authors": [
        "Haoran Xu",
        "Young",
        "Jin Kim",
        "Amr Sharaf",
        "Hany Hassan Awadalla"
      ],
      "year": "2023",
      "venue": "A paradigm shift in machine translation: Boosting translation performance of large language models",
      "doi": "10.48550/ARXIV.2309.11674"
    },
    {
      "id": "b41",
      "title": "Alternating language modeling for cross-lingual pre-training",
      "authors": [
        "Jian Yang",
        "Shuming Ma",
        "Dongdong Zhang",
        "Shuangzhi Wu",
        "Zhoujun Li",
        "Ming Zhou"
      ],
      "year": "2020",
      "venue": "Alternating language modeling for cross-lingual pre-training",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence",
      "authors": [],
      "year": "2020",
      "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Improving massively multilingual neural machine translation and zero-shot translation",
      "authors": [
        "Biao Zhang",
        "Philip Williams",
        "Ivan Titov",
        "Rico Sennrich"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
      "doi": "10.18653/v1/2020.acl-main.148"
    },
    {
      "id": "b44",
      "title": "LIMA: less is more for alignment",
      "authors": [
        "Chunting Zhou",
        "Pengfei Liu",
        "Puxin Xu",
        "Srini Iyer",
        "Jiao Sun",
        "Yuning Mao",
        "Xuezhe Ma",
        "Avia Efrat",
        "Ping Yu",
        "Lili Yu",
        "Susan Zhang",
        "Gargi Ghosh",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Omer Levy"
      ],
      "year": "2023",
      "venue": "LIMA: less is more for alignment",
      "doi": "10.48550/ARXIV.2305.11206"
    },
    {
      "id": "b45",
      "title": "Extrapolating large language models to non-english by aligning languages",
      "authors": [
        "Wenhao Zhu",
        "Yunzhe Lv",
        "Qingxiu Dong",
        "Fei Yuan",
        "Jingjing Xu",
        "Shujian Huang",
        "Lingpeng Kong",
        "Jiajun Chen",
        "Lei Li"
      ],
      "year": "2023",
      "venue": "Extrapolating large language models to non-english by aligning languages",
      "doi": "10.48550/arXiv.2308.04948"
    }
  ],
  "sections": [
    {
      "title": "A Recipe Of Parallel Corpora Exploitation For Multilingual Large Language Models",
      "text": "Peiqin Lin\\({}^{1,2}\\), Andre F. T. Martins\\({}^{3,4,5}\\), Hinrich Schutze\\({}^{1,2}\\) \\({}^{1}\\)Center for Information and Language Processing, LMU Munich \\({}^{2}\\)Munich Center for Machine Learning \\({}^{3}\\)Instituto Superior Tecnico, Universidade de Lisboa (Lisbon ELLIS Unit) \\({}^{4}\\)Instituto de Telecomunicacoes \\({}^{5}\\)Unbabel linpq@cis.lmu.de"
    },
    {
      "title": "Abstract",
      "text": "Recent studies have highlighted the potential of exploiting parallel corpora to enhance multilingual large language models in both bilingual tasks, e.g., machine translation, and general-purpose tasks, e.g., text classification. Building upon these findings, our comprehensive study aims to identify the most effective strategies for leveraging parallel corpora. We investigate the impact of parallel corpus quality and quantity, training objectives, and model size on the performance of multilingual large language models enhanced with parallel corpora across diverse languages and tasks. Our analysis reveals several key insights: (i) filtering noisy translations is essential for exploiting parallel corpora, while language identification and short sentence filtering have little effect; (ii) even a corpus with just 10K parallel sentences can yield results comparable to those obtained from larger datasets; (iii) employing only the machine translation objective yields the best results among various training objectives and their combinations; (iv) larger multilingual language models benefit more from parallel corpora than smaller models. Our study offers valuable insights into the optimal utilization of parallel corpora to enhance multilingual large language models, extending the generalizability of previous findings from limited languages and tasks to a broader range of scenarios."
    },
    {
      "title": "1 Introduction",
      "text": "Recent multilingual large language models (mLLMs), represented by BLOOM (Scao et al., 2022), MaLA500 (Lin et al., 2024), and Aya (Ustun et al., 2024), have shown impressive capacity on diverse tasks across languages. Parallel corpora have emerged as crucial resources for enhancing mLLMs, both for specific tasks, e.g., machine translation (Xu et al., 2023; Alves et al., 2024), and for general-purpose tasks (Cahyawijaya et al., 2023; Zhu et al., 2023; Li et al., 2023). However, existing studies often lack in comprehensive exploration of methodologies for harnessing parallel corpora. The quality and quantity of parallel corpora remain inadequately explored, inhibiting the full potential of such resources. Moreover, the influence of different training objectives and mLLM sizes across diverse languages and tasks remains under-investigated. This limitation impedes the generalization of parallel corpus exploitation methods across varied linguistic landscapes and task domains. Therefore, this paper aims to address these gaps by presenting a comprehensive recipe for exploiting parallel corpora for mLLMs. We focus on four key factors, with some main results shown in Figure 1. **Quality:** We explore three dimensions of paral Figure 1: Average performance improvements (y-axis) achieved by mLLMs enhanced with parallel corpora compared to their base models. **Best: Instruction tuning of BLOOM-7B1 with the machine translation objective (MT) using 10K high-quality (i.e., filtered) parallel sentences yields the best results**. Main variations explored include: **Filter (No)** (using the original data); **OBJ (TLM)** (translation language modeling objective); **OBJ (XSS)** (cross-lingual semantic similarity objective); **[Datal (50K)** (a larger 50K-sentence dataset); **[Model (1B7)** (BLOOM-1B7 model). lel corpus quality: translation accuracy, sentence length, and language identification. Our results show that _translation quality_ is vital for exploiting parallel corpora, while sentence length filtering and language identification have minimal impact. **Quantity:** Acquiring large amounts of high-quality parallel corpora is challenging, especially for relatively low-resource languages. Our study examines the minimum corpus size necessary to achieve performance improvements across diverse tasks. We find that even a corpus of just _10K sentences_ can yield results comparable to those obtained from much larger datasets. **Objective:** Previous studies (Cahyawijaya et al., 2023) have investigated the effectiveness of different training objectives and their combinations on classification tasks of Indonesian local languages, using smaller-sized mLLMs up to 1B7 parameters. We extend this investigation by examining the impact of various training objectives and their combinations on larger mLLMs across a range of languages and tasks. Our experiments demonstrate that employing _the machine translation objective_ produces the most promising results. **Model Size:** The size of mLLMs can greatly impact their ability to comprehend instructions derived from parallel corpora. Our findings indicate that _larger mLLMs_ exhibit superior comprehension and cross-task transferability compared to their smaller counterparts. Consequently, they achieve more substantial improvements across a broader spectrum of tasks. In light of the critical role parallel corpora play in mLLMs, our study provides a comprehensive recipe for effectively exploiting parallel corpora. We have identified four primary factors: quality (SS4), quantity (SS5), objective (SS6), and model size (SS7). Our detailed analysis of these factors reveals their great impact on mLLM performance across diverse languages and tasks. By delving into these aspects, we offer actionable insights that can inform the development and optimization of strategies for parallel corpus exploitation, ultimately contributing to the advancement of mLLMs in both bilingual and general-purpose tasks."
    },
    {
      "title": "2 Related Work",
      "text": ""
    },
    {
      "title": "Parallel Data For Multilingual Language Models",
      "text": "Over the years, multilingual language models have evolved from earlier, smaller models, such as XLM (Conneau and Lample, 2019), XLM-R (Conneau et al., 2020), and Glot500 (Imani et al., 2023), to more recent, larger models, including BLOOM (Scao et al., 2022), MaLA500 (Lin et al., 2024), and Aya (Ustun et al., 2024). These models consistently demonstrate strong performance across various downstream tasks (Ahuja et al., 2023; Lin et al., 2024). Parallel corpora have played a pivotal role in both the analysis (Piqueras and Sogaard, 2022; Lin et al., 2024) and enhancement (Conneau and Lample, 2019; Ouyang et al., 2020; Yang et al., 2020; Huang et al., 2019; Chi et al., 2021; Wei et al., 2021; Hu et al., 2021; Chi et al., 2021; Reid and Artetxe, 2022; Liu et al., 2023) of small multilingual language models. In the era of mLLMs, parallel corpora are constructed as instruction data and used to enhance mLLMs through supervised fine-tuning (Cahyawijaya et al., 2023; Zhu et al., 2023; Li et al., 2023). Specifically, Cahyawijaya et al. (2023) propose three methods of incorporating parallel corpora as instruction tuning data: Machine Translation (MT), Translation Language Modeling (TLM), and Cross-Lingual Semantic Similarity (XSS) (see SS3.3). However, their evaluation is limited to small models with up to 1.7 billion parameters and focuses solely on classification tasks within Indonesian local languages. Both Zhu et al. (2023) and Li et al. (2023) propose using machine-translation-style instruction data to improve mLLMs but do not explore different training objectives. While these studies yield promising results, their scope is limited. Firstly, they do not explore critical factors such as the quality and quantity of parallel corpora, considering the high cost of collecting high-quality and massive parallel corpora, especially for relatively low-resource languages. Secondly, their investigations do not encompass an in-depth analysis of training objectives and mLLMs with varied model sizes across diverse languages and tasks."
    },
    {
      "title": "Key Elements For Language Modeling",
      "text": "Previous research has extensively examined critical factors essential for the pretraining and enhancement of language models. **Quality:** Kreutzer et al. (2022) conducted manual audits of prevalent monolingual and parallel corpora, revealing significant portions of low-quality data, particularly in corpora for relatively low-resource languages. Follow-up studies have investigated the impact of data quality on model per formance. Artetxe et al. (2022) observed that similar results on downstream tasks can be achieved regardless of the degree of quality of the corpus used for pretraining, while other studies found that the quality of parallel corpora matters for machine translation (Ranathunga et al., 2024) and general-purpose tasks (Reid and Artetxe, 2022). **Quantity:** Recent works (Chen et al., 2023; Zhou et al., 2023; Gupta et al., 2023) have focused on the impact of fine-tuning with small amounts of high-quality instruction data, such as one or a few thousand instances, showing promising performance gains in evaluation tasks. Xu et al. (2023) demonstrate that as few as 10K high-quality parallel sentences can significantly enhance machine translation performance. **Objective:** Different training objectives based on parallel corpora for enhancing mLLMs can be viewed as distinct instructions. Wang et al. (2023) explore the impact of various types of instruction tuning data and find that their combination can be optimal in certain scenarios. **Model Size:** Recent studies indicate that scaling up language models enhances their capability to excel in diverse and complex reasoning tasks (Wei et al., 2022, 2023; Lu et al., 2023). Follow-up studies (Wei et al., 2023) further illustrate distinct behavioral differences between larger and smaller models. However, these factors have not yet been comprehensively explored in the context of leveraging parallel corpora to enhance mLLMs across diverse languages and tasks."
    },
    {
      "title": "3 Setup",
      "text": ""
    },
    {
      "title": "Language",
      "text": "We use three criteria for language selection. Firstly, we select languages well covered by mLLMs as our goal is to assess how parallel data enhances performance after pre-training an mLLM with monolingual data. Secondly, the selected languages should be also covered by several different evaluation benchmarks, allowing for robust evaluation across diverse downstream tasks. Lastly, we select typologically diverse languages, enabling our investigation to generalize to a wide range of relatively low-resource languages. Thus, we select five languages: Arabic (ar), Spanish (es), Hindi (hi), Vietnamese (vi) and Chinese (zh)."
    },
    {
      "title": "Data",
      "text": "We utilize the OPUS100 dataset (Zhang et al., 2020), an English-centric multilingual corpus, to gather parallel sentences between English (en) and each target language. The quality of OPUS100 is assessed across three dimensions: Translation QualityManual quality assessment of the vast amount of parallel corpora is impractical. Instead, we employ COMETKWII (Rei et al., 2022)1, a tool for estimating the quality of machine translation outputs across multiple languages. We set a COMETKWII score threshold \\(\\tau_{c}\\), retaining parallel corpora with scores not lower than \\(\\tau_{c}\\). Footnote 1: [https://huggingface.co/Unbabel/wmt23-cometkiwi-da-xxl](https://huggingface.co/Unbabel/wmt23-cometkiwi-da-xxl) Sentence LengthGiven the variation in character length across languages, we avoid using it as a metric for consistency. Instead, we measure sentence length by the number of tokens, as determined by the tokenizer of our chosen mLLM, BLOOM-7B1. We establish a length threshold \\(\\tau_{l}\\), retaining parallel corpora where both source and target sentences contain no fewer than \\(\\tau_{l}\\) tokens. Language IdentificationTo identify sentences potentially not in the correct language, we employ GlotLID (Kargaran et al., 2023), an open-source language identification model. This language identification filter is applied to both the source and target sentences."
    },
    {
      "title": "Training",
      "text": "We select the BLOOM series (Scao et al., 2022) for our investigation due to its offering of different sizes of mLLMs which well cover the five target languages under consideration.2 The pretraining data size for the five target languages ranges from 23GB (Hindi) to 452GB (English). We explore BLOOM models of various sizes, including 7B1, 3B, and 1B7. Due to limited computational resources, we use LoRA (Hu et al., 2022), which is known for its competitive performance compared to full-parameter training (Alves et al., 2023), for instruction tuning of BLOOM. We configure the learning rate to \\(1\\times 10^{-4}\\), weight decay to \\(0.1\\), and set the rank of LoRA to \\(16\\) based on preliminary experiment in SSB. The maximum sequence length for both source and target sentences is set to \\(128\\). To maintain consistency across experiments with [MISSING_PAGE_FAIL:4] proximately 10% of the data has COMETKIWI scores below 0.25, indicating very poor translation quality. Additionally, between 10% to 30% of the data falls within the 0.25-0.5 score range, which is still considered sub-optimal. Regarding sentence length, we find that over 20% of the OPUS100 data consists of very short sentences, with a length of no more than five tokens. For language identification, 13% to 25% of the data is removed due to incorrect language identification results in one of the two parallel sentences. **Relatively low-resource languages suffer more from low-quality issues**. For relatively low-resource languages like Hindi, there are fewer high-quality parallel sentences compared to high-resource languages such as Spanish. Analysis of translation quality indicates that the English-Hindi pair has less than 20% of parallel sentences with high COMETWIKI scores (0.75-1), whereas the English-Spanish pair has around 45%. For sentence length, the English-Hindi pair contains 10% more short sentences (0-5 tokens) compared to high-resource language pairs. Moreover, both the English-Arabic and English-Hindi pairs exhibit about 10% more parallel sentences that may be in the wrong languages. These comprehensive findings underscore the critical importance of data quality when exploiting parallel corpora for mLLM training."
    },
    {
      "title": "Effect Of Quality",
      "text": "Table 3 presents the performance of BLOOM-7B1 after instruction tuning with the machine translation objective, using 10K parallel corpora with various quality filtering strategies. **Parallel corpora containing noisy translations still improve results.** Comparing the results of the experiment with \\(\\tau_{c}=0\\) (ID 1) to the original model (ID 0), there's an average improvement of 0.4% for all tasks. The most notable improvements are observed in both bilingual tasks (en-xx) and in-language tasks. However, generating English for bilingual tasks yields degraded or marginally improved results. Experiment 1 exhibits 0.7% and 1.1% decrements in FLORES and MUSE respectively, with only a 0.3% improvement in MLQA. **Filtering out noisy translations leads to notable improvements.** When \\(\\tau_{c}=0.5\\), the average performance rises from 53.2% to 53.7%. Further refinement to \\(\\tau_{c}=0.75\\) achieves an additional 0.3% improvement. These improvements are consistently observed across all evaluated tasks. In the optimal setting (ID 5), there's a 1.2% improvement compared to BLOOM-7B1 (ID 0). The improvements corroborate the reliability of COMETKIWI as a metric for filtering low-quality translations. **Filtering short sentences yields slightly worse results than using unfiltered data.** The experi Figure 4: Percentage of sentences retained after language identification filtering of 500K parallel sentences from OPUS100 for our five language pairs. Figure 3: Sentence length of 500K parallel sentences from OPUS100 for our five language pairs. The three categories are 0-5, 5-10, greater than 10 tokens. Figure 2: Translation quality measured by COMETWIKI of 500K parallel sentences from OPUS100 for our five language pairs. The COMETWIKI scores are segmented into four ranges: 0-0.25, 0.25-0.5, 0.5-0.75, and 0.75-1. Higher scores represent better translation quality. ment with filtering short sentences (ID 3) achieves comparable or slightly worse results compared to that without filtering short sentences (ID 5). This suggests that short sentences, whether at the word or phrase level, may offer some benefits for sentence-level tasks. **Using data with language identification filtering results in only a 0.1% improvement on average.** A comparison of experimental outcomes with and without language identification filtering (ID 4 and 5) reveals that using data with language identification filtering yields merely a 0.1% improvement on average. The most notable performance difference is observed in the MUSE task, where using data with language identification filtering leads to improvements of 0.6% (en-xx) and 0.2% (xx-en). This marginal enhancement may be attributed to the presence of sentences in similar languages within OPUS100, which exhibit minor linguistic variations compared to the true language. These variations could potentially have a slight negative impact on word-level translations while having little impact on sentence-level tasks."
    },
    {
      "title": "5 Quantity",
      "text": ""
    },
    {
      "title": "Effect Of Quantity Across Tasks",
      "text": "Based on Table 4, which shows the performance of BLOOM-7B1 after instruction tuning with the machine translation objective using different amounts of parallel sentences, we can derive the following key findings: **Adding merely 1K parallel sentences helps.** Exploiting 1K parallel sentences for instruction tuning improves the overall average score by 1%. This increase is observed across most tasks, with notable improvements in FLORES (en-xx), MUSE (en-xx), and SIB. **Using 10K parallel sentences leads to the optimal performance.** The best overall performance is achieved with 10K parallel sentences, resulting in an average score of 54.0%. This setting yields the highest scores in MUSE and SIB. This aligns with the findings in Xu et al. (2023). **More data achieves comparable results.** Increasing the number of parallel sentences beyond 10K results in comparable performance. Specifically, using 25K or 50K parallel sentences yields average scores of 53.9%, which are very close to the score obtained with 10K sentences. The analysis suggests that instruction tuning with a moderate amount of parallel sentences (around 10K) yields the best overall improvement in performance for the BLOOM-7B1 model across various tasks."
    },
    {
      "title": "Effect Of Quantity Across Languages",
      "text": "We delve deeper into the influence of parallel corpora quantity, as depicted in Table 5. **Using 10K parallel sentences achieves optimal performance across most languages.** For the majority of languages, except Vietnamese (vi) and Chinese (zh), the highest performance is obtained with 10K parallel sentences. Even for Vietnamese and Chinese, leveraging 10K parallel sentences can yield comparable results. These observations align with the findings in SS5.1. **Different languages exhibit varying appetites for parallel corpora.** Across most languages, increasing the number of parallel sentences used for instruction tuning generally leads to incremental improvements in performance. However, for Hindi (hi) and Chinese (zh), transitioning from 1K to 10K parallel sentences does not yield improvement. \\begin{table} \\begin{tabular}{c|c c c|c c c c c c c c} \\hline \\hline \\multirow{2}{*}{ID} & \\multirow{2}{*}{Model} & \\multicolumn{3}{c}{FLORES} & \\multicolumn{3}{c}{MUSE} & \\multicolumn{3}{c}{MLQA} & \\multirow{2}{*}{XQuAD} & \\multirow{2}{*}{SIB} & \\multirow{2}{*}{AVG} \\\\ \\cline{3-3} \\cline{6-11} & & & en-xx & xx-en & & en-xx & xx-en & & en-xx & xx-en & \\\\ \\hline 0 & BLOOM-7B1 & 69.1 & **72.4** & 43.1 & 53.7 & 36.4 & 42.7 & 47.2 & 58.1 & 52.8 \\\\ \\hline & \\(\\tau_{c}\\) & \\(\\tau_{l}\\) & LID & & & & & & & & \\\\ \\hline 1 & 0 & 0 & ✓ & 69.7 & 71.7 & 44.4 & 52.6 & 37.8 & 43.0 & 47.7 & 58.8 & 53.2 \\\\ 2 & 0.5 & 0 & ✓ & 69.9 & 72.1 & 45.0 & 53.0 & **38.1** & 43.7 & 48.1 & **59.8** & 53.7 \\\\ 3 & 0.75 & 5 & ✓ & 70.3 & 72.1 & **45.7** & 53.6 & **38.1** & 43.5 & 47.8 & 59.2 & 53.8 \\\\ 4 & 0.75 & 0 & ✗ & **70.5** & 72.1 & 44.9 & 53.7 & 37.7 & **44.0** & **48.3** & 59.6 & 53.9 \\\\ 5 & 0.75 & 0 & ✓ & 70.3 & 72.3 & 45.5 & **53.9** & 38.0 & 43.9 & **48.3** & 59.5 & **54.0** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Performance (%) of BLOOM-7B1 after instruction tuning with the machine translation objective using 10K parallel corpora with various quality filtering strategies. Parameters include \\(\\tau_{c}\\) for COMETWIKI score threshold, \\(\\tau_{l}\\) for sentence length threshold, and LID indicating the adoption of language identification filtering. This phenomenon may be attributed to BLOOM-7B1's limited proficiency in these languages compared to others, as reflected in the results of the original BLOOM-7B1 model (|Sent|=0)."
    },
    {
      "title": "6 Objective",
      "text": "We explore the effectiveness of different objectives and their combinations, with results in Table 6. **BLOOM-7B1 performs well on English generation tasks.** The baseline BLOOM-7B1 model exhibits robust performance across a spectrum of evaluation tasks, notably excelling in English generation tasks such as FLORES (xx-en) and MUSE (xx-en). Further exploitation of parallel corpora fails to yield any discernible improvement. **MT emerges as the top performer.** The MT objective consistently outperforms the baseline BLOOM-7B1 model, showcasing an average improvement of 1.2%. Moreover, MT achieves the highest performance in 5 out of 8 evaluated tasks. **TLM exhibits limited effectiveness.** While TLM shows slight improvements on average (0.2%), primarily driven by enhancements in tasks like MUSE (en-xx), MLQA (xx-en), XQuAD, and SIB, it also leads to degradation in tasks including FLORES and MUSE (xx-en). **XSS achieves strong performance for classification.** Using the XSS objective improves BLOOM-7B1 by 0.7%, though it performs 0.5% worse than MT. The major decrease is observed in translation tasks, especially from English to other languages. However, XSS can still slightly improve translation tasks compared to BLOOM-7B1. Notably, XSS achieves 0.3% better performance on SIB, highlighting its effectiveness for classification. **Combining training objectives does not provide large benefits.** While combinations of different objectives can improve BLOOM-7B1 by 0.2% to 1.0%, none surpass the performance of using the MT objective alone. The combination of MT and XSS is the best among the combinations, slightly worse than MT by 0.2%, but better than all other objectives. Notably, MT +XSS achieves the best results on SIB, and TLM +XSS yields the best results on MLQA (xx-en). These observations indicate that no single objective excels across all tasks."
    },
    {
      "title": "7 Model Size",
      "text": "We explore the impact of parallel corpora on various sizes of BLOOM models, detailed in Table 7. **Smaller models exhibit more pronounced improvements in FLORES.** Notably, BLOOM-1B7 demonstrates larger improvements compared to its larger counterparts in the FLORES task, where the prompt is the same as the one used during instruction tuning with the MT objective. This is attributed \\begin{table} \\begin{tabular}{c|c c c c c c c c} \\hline \\hline \\multirow{2}{*}{|Sent|} & \\multicolumn{2}{c}{FLORES} & \\multicolumn{2}{c}{MUSE} & \\multicolumn{2}{c}{MLQA} & \\multirow{2}{*}{XQuAD} & \\multirow{2}{*}{SIB} & \\multirow{2}{*}{AVG} \\\\ & en-xx & xx-en & en-xx & xx-en & en-xx & xx-en & & \\\\ \\hline 0 & 69.1 & **72.4** & 43.1 & 53.7 & 36.4 & 42.7 & 47.2 & 58.1 & 52.8 \\\\ 1K & 70.0 & 72.2 & 45.3 & 53.6 & **38.2** & 43.6 & 47.9 & 59.2 & 53.8 \\\\ 5K & 70.3 & 72.2 & 45.4 & 53.5 & **38.2** & 43.8 & 48.2 & **59.5** & 53.9 \\\\ 10K & 70.3 & 72.3 & **45.5** & **53.9** & 38.0 & 43.9 & 48.3 & **59.5** & **54.0** \\\\ 25K & 70.3 & 72.2 & 45.1 & 53.8 & 38.0 & **44.0** & **48.4** & **59.5** & 53.9 \\\\ 50K & **70.4** & 72.2 & 45.1 & 53.8 & 38.1 & 43.7 & 48.3 & **59.5** & 53.9 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Task performance (%) of BLOOM-7B1 after instruction tuning with the machine translation objective using varying amounts of parallel sentences, obtained with the best filtering strategy (ID 5) as shown in Table 3. |Sent| indicates the number of parallel sentences used for instruction tuning, with |Sent|=0 (left) and BLOOM-7B1 model. \\begin{table} \\begin{tabular}{c|c c c c c} \\hline \\hline & ar & es & hi & vi & zh \\\\ \\hline 0 & 49.5 & 57.7 & 46.5 & 63.8 & 46.7 \\\\ 1K & 50.8 & 58.1 & **47.7** & 64.3 & 47.8 \\\\ 5K & 51.2 & 58.2 & 47.6 & 64.6 & **47.9** \\\\ 10K & **51.3** & **58.4** & **47.7** & 64.6 & 47.8 \\\\ 25K & 51.2 & 58.2 & **47.7** & **64.7** & 47.7 \\\\ 50K & 51.2 & 58.2 & **47.7** & **64.7** & 47.6 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: Language performance (%) of BLOOM-7B1 after instruction tuning with the machine translation objective using varying amounts of parallel sentences, obtained with the best filtering strategy (ID 5) as shown in Table 3. |Sent| indicates the number of parallel sentences used for instruction tuning, with |Sent|=0 representing the original BLOOM-7B1 model. to the smaller models' less developed in-context learning capabilities before instruction tuning, allowing for more substantial improvements when supplemented with parallel corpora. **Larger models excel in diverse tasks.** Conversely, larger models generally demonstrate greater enhancements in tasks beyond machine translation. Both BLOOM-7B1 and BLOOM-3B exhibit a 1.2% improvement compared to their original mLLMs, while BLOOM-1B7 shows a slight 0.3% improvement. Specifically, BLOOM-7B1 and BLOOM-3B display notable improvements in tasks except for FLORES, while BLOOM-1B7 achieves comparable or even worse results. These findings demonstrate that when leveraging parallel corpora to enhance mLLMs, larger models not only exhibit improvements in direct tasks, such as machine translation, but also demonstrate a more substantial overall enhancement across a variety of tasks. In contrast, smaller models primarily show benefits in direct tasks. This difference can be attributed to the superior cross-task transferability of larger mLLMs, where insights gained from parallel corpora in one task contribute to improved performance in others."
    },
    {
      "title": "8 Conclusion",
      "text": "This paper investigates the impact of four critical factors - data quality, data quantity, objectives, and mLLM sizes - on leveraging parallel corpora to enhance mLLMs across diverse languages and tasks. Our findings underscore the crucial importance of filtering out noisy translations to procure high-quality training data for improving mLLMs. Surprisingly, even a relatively modest dataset of 10K samples can yield promising results. Furthermore, our analysis shows that employing the machine translation objective leads to optimal outcomes. Importantly, larger models exhibit a greater capacity to benefit from parallel corpora, achieving more substantial improvements. This study provides a comprehensive recipe for effectively lever \\begin{table} \\begin{tabular}{c|c c c c c c c c} \\hline \\hline Model & \\multicolumn{2}{c}{FLORES} & \\multicolumn{2}{c}{MUSE} & \\multicolumn{2}{c}{MLQA} & \\multirow{2}{*}{XQuAD} & \\multirow{2}{*}{SIB} & \\multirow{2}{*}{AVG} \\\\ & en-xx & xx-en & en-xx & xx-en & en-xx & xx-en & & \\\\ \\hline BLOOM-7B1 & 69.1 & **72.4** & 43.1 & 53.7 & 36.4 & 42.7 & 47.2 & 58.1 & 52.8 \\\\ \\hline MT & **70.3** & 72.3 & **45.5** & **53.9** & **38.0** & 43.9 & **48.3** & 59.5 & **54.0** \\\\ TLM & 67.2 & 72.2 & 44.3 & 53.0 & 36.3 & 44.4 & 47.6 & 58.7 & 53.0 \\\\ XSS & 69.4 & 72.2 & 43.7 & 53.5 & 37.0 & 44.2 & **48.3** & 60.0 & 53.5 \\\\ MT +TLM & 69.3 & 72.1 & 44.1 & 53.2 & 36.8 & 43.8 & 47.2 & 59.5 & 53.2 \\\\ MT +XSS & 70.3 & 72.1 & 44.9 & 53.3 & 37.4 & 44.5 & 47.9 & **60.4** & 53.8 \\\\ TLM +XSS & 67.7 & 72.2 & 43.0 & 52.5 & 34.9 & **45.6** & 48.2 & 60.0 & 53.0 \\\\ MT +TLM +XSS & 69.5 & 72.1 & 44.2 & 53.2 & 36.1 & 45.1 & 47.7 & 59.0 & 53.4 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: Performance (%) of BLOOM-7B1 after instruction tuning with different objectives and their combinations using 10K parallel corpora, obtained with the best filtering strategy (ID 5) as shown in Table 3. \\begin{table} \\begin{tabular}{c|c c c c c c c c c} \\hline \\hline Model & \\multicolumn{2}{c}{FLORES} & \\multicolumn{2}{c}{MUSE} & \\multicolumn{2}{c}{MLQA} & \\multirow{2}{*}{XQUAD} & \\multirow{2}{*}{SIB} & \\multirow{2}{*}{AVG} \\\\ & en-xx & xx-en & en-xx & xx-en & en-xx & xx-en & & \\\\ \\hline BLOOM-7B1 & 69.1 & 72.4 & 43.1 & 53.7 & 36.4 & 42.7 & 47.2 & 58.1 & 52.8 \\\\ + Parallel Data & 70.3 & 72.3 & 45.5 & 53.9 & 38.0 & 43.9 & 48.3 & 59.5 & 54.0 \\\\ \\(\\Delta\\) & 01.2 & -00.1 & 02.4 & 00.2 & 01.6 & 01.2 & 01.0 & 01.4 & 01.2 \\\\ \\hline BLOOM-3B & 64.0 & 68.9 & 39.7 & 50.9 & 29.4 & 26.2 & 32.7 & 54.5 & 45.8 \\\\ + Parallel Data & 65.0 & 69.1 & 41.4 & 51.6 & 30.9 & 26.7 & 34.5 & 56.9 & 47.0 \\\\ \\(\\Delta\\) & 01.0 & 00.2 & 01.8 & 00.7 & 01.5 & 00.5 & 01.8 & 02.4 & 01.2 \\\\ \\hline BLOOM-1B7 & 59.0 & 65.8 & 37.2 & 48.5 & 20.0 & 22.2 & 24.8 & 53.0 & 41.3 \\\\ + Parallel Data & 61.1 & 65.7 & 38.9 & 48.0 & 20.8 & 20.9 & 24.4 & 53.0 & 41.6 \\\\ \\(\\Delta\\) & 02.0 & -00.1 & 01.6 & -00.6 & 00.8 & -01.3 & -00.3 & 00.0 & 00.3 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 7: Effect of parallel corpora on BLOOM models of different sizes across various tasks. ‘+ Parallel Data’ indicates instruction tuning of the given mLLM with the MT objective, using 10K parallel corpora obtained with the best filtering strategy (ID 5) as shown in Table 3. aging parallel corpora to enhance mLLMs. These insights significantly contribute to advancing the understanding and optimization of mLLMs across different languages and tasks."
    },
    {
      "title": "Limitations",
      "text": "Due to limited computational resources, we opted not to explore full-parameter training for leveraging parallel corpora. Instead, we focused on LoRA, drawing on insights from previous studies. Additionally, our investigation is restricted to the BLOOM series, and we did not extend our analysis to other mLLMs. Furthermore, we did not also explore mLLMs larger than 7B1."
    },
    {
      "title": "Acknowledgements",
      "text": "This work was funded by DFG (SCHU 2246/14-1), the European Research Council (DECOLLAGE, ERC-2022-CoG #101088763), EU's Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (Center for Responsible AI), by the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research, and by FCT/MECI through national funds and when applicable co-funded EU funds under UID/50008: Instituto de Telecomunicacoes."
    },
    {
      "title": "References",
      "text": "* Adelani et al. (2023) David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba O. Alabi, Yanke Mao, Haonan Gao, and En-Shiun Annie Lee. 2023. SIB-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. _CoRR_, abs/2309.07445. * Ahuja et al. (2023) Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, and Sunayana Sitaram. 2023. MEGA: multilingual evaluation of generative AI. _CoRR_, abs/2303.12528. * Alves et al. (2023) Duarte M. Alves, Nuno Miguel Guerreiro, Joao Alves, Jose Pombal, Ricardo Rei, Jose Guilherme Camargo de Souza, Pierre Colombo, and Andre F. T. Martins. 2023. Steering large language models for machine translation with finetuning and in-context learning. In _Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023_, pages 11127-11148. Association for Computational Linguistics. * Alves et al. (2024) Duarte M. Alves, Jose Pombal, Nuno M. Guerreiro, Pedro Henrique Martins, Joao Alves, M. Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, Jose G. C. de Souza, and Andre F. T. Martins. 2024. Tower: An open multilingual large language model for translation-related tasks. _CoRR_, abs/2402.17733. * Artetxe et al. (2022) Mikel Artetxe, Itziar Aldabe, Rodrigo Agerri, Olatz Perez-de-Vinaspre, and Aitor Soroa. 2022. Does corpus quality really matter for low-resource languages? In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11,_, pages 7383-7390. Association for Computational Linguistics. * Artetxe et al. (2020) Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, pages 4623-4637. Association for Computational Linguistics. * Cahyawijaya et al. (2023) Samuel Cahyawijaya, Holy Lovenia, Tiezheng Yu, Willy Chung, and Pascale Fung. 2023. Instructualign: Teaching novel languages with to llms through alignment-based cross-lingual instruction. _CoRR_, abs/2305.13627. * Chen et al. (2023) Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo Zhao. 2023. Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning. _CoRR_, abs/2305.09246. * Chi et al. (2021a) Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. 2021a. Infoxlm: An information-theoretic framework for cross-lingual language model pre-training. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021_, pages 3576-3588. Association for Computational Linguistics. * Chi et al. (2021b) Zewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-Ling Mao, Heyan Huang, and Furu Wei. 2021b. Improving pretrained cross-lingual language models via self-labeled word alignment. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021_, pages 3418-3430. Association for Computational Linguistics. * Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 8440-8451. External Links: Link, Document Cited by: SS2. * A. Conneau and G. Lample (2019)Cross-lingual language model pretraining. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 7057-7067. External Links: Link, Document Cited by: SS2. * M. R. Costa-jussa, J. Cross, O. Celebi, M. Elbayad, K. Heafield, K. Heffeman, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. Mejia Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. Ram Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. Fazil Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzman, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang (2022)No language left behind: scaling human-centered machine translation. CoRRabs/2207.04672. External Links: Link, 2007.04672 Cited by: SS2. * H. Gupta, S. Arjun Sawant, S. Mishra, M. Nakamura, A. Mitra, S. Mashetty, and C. Baral (2023)Instruction tuned models are quick learners. CoRRabs/2306.05539. External Links: Link, 2306.05539 Cited by: SS2. * E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen (2022)Lora: low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, External Links: Link, 2002.0226 Cited by: SS2. * J. Hu, M. Johnson, O. Firat, A. Siddhant, and G. Neubig (2021)Explicit alignment objectives for multilingual bidirectional encoders. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 3633-3643. External Links: Link, 2102.0226 Cited by: SS2. * H. Huang, Y. Liang, N. Duan, M. Gong, L. Shou, D. Jiang, and M. Zhou (2019)Unicoder: a universal language encoder by pretraining with multiple cross-lingual tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 2485-2494. External Links: Link, 1905.05539 Cited by: SS2. * A. Imani, P. Lin, A. Hossein Kargaran, S. Severini, M. Jalili Sabet, N. Kassner, C. Ma, H. Schmid, A. F. T. Martins, F. Yvon, and H. Schutze (2023)Glot500: scaling multilingual corpora and language models to 500 languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), London, Canada, July 9-14, 2023, Toronto, Canada, July 9-14, 2023, pp. 1082-1117. External Links: Link, Document Cited by: SS2. * A. Kargaran, A. Imani, F. Yvon, and H. Schutze (2023)Glot1d: language identification for low-resource languages. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 6155-6218. External Links: Link, Document Cited by: SS2. * J. Kreutzer, I. Caswell, L. Wang, A. Wahab, D. van Esch, N. Ulzii-Orshikh, A. Tapo, N. Subramani, A. Sokolov, C. Sikasote, M. Setyawan, S. Sarin, S. Sagnt, C. Rivera, A. Rios, I. Papadimitriou, S. Osei, P. Javier Ortiz Suarez, I. Orife, K. Ogueji, A. Niyongabo Rubungo, T. Q. Nguyen, M. Muller, A. Muller, S. Hassan Muhammad, N. Muhammad, A. Mnyakeni, J. Mirzakhalov, T. Matangira, C. Leong, N. Lawson, S. Kudugunta, Y. Jernite, M. Jenny, O. Firat, B. F. P. Dossou, S. Dlamini, N. de Silva, S. Cabuk Balli, S. Biderman, A. Battisti, A. Barawa, A. Bapna, P. Baljiekar, I. A. Azime, A. Awokoya, D. Ataman, O. Ahia, O. Ahia, S. Aagrawal, and M. Adeyemi (2022)Quality at a glance: an audit of web-crawled multilingual datasets. Trans. Assoc. Comput. Linguistics10, pp. 50-72. External Links: Link, 2204.04675 Cited by: SS2. * May 3, 2018, Conference Track Proceedings, External Links: Link, Document Cited by: SS2. * P. S. H. Lewis, B. Oguz, R. Rinott, S. Riedel, and H. Schwenk (2020)MLQA: evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 7315-7330. External Links: Link, Document Cited by: SS2. * C. Li, S. Wang, J. Zhang, and C. Zong (2023)Align after pre-train: improving multilingual generative models with cross-lingual alignment. CoRRabs/2311.08089. External Links: Link, 2311.08089 Cited by: SS2. * P. Lin, C. Hu, Z. Zhang, A. F. T. Martins, and H. Schutze (2024)mplm-sim: better cross-lingual similarity and transfer in multilingual pretrained language models. In Findings of the Association for Computational Linguistics: EACL 2024, St. Julian's, Malta, March 17-22, 2024_, pages 276-310. Association for Computational Linguistics. * Lin et al. (2024b) Peiqin Lin, Shaoxiong Ji, Jorg Tiedemann, Andre F. T. Martins, and Hinrich Schutze. 2024b. Mala-500: Massive language adaptation of large language models. _CoRR_, abs/2401.13303. * Lin et al. (2024c) Peiqin Lin, Andre F. T. Martins, and Hinrich Schutze. 2024c. XAMPLER: learning to retrieve cross-lingual in-context examples. _CoRR_, abs/2405.05116. * Liu et al. (2023) Yihong Liu, Peiqin Lin, Mingyang Wang, and Hinrich Schutze. 2023. OFA: A framework of initializing unseen subword embeddings for efficient large-scale multilingual continued pretraining. _CoRR_, abs/2311.08849. * Lu et al. (2023) Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. 2023. Are emergent abilities in large language models just in-context learning? _CoRR_, abs/2309.01809. * Ouyang et al. (2020) Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2020. ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora. _CoRR_, abs/2012.15674. * Piqueras and Sogaard (2022) Laura Cabello Piqueras and Anders Sogaard. 2022. Are pretrained multilingual models equally fair across languages? In _Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022_, pages 3597-3605. International Committee on Computational Linguistics. * Ranathunga et al. (2024) Surangika Ranathunga, Nisansa de Silva, Menan Velayuthan, Aloka Fernando, and Charitha Rathnayake. 2024. Quality does matter: A detailed look at the quality and utility of web-mined parallel corpora. _CoRR_, abs/2402.07446. * Rei et al. (2022) Ricardo Rei, Marcos V. Treviso, Nuno Miguel Guerreiro, Chrysoula Zerva, Ana C. Farinha, Christine Maroti, Jose G. C. de Souza, Taisiya Glushkova, Duarte M. Alves, Luisa Coheur, Alon Lavie, and Andre F. T. Martins. 2022. Comettiwi: Ist-unbabel 2022 submission for the quality estimation shared task. In _Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022_, pages 634-645. Association for Computational Linguistics. * Reid and Artetxe (2022a) Machel Reid and Mikel Artetxe. 2022a. On the role of parallel data in cross-lingual transfer learning. _CoRR_, abs/2212.10173. * Reid and Artetxe (2022b) Machel Reid and Mikel Artetxe. 2022b. PARADISE: exploiting parallel data for multilingual sequence-to-sequence pretraining. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, pages 800-810. Association for Computational Linguistics. * Le Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Elie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandre Sash Luccioni, Francois Yvon, Matthias Galle, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Westbson, Pawan Sasanka Ammannanchi, Thomas Wang, Benoit Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurencion, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. _CoRR_, abs/2211.05100. * Ustun et al. (2024) Ahmet Ustun, Viraat Aryabumi, Zheng Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024. Aya model: An instruction finetuned open-access multilingual language model. _CoRR_, abs/2402.07827. * Wang et al. (2022) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. How far can camels go? exploring the state of instruction tuning on open resources. _CoRR_, abs/2306.04751. * Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. _Trans. Mach. Learn. Res._, 2022. * Wei et al. (2023) Jerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning differently. _CoRR_, abs/2303.03846. * Wei et al. (2021) Xiangpeng Wei, Rongxiang Weng, Yue Hu, Luxi Xing, Heng Yu, and Weihua Luo. 2021. On learning universal representations across languages. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net. * Xu et al. (2023) Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2023. A paradigm shift in machine translation: Boosting translation performance of large language models. _CoRR_, abs/2309.11674. * Yang et al. (2020) Jian Yang, Shuming Ma, Dongdong Zhang, Shuangzhi Wu, Zhoujun Li, and Ming Zhou. 2020. Alternating language modeling for cross-lingual pre-training. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 9386-9393. AAAI Press. * Zhang et al. (2020) Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020. Improving massively multilingual neural machine translation and zero-shot translation. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, pages 1628-1639. Association for Computational Linguistics. * Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: less is more for alignment. _CoRR_, abs/2305.11206. * Zhu et al. (2023) Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023. Extrapolating large language models to non-english by aligning languages. _CoRR_, abs/2308.04948. [MISSING_PAGE_FAIL:13]"
    }
  ]
}