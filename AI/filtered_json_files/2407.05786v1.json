{
  "title": "Large Language Models for Judicial Entity Extraction: A Comparative Study",
  "authors": [
    "Atin S Hussain",
    "Anu Thomas"
  ],
  "abstract": "\n Domain-specific Entity Recognition holds significant importance in legal contexts, serving as a fundamental task that supports various applications such as question-answering systems, text summarization, machine translation, sentiment analysis, and information retrieval specifically within case law documents. Recent advancements have highlighted the efficacy of Large Language Models in natural language processing tasks, demonstrating their capability to accurately detect and classify domainspecific facts (entities) from specialized texts like clinical and financial documents. This research investigates the application of Large Language Models in identifying domain specific entities (e.g., courts, petitioner, judge, lawyer, respondents, FIR nos.) within case law documents, with a specific focus on their aptitude for handling domain-specific language complexity and contextual variations. The study evaluates the performance of state-of-the-art Large Language Model architectures, including Large Language Model Meta AI 3, Mistral, and Gemma, in the context of extracting judicial facts tailored to Indian judicial texts. Mistral and Gemma emerged as the top-performing models, showcasing balanced precision and recall crucial for accurate entity identification. These findings confirm the value of Large Language Models in judicial documents and demonstrate how they can facilitate and quicken scientific research by producing precise, organised data outputs that are appropriate for in-depth examination. \n",
  "references": [
    {
      "id": null,
      "title": "Large Language Models for Judicial Entity Extraction: A Comparative Study",
      "authors": [
        "Atin S Hussain",
        "Anu Thomas"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "",
      "authors": [
        "M Abdin",
        "S A Jacobs",
        "A A Awan",
        "J Aneja",
        "A Awadallah",
        "H Awadalla",
        "N Bach",
        "A Bahree",
        "A Bakhtiari",
        "J Bao",
        "H Behl",
        "A Benhaim",
        "M Bilenko",
        "J Bjorck",
        "S Bubeck",
        "Q Cai",
        "M Cai",
        "C C T Mendes",
        "W Chen",
        "V Chaudhary",
        "D Chen",
        "D Chen",
        "Y C Chen",
        "Y L Chen",
        "P Chopra",
        "X Dai",
        "A D Giorno",
        "G De Rosa",
        "M Dixon",
        "R Eldan",
        "V Fragoso",
        "D Iter",
        "M Gao",
        "M Gao",
        "J Gao",
        "A Garg",
        "A Goswami",
        "S Gunasekar",
        "E Haider",
        "J Hao",
        "R J Hewett",
        "J Huynh",
        "M Javaheripi",
        "X Jin",
        "P Kauffmann",
        "N Karampatziakis",
        "D Kim",
        "M Khademi",
        "L Kurilenko",
        "J R Lee",
        "Y T Lee",
        "Y Li",
        "Y Li",
        "C Liang",
        "L Liden",
        "C Liu",
        "M Liu",
        "W Liu",
        "E Lin",
        "Z Lin",
        "C Luo",
        "P Madan",
        "M Mazzola",
        "A Mitra",
        "H Modi",
        "A Nguyen",
        "B Norick",
        "B Patra",
        "D Perez-Becker",
        "T Portet",
        "R Pryzant",
        "H Qin",
        "M Radmilac",
        "C Rosset",
        "S Roy",
        "O Ruwase",
        "O Saarikivi",
        "A Saied",
        "A Salim",
        "M Santacroce",
        "S Shah",
        "N Shang",
        "H Sharma",
        "S Shukla",
        "X Song",
        "M Tanaka",
        "A Tupini",
        "X Wang",
        "L Wang",
        "C Wang",
        "Y Wang",
        "R Ward",
        "G Wang",
        "P Witte",
        "H Wu",
        "M Wyatt",
        "B Xiao",
        "C Xu",
        "J Xu",
        "W Xu",
        "S Yadav",
        "F Yang",
        "J Yang",
        "Z Yang",
        "Y Yang",
        "D Yu",
        "L Yuan",
        "C Zhang",
        "C Zhang",
        "J Zhang",
        "L L Zhang",
        "Y Zhang",
        "Y Zhang",
        "Y Zhang",
        "X Zhou"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Llama 3 Model Card",
      "authors": [
        "Ai@meta"
      ],
      "year": "2024",
      "venue": "Llama 3 Model Card",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "LEGAL-BERT: The Muppets straight out of Law School",
      "authors": [
        "I Chalkidis",
        "M Fergadiotis",
        "P Malakasiotis",
        "N Aletras",
        "I Androutsopoulos"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "",
      "authors": [
        "A Q Jiang",
        "A Sablayrolles",
        "A Mensch",
        "C Bamford",
        "D S Chaplot",
        "D De Las Casas",
        "F Bressand",
        "G Lengyel",
        "G Lample",
        "L Saulnier",
        "L R Lavaud",
        "M A Lachaux",
        "P Stock",
        "T L Scao",
        "T Lavril",
        "T Wang",
        "T Lacroix"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Named Entity Recognition in Indian court judgments",
      "authors": [
        "P Kalamkar",
        "A Agarwal",
        "A Tiwari",
        "S Gupta",
        "S Karn",
        "V Raghavan"
      ],
      "year": "2022",
      "venue": "Proceedings of the Natural Legal Language Processing Workshop 2022, Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Pretrained Language Models for the Legal Domain: A Case Study on Indian Law",
      "authors": [
        "S Paul",
        "A Mandal",
        "P Goyal",
        "S Ghosh"
      ],
      "year": "2023",
      "venue": "Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "",
      "authors": [
        "G Team",
        "T Mesnard",
        "C Hardin",
        "R Dadashi",
        "S Bhupatiraju",
        "S Pathak",
        "L Sifre",
        "M Rivière",
        "M S Kale",
        "J Love",
        "P Tafti",
        "L Hussenot",
        "P G Sessa",
        "A Chowdhery",
        "A Roberts",
        "A Barua",
        "A Botev",
        "A Castro-Ros",
        "A Slone",
        "A Héliou",
        "A Tacchetti",
        "A Bulanova",
        "A Paterson",
        "B Tsai",
        "B Shahriari",
        "C L Lan",
        "C A Choquette-Choo",
        "C Crepy",
        "D Cer",
        "D Ippolito",
        "D Reid",
        "E Buchatskaya",
        "E Ni",
        "E Noland",
        "G Yan",
        "G Tucker",
        "G C Muraru",
        "G Rozhdestvenskiy",
        "H Michalewski",
        "I Tenney",
        "I Grishchenko",
        "J Austin",
        "J Keeling",
        "J Labanowski",
        "J B Lespiau",
        "J Stanway",
        "J Brennan",
        "J Chen",
        "J Ferret",
        "J Chiu",
        "J Mao-Jones",
        "K Lee",
        "K Yu",
        "K Millican",
        "L L Sjoesund",
        "L Lee",
        "L Dixon",
        "M Reid",
        "M Mikuła",
        "M Wirth",
        "M Sharman",
        "N Chinaev",
        "N Thain",
        "O Bachem",
        "O Chang",
        "O Wahltinez",
        "P Bailey",
        "P Michel",
        "P Yotov",
        "R Chaabouni",
        "R Comanescu",
        "R Jana",
        "R Anil",
        "R Mcilroy",
        "R Liu",
        "R Mullins",
        "S L Smith",
        "S Borgeaud",
        "S Girgin",
        "S Douglas",
        "S Pandya",
        "S Shakeri",
        "S De",
        "T Klimenko",
        "T Hennigan",
        "V Feinberg",
        "W Stokowiec",
        "Y Hui Chen",
        "Z Ahmed",
        "Z Gong",
        "T Warkentin",
        "L Peran",
        "M Giang",
        "C Farabet",
        "O Vinyals",
        "J Dean",
        "K Kavukcuoglu",
        "D Hassabis",
        "Z Ghahramani",
        "D Eck",
        "J Barral",
        "F Pereira",
        "E Collins"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Exploring the Power of AI-Driven Decision Making in the Judicial Domain",
      "authors": [
        "A Thomas"
      ],
      "year": "2024",
      "venue": "Exploring the Power of AI-Driven Decision Making in the Judicial Domain",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "An innovative hybrid approach for extracting named entities from unstructured text data",
      "authors": [
        "A Thomas",
        "S Sangeetha"
      ],
      "year": "2019",
      "venue": "Computational Intelligence",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Knowledge graph based question-answering system for effective case law analysis",
      "authors": [
        "A Thomas",
        "S Sangeetha"
      ],
      "year": "2022",
      "venue": "Evolution in Computational Intelligence: Proceedings of the 9th International Conference on Frontiers in Intelligent Computing: Theory and Applications (FICTA 2021)",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "An adaptable, highperformance relation extraction system for complex sentences",
      "authors": [
        "A Thomas",
        "S Sivanesan"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Large Language Models For Judicial Entity Extraction: A Comparative Study",
      "text": "Atin S.Hussain atin.s@u.nus.edu Anu Thomas anu_t@gcarvithura.ac.in National University of Singapore Department of Computer Applications, St.George's College, Arwithura"
    },
    {
      "title": "Abstract",
      "text": "Domain-specific Entity Recognition holds significant importance in legal contexts, serving as a fundamental task that supports various applications such as question-answering systems, text summarization, machine translation, sentiment analysis, and information retrieval specifically within case law documents. Recent advancements have highlighted the efficacy of Large Language Models in natural language processing tasks, demonstrating their capability to accurately detect and classify domain-specific facts (entities) from specialized texts like clinical and financial documents. This research investigates the application of Large Language Models in identifying domain specific entities (e.g., courts, petitioner, judge, lawyer, respondents, FIR nos.) within case law documents, with a specific focus on their aptitude for handling domain-specific language complexity and contextual variations. The study evaluates the performance of state-of-the-art Large Language Model architectures, including Large Language Model Meta AI 3, Mistral, and Gemma, in the context of extracting judicial facts tailored to Indian judicial texts. Mistral and Gemma emerged as the top-performing models, show-casing balanced precision and recall crucial for accurate entity identification. These findings confirm the value of Large Language Models in judicial documents and demonstrate how they can facilitate and quicken scientific research by producing precise, organised data outputs that are appropriate for in-depth examination. keywords: Large language Models, Natural Language Processing, Judicial Domain, Judicial Entity Recognition, Information Extraction, Court Judgments + Footnote †: journal: Computer Applications"
    },
    {
      "title": "1 Introduction",
      "text": "Domain-specific entity recognition is a pivotal component in the realm of natural language processing, especially within specialized domains such as the legal field. The task involves identifying and classifying judicial entities such as petitioner, respondents, and judges, attorneys etc. which are foundational for a variety of applications. These applications include relation extraction,, machine translation, sentiment analysis at the entity level, faceted search, knowledge base construction, and information retrieval Thomas and Sangeetha (2019). Finding domain-specific entities and their relationships helps improve the indexing and retrieval of legal texts and is helpful as a first step in feature selection for text clustering, classification, as well as information selection for text summarization. Furthermore, a well-tuned entity recognition(ER) system forms a basis for various applications in the legal domain as follows. Legal Question-answering system: Judicial facts are essential in determining the responses to factoid queries. For instance, if the query is, \"Who is the appellant in a particular judgement?\" The answer will be predicted by the question processing module to be some judicial entity. In the event that \"Mr. X\" is the response and the data set has judicial entities assigned to it, the question-answering system would recognise that \"Mr. X\" is an entity and that it may be the response. Creation of a knowledge graph: We can present the textual data in graphical form, such as entity-relationship graphs, if we could identify the NEs in the judicial text and the relationships among those entities. These graphs can be used to answer complicated relationship inquiries. Moreover, text summary is facilitated by the detection and annotation of the most pertinent information associated with a NE. Thomas and Sangeetha (2022) Case-Based Reasoning: The foundation of case-based reasoning is the knowledge input that may be obtained from extracted information found in court language. This information can be fed into a variety of expert systems, including business intelligence tools and predictive analytics software. Thomas (2024) Relation Extraction (RE): Entity Recognition plays a pivotal role in relation extraction from judicial text by identifying key entities such as names of judges, plaintiffs, defendants, legal entities, and locations mentioned within the text. Once these entities are identified, RE identifies the relationships between them, aiding in the extraction of pertinent legal relations, such as \"defendant accused of crime,\" \"plaintiff filed a lawsuit against defendant,\" or \"court ruled in favor of plaintiff.\" Thomas and Sivanesan (2022). Moreover, relation triplets can be utilized as features for other machine learning applications, such text categorization, document summarization, paraphrase detection, and so on. The capabilities of ER systems have significantly advanced with the introduction of Large Language Models (LLMs). Equipped with advanced natural language processing methods, LLMs have shown to be extraordinarily adept in identifying and classifying objects in a wide range of complex texts. They excel at understanding and processing natural language, which makes them well-suited to handle the complexities of legal documents, which frequently include complex context and specialized terminologies. Through this exploration, we aim to shed light on the potential of LLMs to revolutionize ER in legal texts with zero-shot learning, paving the way for more efficient and accurate information retrieval and management within the judicial system. The key contribution of this paper is: * Evaluating the effectiveness of cutting-edge LLMs (like LLaMA 3 (Large Language Model Meta AI 3), Mistral, and Gemma) for domain-specific ER tasks within Indian legal texts. The paper is structured as follows. Section 2 explores the related works. Section 3 explains the state-of-the-art LLMs. Section 4 discusses the proposed methodology followed by results and discussions in section 5. Section 6 concludes the paper."
    },
    {
      "title": "2 Related Works",
      "text": "The field of generic Named Entity Recognition (NER) has seen substantial advancements, particularly with the integration of machine learning and deep learning techniques. Early approaches relied on rule-based and statistical methods, such as Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs), which, while effective to some extent, often struggled with domain-specific language and lacked generalization capabilities. The introduction of neural network-based models marked a significant leap in NER performance. Recurrent Neural Networks (RNNs), and more specifically Long Short-Term Memory networks (LSTMs), improved the ability to capture sequential dependencies in text. The advent of attention mechanisms and Transformers further revolutionized the field, leading to the development of pre-trained language models such as BERT (Bidirectional Encoder Representations from Transformers). BERT's contextual understanding and fine-tuning abilities demonstrated remarkable improvements in NER tasks across various domains. In the legal domain, specialized ER systems have been developed to address the unique challenges posed by legal texts, including the use of complex terminology and context-specific references. Models like Legal-BERT [18] and CaseLaw-BERT [19], which are pre-trained on legal corpora, have shown promise in enhancing entity recognition within legal documents. However, these models often require extensive domain-specific training data to achieve optimal performance. Recent advancements in LLMs have further pushed the boundaries of NER capabilities. Models such as GPT-3 and its successors have exhibited exceptional proficiency in understanding and generating human-like text, which translates into improved accuracy in entity recognition tasks. The emergence of models such as LLaMA 3 and Gemma represents the latest frontier in this evolution, promising even greater performance through enhanced architectural innovations and larger training datasets. LLMs hold the added advantage of not having to be trained on legal datasets for domain-specific ER tasks. This study builds on these advancements by evaluating the effectiveness of LLMs including LLaMA 3 [1], Gemma [20], Phi3 [1] and Mistral [11] in performing domain specific ER tasks within the context of Indian judicial texts with system prompting. By focusing on these state-of-the-art models, we aim to contribute to the growing body of research that seeks to harness the power of LLMs for specialized applications in the legal domain."
    },
    {
      "title": "3 Large Language Models",
      "text": "This paper compares the following 4 different state-of-the-art Large Language Models in the task of domain specific Entity Recognition for legal documents: * **LLaMA 3**[1] : The latest generation of Meta's open-source large language model, represents a significant advancement in natural language processing capabilities, making it highly suitable for complex tasks such as Entity Recognition (ER) in legal documents. Featuring models with up to 70 billion parameters, LLaMA 3 excels in understanding and generating human-like text, demonstrating state-of-the-art performance across various benchmarks. Its enhanced architecture, including a more efficient tokenizer and grouped query attention, ensures superior inference efficiency and accuracy. These improvements make LLaMA 3 particularly effective in handling the specialized terminology and nuanced context typical of legal texts, thereby facilitating precise entity identification and categorization critical for legal information retrieval and document management. * **Gemma**[20] : Developed by Google DeepMind and other teams across Google, represents a family of lightweight, state-of-the-art open models designed for high performance and broad accessibility. Available in two sizes, Gemma 2B and Gemma 7B, these models are optimized for diverse AI applications, including Entity Recognition (ER) in legal documents. Gemma models are pre-trained and instruction-tuned, allowing them to efficiently handle the complex and domain-specific language found in case law texts. They surpass significantly larger models on key benchmarks, making them suitable for deployment on various platforms, from laptops to cloud infrastructures like Google Cloud. The incorporation of advanced fine-tuning techniques and robust evaluation processes ensures Gemma models produce safe and reliable outputs, crucial for maintaining the integrity of legal document processing. By leveraging these capabilities, the Gemma model holds promise for enhancing the accuracy and efficiency of ER tasks in the legal domain. * **Phi 3**[1] : The model,developed by Microsoft, represents a significant advancement in small language models (SLMs), offering exceptional performance and cost-effectiveness. Particularly relevant to Entity Recognition tasks in legal documents, Phi-3 models, such as the Phi-3-mini, excel due to their ability to handle long context windows up to 128K tokens. This capacity is crucial for processing extensive legal texts, ensuring comprehensive entity recognition across large document spans. Phi-3's instruction-tuned design and optimized performance across various hardware platforms, including on-device use, facilitate efficient and accurate ER in resource-constrained environments. The model's strong reasoning and logic capabilities further enhance its suitability for the analytical demands of legal document processing, providing a powerful tool for improving the efficiency and accuracy of legal information retrieval. * **Mistral**[Jiang et al. (2023)] : A powerful 7.3 billion parameter language model, demonstrates remarkable capabilities in natural language processing tasks, outperforming larger models like LLama 2 13B across various benchmarks. Utilizing advanced techniques such as Grouped-query attention (GQA) and Sliding Window Attention (SWA), Mistral 7B achieves faster inference and handles longer sequences more efficiently. These features make it particularly suitable for ER tasks in legal documents, which often involve processing extensive texts with complex domain-specific language. The model's ability to be fine-tuned easily for specific tasks further enhances its applicability in the legal domain, where precision and context understanding are crucial. Given its superior performance and efficiency, Mistral 7B is well-equipped to improve the accuracy and effectiveness of ER systems in legal document analysis."
    },
    {
      "title": "4 Methodology",
      "text": "In this study, we employ few-shot prompt engineering to leverage the capabilities of large language models for judicial ER in legal documents. This technique involves crafting a single, carefully designed prompt that instructs the LLM to generate responses in a specified JSON format. The JSON response includes both the extracted text and the corresponding entity labels from the input document. This approach is particularly advantageous as it eliminates the necessity for extensive task-specific training. By directly utilizing the pre-trained LLM's advanced natural language understanding, we can efficiently identify and label entities within legal texts, streamlining the process and reducing the overhead typically associated with model training and fine-tuning."
    },
    {
      "title": "5 Results And Discussions",
      "text": ""
    },
    {
      "title": "Experimental Setup",
      "text": "We evaluate the model on the InLegalNER dataset [Kalamkar et al. (2022)] to rigorously assess the performance of Large Language Models on domain-specific Entity Recognition tasks. The InLegalNER dataset is specifically designed to encompass a comprehensive range of entities pertinent to the legal domain, thereby providing a robust benchmark for evaluating the capability of LLMs in recognizing and categorizing legal entities accurately. Table 1 presents a detailed breakdown of the various entities included in the dataset, offering insights into the diversity and complexity of the entity types that the models are required to identify. This evaluation aims to highlight the effectiveness of LLMs in handling the specialized terminology and context inherent in legal documents, thereby contributing to the advancement of ER methodologies in this critical domain."
    },
    {
      "title": "Model Evaluation",
      "text": "In our study, we evaluated the performance of several state-of-the-art large language models for the Entity Recognition task within legal documents. The models evaluated include LLaMA 3,Gemma, Mistral, and Phi 3. We utilized precision, recall, and F1 score as our evaluation metrics, which provide a comprehensive assessment of each model's accuracy and effectiveness in identifying and labeling entities."
    },
    {
      "title": "5.2.1 **Llama 3 Evaluation**",
      "text": "The LLaMA model demonstrated a precision of 0.7366, a recall of 0.6286, and an F1 score of 0.5917. While the model shows high precision, indicating a strong ability to correctly identify relevant entities, its recall is relatively lower, suggesting some missed entities within the text."
    },
    {
      "title": "5.2.2 **Gemma Evaluation**",
      "text": "The Gemma model yielded a precision of 0.7131, a recall of 0.6534, and an F1 score of 0.6353. Gemma's balanced precision and recall indicate a more consistent performance in identifying entities correctly and ensuring fewer missed entities, resulting in a higher F1 score compared to LLaMA."
    },
    {
      "title": "5.2.3 **Mistral Evaluation**",
      "text": "The Mistral model achieved a precision of 0.7097, a recall of 0.6628, and an F1 score of 0.6376. Mis \\begin{table} \\begin{tabular}{l l l} \\hline \\hline **Named Entity** & **Description** & **\\% Occurence** \\\\ \\hline **COURT** & Name of any court mentioned if extracted & 7.90\\% \\\\ **PETITIONER** & Name of the petitioners / appellants / revision- & 10.24\\% \\\\ & ist from current case & 12.89\\% \\\\ **RESPONDENT** & Name of the respondents / defendants / opposition from current case & 11.70\\% \\\\ **JUDGE** & Name of the judges & 7.76\\% \\\\ **LAWYER** & Name of the lawyers from both the parties & 11.70\\% \\\\ **DATE** & Any date mentioned in the judgment & 6.29\\% \\\\ **ORG** & Name of organizations mentioned in text apart & 4.81\\% \\\\ & from the court. & 4.67\\% \\\\ **GPE** & Geopolitical locations which include names of states, cities, villages & 6.02\\% \\\\ **STATUTE** & Name of the act or law mentioned in the judgement & 7.96\\% \\\\ **PROVISION** & Sections, sub-sections, articles, orders, rules under a statute & 7.96\\% \\\\ **PRECEDENT** & All the past court cases referred to in the judgement as precedent. & 4.51\\% \\\\ **CASE\\_NUMBER** & All the other case numbers mentioned in the judgment (apart from precedent) & 3.47\\% \\\\ **WITNESS** & Name of witnesses in current judgment & 2.94\\% \\\\ **OTHER\\_PERSON** & Name of all the persons that are not included in petitioner, respondent, judge and witness & 8.85\\% \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: InLegalNER Dataset Entity Information \\begin{table} \\begin{tabular}{l l l l} \\hline \\hline Model & Precision & Recall & F1 Score \\\\ \\hline LLaMA 3 & **0.7366** & 0.6286 & 0.5917 \\\\ Gemma & 0.7131 & 0.6534 & 0.6353 \\\\ Mistral & 0.7097 & **0.6628** & **0.6376** \\\\ Phi 3 & 0.5975 & 0.5617 & 0.5440 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Evaluation of the LLM Modelstral's performance is similar to Gemma, with a slightly lower precision but higher recall, which translates to a marginally better F1 score. This suggests that Mistral is effective in identifying a comprehensive set of entities while maintaining a reasonable level of accuracy."
    },
    {
      "title": "5.2.4 Phi 3 Evaluation",
      "text": "The PHI3 model showed a precision of 0.5975, a recall of 0.5617, and an F1 score of 0.5440. PHI3's lower precision and recall indicate challenges in both correctly identifying and not missing entities, resulting in the lowest F1 score among the evaluated models."
    },
    {
      "title": "Comparative Analysis",
      "text": "Overall, Mistral emerged as the best-performing model with the highest F1 score of 0.6376, closely followed by Gemma with an F1 score of 0.6353. Both models demonstrated a good balance between precision and recall, making them suitable for the NER task in legal documents. LLaMA 3, despite its higher precision, lagged in recall, indicating potential gaps in entity recognition. Phi 3 showed the least favorable performance across all metrics, suggesting it is less suited for this specific task compared to the other models evaluated. These evaluations underscore the importance of considering both precision and recall in ER tasks, particularly in the legal domain where the accurate and comprehensive identification of entities is crucial. The results highlight Mistral and Gemma as robust options for further exploration and deployment in legal ER applications."
    },
    {
      "title": "6 Conclusion",
      "text": "In conclusion, our study evaluated several state-of-the-art LLMs for legal entity recognition from Case Law Documents, focusing on their performance in handling domain-specific language within Indian judicial texts. Mistral and Gemma emerged as the top-performing models, showcasing balanced precision and recall crucial for accurate entity identification. These findings underscore the potential of LLMs to revolutionize ER in legal documents, offering efficient and precise entity recognition capabilities that benefit legal information management and analysis. Continued advancements in LLM architectures hold promise for further enhancing ER systems in the legal domain."
    },
    {
      "title": "7 Funding",
      "text": "This study received no external funding."
    },
    {
      "title": "8 Competing Interests",
      "text": "The authors declare that they have no competing interests"
    },
    {
      "title": "9 Availability Of Data And Materials",
      "text": "The used and/or during the current study (the bibliography of included studies) are available from the corresponding author upon request."
    },
    {
      "title": "Acknowledgements",
      "text": "Not applicable."
    },
    {
      "title": "References",
      "text": "* M. Abdin, S. A. Jacobs, A. Awan, J. Aneja, J. Awadalah, A. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, A. Bao, H. Behl, A. Benhaim, M. Bilenko, J. Bjorck, S. Bubeck, Q. Cai, M. Cai, C. C. T. Mendes, W. Chen, V. Chaudhary, D. Chen, Y. Chen, Y. C. Chen, Y. L. Chopra, X. Dai, A. Giorno, A. D. de Rosa, M. Dixon, R. Eldan, V. Fragoso, D. Iter, M. Gao, M. Gao, J. Garg, A. Goswami, S. Gunasekar, E. Haider, J. Hao, R. Hewett, J. Huynh, J. Javaheriqi, M. Jin, X. Kauffmann, P. Karampatziakis, N. Kim, M. Khademi, L. Kurilenko, J. Lee, J. R. Lee, Y. T. Li, Y. Li, Y. Liang, C. Liden, L. Liu, C. Liu, M. Liu, W. Lin, E. Lin, Z. Luo, C. Madan, P. Mazzola, M. Mitra, A. Modi, H. Nguyen, B. Norick, B. Patra, B. Perez-Becker, D. Portet, T. Pryzant, R. Qin, H. Radmilac, M. Rosset, C. Roy, S. Ruwase, O. Saarikivi, O. Saied, A. Salim, A. Santacroce, M. Shah, S. Shang, N. Sharma, H. Shukla, S. Song, X. Tanaka, M. Tupini, A. Wang, L. Wang, C. Wang, Y. Ward, R. Wang, G. Witte, H. Wu, M. Wyatt, M. Xiao, B. Xu, C. Xu, J. Xu, W. Yadav, S. Yang, F. Yang, J. Yang, Z. Yang, Y. Yu, D. Yuan, L. Zhang, C. Zhang, Y. Zhang, J. Zhang, L.L. Zhang, Y. Zhang, Y. Zhang, X. Zhou, 2024. Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone. AI@Meta, 2024. Llama 3 Model Card. * Chalkidis et al. (2020) Chalkidis, I., Fergadiots, M., Malakasiotis, P., Aletras, N., Androutsopoulos, I., 2020. LEGAL-BERT: The Muppets straight out of Law School, in: Cohn, T., He, Y., Liu, Y. (Eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics, Online. pp. 2898-2904. * Jiang et al. (2023) Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L.R., Lachaux, M.A., Stock, P., Scao, T.L., Lavril, T., Wang, T., Lacroix, T., Sayed, W.E., 2023. Mistral 7B. * Kalamkar et al. (2022) Kalamkar, P., Agarwal, A., Tiwari, A., Gupta, S., Karn, S., Raghavan, V., 2022. Named Entity Recognition in Indian court judgments, in: Proceedings of the Natural Legal Language Processing Workshop 2022, Association for Computational Linguistics, Abu Dhabi, United Arab Emirates (Hybrid). pp. 184-193. * Paul et al. (2023) Paul, S., Mandal, A., Goyal, P., Ghosh, S., 2023. Pretrained Language Models for the Legal Domain: A Case Study on Indian Law, in: Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law, Association for Computing Machinery, New York, NY, USA. pp. 187-196. * Team et al. (2021) Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Riviere, M., Kale, M.S., Love, J., Tafti, P., Hussenot, L., Sessa, P.G., Chowdhery, A., Roberts, A., Barua, A., Botev, A., Castro-Ros, A., Slone, A., Heliou, A., Tacchetti, A., Bulanova, A., Paterson, A., Tsai, B., Shahriari, B., Lan, C.L., Choquette-Choo, C.A., Crepy, C., Cer, D., Ippolito, D., Reid, D., Buchatskaya, E., Ni, E., Noland, E., Yan, G., Tucker, G., Muraru, G.C., Rozhdestvenskiy, G., Michalewski, H., Tenney, I., Grishchenko, I., Austin, J., Keeling, J., Labanowski, J., Lespiau, J.B., Stanway, J., Brennan, J., Chen, J., Ferret, J., Chiu, J., Mao-Jones, J., Lee, K., Yu, K., Millican, K., Sjoesund, L.L., Lee, L., Dixon, L., Reid, M., Mikula, M., Wirth, M., Sharman, M., Chinaev, N., Thain, N., Bachem, O., Chang, O., Wahltime, O., Bailey, P., Michel, P., Yotov, P., Chaabouni, R., Comanescu, R., Jana, R., Anil, R., McIlroy, R., Liu, R., Mullins, R., Smith, S.L., Borgeaud, S., Girgin, S., Douglas, S., Pandya, S., Shakeri, S., De, S., Klimenko, T., Hennigan, T., Feinberg, V., Stokowiec, W., hui Chen, Y., Ahmed, Z., Gong, Z., Warkentin, T., Peran, L., Giang, M., Farabet, C., Vinyals, O., Dean, J., Kavukcuoglu, K., Hassabis, D., Ghahramani, Z., Eck, D., Barral, J., Pereira, F., Collins, E., Joulin, A., Fiedel, N., Senter, E., Andreev, A., Kenealy, K., 2024. Gemma: Open Models Based on Gemini Research and Technology. * Thomas (2024) Thomas, A., 2024. Exploring the Power of AI-Driven Decision Making in the Judicial Domain: Case Studies, Benefits, Challenges, and Solutions. * Thomas and Sangeetha (2019) Thomas, A., Sangeetha, S., 2019. An innovative hybrid approach for extracting named entities from unstructured text data. Computational Intelligence 35, 799-826. * Thomas and Sangeetha (2022) Thomas, A., Sangeetha, S., 2022. Knowledge graph based question-answering system for effective case law analysis, in: Evolution in Computational Intelligence: Proceedings of the 9th International Conference on Frontiers in Intelligent Computing: Theory and Applications (FICTA 2021), pp. 291-300. * Thomas and Sivanesan (2022) Thomas, A., Sivanesan, S., 2022. An adaptable, high-performance relation extraction system for complex sentences. Knowledge-Based Systems 251, 108956."
    }
  ]
}