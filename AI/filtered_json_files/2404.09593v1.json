{
  "title": "Improving Recall of Large Language Models: A Model Collaboration Approach for Relational Triple Extraction",
  "authors": [
    "Ding Zepeng",
    "Wenhao ♢♡",
    "♢♠ Huang",
    "Jiaqing Liang",
    "Yang ♢♡",
    "Yanghua Xiao"
  ],
  "abstract": "\n Relation triple extraction, which outputs a set of triples from long sentences, plays a vital role in knowledge acquisition. Large language models can accurately extract triples from simple sentences through few-shot learning or fine-tuning when given appropriate instructions. However, they often miss out when extracting from complex sentences. In this paper, we design an evaluation-filtering framework that integrates large language models with small models for relational triple extraction tasks. The framework includes an evaluation model that can extract related entity pairs with high precision. We propose a simple labeling principle and a deep neural network to build the model, embedding the outputs as prompts into the extraction process of the large model. We conduct extensive experiments to demonstrate that the proposed method can assist large language models in obtaining more accurate extraction results, especially from complex sentences containing multiple relational triples. Our evaluation model can also be embedded into traditional extraction models to enhance their extraction precision from complex sentences. \n",
  "references": [
    {
      "id": null,
      "title": "Improving Recall of Large Language Models: A Model Collaboration Approach for Relational Triple Extraction",
      "authors": [
        "Ding Zepeng",
        "Wenhao ♢♡",
        "♢♠ Huang",
        "Jiaqing Liang",
        "Yang ♢♡",
        "Yanghua Xiao"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Large language models are zero-shot clinical information extractors",
      "authors": [
        "Monica Agrawal",
        "Stefan Hegselmann",
        "Hunter Lang",
        "Yoon Kim",
        "David Sontag"
      ],
      "year": "2022",
      "venue": "Large language models are zero-shot clinical information extractors",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "A simple zero-shot prompt weighting technique to improve prompt ensembling in textimage models",
      "authors": [
        "James Urquhart Allingham",
        "Jie Ren",
        "Xiuye Michael W Dusenberry",
        "Yin Gu",
        "Dustin Cui",
        "Jeremiah Zhe Tran",
        "Balaji Liu",
        "Lakshminarayanan"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Qwen technical report",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "HacRED: A large-scale relation extraction dataset toward hard cases in practical applications",
      "authors": [
        "Qiao Cheng",
        "Juntao Liu",
        "Xiaoye Qu",
        "Jin Zhao",
        "Jiaqing Liang",
        "Zhefeng Wang",
        "Baoxing Huai",
        "Nicholas Jing Yuan",
        "Yanghua Xiao"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
      "doi": "10.18653/v1/2021.findings-acl.249"
    },
    {
      "id": "b5",
      "title": "Insrl: A multi-view learning framework fusing multiple information sources for distantly-supervised relation extraction",
      "authors": [
        "Zhendong Chu",
        "Haiyun Jiang",
        "Yanghua Xiao",
        "Wei Wang"
      ],
      "year": "2020",
      "venue": "Insrl: A multi-view learning framework fusing multiple information sources for distantly-supervised relation extraction",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Eric Li",
        "Xuezhi Wang",
        "Mostafa Dehghani"
      ],
      "year": "",
      "venue": "Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "id": "b8",
      "title": "Span-level model for relation extraction",
      "authors": [
        "Kalpit Dixit",
        "Yaser Al-Onaizan"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1525"
    },
    {
      "id": "b9",
      "title": "Linguistic resources for 2013 knowledge base population evaluations",
      "authors": [
        "Joe Ellis",
        "Xuansong Li",
        "Kira Griffitt",
        "Stephanie M Strassel",
        "Jonathan Wright"
      ],
      "year": "2012",
      "venue": "Linguistic resources for 2013 knowledge base population evaluations",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Exploring the feasibility of chatgpt for event extraction",
      "authors": [
        "Jun Gao",
        "Huan Zhao",
        "Changlong Yu",
        "Ruifeng Xu"
      ],
      "year": "2023",
      "venue": "Exploring the feasibility of chatgpt for event extraction",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Knowledge-based weak supervision for information extraction of overlapping relations",
      "authors": [
        "R Hoffmann",
        "Congle Zhang",
        "Xiao Ling",
        "Luke Zettlemoyer",
        "Daniel S Weld"
      ],
      "year": "2011",
      "venue": "Proceedings of ACL",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Chatgpt makes medicine easy to swallow: an exploratory case study on simplified radiology reports",
      "authors": [
        "Katharina Jeblick",
        "Balthasar Schachtner",
        "Jakob Dexl",
        "Andreas Mittermeier",
        "Anna Theresa Stüber",
        "Johanna Topalis",
        "Tobias Weber",
        "Philipp Wesp",
        "Bastian Oliver Sabel",
        "Jens Ricke"
      ],
      "year": "2023",
      "venue": "European Radiology",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Span-based joint entity and relation extraction with attentionbased span-specific and contextual semantic representations",
      "authors": [
        "Bin Ji",
        "Jie Yu",
        "Shasha Li",
        "Jun Ma",
        "Qingbo Wu",
        "Yusong Tan",
        "Huijun Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.8"
    },
    {
      "id": "b15",
      "title": "LLM-blender: Ensembling large language models with pairwise ranking and generative fusion",
      "authors": [
        "Dongfu Jiang",
        "Xiang Ren",
        "Bill Yuchen",
        "Lin"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.792"
    },
    {
      "id": "b16",
      "title": "Surface pattern-enhanced relation extraction with global constraints",
      "authors": [
        "Haiyun Jiang",
        "Juntao Liu",
        "Sheng Zhang",
        "Deqing Yang",
        "Yanghua Xiao",
        "Wei Wang"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Co-training improves prompt-based learning for large language models",
      "authors": [
        "Hunter Lang",
        "Monica N Agrawal",
        "Yoon Kim",
        "David Sontag"
      ],
      "year": "2022",
      "venue": "Proceedings of the 39th International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Fast inference from transformers via speculative decoding",
      "authors": [
        "Yaniv Leviathan",
        "Matan Kalman",
        "Yossi Matias"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Entity-relation extraction as multi-turn question answering",
      "authors": [
        "Xiaoya Li",
        "Fan Yin",
        "Zijun Sun",
        "Xiayu Li",
        "Arianna Yuan",
        "Duo Chai",
        "Mingxin Zhou",
        "Jiwei Li"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1129"
    },
    {
      "id": "b20",
      "title": "Fine-grained entity recognition",
      "authors": [
        "Xiao Ling",
        "Daniel S Weld"
      ],
      "year": "2012",
      "venue": "Twenty-Sixth AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Heterogeneous supervision for relation extraction: A representation learning approach",
      "authors": [
        "Liyuan Liu",
        "Xiang Ren",
        "Qi Zhu",
        "Shi Zhi",
        "Huan Gui",
        "Ji Heng",
        "Jiawei Han"
      ],
      "year": "2017",
      "venue": "Heterogeneous supervision for relation extraction: A representation learning approach",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Cotype: Joint extraction of typed entities and relations with knowledge bases",
      "authors": [
        "Xiang Ren",
        "Zeqiu Wu",
        "Wenqi He",
        "Meng Qu",
        "Clare R Voss",
        "Heng Ji",
        "Jiawei Tarek F Abdelzaher",
        "Han"
      ],
      "year": "2017",
      "venue": "Proceedings of the 26th International Conference on World Wide Web",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Modeling relations and their mentions without labeled text",
      "authors": [
        "Sebastian Riedel",
        "Limin Yao",
        "Andrew Mccallum"
      ],
      "year": "2010",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "ZS4IE: A toolkit for zero-shot information extraction with simple verbalizations",
      "authors": [
        "Oscar Sainz",
        "Haoling Qiu",
        "Oier Lopez De Lacalle",
        "Eneko Agirre",
        "Bonan Min"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations",
      "doi": "10.18653/v1/2022.naacl-demo.4"
    },
    {
      "id": "b27",
      "title": "Roformer: Enhanced transformer with rotary position embedding",
      "authors": [
        "Jianlin Su",
        "Yu Lu",
        "Shengfeng Pan",
        "Bo Wen",
        "Yunfeng Liu"
      ],
      "year": "2021",
      "venue": "Roformer: Enhanced transformer with rotary position embedding",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "A hierarchical framework for relation extraction with reinforcement learning",
      "authors": [
        "Ryuichi Takanobu",
        "Tianyang Zhang",
        "Jiexi Liu",
        "Minlie Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of AAAI",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Does synthetic data generation of llms help clinical text mining",
      "authors": [
        "Ruixiang Tang",
        "Xiaotian Han",
        "Xiaoqian Jiang",
        "Xia Hu"
      ],
      "year": "2023",
      "venue": "Does synthetic data generation of llms help clinical text mining",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "year": "",
      "venue": "Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "2023b. Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "",
      "venue": "2023b. Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Revisiting relation extraction in the era of large language models",
      "authors": [
        "Somin Wadhwa",
        "Silvio Amir",
        "Byron Wallace"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.868"
    },
    {
      "id": "b33",
      "title": "TPLinker: Single-stage joint extraction of entities and relations through token pair linking",
      "authors": [
        "Yucheng Wang",
        "Bowen Yu",
        "Yueyang Zhang",
        "Tingwen Liu",
        "Hongsong Zhu",
        "Limin Sun"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.138"
    },
    {
      "id": "b34",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Y Vincent",
        "Kelvin Zhao",
        "Adams Wei Guu",
        "Brian Yu",
        "Nan Lester",
        "Andrew M Du",
        "Quoc V Dai",
        "Le"
      ],
      "year": "2021",
      "venue": "Finetuned language models are zero-shot learners",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Zero-shot information extraction via chatting with chatgpt",
      "authors": [
        "Xiang Wei",
        "Xingyu Cui",
        "Ning Cheng",
        "Xiaobin Wang",
        "Xin Zhang",
        "Shen Huang",
        "Pengjun Xie",
        "Jinan Xu",
        "Yufeng Chen",
        "Meishan Zhang"
      ],
      "year": "2023",
      "venue": "Zero-shot information extraction via chatting with chatgpt",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "A novel cascade binary tagging framework for relational triple extraction",
      "authors": [
        "Zhepei Wei",
        "Jianlin Su",
        "Yue Wang",
        "Yuan Tian",
        "Yi Chang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.136"
    },
    {
      "id": "b37",
      "title": "Revisiting the negative data of distantly supervised relation extraction",
      "authors": [
        "Chenhao Xie",
        "Jiaqing Liang",
        "Jingping Liu",
        "Chengsong Huang",
        "Wenhao Huang",
        "Yanghua Xiao"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.277"
    },
    {
      "id": "b38",
      "title": "Small models are valuable plug-ins for large language models",
      "authors": [
        "Canwen Xu",
        "Yichong Xu",
        "Shuohang Wang",
        "Yang Liu",
        "Chenguang Zhu",
        "Julian Mcauley"
      ],
      "year": "2023",
      "venue": "Small models are valuable plug-ins for large language models",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Joint extraction of entities and relations based on a novel decomposition strategy",
      "authors": [
        "Bowen Yu",
        "Zhenyu Zhang",
        "Xiaobo Shu",
        "Yubin Wang",
        "Tingwen Liu",
        "Bin Wang",
        "Sujian Li"
      ],
      "year": "2020",
      "venue": "Proceedings of ECAI",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Zero-shot temporal relation extraction with ChatGPT",
      "authors": [
        "Chenhan Yuan",
        "Qianqian Xie",
        "Sophia Ananiadou"
      ],
      "year": "2023",
      "venue": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
      "doi": "10.18653/v1/2023.bionlp-1.7"
    },
    {
      "id": "b41",
      "title": "Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena",
      "authors": [
        "Lianmin Zheng",
        "Wei-Lin Chiang",
        "Ying Sheng",
        "Siyuan Zhuang",
        "Zhanghao Wu",
        "Yonghao Zhuang",
        "Zi Lin",
        "Zhuohan Li",
        "Dacheng Li"
      ],
      "year": "",
      "venue": "Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "2 Related Works",
      "text": ""
    },
    {
      "title": "Large Language Models For Relational Triple Extraction",
      "text": "Large Language Models (LLMs) have gained widespread attention due to their strong ability for various NLP tasks. In addition to the robust GPT series Brown et al. (2020); OpenAI (2023), open-source LLMs have been also widely studied and applied, including Llama series Touvron et al. (2023a, b), Owen Bai et al. (2023) and Vicuna Zheng et al. (2023). Recent studies on LLMs suggest that they perform well in a variety of downstream tasks, even when provided with only a few \\begin{table} \\begin{tabular}{c c c c} \\hline \\hline **Dataset** & **Fine-tune** & **Precision** & **Recall** \\\\ \\hline NYT10 & w/o fine-tune & 13.75 & 7.75 \\\\ NYT10 & w/ fine-tune & 78.05 & 46.38 \\\\ SKE21 & w/o fine-tune & 41.30 & 34.17 \\\\ SKE21 & w/ fine-tune & 72.56 & 57.42 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Precision and recall when extracting multiple relational triples by a large language model. We only consider the complex sentences that contain more than 7 triples. The model used is Vicuna-13B for NYT10 and Owen-7B for SKE21. Figure 1: (a) Illustration of multiple relational triple extraction by LLMs, based on ChatGPT or Vicuna-13B. Both models are given appropriate instructions, limited predicates list and asked to extract as many as possible. (b) Compelling LLM to generate more triples results in repetitive outputs. examples as instructions (Agrawal et al., 2022; Jeblick et al., 2023). In extraction-related tasks, some works show that with proper prompting, ChatGPT can achieve comparable performance with the supervised methods on zero-shot or few-shot settings of extraction tasks (Wei et al., 2023; Gao et al., 2023; Tang et al., 2023). For open-source LLMs, previous work shows Flan-T5 (Chung et al., 2022) can yield outstanding performance by supervising and fine-tuning and suggests LLMs should be a standard baseline for relation extractions (Wadhwa et al., 2023). However, these studies did not specifically consider the model's extraction ability on complex sentences containing multiple relational triples. Furthermore, the manual evaluation of the results was not as rigorous as exact matching, and most of these studies focus on chatGPT and do not consider various open-source LLMs."
    },
    {
      "title": "_Model Collaboration In The Era Of Large Language Models_",
      "text": "Current methods of model collaboration involving large language models can be primarily categorized into three types. First, the output results of the small model are utilized as a component of the overall framework to assist the LLMs to perform better on downstream tasks (Xu et al., 2023; Leviathan et al., 2023). Second, large and small models are collaboratively trained based on task data to efficiently utilize the unlabeled data and minimize the bias of models (Lang et al., 2022). Third, ensembling multiple prompts or multiple LLMs to achieve more stable output results, as well as improved generalization performance (Allingham et al., 2023; Jiang et al., 2023). In the field of relational triple extraction, research based on traditional models is relatively comprehensive, and some novel and effective multi-step or joint methods are proposed to extract multiple triples (Li et al., 2019; Wei et al., 2020; Yu et al., 2020; Xie et al., 2021). For example, ReRe (Xie et al., 2021) carefully compares different types of multi-step settings and shows that the _relation-then-entity_ extraction paradigm exhibits a good performance since it suffers less from the problem of data imbalance, which is often encountered in relational triple extraction tasks. However, these methods cannot fully solve complex relational triple extraction tasks. Inspired by this, we propose to design an evaluation model and integrate this small model as a plug-in within the extraction framework based on LLMs."
    },
    {
      "title": "3 Methods",
      "text": ""
    },
    {
      "title": "_Solution Framework_",
      "text": "Our LLM-based relational triple extraction framework comprises **two stages** (see Figure 2). In the first stage, the LLMs directly extract triples from sentences according to the provided instructions. Subsequently, in the second stage, we design an \"evaluation-filtering\" method, which extracts the positive entity pairs by our evaluation model and uses prompts to inform the LLMs that \"these entity pairs may have certain relations in the relations list\". These candidate pairs will be provided to the LLMs along with the instructions and the first-stage extraction results. LLMs will further scrutinize these candidates and assign appropriate relations based on their language comprehension capabilities, thereby achieving comprehensive and accurate extraction results. An example of the whole workflow is shown in Figure 3."
    },
    {
      "title": "_Basic Idea Of Evaluation Model_",
      "text": "The _evaluation model_ (bottom-right part of Figure 2) uses a sentence (a list containing \\(N\\) tokens) as the input and outputs a token pair evaluation matrix (\\(N*N\\)). Each element in the matrix is an evaluation score for a token pair. The evaluation score of a token pair \\(t_{i}\\) and \\(t_{j}\\) is used to compose the evaluation score of an entity pair \\((s,o)\\), where \\(s\\) contains \\(t_{i}\\) and \\(o\\) contains \\(t_{j}\\). Obviously, for an input sentence, no matter how many candidate pairs are to be evaluated, only one inference is needed to get the evaluation matrix. Our goal is to build such a model that scores candidate entity pairs based on the sentence from which the triples are extracted, as Problem 1 shows. Clearly, this evaluation model could be used as a filter, removing the extracted candidate entity pairs with low scores while retaining those with high scores. After this filtering process, we can obtain a set of precise and complete positive samples (i.e., truly related entity pairs), which can then be supplied to LLMs as prompts to facilitate high-precision extraction of multiple relational triples. **Problem 1** (Evaluation of entity pairs): _Given a sentence \\(T\\) and a candidate entity pair set \\(C\\), the evaluation model outputs a score \\(F(s,o)\\) for each pair \\((s,o)\\in C\\)._ Moreover, in order to overcome the noisy entity problem, we use token-level representation to support any possible entities in the sentence. Rationality for providing entity pairsNote that we only evaluate entity pairs \\((s,o)\\) and provide them to LLMs, ignoring the predicate \\(p\\). The rationality of ignoring the predicate is as follows. First, we find that in most real datasets, the entity pairs are more accurate than predicates in a labeled sample1. Second, the model structure based on entity pairs evaluation is more straightforward. It only needs to generate one evaluation matrix for a sentence, while the evaluation model for entire \\((s,p,o)\\) triples requires to generate k matrices, where k represents the number of relations contained within the sentence. Footnote 1: E.g., in the NYT11 training set, there are 20% wrong triples, but only 6% wrong entity pairs. Rationality of token based representationOur model aims at evaluating any candidate entity pairs in the sentences. However, extracting the entity span accurately is still a problem [11, 12]. For example, \"_Gates and Steve_\" might be wrongly identified as an entity (in Figure 4). Thus, it is necessary to evaluate the candidate \"entities\" represented by arbitrary tokens."
    },
    {
      "title": "Self Labeling",
      "text": "The evaluation model has to distinguish between correct entity pairs and wrong entity pairs, which is a binary classification task, thus we need positive and negative training samples. In original extraction datasets, a sentence is labeled with some triples, which correspond to some entity pairs with one of the target relations. Obviously, these entity pairs are positive samples \\((y=1)\\). Then, it is important to obtain negative samples. We generate negative samples with the following assumption: **Assumption 1**: _If a labeled sentence contains multiple triples, which involve multiple entities, then the unlabeled entity pairs are negative samples._ Rationality of Assumption 1Assumption 1 will generate a false negative entity pair \\((e_{1},e_{2})\\)_only_ when all the four conditions are satisfied (\"** means any relations or entities): * \\((e_{1},*,e_{2})\\) is mentioned in the sentence. * Any triples \\((e_{1},*,e_{2})\\) are not labeled in the sentence. Figure 3: An example of the workflow of our Evaluation-Filtering method. Figure 2: Model framework. On the bottom left is an arbitrary entity-extraction model. On the bottom right is our evaluation model, which outputs a token pair scoring matrix. * Triple \\((e_{1},*,*)\\) or \\((*,*,e_{1})\\) is labeled in the sentence. * Triple \\((e_{2},*,*)\\) or \\((*,*,e_{2})\\) is labeled in the sentence. However, it is seldom the case that the four conditions are simultaneously met. The false negative case means that an annotator (no matter whether it is distant supervision via knowledge base or hand annotation via human) labels other triples in a sentence for both \\(e_{1}\\) and \\(e_{2}\\), but only misses the relation between them. Token-level labelingThe above process generates entity pair samples, and the labeled token pairs can be simply and effectively obtained. For example, in Figure 4, we simply split the negative entity pair _(Microsoft, Steve Jobs)_ into token pairs _(Microsoft, Steve)_ and _(Microsoft, Jobs)_, which are labeled as negative token pairs. This process not only increases the number of training pairs, but also enables the model to evaluate unseen or wrong entities, or even any token sequence. Note that, for any other token pairs in the sentence (e.g. _(founders, Microsoft)_), they are not labeled as negative or positive \\((y=0)\\). They will be masked in the training process of our evaluation model since we have no information about whether they are positive or negative."
    },
    {
      "title": "Evaluation Model Structure",
      "text": "Following the Transformer architecture, our evaluation model adopts a BERT-based encoder and an attention-like 2-dim decoder (as shown in the right part of Figure 2)."
    },
    {
      "title": "3.4.1 Encoder",
      "text": "We use a regular Transformer model as our encoder. Specifically, we use BERT [11] for English and RoBERTa [10] for Chinese. They have the same network structure. More formally, for an input sentence with \\(N\\) tokens \\([t_{1},t_{2},...,t_{N}]\\), where \\(t_{1}=[CLS]\\) and \\(t_{N}=[SEP]\\) are fixed special tokens, the BERT encoder converts these tokens into hidden vectors \\([\\mathbf{h}_{1},\\mathbf{h}_{2},...,\\mathbf{h}_{N}]\\), where each \\(\\mathbf{h}_{i}\\) is a \\(d_{1}\\)-dimension vector. In the BERT-base structure, \\(d_{1}=768\\)."
    },
    {
      "title": "3.4.2 2-Dim Decoder",
      "text": "For an input sentence with \\(N\\) tokens, the BERT-based encoder encodes the tokens into \\(N\\) vectors \\([\\mathbf{h}_{1},\\mathbf{h}_{2},...,\\mathbf{h}_{N}]\\). Then, following the computation of the attention matrix in Transformers, we use a one-head self-attention to compute the 2-dim attention matrix as the output of the decoder. In detail, we first use two linear layers to convert the vectors \\(\\mathbf{h}_{i}\\) to \\(d_{2}\\)-dimension vectors \\(\\mathbf{q}_{i}\\) and \\(\\mathbf{k}_{i}\\): \\[\\begin{split}\\mathbf{q}_{i}&=\\mathbf{W}^{(a)} \\mathbf{h}_{i}+\\mathbf{b}^{a},\\\\ \\mathbf{k}_{i}&=\\mathbf{W}^{(k)}\\mathbf{h}_{i}+ \\mathbf{b}^{k},\\end{split} \\tag{1}\\] where \\(\\mathbf{W}\\) and \\(\\mathbf{b}\\) are trainable parameters of the linear layers, and \\(d_{2}=64\\). Then, we compute their scaled dot-product attention as the output: \\[A_{ij}=\\mathbf{q}_{i}^{T}\\mathbf{k}_{j}/\\sqrt{d_{2}}. \\tag{2}\\] As proposed by Roformer [13], it is advantageous to add relative position embeddings (RoPE) before computing the attention output. The relative position embeddings \\(\\mathbf{R}_{i}\\) are realized by constructing sine and cosine functions that satisfy \\(\\mathbf{R}_{i}^{T}\\mathbf{R}_{j}=\\mathbf{R}_{j-i}\\), we refer the readers to [13] for technical details. The intuition is that when encoding positional information \\(R_{i}\\) and \\(R_{j}\\) at position \\(i\\) and \\(j\\), the output attention will naturally contain the relative positional information. The final form of the attention output is: \\[\\begin{split} A_{ij}^{\\prime}&=(\\mathbf{R}_{i} \\mathbf{q}_{i})^{T}(\\mathbf{R}_{j}\\mathbf{k}_{j})/\\sqrt{d_{2}}\\\\ &=\\mathbf{q}_{i}^{T}\\mathbf{R}_{j-i}\\mathbf{k}_{j}/\\sqrt{d_{2}}.\\end{split} \\tag{3}\\] Recall that this decoder (without the relative position embeddings) is only a part of regular one-head self-attention, although it has \\(O(N^{2})\\) outputs, its computational cost is smaller than the Transformer-based encoder. Hence, the cost of training such an evaluation model is lower than a Transformer-based extraction model."
    },
    {
      "title": "Loss Function",
      "text": "Since our task is a classification task with positive and negative labels, we use the binary cross-entropy loss function to train our evaluation model: \\[L=-\\sum_{i,j:y_{ij}=1}\\log{(\\sigma(A_{ij}^{\\prime}))}-\\sum_{i,j:y_{ij}=-1}\\log{ (1-\\sigma(A_{ij}^{\\prime}))}, \\tag{4}\\] Figure 4: This sentence contains 6 entity pairs, but only 2 pairs are positive. where \\(y_{ij}\\) is the label of the token pair \\((t_{i},t_{j})\\), and \\(\\sigma\\) is the sigmoid function. Note that, our task is not a pure binary classification task, since there are many unlabeled token pairs in our task. Thus, the positive and negative examples are not complementary. In the implementation of the loss, we ignore the part of unlabeled pairs (i.e. \\(y=0\\))."
    },
    {
      "title": "_Candidate Pairs Evaluation_",
      "text": "After training such an evaluation model, we adapt the model to an existing extraction method to obtain better extraction results. We score each candidate extracted pairs \\((s,o)\\), where \\(s=[t_{s_{st}},t_{s_{st}+1},...,t_{s_{ed}}]\\) and \\(o=[t_{o_{st}},t_{o_{st}+1},...,t_{o_{ed}}]\\) are sub-token-sequences in the given sentence. Recall that, our evaluation model outputs a token pair evaluation matrix \\(A^{\\prime}\\). Based on this matrix, we compute the score between \\(s\\) and \\(o\\) by the mean of the matching scores of their tokens: \\[F(s,o)=\\frac{\\sum_{k=s_{st}}^{s_{ed}}\\sum_{l=o_{st}}^{o_{ed}}A^{\\prime}_{kl}}{ (s_{ed}-s_{st}+1)(o_{ed}-o_{st}+1)} \\tag{5}\\] where \\(s_{st}\\) and \\(o_{st}\\) are the indexes of the first element of token lists \\(s\\) and \\(o\\), \\(s_{ed}\\) and \\(o_{ed}\\) are the indexes of the last element of \\(s\\) and \\(o\\). Finally, only \\((s,o)\\) satisfying \\(F(s,o)>0\\) will be added to the result. Note that, we only need to predict the matrix \\(A^{\\prime}\\) once for each sentence, no matter how many triples of this sentence should be evaluated, thus the evaluation process is efficient."
    },
    {
      "title": "_Parameter-Efficient Fine-Tuning For Llms_",
      "text": "For multiple relational triple extractions, employing instruction-tuning or in-context learning (ICL) to guide LLMs, as is done for general tasks, often yields unsatisfactory results. This is because LLMs possess strong generalization capabilities and language comprehension, leading them to inexactly recognize the span of entities or relations. For instance, they may extract predicates not presented in the predicate list, or consider book titles as part of an entity, even when their extraction range is explicitly limited through prompts. Consequently, parameter fine-tuning is necessary to adapt the model to the corresponding datasets and to potentially non-natural language representations of predicates (e.g. NYT10 dataset). In this paper, we mainly adopt the LoRA technology [11]. LoRA is a parameter-efficient fine-tuning (PEFT) method. It freezes the large-scale parameters of a pre-trained model and simulates parameter changes through low-rank decomposition of the matrix, thereby adapting the model to downstream tasks with small-scale parameter adjustments. Compared to full-parameter fine-tuning, this method is more time-efficient and requires less computing resources and storage."
    },
    {
      "title": "_Instruction Template_",
      "text": "To better guide LLMs in performing multiple relational triple extraction tasks, we design an instruction template that explicitly includes the task description, the restricted range of extracted predicates, the output format, and other requirements. We also explicitly require the model to extract as many relation triples as possible. Additionally, for the original large model without PEFT, a complete input-output example can also be placed after the instruction template for in-context learning. After the evaluation model extracts candidate entity pairs, these candidates will be provided to the LLMs as part of the prompt, along with the aforementioned instructions and the first-stage extraction results, to guide the model in completing the extraction. See Appendix C for detailed prompts."
    },
    {
      "title": "4 Experiments",
      "text": ""
    },
    {
      "title": "_Datasets_",
      "text": "We evaluate our methods on several public and downloadable complex extraction datasets, including NYT series [14, 15], Wiki-KBP [13], SKE21 [16] and HacRED [15], which are challenging for many ex \\begin{table} \\begin{tabular}{c c c c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Dataset} & \\multicolumn{3}{c}{\\(|T|>=0\\)} & \\multicolumn{3}{c}{\\(|T|>=00\\)} & \\multicolumn{3}{c}{\\(|T|>=50\\)} & \\multicolumn{3}{c}{\\(|T|>=70\\)} & \\multicolumn{3}{c}{\\(|T|>=100\\)} \\\\ \\cline{2-13} & avgE & avgH & \\#sen & avgE & avgH & \\#sen & avgH & \\#sen & avgE & avgH & \\#sen & avgE & avgR & \\#sen \\\\ \\hline NYT10 & 2.2 & 1.7 & 5000 & 2.2 & 1.8 & 4091 & 2.2 & 1.8 & 1798 & 2.3 & 1.9 & 441 & 2.3 & 2.1 & 51 \\\\ NYT11+HPL & 2.0 & 1.0 & 369 & 2.0 & 1.0 & 283 & 2.0 & 1.0 & 120 & 2.0 & 1.0 & 28 & 2.0 & 1.0 & 3 \\\\ SKE21 & 3.3 & 2.4 & 1150 & 3.5 & 2.6 & 901 & 3.9 & 3.0 & 423 & 3.9 & 3.0 & 202 & 4.0 & 3.2 & 80 \\\\ WikiKBP & 2.1 & 1.1 & 182 & 2.2 & 1.2 & 98 & 2.1 & 1.0 & 25 & 2.3 & 1.2 & 6 & - & - \\\\ \\hline \\hline \\multirow{2}{*}{Dataset} & \\multicolumn{3}{c}{\\(|T|>=50\\)} & \\multicolumn{3}{c}{\\(|T|>=100\\)} & \\multicolumn{3}{c}{\\(|T|>=150\\)} & \\multicolumn{3}{c}{\\(|T|>=200\\)} & \\multicolumn{3}{c}{\\(|T|>=250\\)} \\\\ \\cline{2-13} & avgE & avgH & \\#sen & avgE & avgH & \\#sen & avgE & avgH & \\#sen & avgE & avgR & \\#sen & avgR & \\#sen \\\\ \\hline HacRED & 7.1 & 7.4 & 1500 & 7.4 & 7.7 & 1372 & 8.2 & 8.8 & 1012 & 9.1 & 10.0 & 693 & 10.2 & 11.4 & 410 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: The statistics of complex sentences of testing datasets. \\(|T|\\) means the number of tokens in the sentences. \\(|T|>=x\\) only reports results for sentences with at least \\(x\\) tokens. \\(avgE\\), \\(avgR\\) denote the average numbers of labeled entities, labeled triples in the sentence, respectively. \\(\\#sen\\) denotes the number of sentences. [MISSING_PAGE_FAIL:7] model to extract triples from sentences of different complexity levels. The results show that as the number of triples within a sentence increases, our model demonstrates a progressively noticeable improvement in the recall of relational triple extraction results, compared to the base model. Moreover, it can maintain the F1 score of the results at a relatively high level. This suggests that our method is particularly effective for extracting multiple relational triples from complex sentences, and it can sustain a high level of precision of results. In addition, the results of the small extraction model in Table 4 show that our method achieves a large precision improvement with a small recall decline, which leads to a better F1 score. This indicates that our evaluation model can accurately and reliably obtain candidate pairs, which can be applied to the traditional small extraction model to improve the performance of multiple relational triple extraction."
    },
    {
      "title": "Further Analysis",
      "text": "In Table 5, we try three ablation settings. First, we remove the second stage of the framework, that is, the LLMs extraction part after receiving the candidate pairs prompt. Instead, we incorporated an additional relation classification model prior to the evaluation model. In other words, we use a small extraction model (ReReXie et al., 2021) to extract triples, which are then filtered according to the evaluation model. The filtered triples are combined with the first-stage LLMs' extraction results as the final results. The results indicate that the omission of the LLMs in the second stage leads to a decrease in the precision and F1 score of triple extraction results. Therefore, a large model in the second stage is still necessary for judgment and relation identification. Second, we remove the first stage of the framework, that is, when inputting instructions and original sentences, we also provide the LLMs evaluation-filtering prompt, which will strictly limit the scope of triple extraction to the candidates provided by the evaluation model. The results show that our model can still enhance the recall rate of multiple triple extraction, but less effectively compared to the complete framework. This could be attributed to the presence of positive entity pairs that the evaluation model fails to recognize. However, without stringent restrictions, LLMs are capable of identifying and retaining these results. Third, we remove the filtering step in the framework, that is, directly provide all entity pairs recognized by the entity-extraction model as prompts to the LLMs. The results show that the precision and F1 score of extraction results significantly decrease. This suggests that our evaluation-filtering method is indispensable. \\begin{table} \\begin{tabular}{l c c c c c c c c c c c c} \\hline \\hline & \\multicolumn{3}{c}{NYT10} & \\multicolumn{3}{c}{NYT10-HRL} & \\multicolumn{3}{c}{SKE21} & \\multicolumn{3}{c}{HacRED} \\\\ \\cline{2-13} & Prec. & Reca. & F1 & Prec. & Reca. & F1 & Prec. & Reca. & F1 & Prec. & Reca. & F1 \\\\ \\hline TPLinker & 84.96 & 89.66 & 87.25 & 74.31 & 61.06 & 67.04 & 72.73 & 77.94 & 75.24 & 54.64 & 61.21 & 57.74 \\\\ + Ours & 86.87 & 89.36 & **88.10** & 74.79 & 60.86 & **67.11** & 81.31 & 77.63 & **79.43** & 61.78 & 59.04 & **60.38** \\\\ \\hline CasRel & 83.82 & 87.63 & 85.60 & 70.25 & 65.51 & 67.58 & 84.24 & 67.50 & 74.95 & 62.62 & 34.62 & 44.59 \\\\ + Ours & 88.23 & 87.40 & **87.81** & 72.35 & 54.88 & **68.41** & 84.89 & 67.42 & 78.15 & 69.48 & 34.16 & **45.82** \\\\ \\hline ReRe & 81.28 & 89.16 & 85.04 & 68.66 & 63.77 & 66.12 & 81.01 & 82.15 & 81.58 & 46.42 & 61.37 & 52.86 \\\\ + Ours & 87.03 & 88.80 & **87.90** & 71.32 & 63.61 & **67.42** & 83.44 & 81.68 & **82.55** & 69.92 & 59.37 & **64.21** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: The main evaluation results of different small models. We only report the results for sentences with at least 50 tokens. Best exact match F1 scores are marked **bold**. Figure 5: Recall and F1-score curve of Owen-7B (w/ pleft) on NYT10, with and without our evaluation-filtering method. Minimum # of triples means we only consider sentences that contain a number of triples greater than this value. Note that the coordinates do not start from 0. \\begin{table} \\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Models (w/ pleft)} & \\multicolumn{3}{c}{SKE21 (\\(l\\)=7)} & \\multicolumn{3}{c}{NYT11-HRL (\\(l\\)=3)} \\\\ \\cline{2-7} & Prec. & Reca. & F1 & Prec. & Reca. & F1 \\\\ \\hline Owen-7B + Ours & 68.32 & 74.46 & **71.26** & 60.76 & 60.55 & **60.86** \\\\ wo stage 2 & 64.69 & 75.01 & 69.47 & 57.38 & 62.62 & 62.37 & 57.73 \\\\ wo stage 1 & 27.77 & 68.86 & 70.76 & 63.33 & 50.67 & 56.30 \\\\ wo parts filtering & 65.12 & 66.04 & 65.57 & 58.86 & 59.05 & 58.96 \\\\ \\hline Lima-13D + Ours & 53.79 & 26.06 & 36.88 & 52.37 & 60.00 & 55.92 \\\\ wo stage 2 & 40.23 & 41.19 & **40.70** & 36.38 & 62.20 & 45.91 \\\\ wo stage 1 & 63.17 & 25.06 & 35.67 & 68.11 & 52.00 & **58.97** \\\\ wo pairs filtering & 44.75 & 27.25 & 33.87 & 37.76 & 49.33 & **42.77** \\\\ \\hline Vicuna-13D + Ours & 71.49 & 56.83 & **63.33** & 58.82 & 57.14 & **57.97** \\\\ wo stage 2 & 62.56 & 59.90 & 62.92 & 44.00 & 61.33 & 51.24 \\\\ wo stage 1 & 29.72 & 36.06 & 51.93 & 70.91 & 41.67 & 52.49 \\\\ wo pairs filtering & 63.97 & 57.67 & 60.66 & 36.28 & 55.18 & 40.82 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: The ablation experiments results of different LLMs. We only report the results for sentences containing at least \\(t\\) triples. Best exact match F1 scores are marked **bold**. Conclusion In this paper, we propose an evaluation model that can act as a filter to assess and identify entity pairs that have relations, thereby providing high-precision candidates for the subsequent extraction process. We incorporate this evaluation model into our proposed evaluation-filtering framework for LLMs multiple relation triple extraction. The candidates filtered by the evaluation model are integrated into the extraction process of LLMs in the form of prompts. This effectively addresses the issue of low recall rate in triple extraction tasks performed by LLMs, without diminishing precision. The experimental results that derived from multiple LLMs and datasets validate the effectiveness and completeness of our framework. Additionally, we confirm that our evaluation model can also be implemented in traditional small extraction models to enhance their precision and F1 score."
    },
    {
      "title": "Acknowledgements",
      "text": "This work was supported by Chinese NSF Youth Fund (No. 62102095), Major Research Plan (No. 92270121), and Shanghai Science and Technology Innovation Action Plan (No.21511100401).The computations in this research were performed using the CFFF platform of Fudan University."
    },
    {
      "title": "Limitations",
      "text": "Extraction PerformanceDespite the effectiveness of our model, the overall extraction results may still miss some correct triples and contain errors. On the one hand, a small amount of related entity pairs are not correctly evaluated by the evaluation model. On the other hand, it is difficult for LLMs to completely avoid errors or omissions in the second stage, although we prompt them to pick the correct candidate pairs and recheck the original results. Subsequent research could explore the optimization of the evaluation model, as well as further improvements in the extraction precision and recall of the model collaboration approach. Complexity of Our MethodOur framework involves multiple components and requires the LLMs to perform extraction twice. Our method is more complicated and more time-consuming with the direct application of LLMs, and its inference time roughly doubled. To obtain stable effect improvement when dealing with complex sentences, our method is more suitable, while for simple extraction tasks, we suggest single-stage direct extraction."
    },
    {
      "title": "References",
      "text": "* A. Agarwal, S. Hegselmann, H. Lang, Y. Kim, and D. Sontag (2022)Large language models are zero-shot clinical information extractors. arXiv preprint arXiv:2205.12689. Cited by: SS1. * J. U. Allingham, J. Ren, M. W. Dusenberry, X. Gu, Y. Cui, D. Tran, J. Z. Liu, and B. Lakshminarayanan (2023)A simple zero-shot prompt weighting technique to improve prompt ensembling in text-image models. In Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research, pp. 547-568. Cited by: SS1. * J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. (2023)Qwen technical report. arXiv preprint arXiv:2309.16609. Cited by: SS1. * T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020)Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1. * Q. Cheng, J. Liu, X. Qu, J. Zhao, J. Liang, Z. Wang, B. Huai, N. Jing Yuan, and Y. Xiao (2021)HacRED: a large-scale relation extraction dataset toward hard cases in practical applications. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online, pp. 2819-2831. Cited by: SS1. * Z. Chu, H. Jiang, Y. Xiao, and W. Wang (2020)Insrl: a multi-view learning framework fusing multiple information sources for distantly-supervised relation extraction. arXiv preprint arXiv:2012.09370. Cited by: SS1. * H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al. (2022)Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Cited by: SS1. * J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019)BERT: pre-training of deep bidirectional transformers for language understanding. Minnesota. Association for Computational Linguistics. * Dixit and Al-Onaizan (2019) Kalpit Dixit and Yaser Al-Onaizan. 2019. Span-level model for relation extraction. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 5308-5314, Florence, Italy. Association for Computational Linguistics. * Ellis et al. (2012) Joe Ellis, Xuansong Li, Kira Griffitt, Stephanie M Strassel, and Jonathan Wright. 2012. Linguistic resources for 2013 knowledge base population evaluations. In _TAC_. * Gao et al. (2023) Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023. Exploring the feasibility of chatgpt for event extraction. _arXiv preprint arXiv:2303.03836_. * Hoffmann et al. (2011) R. Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In _Proceedings of ACL_. * Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_. * Jelbick et al. (2023) Katharina Jelbick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa Stuber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Oliver Sabel, Jens Ricke, et al. 2023. Chatgpt makes medicine easy to swallow: an exploratory case study on simplified radiology reports. _European Radiology_, pages 1-9. * Ji et al. (2020) Bin Ji, Jie Yu, Shasha Li, Jun Ma, Qingbo Wu, Yusong Tan, and Huijun Liu. 2020. Span-based joint entity and relation extraction with attention-based span-specific and contextual semantic representations. In _Proceedings of the 28th International Conference on Computational Linguistics_, pages 88-99, Barcelona, Spain (Online). International Committee on Computational Linguistics. * Jiang et al. (2023) Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023. LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 14165-14178, Toronto, Canada. Association for Computational Linguistics. * Jiang et al. (2020) Haiyun Jiang, JunTao Liu, Sheng Zhang, Deqing Yang, Yanghua Xiao, and Wei Wang. 2020. Surface pattern-enhanced relation extraction with global constraints. _Knowledge and Information Systems_, 62(12):4509-4540. * Lang et al. (2022) Hunter Lang, Monica N Agrawal, Yoon Kim, and David Sontag. 2022. Co-training improves prompt-based learning for large language models. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 11985-12003. PMLR. * Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 19274-19286. PMLR. * Li et al. (2019) Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019. Entity-relation extraction as multi-turn question answering. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 1340-1350, Florence, Italy. Association for Computational Linguistics. * Ling and Weld (2012) Xiao Ling and Daniel S Weld. 2012. Fine-grained entity recognition. In _Twenty-Sixth AAAI Conference on Artificial Intelligence_. * Liu et al. (2017) Liyuan Liu, Xiang Ren, Qi Zhu, Shi Zhi, Huan Gui, Heng Ji, and Jiawei Han. 2017. Heterogeneous supervision for relation extraction: A representation learning approach. _arXiv preprint arXiv:1707.00166_. * Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_. * OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. * Ren et al. (2017) Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R Voss, Heng Ji, Tarek F Abdelzaher, and Jiawei Han. 2017. Cotype: Joint extraction of typed entities and relations with knowledge bases. In _Proceedings of the 26th International Conference on World Wide Web_, pages 1015-1024. * Riedel et al. (2010) Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 148-163. Springer. * Sainz et al. (2022) Oscar Sainz, Haoling Qiu, Oier Lopez de Lacalle, Eneko Agirre, and Bonan Min. 2022. ZS4IE: A toolkit for zero-shot information extraction with simple verbalizations. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations, Hybrid: Seattle, Washington + Online, pp. 27-38. External Links: ISBN 978-0-030-0188-1, Link, Document Cited by: SS1. * J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu (2021)Roformer: enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864. Cited by: SS1. * R. Takanobu, T. Zhang, J. Liu, and M. Huang (2019)A hierarchical framework for relation extraction with reinforcement learning. In Proceedings of AAAI, Vol. 33, pp. 7072-7079. Cited by: SS1. * R. Tang, X. Han, X. Jiang, and X. Hu (2023)Does synthetic data generation of lms help clinical text mining?. arXiv preprint arXiv:2303.04360. Cited by: SS1. * H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. (2023)LIMA: open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Cited by: SS1. * H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. (2023)LIMA 2: open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Cited by: SS1. * S. Wadhwa, S. Amir, and B. Wallace (2023)Revisiting relation extraction in the era of large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, pp. 15566-15589. External Links: Link, Document Cited by: SS1. * Y. Wang, B. Yu, Y. Zhang, T. Liu, H. Zhu, and L. Sun (2020)TPLinker: single-stage joint extraction of entities and relations through token pair linking. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 1572-1582. External Links: Link, Document Cited by: SS1. * J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. Wei Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le (2021)Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652. Cited by: SS1. * X. Wei, X. Cui, N. Cheng, X. Wang, X. Zhang, S. Huang, P. Xie, J. Xu, Y. Chen, M. Zhang, et al. (2023)Zero-shot information extraction via chatting with chatopt. arXiv preprint arXiv:2302.10205. Cited by: SS1. * Z. Wei, J. Su, Y. Wang, Y. Tian, and Y. Chang (2020)A novel cascade binary tagging framework for relational triple extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online, pp. 1476-1488. External Links: Link, Document Cited by: SS1. * C. Xie, J. Liang, J. Liu, C. Huang, W. Huang, and Y. Xiao (2021)Revisiting the negative data of distantly supervised relation extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online, pp. 3572-3581. External Links: Link, Document Cited by: SS1. * C. Xu, Y. Xu, S. Wang, Y. Liu, C. Zhu, and J. McAuley (2023)Small models are valuable plug-ins for large language models. arXiv preprint arXiv:2305.08848. Cited by: SS1. * B. Yu, Z. Zhang, X. Shu, Y. Wang, T. Liu, B. Wang, and S. Li (2020)Joint extraction of entities and relations based on a novel decomposition strategy. In Proceedings of ECAI, Cited by: SS1. * C. Yuan, Q. Xie, and S. Ananiadou (2023)Zero-shot temporal relation extraction with chatGPT. In The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, Toronto, Canada, pp. 92-102. External Links: Link, Document Cited by: SS1. * L. Zheng, W. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. (2023)Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685. Cited by: SS1. Dataset Introduction NYT seriesNYT is based on the articles in New York Times. There are many derived datasets with better labeling. NYT10 (Riedel et al., 2010) and NYT11 (Hoffmann et al., 2011) label the complete entities. Moreover, NYT10-HRL and NYT11-HRL (Takanobu et al., 2019) are better versions that are processed by optimizing the relation labels. HacREDHacRED (Cheng et al., 2021)2 is a novel challenging extraction dataset. It analyzes the performance gap between popular datasets and practical applications, and carefully selects and designs more hard cases. HacRED consists of 65,225 relational facts annotated from 9,231 wiki documents with sufficient and diverse hard cases, which poses a very high challenge to many current complex extraction methods. Footnote 2: [https://github.com/qiaojim/hacred](https://github.com/qiaojim/hacred) Ske21Ske193 is published by Baidu, and is currently the largest dataset available for complex relational triple extraction. Since its testing set is unpublished, and there are some errors in the validation set, a version named SKE21 is published by Xie et al. (2021). The testing set of SKE21 is carefully manually relabeled and contains 1,150 sentences and 2,765 annotated triples. Footnote 3: [http://ai.baidu.com/broad/download?dataset=sked](http://ai.baidu.com/broad/download?dataset=sked) Wiki-KBPWiki-KBP (Ling and Weld, 2012) is based on the articles in Wikipedia. There're 1.5M sentences in training set which are automatically labeled using distant supervision and handcrafted patterns by (Liu et al., 2017), and the test set contains 289 sentences selected by the author of (Ren et al., 2017) from the manual annotations in 2013 KBP slot filling results (Ellis et al., 2012)."
    },
    {
      "title": "Appendix B Experiment Details",
      "text": "Our experiments are conducted on two A800 GPUs. All deep models, including the LLMs and the evaluation model, are fine-tuned or implemented using the PyTorch framework. We employed AdamW optimizer as the optimizer. For the evaluation model, we first initialize the model with bert-base-cased and chinese-roberta-wwm-ext respectively, then train 20 epochs in English corpus task, and 40 epochs in Chinese. For the fine-tuning of LLMs, we randomly select 1500 items from the train set for each dataset and train 30 epochs. Our codes and hyper-parameters can be found at [https://github.com/Ding-Papa/Evaluating-filtering-coling24](https://github.com/Ding-Papa/Evaluating-filtering-coling24)."
    },
    {
      "title": "Appendix C Instruction Template",
      "text": "Here we provide the instruction templates that guide the LLMs for relational triple extraction. First is the template for directly using the LLMs to perform extraction, i.e., the first stage of our method. **Template for the first stage**: Pre-define the following relation list r, please extract all triples containing the above relations from the given sentence \\(S\\). Note that the relation name of the triple must be selected from the above list, and other relations not listed are not considered. Please output according to the specified format: [(\"s\": subject1, \"o\": object1, \"p\": relation1), {\"s\": subject2, \"o\": object2, \"p\": relation2},...] (Optional) Here are some examples:... Now given the following input, please complete the extracting task. Please output as many triples as possible that meet the requirements. Input:\\(S_{i},r\\) In the second stage, the input of the LLMs consists of the first-stage extraction results and candidate pairs extracted by the evaluation model. The LLMs are prompted to recheck the original results, assign relations to the appropriate candidate entity pairs, and output the final extracted triples to complete the extraction. **Template for the second stage**: Pre-define the following relation list r. We want to extract all triples containing the above relations from the given sentence \\(S\\). Here are the original extraction results \\(A\\). Now we claim that the entity pairs that may be related in the above sentence are \\((s_{1},o_{1}),(s_{2},o_{2}),...\\) Please check the original results and fill in the missing triples, remove the wrong triples and output the final results. Constraints and output format are the same as stage 1. Please output according to the specified format."
    }
  ]
}