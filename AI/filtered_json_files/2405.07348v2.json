{
  "title": "MedConceptsQA: Open Source Medical Concepts QA Benchmark",
  "authors": [
    "Ben Ofir",
    "Shoham",
    "Nadav Rappoport"
  ],
  "abstract": "\n We present MedConceptsQA, a dedicated open source benchmark for medical concepts question answering. The benchmark comprises of questions of various medical concepts across different vocabularies: diagnoses, procedures, and drugs. The questions are categorized into three levels of difficulty: easy, medium, and hard. We conducted evaluations of the benchmark using various Large Language Models. Our findings show that pre-trained clinical Large Language Models achieved accuracy levels close to random guessing on this benchmark, despite being pre-trained on medical data. However, GPT-4 achieves an absolute average improvement of nearly 27%-37% (27% for zero-shot learning and 37% for few-shot learning) when compared to clinical Large Language Models. Our benchmark serves as a valuable resource for evaluating the understanding and reasoning of medical concepts by Large Language Models. Our benchmark is available at  https://huggingface.co/datasets/ofir408/MedConceptsQA   \n",
  "references": [
    {
      "id": null,
      "title": "MedConceptsQA: Open Source Medical Concepts QA Benchmark",
      "authors": [
        "Ben Ofir",
        "Shoham",
        "Nadav Rappoport"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "A survey of large language models",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Challenges and applications of large language models",
      "authors": [
        "Jean Kaddour",
        "Joshua Harris",
        "Maximilian Mozes",
        "Herbie Bradley",
        "Roberta Raileanu",
        "Robert Mchardy"
      ],
      "year": "2023",
      "venue": "Challenges and applications of large language models",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Meditron-70b: Scaling medical pretraining for large language models",
      "authors": [
        "Zeming Chen",
        "Alejandro Hernández Cano",
        "Angelika Romanou",
        "Antoine Bonnet",
        "Kyle Matoba",
        "Francesco Salvi",
        "Matteo Pagliardini",
        "Simin Fan",
        "Andreas Köpf",
        "Amirkeivan Mohtashami"
      ],
      "year": "2023",
      "venue": "Meditron-70b: Scaling medical pretraining for large language models",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
      "authors": [
        "Yanis Labrak",
        "Adrien Bazoge",
        "Emmanuel Morin",
        "Pierre-Antoine Gourraud",
        "Mickael Rouvier",
        "Richard Dufour"
      ],
      "year": "2024",
      "venue": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine",
      "authors": [
        "Yizhen Luo",
        "Jiahuan Zhang",
        "Siqi Fan",
        "Kai Yang",
        "Yushuai Wu",
        "Mu Qiao",
        "Zaiqing Nie"
      ],
      "year": "2023",
      "venue": "Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "A large language model for electronic health records",
      "authors": [
        "Xi Yang"
      ],
      "year": "2022",
      "venue": "Digital Medicine",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks",
      "authors": [
        "Hyunjae Kim",
        "Hyeon Hwang",
        "Jiwoo Lee",
        "Sihyeon Park",
        "Dain Kim",
        "Taewhoo Lee",
        "Chanwoong Yoon",
        "Jiwoong Sohn",
        "Donghee Choi",
        "Jaewoo Kang"
      ],
      "year": "2024",
      "venue": "Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Cpllm: Clinical prediction with large language models",
      "authors": [
        "Ben Ofir",
        "Nadav Shoham",
        "Rappoport"
      ],
      "year": "2023",
      "venue": "Cpllm: Clinical prediction with large language models",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions",
      "authors": [
        "Yuting He",
        "Fuxiang Huang",
        "Xinrui Jiang",
        "Yuxiang Nie",
        "Minghao Wang",
        "Jiguang Wang",
        "Hao Chen"
      ],
      "year": "2024",
      "venue": "Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "BioASQ-QA: A manually curated corpus for Biomedical Question Answering",
      "authors": [
        "Anastasia Krithara",
        "Anastasios Nentidis",
        "Konstantinos Bougiatiotis",
        "Georgios Paliouras"
      ],
      "year": "2023",
      "venue": "Scientific Data",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "emrQA: A Large Corpus for Question Answering on Electronic Medical Records",
      "authors": [
        "Anusri Pampari",
        "Preethi Raghavan",
        "Jennifer Liang",
        "Jian Peng"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "PubMedQA: A Dataset for Biomedical Research Question Answering",
      "authors": [
        "Qiao Jin",
        "Bhuwan Dhingra",
        "Zhengping Liu",
        "William Cohen",
        "Xinghua Lu ; Kentaro",
        "Jing Inui",
        "Vincent Jiang",
        "Xiaojun Ng",
        "Hong Wan",
        "Kong"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1259"
    },
    {
      "id": "b12",
      "title": "MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations",
      "authors": [
        "Olumide E Ojo",
        "O Olaronke",
        "Alexander Adebanji",
        "Hiram Gelbukh",
        "Anna Calvo",
        "Feldman"
      ],
      "year": "2023",
      "venue": "MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "A corpus for research in text processing for evidence based medicine",
      "authors": [
        "Diego Mollá",
        "Elena María",
        "Abeed Santiago-Martínez",
        "Cécile Sarker",
        "Paris"
      ],
      "year": "2016",
      "venue": "Language Resources and Evaluation",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "CLUE: A Clinical Language Understanding Evaluation for LLMs",
      "authors": [
        "Amin Dada",
        "Marie Bauer",
        "Amanda Butler Contreras",
        "Constantin Osman Alperen Koraş",
        "Kaleb E Marc Seibold",
        "Jens Smith",
        "Kleesiek"
      ],
      "year": "2024",
      "venue": "CLUE: A Clinical Language Understanding Evaluation for LLMs",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Large Language Models Are Poor Medical Coders-Benchmarking of Medical Code Querying",
      "authors": [
        "Ali Soroush",
        "Eyal Benjamin S Glicksberg",
        "Yiftach Zimlichman",
        "Robert Barash",
        "Alexander W Freeman",
        "Girish N Charney",
        "Eyal Nadkarni",
        "Klang"
      ],
      "year": "2024",
      "venue": "AIdbp2300040",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman",
        "Shyamal Anadkat"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Instruction tuning for large language models: A survey",
      "authors": [
        "Shengyu Zhang",
        "Linfeng Dong",
        "Xiaoya Li",
        "Sen Zhang",
        "Xiaofei Sun",
        "Shuhe Wang",
        "Jiwei Li",
        "Runyi Hu",
        "Tianwei Zhang",
        "Fei Wu"
      ],
      "year": "2023",
      "venue": "Instruction tuning for large language models: A survey",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "PyHealth: A Deep Learning Toolkit for Healthcare Predictive Modeling",
      "authors": [
        "Chaoqi Yang",
        "Zhenbang Wu",
        "Patrick Jiang",
        "Zhen Lin",
        "Junyi Gao",
        "Benjamin Danek",
        "Jimeng Sun"
      ],
      "year": "",
      "venue": "Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) 2023. 2023",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "OpenBioLLMs: Advancing Open-Source Large Language Models for Healthcare and Life Sciences",
      "authors": [
        "Malaikannan Sankarasubbu",
        "Ankit Pal"
      ],
      "year": "",
      "venue": "OpenBioLLMs: Advancing Open-Source Large Language Models for Healthcare and Life Sciences",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
      "authors": [
        "Jinhyuk Lee",
        "Wonjin Yoon",
        "Sungdong Kim",
        "Donghyeon Kim",
        "Sunkyu Kim",
        "Chan Ho",
        "So",
        "Jaewoo Kang"
      ],
      "year": "2020",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "A comparative study of pretrained language models for long clinical text",
      "authors": [
        "Yikuan Li",
        "Ramsey M Wehbe",
        "S Faraz",
        "Hanyin Ahmad",
        "Yuan Wang",
        "Luo"
      ],
      "year": "2023",
      "venue": "Journal of the American Medical Informatics Association",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Huggingface's transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Rémi Louf",
        "Morgan Funtowicz"
      ],
      "year": "2019",
      "venue": "Huggingface's transformers: State-of-the-art natural language processing",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Appendix Table 3 presents the comprehensive zero-shot learning results for all models, vocabularies, and difficulty levels (easy, medium, and hard)",
      "authors": [],
      "year": "",
      "venue": "Appendix Table 3 presents the comprehensive zero-shot learning results for all models, vocabularies, and difficulty levels (easy, medium, and hard)",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Zero-shot learning full results for all the models, vocabularies, and levels",
      "authors": [],
      "year": "",
      "venue": "Zero-shot learning full results for all the models, vocabularies, and levels",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Few-shot learning full results for all the models, vocabularies, and levels",
      "authors": [],
      "year": "",
      "venue": "Few-shot learning full results for all the models, vocabularies, and levels",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Medconceptsqa: Open Source Medical Concepts Qa Benchmark",
      "text": "Ofir Ben Shoham Department of Software and Information Systems Engineering Ben-Gurion University of the Negev benshoho@post.bgu.ac.il Nadav Rappoport Department of Software and Information Systems Engineering Ben-Gurion University of the Negev nadavrap@bgu.ac.il"
    },
    {
      "title": "Abstract",
      "text": "We present MedConceptsQA, a dedicated open source benchmark for medical concepts question answering. The benchmark comprises of questions of various medical concepts across different vocabularies: diagnoses, procedures, and drugs. The questions are categorized into three levels of difficulty: easy, medium, and hard. We conducted evaluations of the benchmark using various Large Language Models. Our findings show that pre-trained clinical Large Language Models achieved accuracy levels close to random guessing on this benchmark, despite being pre-trained on medical data. However, GPT-4 achieves an absolute average improvement of nearly 27%-37% (27% for zero-shot learning and 37% for few-shot learning) when compared to clinical Large Language Models. Our benchmark serves as a valuable resource for evaluating the understanding and reasoning of medical concepts by Large Language Models. Our benchmark is available at [https://huggingface.co/datasets/ofir408/MedConceptsQA](https://huggingface.co/datasets/ofir408/MedConceptsQA)"
    },
    {
      "title": "1 Introduction",
      "text": "Large Language Models (LLMs) are trained on huge datasets and comprising billions of parameters. LLMs have demonstrated remarkable effectiveness across a range of language-related tasks, including question-answering and text generation [1]. Their applicability extends beyond traditional language domains to areas such as robotics, cybersecurity, law, and also medicine [2]. Clinical LLMs (CLLMs) are LLMs that are trained on medical datasets. To name a few: Meditron [3], BioMistral [4], BioMedGPT [5], GatorTron [6], and Meerkat [7]. CLLMs have shown their effectiveness to be used in tasks such as clinical text classification [8], medical chatbots, healthcare education, and clinical text generation [9]. Clinical data is frequently represented using standardized medical codes rather than natural language descriptions. Therefore, it's essential that CLLMs understand the meaning of these medical codes and their differences. One way to test the understanding of CLLMs is to evaluate their ability to interpret clinical codes. Existing clinical benchmarks are available for evaluating CLLMs. For example, BioASQ-QA [10] is a manually curated corpus of Biomedical Question Answering (QA) for documented retrieval, text snippets extraction, and summarization. EmrQA [11] is a corpus for QA on Electronic Medical Records based on clinical notes. PubMedQA [12] is a dataset that contains biomedical QA based on PubMed abstracts. In addition to QA datasets, there are clinical datasets in other fields such asclassification [13] and summarization [14; 15]. Soroush et al. [16] used real clinical visit data to provide LLMs with a code description and prompted them to generate a billing code. They found that all general-purpose LLMs tested performed poorly at this task. Notably, they did not evaluate CLLMs that were trained on medical data and they only used zero-shot. We used a different approach for evaluation: our proposed benchmark includes a broader set of questions and answers relating to medical concepts and also the differences between them, spanning various levels of difficulty. Furthermore, our evaluation encompasses not only general-purpose LLMs but also CLLMs specialized for the medical domain. Unlike their focus on a single hospital site, our medical coding evaluation covers a wider scope and also contains drugs medical concepts. Additionally, our evaluation also incorporates few-shot learning learning and not only zero-shot learning. Moreover, Soroush et al. generated code from natural language descriptions (a task that is currently performed by human coders), while our study evaluates the \"clinical understanding\" of LLMs by investigate the inverse direction: selection the right description given a clinical code. In contrast to their approach, which assessed the capability of models to potentially replace human coders, our goal is to provide a more general estimation of the clinical reasoning abilities of LLMs. In this study, we introduce MedConceptsQA, a benchmark for evaluating understanding and reasoning abilities in the medical domain. MedConceptsQA comprises over 800,000 questions and answers covering medical concepts, including ICD10 and ICD9 diagnoses codes, ICD9-PROC and ICD10-PROC procedures codes, and ATC drug codes. The benchmark includes questions categorized into three difficulty levels: easy, medium, and hard. In addition, we evaluate different LLMs on the MedConceptsQA benchmark using zero-shot and few-shot learning. Surprisingly, we find that the current state-of-the-art CLLMs, as well as general LLMs except two, achieve performance levels comparable to Random guessing. Even medical fine-tuned models were not better than Random guessing. However, GPT-4 [17] outperforms all medical LLMs, despite its primary focus being a general LLM rather than specifically tailored to the medical domain like the others. **Contributions**: In summary, we make three main contributions: 1. We propose a challenging open-source benchmark for evaluating LLMs on their understanding and reasoning of medical concepts. Additionally, our evaluation code is also open-source and available for any use. 2. Due to our large number of examples in the benchmark (more than 800,000), our benchmark can be utilized for training LLMs to comprehend medical concepts and the distinctions between them, making it suitable for techniques such as instruction-tuning [18]. 3. We demonstrate that GPT-3.5 and GPT-4 outperform the current state-of-the-art CLLMs in understanding and reasoning about medical concepts, despite CLLMs being specifically designed for the medical domain."
    },
    {
      "title": "2 Medconceptsqa Benchmark",
      "text": "The MedConceptsQA benchmark consists of multiple question-answering medical concepts datasets, categorized into three vocabularies: Diagnoses, Procedures, and Drugs. Within the Diagnoses category, it includes questions related to ICD9-CM and ICD10-CM medical codes. For Procedures, it covers ICD9-PROC and ICD10-PROC codes, and ATC for Drugs. Each vocabulary is further divided into three levels of difficulty: easy, medium, and hard. Table 1 illustrates the distribution of questions within the benchmark. Each question is about a single medical code and contains four optional answers. The options are four descriptions of medical codes, where only one is the true description of the given medical codes, and the others are randomly chosen according to the category of difficulty of the specific level. The order of the options and the placement of correct answers are chosen randomly."
    },
    {
      "title": "Difficulty Levels Creation",
      "text": "We represent the medical code vocabulary hierarchy as an undirected graph using PyHealth [19]. Each vocabulary is divided into three difficulty levels: easy, medium, and hard. The distinction lies in the options provided for the correct answer. For the easy level, the options are randomly chosen from all the medical concept codes within the vocabulary. For the medium level, we select alternative medical codes within a distance of three, four, or five edges from each node in the vocabulary. For the hard level, the required distance is reduced to two edges. This means that for the hard level, the candidates are closely related, sharing a common parent or a node and its grand-parent or a node and its grand-child. For example, one of the candidates for the medical code S46.211D in ICD10-CM is S46.212A, because these codes share a common parent. Figure 1 presents examples of questions for the same ICD10-CM code across each difficulty level. Algorithm 2.1 presents the pseudo-code we used to generate the questions based on the required distance for each level. We used this algorithm to create the medium and hard questions. Easy questions were generated by a uniform sampling of the codes across the entire codes' graph. ``` 1:functioncreateQuestionsByDistance(vocab_graph, required_edges_distance, num_of_options) 2:Input: Vocabulary graph vocab_graph, Required edges distance: required_edges_distance, Number of options: num_of_options 3:Output: List of generated questions 4: nodes \\(\\leftarrow\\) vocab_graph.nodes 5: questions \\(\\leftarrow\\varnothing\\) 6:fornode in nodes do 7: candidate_nodes \\(\\leftarrow\\varnothing\\) 8:forcandidate_node in nodes \\(\\setminus\\) {node} do 9: path_length \\(\\leftarrow\\) get_shortest_path_length(node, candidate_node) 10:ifpath_length in required_edges_distance then 11: candidate_nodes \\(\\leftarrow\\) candidate_nodes \\(\\cup\\) {candidate_node} 12:endif 13:endfor 14: candidates_number \\(\\leftarrow\\) length of candidate_nodes 15:ifcandidates_number \\(\\geq\\) num_of_options then 16: options \\(\\leftarrow\\) choose_random(candidate_nodes, num_of_options) 17: question \\(\\leftarrow\\) generate_question(node, options) 18: questions \\(\\leftarrow\\) questions \\(\\cup\\) {question} 19:endif 20:endfor 21:return questions 22:endfunction ``` **Algorithm 1** Generating Questions by Distance \\begin{table} \\begin{tabular}{c l r} \\hline \\hline Vocab & Level & Questions num \\\\ \\hline \\multirow{2}{*}{ATC} & easy & 6440 \\\\ & medium & 6440 \\\\ & hard & 5938 \\\\ \\hline \\multirow{2}{*}{ICD10-CM} & easy & 94580 \\\\ & medium & 81757 \\\\ & hard & 88013 \\\\ \\hline \\multirow{2}{*}{ICD10-PROC} & easy & 190987 \\\\ & medium & 190987 \\\\ & hard & 88582 \\\\ \\hline \\multirow{2}{*}{ICD9-CM} & easy & 17736 \\\\ & medium & 17736 \\\\ & hard & 16858 \\\\ \\hline \\multirow{2}{*}{ICD9-PROC} & easy & 4670 \\\\ & medium & 4670 \\\\ & hard & 4438 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: The number of generated questions across different vocabularies and difficulty levels in the benchmark."
    },
    {
      "title": "3 Experiments",
      "text": "In this section, we evaluate CLLMs on the MedConceptsQA benchmark. The CLLMs we use for evaluations are: BioMistral (BioMistral-7B-DARE) [4], BioMedGPT (BioMedGPT-LM-7B) [5], Gatortron (gatortron-large) [6], Llama3-OpenBioLLM (70B) [20], BioBert [21], Meerket (7B) [7], Meditron (70B) [3], Clinical-Longformer [22]. Additionally, we include GPT-3.5 and GPT-4 [17] for our evaluation, although the main focus of these models is to be general-purpose LLMs and not specifically focused on a clinical domain. Our evaluation is based on zero-shot learning and few-shot learning for all the models. For zero-shot learning we contain the question and general instruction description. For few-shot learning we include 4 shots in the prompt and then the question we want to get the answer for. Figure 1(a) shows an example for zero-shot learning prompt for our drugs dataset (ATC) in the benchmark, and figure 1(b) shows an example for few-shot learning example from our benchmark, for ICD10-CM vocabulary."
    },
    {
      "title": "Experiments Setup",
      "text": "For each dataset in the benchmark, we conduct zero-shot and few-shot evaluations at each difficulty level (easy, medium, and hard). We repeat each evaluation for each model three times and calculate a 95% confidence interval. Due to the large amount of resources required by some of the models, especially GPT4, we limit each type of test (vocabulary and difficulty and shots) to 250 randomly sampled Q&As. Few-shot learning is performed using 4-shot learning. All models except GPT-3.5 (gpt-3.5-turbo) and GPT-4 (gpt-4-0125-preview) were evaluated using HuggingFace [23] on RTX-6000 GPU, while GPT-3.5 and GPT-4 were inferred using the OpenAI API because these models are not provided as open-source. We use accuracy as the evaluation metric because the datasets in the benchmark are balanced, as we selected the placement of the correct answer randomly during the creation of the benchmark."
    },
    {
      "title": "Aggregated Analysis",
      "text": "We perform zero-shot and few-shot evaluations for each model across all vocabularies and difficulty levels (easy, medium, hard). In this section, we present aggregated results for each model. The aggregation includes averaging both the accuracy and the 95% confidence interval. Figure 3 presents the results of zero-shot and few-shot learning evaluation. The full results across all the models, vocabularies, and difficulty levels can be found in the appendix (Table 3 for zero-shot learning and Table 3 for few-shot learning). Figure 1: Questions across three difficulty levels for the S46.211D medical code within the ICD10-CM vocabulary. The CLLMs achieve performance close to Random guessing. For zero-shot learning evaluation, None of the CLLMs outperforms the Random guessing threshold when considering the error rate (confidence intervals). However, both GPT-3.5 and GPT-4 outperform the CLLMs, with GPT-4 achieving the highest performance at 52.49% accuracy. This result represents a significant improvement over random guessing, with an absolute increase of 27.49%. In the case of few-shot learning, the results are similar for the CLLMs. However, GPT-3.5 shows an improvement in accuracy of 4.418% (absolute), and GPT-4 demonstrates absolute accuracy improvements of 9.422% between zero-shot and few-shot evaluations."
    },
    {
      "title": "Results For Difficulty Level",
      "text": "We create three difficulty levels (easy, medium, and hard) for each dataset in our benchmark. Figure 4 shows the results of all models across the different levels for both zero-shot and few-shot learning settings. Nearly all the CLLMs achieve random guessing accuracy across levels (considering error bars), except for LLaMA-3B OpenBioLLM-70B, which performs better than random guessing on the easy level under few-shot learning. GPT-3.5 and GPT-4 demonstrate improvement from zero-shot to few-shot learning, indicating that the 4-shot examples enhance their performance. Moreover, the performance of GPT-3.5 and GPT-4 deteriorates as task difficulty increases, validating that our benchmark levels indeed represent a gradual increase in complexity. In Section 3.5, we present a case study focusing on the ICD9-CM vocabulary and analyze results across difficulty levels. Figure 3: Aggregated results for zero-shot and few-shot evaluations. The vertical line represents the accuracy of random guessing for comparison. Figure 2: Examples of zero-shot and few-shot learning prompts."
    },
    {
      "title": "Results For Different Vocabularies",
      "text": "The accuracy results for each vocabulary (ATC, ICD9-CM, ICD10-CM, ICD9-PROC, ICD10-PROC) are presented in Figure 5. The aggregation for the accuracy and 95% CI is for all the levels for each vocabulary. Also here, GPT-4 consistently outperform all other models across all vocabularies, with GPT-3.5 in second place. The CLLMs achieve performance close to random guessing. For ATC, GPT-4 get an accuracy of 45.6% with zero-shot learning but 62.045% with few-shot learning. In the case of ICD10-CM, GPT-4 demonstrates the highest accuracy at 73.022% with few-shot learning, while for ICD9-CM, it achieves 74.711%. For procedures medical codes (ICD9-PROC and ICD10-PROC), the performance is lower than the other vocabularies. The best model is also GPT-4 for both few-shot evaluations, achieving scores of 44.489% for ICD10-PROC and 55.289% for ICD9-PROC. Overall, these experiments show that almost all CLLMs achieve performance close to random guessing, not only in the overall aggregated score but also for each individual vocabulary. However, for ICD9-CM and ICD10-CM, Llama3-OpenBioLLM-70B outperformed random guessing."
    },
    {
      "title": "Case Study: Icd9-Cm Performance By Difficulty Level",
      "text": "Table 2 illustrates a case study of the ICD9-CM vocabulary and the performance of the models across these levels (easy, medium, and hard). The objective of these experiments is to highlight the diverse range of difficulty levels represented by the questions within our proposed benchmark. As depicted in this table, the results of both GPT-4 and GPT-3.5 decrease as the level becomes more challenging. For easy questions, GPT-4 achieves very high accuracy scores, with 98% for few-shot learning and 95.333% for zero-shot learning. However, for medium questions, which are more difficult than the Figure 4: Zero-shot and few-shot results for each of the levels (easy, medium, and hard) with 95% confidence intervals over three runs. Results are aggregated over difficulty vocabularies. easy ones, GPT-4 achieves only 65.467% for zero-shot and 72.4% for few-shot learning. As for the hard level questions, the accuracy decreases dramatically, with only 47.467% for zero-shot and 53.733% for few-shot learning. This represents an absolute decrease of 44.267% in few-shot accuracy between the easy and hard questions, and 47.866% for zero-shot."
    },
    {
      "title": "4 Discussion",
      "text": "With the proposed benchmark in this work, we evaluate the understanding of medical concepts across LLMs, including those not pre-trained on medical data. Our experiments show that the current state-of-the-art CLLMs are not adequate in understanding the meaning of medical concepts such as diagnoses, procedures, and drug codes, and the differences between them. The performances of these CLLMs were close to random guessing. However, GPT-3.5 and GPT-4 achieved better accuracy for our benchmark, although the accuracy is still not high enough in some cases. For example, the few-shot learning accuracy of GPT-4 for ICD10-PROC and ICD9-PROC is only 44.489% and 55.289%, respectively. Additionally, we present a case study of the ICD9-CM vocabulary, which shows a significant performance difference between GPT-4 and GPT-3.5 models across different difficulty levels, demonstrating that our benchmark indeed contains questions of varying difficulty. In order to investigate why the CLLMs achieve a random guessing guess performance, we calculate the confusion matrix for three CLLMs (Meditron-70B, Meerkat-7B, and BioMistral-7B) for our case study vocabulary, ICD9-CM, under few-shot learning settings. We choose this level because it is the easiest level on the benchmark, and GPT-4 achieved very high accuracy, however, the CLLMs also achieved Random guessing performance on this level. Figure 6 shows the error analysis using three confusion matrices for Meditron-70B, BioMistral-7B-DARE, and Meerkat-7B on the ICD9-CM easy level. Meditron-70B almost always predicts the label B, and Meerkat-7B almost always predicts A or D but never predicts C. BioMistral-7B does not always predict the same label but predicts diverse answers among the given options (A, B, C, D). However, the answers from this model are mostly incorrect since it also achieved Random guessing accuracy. Our careful dataset design, incorporating diverse label representation through random sampling in the 4-shot examples, ensures that we did not inadvertently influence the models (Meditron-70B and Meerkat-7B) to consistently predict the same label for this dataset. We encourage further studies to investigate this issue, focusing on how effectively existing CLLMs follow instructions, whether in zero-shot or few-shot learning scenarios."
    },
    {
      "title": "5 Limitations And Future Work",
      "text": "**Limitations:** We did not include all medical code vocabularies, for example, LOINC, which is a vocabulary for identifying medical laboratory observations. Additionally, our benchmark contains questions in only a single form. **Future Work:** In this work, we demonstrated that CLLMs achieved random guessing performance on the MedConceptsQA benchmark. Future research should focus on improving CLLMs to attain a better understanding of medical concepts, aiming for performance comparable to or better than GPT-4, whose primary focus is not medical, unlike CLLMs. Additionally, since our benchmark is competitive and evaluates a LLMs understanding of medical concepts, we encourage new CLLMs to \\begin{table} \\begin{tabular}{|l|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{Model} & \\multicolumn{3}{c|}{Zero-shot} & \\multicolumn{3}{c|}{Few-shot} \\\\ \\cline{2-7} & Easy & Medium & Hard & Easy & Medium & Hard \\\\ \\hline BioMistral & \\(25.333\\pm 2.144\\) & \\(24.933\\pm 4.091\\) & \\(27.333\\pm 2.045\\) & \\(25.6\\pm 2.759\\) & \\(23.067\\pm 1.593\\) & \\(26.8\\pm 2.357\\) \\\\ BioBert & \\(23.333\\pm 2.297\\) & \\(27.467\\pm 5.257\\) & \\(26.133\\pm 3.343\\) & \\(25.467\\pm 6.81\\) & \\(25.067\\pm 1.893\\) & \\(25.067\\pm 1.458\\) \\\\ Llama3-OpenBiLML - 70B & \\(31.2\\pm 7.298\\) & \\(27.467\\pm 9.174\\) & \\(23.467\\pm 4.114\\) & \\(36.4\\pm 0.454\\) & \\(26.533\\pm 3.666\\) & \\(27.2\\pm 1.571\\) \\\\ BioMedGPT & \\(24.0\\pm 2.759\\) & \\(25.067\\pm 2.579\\) & \\(22.133\\pm 0.944\\) & \\(24.533\\pm 3.541\\) & \\(24.267\\pm 3.435\\) & \\(24.267\\pm 2.046\\) \\\\ Clinical-Longformer & \\(25.2\\pm 0.454\\) & \\(24.933\\pm 3.404\\) & \\(24.667\\pm 2.328\\) & \\(28.4\\pm 1.636\\) & \\(26.533\\pm 4.808\\) & \\(26.4\\pm 1.2\\) \\\\ GPT-3.5 & \\(64.0\\pm 0.454\\) & \\(36.133\\pm 1.309\\) & \\(30.531\\pm 1.993\\) & \\(77.067\\pm 3.282\\) & \\(40.267\\pm 3.282\\) & \\(39.467\\pm 1.718\\) \\\\ GPT-4 & **95.333** & \\(2.238\\) & **65.467** & \\(2.283\\) & **47.647** & \\(4.214\\) & \\(98.0\\pm 1.2\\) & **72.4**\\(\\pm 1.815\\) & **53.733** & \\(2.499\\) \\\\ Gatortron & \\(24.667\\pm 4.763\\) & \\(26.933\\pm 4.806\\) & \\(26.133\\pm 2.498\\) & \\(25.6\\pm 2.078\\) & \\(26.2\\pm 3.542\\) & \\(24.667\\pm 3.464\\) \\\\ MedMNX-7B & \\(23.6\\pm 5.499\\) & \\(24.4\\pm 3.543\\) & \\(22.8\\pm 4.374\\) & \\(24.267\\pm 1.458\\) & \\(25.067\\pm 1.593\\) & \\(25.2\\pm 2.975\\) \\\\ Meditron-70B & \\(27.733\\pm 4.655\\) & \\(25.2\\pm 2.4\\) & \\(26.533\\pm 3.694\\) & \\(27.067\\pm 5.316\\) & \\(24.133\\pm 4.634\\) & \\(22.0\\pm 5.684\\) \\\\ Meerkat & \\(25.2\\pm 2.079\\) & \\(27.467\\pm 3.858\\) & \\(25.333\\pm 2.498\\) & \\(26.667\\pm 2.499\\) & \\(27.067\\pm 0.693\\) & \\(24.133\\pm 1.458\\) \\\\ \\hline \\end{tabular} \\end{table} Table 2: Zero-shot and Few-shot learning results with 95% confidence intervals, for ICD9-CM vocabulary. include MedConceptsQA in their evaluation, with the aim of achieving superior results compared to current CLLMs."
    },
    {
      "title": "6 Conclusion",
      "text": "In this study, we introduced MedConceptsQA, an open-source benchmark comprising over 800,000 questions spanning three difficulty levels. It was designed to evaluate the understanding and reasoning capabilities of LLMs on medical concepts across diagnoses, procedures, and drugs. Experimental results showed that state-of-the-art CLLMs, despite being pre-trained on medical data, achieved accuracy levels close to random guessing on this benchmark. However, general-purpose models (GPT-3.5 and GPT-4) outperformed CLLMs. Notably, GPT-4 exhibited the best performance, although its accuracy remained insufficient for certain datasets in our benchmark."
    },
    {
      "title": "7 Reproducibility",
      "text": "The MedConceptsQA benchmark is available at [https://huggingface.co/datasets/ofir408/MedConceptsQA](https://huggingface.co/datasets/ofir408/MedConceptsQA). Our code for creating this benchmark, and the evaluation code are available at the following link: [https://github.com/nadavlab/MedConceptsQA](https://github.com/nadavlab/MedConceptsQA)."
    },
    {
      "title": "References",
      "text": "* [1] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. \"A survey of large language models\". In: _arXiv preprint arXiv:2303.18223_ (2023). * [2] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. \"Challenges and applications of large language models\". In: _arXiv preprint arXiv:2307.10169_ (2023). * [3] Zeming Chen, Alejandro Hernandez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Kopf, Amikeivan Mohtashami, et al. \"Meditron-70b: Scaling medical pretraining for large language models\". In: _arXiv preprint arXiv:2311.16079_ (2023). * [4] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. \"BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains\". In: _arXiv preprint arXiv:2402.10373_ (2024). * [5] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. \"Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine\". In: _arXiv preprint arXiv:2308.09442_ (2023). * [6] Xi Yang et al. \"A large language model for electronic health records\". In: _npj Digital Medicine_ 5.1 (2022), p. 194. * [7] Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee, Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, and Jaewoo Kang. \"Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks\". In: _arXiv preprint arXiv:2404.00376_ (2024). * [8] Ofir Ben Shoham and Nadav Rappoport. \"Cpllm: Clinical prediction with large language models\". In: _arXiv preprint arXiv:2309.11295_ (2023). * [9] Yuting He, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. \"Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions\". In: _arXiv preprint arXiv:2404.03264_ (2024). * [10] Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras. \"BioASQ-QA: A manually curated corpus for Biomedical Question Answering\". In: _Scientific Data_ 10.1 (2023), p. 170. * [11] Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. \"emrQA: A Large Corpus for Question Answering on Electronic Medical Records\". In: _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_. 2018, pp. 2357-2368. * [12] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. \"PubMedQA: A Dataset for Biomedical Research Question Answering\". In: _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_. Ed. by Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan. Hong Kong, China: Association for Computational Linguistics, Nov. 2019, pp. 2567-2577. doi: 10.18653/v1/D19-1259. url: [https://aclanthology.org/D19-1259](https://aclanthology.org/D19-1259). * [13] Olumide E Ojo, Olaronke O Adebanji, Alexander Gelbukh, Hiram Calvo, and Anna Feldman. \"MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations\". In: _arXiv preprint arXiv:2310.12489_ (2023). * [14] Diego Molla, Maria Elena Santiago-Martinez, Abeed Sarker, and Cecile Paris. \"A corpus for research in text processing for evidence based medicine\". In: _Language Resources and Evaluation_ 50 (2016), pp. 705-727. * [15] Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koras, Constantin Marc Seibold, Kaleb E Smith, and Jens Kleesiek. \"CLUE: A Clinical Language Understanding Evaluation for LLMs\". In: _arXiv preprint arXiv:2404.04067_ (2024). * [16] Ali Soroush, Benjamin S Glicksberg, Eyal Zimlichman, Yiftach Barash, Robert Freeman, Alexander W Charney, Girish N Nadkarni, and Eyal Klang. \"Large Language Models Are Poor Medical Coders--Benchmarking of Medical Code Querying\". In: _NEJM AI_ (2024), Aldbp2300040. * [17] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. \"Gpt-4 technical report\". In: _arXiv preprint arXiv:2303.08774_ (2023). * [18] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. \"Instruction tuning for large language models: A survey\". In: _arXiv preprint arXiv:2308.10792_ (2023). * [19] Chaoqi Yang, Zhenbang Wu, Patrick Jiang, Zhen Lin, Junyi Gao, Benjamin Danek, and Jimeng Sun. \"PyHealth: A Deep Learning Toolkit for Healthcare Predictive Modeling\". In: _Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) 2023_. 2023. url: [https://github.com/sunlabuiuc/PyHealth](https://github.com/sunlabuiuc/PyHealth). * [20] Malaikannan Sankarasubbu Ankit Pal. _OpenBioLLMs: Advancing Open-Source Large Language Models for Healthcare and Life Sciences_. [https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B](https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B). 2024. * [21] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. \"BioBERT: a pre-trained biomedical language representation model for biomedical text mining\". In: _Bioinformatics_ 36.4 (2020), pp. 1234-1240. * [22] Yikuan Li, Ramsey M Wehbe, Faraz S Ahmad, Hanyin Wang, and Yuan Luo. \"A comparative study of pretrained language models for long clinical text\". In: _Journal of the American Medical Informatics Association_ 30.2 (2023), pp. 340-347. * [23] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. \"Huggingface's transformers: State-of-the-art natural language processing\". In: _arXiv preprint arXiv:1910.03771_ (2019)."
    },
    {
      "title": "8 Appendix",
      "text": "Table 3 presents the comprehensive zero-shot learning results for all models, vocabularies, and difficulty levels (easy, medium, and hard). Table 4 presents the few-shot learning results. [MISSING_PAGE_FAIL:10] Figure 5: Zero-shot and few-shot results for each of the vocabularies with 95% confidence intervals over three runs. Results are aggregated over difficulty levels. Figure 6: Error analysis (confusion matrix) of few-shot learning for ICD9-CM at the easy level, for three CLLMs that achieved random guessing performance on our benchmark."
    }
  ]
}