{
  "title": "TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety",
  "authors": [
    "PhD Ou Zheng",
    "PhD Mohamed Abdel-Aty",
    "PhD Dongdong Wang",
    "PhD Chenzhu Wang",
    "Shengxuan Ding"
  ],
  "abstract": "\n Large Language Models (LLMs) have shown remarkable effectiveness in various generaldomain natural language processing (NLP) tasks. However, their performance in transportation safety domain tasks has been suboptimal, primarily attributed to the requirement for specialized transportation safety expertise in generating accurate responses  [1] . To address this challenge, we introduce TrafficSafetyGPT, a novel LLaMA-based model, which has undergone supervised fine-tuning using TrafficSafety-2K dataset which has human labels from government produced guiding books and ChatGPT-generated instruction-output pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset are accessible at  https://github.com/ozheng1993/TrafficSafetyGPT   \n",
  "references": [
    {
      "id": null,
      "title": "TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety",
      "authors": [
        "Ou Zheng",
        "Mohamed Abdel-Aty",
        "Dongdong Wang",
        "Chenzhu Wang",
        "Shengxuan Ding"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "ChatGPT is on the horizon: Could a large language model be all we need for Intelligent Transportation?",
      "authors": [
        "O Zheng",
        "M Abdel-Aty",
        "D Wang",
        "Z Wang",
        "S Ding"
      ],
      "year": "2023",
      "venue": "ChatGPT is on the horizon: Could a large language model be all we need for Intelligent Transportation?",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "GPT-4 Technical Report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Palm 2 technical report",
      "authors": [
        "R Anil"
      ],
      "year": "2023",
      "venue": "Palm 2 technical report",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
      "authors": [
        "M Turpin",
        "J Michael",
        "E Perez",
        "S R Bowman"
      ],
      "year": "2023",
      "venue": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Holistic Evaluation of Language Models",
      "authors": [
        "R Bommasani",
        "P Liang",
        "T Lee"
      ],
      "year": "2023",
      "venue": "Annals of the New York Academy of Sciences",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Falcon: malware detection and categorization with network traffic images",
      "authors": [
        "P Xu",
        "C Eckert",
        "A Zarras"
      ],
      "year": "2021",
      "venue": "Artificial Neural Networks and Machine Learning-ICANN 2021: 30th International Conference on Artificial Neural Networks",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "LLaMA: Open and efficient foundation language models",
      "authors": [
        "H Touvron"
      ],
      "year": "2023",
      "venue": "LLaMA: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Capabilities of gpt-4 on medical challenge problems",
      "authors": [
        "H Nori",
        "N King",
        "S M Mckinney",
        "D Carignan",
        "E Horvitz"
      ],
      "year": "2023",
      "venue": "Capabilities of gpt-4 on medical challenge problems",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Instruction tuning with gpt-4",
      "authors": [
        "B Peng",
        "C Li",
        "P He",
        "M Galley",
        "J Gao"
      ],
      "year": "2023",
      "venue": "Instruction tuning with gpt-4",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
      "authors": [
        "Y.-T Lin",
        "Y.-N Chen"
      ],
      "year": "2023",
      "venue": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "A step-by-step researcher's guide to the use of an AI-based transformer in epidemiology: an exploratory analysis of ChatGPT using the STROBE checklist for observational studies",
      "authors": [
        "F Sanmarchi"
      ],
      "year": "2023",
      "venue": "Journal of Public Health",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "hybrid-Falcon: Hybrid Pattern Malware Detection and Categorization with Network Traffic and Program Code",
      "authors": [
        "P Xu",
        "C Eckert",
        "A Zarras"
      ],
      "year": "2021",
      "venue": "hybrid-Falcon: Hybrid Pattern Malware Detection and Categorization with Network Traffic and Program Code",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Meta LLaMA leak raises risk of AI-linked harms",
      "authors": [
        "O Analytica"
      ],
      "year": "2023",
      "venue": "Emerald Expert Briefings, no. oxan-es",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "LLaMA 2: Open Foundation and Fine-Tuned Chat Models",
      "authors": [
        "H Touvron"
      ],
      "year": "2023",
      "venue": "LLaMA 2: Open Foundation and Fine-Tuned Chat Models",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "CitySim: A drone-based vehicle trajectory dataset for safety oriented research and digital twins",
      "authors": [
        "O Zheng",
        "M Abdel-Aty",
        "L Yue",
        "A Abdelraouf",
        "Z Wang",
        "N Mahmoud"
      ],
      "year": "2022",
      "venue": "CitySim: A drone-based vehicle trajectory dataset for safety oriented research and digital twins",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "AVOID: Autonomous Vehicle Operation Incident Dataset Across the Globe",
      "authors": [
        "O Zheng",
        "M Abdel-Aty",
        "Z Wang",
        "S Ding",
        "D Wang",
        "Y Huang"
      ],
      "year": "2023",
      "venue": "AVOID: Autonomous Vehicle Operation Incident Dataset Across the Globe",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Exploratory Analysis of the Crash Severity between Vehicular Automation (SAE L2-5) with Multi-Source Data",
      "authors": [
        "S Ding"
      ],
      "year": "2023",
      "venue": "Exploratory Analysis of the Crash Severity between Vehicular Automation (SAE L2-5) with Multi-Source Data",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Chatdoctor: A medical chat model fine-tuned on LLaMA model using medical domain knowledge",
      "authors": [
        "L Yunxiang",
        "L Zihan",
        "Z Kai",
        "D Ruilong",
        "Z You"
      ],
      "year": "2023",
      "venue": "Chatdoctor: A medical chat model fine-tuned on LLaMA model using medical domain knowledge",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Huatuo: Tuning LLaMA model with chinese medical knowledge",
      "authors": [
        "H Wang"
      ],
      "year": "2023",
      "venue": "Huatuo: Tuning LLaMA model with chinese medical knowledge",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Pmc-LLaMA: Further finetuning LLaMA on medical papers",
      "authors": [
        "C Wu",
        "X Zhang",
        "Y Zhang",
        "Y Wang",
        "W Xie"
      ],
      "year": "2023",
      "venue": "Pmc-LLaMA: Further finetuning LLaMA on medical papers",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Effective domain knowledge transfer with soft fine-tuning",
      "authors": [
        "Z Zhao",
        "B Zhang",
        "Y Jiang",
        "L Xu",
        "L Li",
        "W.-Y Ma"
      ],
      "year": "2019",
      "venue": "Effective domain knowledge transfer with soft fine-tuning",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "LLaMA-adapter: Efficient fine-tuning of language models with zero-init attention",
      "authors": [
        "R Zhang"
      ],
      "year": "2023",
      "venue": "LLaMA-adapter: Efficient fine-tuning of language models with zero-init attention",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Self-instruct: Aligning language model with self generated instructions",
      "authors": [
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Self-instruct: Aligning language model with self generated instructions",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Stanford alpaca: An instruction-following LLaMA model",
      "authors": [
        "R Taori"
      ],
      "year": "2023",
      "venue": "Stanford alpaca: An instruction-following LLaMA model",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Abstract",
      "text": "Large Language Models (LLMs) have shown remarkable effectiveness in various general-domain natural language processing (NLP) tasks. However, their performance in transportation safety domain tasks has been suboptimal, primarily attributed to the requirement for specialized transportation safety expertise in generating accurate responses [1]. To address this challenge, we introduce TrafficSafetyGPT, a novel LLaMA-based model, which has undergone supervised fine-tuning using TrafficSafety-2K dataset which has human labels from government produced guiding books and ChatGPT-generated instruction-output pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset are accessible at [https://github.com/ozheng1993/TrafficSafetyGPT](https://github.com/ozheng1993/TrafficSafetyGPT) **Keywords:** ChatGPT, Natural Language Processing, Deep Learning, Traffic Safety, Large Language Models, Generative Pre-trained Transformers"
    },
    {
      "title": "1 Introduction",
      "text": "In the realm of natural language processing (NLP) and large language models, a surge in advancements has unfolded a plethora of potential applications. This rapid development, spearheaded by pre-trained large language models like OpenAI's ChatGPT and its derivatives, has drastically augmented our capabilities in language comprehension, generation, and interactivity. The foundational strength of these models lies in their pre-training on extensive and diverse datasets, empowering them to decipher intricate language patterns and contextual interconnections. Nevertheless, while these pre-trained models exhibit commendable proficiency across an array of tasks, their generic nature could constrain their efficacy in niche applications, such as transportation safety. The training of a Large Language Model (LLM) from scratch necessitates the ingestion of vast, heterogeneous data to master sophisticated language patterns and relationships, which in turn requires potent hardware like high-performance GPUs or TPUs to manage the colossal computational load during training. Furthermore, the training duration may extend from a few days to weeks, subject to the model's size and complexity. This research paper aims to address this conundrum by concentrating on the fine-tuning of a pre-trained offline large language model, Meta AI's LLaMA, to cultivate a subject-matter expert in transportation safety. The process capitalizes on the pre-trained model's inherent knowledge and linguistic proficiency, amalgamating it with a comprehensive transportation safety dataset gleaned from government-produced guidelines and instruction-output pairs generated by ChatGPT. The objective is to engineer a specialized language model capable of producing precise, context-sensitive, and safety-conscious responses in traffic-related situations. The novelty of TrafficSafetyGPT is its potential to redefine how language models augment traffic safety and the broader transportation field. Instead of constructing a model from the ground up, our methodology refines the existing linguistic aptitude through fine-tuning with small dataset TrafficSafety-2k, yielding a more efficient allocation of computational resources and training time. Moreover, this tailored model assures that the output aligns with ratified traffic safety standards and guidelines, thus ensuring its practical applicability."
    },
    {
      "title": "2 Related Works",
      "text": ""
    },
    {
      "title": "Large Language Models",
      "text": "Large Language modeling (LLM) offers impressive capabilities in natural language processing and artificial intelligence. There were many models which have been proposed in the current analysis, including GPT4 [2], Palm2 [3], Claude [4], Cohere [5], Falcon [6], and Meta LLaMA [7].Some of the key roles of these large language models include natural language understanding, language generation, reasoning tasks, text processing, language tutoring and learning. The introduction of the GPT-4 model marked a significant advancement in natural language understanding and generation [8]. This latest iteration showcased remarkable abilities across various domains, including complex reasoning, comprehensive comprehension, advanced coding capabilities, proficiency in diverse academic exams, and even human-level performance in numerous tasks [9]. Its impressive attributes have garnered widespread attention and acclaim. For reasoning tasks, PaLm2 was an advanced language model from Google with enhanced multilingual capabilities, improved reasoning skills, and proficient coding capabilities, having been trained on a diverse dataset encompassing over 100 languages, scientific papers, web pagescontaining mathematical expressions, and publicly available source code datasets [3]. In the aspect of text processing, Claude offers a wide range of use cases, such as summarization, search, creative and collaborative writing, Q&A, coding, and much more. Just like ChatGPT, Claude boasts a user-friendly chat interface and API, which developers can access through our developer console. Its outstanding abilities in handling various conversational and text processing tasks are complemented by its exceptional reliability and predictability [10]. LLM impressed many users by virtue of its exceptionally superior performance based on billions of parameters, which provides a solid foundation for the fine-tuning of models. For example, Cohere offers offer multiple models, varying in size from small to large, with as few as 6 billion parameters to the larger models trained on an extensive 52 billion parameters [11]. Moreover, Falcon is open-source and released under the Apache 2.0 license [12]. This permits users to utilize the model for commercial purposes without any royalties or restrictive limitations. In addition, the official release of LLaMA models comes in different sizes, ranging from 7 billion to 65 billion parameters. Numerous developers are utilizing LLaMA for fine-tuning and creating exceptional open-source models [13]. However, it is important to note that LLaMA has been made available for research purposes exclusively. As an extension of LLaMA, Meta LLaMA 2 introduces pertained and fine-tuned LLMs with 7B, 13B, and 70B parameters. These models improve upon LLaMA 1 with 40% more tokens, a context length of 4,000 tokens for better language comprehension, and grouped-query attention for faster inference in the 70B model [14]. Given this, the existing Large Language Models were developed to be more academic and human-level in many aspects of studies. However, in the field of traffic safety, a limited body of research efforts has been proposed to analyze trajectory prediction [15], safety planning and design [16], safety rules [17], etc. And LLaMA is proven to be effective and available in research analysis, which will be used as the base model of the TrafficSafetyGPT in the current study."
    },
    {
      "title": "Pre-Trained Models With Domain Knowledge Fine-Tuning",
      "text": "Due to the swift advancements in LLM and the increasing demands for practical applications across diverse fields, researchers recognized the need to enhance the performance of LLMs by tune turning the models based on domain knowledge. For example, researchers have acknowledged the imperative to improve the performance of LLMs in medical applications, where precision is of utmost importance, primarily due to their limited domain-specific knowledge. To address this issue, they developed ChatDoctor, a specialized model tailored specifically for the biomedical field. The main objective of ChatDoctor is to enhance the understanding of LLMs in this domain. This endeavor underscores the significance of integrating domain-specific knowledge to attain superior outcomes in medical applications [18]. Another notable study focused on enhancing LLMs' performance in the specific domain through domain-specific knowledge transfer with different languages. As an illustration, researchers effectively fine-tuned the LLaMA model with Chinese medical knowledge, which contributed to improved comprehension and performance in Chinese LLM applications [19]. This research emphasized the importance of domain-specific tuning to optimize LLMs for specific fields [20]. Several technical approaches were explored to enhance domain knowledge transfer and fine-tuning of LLMs. Effective techniques such as soft fine-tuning [21], large-scale fine-grained categorization [21], and domain-specific transfer learning were employed to optimize LLM performance in specialized domains(). Additionally, the LLaMA-adapter method demonstrated efficient fine-tuning of language models with zero-init attention, contributing to better adaptability in domain-specific tasks [22]. There are some domain knowledge datasets that support LLM fine-tuning, such as Alpaca, which contributes 52k instruction-following data points by leveraging Self-Instruct techniques to encode specific instructions within conversations [23].The HealthCareMagic-100k dataset encompasses 100k real-world patient-physician conversations, providing valuable insights into authentic medical interactions [18]. On the other hand, GenMedGPT-5k enriches dataset diversity with 5k synthetic conversations between patients and physicians [18]."
    },
    {
      "title": "3 Trafficsafetygpt",
      "text": ""
    },
    {
      "title": "Foundation Model",
      "text": "We selected a lightweight version of ChatGPT- LLaMA as a foundation model. A family of models derived from LLaMA represents a cutting-edge collection of open-access large language models ranging from 7B to 70B parameters (7B, 13B, 70B) introduced by Meta AI, showcasing significant advancements in natural language processing research. However, LLaMA is made accessible under a non-commercial license, restricting its usage exclusively to academics with specific credentials. However, recent developments have been unveiled by Meta, with the announcement of LLaMA 2--a new family of AI language models that also falls under the source-available category but distinguishes itself with the inclusion of a commercial license, enabling broader usage beyond non-commercial purposes."
    },
    {
      "title": "Transportation Safety Knowledge",
      "text": "Government departments and agencies produce guiding books pertaining to specific domains, such as healthcare, education, finance, and more. This allows Large Language Models (LLMs) to grasp domain-specific language and terminologies, enhancing their ability to communicate effectively in specialized contexts. Moreover, these government guiding books encompass authoritative and accurate information on various subjects, including laws, regulations, policies, and procedures. By training LLMs on this data, they gain reliable knowledge, enabling them to deliver well-informed and precise responses. Additionally, exposure to real-world contexts and scenarios in these guiding books deepens LLMs' understanding of practical language applications, leading to more contextually relevant and useful outputs. Furthermore, the information contained in these books often involves decision-making processes, problem-solving guidelines, and strategies for complex situations. Consequently, LLMs trained on such data can offer valuable insights and recommendations in diverse governance and administrative tasks. We have manually labeled 665 rows of data from the perspective of traffic management, traffic operation, and drivers. The ground truth follows the government book as fine-tuning instruction data incorporating the NSTHA Model Minimum Uniform Crash Criteria (MMUCC) Guideline Fourth edition and FHWA's Highway Safety Manual (HSM) in the training process of LLMs contributes to creating safer, more informed, and reliable language models in the domain of traffic safety and transportation. * NSTHA Model Minimum Uniform Crash Criteria (MMUCC) Guideline Fourth edition MMUCC was established with the aim of fostering increased uniformity and consistency. Its purpose is to offer State and Local agencies a standardized set of data variables related to motor vehicle traffic crashes, encouraging them to consider these elements for collection. The key components of MMUCC Guidelines cover various aspects of crash data collection and reporting, including 1689 rows of data. The first type is the definition. The guidelines provide standardized definitions for each data element, ensuring that different agencies interpret and record the information in the same way. In addition, MMUCC outlines the classification of crashes based on severity, collision type, and other relevant factors to enable meaningful analysis and comparison. Lastly, the guidelines offer recommendations on reporting formats and methods, encouraging efficient data exchange and sharing among agencies. * FHWA The Highway Safety Manual (HSM) The Highway Safety Manual (HSM), published by the American Association of State Highway Transportation Officials (AASHTO), is a valuable resource for evaluating traffic safety on existing or proposed roadways. It offers a science-based and technical approach that helps state and local agencies analyze safety without relying on guesswork. The HSM promotes a safety management process that involves identifying high-crash locations, analyzing contributing factors, and implementing countermeasures to mitigate risks and improve overall safety. 311 rows of data related to the safety management process are labeled in our work. It utilizes a data-driven approach and employs decision-making strategies to allocate resources more efficiently by focusing on locations with the greatest safety improvement potential. However, books have a limited amount of labeled data, and relying solely on them for training a domain-specific language model may not be sufficient. rows of data related to transportation safety were generated by ChatGPT. Using generated conversations from ChatGPT with content from government departments and agencies guiding books as training data for a pre-trained large language model to become a domain-specific expert in transportation safety offers several benefits. The specialized nature of generated conversations ensures the model focuses solely on transportation safety topics while incorporating the comprehensive information from guiding books helps it gain an authoritative understanding of the subject. By simulating real-world scenarios, the model learns to respond appropriately to various safety concerns, enhancing its practical applicability. Moreover, using standardized information from government sources promotes consistency and accuracy in the model's responses while mitigating potential biases. \\begin{table} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline & NSTHA Model & FHWA The Highway & ChatGPT generated \\\\ Minimum Uniform & Safety Manual & Data \\\\ Crash Criteria & (HSM) & \\\\ (MMUCC) Guideline & & \\\\ Fourth edition & & \\\\ \\hline Number of label & 1689 & 311 & 2000 \\\\ \\hline \\end{tabular} \\end{table} Table 1: TrafficSafety-2K number of data Achieving optimal performance necessitates a balanced approach, combining both generated and real data, and fine-tuning the model to ensure it provides reliable and knowledgeable insights in the transportation safety domain. We name this dataset TrafficSafety-2K."
    },
    {
      "title": "Knowledge-Based Instruction Data",
      "text": "\\begin{table} \\begin{tabular}{|p{56.9pt}|p{142.3pt}|p{142.3pt}|} \\hline Type & Input & Knowledge \\\\ \\hline Definition & What is the definition of a van in Motor Vehicle & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width. Before classification, vehicle width should be rounded to the nearest inch. & A van is a motor vehicle consisting primarily of a transport device that has a gross vehicle weight rating (GVWR) of 10,000 pounds or less and is basically a “box on wheels” that is identifiable by its enclosed passenger and/or cargo area, step-up floor, and relatively short (or nonexistent) hood. Vans are classified by size based on frame type and overall vehicle body width."
    },
    {
      "title": "4 Experiment",
      "text": "We select ChatGPT as a baseline foundation model for text generation assessment. For efficient implementation, a pre-trained LLaMA-7B foundation model is chosen as a practical alternative and fine-tuned with domain knowledge. To improve domain-specific knowledge with limited data, we only tuned the last two layers. This fine-tuning strategy is developed for the reasons as follows. First, although our traffic-safety-domain specific data are adequate, the number is not comparable to the scale of a large language knowledge database, like Wikipedia. Fine-tuning all layers with limited data can easily result in overfitting. Second, the original features in the pre-trained LLaMA are still helpful for language generation, like reasoning and fluency. Third, this fine-tuning on the last two layers saves lots of both space and time cost, which enables efficient training and practical implementation. Based upon the designed strategy, we fine-tuned the LLaMA-7B model on the Lambda server with 8 RTX TITAN GPUs, 128 CPU cores, and 500GB RAM. The fine-tuning process is completed with three training epochs which costs ~70 CPU hours. The training is implemented with the hyper parameters as follows: the batch size of 16, the learning rate of 2X10-5, the epoch of 3, the maximum sequence length of 152 tokens, and the warm-up ratio of 0.03 without weight decay."
    },
    {
      "title": "Base Model",
      "text": "LLaMA is one of important foundational large language models created by Meta AI. Just like its counterparts in the field, LLaMA operates on the principle of accepting a word sequence as input and employing its predictive capabilities to generate subsequent words recursively, ultimately producing coherent and contextually appropriate text. In the study, we follow the Stanford instruction-following LLaMA Model train processing: Alpaca [24]. Figure 1 demonstrates the process for instruction-following, employing a methodology that builds upon the self-instruct approach utilized in MUCC and HSM. To initiate this process, we utilized a set of 1k instruction-output pairs that were previously human-labeled in those specific books. Subsequently, we leveraged the In ChatGPT model, prompting it to generate additional instructions while utilizing the aforementioned human-labeled dataset asin-context examples. Figure 2 shows the example of human labels and ChatGPT generate instruction-output pairs."
    },
    {
      "title": "Metrics",
      "text": "We evaluate the proficiency of generated texts in traffic safety with domain questions of different kinds, including definition, inclusion, exclusion, category, example, and guidance. The generated answers to these questions are evaluated by several popular and reliable quantitative metrics, which examines the results from coverage, fluency, and brevity. For coverage, we Figure 1: Data generation process for instruction-following. Figure 2: Example of human label and ChatGPT generate instruction-following. selected BLEU, ROUGE, and BERTScore. For fluency, we chose BLEURT. For brevity, we count the words of the generated sentences. **Coverage.** BLEU (bilingual evaluation understudy) is a popular metric for natural language generation. This algorithm is first proposed to evaluate machine translation performance. Since computer generated texts can be taken as a different language system from human language, this score can also reflect the quality of machine-generated texts. BLEU ranges from 0 to 1, which indicates the similarity between human languages and machine generated texts. However, this score is quantified heavily based upon word matching on golden labels, which can yield some bias. For example, a correctly generated machine language can still show a low BLEU score because of the paraphrase; meanwhile, just one word change may yield totally different meaning, but the BLEU score can stay similar. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is an algorithm to assess text summarization and machine translation. The metric can evaluate the information coverage of automatically produced texts on a reference passage which is usually a human-written summary. ROUGE relies on n-gram modeling which considers word overlapping between generated and reference texts. The common ROUGE scores include ROUGE-1 (overlap of unigrams), ROUGE-2 (overlap of bigrams), and ROUGE-L (longest common subsequence based statistics). Since ROUGE is proposed to solve a summarization problem, it is very effective and reliable to assess the information coverage. However, it can also result in the bias due to lack of semantic meaning understanding. Therefore, like BLEU, a high ROUGE score may lead to a sentence with a different meaning. BERTScore is a recently proposed metric based upon a milestone large language model of BERT. It relies on the prediction results from BERT to evaluate candidate sentences and reference sentences by tokens. BERTScore computes the token similarity with the context embeddings, which replaces word exact match. This evaluation is more focused on comparison on a vectorized embedding space instead of a common letter system, which is more suitable for large language model evaluation. Since BERT is developed upon mask modeling, its word embeddings can reflect mask filling accuracy and help effectively assess information coverage. However, one of its important weaknesses is language model dependency, where model bias will be induced in the evaluation process. **Fluency.** BLEURT (Bilingual Evaluation Understudy with Representations from Transformer) is adopted to evaluate the fluency of generated texts. This evaluation metric is developed based upon BERT, which is derived from the outputs of a robust BERT model pretrained by perturbed synthetic sentences from Wikipedia. This metric can reflect the paraphrase level and language fluency. Although it relies on a large language model and may induce model bias, its fluency assessment is more reliable due to pretraining on a large set of synthetic data. **Brevity.** We adopt the Word Count to assess the brevity of the expression, which is straightforward and also popular in summarization evaluation. Fewer words indicate better brevity of the generated texts."
    },
    {
      "title": "Results",
      "text": "To evaluate the efficacy of TrafficSafetyGPT, we conduct extensive experiments on various transportation safety-related tasks. We compare its performance with the state-of-the-art non-specialized LLM of LLaMA. The results justify that TrafficSafetyGPT consistently outperforms on different specific tasks. This observation results from the success of domain-specific fine-tuning, especially with more structural presentation and professional expression. It is obtainedthat the information coverage of TrafficSafetyGPT is significantly higher than LLaMA, even on the BLEU score. The performance also shows that the enumeration tasks, like inclusions, exclusions, categories, and examples, are more challenging for language modeling because these tasks are sensitive to exact match with each single word. The performance gain of TrafficSafetyGPT on these tasks results from a larger domain specific terminology database, which helps enhance including more professional terms. For guidance, LLaMA exhibits word expressions with higher word count since the generalized domain does not provide technical information focus. TrafficSafetyGPT yields briefer expressions after fine-tuning on domain-specific contexts. \\begin{table} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline \\multicolumn{1}{|c|}{**Type of task**} & \\multicolumn{1}{c|}{**ChatGPT**} & \\multicolumn{1}{c|}{**TrafficSafetyGPT**} \\\\ \\hline \\multicolumn{4}{|c|}{**Definitions**} \\\\ \\hline BLEU & 7.38 & 36.04 \\\\ \\hline ROUGE-1 & 40.54 & 80.66 \\\\ \\hline ROUGE-2 & 10.11 & 50.99 \\\\ \\hline \\end{tabular} \\end{table} Table 4: Comparison of TrafficSafetyGPT output \\begin{table} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline \\multicolumn{1}{|c|}{**Instruction**} & \\multicolumn{1}{c|}{**Input**} & \\multicolumn{1}{c|}{**Response**} \\\\ \\hline \\multirow{3}{*}{If you are a transportation engineer, please answer the transportation questions based on the student’s description.} & \\multirow{3}{*}{What is the consumer vehicle?} & A Nissan Leaf. A Chevy Spark, Charge, or Volt. An Underwood Navigator. A Smart ForTwo. A Rhino. A parking lot Fuller hog. A metallic or all-plas \\\\ \\cline{2-2} & & & \\\\ \\cline{2-2} & & & \\\\ \\cline{2-2} & & & \\\\ \\cline{2-2} & & & \\\\ \\hline \\end{tabular} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline \\multicolumn{1}{|c|}{If you are a transportation engineer, please answer the transportation questions based on the student’s description.} & \\multirow{3}{*}{What is the working vehicle?} & A working vehicle is any vehicle designed primarily for carrying property. \\\\ \\cline{2-2} & & & \\\\ \\cline{2-2} & & & \\\\ \\cline{2-2} & & & \\\\ \\hline \\end{tabular} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline \\multicolumn{1}{|c|}{If you are a transportation engineer, please answer the transportation questions based on the student’s description.} & \\multirow{3}{*}{What is the working vehicle?} & A consumer vehicle is any vehicle other than a working vehicle. A consumer vehicle is usually stored in its owner’s home or garage most of the time. \\\\ \\cline{2-2} & & & \\\\ \\cline{2-2} & & & \\\\ \\cline{2-2} & & & \\\\ \\hline \\end{tabular} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline \\multicolumn{1}{|c|}{If you are a transportation engineer, please answer the transportation questions based on the student’s description.} & \\multirow{3}{*}{Is patrol a working vehicle?} & \\multirow{3}{*}{No, a patrol is a transport vehicle.} \\\\ \\cline{2-2} & & & \\\\ \\cline{2-2} & & & \\\\ \\cline{2-2} & & & \\\\ \\hline \\end{tabular} \\end{table} Table 3: The examples of TrafficSafetyGPT generated texts. \\begin{tabular}{|l|l|l|} \\hline ROUGE-L & 39.10 & **78.98** \\\\ \\hline BERT-P & 84.74 & **96.75** \\\\ \\hline BERT-R & 91.55 & **97.98** \\\\ \\hline BLEURT & 47.34 & **69.30** \\\\ \\hline Word Count & 74 & 37 \\\\ \\hline \\multicolumn{3}{|c|}{**Inclusions**} \\\\ \\hline BLEU & 2.39 & **32.75** \\\\ \\hline ROUGE-1 & 38.33 & **81.47** \\\\ \\hline ROUGE-2 & 15.28 & **67.76** \\\\ \\hline ROUGE-L & 37.95 & **81.09** \\\\ \\hline BERT-P & 96.35 & **98.07** \\\\ \\hline BERT-R & 96.45 & **98.00** \\\\ \\hline BLEURT & 35.54 & **58.55** \\\\ \\hline Word Count & 81 & **42** \\\\ \\hline \\multicolumn{3}{|c|}{**Exclusions**} \\\\ \\hline BLEU & 4.56 & **31.64** \\\\ \\hline ROUGE-1 & 29.93 & **78.31** \\\\ \\hline ROUGE-2 & 13.14 & **60.20** \\\\ \\hline ROUGE-L & 28.77 & **77.99** \\\\ \\hline BERT-P & 96.79 & **98.11** \\\\ \\hline BERT-R & 96.68 & **98.09** \\\\ \\hline BLEURT & 26.48 & **50.15** \\\\ \\hline Word Count & 90 & **47** \\\\ \\hline \\multicolumn{3}{|c|}{**Categories**} \\\\ \\hline BLEU & 1.79 & **30.47** \\\\ \\hline ROUGE-1 & 26.03 & **77.49** \\\\ \\hline \\end{tabular} \\begin{tabular}{|l|l|l|} \\hline ROUGE-2 & 11.31 & 55.21 \\\\ \\hline ROUGE-L & 26.02 & 77.23 \\\\ \\hline BERT-P & 96.85 & 98.21 \\\\ \\hline BERT-R & 96.76 & 98.01 \\\\ \\hline BLEURT & 35.09 & 59.01 \\\\ \\hline Word Count & 91 & 51 \\\\ \\hline \\multicolumn{3}{|c|}{**Examples**} \\\\ \\hline BLEU & 2.03 & 30.90 \\\\ \\hline ROUGE-1 & 32.50 & 80.39 \\\\ \\hline ROUGE-2 & 13.44 & 61.11 \\\\ \\hline ROUGE-L & 31.99 & 80.31 \\\\ \\hline BERT-P & 96.77 & 98.10 \\\\ \\hline BERT-R & 96.07 & 97.97 \\\\ \\hline BLEURT & 38.97 & 62.38 \\\\ \\hline Word Count & 101 & 46 \\\\ \\hline \\multicolumn{3}{|c|}{**Guidance**} \\\\ \\hline BLEU & 3.92 & 33.24 \\\\ \\hline ROUGE-1 & 28.39 & 75.39 \\\\ \\hline ROUGE-2 & 10.30 & 56.13 \\\\ \\hline ROUGE-L & 27.98 & 74.97 \\\\ \\hline BERT-P & 96.77 & 98.19 \\\\ \\hline BERT-R & 96.47 & 98.03 \\\\ \\hline BLEURT & 40.44 & 64.73 \\\\ \\hline Word Count & 123 & 50 \\\\ \\hline \\end{tabular}"
    },
    {
      "title": "5 Conclusions",
      "text": "In this paper, we propose the TrafficSafetyGPT, a LLaMA-based model that addresses the limitations faced by Large Language Models in transportation safety domain tasks. By employing supervised fine-tuning with the TrafficSafety-2k dataset, human labels from the government produce guiding books and ChatGPT-generated instruction-output pairs. TrafficSafetyGPT demonstrates superior performance in generating responses with reliable traffic safety knowledge, allocating efficient computational resources and saving training time. Our research opens new avenues for enhancing language models' capabilities in transportation safety domains and contributes to the advancement of natural language processing applications in specialized fields."
    },
    {
      "title": "6 Author Contributions",
      "text": "The authors confirm contribution to the paper as follows: study conception and design: Ou Zheng, Mohamed Abdel-Aty,Dongdong Wang,Shengxuan Ding,Chenzhu Wang; data collection and processing: Ou Zheng, Shengxuan Ding, Chenzhu Wang; analysis and interpretation of results: Chenzhu Wang, Dongdong wang; draft manuscript preparation: Ou Zheng, Mohamed Abdel-Aty,Dongdong Wang, Shengxuan Ding,Chenzhu Wang. All authors reviewed the results and approved the final version of the manuscript."
    },
    {
      "title": "7 Availability",
      "text": "The TrafficSafetyGPT model and the curated training dataset are publicly accessible at [https://github.com/ozheng1993/TrafficSafetyGPT](https://github.com/ozheng1993/TrafficSafetyGPT) for research and non-commercial purposes. It is crucial to emphasize that the accuracy of responses generated by large language models cannot be assured. The safety knowledge presented by these models should not be considered a replacement for professional engineer advice.We encourage researchers and practitioners to leverage this resource to further explore and improve language models' capabilities in transportation safety domains."
    },
    {
      "title": "References",
      "text": "* [1] O. Zheng, M. Abdel-Aty, D. Wang, Z. Wang, and S. Ding, \"ChatGPT is on the horizon: Could a large language model be all we need for Intelligent Transportation?,\" _arXiv preprint arXiv:2303.05382_, 2023. * [2] OpenAI, \"GPT-4 Technical Report,\" _ArXiv_, vol. abs/2303.08774, 2023. * [3] R. Anil _et al._, \"Palm 2 technical report,\" _arXiv preprint arXiv:2305.10403_, 2023. * [4] M. Turpin, J. Michael, E. Perez, and S. R. Bowman, \"Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,\" _arXiv preprint arXiv:2305.04388_, 2023. * [5] R. Bommasani, P. Liang, and T. Lee, \"Holistic Evaluation of Language Models,\" _Annals of the New York Academy of Sciences_, 2023. * [6] P. Xu, C. Eckert, and A. Zarras, \"Falcon: malware detection and categorization with network traffic images,\" in _Artificial Neural Networks and Machine Learning-ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14-17, 2021, Proceedings, Part I 30_, 2021: Springer, pp. 117-128. * [7] H. Touvron _et al._, \"LLaMA: Open and efficient foundation language models,\" _arXiv preprint arXiv:2302.13971_, 2023. * [8] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, \"Capabilities of gpt-4 on medical challenge problems,\" _arXiv preprint arXiv:2303.13375_, 2023. * [9] B. Peng, C. Li, P. He, M. Galley, and J. Gao, \"Instruction tuning with gpt-4,\" _arXiv preprint arXiv:2304.03277_, 2023. * [10] Y.-T. Lin and Y.-N. Chen, \"LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models,\" _arXiv preprint arXiv:2305.13711_, 2023. * [11] F. Sanmarchi _et al._, \"A step-by-step researcher's guide to the use of an AI-based transformer in epidemiology: an exploratory analysis of ChatGPT using the STROBE checklist for observational studies,\" _Journal of Public Health_, pp. 1-36, 2023. * [12] P. Xu, C. Eckert, and A. Zarras, \"hybrid-Falcon: Hybrid Pattern Malware Detection and Categorization with Network Traffic and Program Code,\" _arXiv preprint arXiv:2112.10035_, 2021. * [13] O. Analytica, \"Meta LLaMA leak raises risk of AI-linked harms,\" _Emerald Expert Briefings_, no. oxan-es, 2023. * [14] H. Touvron _et al._, \"LLaMA 2: Open Foundation and Fine-Tuned Chat Models,\" _arXiv preprint arXiv:2307.09288_, 2023. * [15] O. Zheng, M. Abdel-Aty, L. Yue, A. Abdelraouf, Z. Wang, and N. Mahmoud, \"CitySim: A drone-based vehicle trajectory dataset for safety oriented research and digital twins,\" _arXiv preprint arXiv:2208.11036_, 2022. * [16] O. Zheng, M. Abdel-Aty, Z. Wang, S. Ding, D. Wang, and Y. Huang, \"AVOID: Autonomous Vehicle Operation Incident Dataset Across the Globe,\" _arXiv preprint arXiv:2303.12889_, 2023. * [17] S. Ding _et al._, \"Exploratory Analysis of the Crash Severity between Vehicular Automation (SAE L2-5) with Multi-Source Data,\" _arXiv preprint arXiv:2303.17788_, 2023. * [18] L. Yunxiang, L. Zihan, Z. Kai, D. Ruilong, and Z. You, \"Chatdoctor: A medical chat model fine-tuned on LLaMA model using medical domain knowledge,\" _arXiv preprint arXiv:2303.14070_, 2023. * [19] H. Wang _et al._, \"Huatuo: Tuning LLaMA model with chinese medical knowledge,\" _arXiv preprint arXiv:2304.06975_, 2023. * [20] C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, \"Pmc-LLaMA: Further finetuning LLaMA on medical papers,\" _arXiv preprint arXiv:2304.14454_, 2023. * [21] Z. Zhao, B. Zhang, Y. Jiang, L. Xu, L. Li, and W.-Y. Ma, \"Effective domain knowledge transfer with soft fine-tuning,\" _arXiv preprint arXiv:1909.02236_, 2019. * [22] R. Zhang _et al._, \"LLaMA-adapter: Efficient fine-tuning of language models with zero-init attention,\" _arXiv preprint arXiv:2303.16199_, 2023. * [23] Y. Wang _et al._, \"Self-instruct: Aligning language model with self generated instructions,\" _arXiv preprint arXiv:2212.10560_, 2022. * [24] R. Taori _et al._, \"Stanford alpaca: An instruction-following LLaMA model,\" ed, 2023."
    }
  ]
}