{
  "title": "How well do Large Language Models perform in Arithmetic tasks?",
  "authors": [
    "Zheng Yuan",
    "Hongyi Yuan",
    "Chuanqi Tan",
    "Wei Wang",
    "Songfang Huang",
    "Alibaba Group"
  ],
  "abstract": "\n Large language models have emerged abilities including chain-of-thought to answer math word problems step by step (Wei et al., 2022b). Solving math word problems not only requires abilities to disassemble problems via chain-ofthought but also needs to calculate arithmetic expressions correctly for each step. To the best of our knowledge, there is no work to focus on evaluating the arithmetic ability of large language models. In this work, we propose an arithmetic dataset MATH 401 to test latest large language models including GPT-4, ChatGPT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provide a detailed analysis of the ability of large language models. MATH 401 and evaluation codes are released at  https://github.com/  GanjinZero/math401-llm . 1 \n",
  "references": [
    {
      "id": null,
      "title": "How well do Large Language Models perform in Arithmetic tasks?",
      "authors": [
        "Zheng Yuan",
        "Hongyi Yuan",
        "Chuanqi Tan",
        "Wei Wang",
        "Songfang Huang",
        "Alibaba Group"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "GPT-NeoX-20B: An opensource autoregressive language model",
      "authors": [
        "Sid Black",
        "Stella Biderman",
        "Eric Hallahan",
        "Quentin Anthony",
        "Leo Gao",
        "Laurence Golding",
        "Horace He",
        "Connor Leahy",
        "Kyle Mcdonell",
        "Jason Phang",
        "Michael Pieler",
        "Shivanshu Usvsn Sai Prashanth",
        "Laria Purohit",
        "Jonathan Reynolds",
        "Ben Tow",
        "Samuel Wang",
        "Weinbach"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large Language Models",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Language models are few-shot learners",
      "authors": [
        "B Tom",
        "Benjamin Brown",
        "Nick Mann",
        "Melanie Ryder",
        "Jared Subbiah",
        "Prafulla Kaplan",
        "Arvind Dhariwal",
        "Pranav Neelakantan",
        "Girish Shyam",
        "Amanda Sastry",
        "Sandhini Askell",
        "Ariel Agarwal",
        "Gretchen Herbert-Voss",
        "T J Krueger",
        "Rewon Henighan",
        "Aditya Child",
        "Daniel M Ramesh",
        "Jeff Ziegler",
        "Clemens Wu",
        "Christopher Winter",
        "Mark Hesse",
        "Eric Chen",
        "Mateusz Sigler",
        "Scott Litwin",
        "Benjamin Gray",
        "Jack Chess",
        "Christopher Clark",
        "Sam Berner",
        "Alec Mc-Candlish",
        "Ilya Radford",
        "Dario Sutskever",
        "Amodei"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Evaluating large language models trained on code",
      "authors": [
        "Mark Chen",
        "Jerry Tworek",
        "Heewoo Jun",
        "Qiming Yuan",
        "Henrique Ponde",
        "Jared Kaplan",
        "Harrison Edwards",
        "Yura Burda",
        "Nicholas Joseph",
        "Greg Brockman",
        "Alex Ray",
        "Raul Puri",
        "Gretchen Krueger",
        "Michael Petrov",
        "Heidy Khlaaf",
        "Girish Sastry",
        "Pamela Mishkin",
        "Brooke Chan",
        "Scott Gray",
        "Nick Ryder",
        "Mikhail Pavlov",
        "Alethea Power",
        "Lukasz Kaiser",
        "Mohammad Bavarian",
        "Clemens Winter",
        "Philippe Tillet",
        "Felipe Petroski Such",
        "David W Cummings",
        "Matthias Plappert",
        "Fotios Chantzis",
        "; William",
        "H Guss",
        "Alex Nichol",
        "Igor Babuschkin",
        "S Arun Balaji",
        "Shantanu Jain",
        "Andrew Carr",
        "Jan Leike",
        "Joshua Achiam",
        "Vedant Misra",
        "Evan Morikawa",
        "Alec Radford",
        "Matthew M Knight",
        "Miles Brundage",
        "Mira Murati",
        "Katie Mayer",
        "Peter Welinder",
        "Bob Mcgrew",
        "Dario Amodei"
      ],
      "year": "2021",
      "venue": "Evaluating large language models trained on code",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Eric Li",
        "Xuezhi Wang",
        "Mostafa Dehghani"
      ],
      "year": "",
      "venue": "Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Training verifiers to solve math word problems",
      "authors": [
        "Karl Cobbe",
        "Vineet Kosaraju",
        "Mohammad Bavarian",
        "Mark Chen",
        "Heewoo Jun",
        "Lukasz Kaiser",
        "Matthias Plappert",
        "Jerry Tworek",
        "Jacob Hilton",
        "Reiichiro Nakano",
        "Christopher Hesse",
        "John Schulman"
      ],
      "year": "2021",
      "venue": "Training verifiers to solve math word problems",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Mathematical capabilities of chatgpt",
      "authors": [
        "Simon Frieder",
        "Luca Pinchetti",
        "Ryan-Rhys Griffiths",
        "Tommaso Salvatori",
        "Thomas Lukasiewicz",
        "Philipp Christian Petersen",
        "Alexis Chevalier",
        "J J Berner"
      ],
      "year": "2023",
      "venue": "Mathematical capabilities of chatgpt",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Symbolic math reasoning with language models",
      "authors": [
        "Vedant Gaur",
        "Nikunj Saunshi"
      ],
      "year": "2022",
      "venue": "2022 IEEE MIT Undergraduate Research Technology Conference (URTC)",
      "doi": "10.1109/URTC56832.2022.10002218"
    },
    {
      "id": "b7",
      "title": "Measuring mathematical problem solving with the math dataset",
      "authors": [
        "Dan Hendrycks",
        "Collin Burns",
        "Saurav Kadavath",
        "Akul Arora",
        "Steven Basart",
        "Eric Tang",
        "Dawn Song",
        "Jacob Steinhardt"
      ],
      "year": "2021",
      "venue": "Measuring mathematical problem solving with the math dataset",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Have you seen that number? investigating extrapolation in question answering models",
      "authors": [
        "Jeonghwan Kim",
        "Giwon Hong",
        "Kyung Min Kim",
        "Junmo Kang",
        "Sung-Hyon Myaeng"
      ],
      "year": "2021",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Large language models are zero-shot reasoners",
      "authors": [
        "Takeshi Kojima",
        "Shane Shixiang",
        "Machel Gu",
        "Yutaka Reid",
        "Yusuke Matsuo",
        "Iwasawa"
      ],
      "year": "2022",
      "venue": "Large language models are zero-shot reasoners",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Crosslingual generalization through multitask finetuning",
      "authors": [
        "Niklas Muennighoff",
        "Thomas Wang",
        "Lintang Sutawika",
        "Adam Roberts",
        "Stella Biderman",
        "Teven Le Scao",
        "M Saiful Bari",
        "Sheng Shen"
      ],
      "year": "2022",
      "venue": "Crosslingual generalization through multitask finetuning",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Evaluating transformer language models on arithmetic operations using number decomposition",
      "authors": [
        "Matteo Muffo",
        "Aldo Cocco",
        "Enrico Bertino"
      ],
      "year": "2022",
      "venue": "International Conference on Language Resources and Evaluation",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Investigating the limitations of the transformers with simple arithmetic tasks",
      "authors": [
        "Rodrigo Nogueira",
        "Zhiying Jiang",
        "Jimmy J Li"
      ],
      "year": "2021",
      "venue": "Investigating the limitations of the transformers with simple arithmetic tasks",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Pretrained language models are symbolic mathematics solvers too! arXiv preprint",
      "authors": [
        "Kimia Noorbakhsh",
        "Modar Sulaiman",
        "Mahdi Sharifi",
        "Kallol Roy",
        "Pooyan Jamshidi"
      ],
      "year": "2021",
      "venue": "Pretrained language models are symbolic mathematics solvers too! arXiv preprint",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll L Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Generative language modeling for automated theorem proving",
      "authors": [
        "Stanislas Polu",
        "Ilya Sutskever"
      ],
      "year": "2020",
      "venue": "Generative language modeling for automated theorem proving",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Impact of pretraining term frequencies on few-shot reasoning",
      "authors": [
        "Yasaman Razeghi",
        "I V Robert L Logan",
        "Matt Gardner",
        "Sameer Singh"
      ],
      "year": "2022",
      "venue": "Impact of pretraining term frequencies on few-shot reasoning",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "",
      "authors": [
        "Victor Sanh",
        "Albert Webson",
        "Colin Raffel",
        "Stephen H Bach",
        "Lintang Sutawika",
        "Zaid Alyafeai",
        "Antoine Chaffin",
        "Arnaud Stiegler",
        "Teven Le Scao",
        "Arun Raja",
        "Manan Dey",
        "M Saiful Bari",
        "Canwen Xu",
        "Urmish Thakker",
        "Shanya Sharma Sharma",
        "Eliza Szczechla",
        "Taewoon Kim",
        "Gunjan Chhablani",
        "Nihal Nayak",
        "Debajyoti Datta",
        "Jonathan Chang",
        "Mike Tian-Jian",
        "Han Jiang",
        "Matteo Wang",
        "Sheng Manica",
        "Zheng Xin Shen",
        "Harshit Yong",
        "Rachel Pandey",
        "Thomas Bawden",
        "Wang"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Analysing mathematical reasoning abilities of neural models",
      "authors": [
        "David Saxton",
        "Edward Grefenstette",
        "Felix Hill",
        "Pushmeet Kohli"
      ],
      "year": "2019",
      "venue": "Analysing mathematical reasoning abilities of neural models",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Bloom: A 176bparameter open-access multilingual language model",
      "authors": [
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilić",
        "Roman Hesslow",
        "Alexandra Castagné",
        "François Sasha Luccioni",
        "Matthias Yvon",
        "Gallé"
      ],
      "year": "2022",
      "venue": "Bloom: A 176bparameter open-access multilingual language model",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Toolformer: Language models can teach themselves to use tools",
      "authors": [
        "Timo Schick",
        "Jane Dwivedi-Yu",
        "Roberto Dessì",
        "Roberta Raileanu",
        "Maria Lomeli",
        "Luke Zettlemoyer",
        "Nicola Cancedda",
        "Thomas Scialom"
      ],
      "year": "2023",
      "venue": "Toolformer: Language models can teach themselves to use tools",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "An independent evaluation of chatgpt on mathematical word problems (mwp)",
      "authors": [
        "Paulo Shakarian",
        "Abhinav Koyyalamudi",
        "Noel Ngu",
        "Lakshmivihari Mareedu"
      ],
      "year": "2023",
      "venue": "An independent evaluation of chatgpt on mathematical word problems (mwp)",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Language models are multilingual chain-of-thought reasoners",
      "authors": [
        "Freda Shi",
        "Mirac Suzgun",
        "Markus Freitag",
        "Xuezhi Wang",
        "Suraj Srivats",
        "Soroush Vosoughi",
        "Hyung Won Chung",
        "Yi Tay",
        "Sebastian Ruder",
        "Denny Zhou",
        "Dipanjan Das",
        "Jason Wei"
      ],
      "year": "2022",
      "venue": "Language models are multilingual chain-of-thought reasoners",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model",
      "authors": [
        "Shaden Smith",
        "Mostofa Patwary",
        "Brandon Norick",
        "Patrick Legresley",
        "Samyam Rajbhandari",
        "Jared Casper",
        "Zhun Liu",
        "Shrimai Prabhumoye",
        "George Zerveas",
        "Vijay Anand Korthikanti",
        "Elton Zhang",
        "Rewon Child",
        "Reza Yazdani Aminabadi",
        "Julie Bernauer",
        "Xia Song",
        "Mohammad Shoeybi",
        "Yuxiong He",
        "Michael Houston",
        "Saurabh Tiwary",
        "Bryan Catanzaro"
      ],
      "year": "2022",
      "venue": "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Galactica: A large language model for science",
      "authors": [
        "Ross Taylor",
        "Marcin Kardas",
        "Guillem Cucurull",
        "Thomas Scialom",
        "Anthony Hartshorn",
        "Elvis Saravia",
        "Andrew Poulton",
        "Viktor Kerkez",
        "Robert Stojnic"
      ],
      "year": "2022",
      "venue": "Galactica: A large language model for science",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "",
      "authors": [
        "Romal Thoppilan",
        "Daniel De Freitas",
        "Jamie Hall",
        "Noam M Shazeer",
        "Apoorv Kulshreshtha",
        "Heng-Tze",
        "Alicia Cheng",
        "Taylor Jin",
        "Leslie Bos",
        "Yu Baker",
        "Yaguang Du",
        "Hongrae Li",
        "Huaixiu Lee",
        "Amin Zheng",
        "Marcelo Ghafouri",
        "Yanping Menegali",
        "Maxim Huang",
        "Dmitry Krikun",
        "James Lepikhin",
        "Dehao Qin",
        "Yuanzhong Chen",
        "Zhifeng Xu",
        "Adam Chen",
        "Maarten Roberts",
        "Yanqi Bosma",
        "Chung-Ching Zhou",
        "I A Chang",
        "Willard James Krivokon",
        "Marc Rusch",
        "Kathleen S Pickett",
        "Meredith Ringel Meier-Hellstern",
        "Tulsee Morris",
        "Renelito Delos Doshi",
        "Toju Santos",
        "Johnny Hartz Duke",
        "Ben Søraker",
        "Vinodkumar Zevenbergen",
        "Mark Prabhakaran",
        "Ben Díaz",
        "Kristen Hutchinson",
        "Alejandra Olson",
        "Erin Molina",
        "Josh Hoffman-John",
        "Lora Lee",
        "Ravindran Aroyo",
        "Alena Rajakumar",
        "Matthew Butryna",
        "V O Lamm",
        "Joseph Kuzmina",
        "Aaron Fenton",
        "Rachel Cohen",
        "Ray Bernstein",
        "Kurzweil"
      ],
      "year": "",
      "venue": "Blaise Aguera-Arcas, Claire Cui, Marian Croak",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "year": "",
      "venue": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
      "authors": [
        "Ben Wang",
        "Aran Komatsuzaki"
      ],
      "year": "2021",
      "venue": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Exploring generalization ability of pretrained language models on arithmetic and logical reasoning",
      "authors": [
        "Cunxiang Wang",
        "Boyuan Zheng",
        "Yuchen Niu",
        "Yue Zhang"
      ],
      "year": "2021",
      "venue": "Natural Language Processing and Chinese Computing",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Emergent abilities of large language models",
      "authors": [
        "Jason Wei",
        "Yi Tay",
        "Rishi Bommasani",
        "Colin Raffel",
        "Barret Zoph",
        "Sebastian Borgeaud",
        "Dani Yogatama",
        "Maarten Bosma",
        "Denny Zhou",
        "Donald Metzler"
      ],
      "year": "2022",
      "venue": "Emergent abilities of large language models",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Chain of thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Ed Huai Hsin Chi",
        "Quoc Le",
        "Denny Zhou"
      ],
      "year": "2022",
      "venue": "Chain of thought prompting elicits reasoning in large language models",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Autoformalization with large language models",
      "authors": [
        "Yuhuai Wu",
        "Albert Qiaochu Jiang",
        "Wenda Li",
        "Markus N Rabe",
        "Charles Staats",
        "Mateja Jamnik",
        "Christian Szegedy"
      ],
      "year": "2022",
      "venue": "Autoformalization with large language models",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "authors": [
        "Aohan Zeng",
        "Xiao Liu",
        "Zhengxiao Du",
        "Zihan Wang",
        "Hanyu Lai",
        "Ming Ding",
        "Zhuoyi Yang",
        "Yifan Xu",
        "Wendi Zheng",
        "Xiao Xia",
        "Weng Lam Tam",
        "Zixuan Ma",
        "Yufei Xue",
        "Jidong Zhai",
        "Wenguang Chen",
        "Peng Zhang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "Glm-130b: An open bilingual pre-trained model",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Opt: Open pre-trained transformer language models",
      "authors": [
        "Susan Zhang",
        "Stephen Roller",
        "Naman Goyal",
        "Mikel Artetxe",
        "Moya Chen",
        "Shuohui Chen",
        "Christopher Dewan",
        "Mona Diab",
        "Xian Li",
        "Xi Victoria Lin"
      ],
      "year": "2022",
      "venue": "Opt: Open pre-trained transformer language models",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Least-to-most prompting enables complex reasoning in large language models",
      "authors": [
        "Denny Zhou",
        "Nathanael Scharli",
        "Le Hou",
        "Jason Wei",
        "Nathan Scales",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Olivier Bousquet",
        "Quoc Le",
        "Ed Huai Hsin",
        "Chi"
      ],
      "year": "2022",
      "venue": "Least-to-most prompting enables complex reasoning in large language models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "How Well Do Large Language Models Perform In Arithmetic Tasks?",
      "text": "Zheng Yuan\\({}^{1}\\) Hongyi Yuan\\({}^{12}\\) Chuanqi Tan\\({}^{1}\\) Wei Wang\\({}^{1}\\) Songfang Huang\\({}^{1}\\) \\({}^{1}\\)Alibaba Group \\({}^{2}\\)Tsinghua University {yuanzheng.yuanzhen,chuanqi.tcq,hebian.ww,songfang.hsf}@alibaba-inc.com yuanhy20@mails.tsinghua.edu.cn"
    },
    {
      "title": "Abstract",
      "text": "Large language models have emerged abilities including chain-of-thought to answer math word problems step by step Wei et al. (2022). Solving math word problems not only requires abilities to disassemble problems via chain-of-thought but also needs to calculate arithmetic expressions correctly for each step. To the best of our knowledge, there is no work to focus on evaluating the arithmetic ability of large language models. In this work, we propose an arithmetic dataset **MATH 401** to test latest large language models including GPT-4, ChatGPT, InstructGPT, Galactica, and LLaMA with various arithmetic expressions and provide a detailed analysis of the ability of large language models. MATH 401 and evaluation codes are released at [https://github.com/GanjinZero/math401-1lm](https://github.com/GanjinZero/math401-1lm). 1 Footnote 1: This project is working in progress."
    },
    {
      "title": "1 Introduction",
      "text": "Emergent abilities show in sufficiently large language models (LLMs) Wei et al. (2022) like chain-of-thought reasoning (COT) Wei et al. (2022). Chain-of-thought reasoning requires LLMs to solve a question by thinking questions step by step which performs well in school math word problems Wei et al. (2022); Kojima et al. (2022). Recent LLMs are further fine-tuned with instruction tuning Sanh et al. (2021); Chung et al. (2022); Ouyang et al. (2022) which demonstrates improved COT ability compared to only self-supervised pre-training. To solve a math word problem, COT disassembles the problem into simple steps. For each step, LLMs have to compute correctly based on arithmetic expressions. Thus, evaluating the arithmetic ability of LLMs is necessary since it is the upper bound of LLMs' ability for solving math word problems. To this end, we propose an arithmetic dataset named **MATH 401**. Different difficulties are contained in this dataset including addition (\\(+\\)), subtraction (\\(-\\)), multiplication (\\(\\times\\)), division (\\(\\div\\)), exponentiation (\\(\\land\\)), trigonometry functions (\\(\\sin,\\cos,\\tan\\)), and logarithm functions (\\(\\log,\\ln\\)) of integers, decimals, and irrational numbers (\\(\\pi,e\\)). Long arithmetic expressions with brackets are also included which are common in complex math word problems. Results in Table 1 show detailed evaluations on OpenAI's GPTs including GPT-4 OpenAI, 2023), ChatGPT2, GPT-3.5 Ouyang et al. (2022) and other open-sourced LLMs. We find that GPT-4 and ChatGPT outperform other models by a large margin in all kinds of arithmetic abilities. InstructGPT Ouyang et al. (2022) and Galactica Taylor et al. (2022) do have some arithmetic abilities. We analyze factors affecting LLMs' arithmetic ability systematically including tokenization (SS4.2), pre-training (SS4.3), prompts (SS4.4), interpolation and extrapolation (SS4.5), scaling laws (SS4.6), COT (SS4.7), and ICL (SS4.8). Footnote 2: [https://openai.com/blog/introducing-chatgpt-and-whisper-apis](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) One may say that the ability to solve arithmetic tasks is not necessary for a large language model. LLMs can use the calculator API when they need to decode an answer Schick et al. (2023). Arithmetic ability evaluation can be a gauge for general intelligence since mastering arithmetic serves as a fundamental requirement for performing intricate mathematical tasks including symbolic math reasoning Noorbakhsh et al. (2021); Gaur and Saunshi (2022) and automatic theorem proofing Polu and Sutskever (2020); Wu et al. (2022)."
    },
    {
      "title": "2 Related Works",
      "text": "Evaluate Math Ability of LLMsTo show the math reasoning ability of LLMs, Wang and Komatsuzaki (2021); Chung et al. (2022); Thoppilan et al. (2022) evaluate their models on various math word problems benchmark Saxton et al. (2019); Hendrycks et al. (2021); Cobbe et al. (2021); Shiet al., 2022). For newly released LLM ChatGPT, Shakarian et al. (2023); Frieder et al. (2023) evaluate its mathematical ability independently. To notice, our paper evaluates ChatGPT using gpt-3.5-turbo-0301 version and GPT-4 using chat UI on March 16th which may have different performances compared to their reported results and future analysis. **Evaluate Arithmetic Ability of LLMs**Nogueira et al. (2021); Wang et al. (2021) evaluate pretrained language models on simple arithmetic expressions including addition (\\(+\\)) and subtraction (\\(-\\)). Muffo et al. (2022) have further tested the multiplication (\\(\\times\\)) of language models. They found tokenization (Nogueira et al., 2021; Kim et al., 2021) and token frequency (Razeghi et al., 2022) are two important factors for language model arithmetic ability. Compared to previous work, we focus on evaluating **Large** LMs (with instruction fine-tuning) on comprehensive arithmetic abilities with different types of operators and numbers."
    },
    {
      "title": "3 Evaluation Settings",
      "text": ""
    },
    {
      "title": "Arithmetic Expression Settings",
      "text": "We construct 401 arithmetic expressions to test large language models which include Euler equation (\\(e^{i\\pi}+1=0\\)) as group 0 and 25 problems each for group 1\\(\\sim\\)16. If not otherwise mentioned, used numbers are positive integers. * Euler Equation. * Add & Subtract of two integers within 10. * Add & Subtract of two integers within 100. * Add & Subtract of two integers within 1,000. * Add & Subtract of two integers within -10\\(\\sim\\)10. * Add & Subtract of two decimal numbers within -100\\(\\sim\\)100. * Multiply two integers within 100. * Multiply two decimal numbers within 10. * Multiply two integers within 100,000. * Division of two integers within 100. * Exponentiation of with integer base within 10 and integer exponent within 2\\(\\sim\\)4. * Exponentiation of with a decimal number within 10 as the base and a decimal number within 2\\(\\sim\\)4 as the exponent. * Add, Subtract & Multiply with one integer within 10 and a common irrational number (i.e. \\(e\\) or \\(\\pi\\)). * Long arithmetic expressions with brackets, involved integers are all within 100 and operators contain add, subtract, multiply, and division. * Trigonometry functions including \\(\\sin\\), \\(\\cos\\), and \\(\\tan\\). Inputs can be in the format of degrees and radians (\\(\\pi\\) can also appear in the inputs). * Logarithm of integers within 1000 of different bases: \\(2,e,10\\). \\begin{table} \\begin{tabular}{l c|c c c c c c|c c c c|c c c c} \\hline Model & Size & E & \\(+-\\) & \\(\\times\\) & \\(\\div\\) & \\(\\wedge\\) & Tri & log & Dec & Neg & Irr & Big & Long & Easy & Hard & All \\\\ \\hline GPT-4 &? & ✓ & **99** & **67** & **100** & **50** & **68** & **76** & **67** & **67** & **100** & **48** & **96** & **100** & **67** & **84** \\\\ ChatGPT &? & ✓ & 675 & 80 & **50** & **54** & 56 & **67** & 64 & 40 & 68 & **100** & 49 & 74 \\\\ InstructGPT & 175B & \\(\\times\\) & _83_ & 59 & 80 & 36 & 8 & _16_ & 64 & 64 & 36 & \\(4\\) & _24_ & 92 & 22 & 57 \\\\ CodeX & 175B & ✓ & 36 & 27 & 8 & 10 & 8 & 0 & 25 & 25 & 12 & 0 & 0 & 40 & 4 & 22 \\\\ Galactica & 120B & ✓ & 69 & 43 & _24_ & _44_ & _16_ & 0 & _57_ & _57_ & 28 & 0 & _24_ & 78 & 12 & 45 \\\\ LLAMA & 65B & ✓ & 44 & 35 & 8 & 22 & 8 & 0 & 41 & 41 & 20 & 0 & 4 & 52 & 5 & 28 \\\\ OPT & 175B & ✓ & 33 & 35 & 4 & 12 & 0 & 4 & 25 & 25 & 8 & 0 & 0 & 41 & 2 & 22 \\\\ GPT-Neox & 20B & ✓ & 51 & 48 & 4 & 40 & 4 & 0 & 43 & 43 & 20 & 0 & 8 & 66 & 4 & 35 \\\\ GLM & 130B & ✓ & 39 & 31 & 8 & 22 & 0 & 0 & 29 & 29 & 24 & 0 & 8 & 46 & 5 & 26 \\\\ BloomZ & 176B & \\(\\times\\) & 23 & 37 & 12 & 30 & 8 & 0 & 43 & 43 & 20 & 0 & 8 & 39 & 6 & 22 \\\\ Bloom & 176B & \\(\\times\\) & 21 & 37 & 12 & 30 & 0 & 0 & 37 & 37 & 16 & 0 & 0 & 37 & 4 & 20 \\\\ T0++ & 11B & \\(\\times\\) & 6 & 3 & 0 & 6 & 8 & 0 & 3 & 3 & 4 & 0 & 0 & 7 & 2 & 4 \\\\ Flan-T5 & 11B & \\(\\times\\) & 1 & 13 & 4 & 0 & 0 & 0 & 11 & 11 & 8 & 0 & 0 & 6 & 2 & 4 \\\\ \\hline \\end{tabular} \\end{table} Table 1: Arithmetic ability for LLMs measured by accuracy, we only list models with largest parameter counts. E = Euler, Dec = Decimal, Neg = Negative, Irr = Irrational, Big = Big Numbers, Long = Long Expressions. [MISSING_PAGE_FAIL:3] large margin4. GPT-4 surpasses ChatGPT with accuracy of 10 points and reduce relative error half. InstructGPT performs third measured by accuracy and Galactica-30B performs third measured by relative error. Compared to models proposed before InstructGPT (text-davinci-003), GPT-series applies Reinforcement Learning from Human Feedback (RLHF) which may enhance their arithmetic ability significantly. Galactica is pre-trained with massive LaTeX source codes which could be the reason why Galactica performs well in arithmetics. Footnote 4: OpenAI states they improve the math of ChatGPT since version Jan 30, and we cannot evaluate any previous version. Grouped ResultsTo clearly understand the arithmetic ability of LLMs, we show grouped accuracy in Table 1. GPT-4 obtains first places and ChatGPT obtains second places for all groups. Most LLMs are only capable of doing addition and subtraction and have some ability for multiplication. Division, exponentiation, trigonometry functions, and logarithm functions are hard for most LLMs. LLMs have some abilities dealing with decimal, negative, and irrational numbers. Only GPT-4 and ChatGPT have the ability to deal with big numbers (\\(>1e12\\)) and complex long queries which proves their generalization and reasoning abilities. GPT-4 shows extremely good ability in long arithmetic expressions. When will ChatGPT fail?Though ChatGPT obtains such a good performance, we will check when ChatGPT fails to answer. For multiplication (\\(\\times\\)), ChatGPT passes all queries in Group 7 and 8 and get wrong answers for all queries in Group 9. An example is ChatGPT predicts \\(71786\\times 21638=1,551,402,068\\), while the true answer is \\(1,553,305,468\\). ChatGPT gives a very close estimation with the correct head and tail, which proves that ChatGPT does not use a calculator API for math calculation. For division in Group 11, ChatGPT sometimes gives correct answers to two decimal places which will be considered incorrect in our metric. We can see in Table 5, requiring ChatGPT to output four decimal places will improve its accuracy in multiplication and division. For exponentiation (\\(\\wedge\\)), ChatGPT correctly answers all queries in Group 10 which contain only integers as bases. It is too hard for any language model (even ChatGPT) correctly estimate the exponentiation of a decimal number as the base and a decimal number as the exponent. It seems that ChatGPT treats \\(**\\) as multiplication sometimes, for example, ChatGPT estimates \\(5.5507\\)\\(**\\)\\(2.0434=10.31554\\) which is close to \\(5.5507\\times 2.0434=11.3423\\) and far from answer \\(33.1895\\). For calculating trigonometry functions, ChatGPT understands degrees and radians correctly and generates exact answers for special inputs like \\(cos(-210^{\\circ})=-\\frac{\\sqrt{3}}{2}\\) (we omit explanation generated by ChatGPT here). However, ChatGPT may generate wrong explanations which mislead itself. An example is: \"We know that the sine function is periodic with a period of \\(2\\pi\\), which means that \\(\\sin(x+2\\pi)=\\sin(x)\\) for any value of \\(x\\). Therefore, we can subtract multiples of \\(2\\pi\\) from \\(-3.75\\pi\\) until we get a value between 0 and \\(2\\pi\\): \\(-3.75\\pi=-3\\pi-0.75\\pi=-9.42477-2.35619=-11.78096\\). Adding \\(2\\pi\\), we get: \\(-11.78096+2\\pi\\) = -9.42477 etc.\" Any mistake in explanations may result in a wrong answer. For logarithm functions, we find that ChatGPT is capable of using change of base formula and predicting answers within two decimal places. For long expressions, ChatGPT can understand the operators' priorities. ChatGPT sometimes generates answers step by step and sometimes generates answers directly. It is very likely to generate wrong answers when it decodes answers directly. What about GPT-4?For big number multiplication (\\(\\times\\)) in group 9, GPT-4 also fails in all cases with similar problems occurring in ChatGPT. For exponentiation (\\(\\wedge\\)), GPT-4 will not consider \\(**\\) as \\(\\times\\) anymore and give better estimations. For calculating expressions with irrational numbers, GPT-4 will consider \\(e\\) as natural logarithm correctly. For logarithm functions, GPT-4 calculates logarithm base \\(e\\) and 10 by \"using a calculator\" (this is a message generated by GPT-4). GPT-4 calculates logarithm base 2 by change of base formula and generates approximate results. For long equations, GPT-4 solves all equations step by step and obtains a much higher accuracy. We compare and summarize how GPT-4 outperforms ChatGPT here: * Better division ability. * Better trigonometry ability. * Understand irrational numbers properly. * Always calculate long expressions step by step."
    },
    {
      "title": "Tokenization",
      "text": "Arithmetic expressions have special tokens including \\(\\pi,\\times,\\div,\\div,\\circ\\) which are not within T5 series models (i.e. T0++ and Flan-T5). T0++-11B (Acc 4.24 and RE 3.34) and Flan-T5-xxl-11B (Acc 3.74 and RE 5.78) perform badly on arithmetic tasks compared to other similar-size models: Opt-13B (Acc 15.21 and RE 2.19) and LLaMA-13B (Acc 27.68 and RE 2.4). We notice that Galactica and LLaMA split numbers into individual tokens. For example \\(123.456\\) is converted into \\(1\\ 2\\ 3\\.\\ 4\\ 5\\ 6\\). Razeghi et al. (2022) show that arithmetic ability is related to pre-training term frequencies. For tokens that appear more in pre-training, LLMs can have better accuracy in answering arithmetic expressions about them. Number tokens with more digits (e.g. 23) apparently appear less than single digit token (e.g. 2 and 3). Splitting numbers into individual tokens neglects all number tokens with more digits and makes all single digit tokens (mainly \\(0\\sim 9\\)) appear in the pre-training corpus in the same order of magnitude. Galactica-30B and LLaMA-30B obtain 45.14 and 30.17 in terms of accuracy (list in Table 3) that outperforms OPT-30B (15.96), Bloom-176B (20.2), and GLM-130B (25.94), which show superiority of digit-level tokenization."
    },
    {
      "title": "Training",
      "text": "Self-supervised Pre-trainingWhile pre-training, code corpus and LaTeX-sources are possible to relate to arithmetic ability since they all contain arithmetic operators and numbers. Code-davinci-002 is pretrained with code corpus. Code-davinci-002 performs well on many reasoning-related tasks (Zhou et al., 2022), however, it performs not good compared to other LLMs in arithmetics. This proves that mathematical reasoning ability is different from arithmetic ability which needs to understand numbers deeply. Galactica with numerous LaTeX-sources outperforms other LLMs except for InstructGPT and ChatGPT which show LaTeX is useful. Instruction Tuningis also very important in arithmetic ability. Comparing Opt-30B (Acc 15.96 RE 2.28 NNR 11.22) with Opt-Iml-Max-30B (Acc 17.46 RE 1.52 NNR 6.23), Bloom (Acc 20.2 RE 2.6 NNR 18.45) with BloomZ (Acc 22.44 RE 1.5 NNR 4.74), and code-davinci-002 (Acc 21.7) with text-davinci-002 (Acc 42.89) in Table 3 show that instruction tuning can boost the performance in all metrics. Text-davinci-003 (RLHF) outperforms text-davinci-002 (SFT) in arithmetic tasks which shows RLHF is important for building arithmetic ability."
    },
    {
      "title": "Prompts",
      "text": "Input PromptsWe find the best prompts are different across LLMs. We list the best and worst prompts for LLMs in Table 8. We find models are sensitive to input prompts and not using prompts is the worst option for most LLMs. For InstructGPT and ChatGPT, using \"Calculate\" as a prompt perform best. For other LLMs, using LaTeX-related prompts perform best. System PromptsFor ChatGPT, we can also provide system-level messages as instruction prompts. Table 5 shows providing system-level messages improves ChatGPT's accuracy and reduces relative error significantly. The most different groups are group 13 irrational numbers and group 16 logarithm functions. Without a system-level message, ChatGPT thinks \\(e\\) can be Euler's number or a variable and cannot give an answer. For logarithm functions, ChatGPT tries to explain how it calculates which may mislead our provided parser. We notice that if we require ChatGPT to output results to four decimal places, it will have a zero non-number ratio. To conclude, ChatGPT will try to explain the calculation procedure without a system-level prompt and will only provide answers with a system-level prompt."
    },
    {
      "title": "Interpolation And Extrapolation",
      "text": "LLMs have strong abilities to fit on in-domain data. If pretraining corpora contain arithmetic expressions, it is easy for LLMs to memorize them. For out-of-domain data, LLMs need to extrapolate how to calculate them. We do not know what are in-domain data and out-of-domain data for models (especially ChatGPT), so it is hard to test their interpolation and extrapolation abilities. We use the easy group and the hard group to estimate the interpolation and extrapolation abilities. The easy group queries have possibilities that appear in the pretraining corpora or instruct-tuning, while the hard group queries contain big numbers / decimal numbers / long expressions which are very unlikely to be covered by pretraining corpora or instructions. Thus answering easy queries may examine the interpolation ability of models and answering hard queries must examine the extrapolation ability of the models. We find ChatGPT performs best on hard queries, and all other models have limited performance on hard queries which show limited extrapolation."
    },
    {
      "title": "Scaling Laws",
      "text": "To understand how parameter counts influence arithmetic ability, we plot the results with different-size LLMs in Figure 1. We do not plot text-davinci-003, gpt-3.5-turbo-0301 and gpt-4 since they do not have smaller versions with the same setting. We find that LLMs have better abilities with larger parameter counts. An interesting phenomenon we found is model over 30B does not improve significantly compared with 30B models, especially in Galactic where the 120B model performs the same as the 30B model. We hypothesize that 30B may be enough for arithmetic ability. ChatGPT may be a model smaller than 175B which outperforms other 175B models a lot, thus larger parameter count does not guarantee better arithmetic ability. For GPT-4, we cannot have any possible guess. Considering its much slower inference speed, we guess it has larger \\begin{table} \\begin{tabular}{l|c c c c} \\hline Model & Best & Acc & Worst & Acc \\\\ \\hline gpt-3.5-turbo-0301 & Cal* & **75.06** & SS & 64.59 \\\\ text-davinci-003 & Cal & 56.61 & Eqa & 43.64 \\\\ galactic-120b & Eqa & 45.14 & \\(\\emptyset\\) & 38.9 \\\\ llama-65b & Eqa & 28.43 & Cal & 4.74 \\\\ opt-175b & Cal & 21.7 & \\(\\emptyset\\) & 15.21 \\\\ gpt-neox-20b & Eqa & 35.41 & \\(\\emptyset\\) & 26.93 \\\\ glm-130b & \\$ & 25.94 & \\(\\emptyset\\) & 22.44 \\\\ bloomz-176b & SS & 22.44 & \\(\\emptyset\\) & 11.72 \\\\ \\hline \\end{tabular} \\end{table} Table 4: Best and worst prompts for different LLMs. Figure 1: Performances of MATH 401 on LLMs with different sizes. We do not know the parameter count of ChatGPT. We list InstructGPT results with SFT setting (text-davinci-002) only for a fair comparison. \\begin{table} \\begin{tabular}{l|c c c c c} \\hline Group & \\multicolumn{2}{c}{Cal} & \\multicolumn{2}{c}{Cal*} & \\multicolumn{2}{c}{Cal*4} \\\\ & Acc & RE & Acc & RE & Acc & RE \\\\ \\hline 0 Euler & **100** & **.00** & **100** & **.00** & **100** & **.00** \\\\ \\(1\\sim 6+-\\) & **97** & **.00** & 96 &.00 & 93 &.01 \\\\ \\(7\\sim 10\\times+\\) & 69 &.20 & 69 & **.01** & **71** & **.01** \\\\ \\(11\\sim 12\\wedge\\) & **50** & **.24** & **50** &.32 & **50** &.27 \\\\ \\(13\\) Irr. & 64 & 1.73 & 72 &.56 & **84** & **.11** \\\\ \\(14\\) Long & **68** & **.19** & 64 &.46 & 60 &.59 \\\\ \\(15\\) Tri. & 44 & 1.21 & **48** & **.96** & 44 & 1.40 \\\\ \\(16\\) Log & 56 &.80 & **60** &.04 & 56 & **.01** \\\\ \\hline Overall & 74 &.33 & **75** & **.14** & 74 & **.14** \\\\ \\hline \\end{tabular} \\end{table} Table 5: Comparing different system prompts in ChatGPT on MATH 401. Cal means no system prompt. * = “You are an accurate calculator.” 4 = “Calculating to four decimal places.” [MISSING_PAGE_FAIL:7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. _ArXiv_, abs/2005.14165. * Chen et al. (2020) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Feelipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. _ArXiv_, abs/2107.03374. * Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_. * Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_. * Frieder et al. (2023) Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and J J Berner. 2023. Mathematical capabilities of chatgpt. _ArXiv_, abs/2301.13867. * Gaur and Saunshi (2022) Vedant Gaur and Nikunj Saunshi. 2022. Symbolic math reasoning with language models. In _2022 IEEE MIT Undergraduate Research Technology Conference (URTC)_, pages 1-5. * Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. _arXiv preprint arXiv:2103.03874_. * Kim et al. (2021) Jeonghwan Kim, Giwon Hong, Kyung min Kim, Junmo Kang, and Sung-Hyon Myaeng. 2021. Have you seen that number? investigating extrapolation in question answering models. In _Conference on Empirical Methods in Natural Language Processing_. * Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. _ArXiv_, abs/2205.11916. * Muennighoff et al. (2022) Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2022. Crosslingual generalization through multitask finetuning. * Muffo et al. (2022) Matteo Muffo, Aldo Cocco, and Enrico Bertino. 2022. Evaluating transformer language models on arithmetic operations using number decomposition. In _International Conference on Language Resources and Evaluation_. * Nogueira et al. (2021) Rodrigo Nogueira, Zhiying Jiang, and Jimmy J. Li. 2021. Investigating the limitations of the transformers with simple arithmetic tasks. _ArXiv_, abs/2102.13019. * Noorbakhsh et al. (2021) Kimia Noorbakhsh, Modar Sulaiman, Mahdi Sharifi, Kallol Roy, and Pooyan Jamshidi. 2021. Pretrained language models are symbolic mathematics solvers too! _arXiv preprint arXiv:2110.03501_. * OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. * Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_. * Polu and Sutskever (2020) Stanislas Polu and Ilya Sutskever. 2020. Generative language modeling for automated theorem proving. _ArXiv_, abs/2009.03393. * Razeghi et al. (2022) Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot reasoning. _ArXiv_, abs/2202.07206. * Sanh et al. (2021) Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheeshth Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. 2021. Multitask prompted training enables zero-shot task generalization. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. _arXiv preprint arXiv:1904.01557_. * Le Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Elie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_. * Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. _ArXiv_, abs/2302.04761. * Shakarian et al. (2023) Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu. 2023. An independent evaluation of chatgpt on mathematical word problems (mwp). * Shi et al. (2022) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2022. Language models are multilingual chain-of-thought reasoners. _ArXiv_, abs/2210.03057. * Smith et al. (2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Anand Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022. Using deepspeed and megaton to train megatron-turing nlg 530b, a large-scale generative language model. _ArXiv_, abs/2201.11990. * Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. _arXiv preprint arXiv:2211.09085_. * Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee, Huaixin Zheng, Amin Ghafouri, Marcelo Menegal, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Hartz Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffmann, John, Josh Lee, Lora Aroyo, Ravindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Huai hsin Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications. _ArXiv_, abs/2201.08239. * Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_. * Wang and Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax). * Wang et al. (2021) Cunxiang Wang, Boyuan Zheng, Yuchen Niu, and Yue Zhang. 2021. Exploring generalization ability of pretrained language models on arithmetic and logical reasoning. In _Natural Language Processing and Chinese Computing_. * Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_. * Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. _ArXiv_, abs/2201.11903. * Wu et al. (2022) Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. 2022. Autoformalization with large language models. _ArXiv_, abs/2205.12615. * Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. 2022. Glm-130b: An open bilingual pre-trained model. _arXiv preprint arXiv:2210.02414_. * Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_. * Zhou et al. (2022) Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Huai hsin Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. _ArXiv_, abs/2205.10625."
    },
    {
      "title": "Appendix A Examples From Math 401",
      "text": "We list examples for each group from MATH 401. * \\(e^{i\\pi}+1=0\\)* \\(5+9=14\\) * \\(21+97=118\\) * \\(721-847=-126\\) * \\(714637232158-667119914538=47517317620\\) * \\(-1+(-6)=-7\\) * \\(-0.038+0.0092=-0.0288\\) * \\(78\\times 64=4992\\) * \\(5.0\\times 0.09=0.045\\) * \\(45960\\times 59693=2743490280\\) * \\(70+61=1.1475\\) * \\(7^{4}=2401\\) * \\(2.242^{3.7342}=20.3865\\) * \\(e+\\pi=5.8598\\) * \\((4\\times 64)\\times(39+12)=13056\\) * \\(\\sin(-3.75\\pi)=0.7071\\) * \\(\\log_{10}(797)=2.9015\\)"
    }
  ]
}