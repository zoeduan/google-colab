{
  "title": "How to Protect Copyright Data in Optimization of Large Language Models?",
  "authors": [
    "Timothy Chu",
    "Zhao Song",
    "Chiwun Yang"
  ],
  "abstract": "\n Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function. In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data. \n",
  "references": [
    {
      "id": null,
      "title": "How to Protect Copyright Data in Optimization of Large Language Models?",
      "authors": [
        "Timothy Chu",
        "Zhao Song",
        "Chiwun Yang"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Palm 2 technical report",
      "authors": [
        "Andrew M Adf + 23 ; Rohan Anil",
        "Orhan Dai",
        "Melvin Firat",
        "Dmitry Johnson",
        "Alexandre Lepikhin",
        "Siamak Passos",
        "Emanuel Shakeri",
        "Paige Taropa",
        "Zhifeng Bailey",
        "Chen"
      ],
      "year": "2023",
      "venue": "Palm 2 technical report",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks",
      "authors": [
        "Adh + 19a] Sanjeev",
        "Simon Arora",
        "Wei Du",
        "Zhiyuan Hu",
        "Ruosong Li",
        "Wang"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "On exact computation with an infinitely wide neural net",
      "authors": [
        "Adh + 19b] Sanjeev",
        "Simon S Arora",
        "Wei Du",
        "Zhiyuan Hu",
        "Russ R Li",
        "Ruosong Salakhutdinov",
        "Wang"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "A theory for emergence of complex skills in language models",
      "authors": [
        "Sanjeev Arora",
        "Anirudh Goyal"
      ],
      "year": "2023",
      "venue": "A theory for emergence of complex skills in language models",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Bypass exponential time preprocessing: Fast neural network training via weight-data correlation preprocessing",
      "authors": [
        "Josh Alman",
        "Jiehao Liang",
        "Zhao Song",
        "Ruizhe Zhang",
        "Danyang Zhuo"
      ],
      "year": "2022",
      "venue": "Bypass exponential time preprocessing: Fast neural network training via weight-data correlation preprocessing",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Fast attention requires bounded entries",
      "authors": [
        "Josh Alman",
        "Zhao Song"
      ],
      "year": "2023",
      "venue": "Fast attention requires bounded entries",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "A convergence theory for deep learning via over-parameterization",
      "authors": [
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li",
        "Zhao Song"
      ],
      "year": "2019",
      "venue": "International conference on machine learning",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "On the convergence rate of training recurrent neural networks",
      "authors": [
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li",
        "Zhao Song"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Try bard, an ai experiment by google",
      "authors": [
        "Bard"
      ],
      "year": "2023",
      "venue": "Try bard, an ai experiment by google",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "authors": [
        "Bce + 23 ; Sébastien",
        "Varun Bubeck",
        "Ronen Chandrasekaran",
        "Johannes Eldan",
        "Eric Gehrke",
        "Ece Horvitz",
        "Peter Kamar",
        "Yin Lee",
        "Yuanzhi Tat Lee",
        "Scott Li",
        "Lundberg"
      ],
      "year": "2023",
      "venue": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "All are worth words: A vit backbone for diffusion models",
      "authors": [
        "Bnx + 23] Fan",
        "Shen Bao",
        "Kaiwen Nie",
        "Yue Xue",
        "Chongxuan Cao",
        "Hang Li",
        "Jun Su",
        "Zhu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Training (overparametrized) neural networks in near-linear time",
      "authors": [
        "Jan Van Den Brand",
        "Binghui Peng",
        "Zhao Song",
        "Omri Weinstein"
      ],
      "year": "2020",
      "venue": "Training (overparametrized) neural networks in near-linear time",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Algorithm and hardness for dynamic attention maintenance in large language models",
      "authors": [
        "Jan Van Den",
        "Zhao Brand",
        "Tianyi Song",
        "Tri Zhou ; Beidi Chen",
        "Eric Dao",
        "Zhao Winsor",
        "Atri Song",
        "Christopher Rudra",
        "Ré"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Generalization bounds of stochastic gradient descent for wide and deep neural networks",
      "authors": [
        "Yuan Cao",
        "Quanquan Gu"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Gram-gauss-newton method: Learning overparameterized neural networks for regression problems",
      "authors": [
        "Cgh + 19 ; Tianle",
        "Ruiqi Cai",
        "Jikai Gao",
        "Siyu Hou",
        "Dong Chen",
        "Di Wang",
        "Zhihua He",
        "Liwei Zhang",
        "Wang"
      ],
      "year": "2019",
      "venue": "Gram-gauss-newton method: Learning overparameterized neural networks for regression problems",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Mongoose: A learnable lsh framework for efficient neural network training",
      "authors": [
        "Beidi Chatgpt",
        "Zichang Chen",
        "Binghui Liu",
        "Zhaozhuo Peng",
        "Jonathan Lingjie Xu",
        "Tri Li",
        "Zhao Dao",
        "Anshumali Song",
        "Christopher Shrivastava",
        "Re"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2022",
      "venue": "Scaling language modeling with pathways",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Exploring vision transformers as diffusion learners",
      "authors": [
        "Jianan Cao",
        "Tianhe Wang",
        "Xianbiao Ren",
        "Yihao Qi",
        "Yuan Chen",
        "Lei Yao",
        "Zhang"
      ],
      "year": "2022",
      "venue": "Exploring vision transformers as diffusion learners",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Dbk + 20 ; Alexey",
        "Lucas Dosovitskiy",
        "Alexander Beyer",
        "Dirk Kolesnikov",
        "Xiaohua Weissenborn",
        "Thomas Zhai",
        "Mostafa Unterthiner",
        "Matthias Dehghani",
        "Georg Minderer",
        "Sylvain Heigold",
        "Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pretraining of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Creating images from text",
      "authors": [
        "Dall-E Dall"
      ],
      "year": "2021",
      "venue": "Creating images from text",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "",
      "authors": [
        "Dall-E2"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Dall•e 2 pre-training mitigations",
      "authors": [],
      "year": "2022",
      "venue": "OpenAI Research",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Zero-th order algorithm for softmax attention optimization",
      "authors": [
        "Yichuan Deng",
        "Zhihang Li",
        "Zhao Sridhar Mahadevan",
        "Song"
      ],
      "year": "2023",
      "venue": "Zero-th order algorithm for softmax attention optimization",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Attention scheme inspired softmax regression",
      "authors": [
        "Yichuan Deng",
        "Zhihang Li",
        "Zhao Song"
      ],
      "year": "2023",
      "venue": "Attention scheme inspired softmax regression",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Gradient descent provably optimizes over-parameterized neural networks",
      "authors": [
        "Xiyu Simon S Du",
        "Barnabas Zhai",
        "Aarti Poczos",
        "Singh"
      ],
      "year": "2018",
      "venue": "Gradient descent provably optimizes over-parameterized neural networks",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "United State Courts for the 9th circuits. Copying-access and substantial similarity. Model Civil Jury instructions",
      "authors": [],
      "year": "2022",
      "venue": "United State Courts for the 9th circuits. Copying-access and substantial similarity. Model Civil Jury instructions",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Copyright infringement in ai-generated artworks",
      "authors": [
        "Jessica L Gillotte"
      ],
      "year": "2019",
      "venue": "UC Davis L. Rev",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "An over-parameterized exponential regression",
      "authors": [
        "Yeqi Gao",
        "Zhao Sridhar Mahadevan",
        "Song"
      ],
      "year": "2023",
      "venue": "An over-parameterized exponential regression",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Differentially private attention computation",
      "authors": [
        "Yeqi Gao",
        "Zhao Song",
        "Xin Yang"
      ],
      "year": "2023",
      "venue": "Differentially private attention computation",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Gradientcoin: A peer-to-peer decentralized large language models",
      "authors": [
        "Yeqi Gao",
        "Zhao Song",
        "Junze Yin"
      ],
      "year": "2023",
      "venue": "Gradientcoin: A peer-to-peer decentralized large language models",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Fast quantum algorithm for attention computation",
      "authors": [
        "Yeqi Gao",
        "Zhao Song",
        "Xin Yang",
        "Ruizhe Zhang"
      ],
      "year": "2023",
      "venue": "Fast quantum algorithm for attention computation",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Patents in an era of infinite monkeys and artificial intelligence",
      "authors": [
        "Ben Hattenbach",
        "Joshua Glucoft"
      ],
      "year": "2015",
      "venue": "Stan. Tech. L. Rev",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Fl-ntk: A neural tangent kernel-based framework for federated learning analysis",
      "authors": [
        "Baihe Huang",
        "Xiaoxiao Li",
        "Zhao Song",
        "Xin Yang"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Artificial intelligence and the copyright dilemma",
      "authors": [
        "Kalin Hristov",
        "; Kai Han",
        "Yunhe Wang",
        "Hanting Chen",
        "Xinghao Chen",
        "Jianyuan Guo",
        "Zhenhua Liu",
        "Yehui Tang",
        "An Xiao",
        "Chunjing Xu",
        "Yixing Xu"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Protecting intellectual property of language generation apis with lexical watermark",
      "authors": [
        "Qiongkai Hxl + 22] Xuanli He",
        "Lingjuan Xu",
        "Fangzhao Lyu",
        "Chenguang Wu",
        "Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Cater: Intellectual property protection on text generation apis via conditional watermarks",
      "authors": [
        "Qiongkai Hxz + 22] Xuanli He",
        "Yi Xu",
        "Lingjuan Zeng",
        "Fangzhao Lyu",
        "Jiwei Wu",
        "Ruoxi Li",
        "Jia"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "A phd student's perspective on research in nlp in the era of very large language models",
      "authors": [
        "Ija + 23] Oana",
        "Zhijing Ignat",
        "Artem Jin",
        "Laura Abzaliev",
        "Santiago Biester",
        "Naihao Castro",
        "Xinyi Deng",
        "Aylin Gao",
        "Jacky Gunal",
        "Ashkan He",
        "Kazemi"
      ],
      "year": "2023",
      "venue": "A phd student's perspective on research in nlp in the era of very large language models",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
      "authors": [
        "Dongfu Jiang",
        "Xiang Ren",
        "Bill Yuchen",
        "Lin"
      ],
      "year": "2023",
      "venue": "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks",
      "authors": [
        "Ziwei Ji",
        "Matus Telgarsky"
      ],
      "year": "2019",
      "venue": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "A watermark for large language models",
      "authors": [
        "Kgw + 23] John",
        "Jonas Kirchenbauer",
        "Yuxin Geiping",
        "Jonathan Wen",
        "Ian Katz",
        "Tom Miers",
        "Goldstein"
      ],
      "year": "2023",
      "venue": "A watermark for large language models",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "The efficient transformer",
      "authors": [
        "Nikita Kitaev",
        "Lukasz Kaiser",
        "Anselm Levskaya",
        "Reformer"
      ],
      "year": "2020",
      "venue": "The efficient transformer",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Learning overparameterized neural networks via stochastic gradient descent on structured data. Advances in neural information processing systems",
      "authors": [
        "Yuanzhi Li",
        "Yingyu Liang"
      ],
      "year": "2018",
      "venue": "Learning overparameterized neural networks via stochastic gradient descent on structured data. Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "A scalable stochastic second-order optimizer for language model pre-training",
      "authors": [
        "Llh + 23] Hong",
        "Zhiyuan Liu",
        "David Li",
        "Percy Hall",
        "Tengyu Liang",
        "Ma",
        "Sophia"
      ],
      "year": "2023",
      "venue": "A scalable stochastic second-order optimizer for language model pre-training",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Generalized leverage score sampling for neural networks",
      "authors": [
        "Ruoqi Lss + 20] Jason D Lee",
        "Zhao Shen",
        "Mengdi Song",
        "Wang"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "The closeness of in-context learning and weight shifting for softmax regression",
      "authors": [
        "Lsx + 23] Shuai",
        "Zhao Li",
        "Yu Song",
        "Tong Xia",
        "Tianyi Yu",
        "Zhou"
      ],
      "year": "2023",
      "venue": "The closeness of in-context learning and weight shifting for softmax regression",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Solving regularized exp, cosh and sinh regression problems",
      "authors": [
        "Zhihang Li",
        "Zhao Song",
        "Tianyi Zhou"
      ],
      "year": "2023",
      "venue": "Solving regularized exp, cosh and sinh regression problems",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Deja vu: Contextual sparsity for efficient llms at inference time",
      "authors": [
        "Lwd + 23 ; Zichang",
        "Jue Liu",
        "Tri Wang",
        "Tianyi Dao",
        "Binhang Zhou",
        "Zhao Yuan",
        "Anshumali Song",
        "Ce Shrivastava",
        "Yuandong Zhang",
        "Christopher Tian",
        "Re"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Fine-tuning language models with just forward passes",
      "authors": [
        "Mgn + 23] Sadhika",
        "Tianyu Malladi",
        "Eshaan Gao",
        "Alex Nichani",
        "Jason D Damian",
        "Danqi Lee",
        "Sanjeev Chen",
        "Arora"
      ],
      "year": "2023",
      "venue": "Fine-tuning language models with just forward passes",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Bounding the width of neural networks via coupled initialization a worst case analysis",
      "authors": [
        "Alexander Munteanu",
        "Simon Omlor",
        "Zhao Song",
        "David Woodruff"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Signal propagation in transformers: Theoretical perspectives and the role of rank collapse",
      "authors": [
        "Lorenzo Noci",
        "Sotiris Anagnostidis",
        "Luca Biggio",
        "Antonio Orvieto",
        "Sidak Pal Singh",
        "Aurelien Lucchi"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks",
      "authors": [
        "Samet Oymak",
        "Mahdi Soltanolkotabi"
      ],
      "year": "2020",
      "venue": "IEEE Journal on Selected Areas in Information Theory",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Trainable transformer in transformer",
      "authors": [
        "Abhishek Panigrahi",
        "Sadhika Malladi",
        "Mengzhou Xia",
        "Sanjeev Arora"
      ],
      "year": "2023",
      "venue": "Trainable transformer in transformer",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Efficient sgd neural network training via sublinear activated neuron identification",
      "authors": [
        "Lianke Qin",
        "Zhao Song",
        "Yuanyuan Yang"
      ],
      "year": "2023",
      "venue": "Efficient sgd neural network training via sublinear activated neuron identification",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "On the efficiency of adapters in transformers",
      "authors": [
        "Andreas Rücklé",
        "Gregor Geigle",
        "Max Glockner",
        "Tilman Beck",
        "Jonas Pfeiffer",
        "Nils Reimers",
        "Iryna Gurevych",
        "Adapterdrop"
      ],
      "year": "2020",
      "venue": "On the efficiency of adapters in transformers",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "authors": [
        "Archit Rsm + 23 ; Rafael Rafailov",
        "Eric Sharma",
        "Stefano Mitchell",
        "Christopher D Ermon",
        "Chelsea Manning",
        "Finn"
      ],
      "year": "2023",
      "venue": "Direct preference optimization: Your language model is secretly a reward model",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "The new legal landscape for text mining and machine learning",
      "authors": [
        "Matthew Sag"
      ],
      "year": "2018",
      "venue": "J. Copyright Soc'y USA",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Intellicode compose: Code generation using transformer",
      "authors": [
        "Alexey Svyatkovskiy",
        "Shengyu Shao Kun Deng",
        "Neel Fu",
        "Sundaresan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Representational strengths and limitations of transformers",
      "authors": [
        "Clayton Sanford",
        "Daniel Hsu",
        "Matus Telgarsky"
      ],
      "year": "2023",
      "venue": "Representational strengths and limitations of transformers",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "Quadratic suffices for over-parametrization via matrix chernoff bound",
      "authors": [
        "Zhao Song",
        "Xin Yang"
      ],
      "year": "2019",
      "venue": "Quadratic suffices for over-parametrization via matrix chernoff bound",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Does fine-tuning gpt-3 with the openai api leak personally-identifiable information",
      "authors": [
        "Albert Yu Sun",
        "Eliott Zemour",
        "Arushi Saxena",
        "Udith Vaidyanathan",
        "Eric Lin",
        "Christian Lau",
        "Vaikkunth Mugunthan"
      ],
      "year": "2023",
      "venue": "Does fine-tuning gpt-3 with the openai api leak personally-identifiable information",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Training multi-layer over-parametrized neural network in subquadratic time",
      "authors": [
        "Zhao Song",
        "Lichen Zhang",
        "Ruizhe Zhang"
      ],
      "year": "2021",
      "venue": "Training multi-layer over-parametrized neural network in subquadratic time",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "Synthesizer: Rethinking self-attention for transformer models",
      "authors": [
        "Tbm + 21] Yi",
        "Dara Tay",
        "Donald Bahri",
        "Da-Cheng Metzler",
        "Zhe Juan",
        "Che Zhao",
        "Zheng"
      ],
      "year": "2021",
      "venue": "International conference on machine learning",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Long range arena: A benchmark for efficient transformers",
      "authors": [
        "Tda + 20] Yi",
        "Mostafa Tay",
        "Samira Dehghani",
        "Yikang Abnar",
        "Dara Shen",
        "Philip Bahri",
        "Jinfeng Pham",
        "Liu Rao",
        "Sebastian Yang",
        "Donald Ruder",
        "Metzler"
      ],
      "year": "2020",
      "venue": "Long range arena: A benchmark for efficient transformers",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Thibaut Tli + 23] Hugo Touvron",
        "Gautier Lavril",
        "Xavier Izacard",
        "Marie-Anne Martinet",
        "Timothée Lachaux",
        "Baptiste Lacroix",
        "Naman Rozière",
        "Eric Goyal",
        "Faisal Hambro",
        "Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Louis Tms + 23] Hugo Touvron",
        "Kevin Martin",
        "Peter Stone",
        "Amjad Albert",
        "Yasmine Almahairi",
        "Nikolay Babaei",
        "Soumya Bashlykov",
        "Prajjwal Batra",
        "Shruti Bhargava",
        "Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "Provable copyright protection for generative models",
      "authors": [
        "Nikhil Vyas",
        "Sham Kakade",
        "Boaz Barak"
      ],
      "year": "2023",
      "venue": "Provable copyright protection for generative models",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vsp + 17 ;",
        "Noam Vaswani",
        "Niki Shazeer",
        "Jakob Parmar",
        "Llion Uszkoreit",
        "Aidan N Jones",
        "Lukasz Gomez",
        "Illia Kaiser",
        "Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "Medsegdiff-v2: Diffusion based medical image segmentation with transformer",
      "authors": [
        "Wff + 23] Junde",
        "Rao Wu",
        "Huihui Fu",
        "Yu Fang",
        "Yanwu Zhang",
        "; Xu",
        "Mike Wu",
        "Zhifeng Schuster",
        "Chen",
        "V Quoc",
        "Mohammad Le",
        "Wolfgang Norouzi",
        "Maxim Macherey",
        "Yuan Krikun",
        "Qin Cao",
        "Klaus Gao",
        "Macherey"
      ],
      "year": "2016",
      "venue": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Wws + 22] Jason",
        "Xuezhi Wei",
        "Dale Wang",
        "Maarten Schuurmans",
        "Fei Bosma",
        "Ed Xia",
        "Chi",
        "V Quoc",
        "Denny Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "Infoprompt: Information-theoretic soft prompt tuning for natural language understanding",
      "authors": [
        "Wyw + 23] Junda",
        "Tong Wu",
        "Rui Yu",
        "Zhao Wang",
        "Ruiyi Song",
        "Handong Zhang",
        "Chaochao Zhao",
        "Shuai Lu",
        "Ricardo Li",
        "; Zheng Henao",
        "Yanxiang Xu",
        "Galen Zhang",
        "Christopher A Andrew",
        "Peter Choquette-Choo",
        "Brendan Kairouz",
        "Jesse Mcmahan",
        "Yuanbo Rosenstock",
        "Zhang"
      ],
      "year": "2023",
      "venue": "Federated learning of gboard language models with differential privacy",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "An improved analysis of training over-parameterized deep neural networks",
      "authors": [
        "Difan Zou",
        "Quanquan Gu"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b73",
      "title": "Speeding up optimizations via data structures: Faster search, sample and maintenance",
      "authors": [
        "Lichen Zhang"
      ],
      "year": "2022",
      "venue": "Speeding up optimizations via data structures: Faster search, sample and maintenance",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "Just one byte (per gradient): A note on low-bandwidth decentralized language model finetuning using shared randomness",
      "authors": [
        "Qian Zhl + 23 ; Eric Zelikman",
        "Percy Huang",
        "Nick Liang",
        "Noah D Haber",
        "Goodman"
      ],
      "year": "2023",
      "venue": "Just one byte (per gradient): A note on low-bandwidth decentralized language model finetuning using shared randomness",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "Why are adaptive methods good for attention models?",
      "authors": [
        "Zkv + 20] Jingzhao",
        "Sai Zhang",
        "Andreas Praneeth Karimireddy",
        "Seungyeon Veit",
        "Sashank Kim",
        "Sanjiv Reddi",
        "Suvrit Kumar",
        "Sra"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b76",
      "title": "Fast convergence of natural gradient descent for over-parameterized neural networks",
      "authors": [
        "Guodong Zhang",
        "James Martens",
        "Roger B Grosse"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b77",
      "title": "Over-parameterized adversarial training: An analysis overcoming the curse of dimensionality",
      "authors": [
        "Zpd + 20] Yi",
        "Orestis Zhang",
        "Plevrakis",
        "Xingguo Simon S Du",
        "Zhao Li",
        "Sanjeev Song",
        "Arora"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b78",
      "title": "Do transformers parse while predicting the masked word? arXiv preprint",
      "authors": [
        "Haoyu Zhao",
        "Abhishek Panigrahi",
        "Rong Ge",
        "Sanjeev Arora"
      ],
      "year": "2023",
      "venue": "Do transformers parse while predicting the masked word? arXiv preprint",
      "doi": ""
    },
    {
      "id": "b79",
      "title": "Opt: Open pre-trained transformer language models",
      "authors": [
        "Zrg + 22] Susan",
        "Stephen Zhang",
        "Naman Roller",
        "Mikel Goyal",
        "Moya Artetxe",
        "Shuohui Chen",
        "Christopher Chen",
        "Mona Dewan",
        "Xian Diab",
        "Xi Li",
        "Victoria Lin"
      ],
      "year": "2022",
      "venue": "Opt: Open pre-trained transformer language models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "How To Protect Copyright Data In Optimization Of Large Language Models?",
      "text": "Timothy Chu hungryTOAN@gmail.com. Google. Zhao Song zsong@adobe.com. Adobe Research. Chiwun Yang christianwang37@gmail.com. Sun Yat-sen University."
    },
    {
      "title": "Contents",
      "text": "* 1 Introduction * 2 Related Work * 3 Preliminary * 3.1 Notations * 3.2 Problem Definition * 4 Methodology: Copyright Regression * 4.1 Softmax Regression * 4.2 Copyright Regression * 4.3 Regularization * 5 Optimization Properties of Objective Function \\(L\\) * 5.1 Gradient and Hessian of \\(L\\) * 5.2 Hessian of \\(L\\) is Positive Definite * 5.3 Hessian of \\(L\\) is Lipschitz * 6 Optimization and Copyright Protection Guarantees * 6.1 Minimizing Loss Guarantee * 6.2 \\(L\\) is \\(\\tau_{c}\\)-Copyright-Protected * 7 Experiment * 7.1 Setup * 7.2 Results and Analysis * 8 Conclusion * A Preliminary * A.1 Notations * A.2 Basic Algebras * A.3 Basic Vector Norm Bounds * A.4 Basic Matrix Norm Bounds * A.5 Basic Positive Semidefinite * A.6 Basic Calculus * B Copyright Regression * B.1 Definitions * B.2 Regularization * C Gradients and Hessians * C.1 Gradient of \\(\\ell(x)\\) * C.2 Gradient of \\(\\ell(x)^{-1}\\) * C.3 Hessian of \\(\\ell(x)\\) * C.4 Hessian of \\(\\ell(x)^{-1}\\) * C.5 Gradient and Hessian of \\(L_{\\rm reg}\\) * C.6 Gradient and Hessian of \\(L\\) D Lower Bound on Hessian * D.1 Main Result * D.2 Definition of matrix functions \\(P_{i}(x)\\) * D.3 Lower Bound Property for Matrix Function \\(P(x)\\) * D.4 Helpful Lemma * D.5 Lower Bound Property for Matrix Function \\(P_{1}(x)\\) * D.6 Lower Bound Property for Matrix Function \\(P_{2.5}(x)\\) * D.7 Lower Bound Property for Matrix Function \\(P_{4}(x)\\) * D.8 Lower Bound Property for Matrix Function \\(P_{5}(x)\\) * E Hessian is Lipschitz * E.1 Main Result * E.2 Helpful Lemma * E.3 Definition of Matrix Functions \\(Q_{i}(x)\\) * E.4 Lipschitz Property for Matrix Function \\(Q_{1}(x)\\) * E.5 Lipschitz Property for Matrix Function \\(Q_{2}(x)\\) * E.6 Lipschitz Property for Matrix Function \\(Q_{3}(x)\\) * E.7 Lipschitz Property for Matrix Function \\(Q_{4}(x)\\) * E.8 Lipschitz Property for Matrix Function \\(Q_{5}(x)\\) * F Lipschitz Tools * F.1 Lipschitz Tool: Scalar Function * F.2 Lipschitz Tool: Vector Function * G Helpful Bounds * H Minimizing Loss Guarantee * I \\(L\\) is \\(\\tau_{c}\\)-Copyright-Protected * I.1 Definitions * I.2 Copyright-Protected Property for \\(x^{*}\\) * J Approximate Newton Method * J.1 Definition and Update Rule * J.2 Approximate of Hessian and Update Rule Introduction Large language models have changed the world, with the rise of generative AI models such as ChatGPT, GPT-4, Llama, BERT, BARD, PaLM, and OPT [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]. These models are able to process natural language effectively, handling a wide range of tasks including story generation, code creation, machine translation, and elementary mathematical problem solving [2, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32]. One core component in the large language model is the **transformer** architecture [23], which is built on a computational step known as _attention_. Transformers have been used in a wide variety of tasks outside of large language models, including generative image systems such as DALL-E [14] and DALL-E2 [14]. Recent research has integrated the transformer architecture with scalable diffusion-based image generation models [24, 25, 26, 27, 28, 29, 30, 31]. Once challenge in generative AI is guaranteeing that outputs are protected from copyright infringement and intellectual property issues [1, 1, 19, 28, 29]. Generative models trained on large corpuses of data can inadvertently generate outputs that are direct copies, or close variants, of copyrighted text or images that the model is trained on. This has led to controversy in using generative artificial intelligence, and past researchers have considered models and theoretical frameworks for evaluating whether generative models are copying data, and how to evaluate and avoid copyright issues that arise [22]. Our paper has two main contributions: 1. We provide an approach for solving general regression problems in a way that avoids generating copyright data. We term this approach _copyright regression_. 2. We show how to protect copyright data in the optimization and training of transformer-based architectures (including most large language models), by solving copyright regression for the softmax function. Solving the copyright regression problem for the softmax function is the key technical contribution of our paper. To establish the copyright regression framework, we provide a new optimization objective for a general regression problem where some outputs are copyrighted. Such a case can arise in when regression outputs are images or sentences, which occurs in transformer-based architectures for language generation and image generation (where we rely on the fact that transformer training can be viewed as a softmax regression problem [13, 22]). To solve copyright regression for the softmax function, we show that the objective function of the softmax copyright regression is convex, and that its Hessian is bounded. Showing this convexity is non-trivial, and requires intricate bounding of key matrix and vector quantities that arise in the softmax copyright regression problem. Establishing convexity and the bounded Hessian property of the objective function in softmax copyright regression allows us to use gradient-based methods to efficiently solve this problem, with guaranteed bounds on convergence and good stability properties. We formally define copyright regression in Section 4, and provide our formal proof guarantees in Section 6."
    },
    {
      "title": "2 Related Work",
      "text": "This section briefly reviews the related research work on privacy and security of AI, theoretical large language model work, and optimization of neural networks. These topics have a close connection to our work. **Privacy and Security.** Generative AI has achieved impressive results in various domains, including images, text, and code. However, preventing copyright infringement is a challenge that needs [MISSING_PAGE_FAIL:5] Preliminary In this section, we present the preliminary concepts and definitions that form the foundation of our paper. We begin by introducing the notations we utilize in Section 3.1. In Section 3.2 we provide the problem definition that we aim to solve."
    },
    {
      "title": "Notations",
      "text": "Now we utilize the following notations and definitions: The \\(\\ell_{p}\\) norm of a vector \\(x\\) is denoted as \\(\\|x\\|_{p}\\), for examples, \\(\\|x\\|_{1}:=\\sum_{i=1}^{n}|x_{i}|\\), \\(\\|x\\|_{2}:=(\\sum_{i=1}^{n}x_{i}^{2})^{1/2}\\) and \\(\\|x\\|_{\\infty}:=\\max_{i\\in[n]}|x_{i}|\\). For a vector \\(x\\in\\mathbb{R}^{n}\\), \\(\\exp(x)\\in\\mathbb{R}^{n}\\) denotes a vector where whose i-th entry is \\(\\exp(x_{i})\\) for all \\(i\\in[n]\\). For \\(n>k\\), for any matrix \\(A\\in\\mathbb{R}^{n\\times k}\\), we denote the spectral norm of \\(A\\) by \\(\\|A\\|\\), i.e., \\(\\|A\\|:=\\sup_{x\\in\\mathbb{R}^{k}}\\|Ax\\|_{2}/\\|x\\|_{2}\\). We denote \\(\\sigma_{\\min}(A)\\) as the minimum singular value of \\(A\\). For two vectors \\(x,y\\in\\mathbb{R}^{n}\\), we denote \\(\\langle x,y\\rangle=\\sum_{i=1}^{n}\\) for \\(i\\in[n]\\). Given two vectors \\(x,y\\in\\mathbb{R}^{n}\\), we denote \\(x\\circ y\\) as a vector whose i-th entry is \\(x_{i}y_{i}\\) for all \\(i\\in[n]\\). We use \\(e_{i}\\in\\mathbb{R}^{n}\\) to denote a vector whose i-th entry is \\(1\\) and all the other entries are \\(0\\). Let \\(x\\in\\mathbb{R}^{n}\\) be a vector. For a vector \\(x\\in\\mathbb{R}^{n}\\), \\(\\operatorname{diag}(x)\\in\\mathbb{R}^{n\\times n}\\) is defined as a diagonal matrix with its diagonal entries given by \\(\\operatorname{diag}(x)_{i,i}=x_{i}\\) for \\(i=1,...,n\\), and all off-diagonal entries are \\(0\\). A symmetric matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) is said to be positive definite (PD) when \\(A\\succ 0\\), for all non-zero vectors \\(x\\in\\mathbb{R}^{n}\\), we have \\(x^{\\top}Ax>0\\). Similarly, a symmetric matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) is said to be positive semidefinite (PSD) when \\(A\\succeq 0\\), for all vectors \\(x\\in\\mathbb{R}^{n}\\), we have \\(x^{\\top}Ax\\geq 0\\)."
    },
    {
      "title": "Problem Definition",
      "text": "To achieve a successful copyright infringement claim in the United States and many other jurisdictions, the plaintiff must provide evidence that demonstrates two key elements. Firstly, they must establish that the defendant had access to the plaintiff's copyrighted work. Secondly, they must show that there are substantial similarities between the defendant's work and the original elements of the plaintiff's work [14]. While access to high-quality copyrighted data is essential for enhancing the performance of AI models, it also introduces legal risks. Therefore, when considering the safety and legality of AI systems, it is imperative to ensure that the ideal language model can effectively learn from all data without producing output that resembles copyrighted material present in its training set. By adhering to these considerations, we can maintain both the integrity of intellectual property rights and the lawful operation of AI technologies. For convenience, we denote training dataset \\(\\mathcal{D}\\), copyright data \\(\\mathcal{C}\\subset\\mathcal{D}\\) and other data \\(\\mathcal{O}=\\mathcal{D}-\\mathcal{C}\\). Our objective is to ensure a model \\(f\\), satisfies: for any input \\(x\\), given a metric \\(L\\), the model's output \\(f(x)\\) will not exhibit substantial similarity to any copyrighted content present in its training set. We enforce this by defining a strict gap \\(\\tau\\) such that the metric \\(L(f(x),C)\\), where \\(C\\in\\mathcal{C}\\), is greater than or equal to \\(\\tau\\) plus the metric \\(L(f(x),O)\\), where \\(O\\in\\mathcal{O}\\). That is \\[L(f(x),C)\\geq\\tau+L(f(x),O).\\] The choice of metric \\(L\\) depends on the specific task, such as Cross Entropy for text generation, mean absolute error or mean square error for regression problems, and Kullback-Leibler divergence or image similarity for image generation, etc. To ensure compliance with copyright laws, we apply \\(\\tau\\) to the average metric \\(L\\) calculated over both \\(\\mathcal{C}\\) and \\(\\mathcal{O}\\), thus implementing a formal and conservative definition. And we convert dataset \\(\\mathcal{D}\\) to a input matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and a target vector \\(b\\in\\mathbb{R}^{n}\\), where \\(n\\) is the size of dataset, \\(d\\) is the dimension of input data. We now provide the definition of problem below. **Definition 1** (\\(\\tau\\)-Copyright-Protected).: _Given matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and vector \\(b\\in\\mathbb{R}^{n}\\) that \\(A=\\begin{bmatrix}A_{1}\\\\ A_{2}\\end{bmatrix}\\), and \\(b=\\begin{bmatrix}b_{1}\\\\ b_{2}\\end{bmatrix}\\), where \\(A_{1}\\in\\mathbb{R}^{n_{1}\\times d}\\), \\(A_{2}\\in\\mathbb{R}^{n_{2}\\times d}\\), \\(b_{1}\\in\\mathbb{R}^{n_{1}}\\), \\(b_{2}\\in\\mathbb{R}^{n_{2}}\\) and \\(n=n_{1}+n_{2}\\). \\(A_{1}\\), \\(b_{1}\\) are the data has copyright issue and \\(A_{2}\\), \\(b_{2}\\) are the data does not have copyright issue. Denote the train objective \\(L\\). Denote \\(\\tau>0\\) a scalar._ _If there is a trained model \\(f_{\\theta}\\) with parameter \\(\\theta\\) that satisfies_ \\[\\frac{L(f_{\\theta}(A_{1}),b_{1})}{n_{1}}\\geq\\tau+\\frac{L(f_{\\theta}(A_{2}),b_ {2})}{n_{2}}\\] _then we say this model \\(f_{\\theta}\\) is \\(\\tau\\)-Copyright-Protected._"
    },
    {
      "title": "4 Methodology: Copyright Regression",
      "text": "A prominent existing approach, as outlined in the work by [23], introduces an algorithm that involves training an additional generative model, denoted as \\(p\\), using non-copyrighted data. This algorithm employs rejection sampling to effectively manage the probability of the model generating copyrighted data. However, it is important to note that this method does have certain limitations. Specifically, it incurs higher computational costs during the decoding process and necessitates the retraining of an additional model. Now we introduce that our method, a simple modification to the standard training objective of generative language models to ensure that their outputs do not infringe upon copyright laws. In accordance with the findings presented in [10], our approach involves decomposing the mechanism of **Attention**[23], into a regression problem termed Softmax Regression. This decomposition enables a deeper examination of the learning process underlying attention training. By adopting this method, we gain valuable insights into the intricacies of attention and its associated learning mechanisms. We propose a modification to the standard training objective of generative language models based on the principles of Softmax Regression. The objective is to train the model to generate desired outputs, denoted as \\(f(A)=b\\). However, in the case of copyrighted data, represented by \\(A_{1}\\in\\mathbb{R}^{n_{1}\\times d}\\) and \\(b_{1}\\in\\mathbb{R}^{n_{1}}\\), we aim to prevent the model from learning to generate these specific outputs. To address this concern, we introduce an additional term \\(L(f(A_{1}),b_{1})^{-1}\\) to the training objective to discourage the model from generating outputs matching the copyrighted data. To control the level of this protection, we introduce a scalar coefficient \\(\\gamma_{c}>0\\). Consequently, the modified training objective becomes \\(L(f(A),b)+\\gamma_{c}L(f(A_{1}),b_{1})^{-1}\\). This modification serves to strike a balance between achieving the desired outputs and avoiding the generation of copyrighted data. The addition of the inverse term in the training objective helps mitigate the model's tendency to generate prohibited outputs, while the coefficient \\(\\gamma_{c}\\) allows for fine-tuning the level of protection. Compare to [23], our approach does not necessitate training additional models and impact the generation speed of the model during decoding. It offers a simple and practical method that can be plug-and-play applied on all training objectives and algorithms in attention-based models, to prevent the output of model from outputting copyrighted data. In Section 4.1, we present the definition of Softmax Regression. In Section 4.2, we present the definition of Copyright Regression. In Section 4.3, we present the regularization of parameters for better optimization."
    },
    {
      "title": "Softmax Regression",
      "text": "In [10], Softmax Regression applies a softmax function, denoted as \\(f\\), to the product of the input matrix \\(A\\) and the parameter vector \\(x\\). The training objective is then defined as minimizing the squared Euclidean distance between \\(f(x)\\) and the target vector \\(b\\), represented as \\(\\langle f(x)-b,f(x)-b\\rangle\\). By optimizing this objective, Softmax Regression aims to gain insights into the learning process of the attention mechanism. We define Softmax Regression as follows **Definition 2** (Softmax Regression in [10]).: _Given a matrix \\(A\\in\\mathbb{R}^{n\\times d}\\), we define_ \\[f(x):=\\langle\\exp(Ax),\\mathbf{1}_{n}\\rangle^{-1}\\exp(Ax)\\] For the convenience of calculation, we define a intermediate operator \\(c(x)\\) as follows **Definition 3**.: _Given a matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and a vector \\(b\\in\\mathbb{R}^{n}\\), let \\(f(x)\\) be defined as Definition 2, we define_ \\[c(x):=f(x)-b\\] We define the training objective of Softmax Regression as follows **Definition 4** (Training Objective of Softmax Regression in [10]).: _Given matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and vector \\(b\\in\\mathbb{R}^{n}\\), let \\(c(x)\\) be defined as Definition 3, we define_ \\[\\ell(x)=\\langle c(x),c(x)\\rangle\\]"
    },
    {
      "title": "Copyright Regression",
      "text": "Given a matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and a vector \\(b\\in\\mathbb{R}^{n}\\) that \\(A=\\begin{bmatrix}A_{1}\\\\ A_{2}\\end{bmatrix}\\), and \\(b=\\begin{bmatrix}b_{1}\\\\ b_{2}\\end{bmatrix}\\), where \\(A_{1}\\in\\mathbb{R}^{n_{1}\\times d}\\), \\(A_{2}\\in\\mathbb{R}^{n_{2}\\times d}\\), \\(b_{1}\\in\\mathbb{R}^{n_{1}}\\), \\(b_{2}\\in\\mathbb{R}^{n_{2}}\\) and \\(n=n_{1}+n_{2}\\). \\(A_{1}\\), \\(b_{1}\\) are the data has copyright issue and \\(A_{2}\\), \\(b_{2}\\) are the data does not have copyright issue. Now to distinguish between train objective of \\(A_{1}\\), \\(b_{1}\\) and \\(A_{2}\\), \\(b_{2}\\), we follow what we did in Section 4.1. We first provide the definition of Softmax Regression function on Copyright Data as follows **Definition 5** (Softmax Regression function on Copyrighted Data).: _Given all data matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and copyrighted data matrix \\(A_{1}\\in\\mathbb{R}^{n_{1}\\times d}\\), we define_ \\[f_{1}(x):=\\langle\\exp(A_{i,*}x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(Ax)\\] _where \\(i\\in[1,n_{1}]\\) denote a integer._ Also, we provide the definition of intermediate operator \\(c(x)\\) as follows **Definition 6**.: _Given all data matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and copyrighted data matrix \\(A_{1}\\in\\mathbb{R}^{n_{1}\\times d}\\) and vector \\(b_{1}\\in\\mathbb{R}^{n}\\), let \\(f_{1}(x)\\) be defined as Definition 5, we define_ \\[c_{1}(x):=f_{1}(x)-b_{1}\\] Now we have officially provided our definition of Copyright Regression below, which can prevent language models from infringing copyright with controllable performance damage and without occupying more resources. **Definition 7**.: _We denote \\(\\ell(x)\\) as Definition 4. The function \\(c_{1}(x)\\) is defined as Definition 6, and we denote \\(\\ell_{1}(x)=\\langle c_{1}(x),c_{1}(x)\\rangle\\) and \\(\\ell_{2}(x):=\\ell(x)-\\ell_{1}(x)\\). Let \\(\\gamma_{c}>0\\) denote a parameter that control loss related to copyright data._ _We consider the following copyright loss_ \\[L_{\\mathrm{copyright}}(x):=0.5\\ell_{1}(x)+\\gamma_{c}\\cdot\\ell_{1}(x)^{-1}+0. 5\\ell_{2}(x)\\] Additionally, by adjusting the value of \\(\\gamma_{c}\\), one can easily control the learning of copyrighted data within the model. This flexibility allows for a more effective and data-sensitive approach to training language models."
    },
    {
      "title": "Regularization",
      "text": "To make sure the stability during training, we add a regularization term on \\(L_{\\mathrm{copright}}(x)\\). We define \\(L_{\\mathrm{reg}}\\) as follows **Definition 8**.: _Given a matrix \\(A\\in\\mathbb{R}^{n\\times d}\\). Given a vector \\(w\\in\\mathbb{R}^{n}\\), we define \\(W=\\mathrm{diag}(w)\\). We define \\(L_{reg}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\) as follows_ \\[L_{\\mathrm{reg}}:=0.5\\|WAx\\|_{2}^{2}\\] After adding regularization term, we define our final objective \\(L\\) as follows **Definition 9**.: _We denote \\(L_{\\mathrm{copyright}}(x)\\) as Definition 7, let \\(L_{reg}\\) be defined as Definition 8, then we define_ \\[L:=L_{\\mathrm{copyright}}(x)+L_{\\mathrm{reg}}\\] _Minimizing \\(L\\) is the **softmax regression on copyrighted data** problem._"
    },
    {
      "title": "5 Optimization Properties Of Objective Function \\(L\\)",
      "text": "The main contribution of this section involves addressing the convexity of the objective function \\(L\\), which allows for more efficient and reliable optimization of \\(L\\). This achievement not only enables us to optimize the objective more effectively but also validates the feasibility of utilizing Copyright Regression for achieving convergence in LLM (Language Model) training. For instance, we can leverage popular optimization algorithms such as gradient descent, Newton's method, and their variants to solve the optimization problem efficiently (see Section 8 in [1]). In Section 5.1, we compute the gradient and hessian of our train objective. In Section 5.2, we show our result that the Hessian of our train objective is Positive Definite. In Section 5.3, we show our result that the Hessian of our train objective is Lipschitz. Thus, we can say our train objective \\(L\\) is convex."
    },
    {
      "title": "Gradient And Hessian Of \\(L\\)",
      "text": "In order to calculate the convergence and optimization of \\(L\\), we first compute the \\(\\nabla L\\) and \\(\\nabla^{2}L\\). We show our result as follows **Lemma 10** (Gradient of \\(L\\), informal version of Lemma 40).: _Given matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) that \\(A=\\begin{bmatrix}A_{1}\\\\ A_{2}\\end{bmatrix}\\), where \\(A_{1},A_{2}\\in\\mathbb{R}^{n_{2}\\times d}\\) and \\(n=n_{1}+n_{2}\\). Also, we are given a vector \\(b\\in\\mathbb{R}^{n}\\) with \\(b=\\begin{bmatrix}b_{1}\\\\ b_{2}\\end{bmatrix}\\), where \\(b_{1},b_{2}\\in\\mathbb{R}^{n_{2}}\\)._ _We denote \\(\\ell_{1}(x)\\) and \\(\\ell_{2}(x)\\) as Definition 7, denote \\(L\\) as Definition 9, denote \\(f(x)\\) as Definition 2, denote \\(c(x)\\) as Definition 3. Give a vector \\(w\\in\\mathbb{R}^{n}\\), we define \\(W=\\operatorname{diag}(w)\\)._ _We have_ \\[\\frac{\\mathrm{d}L}{\\mathrm{d}x}= \\ A_{*,i}^{\\top}(-f(x)c(x)^{\\top}f(x)+\\operatorname{diag}(f(x))c (x))+2\\gamma_{c}\\ell_{1}(x)^{-2}\\cdot A_{1*,i}^{\\top}(f_{1}(x)c_{1}(x)^{\\top}f _{1}(x)\\] \\[-\\operatorname{diag}(f_{1}(x))c_{1}(x))+A^{\\top}W^{2}Ax\\] _where \\(i\\in[1,n]\\) denote a integer._ Please see Appendix C.6 for the proof of Lemma 10. For convenient, we define \\(B(x)\\) and \\(B_{c}(x)\\) (\\(B(x)\\) function on copyrighted data) **Definition 11** (Definition 6.1 in [4]).: _Given matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and vector \\(b\\in\\mathbb{R}^{n}\\) that \\(A=\\begin{bmatrix}A_{1}\\\\ A_{2}\\end{bmatrix}\\), and \\(b=\\begin{bmatrix}b_{1}\\\\ b_{2}\\end{bmatrix}\\), where \\(A_{1}\\in\\mathbb{R}^{n_{1}\\times d}\\), \\(A_{2}\\in\\mathbb{R}^{n_{2}\\times d}\\), \\(b_{1}\\in\\mathbb{R}^{n_{1}}\\), \\(b_{2}\\in\\mathbb{R}^{n_{2}}\\) and \\(n=n_{1}+n_{2}\\). \\(A_{1}\\), \\(b_{1}\\) are the data has copyright issue and \\(A_{2}\\), \\(b_{2}\\) are the data does not have copyright issue._ _Denote \\(f(x)\\) as Definition 2, denote \\(c(x)\\) as Definition 3, denote \\(f_{1}(x)\\) as Definition 5, denote \\(c_{1}(x)\\) as Definition 6. We define \\(B(x)\\) as follows_ \\[B(x)= \\ \\langle 3f(x)-2b,f(x)\\rangle\\cdot f(x)f(x)^{\\top}+\\langle f(x)-b,f(x)\\rangle\\cdot\\operatorname{diag}(f(x))\\] \\[+\\operatorname{diag}((2f(x)-b)\\circ f(x))+(b\\circ f(x))\\cdot f(x) ^{\\top}+f(x)\\cdot(b\\circ f(x))^{\\top}\\] _and then we also define \\(B_{c}(x)\\) as follows_ \\[B_{c}(x)= \\ \\langle 3f_{1}(x)-2b_{1},f_{1}(x)\\rangle\\cdot f_{1}(x)f_{1}(x)^{ \\top}+\\langle f_{1}(x)-b_{1},f_{1}(x)\\rangle\\cdot\\operatorname{diag}(f_{1}(x))\\] \\[+\\operatorname{diag}((2f_{1}(x)-b_{1})\\circ f_{1}(x))+(b_{1}\\circ f _{1}(x))\\cdot f_{1}(x)^{\\top}+f_{1}(x)\\cdot(b_{1}\\circ f_{1}(x))^{\\top}\\] With \\(B(x)\\) and \\(B_{c}(x)\\), we can abbreviate our compute result of Hessian of \\(L\\) as follows **Lemma 12** (Hessian of \\(L\\), informal version of Lemma 41).: _Given matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) that \\(A=\\begin{bmatrix}A_{1}\\\\ A_{2}\\end{bmatrix}\\), where \\(A_{1},A_{2}\\in\\mathbb{R}^{n_{2}\\times d}\\) and \\(n=n_{1}+n_{2}\\). Also, we are given a vector \\(b\\in\\mathbb{R}^{n}\\) with \\(b=\\begin{bmatrix}b_{1}\\\\ b_{2}\\end{bmatrix}\\), where \\(b_{1},b_{2}\\in\\mathbb{R}^{n_{2}}\\)._ _Denote \\(\\ell_{1}(x)\\) and \\(\\ell_{2}(x)\\) as Definition 7, denote \\(L\\) as Definition 9, denote \\(f(x)\\) as Definition 2, denote \\(c(x)\\) as Definition 3, denote \\(B(x)\\) and \\(B_{c}(x)\\) be defined as Definition 11. Given a vector \\(w\\in\\mathbb{R}^{n}\\), we define \\(W=\\operatorname{diag}(w)\\)._ _We have_ \\[\\frac{\\mathrm{d}^{2}L}{\\mathrm{d}x_{i}\\mathrm{d}x_{i}}= \\ A_{*,i}^{\\top}B(x)A_{*,i}^{\\top}+A^{\\top}W^{2}A+2\\gamma_{c} \\ell_{1}(x)^{-2}(16\\cdot\\ell_{1}(x)^{-1}\\cdot(A_{1*,i}^{\\top}(-f_{1}(x)c_{1}(x )^{\\top}f_{1}(x)\\] \\[+\\operatorname{diag}(f_{1}(x))c_{1}(x)))^{2}-A_{1*,i}^{\\top}B_{1 }(x)A_{1*,i}^{\\top})\\] _where \\(i\\in[0,n]\\) denote a integer.__And we also have_ \\[\\frac{\\mathrm{d}^{2}L}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}}= A_{*,i}^{\\top}B(x)A_{*,j}^{\\top}+A^{\\top}W^{2}A+2\\gamma_{c}\\ell_{1}(x)^{-2}(16 \\cdot\\ell_{1}(x)^{-1}\\cdot A_{1*,i}^{\\top}(-f_{1}(x)c_{1}(x)^{\\top}f_{1}(x)\\] \\[+\\mathrm{diag}(f_{1}(x))c_{1}(x))\\cdot A_{1*,j}^{\\top}(-f_{1}(x)c _{1}(x)^{\\top}f_{1}(x)+\\mathrm{diag}(f_{1}(x))c_{1}(x))-A_{1*,i}^{\\top}B_{c}(x) A_{1*,j}^{\\top})\\] _where \\(i,j\\in[1,n]\\) denote two integers, \\(i\\neq j\\)._ Please see Appendix C.6 for the proof of Lemma 12."
    },
    {
      "title": "Hessian Of \\(L\\) Is Positive Definite",
      "text": "After computing the Hessian of \\(L\\), we now show our result that can confirm it is positive definite, which implies that \\(\\nabla^{2}L\\succ 0\\). Therefore, we have strong evidence that \\(L\\) satisfies the condition of convexity, which is a desirable property for optimization purposes. **Lemma 13** (Hessian is positive definite, informal version of Lemma 13).: _Given matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and vector \\(b\\in\\mathbb{R}^{n}\\). Denote \\(\\gamma\\in(0,1)\\) a scalar. Given a vector \\(w\\), denote \\(W=\\mathrm{diag}(w)\\in\\mathbb{R}^{n\\times n}\\). We define \\(w_{i,i}^{2}\\) as the i-th diagonal entry of matrix \\(W^{2}\\in\\mathbb{R}^{n\\times n}\\). Let \\(l>0\\) denote a scalar._ _If for all \\(i\\in[n]\\), \\(w_{i}^{2}\\geq 8+200\\gamma_{c}\\gamma^{-3}+l/\\sigma_{\\min}(A)^{2}\\), we have_ \\[\\nabla^{2}L\\succeq l\\cdot I_{d}\\] Please see Appendix D for the proof of Lemma 13."
    },
    {
      "title": "Hessian Of \\(L\\) Is Lipschitz",
      "text": "We now show our result that confirm the Hessian of \\(L\\) is Lipschitz, which is a desirable property in optimization. This indicates that the second derivatives of \\(L\\) change smoothly within a defined range. By leveraging this Lipschitz property, we can employ gradient-based methods with guaranteed convergence rates and improved stability. Overall, this finding validates the feasibility of utilizing Copyright Regression for achieving convergence in LLM (Language Model) training. **Lemma 14** (Informal version of Lemma 52).: _Denote \\(R\\geq 4\\) denote a scalar. Given a matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and a vector \\(b\\in\\mathbb{R}^{n}\\), \\(\\|A\\|\\leq R\\), \\(\\|b\\|_{2}\\leq 1\\). Given \\(x,y\\in\\mathbb{R}^{d}\\) be two vector parameter for Copyright Regression with conditions \\(\\|x\\|_{2}\\leq R\\), \\(\\|y\\|_{2}\\leq R\\) and \\(\\|A(x-y)\\|_{\\infty}\\leq 0.01\\). Let \\(L\\) be defined as Definition 9, let \\(\\gamma\\in(0,1)\\), let \\(\\beta\\in(0,0.1)\\). Denote \\(H(x):=\\nabla^{2}L(x)\\)._ _Then,_ \\[\\|H(x)-H(y)\\|\\leq(13344\\gamma_{c}+2)\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2}) \\|x-y\\|_{2}\\] Please see Appendix E for the proof of Lemma 14."
    },
    {
      "title": "6 Optimization And Copyright Protection Guarantees",
      "text": "We have already established the convexity of the training objective \\(L\\) in Section 5, providing a strong foundation to confidently pursue the global optimal value of \\(L\\) through optimization techniques. Now we present the main results of this paper: 1) the minimization guarantee of \\(L\\), 2) the copyright protection efficiency of Copyright Regression. Firstly, in Section 6.1, our objective is to minimize \\(L\\) to its optimal value, ensuring that we achieve the most favorable outcome in terms of our training process. The minimization guarantee of \\(L\\) confirms our main result on optimization of Copyright Regression, it also demonstrates the ease of use of Copyright Regression, which can be optimized on any attention-based model. At the same time, denote \\(x^{*}\\) as the optimal solution of training objective \\(L\\), analyzing \\(L(x^{*})\\)'s performance on copyright data can help us to understand how the trained Copyright Regression can avoid copyright infringement. Secondly, in Section 6.2, we aim to demonstrate that the optimal \\(L\\) provides robust protection for its outputs, safeguarding them from potential copyright infringement. By delineating this boundary, we can quantitatively assess the extent to which Copyright Regression preserves the integrity and exclusivity of copyrighted content. This analysis will provide valuable insights into the effectiveness of our approach and its ability to strike a balance between data protection and the need for authorized access."
    },
    {
      "title": "Minimizing Loss Guarantee",
      "text": "We provide our minimum training objective theorem below. **Theorem 15** (Minimizing training objective \\(L\\), informal version of Theorem 69).: _Suppose we have matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and \\(A_{1}\\in\\mathbb{R}^{n_{1}\\times d}\\), \\(n_{1}\\leq n\\), vector \\(b,w\\in\\mathbb{R}^{n}\\). Let \\(L\\) be defined as Definition 9, denote \\(x^{*}\\) as the optimal solution of \\(L\\) where \\(g(x^{*})=\\mathbf{0}_{d}\\) and \\(\\|x^{*}\\|\\leq R\\). Denote \\(R\\geq 10\\) be a positive scalar. Denote \\(M=n^{1.5}\\exp(40R^{2})\\), Let \\(x_{0}\\) be denoted as an initial point where \\(M\\|x_{0}-x^{*}\\|_{2}\\leq 0.11\\), where \\(l>0\\) denoted a scalar._ _For any accuracy \\(\\epsilon\\in(0,0.1)\\) and any failure probability \\(\\delta\\in(0,0.1)\\), there exists a randomized algorithm, with probability \\(1-\\delta\\), it runs \\(T=\\log(\\|x_{0}-x^{*}\\|_{2}/\\epsilon)\\) iteration and outputs a vector \\(\\widetilde{x}\\in\\mathbb{R}^{d}\\) such that_ \\[\\|\\widetilde{x}-x^{*}\\|\\leq\\epsilon\\] _and the time cost of each iteration is_ \\[O((\\mathrm{nnz}(A)+d^{w})\\cdot\\mathrm{poly}(\\log(n/\\delta)))\\] _Here \\(w\\) is the exponent of matrix multiplication. Currently \\(w\\approx 2.373\\)._ Please see Appendix H for the proof of Theorem 15."
    },
    {
      "title": "\\(L\\) Is \\(\\Tau_{C}\\)-Copyright-Protected",
      "text": "Now we provide a boundary that illustrates the efficacy of Copyright Regression in safeguarding copyrighted data, while also addressing the criteria outlined in Definition 1, which serves as our definition of copyright protection in this paper. We set \\(\\ell(x)\\) in Definition 4 as a \\(\\ell_{2}\\) metric for measuring parameter \\(x\\) on learning data \\(A\\). Now we present our result to confirm that training using our Copyright Regression method can ensure that the model's outputs do not infringe copyright. Specifically, we can assert that the trained model \\(L\\) is protected against copyright infringement with a threshold of \\(\\tau_{c}\\) based on Theorem 16 below. **Theorem 16** (Informal version of Theorem 73).: _Let \\(x^{*}\\) be denoted the optimal parameter on Copyright Regression. We define \\(\\ell(x)\\) as Definition 4, denote \\(\\ell(x)\\) as the original train objective of Softmax Regression. Denote \\(\\epsilon_{2}\\in(0,0.1)\\) a scalar. Denote \\(\\tau_{c}:=\\sqrt{2\\gamma_{c}}/n_{1}-\\epsilon_{2}/n_{2}\\), we have_ \\[\\frac{\\ell_{1}(x^{*})}{n_{1}}\\geq\\tau_{c}+\\frac{\\ell_{2}(x^{*})}{n_{2}}\\] _so \\(x^{*}\\) in Copyright Regression is \\(\\tau_{c}\\)-Copyright-Protected._Please see Appendix I for the proof of Theorem 16. Now we have provided evidence of the copyright protection achieved through training under the Copyright Regression objective. This method has been rigorously proven and offers complete control over copyright infringement. However, concerns may arise regarding the potential impact of the Copyright Regression approach on the model's overall performance, particularly when copyright data includes high-quality novels and images that contribute significantly to the model's performance. In fact, language models cannot remember all its train data. Its training loss has a considered range instead of equaling to \\(0\\). Base on this, we only need to let model's performance on copyrighted data be different from model's performance on other data, even this difference is very small, then we can ascertain whether the model has access to these copyright data during output generation and intentionally avoids outputting them. The difference, namely \\(\\tau\\), can be easily control by adjust the value of \\(\\gamma_{c}\\) and \\(n_{1}/n\\), we will continue to explain that why we say this in Section 7."
    },
    {
      "title": "7 Experiment",
      "text": "In order to evaluate and demonstrate the effectiveness of our proposed Copyright Regression approach, we conducted extensive experiments using Softmax Regression. By varying the values of \\(n_{1}\\) (representing the number of data instances with copyright issues) and \\(\\gamma_{c}\\) (the coefficient used to control the Copyright Regression), we compared the results against a baseline model. The experimental findings clearly indicate the efficacy of our method in providing effective copyright protection. In Section 7.1, we provided the details of our experiment. In Section 7.2, we provided experimental results and analyzed the effectiveness of Copyright Regression."
    },
    {
      "title": "Setup",
      "text": "**hyper-parameters.** The hyper-parameters used in each experiment run were set to \\(n=10000\\) and \\(d=512\\). To assess the influence of copyright data with different proportions during training, we varied the value of \\(n_{1}\\) to be \\(n_{1}\\in\\{1000,2000,4000,6000,8000\\}\\). Additionally, to evaluate the impact of different values of \\(\\gamma_{c}\\) on copyright protection, we consider \\(\\gamma_{c}\\) values of \\(\\{0.1,0.15,0.2,0.225,0.25,0.3,\\)\\(0.35,0.4,0.45,0.5\\}\\). **Dataset.** We employed random selection of data from a Gaussian distribution. Specifically, we randomly selected an input matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) from a normal distribution \\(\\mathcal{N}(0,\\mathbf{I}_{d})\\). For the target vector \\(b\\in\\mathbb{R}^{n}\\), we let \\(b=\\langle\\exp(u),\\mathbf{I}_{n}\\rangle^{-1}\\exp(u)\\), where \\(u\\in\\mathcal{N}(0,I_{n})\\). Figure 1: Copyright Regression experiment result **Metrics.** We use two evaluation metrics, including \\(\\text{MAE}(\\widehat{y},y)=\\frac{1}{n}\\sum_{i=1}^{n}(y-\\widehat{y})\\) and \\(\\text{MSE}(\\widehat{y},y)=\\frac{1}{n}\\sum_{i=1}^{n}|y-\\widehat{y}|\\). We define \\(\\tau_{\\text{MSE}}:=\\text{MSE}(f(A_{1}),b_{1})-\\text{MSE}(f(A_{2}),b_{2})\\) and \\(\\tau_{\\text{MAE}}:=\\text{MAE}(f(A_{1}),b_{1})-\\text{MAE}(f(A_{2}),b_{2})\\) as Definition 1, where \\(f\\) denote a model function, \\(A_{1}\\), \\(b_{1}\\) are copyright data and \\(A_{2}\\), \\(b_{2}\\) are other data. **Baseline.** To evaluate the effectiveness of our approach, we conduct a comparative analysis against a baseline method referred to as **Random**. In the **Random** baseline, a parameter vector \\(x\\in\\mathbb{R}^{d}\\) is randomly selected from a normal distribution \\(\\mathcal{N}(0,\\mathbf{I}_{d})\\)."
    },
    {
      "title": "Results And Analysis",
      "text": "**Impact of \\(\\gamma_{c}\\).** The left image of Figure 1 depicts the relationship between the variables \\(\\gamma_{c}\\) and the difference metrics \\(\\tau_{\\text{MSE}}\\) and \\(\\tau_{\\text{MAE}}\\). In this experiment, we set the value of \\(n_{1}=2000\\). Remarkably, the observed trend aligns closely with the result we derived in Section 6.2. Our derived result, stated as \\(\\tau_{\\text{MSE}}=\\frac{\\ell_{1}(x)}{n_{1}}-\\frac{\\ell_{2}(x)}{n_{2}}\\geq\\frac{ \\sqrt{2\\gamma_{c}}}{n_{1}}-\\frac{\\epsilon_{2}}{n_{2}}\\), affirms that our Copyright Regression approach effectively encourages the model to avoid copyright infringement while still maintaining a controllable level of performance degradation. **Impact of the proportion of copyright data.**\\(n_{1}\\) impacts on model performance is illustrated in the right image of Figure 1. This image showcases the relationship between \\(n_{1}\\) and the difference metrics \\(\\tau_{\\text{MSE}}\\) and \\(\\tau_{\\text{MAE}}\\). Notably, the findings indicate that as the ratio \\(n_{1}/n\\) increases, the disparity in model performance between copyright and non-copyright data diminishes. This observation provides valuable insight, suggesting that the addition of data with a distribution similar to that of copyright-protected data can enhance the model's ability to effectively capture the characteristics of copyright data while ensuring that the model's output remains free from copyright infringement. **Comparison with baseline.** Figure 2 shows comparison between Copyright Regression and baseline **Random** on copyright data with metric MSE. While \\(n_{1}\\) increasing and \\(\\text{MAE}(f(A_{1}),b_{1})\\) decreasing, our method shows its strong protection on copyright data even when \\(n_{1}=8000\\), \\(\\text{MAE}(f(A_{1}),b_{1})\\) still greater than **Random**'s MSE on copyright data. This finding provides compelling evidence that our Copyright Regression approach effectively prevents the occurrence of the \"infinite monkey\" phenomenon, ensuring that the model's outputs consistently avoid copyright infringement. By maintaining a reliable level of performance on copyright data, our method Figure 2: Comparison with **Random** and Copyright Regression on copyright datademonstrates its ability to strike a crucial balance between performance and copyright protection."
    },
    {
      "title": "8 Conclusion",
      "text": "Our work shows that the training of transformers can be viewed as a softmax regression problem. We provide a notion of copyright regression, which encourages regression functions to avoid outputting copyrighted data. Then, we combine the two to perform copyright regression on the softmax function, which allows us to train transformers in a way that avoids outputting copyright data. The main idea to solve copyright regression on the softmax function, was to show that the copyright regression problem is convex and that the Hessian is Lipschitz. This guarantees that gradient descent methods will have guaranteed convergence to the optimal solution with good stability properties. We provide experiments showing that our algorithm performs well in preventing copyright issues on data drawn from a Gaussian distribution, one of the fundamental distributions in machine learning and a test bed for many algorithms, where some data is randomly assigned to be copyrighted."
    },
    {
      "title": "Appendix",
      "text": "**Road map.** In Appendix A, we provide the preliminaries used in our proofs. In Appendix B, we reaffirm our definitions of Copyright Regression. In Appendix C, we provide our computation results for several functions. In Appendix D, we prove that \\(\\nabla^{2}L\\succeq 0\\) and thus \\(L\\) is convex. In Appendix E, we provide our result that Hessian of \\(L\\) is Lipschitz. In Appendix F, we provide a set of tools that can assist in the computation of the Lipschitz property. In Appendix G, we present a collection of bound that are valuable for facilitating computations in our proofs. In Appendix H, we provide guarantee of minimizing our final training objective. In Appendix I, we show our result that Copyright Regression avoids model outputting copyright data. In Appendix J, we provide an approximate version of the newton method for convex optimization."
    },
    {
      "title": "Appendix A Preliminary",
      "text": "In this appendix, we present the preliminaries utilized in our proofs. In Appendix A.1 introduces the notations employed throughout the document. In Appendix A.2 provides fundamental facts regarding exact computation. In Appendix A.3 presents useful tools for determining bounds on norms based on vectors. In Appendix A.4 provides useful tools for determining bounds on norms related to matrices. In Appendix A.5 furnishes basic inequalities for positive semidefinite (psd) matrices. In Appendix A.6 supplies essential lemmas for exact computation."
    },
    {
      "title": "Notations",
      "text": "Now we utilize the following notations and definitions: The \\(\\ell_{p}\\) norm of a vector \\(x\\) is denoted as \\(\\|x\\|_{p}\\), for examples, \\(\\|x\\|_{1}:=\\sum_{i=1}^{n}|x_{i}|\\), \\(\\|x\\|_{2}:=(\\sum_{i=1}^{n}x_{i}^{2})^{1/2}\\) and \\(\\|x\\|_{\\infty}:=\\max_{i\\in[n]}|x_{i}|\\). For a vector \\(x\\in\\mathbb{R}^{n}\\), \\(\\exp(x)\\in\\mathbb{R}^{n}\\) denotes a vector where whose i-th entry is \\(\\exp(x_{i})\\) for all \\(i\\in[n]\\). For \\(n>k\\), for any matrix \\(A\\in\\mathbb{R}^{n\\times k}\\), we denote the spectral norm of \\(A\\) by \\(\\|A\\|\\), i.e., \\(\\|A\\|:=\\sup_{x\\in\\mathbb{R}^{k}}\\|Ax\\|_{2}/\\|x\\|_{2}\\). We denote \\(\\sigma_{\\min}(A)\\) as the minimum singular value of \\(A\\). For two vectors \\(x,y\\in\\mathbb{R}^{n}\\), we denote \\(\\langle x,y\\rangle=\\sum_{i=1}^{n}\\) for \\(i\\in[n]\\). Given two vectors \\(x,y\\in\\mathbb{R}^{n}\\), we denote \\(x\\circ y\\) as a vector whose i-th entry is \\(x_{i}y_{i}\\) for all \\(i\\in[n]\\). We use \\(e_{i}\\in\\mathbb{R}^{n}\\) to denote a vector whose i-th entry is \\(1\\) and all the other entries are \\(0\\). Let \\(x\\in\\mathbb{R}^{n}\\) be a vector. For a vector \\(x\\in\\mathbb{R}^{n}\\), \\(\\operatorname{diag}(x)\\in\\mathbb{R}^{n\\times n}\\) is defined as a diagonal matrix with its diagonal entries given by \\(\\operatorname{diag}(x)_{i,i}=x_{i}\\) for \\(i=1,...,n\\), and all off-diagonal entries are \\(0\\). A symmetric matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) is said to be positive definite (PD) when \\(A\\succ 0\\), for all non-zero vectors \\(x\\in\\mathbb{R}^{n}\\), we have \\(x^{\\top}Ax>0\\). Similarly, a symmetric matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) is said to be positive semidefinite (PSD) when \\(A\\succeq 0\\), for all vectors \\(x\\in\\mathbb{R}^{n}\\), we have \\(x^{\\top}Ax\\geq 0\\)."
    },
    {
      "title": "Basic Algebras",
      "text": "**Fact 17**.: _For vectors \\(u,v,w\\in\\mathbb{R}^{n}\\), we have_ * \\(\\langle u,u\\rangle-\\langle v,v\\rangle=(u-v)^{\\top}(u+v)=(u+v)^{\\top}(u-v)\\)__ * \\(\\langle u,v\\rangle=u^{\\top}v=v^{\\top}u\\)__ * \\(u+v=u-w+w-v\\)__ **Fact 18**.: _For scalars \\(a,b\\in\\mathbb{R}\\), we have_ * \\(|a+b|\\leq|a|+|b|\\)__ * \\(|ab|=|a|\\cdot|b|\\)__"
    },
    {
      "title": "Basic Vector Norm Bounds",
      "text": "**Fact 19**.: _For vectors \\(u,v\\in\\mathbb{R}^{n}\\), we have_ * \\(\\langle u,v\\rangle\\leq\\|u\\|_{2}\\cdot\\|v\\|_{2}\\) _(Cauchy-Schwarz inequality)_ * \\(\\|u\\|_{1}\\geq\\|u\\|_{2}\\geq\\|u\\|_{\\infty}\\)__ * \\(\\|u+v\\|_{2}\\leq\\|u\\|_{2}+\\|v\\|_{2}\\)__ * \\(\\|u\\circ v\\|_{2}\\leq\\|u\\|_{\\infty}\\cdot\\|v\\|_{2}\\leq\\|u\\|_{2}\\cdot\\|v\\|_{2}\\)__ * \\(\\|u-v\\|_{2}\\leq\\|u\\|_{2}+\\|v\\|_{2}\\)__ * \\(\\|\\operatorname{diag}(u)\\|\\leq\\|u\\|_{\\infty}\\leq\\|u\\|_{2}\\)__ * \\(\\|u\\|_{\\infty}\\leq\\|u\\|_{2}\\)__ * \\(\\|u\\|_{2}=\\|u^{\\top}\\|_{2}\\)__ * \\(\\|u^{\\top}-v^{\\top}\\|_{2}=\\|u-v\\|_{2}\\)__ * _Let_ \\(\\alpha\\) _denote a scalar, we have_ \\(\\|\\alpha u\\|_{2}=|\\alpha|\\cdot\\|u\\|_{2}\\)__ * _Let_ \\(u_{1},u_{2},\\ldots,u_{n},v_{1},v_{2},\\ldots,v_{n}\\in\\mathbb{R}^{n}\\) _denote_ \\(n\\) _vectors, we have_ \\[\\|\\sum_{i=1}^{n}u_{i}-\\sum_{i=1}^{n}v_{i}\\|_{2}\\leq\\sum_{i=1}^{n}\\|u_{i}-v_{i} \\|_{2}\\]"
    },
    {
      "title": "Basic Matrix Norm Bounds",
      "text": "**Fact 20**.: _For matrices \\(U,V,W\\in\\mathbb{R}^{n}\\), we have_ * \\(\\|U+V\\|\\leq\\|U\\|+\\|V\\|\\)__ * \\(\\|U+V\\|=\\|U-W+W+V\\|\\)__ * _Let_ \\(\\alpha\\in\\mathbb{R}\\) _denote a scalar, then we have_ \\(\\|\\alpha U\\|\\leq|\\alpha|\\cdot\\|U\\|\\)__ * _Let_ \\(u,v\\in\\mathbb{R}^{n}\\) _denote two vectors, then we have_ \\(\\|uv^{\\top}\\|\\leq\\|u\\|_{2}\\cdot\\|v\\|_{2}\\)__"
    },
    {
      "title": "Basic Positive Semidefinite",
      "text": "**Fact 21**.: _Let \\(u,v\\in\\mathbb{R}^{n}\\), we have_ * \\(uu^{\\top}\\preceq\\|u\\|_{2}^{2}\\cdot I_{n}\\)__ * \\(\\operatorname{diag}(u)\\preceq\\|u\\|_{2}\\cdot I_{n}\\)__ * \\((u\\circ v)\\circ u^{\\top}\\preceq\\|v\\|_{\\infty}\\cdot uu^{\\top}\\)__ * \\(u\\circ(u\\circ v)^{\\top}\\preceq\\|v\\|_{\\infty}\\cdot uu^{\\top}\\)__ * \\(uu^{\\top}\\succeq 0\\)__ * _Let_ \\(U\\in\\mathbb{R}^{n}\\) _denote a matrix, we have_ \\(U\\succeq\\sigma_{\\min}(U)\\)__"
    },
    {
      "title": "Basic Calculus",
      "text": "**Lemma 22**.: _We have_ * \\[\\frac{\\mathrm{d}^{2}f(x)^{-1}}{\\mathrm{d}t\\mathrm{d}t}=2f(x)^{-3}\\cdot(\\frac{ \\mathrm{d}f(x)}{\\mathrm{d}t})^{2}-f(x)^{-2}\\cdot\\frac{\\mathrm{d}^{2}f(x)}{ \\mathrm{d}^{2}t}\\] * \\[\\frac{\\mathrm{d}^{2}f(x)^{-1}}{\\mathrm{d}t_{1}\\mathrm{d}t_{2}}=2f(x)^{-3} \\cdot\\frac{\\mathrm{d}f(x)}{\\mathrm{d}t_{1}}\\cdot\\frac{\\mathrm{d}f(x)}{\\mathrm{ d}t_{2}}-f(x)^{-2}\\cdot\\frac{\\mathrm{d}^{2}f(x)}{\\mathrm{d}t_{1}\\mathrm{d}t_{2}}\\] Proof.: We have \\[\\frac{\\mathrm{d}^{2}f(x)^{-1}}{\\mathrm{d}t\\mathrm{d}t}\\] \\[=\\frac{\\mathrm{d}}{\\mathrm{d}t}(\\frac{\\mathrm{d}f(x)^{-1}}{ \\mathrm{d}t})\\] \\[=\\frac{\\mathrm{d}}{\\mathrm{d}t}(-f(x)^{-2}\\frac{\\mathrm{d}f(x)}{ \\mathrm{d}t})\\] \\[=2f(x)^{-3}\\cdot\\frac{\\mathrm{d}f(x)}{\\mathrm{d}t}\\cdot\\frac{ \\mathrm{d}f(x)}{\\mathrm{d}t}-f(x)^{-2}\\cdot\\frac{\\mathrm{d}^{2}f(x)}{\\mathrm{ d}^{2}t}\\] \\[=2f(x)^{-3}\\cdot(\\frac{\\mathrm{d}f(x)}{\\mathrm{d}t})^{2}-f(x)^{- 2}\\cdot\\frac{\\mathrm{d}^{2}f(x)}{\\mathrm{d}^{2}t}\\] where the first equality follows from the expansion of hessian, the second, third equalities follow from differential chain rule, the fourth equality follows from simply algebra. Similarly, we have \\[\\frac{\\mathrm{d}^{2}f(x)^{-1}}{\\mathrm{d}t_{1}\\mathrm{d}t_{2}}\\] \\[=\\frac{\\mathrm{d}}{\\mathrm{d}t_{1}}(\\frac{\\mathrm{d}f(x)^{-1}}{ \\mathrm{d}t_{2}})\\] \\[=\\frac{\\mathrm{d}}{\\mathrm{d}t_{1}}(-f(x)^{-2}\\frac{\\mathrm{d}f (x)}{\\mathrm{d}t_{2}})\\] \\[=2f(x)^{-3}\\cdot\\frac{\\mathrm{d}f(x)}{\\mathrm{d}t_{1}}\\cdot\\frac {\\mathrm{d}f(x)}{\\mathrm{d}t_{2}}-f(x)^{-2}\\cdot\\frac{\\mathrm{d}^{2}f(x)}{ \\mathrm{d}t_{1}\\mathrm{d}t_{2}}\\] where the first equality follows from the expansion of hessian, the second, third equalities follow from differential chain rule. **Lemma 23**.: _If the given conditions are satisfied_ * _Let_ \\(x,y\\in\\mathbb{R}^{d}\\)__ * _For_ \\(u(x),v(x)\\in\\mathbb{R}^{n}\\)__ _We have_ * _Part 1._ \\[|u(x)^{\\top}v(x)-u(y)^{\\top}v(y)|\\] \\[\\leq\\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}+\\|u(x)-u(y)\\|_{2}\\cdot\\|v( y)\\|_{2}\\]* _Part 2._ \\[\\|u(x)v(x)^{\\top}-u(y)v(y)^{\\top}\\|\\] \\[\\leq \\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}+\\|u(x)-u(y)\\|_{2}\\cdot\\|v(y)\\|_{2}\\] * _Part 3. Let_ \\(\\alpha(x)\\in\\mathbb{R}\\) _denote a scalar, then we have_ \\[\\|\\alpha(x)u(x)v(x)^{\\top}-\\alpha(y)u(y)v(y)^{\\top}\\|\\] \\[\\leq |\\alpha(x)|\\cdot\\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}\\] \\[+|\\alpha(x)|\\cdot\\|u(x)-u(y)\\|_{2}\\cdot\\|v(y)\\|_{2}\\] \\[+\\|u(y)\\|_{2}\\cdot|\\alpha(x)-\\alpha(y)|\\cdot\\|v(y)\\|_{2}\\] * _Part 4. Let_ \\(\\alpha(x),\\beta(x)\\in\\mathbb{R}\\) _denote two scalars, then we have_ \\[|\\alpha(x)\\beta(x)-\\alpha(y)\\beta(y)|\\] \\[\\leq |\\alpha(x)|\\cdot|\\beta(x)-\\beta(y)|+|\\alpha(x)-\\alpha(y)|\\cdot| \\beta(y)|\\] * _Part 5._ \\[\\|u(x)\\circ v(x)-u(y)\\circ v(y)\\|_{2}\\] \\[\\leq \\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}+\\|u(x)-u(y)\\|_{2}\\cdot\\|v(y) \\|_{2}\\] Proof.: **Proof of Part 1.** \\[|u(x)^{\\top}v(x)-u(y)^{\\top}v(y)|\\] \\[= |u(x)^{\\top}v(x)-u(x)^{\\top}v(y)+u(x)^{\\top}v(y)-u(y)^{\\top}v(y)|\\] \\[= |u(x)^{\\top}(v(x)-v(y))+(u(x)^{\\top}-u(y)^{\\top})v(y)|\\] \\[\\leq |u(x)^{\\top}(v(x)-v(y))|+|(u(x)^{\\top}-u(y)^{\\top})v(y)|\\] \\[\\leq \\|u(x)^{\\top}\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}+|(u(x)^{\\top}-u(y)^{ \\top})v(y)|\\] \\[= \\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}+|(u(x)^{\\top}-u(y)^{\\top})v(y)|\\] \\[\\leq \\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}+\\|u(x)^{\\top}-u(y)^{\\top}\\|_{2 }\\cdot\\|v(y)\\|_{2}\\] \\[= \\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}+\\|u(x)-u(y)\\|_{2}\\cdot\\|v(y)\\| _{2}\\] where the first equality follows from Fact 17, the second equality follows from simple algebra, the third equality follows from Fact 18, the fourth, fifth, sixth, seventh equalities follow from Fact 19. **Proof of Part 2.** \\[\\|u(x)v(x)^{\\top}-u(y)v(y)^{\\top}\\|\\] \\[= \\|u(x)v(x)^{\\top}-u(x)v(y)^{\\top}+u(x)v(y)^{\\top}-u(y)v(y)^{\\top}\\|\\] \\[\\leq \\|u(x)v(x)^{\\top}-u(x)v(y)^{\\top}\\|+\\|u(x)v(y)^{\\top}-u(y)v(y)^{ \\top}\\|\\] \\[= \\|u(x)(v(x)^{\\top}-v(y)^{\\top})\\|+\\|(u(x)-u(y))v(y)^{\\top}\\|\\] \\[\\leq \\|u(x)\\|_{2}\\cdot\\|v(x)^{\\top}-v(y)^{\\top}\\|_{2}+\\|(u(x)-u(y))v(y )^{\\top}\\|\\] \\[\\leq \\|u(x)\\|_{2}\\cdot\\|v(x)^{\\top}-v(y)^{\\top}\\|_{2}+\\|u(x)-u(y)\\|_{2 }\\cdot\\|v(y)^{\\top}\\|_{2}\\] \\[= \\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}+\\|u(x)-u(y)\\|_{2}\\cdot\\|v(y)\\| _{2}\\]where the first, second equalities follow from Fact 20, the third equality follows from simple algebra, the fourth, fifth equalities follow from Fact 20, the sixth equality follows from Fact 19. **Proof of Part 3.** \\[\\|\\alpha(x)u(x)v(x)^{\\top}-\\alpha(y)u(y)v(y)^{\\top}\\|\\] \\[\\leq \\|\\alpha(x)u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}\\] \\[+\\|\\alpha(x)u(x)-\\alpha(y)u(y)\\|_{2}\\cdot\\|v(y)\\|_{2}\\] where the first equality follows from Part 2 of Lemma 23. For the first term, we have \\[\\|\\alpha(x)u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}\\] \\[= |\\alpha(x)|\\cdot\\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}\\] where the first equality follows from Fact 19. For the second term, we have \\[\\|\\alpha(x)u(x)-\\alpha(y)u(y)\\|_{2}\\cdot\\|v(y)\\|_{2}\\] \\[= \\|\\alpha(x)u(x)-\\alpha(x)u(y)+\\alpha(x)u(y)-\\alpha(y)\\alpha(y)\\| _{2}\\cdot\\|v(y)\\|_{2}\\] \\[= \\|\\alpha(x)(u(x)-u(y))+(\\alpha(x)-\\alpha(y))u(y)\\|_{2}\\cdot\\|v( y)\\|_{2}\\] \\[\\leq (\\|\\alpha(x)(u(x)-u(y))\\|_{2}+\\|(\\alpha(x)-\\alpha(y))u(y)\\|_{2}) \\cdot\\|v(y)\\|_{2}\\] \\[= (|\\alpha(x)|\\cdot\\|u(x)-u(y)\\|_{2}+\\|u(y)\\|_{2}\\cdot|\\alpha(x)- \\alpha(y)|)\\cdot\\|v(y)\\|_{2}\\] \\[= |\\alpha(x)|\\cdot\\|u(x)-u(y)\\|_{2}\\cdot\\|v(y)\\|_{2}\\] \\[+\\|u(y)\\|_{2}\\cdot|\\alpha(x)-\\alpha(y)|\\cdot\\|v(y)\\|_{2}\\] where the first equality follows from Fact 19, the second equality follows from simple algebra, the third, fourth, fifth equalities follow from Fact 19, the sixth equality follows from simple algebra. Then we combine two terms, we have \\[\\|\\alpha(x)u(x)v(x)^{\\top}-\\alpha(y)u(y)v(y)^{\\top}\\|\\] \\[\\leq |\\alpha(x)|\\cdot\\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}\\] \\[+|\\alpha(x)|\\cdot\\|u(x)-u(y)\\|_{2}\\cdot\\|v(y)\\|_{2}\\] \\[+\\|u(y)\\|_{2}\\cdot|\\alpha(x)-\\alpha(y)|\\cdot\\|v(y)\\|_{2}\\] **Proof of Part 4.** \\[|\\alpha(x)\\beta(x)-\\alpha(y)\\beta(y)|\\] \\[= |\\alpha(x)\\beta(x)+\\alpha(x)\\beta(y)-\\alpha(x)\\beta(y)-\\alpha(y) \\beta(y)|\\] \\[= |\\alpha(x)(\\beta(x)-\\beta(y))+(\\alpha(x)-\\alpha(y))\\beta(y)|\\] \\[\\leq |\\alpha(x)(\\beta(x)-\\beta(y))|+|(\\alpha(x)-\\alpha(y))\\beta(y)|\\] \\[= |\\alpha(x)|\\cdot|\\beta(x)-\\beta(y)|+|(\\alpha(x)-\\alpha(y))\\beta(y)|\\] \\[= |\\alpha(x)|\\cdot|\\beta(x)-\\beta(y)|+|\\alpha(x)-\\alpha(y)|\\cdot| \\beta(y)|\\] where the first equality follows from Fact 17, the second equality follows from simple algebra, the third, fourth, fifth equalities follow from Fact 18. **Proof of Part 5.** \\[\\|u(x)\\circ v(x)-u(y)\\circ v(y)\\|_{2}\\] \\[= \\|u(x)\\circ v(x)-u(x)\\circ v(y)+u(x)\\circ v(y)-u(y)\\circ v(y)\\|_{2}\\] \\[= \\|u(x)\\circ(v(x)-v(y))+(u(x)-u(y))\\circ v(y)\\|_{2}\\] \\[\\leq \\|u(x)\\circ(v(x)-v(y))\\|_{2}+\\|(u(x)-u(y))\\circ v(y)\\|_{2}\\] \\[\\leq \\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}+\\|(u(x)-u(y))\\circ v(y)\\|_{2}\\] \\[\\leq \\|u(x)\\|_{2}\\cdot\\|v(x)-v(y)\\|_{2}+\\|u(x)-u(y)\\|_{2}\\cdot\\|v(y)\\|_ {2}\\] where the first equality follows from Fact 17, the second equality follows from simple algebra, the third, fourth, fifth equality follow from Fact 19."
    },
    {
      "title": "Appendix B Copyright Regression",
      "text": "In this appendix, we reaffirm our definitions of Copyright Regression. In Appendix B.1, we provide our formal definitions of Softmax Regression and Copyright Regression. In Appendix B.2, we provide our formal definitions of regularization term \\(L_{\\text{reg}}\\) and train objective \\(L\\)."
    },
    {
      "title": "Definitions",
      "text": "**Definition 24** (Softmax Regression in [14]).: _Given a matrix \\(A\\in\\mathbb{R}^{n\\times d}\\), we define_ \\[f(x):=\\langle\\exp(Ax),\\mathbf{1}_{n}\\rangle^{-1}\\exp(Ax)\\] For the convenience of calculation, we define a intermediate operator \\(c(x)\\) as follows **Definition 25**.: _Given a matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and vector \\(b\\in\\mathbb{R}^{n}\\), let \\(f(x)\\) be denoted as Definition 2, we define_ \\[c(x):=f(x)-b\\] We define train objective of Softmax Regression as follows **Definition 26** (Train Objective of Softmax Regression in [14]).: _Given a matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and vector \\(b\\in\\mathbb{R}^{n}\\), let \\(c(x)\\) be denoted as Definition 25, we define_ \\[\\ell(x)=\\langle c(x),c(x)\\rangle\\] **Definition 27** (Softmax Regression on Copyright Data).: _Given all data matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and copyrighted data matrix \\(A_{1}\\in\\mathbb{R}^{n_{1}\\times d}\\), we define_ \\[f_{1}(x):=\\langle\\exp(A_{i,*}x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(Ax)\\] _where \\(i\\in[1,n_{1}]\\) denote a integer._ Also, we provide the definition of intermediate operator \\(c(x)\\) as follows **Definition 28**.: _Given all data matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and copyrighted data matrix \\(A_{1}\\in\\mathbb{R}^{n_{1}\\times d}\\) and vector \\(b_{1}\\in\\mathbb{R}^{n}\\), let \\(f_{1}(x)\\) be denoted as Definition 27, we define_ \\[c_{1}(x):=f_{1}(x)-b_{1}\\]Now we have officially provided our definition of Copyright Regression below, which can prevent language models from infringing copyright with controllable performance damage and without occupying more resources. **Definition 29**.: _We define \\(\\ell(x)\\) as Definition 26. Denote \\(c_{1}(x)\\) as Definition 28, and we define \\(\\ell_{1}(x)=\\langle c_{1}(x),c_{1}(x)\\rangle\\), define \\(\\ell_{2}(x):=\\ell(x)-\\ell_{1}(x)\\). Let \\(\\gamma_{c}>0\\) denote a parameter that control loss related to copyright data._ _We consider the following copyright loss_ \\[L_{\\mathrm{copyright}}(x):=0.5\\ell_{1}(x)+\\gamma_{c}\\cdot\\ell_{1}(x)^{-1}+0. 5\\ell_{2}(x)\\]"
    },
    {
      "title": "Regularization",
      "text": "**Definition 30**.: _Given a matrix \\(A\\in\\mathbb{R}^{n\\times d}\\). Given a vector \\(w\\in\\mathbb{R}^{n}\\), we denote \\(W=\\mathrm{diag}(w)\\). We define \\(L_{\\mathrm{reg}}:\\mathbb{R}^{d}\\to\\mathbb{R}\\) as follows_ \\[L_{\\mathrm{reg}}:=0.5\\|WAx\\|_{2}^{2}\\] After adding regularization term, we define our final train objective \\(L\\) as follows **Definition 31**.: _Let \\(L_{\\mathrm{copyright}}(x)\\) be denoted as Definition 29, let \\(L_{\\mathrm{reg}}\\) be denoted as Definition 30, then we define_ \\[L:=L_{\\mathrm{copyright}}(x)+L_{\\mathrm{reg}}\\]"
    },
    {
      "title": "Appendix C Gradients And Hessians",
      "text": "In this appendix, we provide our computation results of several functions. In Appendix C.1, we provide our result and proof of gradient of \\(\\ell(x)\\). In Appendix C.2, we provide our result and proof of gradient of \\(\\ell(x)^{-1}\\). In Appendix C.3, we provide our result and proof of Hessian of \\(\\ell(x)\\). In Appendix C.4, we provide our result and proof of Hessian of \\(\\ell(x)^{-1}\\). In Appendix C.5, we provide our result and proof of gradient and Hessian of \\(L_{\\mathrm{reg}}\\). In Appendix C.6, we provide our result and proof of gradient and Hessian of \\(L\\)."
    },
    {
      "title": "Gradient Of \\(\\Ell(X)\\)",
      "text": "**Lemma 32**.: _If the given conditions are satisfied_ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(\\ell(x)\\) _as Definition_ 26__ _then we have_ * _(see Part 7 of Lemma_ 5.6 _in_ _[_12_]__)_ \\[\\frac{\\mathrm{d}0.5\\ell(x)}{\\mathrm{d}x_{i}}=A_{*,i}^{\\top}(-f(x)c(x)^{\\top}f(x )+\\mathrm{diag}(f(x))c(x))\\] **Remark 33**.: _In [12], they write a typo in the equation, they forgot to add a negative sign. They write_ \\[\\frac{\\mathrm{d}0.5\\ell(x)}{\\mathrm{d}x_{i}}=A_{*,i}^{\\top}(f(x)c(x)^{\\top}f(x )+\\mathrm{diag}(f(x))c(x))\\]"
    },
    {
      "title": "Gradient Of \\(\\Ell(X)^{-1}\\)",
      "text": "**Lemma 34**.: _If the given conditions are satisfied_ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(\\ell(x)\\) _as Definition_ 26__ _then we have_ * \\[\\frac{\\mathrm{d}0.5\\ell(x)^{-1}}{\\mathrm{d}x_{i}}=\\ell(x)^{-2} \\cdot A_{*,i}^{\\top}(f(x)c(x)^{\\top}f(x)-\\mathrm{diag}(f(x))c(x))\\] Proof.: We have \\[\\frac{\\mathrm{d}0.5\\ell(x)^{-1}}{\\mathrm{d}x_{i}}= \\ -\\cdot\\ell(x)^{-2}\\cdot\\frac{\\mathrm{d}0.5\\ell(x)}{\\mathrm{d}x_{i}}\\] \\[= \\ -\\ell(x)^{-2}\\cdot A_{*,i}^{\\top}(-f(x)c(x)^{\\top}f(x)+ \\mathrm{diag}(f(x))c(x))\\] \\[= \\ \\ell(x)^{-2}\\cdot A_{*,i}^{\\top}(f(x)c(x)^{\\top}f(x)-\\mathrm{ diag}(f(x))c(x))\\] where the first equality follows from differential chain rule, the second equality follow from Lemma 32, the third equality follows from simple algebra."
    },
    {
      "title": "Hessian Of \\(\\Ell(X)\\)",
      "text": "**Lemma 35** (Hessian of \\(0.5\\ell(x)\\), Lemma 5.13 in [10]).: _We define_ * \\(B_{1}(x)\\in\\mathbb{R}^{n_{1}\\times n_{1}}\\) _such that_ \\[A_{*,i}^{\\top}B_{1}(x)A_{*,j}^{\\top}:=(-\\langle f(x),A_{*,j}\\rangle f(x)+f(x) \\circ A_{*,j})^{\\top}\\cdot(-\\langle f(x),A_{*,i}\\rangle f(x)+f(x)\\circ A_{*,i})\\] * \\(B_{2}(x)\\in\\mathbb{R}^{n_{1}\\times n_{1}}\\) _such that_ \\[A_{*,i}^{\\top}B_{2}(x)A_{*,j}^{\\top}:= \\ c(x)^{\\top}\\cdot(2\\langle f(x),A_{*,i}\\rangle\\langle f(x),A_{*, j}\\rangle f(x)-\\langle f(x),A_{*,i}\\circ A_{*,j}\\rangle f(x)\\] \\[-\\langle f(x),A_{*,i}\\rangle f(x)\\circ A_{*,j}-\\langle f(x),A_{*,j}\\rangle f(x)\\circ A_{*,i}+A_{*,i}\\circ f(x)\\circ A_{*,j})\\] _Then we have_ * _Part 1._ \\[\\frac{\\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{d}x_{i}\\mathrm{d}x_{i}}=A_{*,i}^{\\top }B_{1}(x)A_{*,i}^{\\top}+A_{*,i}^{\\top}B_{2}(x)A_{*,i}^{\\top}\\] * _Part 2._ \\[\\frac{\\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}}=A_{*,i}^{\\top }B_{1}(x)A_{*,j}^{\\top}+A_{*,i}^{\\top}B_{2}(x)A_{*,j}^{\\top}\\] **Lemma 36** (Rewriting \\(B_{1}(x)\\) and \\(B_{2}(x)\\), see Part 3 of Lemma 5.15 in [10]).: _If the given conditions are satisfied_ * _Given a matrix_ \\(A\\in\\mathbb{R}^{n\\times d}\\)_._ * _Denote_ \\(f(x)\\) _as Definition_ 24_._ * _Let_ \\(B(x)=B_{1}(x)+B_{2}(x)\\)__ _then, for \\(B(x)\\in\\mathbb{R}^{n\\times n}\\), we have_ \\[B(x)= \\;\\langle 3f(x)-2b,f(x)\\rangle\\cdot f(x)f(x)^{\\top}\\] \\[+\\langle f(x)-b,f(x)\\rangle\\cdot\\operatorname{diag}(f(x))\\] \\[+\\operatorname{diag}((2f(x)-b)\\circ f(x))\\] \\[+(b\\circ f(x))\\cdot f(x)^{\\top}+f(x)\\cdot(b\\circ f(x))^{\\top}\\] _so we can rewrite hessian of \\(\\ell(x)\\) as follows_ * _Part 1._ \\[\\frac{\\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{d}x_{i}\\mathrm{d}x_{i}}=A_{*,i}^{\\top} B(x)A_{*,i}^{\\top}\\] * _Part 2._ \\[\\frac{\\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}}=A_{*,i}^{\\top} B(x)A_{*,j}^{\\top}\\] For convenient, we define \\(B(x)\\) **Definition 37**.: _If the given conditions are satisfied_ * _Given vectors_ \\(b\\in\\mathbb{R}^{n}\\) _and_ \\(b_{1}\\in\\mathbb{R}^{n}_{1}\\)__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(f_{1}(x)\\) _as Definition_ 27__ * _Denote_ \\(c(x)\\) _as Definition_ 25__ _We define \\(B(x)\\) and \\(B_{c}(x)\\) as follow_ * _Part 1._ \\[B(x)= \\;\\langle 3f(x)-2b,f(x)\\rangle\\cdot f(x)f(x)^{\\top}\\] \\[+\\langle f(x)-b,f(x)\\rangle\\cdot\\operatorname{diag}(f(x))\\] \\[+\\operatorname{diag}((2f(x)-b)\\circ f(x))\\] \\[+(b\\circ f(x))\\cdot f(x)^{\\top}+f(x)\\cdot(b\\circ f(x))^{\\top}\\] * _Part 2._ \\[B_{c}(x)= \\;\\langle 3f_{1}(x)-2b_{1},f_{1}(x)\\rangle\\cdot f_{1}(x)f_{1}(x)^{\\top}\\] \\[+\\langle f_{1}(x)-b_{1},f_{1}(x)\\rangle\\cdot\\operatorname{diag} (f_{1}(x))\\] \\[+\\operatorname{diag}((2f_{1}(x)-b_{1})\\circ f_{1}(x))\\] \\[+(b_{1}\\circ f_{1}(x))\\cdot f_{1}(x)^{\\top}+f_{1}(x)\\cdot(b_{1} \\circ f_{1}(x))^{\\top}\\]"
    },
    {
      "title": "Hessian Of \\(\\Ell(X)^{-1}\\)",
      "text": "**Lemma 38**.: _If the given condition is satisfied_ * _Let_ \\(\\ell(x)\\) _be denoted as Definition_ 26_._ _then we have_ * _Part 1._ \\[\\frac{\\mathrm{d}^{2}0.5\\ell(x)^{-1}}{\\mathrm{d}x_{i}\\mathrm{d}x_{i}} =\\ell(x)^{-2}(16\\cdot\\ell(x)^{-1}\\cdot(A_{*,i}^{\\top}(-f(x)c(x)^{ \\top}f(x)+\\mathrm{diag}(f(x))c(x)))^{2}\\] \\[\\quad-A_{*,i}^{\\top}B(x)A_{*,i}^{\\top})\\] * _Part 2._ \\[\\frac{\\mathrm{d}^{2}0.5\\ell(x)^{-1}}{\\mathrm{d}x_{i}\\mathrm{d}x_{ j}} =\\ell(x)^{-2}(16\\cdot\\ell(x)^{-1}\\cdot A_{*,i}^{\\top}(-f(x)c(x)^{ \\top}f(x)+\\mathrm{diag}(f(x))c(x))\\] \\[\\quad\\cdot A_{*,j}^{\\top}(-f(x)c(x)^{\\top}f(x)+\\mathrm{diag}(f(x) )c(x))-A_{*,i}^{\\top}B(x)A_{*,j}^{\\top})\\] Proof.: **Proof of Part 1.** \\[\\frac{\\mathrm{d}^{2}0.5\\ell(x)^{-1}}{\\mathrm{d}x_{i}\\mathrm{d}x_{ i}} =16\\cdot\\ell(x)^{-3}\\cdot(\\frac{\\mathrm{d}0.5\\ell(x)}{\\mathrm{d}x _{i}})^{2}-\\ell(x)^{-2}\\cdot\\frac{\\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{d}x_{i} \\mathrm{d}x_{i}}\\] \\[=\\ell(x)^{-2}(16\\cdot\\ell(x)^{-1}\\cdot(\\frac{\\mathrm{d}0.5\\ell( x)}{\\mathrm{d}x_{i}})^{2}-\\frac{\\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{d}x_{i} \\mathrm{d}x_{i}})\\] \\[=\\ell(x)^{-2}(16\\cdot\\ell(x)^{-1}\\cdot(A_{*,i}^{\\top}(-f(x)c(x)^{ \\top}f(x)+\\mathrm{diag}(f(x))c(x)))^{2}-\\frac{\\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{ d}x_{i}\\mathrm{d}x_{i}})\\] \\[=\\ell(x)^{-2}(16\\cdot\\ell(x)^{-1}\\cdot(A_{*,i}^{\\top}(-f(x)c(x)^{ \\top}f(x)+\\mathrm{diag}(f(x))c(x)))^{2}-A_{*,i}^{\\top}B(x)A_{*,i}^{\\top})\\] where the first equality follows from Lemma 22, the second equality follows from simple algebra, the third equality follows from Lemma 32, the fourth equality follows from Lemma 35 and Lemma 36. **Proof of Part 2.** \\[\\frac{\\mathrm{d}^{2}0.5\\ell(x)^{-1}}{\\mathrm{d}x_{i}\\mathrm{d}x_ {j}} =16\\cdot\\ell(x)^{-3}\\cdot\\frac{\\mathrm{d}0.5\\ell(x)}{\\mathrm{d}x_ {j}}\\cdot\\frac{\\mathrm{d}0.5\\ell(x)}{\\mathrm{d}x_{i}}-\\ell(x)^{-2}\\cdot\\frac{ \\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}}\\] \\[=\\ell(x)^{-2}(16\\cdot\\ell(x)^{-1}\\cdot\\frac{\\mathrm{d}0.5\\ell( x)}{\\mathrm{d}x_{i}}\\cdot\\frac{\\mathrm{d}0.5\\ell(x)}{\\mathrm{d}x_{j}}-\\frac{ \\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}})\\] \\[=\\ell(x)^{-2}(16\\cdot\\ell(x)^{-1}\\cdot A_{*,i}^{\\top}(-f(x)c(x)^{ \\top}f(x)+\\mathrm{diag}(f(x))c(x))\\] \\[\\quad\\cdot A_{*,j}^{\\top}(-f(x)c(x)^{\\top}f(x)+\\mathrm{diag}(f(x) )c(x))-\\frac{\\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}})\\] \\[=\\ell(x)^{-2}(16\\cdot\\ell(x)^{-1}\\cdot A_{*,i}^{\\top}(-f(x)c(x)^{ \\top}f(x)+\\mathrm{diag}(f(x))c(x))\\] \\[\\quad\\cdot A_{*,j}^{\\top}(-f(x)c(x)^{\\top}f(x)+\\mathrm{diag}(f(x) )c(x))-A_{*,i}^{\\top}B(x)A_{*,j}^{\\top})\\] where the first equality follows from Lemma 22, the second equality follows from simple algebra, the third equality follows from Lemma 32, the fourth equality follows from Lemma 35 and Lemma 36."
    },
    {
      "title": "Gradient And Hessian Of \\(L_{\\Rm Reg}\\)",
      "text": "**Lemma 39**.: _[Folklore, see [13] as an example] For a given vector \\(w\\in\\mathbb{R}^{n}\\), let \\(W=\\operatorname{diag}(w)\\). Let \\(L_{\\rm reg}:\\mathbb{R}^{d}\\to\\mathbb{R}\\) be denoted as Definition 30._ _Then, we have_ * _The gradient of_ \\(L\\) _is_ \\[\\frac{\\mathrm{d}L_{\\rm reg}}{\\mathrm{d}x}=A^{\\top}W^{2}Ax\\] * _The Hessian of_ \\(L\\) _is_ \\[\\frac{\\mathrm{d}^{2}L_{\\rm reg}}{\\mathrm{d}x^{2}}=A^{\\top}W^{2}A\\]"
    },
    {
      "title": "Gradient And Hessian Of \\(L\\)",
      "text": "**Lemma 40** (Formal vision of Lemma 10).: _If the given conditions are satisfied_ * _Given two matrices_ \\(A_{1},A_{2}\\in\\mathbb{R}^{n\\times d}\\)_, where_ \\(A_{1}\\) _is the part of data has copyright_ * _Let_ \\(\\ell_{1}(x)\\) _and_ \\(\\ell_{2}(x)\\) _be denoted as Definition_ 29_._ * _Let_ \\(L\\) _be denoted as Definition_ 31__ * _Let_ \\(B(x)\\) _be denoted ad Definition_ 37__ _we have_ \\[\\frac{\\mathrm{d}L}{\\mathrm{d}x} = A^{\\top}_{*,i}(-f(x)c(x)^{\\top}f(x)+\\operatorname{diag}(f(x))c(x))\\] \\[+2\\gamma_{c}\\ell_{1}(x)^{-2}\\cdot A^{\\top}_{1*,i}(f_{1}(x)c_{1}(x )^{\\top}f_{1}(x)-\\operatorname{diag}(f_{1}(x))c_{1}(x))+A^{\\top}W^{2}Ax\\] Proof.: We have \\[\\frac{\\mathrm{d}L}{\\mathrm{d}x} = \\frac{\\mathrm{d}L}{\\mathrm{d}x}\\] \\[= \\frac{\\mathrm{d}(L_{\\rm copyright}+L_{\\rm reg})}{\\mathrm{d}x}\\] \\[= \\frac{\\mathrm{d}(0.5\\ell_{1}(x)+\\gamma_{c}\\ell_{1}(x)^{-1}+0.5 \\ell_{2}(x)+L_{\\rm reg})}{\\mathrm{d}x}\\] \\[= \\frac{\\mathrm{d}(0.5\\ell(x)+\\gamma_{c}\\ell_{1}(x)^{-1}+L_{\\rm reg })}{\\mathrm{d}x}\\] \\[= \\frac{\\mathrm{d}\\gamma_{c}\\ell_{1}(x)^{-1}}{\\mathrm{d}x}+\\frac{ \\mathrm{d}L_{reg}}{\\mathrm{d}x}+\\frac{\\mathrm{d}0.5\\ell(x)}{\\mathrm{d}x}\\] \\[= \\gamma_{c}\\frac{\\mathrm{d}\\ell_{1}(x)^{-1}}{\\mathrm{d}x}+\\frac{ \\mathrm{d}L_{reg}}{\\mathrm{d}x}+\\frac{\\mathrm{d}0.5\\ell(x)}{\\mathrm{d}x}\\] \\[= 2\\gamma_{c}\\ell_{1}(x)^{-2}\\cdot A^{\\top}_{1*,i}(f_{1}(x)c_{1}(x )^{\\top}f_{1}(x)-\\operatorname{diag}(f_{1}(x))c_{1}(x))+\\frac{\\mathrm{d}L_{reg }}{\\mathrm{d}x}+\\frac{\\mathrm{d}0.5\\ell(x)}{\\mathrm{d}x}\\] \\[= 2\\gamma_{c}\\ell_{1}(x)^{-2}\\cdot A^{\\top}_{1*,i}(f_{1}(x)c_{1}(x )^{\\top}f_{1}(x)-\\operatorname{diag}(f_{1}(x))c_{1}(x))+A^{\\top}W^{2}Ax+\\frac{ \\mathrm{d}0.5\\ell(x)}{\\mathrm{d}x}\\] \\[= A^{\\top}_{*,i}(-f(x)c(x)^{\\top}f(x)+\\operatorname{diag}(f(x))c(x))\\] [MISSING_PAGE_FAIL:27] where the first equality follows from Definition 31, the second equality follows from Definition 29, the third equality follows from \\(\\ell(x)=\\ell_{1}(x)+\\ell_{2}(x)\\), the fourth, fifth equalities follow from simple differential rules, the sixth equality follows from Part 1 of Lemma 36, the seventh equality follows from Part 2 of Lemma 39, the eighth equality follows from Part 1 of Lemma 38. **Proof of Part 2.** \\[\\frac{\\mathrm{d}^{2}L}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}} =\\frac{\\mathrm{d}^{2}(L_{\\mathrm{copyright}}+L_{\\mathrm{reg}})}{ \\mathrm{d}x_{i}\\mathrm{d}x_{j}}\\] \\[=\\frac{\\mathrm{d}^{2}(0.5\\ell_{1}(x)+\\gamma_{c}\\ell_{1}(x)^{-1}+ 0.5\\ell_{2}(x)+L_{\\mathrm{reg}})}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}}\\] \\[=\\frac{\\mathrm{d}^{2}(0.5\\ell(x)+\\gamma_{c}\\ell_{1}(x)^{-1}+L_{ \\mathrm{reg}})}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}}\\] \\[=\\frac{\\mathrm{d}^{2}\\gamma_{c}\\ell_{1}(x)^{-1}}{\\mathrm{d}x_{i} \\mathrm{d}x_{i}}+\\frac{\\mathrm{d}^{2}L_{reg}}{\\mathrm{d}x_{i}\\mathrm{d}x_{i}} +\\frac{\\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}}\\] \\[=\\gamma_{c}\\frac{\\mathrm{d}^{2}\\ell_{1}(x)^{-1}}{\\mathrm{d}x_{i} \\mathrm{d}x_{j}}+\\frac{\\mathrm{d}^{2}L_{reg}}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}} +\\frac{\\mathrm{d}^{2}0.5\\ell(x)}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}}\\] \\[=\\gamma_{c}\\frac{\\mathrm{d}^{2}\\ell_{1}(x)^{-1}}{\\mathrm{d}x_{i} \\mathrm{d}x_{j}}+\\frac{\\mathrm{d}^{2}L_{reg}}{\\mathrm{d}x_{i}\\mathrm{d}x_{j}} +A_{*,i}^{\\top}B(x)A_{*,i}^{\\top}\\] \\[=A_{*,i}^{\\top}B(x)A_{*,j}^{\\top}+A^{\\top}W^{2}A+2\\gamma_{c}\\ell_{ 1}(x)^{-2}(16\\cdot\\ell_{1}(x)^{-1}\\cdot A_{1*,i}^{\\top}(-f_{1}(x)c_{1}(x)^{ \\top}f_{1}(x)\\] \\[\\quad+\\mathrm{diag}(f_{1}(x))c_{1}(x))\\cdot A_{1*,j}^{\\top}(-f_{1 }(x)c_{1}(x)^{\\top}f_{1}(x)+\\mathrm{diag}(f_{1}(x))c_{1}(x))-A_{1*,i}^{\\top}B _{c}(x)A_{1*,j}^{\\top})\\] where the first equality follows from Definition 31, the second equality follows from Definition 29, the third equality follows from \\(\\ell(x)=\\ell_{1}(x)+\\ell_{2}(x)\\), the fourth, fifth equalities follow from simple differential rules, the sixth equality follows from Part 2 of Lemma 36, the seventh equality follows from Part 2 of Lemma 39, the eighth equality follows from Part 2 of Lemma 38. **Lemma 42**.: _If the given conditions are satisfied_ * _Given two matrices_ \\(A_{1},A_{2}\\in\\mathbb{R}^{n\\times d}\\)_, where_ \\(A_{1}\\) _is the part of data has copyright_ * _Let_ \\(\\ell_{1}(x)\\) _and_ \\(\\ell_{2}(x)\\) _be denoted as Definition_ 29_._ * _Let_ \\(L\\) _be denoted as Definition_ 31__ * _Let_ \\(B(x)\\) _be denoted ad Definition_ 37__ * _Denote_ \\(H(x):=\\frac{\\mathrm{d}^{2}L}{\\mathrm{d}x^{2}}\\)__ * _Denote_ \\(H_{1}(x):=\\frac{\\mathrm{d}^{2}\\gamma_{c}\\cdot\\ell_{1}(x)^{-1}}{\\mathrm{d}x^{2}}\\)__ * _Denote_ \\(H_{2}(x):=\\frac{\\mathrm{d}^{2}(0.5\\ell_{1}(x)+L_{reg}(x))}{\\mathrm{d}x^{2}}\\)__ * _Denote_ \\(H_{3}(x):=\\frac{\\mathrm{d}^{2}0.5\\ell_{2}(x)}{\\mathrm{d}x^{2}}\\)__ \\[H(x)=H_{1}(x)+H_{2}(x)+H_{3}(x)\\]Proof.: We have \\[H(x) =\\frac{\\mathrm{d}^{2}L}{\\mathrm{d}x^{2}}\\] \\[=\\frac{\\mathrm{d}^{2}(L_{copyright}(x)+L_{reg})}{\\mathrm{d}x^{2}}\\] \\[=\\frac{\\mathrm{d}^{2}(0.5\\ell_{1}(x)+\\gamma_{c}\\cdot\\ell_{1}(x)^{ -1}+0.5\\ell_{2}(x)+L_{reg})}{\\mathrm{d}x^{2}}\\] \\[=\\frac{\\mathrm{d}^{2}\\gamma_{c}\\cdot\\ell_{1}(x)^{-1}}{\\mathrm{d}x ^{2}}+\\frac{\\mathrm{d}^{2}(0.5\\ell_{1}(x)+L_{reg})}{\\mathrm{d}x^{2}}+\\frac{ \\mathrm{d}^{2}0.5\\ell_{2}(x)}{\\mathrm{d}x^{2}}\\] \\[=H_{1}(x)+\\frac{\\mathrm{d}^{2}(0.5\\ell_{1}(x)+0.5\\ell_{2}(x)+L_{ reg})}{\\mathrm{d}x^{2}}+\\frac{\\mathrm{d}^{2}0.5\\ell_{2}(x)}{\\mathrm{d}x^{2}}\\] \\[=H_{1}(x)+H_{2}(x)+\\frac{\\mathrm{d}^{2}0.5\\ell_{2}(x)}{\\mathrm{d} x^{2}}\\] \\[=H_{1}(x)+H_{2}(x)+H_{3}(x)\\] where the first equality follows from the Definition of \\(H(x)\\), the second equality follows from Definition 31, the third equality follows from simple differential rule, the fourth equality follows from the Definition of \\(H_{1}(x)\\), the fifth equality follows from the Definition of \\(H_{2}(x)\\), the sixth equality follows from the Definition of \\(H_{3}(x)\\)."
    },
    {
      "title": "Appendix D Lower Bound On Hessian",
      "text": "In this appendix, we prove that \\(\\nabla^{2}L\\succeq 0\\) and thus \\(L\\) is convex. In Appendix D.1, we provide our main result that \\(\\nabla^{2}L\\succeq 0\\). In Appendix D.2, we divide \\(H_{1}(x)\\) to \\(5\\) part to help us to compute lower bound on \\(H_{1}(x)\\). In Appendix D.3, we provide our result and proof of lower bound on \\(P(x)\\). In Appendix D.4, we provide some helpful lemma to simplify our proofs. In Appendix D.5, we provide our result and proof of lower bound on \\(P(x)_{1}\\). In Appendix D.6, we provide our result and proof of lower bound on \\(P(x)_{2.5}\\). In Appendix D.7, we provide our result and proof of lower bound on \\(P(x)_{4}\\). In Appendix D.8, we provide our result and proof of lower bound on \\(P(x)_{5}\\)."
    },
    {
      "title": "Main Result",
      "text": "**Lemma 43** (Formal version of Lemma 13).: _If the given conditions are satisfied_ * _Given two matrices_ \\(A_{1},A_{2}\\in\\mathbb{R}^{n\\times d}\\)_, where_ \\(A_{1}\\) _is the part of data has copyright_ * _Let_ \\(\\ell_{1}(x)\\) _and_ \\(\\ell_{2}(x)\\) _be denoted as Definition_ 29__ * _Let_ \\(L\\) _be denoted as Definition_ 31__ * _Denote_ \\(P(x)\\) _as Definition_ 44__ * _Denote_ \\(B(x)\\) _as Definition_ 37__ * \\(\\gamma\\in(0,1)\\)__ * _Given a vector_ \\(w\\)_, let_ \\(W=\\mathrm{diag}(w)\\in\\mathbb{R}^{n\\times n}\\)_. Let_ \\(W^{2}\\in\\mathbb{R}^{n\\times n}\\) _denote the matrix that i-th diagonal entry is_ \\(w_{i,i}^{2}\\)__ * _Denote_ \\(H(x):=\\frac{\\mathrm{d}^{2}L}{\\mathrm{d}x^{2}}\\)__* _Denote_ \\(H_{1}(x):=\\frac{\\mathrm{d}^{2}\\gamma_{c}\\cdot\\ell_{1}(x)^{-1}}{\\mathrm{d}x^{2}}\\)__ * _Denote_ \\(H_{2}(x):=\\frac{\\mathrm{d}^{2}(0.5\\ell_{1}(x)+L_{reg}(x))}{\\mathrm{d}x^{2}}\\)__ * _Denote_ \\(H_{3}(x):=\\frac{\\mathrm{d}^{2}0.5\\ell_{2}(x)}{\\mathrm{d}x^{2}}\\)__ * _Denote_ \\(B_{c}(x)\\) _that_ \\(\\frac{\\mathrm{d}^{2}0.5\\ell_{1}(x)}{\\mathrm{d}x^{2}}=A^{\\top}B_{c}(x)A\\)__ * _Denote_ \\(B_{nc}(x)\\) _that_ \\(\\frac{\\mathrm{d}^{2}0.5\\ell_{2}(x)}{\\mathrm{d}x^{2}}=A^{\\top}B_{nc}(x)A\\)__ * _Let_ \\(l>0\\) _denote a scalar_ _if for all \\(i\\in[n]\\), \\(w_{i}^{2}\\geq 8+200\\gamma_{c}\\gamma^{-3}+l/\\sigma_{\\min}(A)^{2}\\), we have_ \\[H(x)\\succeq l\\cdot I_{d}\\] Proof.: We have \\[H(x) =H_{1}(x)+H_{2}(x)+H_{3}(x)\\] \\[=A^{\\top}P(x)A+H_{2}(x)+H_{3}(x)\\] \\[=A^{\\top}P(x)A+A^{\\top}(B_{c}(x)+W^{2})+H_{3}(x)\\] \\[=A^{\\top}P(x)A+A^{\\top}(B_{c}(x)+W^{2})+A^{\\top}B_{nc}(x)A\\] \\[=A^{\\top}(P(x)+B_{c}(x)+W^{2}+B_{nc}(x))A\\] where the first equality follows from Lemma 42, the second equality follows from Definition 44, the third equality follows from Lemma 39 and the Definition of \\(B_{c}(x)\\), the fourth equality follows from the Definition of \\(B_{nc}(x)\\), the last equality follows from simple algebra. Let \\[D:=P(x)+B_{c}(x)+W^{2}+B_{nc}(x)\\] then, \\(\\frac{\\mathrm{d}^{2}L}{\\mathrm{d}x^{2}}\\) can be rewrite as \\[H(x)=A^{\\top}DA \\tag{1}\\] now we have the boundary of \\(D\\) as follows \\[D =P(x)+B_{c}(x)+W^{2}+B_{nc}(x)\\] \\[\\succeq -200\\gamma_{c}\\gamma^{-3}\\cdot I_{n}+B_{c}(x)+W^{2}+B_{nc}(x)\\] \\[\\succeq -200\\gamma_{c}\\gamma^{-3}\\cdot I_{n}-4I_{n}+W^{2}+B_{nc}(x)\\] \\[\\succeq -200\\gamma_{c}\\gamma^{-3}\\cdot I_{n}-4I_{n}+W^{2}-4I_{n}\\] \\[\\succeq -200\\gamma_{c}\\gamma^{-3}\\cdot I_{n}-4I_{n}+w_{\\min}^{2}-4I_{n}\\] where the first equality follows from the Definition of \\(D\\), the second equality follows from Lemma 45, the third and fourth equalities follow from Lemma 46, the fifth equality follows from Fact 21. When \\(w_{\\min}^{2}\\geq 8+200\\gamma_{c}\\gamma^{-3}+l/\\sigma_{\\min}(A)^{2}\\), we have \\[D\\succeq\\frac{l}{\\sigma_{\\min}(A)^{2}}I_{n} \\tag{2}\\]then \\[H(x) =A^{\\top}DA\\] \\[\\succeq\\sigma_{\\min}(D)\\cdot\\sigma_{\\min}(A)^{2}I_{d}\\] \\[\\succeq l\\cdot I_{d}\\] where the first equality follows from Eq (1), the second equality follows from Fact 21, the last equality follows from Eq (2)."
    },
    {
      "title": "Definition Of Matrix Functions \\(P_{I}(X)\\)",
      "text": "**Definition 44**.: _If the given conditions are satisfied_ * _Let_ \\(\\ell(x)\\) _be denoted as Definition_ 26__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * _Denote_ \\(B(x)\\) _as Definition_ 37__ * _Let_ \\(H_{1}(x):=\\frac{\\mathrm{d}^{2}\\gamma_{c}\\ell_{1}(x)^{-1}}{\\mathrm{d}x^{2}}\\)__ * _Denote_ \\(\\gamma_{c}>0\\) _a scalar_ _We define \\(P_{1}(x),P_{2}(x),P_{3}(x),P_{4}(x),P_{5}(x)\\in\\mathbb{R}^{n\\times n}\\) as follows_ * \\(P_{1}(x)=\\underbrace{\\ell(x)^{-3}}_{\\mathrm{scalar}}\\cdot\\underbrace{\\langle f (x),c(x)\\rangle^{2}}_{\\mathrm{scalar}}\\cdot\\underbrace{f(x)}_{n\\times 1} \\cdot\\underbrace{f(x)}_{1\\times n}^{\\top}\\)__ * \\(P_{2}(x)=\\underbrace{\\ell(x)^{-3}}_{\\mathrm{scalar}}\\cdot\\underbrace{\\langle f (x),c(x)\\rangle}_{\\mathrm{scalar}}\\cdot\\underbrace{\\langle f(x)\\circ c(x) \\rangle}_{n\\times 1}\\cdot\\underbrace{f(x)}_{1\\times n}^{\\top}\\)__ * \\(P_{3}(x)=\\underbrace{\\ell(x)^{-3}}_{\\mathrm{scalar}}\\cdot\\underbrace{\\langle f (x),c(x)\\rangle}_{\\mathrm{scalar}}\\cdot\\underbrace{f(x)}_{n\\times 1}\\cdot \\underbrace{\\langle f(x)\\circ c(x)\\rangle}_{1\\times n}^{\\top}\\)__ * \\(P_{4}(x)=\\underbrace{\\ell(x)^{-3}}_{\\mathrm{scalar}}\\cdot\\underbrace{\\langle f (x)\\circ c(x)\\rangle}_{n\\times 1}\\cdot\\underbrace{\\langle f(x)\\circ c(x) \\rangle}_{1\\times n}^{\\top}\\)__ * \\(P_{5}(x)=\\underbrace{\\ell(x)^{-2}}_{\\mathrm{scalar}}\\cdot\\underbrace{B(x)}_{n \\times n}\\)__ _We define \\(P(x)\\in\\mathbb{R}^{n\\times n}\\) as follows_ \\[P(x):=32\\gamma_{c}(P_{1}(x)-P_{2}(x)-P_{3}(x)+P_{4}(x))-P_{5}(x)\\] _Note that \\(H_{1}(x)=A^{\\top}P(x)A\\)._"
    },
    {
      "title": "Lower Bound Property For Matrix Function \\(P(X)\\)",
      "text": "**Lemma 45**.: _If the given conditions are satisfied_ * _Given a matrix_ \\(A\\in\\mathbb{R}^{n\\times d}\\)_._ * \\(\\gamma\\in(0,1)\\)__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * _Let_ \\(\\ell(x)\\) _be denoted as Definition_ 26__ * _Denote_ \\(P(x)\\) _as Definition_ 44__ * \\(\\ell(x)\\geq\\gamma\\)__ * \\(\\|f(x)\\|_{2}\\leq 1\\)__ _we have_ \\[P(x)\\succeq-104\\gamma^{-3}\\cdot I_{n}\\] Proof.: We have \\[P(x) =16(P_{1}(x)-P_{2}(x)-P_{3}(x)+P_{4}(x))-P_{5}(x)\\] \\[\\succeq 32\\gamma_{c}(-P_{2}(x)-P_{3}(x)+P_{4}(x))-P_{5}(x)\\] \\[\\succeq 32\\gamma_{c}(-P_{2}(x)-P_{3}(x))-P_{5}(x)\\] \\[\\succeq -32\\gamma_{c}P_{2.5}(x)-P_{5}(x)\\] \\[\\succeq -32\\gamma_{c}\\cdot 6\\gamma^{-3}\\cdot f(x)f(x)^{\\top}-P_{5}(x)\\] \\[\\succeq -32\\gamma_{c}\\cdot 6\\gamma^{-3}\\cdot f(x)f(x)^{\\top}-8\\gamma^{2} \\cdot I_{n}\\] \\[\\succeq -192\\gamma_{c}\\gamma^{-3}\\cdot f(x)f(x)^{\\top}-8\\gamma^{2}\\cdot I _{n}\\] \\[\\succeq -192\\gamma_{c}\\gamma^{-3}\\cdot\\|f(x)\\|_{2}^{2}\\cdot I_{n}-8 \\gamma^{2}\\cdot I_{n}\\] \\[\\succeq -192\\gamma_{c}\\gamma^{-3}\\cdot I_{n}-8\\gamma^{2}\\cdot I_{n}\\] \\[\\succeq -192\\gamma_{c}\\gamma^{-3}\\cdot I_{n}-8\\gamma^{3}\\cdot I_{n}\\] \\[\\succeq -200\\gamma_{c}\\gamma^{-3}\\cdot I_{n}\\] where the first equality follows from Definition 44, the second equality follows from Lemma 48, the third equality follows from Lemma 50, the fourth equality follows from Lemma 47, the fifth equality follows from Lemma 49, the sixth equality follows from Lemma 51, the seventh equality follows from simple algebra, the eighth equality follows from Fact 21, the ninth equality follows from \\(\\|f(x)\\|_{2}\\leq 1\\), the tenth equality follows from \\(\\gamma\\in(0,1)\\), the 1first equality follows from simple algebra."
    },
    {
      "title": "Helpful Lemma",
      "text": "**Lemma 46**.: _If the given conditions are satisfied_ * _Let_ \\(B(x)\\in\\mathbb{R}^{n\\times n}\\) _be denoted as Definition_ 37__* _Let_ \\(f(x)\\geq 0_{n}\\)__ * _Let_ \\(b\\geq 0_{n}\\)__ * \\(\\|f(x)\\|_{2}\\leq 1\\)__ * \\(\\|b\\|_{2}\\leq 1\\)__ _we have_ * _(see Part 5 of Lemma_ 6.2 _in_ _[_11_]__)_ \\[-4I_{n}\\preceq B(x)\\preceq 8I_{n}\\] **Lemma 47**.: _We define_ \\[P_{2.5}=\\ell(x)^{-3}\\cdot|\\langle f(x),c(x)\\rangle|\\cdot((f(x)\\circ c(x)) \\cdot(f(x)\\circ c(x))^{\\top}+f(x)f(x)^{\\top})\\] _Then, it is obvious that that_ \\[-P_{2.5}(x)\\preceq P_{2}(x)+P_{3}(x)\\preceq P_{2.5}(x)\\] Proof.: The reason is \\[-aa^{\\top}+bb^{\\top}\\preceq ab^{\\top}+ba^{\\top}\\preceq aa^{\\top}+bb^{\\top}\\]"
    },
    {
      "title": "Lower Bound Property For Matrix Function \\(P_{1}(X)\\)",
      "text": "**Lemma 48**.: _If the given conditions are satisfied_ * _Given a matrix_ \\(A\\in\\mathbb{R}^{n\\times d}\\)_._ * \\(\\gamma\\in(0,1)\\)__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * _Let_ \\(\\ell(x)\\) _be denoted as Definition_ 26__ * _Denote_ \\(P_{1}(x)\\) _as Definition_ 44__ * \\(\\ell(x)\\geq\\gamma\\)__ _we have_ \\[0\\preceq P_{1}(x)\\preceq 4\\gamma^{-3}\\cdot f(x)\\cdot f(x)^{\\top}\\]Proof.: On one hand, we can show that \\[P_{1}(x) = \\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle^{2}\\cdot f(x)\\cdot f(x)^{\\top}\\] \\[\\succeq 0\\] where the first equality follows from Part 1 of Lemma 67, the second equality follows from Part 2 of Lemma 67. On the other hand, we have \\[P_{1}(x) = \\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle^{2}\\cdot f(x)\\cdot f(x) ^{\\top}\\] \\[leq \\gamma^{-3}\\cdot\\langle f(x),c(x)\\rangle^{2}\\cdot f(x)\\cdot f(x) ^{\\top}\\] \\[\\preceq 2^{2}\\gamma^{-3}\\cdot f(x)\\cdot f(x)^{\\top}\\] \\[= 4\\gamma^{-3}\\cdot f(x)\\cdot f(x)^{\\top}\\] where the first equality follows from Definition 44, the second equality follows from \\(\\ell(x)\\geq\\gamma\\), the third equality follows from Part 2 of Lemma 67, the fourth equality follows from simple algebra."
    },
    {
      "title": "Lower Bound Property For Matrix Function \\(P_{2.5}(X)\\)",
      "text": "**Lemma 49**.: _If the given conditions are satisfied_ * _Given a matrix_ \\(A\\in\\mathbb{R}^{n\\times d}\\)_._ * \\(\\gamma\\in(0,1)\\)__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * _Let_ \\(\\ell(x)\\) _be denoted as Definition_ 26__ * _Let_ \\(P_{2.5}(x)\\) _be denoted as Lemma_ 47__ * \\(\\ell(x)\\geq\\gamma\\)__ _we have_ \\[0\\preceq P_{2.5}(x)\\preceq 6\\gamma^{-3}\\cdot f(x)f(x)^{\\top}\\] Proof.: On one hand, we can show that \\[P_{2.5}(x) = \\ell(x)^{-3}\\cdot|\\langle f(x),c(x)\\rangle|\\cdot\\left((f(x) \\circ c(x))\\cdot\\left(f(x)\\circ c(x)\\right)^{\\top}+f(x)f(x)^{\\top}\\right)\\] \\[\\succeq 0\\] where the first equality follows from the Definition of \\(P_{2.5}(x)\\), the second equality follows from Part 2 of Lemma 67. On the other hand, we have \\[P_{2.5}(x) = \\ell(x)^{-3}\\cdot|\\langle f(x),c(x)\\rangle|\\cdot\\left((f(x) \\circ c(x))\\cdot\\left(f(x)\\circ c(x)\\right)^{\\top}+f(x)f(x)^{\\top}\\right)\\] \\[\\leq \\gamma^{-3}\\cdot|\\langle f(x),c(x)\\rangle|\\cdot\\left((f(x)\\circ c (x))\\cdot\\left(f(x)\\circ c(x)\\right)^{\\top}+f(x)f(x)^{\\top}\\right)\\]\\[\\leq\\gamma^{-3}\\cdot 2\\cdot((f(x)\\circ c(x))\\cdot(f(x)\\circ c(x))^{ \\top}+f(x)f(x)^{\\top})\\] \\[\\leq\\gamma^{-3}\\cdot 2\\cdot(\\|c(x)\\|_{\\infty}+1)\\cdot f(x)f(x)^{\\top}\\] \\[\\leq\\gamma^{-3}\\cdot 2\\cdot(\\|c(x)\\|_{2}+1)\\cdot f(x)f(x)^{\\top}\\] \\[\\leq\\gamma^{-3}\\cdot 2\\cdot(2+1)\\cdot f(x)f(x)^{\\top}\\] \\[=6\\gamma^{-3}\\cdot f(x)f(x)^{\\top}\\] where the first equality follows from the Definition of \\(P_{2.5}(x)\\), the second equality follows from \\(\\ell(x)\\geq\\gamma\\), the third equality follows from Part 2 of Lemma 67, the fourth equality follows from Fact 21, the fifth equality follows from Fact 19, the sixth equality follows from Part 1 of Lemma 66, the last equality follows from simple algebra."
    },
    {
      "title": "Lower Bound Property For Matrix Function \\(P_{4}(X)\\)",
      "text": "**Lemma 50**.: _If the given conditions are satisfied_ * _Given a matrix_ \\(A\\in\\mathbb{R}^{n\\times d}\\)_._ * \\(\\gamma\\in(0,1)\\)__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * _Let_ \\(\\ell(x)\\) _be denoted as Definition_ 26__ * _Denote_ \\(P_{4}(x)\\) _as Definition_ 44__ * \\(\\ell(x)\\geq\\gamma\\)__ _we have_ \\[0\\preceq P_{4}(x)\\preceq 2\\gamma^{-3}\\cdot f(x)\\cdot f(x)^{\\top}\\] Proof.: On one hand, we can show that \\[P_{4}(x) =\\ell(x)^{-3}\\cdot(f(x)\\circ c(x))\\cdot(f(x)\\circ c(x))^{\\top}\\] \\[\\succeq 0\\] where the first equality follows from Definition 44, the second equality follows from Fact 21. On the other hand, we have \\[P_{4}(x) =\\ell(x)^{-3}\\cdot(f(x)\\circ c(x))\\cdot(f(x)\\circ c(x))^{\\top}\\] \\[\\leq\\gamma^{-3}\\cdot(f(x)\\circ c(x))\\cdot(f(x)\\circ c(x))^{\\top}\\] \\[\\leq\\gamma^{-3}\\cdot\\|c(x)\\|_{\\infty}\\cdot f(x)f(x)^{\\top}\\] \\[\\leq\\gamma^{-3}\\cdot\\|c(x)\\|_{2}\\cdot f(x)f(x)^{\\top}\\] \\[\\leq\\gamma^{-3}\\cdot 2\\cdot f(x)f(x)^{\\top}\\] \\[=2\\gamma^{-3}\\cdot f(x)f(x)^{\\top}\\] where the first equality follows from Definition 44, the second equality follows from \\(\\ell(x)\\geq\\gamma\\), the third equality follows from Fact 21, the fourth equality follows from Fact 19, the fifth equality follows from Part 1 of Lemma 66, the last equality follows from simple algebra."
    },
    {
      "title": "Lower Bound Property For Matrix Function \\(P_{5}(X)\\)",
      "text": "**Lemma 51**.: _If the given conditions are satisfied_ * _Given a matrix_ \\(A\\in\\mathbb{R}^{n\\times d}\\)_._ * \\(\\gamma\\in(0,1)\\)__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * _Let_ \\(\\ell(x)\\) _be denoted as Definition_ 26__ * _Denote_ \\(P_{5}(x)\\) _as Definition_ 44__ * \\(\\ell(x)\\geq\\gamma\\)__ _we have_ \\[-\\frac{1}{4}I_{n} \\preceq P_{5}(x)\\preceq 8\\gamma^{2}\\cdot I_{n}\\] Proof.: On one hand, we can show that \\[P_{5}(x) = \\ell(x)^{-2}\\cdot B(x)\\] \\[\\succeq \\ell(x)^{-2}\\cdot(-4I_{n})\\] \\[\\geq \\frac{1}{16}\\cdot(-4I_{n})\\] \\[= -\\frac{1}{4}I_{n}\\] where the first equality follows from Definition 44, the second equality follows from Lemma 46, the third equality follows from Part 1 of Lemma 67, the fourth equality follows from simple algebra. On the other hand, we have \\[P_{5}(x) = \\ell(x)^{-2}\\cdot B(x)\\] \\[\\preceq \\ell(x)^{-2}\\cdot 8I_{n}\\] \\[\\leq \\gamma^{-2}\\cdot 8I_{n}\\] \\[= 8\\gamma^{2}\\cdot I_{n}\\] where the first equality follows from Definition 44, the second equality follows from Lemma 46, the third equality follows from \\(\\ell(x)\\geq\\gamma\\), the fourth equality follows from simple algebra."
    },
    {
      "title": "Appendix E Hessian Is Lipschitz",
      "text": "In this appendix, we provide our result that Hessian of \\(L\\) is Lipschitz. In Appendix E.1, we provide our result and proof of \\(\\|\\nabla^{2}L(x)-\\nabla^{2}L(y)\\|\\leq(13344\\gamma_{c}+2)\\gamma^{-4}\\beta^{-2}n ^{1.5}\\exp(40R^{2})\\|x-y\\|_{2}\\). In Appendix E.2, we provide some helpful lemmas to simplify proofs. In Appendix E.3, we divide \\(H_{1}(x)\\) to several part to help us to compute its Lipschitz property. In Appendix E.4, we provide our result and proof of Lipschitz property for matrix function \\(Q_{1}(x)\\). In Appendix E.5, we provide our result and proof of Lipschitz property for matrix function \\(Q_{2}(x)\\). In Appendix E.6, we provide our result and proof of Lipschitz property for matrix function \\(Q_{3}(x)\\). In Appendix E.7, we provide our result and proof of Lipschitz property for matrix function \\(Q_{4}(x)\\). In Appendix E.8, we provide our result and proof of Lipschitz property for matrix function \\(Q_{5}(x)\\)."
    },
    {
      "title": "Main Result",
      "text": "**Lemma 52** (Formal version of Lemma 14).: _If the given conditions are satisfied_ * _Given two matrices_ \\(A_{1},A_{2}\\in\\mathbb{R}^{n\\times d}\\)_, where_ \\(A_{1}\\) _is the part of data has copyright_ * _Let_ \\(L\\) _be denoted as Definition_ 31__ * _Let_ \\(\\gamma\\in(0,1)\\)__ * _Let_ \\(\\beta\\in(0,0.1)\\)__ * _Let_ \\(\\gamma_{c}>0\\) _denote a scalar_ * _Let_ \\(R\\geq 4\\)__ * \\(\\ell(x)\\geq\\gamma\\)__ * \\(H(x):=\\frac{\\mathrm{d}^{2}L}{\\mathrm{d}x^{2}}\\)__ _Then, we have_ * \\[\\|H(x)-H(y)\\|\\leq(13344\\gamma_{c}+2)\\gamma^{-4}\\beta^{-2}n^{1.5} \\exp(40R^{2})\\|x-y\\|_{2}\\] Proof.: \\[\\|H(x)-H(y)\\|\\] \\[=\\|(H_{1}(x)+H_{2}(x)+H_{3}(x))-(H_{1}(y)+H_{2}(y)+H_{3}(y))\\|\\] \\[\\leq\\|H_{1}(x)-H_{1}(y)\\|+\\|H_{2}(x)-H_{2}(y)\\|+\\|H_{3}(x)-H_{3}(y)\\|\\] \\[\\leq 13344\\gamma_{c}\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2})\\|x- y\\|_{2}+\\|H_{2}(x)-H_{2}(y)\\|+\\|H_{3}(x)-H_{3}(y)\\|\\] \\[\\leq 13344\\gamma_{c}\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2})\\|x- y\\|_{2}+\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}+\\|H_{3}(x)-H_{3}(y)\\|\\] \\[\\leq 13344\\gamma_{c}\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2})\\|x- y\\|_{2}+\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}+\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}\\] \\[\\leq 13344\\gamma_{c}\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2})\\|x- y\\|_{2}+2\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}\\] \\[\\leq 13344\\gamma_{c}\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2})\\|x- y\\|_{2}+2\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2})\\|x-y\\|_{2}\\] \\[\\leq 13344\\gamma_{c}\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2})\\|x- y\\|_{2}+2\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2})\\|x-y\\|_{2}\\] \\[\\leq (13344\\gamma_{c}+2)\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2})\\|x- y\\|_{2}\\] where the first equality follows from Lemma 42, the second equality follows from Fact 19, the third equality follows from Lemma 53, the fourth equality follows from Lemma 54, the fifth equality follows from Lemma 61, the sixth equality follows from simple algebra, the seventh equality follows from \\(\\gamma\\in(0,1)\\), the eighth equality follows from \\(\\exp(20R^{2})\\leq\\exp(40R^{2})\\), the last equality follows from simple algebra."
    },
    {
      "title": "Helpful Lemma",
      "text": "**Lemma 53**.: _If the following condition holds_ * _Let_ \\(Q(x),Q_{1}(x),Q_{2}(x),Q_{3}(x),Q_{4}(x),Q_{5}(x)\\) _be denoted as Definition_ 55__* _Let_ \\(\\gamma\\in(0,1)\\)__ * _Let_ \\(\\beta\\in(0,0.1)\\)__ * _Let_ \\(\\gamma_{c}>0\\) _denote a scalar_ * _Let_ \\(R\\geq 4\\)__ * _Let_ \\(R_{f}:=\\beta^{-2}n^{1.5}\\exp(3R^{2})\\)__ * \\(\\ell(x)\\geq\\gamma\\)__ * \\(Q(x)=32\\gamma_{c}(Q_{1}(x)-Q_{2}(x)-Q_{3}(x)+Q_{4}(x))-Q_{5}(x)\\)__ * _Let_ \\(H_{1}(x):=\\frac{\\mathrm{d}^{2}\\gamma_{c}\\cdot\\ell(x)^{-1}}{\\mathrm{d}x^{2}}\\)__ * \\(\\|A\\|\\leq R\\)__ _Then, we have_ * _Part 1._ * _Part 2._ \\[\\|H_{1}(x)-H_{1}(y)\\|\\leq 6672\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2})\\|x-y \\|_{2}\\] Proof.: **Proof of Part 1.** \\[\\|Q(x)-Q(y)\\| =\\|(32\\gamma_{c}(Q_{1}(x)-Q_{2}(x)-Q_{3}(x)+Q_{4}(x))-Q_{5}(x))\\] \\[\\quad-(32\\gamma_{c}(Q_{1}(y)-Q_{2}(y)-Q_{3}(y)+Q_{4}(y))-Q_{5}(y))\\|\\] \\[\\leq 32\\gamma_{c}\\|Q_{1}(x)-Q_{1}(y)\\|+16\\|Q_{2}(x)-Q_{2}(y)\\|\\] \\[\\quad+32\\gamma_{c}\\|Q_{3}(x)-Q_{3}(y)\\|+16\\|Q_{4}(x)-Q_{4}(y)\\|+ \\|Q_{5}(x)-Q_{5}(y)\\|)\\] \\[\\leq 32\\gamma_{c}(\\|Q_{1}(x)-Q_{1}(y)\\|+\\|Q_{2}(x)-Q_{2}(y)\\|\\] \\[\\quad+\\|Q_{3}(x)-Q_{3}(y)\\|+\\|Q_{4}(x)-Q_{4}(y)\\|+\\|Q_{5}(x)-Q_{5} (y)\\|) \\tag{3}\\] where the first equality follows from the Definition of \\(Q(x)\\), the second equality follows from Fact 19, the third equality follows from simple algebra. Then we combine Lemma 56, Lemma 57, Lemma 58, Lemma 59, we show that \\[\\|Q_{1}(x)-Q_{1}(y)\\|+\\|Q_{2}(x)-Q_{2}(y)\\|+\\|Q_{3}(x)-Q_{3}(y)\\|+ \\|Q_{4}(x)-Q_{4}(y)\\|\\] \\[\\leq 68\\gamma^{-4}R_{f}\\|x-y\\|_{2}+64\\gamma^{-4}R_{f}\\|x-y\\|_{2}+64 \\gamma^{-4}R_{f}\\|x-y\\|_{2}+132\\gamma^{-4}R_{f}\\|x-y\\|_{2}\\] \\[= 328\\gamma^{-4}R_{f}\\|x-y\\|_{2}\\] \\[= 328\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(3R^{2})\\|x-y\\|_{2}\\] \\[\\leq 328\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2} \\tag{4}\\] where the second equality follows from simple algebra, the third equality follows from \\(R_{f}=\\beta^{-2}n^{1.5}\\exp(3R^{2})\\), the fourth equality follows from \\(\\exp(3R^{2})\\leq\\exp(20R^{2})\\). So we have \\[\\|Q(x)-Q(y)\\| \\leq 32\\gamma_{c}(\\|Q_{1}(x)-Q_{1}(y)\\|+\\|Q_{2}(x)-Q_{2}(y)\\|\\] \\[\\quad+\\|Q_{3}(x)-Q_{3}(y)\\|+\\|Q_{4}(x)-Q_{4}(y)\\|+\\|Q_{5}(x)-Q_{5}( y)\\|)\\] \\[\\leq 32\\gamma_{c}(328\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y \\|_{2}+\\|Q_{5}(x)-Q_{5}(y)\\|)\\] \\[\\leq 32\\gamma_{c}(328\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y \\|_{2}+89\\gamma^{-3}\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2})\\] \\[= 13344\\gamma_{c}\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_ {2}\\] where the first equality follows from Eq (3), the second equality follows from Eq (4), the third equality follows from Lemma 60, the fourth equality follows from simple algebra. **Proof of Part 2.** \\[\\|H_{1}(x)-H_{1}(y)\\| =A^{\\top}\\|Q(x)-Q(y)\\|A\\] \\[\\leq\\|A\\|\\cdot\\|Q(x)-Q(y)\\|\\cdot\\|A\\|\\] \\[\\leq R^{2}\\cdot\\|Q(x)-Q(y)\\|\\] \\[\\leq R^{2}\\cdot 13344\\gamma_{c}\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(2 0R^{2})\\|x-y\\|_{2}\\] \\[\\leq 13344\\gamma_{c}\\gamma^{-4}\\beta^{-2}n^{1.5}\\exp(40R^{2})\\|x-y \\|_{2}\\] where the first equality follows from Definition 55, the second equality follows from simple algebra, the third equality follows from \\(\\|A\\|\\leq R\\), the fourth equality follows from Part 1 of Lemma 53, the fifth equality follows from simple algebra. **Lemma 54** (Lemma 7.1 of [16]).: _If the following condition holds_ * _Let_ \\(\\beta\\in(0,0.1)\\)__ * _Let_ \\(R\\geq 4\\)__ * _Let_ \\(w\\in\\mathbb{R}^{n\\times d}\\)__ * _Let_ \\(W=\\operatorname{diag}(w)\\)__ * _Let_ \\(L_{reg}(x)=\\|WAx\\|_{2}\\)__ * _Let_ \\(H_{2}(x)=\\frac{\\mathrm{d}^{2}(L_{reg}(x)+0.5\\ell_{1}(x))}{\\mathrm{d}x^{2}}\\)__ _then we have_ \\[\\|H_{2}(x)-H_{2}(y)\\|\\leq\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}\\]"
    },
    {
      "title": "Definition Of Matrix Functions \\(Q_{I}(X)\\)",
      "text": "**Definition 55**.: _If the given conditions are satisfied_ * _Let_ \\(\\ell(x)\\) _be denoted as Definition_ 26__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * _Denote_ \\(B(x)\\) _as Definition_ 37__* _Let_ \\(H_{1}(x):=\\frac{\\mathrm{d}^{2}\\gamma_{c}\\cdot\\ell_{1}(x)^{-1}}{\\mathrm{d}x^{2}}\\)__ _We define \\(Q_{1}(x),Q_{2}(x),Q_{3}(x),Q_{4}(x),Q_{5}(x)\\in\\mathbb{R}^{n\\times n}\\) as follows_ * \\(Q_{1}(x)=\\underbrace{\\ell(x)^{-3}}_{\\mathrm{scalar}}\\cdot\\underbrace{\\langle f (x),c(x)\\rangle^{2}}_{\\mathrm{scalar}}\\cdot\\underbrace{f(x)}_{n\\times 1}\\cdot \\underbrace{f(x)^{\\top}}_{1\\times n}\\)__ * \\(Q_{2}(x)=\\underbrace{\\ell(x)^{-3}}_{\\mathrm{scalar}}\\cdot\\underbrace{\\langle f (x),c(x)\\rangle}_{\\mathrm{scalar}}\\cdot\\underbrace{\\langle f(x)\\circ c(x) \\rangle}_{n\\times 1}\\cdot\\underbrace{f(x)^{\\top}}_{1\\times n}\\)__ * \\(Q_{3}(x)=\\underbrace{\\ell(x)^{-3}}_{\\mathrm{scalar}}\\cdot\\underbrace{\\langle f (x),c(x)\\rangle}_{\\mathrm{scalar}}\\cdot\\underbrace{f(x)}_{n\\times 1}\\cdot \\underbrace{\\langle f(x)\\circ c(x)\\rangle^{\\top}}_{1\\times n}\\)__ * \\(Q_{4}(x)=\\underbrace{\\ell(x)^{-3}}_{\\mathrm{scalar}}\\cdot\\underbrace{\\langle f (x)\\circ c(x)\\rangle}_{n\\times 1}\\cdot\\underbrace{\\langle f(x)\\circ c(x) \\rangle^{\\top}}_{1\\times n}\\)__ * \\(Q_{5}(x)=\\underbrace{\\ell(x)^{-2}}_{\\mathrm{scalar}}\\cdot\\underbrace{B(x)}_{n \\times n}\\)__ _We define \\(Q(x)\\in\\mathbb{R}^{n\\times n}\\) as follows_ \\[Q(x):=32\\gamma_{c}(Q_{1}(x)-Q_{2}(x)-Q_{3}(x)+Q_{4}(x))-Q_{5}(x)\\] _Note that \\(H_{1}(x)=A^{\\top}Q(x)A\\)._"
    },
    {
      "title": "Lipschitz Property For Matrix Function \\(Q_{1}(X)\\)",
      "text": "**Lemma 56**.: _If the given conditions are satisfied_ * _Denote_ \\(Q_{1}(x)\\) _as Definition_ 55__ * _Let_ \\(\\gamma\\in(0,1)\\)__ * _Let_ \\(\\beta\\in(0,0.1)\\)__ * _Let_ \\(R\\geq 4\\)__ * _Let_ \\(R_{f}:=\\beta^{-2}n^{1.5}\\exp(3R^{2})\\)__ * \\(\\ell(x)\\geq\\gamma\\)__ _Then, we have_ * \\[\\|Q_{1}(x)-Q_{1}(y)\\|\\leq 68\\gamma^{-4}\\cdot R_{f}\\cdot\\|x-y\\|_{2}\\] Proof.: \\[\\|Q_{1}(x)-Q_{1}(y)\\| =\\|\\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle^{2}\\cdot f(x)f(x)^{ \\top}-\\ell(y)^{-3}\\cdot\\langle f(y),c(y)\\rangle^{2}\\cdot f(y)f(y)^{\\top}\\|\\] \\[\\leq|\\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle^{2}|\\cdot\\|f(x)\\| _{2}\\cdot\\|f(x)-f(y)\\|_{2}\\] \\[\\quad+|\\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle^{2}|\\cdot\\|f(x )-f(y)\\|_{2}\\cdot\\|f(y)\\|_{2}\\] \\[\\quad+\\|f(x)\\|_{2}\\cdot|\\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle ^{2}-\\ell(y)^{-3}\\cdot\\langle f(y),c(y)\\rangle^{2}|\\cdot\\|f(x)\\|_{2}\\] [MISSING_PAGE_EMPTY:41] [MISSING_PAGE_EMPTY:42] [MISSING_PAGE_EMPTY:43] [MISSING_PAGE_EMPTY:44] \\[\\leq |\\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle-\\ell(y)^{-3}\\cdot \\langle f(y),c(y)\\rangle|\\cdot\\|f(y)\\circ c(y)\\|_{2}\\] \\[\\leq |\\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle-\\ell(y)^{-3}\\cdot \\langle f(y),c(y)\\rangle|\\cdot 2\\] \\[\\leq 27\\gamma^{-4}\\cdot R_{f}\\|x-y\\|_{2}\\cdot 2\\] \\[= 54\\gamma^{-4}\\cdot R_{f}\\|x-y\\|_{2} \\tag{13}\\] where the first equality follows from the Definition of \\(Q_{3,3}(x)\\), the second equality follows from \\(\\|f(x)\\|_{2}\\leq 1\\), the third equality follows from Part 2 of Lemma 66, the fourth equality follows from Part 1 of Lemma 64, the last equality follows from simple algebra. So we have \\[\\|Q_{3}(x)-Q_{3}(y)\\| \\leq Q_{3,1}(x)+Q_{3,2}(x)+Q_{3,3}(x)\\] \\[\\leq 6\\gamma^{-3}\\cdot R_{f}\\cdot\\|x-y\\|_{2}+Q_{3,2}(x)+Q_{3,3}(x)\\] \\[\\leq 6\\gamma^{-3}\\cdot R_{f}\\cdot\\|x-y\\|_{2}+4\\gamma^{-3}\\cdot R _{f}\\cdot\\|x-y\\|_{2}+Q_{3,3}(x)\\] \\[\\leq 6\\gamma^{-3}\\cdot R_{f}\\cdot\\|x-y\\|_{2}+4\\gamma^{-3}\\cdot R _{f}\\cdot\\|x-y\\|_{2}+54\\gamma^{-4}\\cdot R_{f}\\|x-y\\|_{2}\\] \\[\\leq 64\\gamma^{-4}\\cdot R_{f}\\|x-y\\|_{2}\\] where the first equality follows from Eq. (11), the second equality follows from Eq. (12), the third equality follows from Eq. (13), the fourth equality follows from simple algebra."
    },
    {
      "title": "Lipschitz Property For Matrix Function \\(Q_{4}(X)\\)",
      "text": "**Lemma 59**.: _If the given conditions are satisfied_ * _Denote_ \\(Q_{4}(x)\\) _as Definition_ 55__ * _Let_ \\(\\gamma\\in(0,1)\\)__ * _Let_ \\(\\beta\\in(0,0.1)\\)__ * _Let_ \\(R\\geq 4\\)__ * _Let_ \\(R_{f}:=\\beta^{-2}n^{1.5}\\exp(3R^{2})\\)__ * \\(\\ell(x)\\geq\\gamma\\)__ _We have_ \\[\\|Q_{4}(x)-Q_{4}(y)\\|\\leq 132\\gamma^{-3}\\cdot R_{f}\\|x-y\\|_{2}\\] Proof.: \\[\\|Q_{4}(x)-Q_{4}(y)\\| =\\|\\ell(x)^{-3}\\langle f(x),c(x)\\rangle(f(x)\\circ c(x))(f(x) \\circ c(x))^{\\top}\\] \\[\\quad-\\ell(y)^{-2}\\langle f(y),c(y)\\rangle(f(y)\\circ c(y))(f(y) \\circ c(y))^{\\top}\\|\\] \\[\\leq|\\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle|\\cdot\\|f(x)\\circ c (x)\\|_{2}\\cdot\\|f(x)\\circ c(x)-f(y)\\circ c(y)\\|_{2}\\] \\[\\quad+|\\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle|\\cdot\\|f(x)\\circ c (x)-f(y)\\circ c(y)\\|_{2}\\cdot\\|f(y)\\circ c(y)\\|_{2}\\] \\[\\quad+\\|f(y)\\circ c(y)\\|_{2}\\cdot|\\ell(x)^{-3}\\cdot\\langle f(x), c(x)\\rangle-\\ell(y)^{-3}\\cdot\\langle f(y),c(y)\\rangle|\\cdot\\|f(y)\\circ c(y)\\|_{2}\\] [MISSING_PAGE_EMPTY:46] \\[\\leq 12\\gamma^{-3}\\cdot R_{f}\\cdot\\|x-y\\|_{2}+12\\gamma^{-2}\\cdot R_{f} \\cdot\\|x-y\\|_{2}+Q_{3,3}(x)\\] \\[\\leq 12\\gamma^{-3}\\cdot R_{f}\\cdot\\|x-y\\|_{2}+12\\gamma^{-3}\\cdot R_{f} \\cdot\\|x-y\\|_{2}+108\\gamma^{-4}\\cdot R_{f}\\|x-y\\|_{2}\\] \\[\\leq 132\\gamma^{-4}\\cdot R_{f}\\|x-y\\|_{2}\\] where the second equality follows from Eq. (14), the third equality follows from Eq. (15), the fourth equality follows from Eq. (16), the fifth equality follows from simple algebra."
    },
    {
      "title": "Lipschitz Property For Matrix Function \\(Q_{5}(X)\\)",
      "text": "**Lemma 60**.: _If the given conditions are satisfied_ * _Denote_ \\(Q_{5}(x)\\) _as Definition_ 55__ * _Let_ \\(\\gamma\\in(0,1)\\)__ * _Let_ \\(\\beta\\in(0,0.1)\\)__ * _Let_ \\(R\\geq 4\\)__ * _Let_ \\(R_{f}:=\\beta^{-2}n^{1.5}\\exp(3R^{2})\\)__ * \\(\\ell(x)\\geq\\gamma\\)__ _We have_ \\[\\|Q_{5}(x)-Q_{5}(y)\\| \\leq 89\\gamma^{-3}\\cdot\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}\\] Proof.: We have \\[\\|Q_{5}(x)-Q_{5}(y)\\| =\\|\\ell(x)^{-2}B(x)-\\ell(y)^{-2}B(y)\\|\\] \\[\\leq\\|\\ell(x)^{-2}B(x)-\\ell(x)^{-2}B(y)+\\ell(x)^{-2}B(y)-\\ell(y) ^{-2}B(y)\\|\\] \\[\\leq\\|\\ell(x)^{-2}B(x)-\\ell(x)^{-2}B(y)\\|+\\|\\ell(x)^{-2}B(y)- \\ell(y)^{-2}B(y)\\|\\] \\[=|\\ell(x)^{-2}|\\cdot\\|B(x)-B(y)\\|+|\\ell(x)^{-2}-\\ell(y)^{-2}| \\cdot\\|B(y)\\|\\] \\[\\leq\\gamma^{-2}\\cdot\\|B(x)-B(y)\\|+|\\ell(x)^{-2}-\\ell(y)^{-2}| \\cdot\\|B(y)\\|\\] \\[\\leq\\gamma^{-2}\\cdot\\|B(x)-B(y)\\|+|\\ell(x)^{-2}-\\ell(y)^{-2}| \\cdot 11\\] \\[\\leq\\gamma^{-2}\\cdot\\|B(x)-B(y)\\|+8\\gamma^{-3}\\cdot R_{f}\\cdot\\| x-y\\|_{2}\\cdot 11\\] \\[\\leq\\gamma^{-2}\\cdot\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}+8 \\gamma^{-3}\\cdot R_{f}\\cdot\\|x-y\\|_{2}\\cdot 11\\] \\[\\leq\\gamma^{-3}\\cdot\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}+8 \\gamma^{-3}\\cdot R_{f}\\cdot\\|x-y\\|_{2}\\cdot 11\\] \\[\\leq\\gamma^{-3}\\cdot\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}+8 \\gamma^{-3}\\cdot\\beta^{-2}n^{1.5}\\exp(3R^{2})\\cdot\\|x-y\\|_{2}\\cdot 11\\] \\[\\leq 89\\gamma^{-3}\\cdot\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}\\] \\[\\leq 89\\gamma^{-4}\\cdot\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}\\] where the first equality follows from Definition 55, the second, third, fourth equalities follow from Fact 20, the fifth equality follows from \\(\\ell(x)\\geq\\gamma\\), the sixth equality follows from Part 1 of Lemma 68,the seventh equality follows from Part 2 fo Lemma 62, the eighth equality follows from Lemma 61, the ninth equality follows from \\(\\gamma\\in(0,1)\\), the tenth equality follows from \\(R_{f}=\\beta^{-2}n^{1.5}\\exp(3R^{2})\\), the 1first equality follows from \\(\\exp(3R^{2})\\leq\\exp(20R^{2})\\), the 1second equality follows from simple algebra, the last equality follows from \\(\\gamma\\in(0,1)\\). **Lemma 61**.: _If the given conditions are satisfied_ * _Let_ \\(A\\in\\mathbb{R}^{n\\times d}\\)__ * _Let_ \\(\\beta\\in(0,0.1)\\)__ * _Let_ \\(R\\geq 4\\)__ * _Let_ \\(w\\in\\mathbb{R}^{n\\times d}\\)__ * _Let_ \\(W=\\operatorname{diag}(w)\\)__ * _Let_ \\(L_{reg}(x)=\\|WAx\\|_{2}\\)__ * _Let_ \\(H_{2}(x)=\\frac{\\operatorname{d}^{2}(L_{reg}(x)+\\ell(x))}{\\operatorname{d}x^{2}}\\)__ _then we have_ \\[\\|B(x)-B(y)\\|\\leq\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}\\] Proof.: We have \\[\\|H_{2}(x)-H_{2}(y)\\| =\\|\\frac{\\operatorname{d}^{2}(L_{reg}(x)+\\ell(x))}{\\operatorname{ d}x^{2}}-\\frac{\\operatorname{d}^{2}(L_{reg}(y)+\\ell(y))}{\\operatorname{d}y^{2}}\\|\\] \\[=\\|\\frac{\\operatorname{d}^{2}L_{reg}(x)}{\\operatorname{d}x^{2}}+ \\frac{\\operatorname{d}^{2}\\ell(x)}{\\operatorname{d}x^{2}}-\\frac{\\operatorname {d}^{2}L_{reg}(y)}{\\operatorname{d}y^{2}}-\\frac{\\operatorname{d}^{2}\\ell(y)}{ \\operatorname{d}y^{2}}\\|\\] where the first equality follows from the Definition of \\(H(x)\\), the second equality follows from simple differential rule. So we have \\[\\|B(x)-B(y)\\| \\leq A^{\\top}\\cdot\\|B(x)-B(y)\\|\\cdot A\\] \\[=\\|\\frac{\\operatorname{d}^{2}\\ell(x)}{\\operatorname{d}x^{2}}- \\frac{\\operatorname{d}^{2}\\ell(y)}{\\operatorname{d}y^{2}}\\|\\] \\[\\leq\\|\\frac{\\operatorname{d}^{2}L_{reg}(x)}{\\operatorname{d}x^{2 }}+\\frac{\\operatorname{d}^{2}\\ell(x)}{\\operatorname{d}x^{2}}-\\frac{ \\operatorname{d}^{2}L_{reg}(y)}{\\operatorname{d}y^{2}}-\\frac{\\operatorname{d} ^{2}\\ell(y)}{\\operatorname{d}y^{2}}\\|\\] \\[=\\|H(x)-H(y)\\|\\] \\[\\leq\\beta^{-2}n^{1.5}\\exp(20R^{2})\\|x-y\\|_{2}\\] where the first equality follows from simple algebra, the second equality follows from Lemma 36, the third equality follows from, the fourth equality follows from the Definition of \\(H(x)\\), the fifth equality follows from Lemma 54. Lipschitz Tools In this appendix, we provide a set of tools that can assist in the computation of the Lipschitz property. In Appendix F.1, we provide Lipschitz tool of some scalar functions. In Appendix F.2, we provide Lipschitz tool of some vector functions."
    },
    {
      "title": "Lipschitz Tool: Scalar Function",
      "text": "**Lemma 62**.: _If the given conditions are satisfied_ * _Let_ \\(A\\in\\mathbb{R}^{n\\times d}\\)__ * _Let_ \\(\\beta\\in(0,0.1)\\)__ * _Let_ \\(R\\geq 4\\)__ * _Let_ \\(\\gamma\\in(0,1)\\)__ * _Let_ \\(x,y\\in\\mathbb{R}^{d}\\) _satisfy_ \\(\\|A(x-y)\\|_{\\infty}<0.01\\)__ * _Let_ \\(R_{f}:=\\beta^{-2}n^{1.5}\\exp 3R^{2}\\)__ * _Let_ \\(\\ell(x)\\) _be denoted as Definition_ 26__ * \\(\\ell(x)\\geq\\gamma\\)__ _We have_ * _Part 1._ \\[|\\ell(x)-\\ell(y)|\\leq 4R_{f}\\cdot\\|x-y\\|_{2}\\] * _Part 2. Let_ \\(p\\in\\{1,2,3\\}\\)_, then we have_ \\[|\\ell(x)^{-p}-\\ell(y)^{-p}|\\leq 4p\\gamma^{-(1+p)}\\cdot R_{f}\\cdot\\|x-y\\|_{2}\\] Proof.: **Proof of Part 1.** \\[|\\ell(x)-\\ell(y)| =|\\langle c(x),c(x)\\rangle-\\langle c(y),c(y)\\rangle|\\] \\[=|\\langle c(x)-c(y),c(x)+c(y)\\rangle|\\] \\[\\leq\\|c(x)-c(y)\\|_{2}\\cdot\\|c(x)+c(y)\\|_{2}\\] \\[\\leq(\\|c(x)\\|_{2}+\\|c(y)\\|_{2})\\cdot\\|c(x)-c(y)\\|_{2}\\] \\[\\leq 4\\cdot\\|c(x)-c(y)\\|_{2}\\] \\[\\leq 4R_{f}\\cdot\\|x-y\\|_{2}\\] where the first equality follows from Fact 17, the second equality follows from Fact 19 (Cauchy-Schwarz inequality), the third equality follows from Fact 19, the fourth equality follows from Part 1 of Lemma 66, the fifth equality follows from Part 2 of Lemma 65. **Proof of Part 2.** \\[|\\ell(x)^{-p}-\\ell(y)^{-p}|=|\\ell(x)-\\ell(y)|\\cdot|\\sum_{i=0}^{p-1}\\ell(x)^{- p+i}\\ell(y)^{-i-1}|\\]\\[\\leq \\|c(x)-c(y)\\|_{2}+2\\|f(x)^{\\top}-f(x)^{\\top}\\|_{2}\\] \\[\\leq R_{f}\\cdot\\|x-y\\|_{2}+2\\|f(x)^{\\top}-f(x)^{\\top}\\|_{2}\\] \\[\\leq R_{f}\\cdot\\|x-y\\|_{2}+2R_{f}\\cdot\\|x-y\\|_{2}\\]\\[\\leq 3R_{f}\\cdot\\|x-y\\|_{2}\\] where the first equality follows from Fact 17, the second equality follows from Part 1 of Lemma 23, the third equality follows from \\(\\|f(x)\\|\\leq 1\\), the fourth equality follows from Part 1 of Lemma 66, the fifth equality follows from Part 2 of Lemma 65, the sixth equality follows from Part 1 of Lemma 65, the last equality follows from simple algebra. **Proof of Part 2.** \\[|\\langle f(x),c(x)\\rangle^{2}-\\langle f(y),c(y)\\rangle^{2}| \\leq |\\langle f(x),c(x)\\rangle+\\langle f(y),c(y)\\rangle|\\cdot|\\langle f (x),c(x)\\rangle-\\langle f(y),c(y)\\rangle|\\] \\[\\leq |2+2|\\cdot|\\langle f(x),c(x)\\rangle-\\langle f(y),c(y)\\rangle|\\] \\[\\leq |2+2|\\cdot 3R_{f}\\cdot\\|x-y\\|_{2}\\] \\[= 12R_{f}\\cdot\\|x-y\\|_{2}\\] where the first equality follows from simple algebra, the second equality follows from Part 2 of Lemma 67, the third equality follows from Part 1 of Lemma 63, the fourth equality follows from simple algebra. **Lemma 64**.: _If the given conditions are satisfied_ * _Let_ \\(A\\in\\mathbb{R}^{n\\times d}\\)__ * _Let_ \\(\\beta\\in(0,0.1)\\)__ * _Let_ \\(R\\geq 4\\)__ * _Let_ \\(\\gamma\\in(0,1)\\)__ * _Let_ \\(x,y\\in\\mathbb{R}^{d}\\) _satisfy_ \\(\\|A(x-y)\\|_{\\infty}<0.01\\)__ * _Let_ \\(R_{f}:=\\beta^{-2}n^{1.5}\\exp 3R^{2}\\)__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * _Let_ \\(\\ell(x)\\) _be denoted as Definition_ 26__ * \\(\\ell(x)\\geq\\gamma\\)__ _We have_ * _Part 1._ \\[|\\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle-\\ell(y)^{-3}\\cdot\\langle f(y),c(y) \\rangle|\\leq 27\\gamma^{-4}\\cdot R_{f}\\|x-y\\|_{2}\\] * _Part 2._ \\[|\\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle^{2}-\\ell(y)^{-3}\\cdot\\langle f(y), c(y)\\rangle^{2}|\\leq 60\\gamma^{-4}\\cdot R_{f}\\cdot\\|x-y\\|_{2}\\] Proof.: **Proof of Part 1.** \\[|\\ell(x)^{-3}\\cdot\\langle f(x),c(x)\\rangle-\\ell(y)^{-3}\\cdot \\langle f(y),c(y)\\rangle|\\] \\[= |\\ell(x)^{-3}|\\cdot|\\langle f(x),c(x)\\rangle-\\langle f(y),c(y) \\rangle|+|\\ell(x)^{-3}-\\ell(y)^{-3}|\\cdot|\\langle f(y),c(y)\\rangle|\\] [MISSING_PAGE_EMPTY:52] * _Denote_ \\(c(x)\\) _as Definition_ 25__ _We have_ * _Part 1. (see Part 4 of Lemma_ 7.2 _in_ _[_11_]__)_ \\[\\|f(x)-f(y)\\|_{2}\\leq R_{f}\\cdot\\|x-y\\|_{2}\\] * _Part 2. (see Part 5 of Lemma_ 7.2 _in_ _[_11_]__)_ \\[\\|c(x)-c(y)\\|_{2}\\leq R_{f}\\cdot\\|x-y\\|_{2}\\] * _Part 3._ \\[\\|f(x)\\circ c(x)-f(y)\\circ c(y)\\|_{2}\\leq 3R_{f}\\cdot\\|x-y\\|_{2}\\] Proof.: **Proof of Part 3.** \\[\\|f(x)\\circ c(x)-f(y)\\circ c(y)\\|_{2} \\leq\\|f(x)\\|_{2}\\cdot\\|c(x)-c(y)\\|_{2}+\\|f(x)-f(y)\\|_{2}\\cdot\\|c(y )\\|_{2}\\] \\[\\leq\\|c(x)-c(y)\\|_{2}+\\|f(x)-f(y)\\|_{2}\\cdot\\|c(y)\\|_{2}\\] \\[\\leq\\|c(x)-c(y)\\|_{2}+2\\cdot\\|f(x)-f(y)\\|_{2}\\] \\[\\leq R_{f}\\cdot\\|x-y\\|_{2}+2\\cdot\\|f(x)-f(y)\\|_{2}\\] \\[\\leq R_{f}\\cdot\\|x-y\\|_{2}+2R_{f}\\cdot\\|x-y\\|_{2}\\] \\[= 3R_{f}\\cdot\\|x-y\\|_{2}\\] where the first equality follows from Part 5 of Lemma 23, the second equality follows from \\(\\|f(x)\\|_{2}\\leq 1\\), the third equality follows from Part 1 of Lemma 66, the fourth equality follows from Part 2 of Lemma 65, the fifth equality follows from Part 1 of Lemma 65, the sixth equality follows from simple algebra."
    },
    {
      "title": "Appendix G Helpful Bounds",
      "text": "In this appendix, we present a collection of bound that are valuable for facilitating computations in our proofs. **Lemma 66**.: _If the given conditions are satisfied_ * _Let_ \\(b\\in\\mathbb{R}^{n}\\) _satisfy that_ \\(\\|b\\|_{1}\\leq 1\\)__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * \\(\\|f(x)\\|_{2}\\leq 1\\)__ _then we have_ * _Part 1._ \\(\\|c(x)\\|_{2}\\leq 2\\)__ * _Part 2._ \\(\\|f(x)\\circ c(x)\\|_{2}\\leq 2\\)__Proof.: **Proof of Part 1.** \\[\\|c(x)\\|_{2} = \\|f(x)-b\\|_{2}\\] \\[\\leq \\|f(x)\\|_{2}+\\|b\\|_{2}\\] \\[\\leq 1+\\|b\\|_{2}\\] \\[\\leq 1+1=2\\] where the first equality follow from Definition 25, the second equality follows Fact 19, the third equality follows from \\(\\|f(x)\\|_{2}\\leq 1\\), the fourth equality follows from \\(\\|b\\|_{2}\\leq 1\\). **Proof of Part 2.** \\[\\|f(x)\\circ c(x)\\|_{2} \\leq \\|f(x)\\|_{2}\\cdot\\|c(x)\\|_{2}\\] \\[\\leq 1\\cdot\\|c(x)\\|_{2}\\] \\[\\leq 2\\] where the first equalities follow from Fact 19, the second equality follows from \\(\\|f(x)\\|_{2}\\leq 1\\), the third equality follows from Part 1 of Lemma 66. **Lemma 67**.: _If the given conditions are satisfied_ * _Let_ \\(b\\in\\mathbb{R}^{n}\\) _satisfy that_ \\(\\|b\\|_{1}\\leq 1\\)__ * _Denote_ \\(f(x)\\) _as Definition_ 24__ * _Denote_ \\(c(x)\\) _as Definition_ 25__ * _Let_ \\(\\ell(x)\\) _be denoted as Definition_ 26__ * \\(\\|f(x)\\|_{2}\\leq 1\\)__ * _Let_ \\(\\gamma\\in(0,1)\\)__ * \\(\\ell(x)\\geq\\gamma\\)__ _then we have_ * _Part 1._ \\(\\ell(x)\\leq 4\\)__ * _Part 2._ \\(0.25\\|b\\|_{2}^{2}\\leq 0\\leq\\langle f(x),c(x)\\rangle\\leq 2\\)__ * _Part 3. Let_ \\(p,q\\in\\{1,2,3\\}\\)_, we have_ \\(\\ell(x)^{-p}\\langle f(x),c(x)\\rangle^{q}\\leq 2q\\gamma^{-p}\\)__ Proof.: **Proof of Part 1.** \\[\\ell(x) = \\langle c(x),c(x)\\rangle\\] \\[\\leq \\|c(x)\\|_{2}\\cdot\\|c(x)\\|_{2}\\] \\[\\leq 2\\times 2=4\\] where the first equality follow from Definition 26, the second equality follows from Fact 19 (Cauchy-Schwarz inequality), the third equality follows from Part 1 of Lemma 66. **Proof of Part 2.** On one hand, we have \\[\\langle f(x),c(x)\\rangle\\leq\\|f(x)\\|_{2}\\cdot\\|c(x)\\|_{2}\\] [MISSING_PAGE_FAIL:55] \\[\\leq 3+\\|2b\\|_{2}\\] \\[\\leq 3+2=5\\] where the first, second equalities follow from Fact 20, the third, fourth equalities follow from Fact 19, the fifth equality follows from \\(\\|f(x)\\|_{2}\\leq 1\\), the sixth equality follows from \\(\\|b\\|_{2}\\leq 1\\), the last equality follows from simple algebra. For the second term \\[\\|\\langle f(x)-b,f(x)\\rangle\\cdot\\operatorname{diag}(f(x))\\| \\leq |\\langle f(x)-b,f(x)\\rangle|\\cdot\\|\\operatorname{diag}(f(x))\\|\\] \\[\\leq \\|f(x)-b\\|_{2}\\cdot\\|f(x)\\|_{2}\\cdot\\|\\operatorname{diag}(f(x))\\|\\] \\[\\leq \\|f(x)-b\\|_{2}\\cdot\\|f(x)\\|_{2}\\cdot\\|f(x)\\|_{2}\\] \\[\\leq (\\|f(x)\\|_{2}+\\|b\\|_{2})\\cdot\\|f(x)\\|_{2}\\cdot\\|f(x)\\|_{2}\\] \\[\\leq \\|b\\|_{2}\\] \\[\\leq 1\\] where the first equality follows from Fact 20, the second equality follows from Fact 19 (Cauchy-Schwarz inequality), the third, fourth equalities follow from Fact 19, the fifth equality follows from \\(\\|f(x)\\|_{2}\\leq 1\\), the sixth equality follows from \\(\\|b\\|_{2}\\leq 1\\). For the third term \\[\\|\\operatorname{diag}((2f(x)-b)\\circ f(x))\\| \\leq \\|(2f(x)-b)\\circ f(x)\\|_{2}\\] \\[\\leq \\|2f(x)-b\\|_{2}\\cdot\\|f(x)\\|_{2}\\] \\[\\leq (\\|2f(x)\\|_{2}+\\|b\\|_{2})\\cdot\\|f(x)\\|_{2}\\] \\[\\leq 2+\\|b\\|_{2}\\] \\[\\leq 2+1=3\\] where the first, second, third equalities follow from Fact 19, the fourth equality follows from \\(\\|f(x)\\|_{2}\\leq 1\\), the fifth equality follows from \\(\\|b\\|_{2}=1\\), the last equality follows from simple algebra. For the fourth term \\[\\|(b\\circ f(x))\\cdot f(x)^{\\top}+f(x)\\cdot(b\\circ f(x))^{\\top}\\| \\leq \\|(b\\circ f(x))\\cdot f(x)^{\\top}\\|+\\|f(x)\\cdot(b\\circ f(x))^{ \\top}\\|\\] \\[\\leq \\|b\\circ f(x)\\|_{2}\\cdot\\|f(x)\\|_{2}+\\|f(x)\\|_{2}\\cdot\\|b\\circ f(x )\\|_{2}\\] \\[\\leq 2\\|b\\circ f(x)\\|_{2}\\] \\[\\leq 2(\\|b\\|_{2}\\cdot\\|f(x)\\|_{2})\\] \\[\\leq 2(\\|b\\|_{2}\\cdot 1)\\] \\[\\leq 2(1\\cdot 1)\\] \\[= 2\\] where the first equality follows from Fact 19, the second equality follows from Fact 19, the third equality follows from \\(\\|f(x)\\|_{2}\\leq 1\\), the fourth equality follows from \\(\\|b\\|_{2}=1\\), the last equality follows from simple algebra. Then we combine four terms, we have \\[\\|B(x)\\|\\leq 5+1+3+2=11\\] where the last equality follows from simple algebra. [MISSING_PAGE_FAIL:57] The positive definiteness of the Hessian matrix follows directly from Lemma 43. **Proof of Hessian is Lipschitz.** The Lipschitz property of the Hessian matrix can be established using Lemma 52. **Proof of Cost per iteration.** The cost per iteration can be deduced from Lemma 78. **Proof of Convergence per Iteration.** By utilizing Lemma 80, it can be shown that: \\[|x_{k}-x^{*}|2\\leq 0.4|xk-1-x^{*}|_{2}\\] **Proof of Number of Iterations.** After performing \\(T\\) iterations, we obtain the following result: \\[|x_{k}-x^{*}|2\\leq 0.4^{T}|xk-1-x^{*}|_{2}\\] By appropriately choosing the value of \\(T\\), we can achieve the desired bound. The failure probability is derived from applying the union bound over the \\(T\\) iterations."
    },
    {
      "title": "Appendix I \\(L\\) Is \\(\\Tau_{C}\\)-Copyright-Protected",
      "text": "In this appendix, we show our result that Copyright Regression avoids model outputting copyright data. In Appendix I.1, we reaffirm our definition of problem and optimal parameter \\(x^{*}\\) of Copyright Regression. In Appendix I.2, we provide our result and proof of \\(x^{*}\\) is \\(\\tau_{c}\\)-Copyright-Protected, where \\(\\tau_{c}=\\sqrt{2\\gamma_{c}}/n_{1}-\\epsilon_{2}/n_{2}\\)."
    },
    {
      "title": "Definitions",
      "text": "**Definition 70** (\\(\\tau\\)-Copyright-Protected).: _Given a matrix \\(A\\in\\mathbb{R}^{n\\times d}\\) and vector \\(b\\in\\mathbb{R}^{n}\\) that \\(A=\\begin{bmatrix}A_{1}\\\\ A_{2}\\end{bmatrix}\\), and \\(b=\\begin{bmatrix}b_{1}\\\\ b_{2}\\end{bmatrix}\\), where \\(A_{1}\\in\\mathbb{R}^{n_{1}\\times d}\\), \\(A_{2}\\in\\mathbb{R}^{n_{2}\\times d}\\), \\(b_{1}\\in\\mathbb{R}^{n_{1}}\\), \\(b_{2}\\in\\mathbb{R}^{n_{2}}\\) and \\(n=n_{1}+n_{2}\\). \\(A_{1}\\), \\(b_{1}\\) are the data has copyright issue and \\(A_{2}\\), \\(b_{2}\\) are the data does not have copyright issue. Denote the train objective \\(L\\). Denote \\(\\tau>0\\) a scalar._ _If there is a trained model \\(f_{\\theta}\\) with parameter \\(\\theta\\) that satisfies_ \\[\\frac{L(f_{\\theta}(A_{1}),b_{1})}{n_{1}}\\geq\\tau+\\frac{L(f_{\\theta}(A_{2}),b_{ 2})}{n_{2}}\\] _then we say this model \\(f_{\\theta}\\) is \\(\\tau\\)-Copyright-Protected._ **Definition 71**.: _Let \\(x\\in\\mathbb{R}^{d}\\) be a vector parameter in regression problem. Let \\(L\\) be denoted as Definition 31, let \\(\\ell_{1}(x)\\) and \\(\\ell_{2}(x)\\) be denoted as Definition 29. Denote \\(\\gamma_{c}>0\\) a scalar. Denote \\(\\epsilon_{1},\\epsilon_{2}\\in(0,0.1)\\) two scalars, we define that_ \\[x^{*}:=\\operatorname*{arg\\,min}_{x\\in\\mathbb{R}^{d}}L\\] _and \\(x^{*}\\) satisfies_ * \\[0.5\\ell_{1}(x^{*})+\\gamma_{c}\\ell_{1}(x^{*})^{-1}\\] \\[\\leq \\min_{x\\in\\mathbb{R}^{d}}(0.5\\ell_{1}(x^{*})+\\gamma_{c}\\ell_{1}(x ^{*})^{-1})+\\epsilon_{1}\\]\\[\\ell_{2}(x^{*})\\leq\\epsilon_{2}\\leq\\min_{x\\in\\mathbb{R}^{d}}\\ell_{2}(x)+\\epsilon_{2}\\]"
    },
    {
      "title": "Copyright-Pretected Property For \\(X^{*}\\)",
      "text": "**Lemma 72**.: _If the given conditions are satisfied_ * _Let_ \\(x^{*}\\) _be denoted as Definition_ 71__ * _Let_ \\(\\ell_{1}(x)\\) _be denoted as Definition_ 29__ * _Denote_ \\(\\epsilon_{1},\\epsilon_{2}\\in(0,0.1)\\) _two scalars_ * _Denote_ \\(\\gamma_{c}>0\\) _a scalar_ _we have_ \\[\\sqrt{2\\gamma_{c}}\\leq\\ell_{1}(x^{*})\\leq(\\sqrt{2\\gamma_{c}}+ \\epsilon_{1})+\\sqrt{\\epsilon_{1}^{2}+2\\epsilon_{1}\\sqrt{2\\gamma_{c}}}\\] Proof.: To compute the \\(\\min_{x\\in\\mathbb{R}^{d}}(0.5\\ell_{1}(x)+\\gamma_{c}\\ell_{1}(x)^{-1})\\), we have \\[\\frac{\\mathrm{d}(0.5\\ell_{1}(x)+\\gamma_{c}\\ell_{1}(x)^{-1})}{ \\mathrm{d}\\ell_{1}(x)} = \\frac{\\mathrm{d}0.5\\ell_{1}(x)}{\\mathrm{d}\\ell_{1}(x)}+\\frac{ \\mathrm{d}\\gamma_{c}\\ell_{1}(x)^{-1}}{\\mathrm{d}\\ell_{1}(x)} \\tag{18}\\] \\[= 0.5+\\frac{\\mathrm{d}\\gamma_{c}\\ell_{1}(x)^{-1}}{\\mathrm{d}\\ell_ {1}(x)}\\] \\[= 0.5+\\gamma_{c}\\frac{\\mathrm{d}\\ell_{1}(x)^{-1}}{\\mathrm{d}\\ell_ {1}(x)}\\] \\[= 0.5-\\gamma_{c}\\frac{1}{\\ell_{1}(x)^{-2}}\\] where the first, second, third, fourth equalities follow from simple differential rules. Hence, when \\(\\frac{\\mathrm{d}(0.5\\ell_{1}(x)+\\gamma_{c}\\ell_{1}(x)^{-1})}{\\mathrm{d}\\ell_ {1}(x)}=0\\), applying Eq. (18), we have \\[0.5-\\gamma_{c}\\frac{1}{\\ell_{1}(x)^{-2}}=0 \\tag{19}\\] solving Eq. (19) yields \\[\\ell_{1}(x)=\\sqrt{2\\gamma_{c}}\\] Bring \\(\\ell_{1}(x)\\) into the \\(0.5\\ell_{1}(x)+\\gamma_{c}\\ell_{1}(x)^{-1}\\), and we have \\[\\min_{x\\in\\mathbb{R}^{d}}(0.5\\ell_{1}(x)+\\gamma_{c}\\ell_{1}(x)^{ -1}) = 0.5\\sqrt{2\\gamma_{c}}+\\gamma_{c}\\frac{1}{\\sqrt{2\\gamma_{c}}}\\] \\[= \\sqrt{2\\gamma_{c}}\\] where the first equality follows from \\(\\ell_{1}(x)=\\sqrt{2\\gamma_{c}}\\), the second equality follows from simple algebra. So we have \\[0.5\\ell_{1}(x^{*})+\\gamma_{c}\\ell_{1}(x^{*})^{-1}\\leq\\min_{x\\in \\mathbb{R}^{d}}(0.5\\ell_{1}(x^{*})+\\gamma_{c}\\ell_{1}(x^{*})^{-1})+\\epsilon_{1}\\]\\[=\\sqrt{2\\gamma_{c}}+\\epsilon_{1} \\tag{20}\\] solving Eq. (20) yields \\[\\ell_{1}(x^{*})\\leq(\\sqrt{2\\gamma_{c}}+\\epsilon_{1})+\\sqrt{\\epsilon_{1}^{2}+2 \\epsilon_{1}\\sqrt{2\\gamma_{c}}}\\] then we combine two terms, we have \\[\\sqrt{2\\gamma_{c}}\\leq\\ell_{1}(x^{*})\\leq(\\sqrt{2\\gamma_{c}}+\\epsilon_{1})+ \\sqrt{\\epsilon_{1}^{2}+2\\epsilon_{1}\\sqrt{2\\gamma_{c}}}\\] **Theorem 73** (Formal version of Theorem 16).: _Let \\(x^{*}\\) be denoted the trained parameter on Copyright Regression. Let \\(\\ell(x)\\) be denoted as Definition 4, let \\(\\ell(x)\\) be the original train objective of Softmax Regression. Denote \\(\\epsilon_{2}\\in(00.1)\\) a scalar. Denote \\(\\tau_{c}:=\\sqrt{2\\gamma_{c}}/n_{1}-\\epsilon_{2}/n_{2}\\), we have_ \\[\\frac{\\ell_{1}(x^{*})}{n_{1}}\\geq\\tau_{c}+\\frac{\\ell_{2}(x^{*})}{n_{2}}\\] _so \\(x^{*}\\) in Copyright Regression is \\(\\tau_{c}\\)-Copyright-Protected._ Proof.: We have \\[\\frac{\\ell_{1}(x^{*})}{n_{1}}-\\frac{\\ell_{2}(x^{*})}{n_{2}} \\geq\\frac{\\sqrt{2\\gamma_{c}}}{n_{1}}-\\frac{\\ell_{2}(x)}{n_{2}}\\] \\[\\geq\\frac{\\sqrt{2\\gamma_{c}}}{n_{1}}-\\frac{\\epsilon_{2}}{n_{2}}\\] where the first equality follows from Lemma 72, the second equality follows from Definition 71. We define \\(\\tau_{c}:=\\frac{\\sqrt{2\\gamma_{c}}}{n_{1}}-\\frac{\\epsilon_{2}}{n_{2}}\\), then we have \\[\\frac{\\ell_{1}(x^{*})}{n_{1}}-\\frac{\\ell_{2}(x^{*})}{n_{2}} \\geq\\frac{\\sqrt{2\\gamma_{c}}}{n_{1}}-\\frac{\\epsilon_{2}}{n_{2}}\\] \\[\\geq\\tau_{c}\\]"
    },
    {
      "title": "Appendix J Approximate Newton Method",
      "text": "In this appendix, we present an adapted version of the Newton method for convex optimization. In Appendix J.1, we outline the assumptions underlying the conventional Newton method, along with the precise update rule employed by the traditional algorithm. Additionally, in Appendix J.2, we introduce the approximate update rule for the modified Newton method. We also provide an implementation tool for computing the approximation of \\(\\nabla^{2}L\\), and leverage certain lemmas from [10] to analyze the behavior and performance of the approximate Newton method."
    },
    {
      "title": "Definition And Update Rule",
      "text": "In this section, our primary focus is on examining the local convergence properties of the Newton method. We direct our attention towards solving the optimization problem given by: \\[\\min_{x\\in\\mathbb{R}^{d}}L(x)\\] To facilitate our analysis, we make certain assumptions as follows **Definition 74** (\\((l,m)\\)-good Loss function, Definition 8.1 in [23]).: _Let \\(L:\\mathbb{R}^{d}\\to\\mathbb{R}\\) be denote as Definition 31, we say \\(L\\) is \\((l,m)\\)-good when it satisfies the following conditions,_ * \\(l\\)_-local Minimum._ _We define_ \\(l>0\\) _to be a positive scalar. If there exists a vector_ \\(x^{*}\\in\\mathbb{R}^{d}\\) _satisfies_ * \\(\\nabla L(x^{*})=\\mathbf{0}_{d}\\)__ * \\(\\nabla^{2}L(x^{*})\\succeq l\\cdot\\mathbf{I}_{d}\\)__ * _Hessian is_ \\(M\\)_-Lipschitz._ _If there exists a positive scalar_ \\(M>0\\) _with_ \\[\\|\\nabla^{2}L(x)-\\nabla^{2}L(y)\\|\\leq M\\cdot\\|x-y\\|_{2}\\] * _Good Initialization Point._ _Let_ \\(x_{0}\\) _be denoted as the initialization point. If_ \\(r_{0}:=\\|x_{0}-x^{*}\\|_{2}\\) _satisfies_ \\[r_{0}M\\leq 0.1l\\] We define gradient and Hessian as follows **Definition 75** (Gradient and Hessian).: _. The gradient \\(g:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}\\) of the loss function is defined as_ \\[g(x):=\\nabla L(x)\\] _The Hessian \\(H:\\mathbb{R}^{d}\\to R^{d\\times d}\\) of the loss function is defined as_ \\[H(x):=\\nabla^{2}L(x)\\] With the gradient function \\(g:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}\\) and the Hessian matrix \\(H:\\mathbb{R}^{d}\\to\\mathbb{R}^{d\\times d}\\), we define the exact process of the Newton method as follows: **Definition 76** (Exact update of the Newton method, Definition 8.3 in [23]).: \\[x_{t+1}=x_{t}-H(x_{t})^{-1}\\cdot g(x_{t})\\]"
    },
    {
      "title": "Approximate Of Hessian And Update Rule",
      "text": "**Definition 77** (Approximate Hessian, Definition 8.4 in [23]).: _For any Hessian \\(H(x_{t})\\in\\mathbb{R}^{d\\times d}\\), we define the mated Hessian \\(\\widetilde{H}(x_{t})\\in R^{d\\times d}\\) to be a matrix such that the following holds,_ \\[(1-\\epsilon_{0})\\cdot H(x_{t})\\preceq\\widetilde{H}(x_{t})\\preceq(1+\\epsilon _{0})\\cdot H(x_{t})\\]In order to get the approximated Hessian \\(\\widetilde{H}(x_{t})\\) efficiently, here we state a standard tool (see Lemma 8.5 in [10]). **Lemma 78** (Lemma 8.5 in [10]).: _Let \\(\\epsilon_{0}=0.01\\) be a constant precision parameter. Let \\(A\\in\\mathbb{R}^{n\\times d}\\) be a real matrix, then for any positive diagonal (PD) matrix \\(D\\in\\mathbb{R}^{n\\times n}\\), there exists an algorithm which runs in time_ \\[O((\\mathrm{nnz}(A)+d^{w})\\mathrm{poly}(\\log(n/\\delta)))\\] _and it outputs an \\(O(d\\log(n/\\delta))\\) sparse diagonal matrix \\(\\widetilde{D}\\in\\mathbb{R}^{n\\times n}\\) for which_ \\[(1-\\epsilon_{0})A^{\\top}DA\\preceq A^{\\top}\\widetilde{D}A\\preceq(1+\\epsilon_{0 })A^{\\top}DA\\] Following the standard of Approximate Newton Hessian literature [1, 11, 12, 13], we consider the following. **Definition 79** (Approximate update).: _We consider the following process_ \\[x_{t+1}=x_{t}-\\widetilde{H}(x_{t})^{-1}\\cdot g(x_{t})\\] We state a tool from prior work, **Lemma 80** (Iterative shrinking Lemma, Lemma 6.9 in [10]).: _If the following condition hold_ * _Loss Function_ \\(L\\) _is_ \\((l,M)\\)_-good (see Definition_ 74_)_ * _Let_ \\(\\epsilon_{0}\\in(0,0.1)\\) _(see Definition_ 77_)_ * _Let_ \\(r_{t}:=\\|x_{t}-x^{*}\\|_{2}\\)__ * _Let_ \\(\\overline{r}_{t}:=M\\cdot r_{t}\\)__ _Then we have_ \\[r_{t+1}\\leq 2\\cdot(\\epsilon_{0}+\\overline{r}_{t}/(l-\\overline{r}_{t})) \\cdot r_{t}\\] **Lemma 81** (Upper bound of \\(\\beta\\), Lemma 8.3 in [10]).: _If the given conditions are satisfied_ * \\(\\|A\\|\\leq R\\)__ * \\(\\|x\\|_{2}\\leq R\\)__ * _Let_ \\(\\beta\\) _be the lower bound on_ \\(\\langle\\exp(Ax),\\mathbf{1}_{n}\\rangle\\)__ _then we have_ \\[\\beta\\geq\\exp(-R^{2})\\]"
    },
    {
      "title": "References",
      "text": "* [ADF\\({}^{+}\\)23] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023. * [ADH\\({}^{+}\\)19a] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning_, pages 322-332. PMLR, 2019. * [ADH\\({}^{+}\\)19b] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. _Advances in neural information processing systems_, 32, 2019. * [AG23] Sanjeev Arora and Anirudh Goyal. A theory for emergence of complex skills in language models. _arXiv preprint arXiv:2307.15936_, 2023. * [ALS\\({}^{+}\\)22] Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang, and Danyang Zhuo. Bypass exponential time preprocessing: Fast neural network training via weight-data correlation preprocessing. _arXiv preprint arXiv:2211.14227_, 2022. * [AS23] Josh Alman and Zhao Song. Fast attention requires bounded entries. _arXiv preprint arXiv:2302.13214_, 2023. * [AZLS19a] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International conference on machine learning_, pages 242-252. PMLR, 2019. * [AZLS19b] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks. _Advances in neural information processing systems_, 32, 2019. * [BAR23] BARD. Try bard, an ai experiment by google. _Google_, February 2023. * [BCE\\({}^{+}\\)23] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023. * [BMR\\({}^{+}\\)20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020. * [BNX\\({}^{+}\\)23] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22669-22679, 2023. * [BPSW20] Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (over-parametrized) neural networks in near-linear time. _arXiv preprint arXiv:2006.11648_, 2020. * [BSZ23] Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness for dynamic attention maintenance in large language models. _arXiv preprint arXiv:2304.02207_, 2023. * [CDW\\({}^{+}\\)21] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Re. Scatterbrain: Unifying sparse and low-rank attention. _Advances in Neural Information Processing Systems_, 34:17413-17426, 2021. * [CG19] Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural networks. _Advances in neural information processing systems_, 32, 2019. * [CGH\\({}^{+}\\)19] Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua Zhang, and Liwei Wang. Gram-gauss-newton method: Learning overparameterized neural networks for regression problems. _arXiv preprint arXiv:1905.11675_, 2019. * [Cha22] ChatGPT. Optimizing language models for dialogue. _OpenAI Blog_, November 2022. * [CLP\\({}^{+}\\)20] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose: A learnable lsh framework for efficient neural network training. In _International Conference on Learning Representations_, 2020. * [CND\\({}^{+}\\)22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022. * [CWR\\({}^{+}\\)22] He Cao, Jianan Wang, Tianhe Ren, Xianbiao Qi, Yihao Chen, Yuan Yao, and Lei Zhang. Exploring vision transformers as diffusion learners. _arXiv preprint arXiv:2212.13771_, 2022. * [DBK\\({}^{+}\\)20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020. * [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018. * [DE21] DALL-E. Dall-e: Creating images from text. _OpenAI Research_, January 2021. * [DE22] DALL-E2. Dall-e 2 pre-training mitigations. _OpenAI Research_, June 2022. * [DLMS23] Yichuan Deng, Zhihang Li, Sridhar Mahadevan, and Zhao Song. Zero-th order algorithm for softmax attention optimization. _arXiv preprint arXiv:2307.08352_, 2023. * [DLS23] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression. _arXiv preprint arXiv:2304.10411_, 2023. * [DZPS18] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. _arXiv preprint arXiv:1810.02054_, 2018. * [fttc22] United State Courts for the 9th circuits. Copying--access and substantial similarity. _Model Civil Jury instructions_, December 2022. * [Gil19] Jessica L Gillotte. Copyright infringement in ai-generated artworks. _UC Davis L. Rev._, 53:2655, 2019. * [GMS23] Yeqi Gao, Sridhar Mahadevan, and Zhao Song. An over-parameterized exponential regression. _arXiv preprint arXiv:2303.16504_, 2023. * [GSY23a] Yeqi Gao, Zhao Song, and Xin Yang. Differentially private attention computation. _arXiv preprint arXiv:2305.04701_, 2023. * [GSY23b] Yeqi Gao, Zhao Song, and Junze Yin. Gradientcoin: A peer-to-peer decentralized large language models. _arXiv preprint arXiv:2308.10502_, 2023. * [GSYZ23] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm for attention computation. _arXiv preprint arXiv:2307.08045_, 2023. * [HG15] Ben Hattenbach and Joshua Glucoft. Patents in an era of infinite monkeys and artificial intelligence. _Stan. Tech. L. Rev._, 19:32, 2015. * [HLSY21] Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural tangent kernel-based framework for federated learning analysis. In _International Conference on Machine Learning_, pages 4423-4434. PMLR, 2021. * [Hri16] Kalin Hristov. Artificial intelligence and the copyright dilemma. _Idea_, 57:431, 2016. * [HWC\\({}^{+}\\)22] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey on vision transformer. _IEEE transactions on pattern analysis and machine intelligence_, 45(1):87-110, 2022. * [HXL\\({}^{+}\\)22] Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, and Chenguang Wang. Protecting intellectual property of language generation apis with lexical watermark. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 10758-10766, 2022. * [HXZ\\({}^{+}\\)22] Xuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu, Fangzhao Wu, Jiwei Li, and Ruoxi Jia. Cater: Intellectual property protection on text generation apis via conditional watermarks. _Advances in Neural Information Processing Systems_, 35:5431-5445, 2022. * [IJA\\({}^{+}\\)23] Oana Ignat, Zhijing Jin, Artem Abzaliev, Laura Biester, Santiago Castro, Naihao Deng, Xinyi Gao, Aylin Gunal, Jacky He, Ashkan Kazemi, et al. A phd student's perspective on research in nlp in the era of very large language models. _arXiv preprint arXiv:2305.12544_, 2023. * [JRL23] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. _arXiv preprint arXiv:2306.02561_, 2023. * [JT19] Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks. _arXiv preprint arXiv:1909.12292_, 2019. * [KGW\\({}^{+}\\)23] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. _arXiv preprint arXiv:2301.10226_, 2023. * [KKL20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020. * [LL18] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. _Advances in neural information processing systems_, 31, 2018. * [LLH\\({}^{+}\\)23] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training. _arXiv preprint arXiv:2305.14342_, 2023. * [LSS\\({}^{+}\\)20] Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, et al. Generalized leverage score sampling for neural networks. _Advances in Neural Information Processing Systems_, 33:10775-10787, 2020. * [LSX\\({}^{+}\\)23] Shuai Li, Zhao Song, Yu Xia, Tong Yu, and Tianyi Zhou. The closeness of in-context learning and weight shifting for softmax regression, 2023. * [LSZ23] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh and sinh regression problems. _arXiv preprint arXiv:2303.15725_, 2023. * [LWD\\({}^{+}\\)23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In _International Conference on Machine Learning_, pages 22137-22176. PMLR, 2023. * [MGN\\({}^{+}\\)23] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward passes. _arXiv preprint arXiv:2305.17333_, 2023. * [MOSW22] Alexander Munteanu, Simon Omlor, Zhao Song, and David Woodruff. Bounding the width of neural networks via coupled initialization a worst case analysis. In _International Conference on Machine Learning_, pages 16083-16122. PMLR, 2022. * [NAB\\({}^{+}\\)22] Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. _Advances in Neural Information Processing Systems_, 35:27198-27211, 2022. * [OS20] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. _IEEE Journal on Selected Areas in Information Theory_, 1(1):84-105, 2020. * [PMXA23] Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, and Sanjeev Arora. Trainable transformer in transformer. _arXiv preprint arXiv:2307.01189_, 2023. * [QSY23] Lianke Qin, Zhao Song, and Yuanyuan Yang. Efficient sgd neural network training via sublinear activated neuron identification. _arXiv preprint arXiv:2307.06565_, 2023. * [RGG\\({}^{+}\\)20] Andreas Ruckle, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. Adapterdrop: On the efficiency of adapters in transformers. _arXiv preprint arXiv:2010.11918_, 2020. * [RSM\\({}^{+}\\)23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint arXiv:2305.18290_, 2023. * [Sag18] Matthew Sag. The new legal landscape for text mining and machine learning. _J. Copyright Soc'y USA_, 66:291, 2018. * [SDFS20] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. Intelli-code compose: Code generation using transformer. In _Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering_, pages 1433-1443, 2020. * [SHT23] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers. _arXiv preprint arXiv:2306.02896_, 2023. * [SY19] Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound. _arXiv preprint arXiv:1906.03593_, 2019. * [SZS\\({}^{+}\\)23] Albert Yu Sun, Eliott Zemour, Arushi Saxena, Udith Vaidyanathan, Eric Lin, Christian Lau, and Vaikkunth Mugunthan. Does fine-tuning gpt-3 with the openai api leak personally-identifiable information? _arXiv preprint arXiv:2307.16382_, 2023. * [SZZ21] Zhao Song, Lichen Zhang, and Ruizhe Zhang. Training multi-layer over-parametrized neural network in subquadratic time. _arXiv preprint arXiv:2112.07628_, 2021. * [TBM\\({}^{+}\\)21] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention for transformer models. In _International conference on machine learning_, pages 10183-10192. PMLR, 2021. * [TDA\\({}^{+}\\)20] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. _arXiv preprint arXiv:2011.04006_, 2020. * [TLI\\({}^{+}\\)23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023. * [TMS\\({}^{+}\\)23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023. * [VKB23] Nikhil Vyas, Sham Kakade, and Boaz Barak. Provable copyright protection for generative models. _arXiv preprint arXiv:2302.10870_, 2023. * [VSP\\({}^{+}\\)17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017. * [WFF\\({}^{+}\\)23] Junde Wu, Rao Fu, Huihui Fang, Yu Zhang, and Yanwu Xu. Medsegdiff-v2: Diffusion based medical image segmentation with transformer. _arXiv preprint arXiv:2301.11798_, 2023. * [WSC\\({}^{+}\\)16] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. _arXiv preprint arXiv:1609.08144_, 2016. * [WWS\\({}^{+}\\)22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022. * [WYW\\({}^{+}\\)23] Junda Wu, Tong Yu, Rui Wang, Zhao Song, Ruiyi Zhang, Handong Zhao, Chaochao Lu, Shuai Li, and Ricardo Henao. Infoprompt: Information-theoretic soft prompt tuning for natural language understanding. _arXiv preprint arXiv:2306.04933_, 2023. * [XZA\\({}^{+}\\)23] Zheng Xu, Yanxiang Zhang, Galen Andrew, Christopher A Choquette-Choo, Peter Kairouz, H Brendan McMahan, Jesse Rosenstock, and Yuanbo Zhang. Federated learning of gboard language models with differential privacy. _arXiv preprint arXiv:2305.18465_, 2023. * [ZG19] Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. _Advances in neural information processing systems_, 32, 2019. * [Zha22] Lichen Zhang. _Speeding up optimizations via data structures: Faster search, sample and maintenance_. PhD thesis, Master's thesis, Carnegie Mellon University, 2022. * [ZHL\\({}^{+}\\)23] Eric Zelikman, Qian Huang, Percy Liang, Nick Haber, and Noah D Goodman. Just one byte (per gradient): A note on low-bandwidth decentralized language model finetuning using shared randomness. _arXiv preprint arXiv:2306.10015_, 2023. * [ZKV\\({}^{+}\\)20] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? _Advances in Neural Information Processing Systems_, 33:15383-15393, 2020. * [ZMG19] Guodong Zhang, James Martens, and Roger B Grosse. Fast convergence of natural gradient descent for over-parameterized neural networks. _Advances in Neural Information Processing Systems_, 32, 2019. * [ZPD\\({}^{+}\\)20] Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song, and Sanjeev Arora. Over-parameterized adversarial training: An analysis overcoming the curse of dimensionality. _Advances in Neural Information Processing Systems_, 33:679-688, 2020. * [ZPGA23] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while predicting the masked word? _arXiv preprint arXiv:2303.08117_, 2023. * [ZRG\\({}^{+}\\)22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022."
    }
  ]
}