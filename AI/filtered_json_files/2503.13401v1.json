{
  "title": "LEVELS OF ANALYSIS FOR LARGE LANGUAGE MODELS 1 Using the Tools of Cognitive Science to Understand Large Language Models at Different Levels of Analysis",
  "authors": [
    "Alexander Ku",
    "Declan Campbell",
    "Xuechunzi Bai",
    "Jiayi Geng",
    "Ryan Liu",
    "R Thomas Mccoy",
    "Andrew Nam",
    "Ilia Sucholutsky",
    "Veniamin Veselovsky",
    "Liyi Zhang",
    "Jian-Qiao Zhu",
    "Thomas L Griffiths"
  ],
  "abstract": "\n Modern artificial intelligence systems, such as large language models, are increasingly powerful but also increasingly hard to understand. Recognizing this problem as analogous to the historical difficulties in understanding the human mind, we argue that methods developed in cognitive science can be useful for understanding large language models. We propose a framework for applying these methods based on Marr's three levels of analysis. By revisiting established cognitive science techniques relevant to each level and illustrating their potential to yield insights into the behavior and internal organization of large language models, we aim to provide a toolkit for making sense of these new kinds of minds. \n",
  "references": [
    {
      "id": null,
      "title": "LEVELS OF ANALYSIS FOR LARGE LANGUAGE MODELS 1 Using the Tools of Cognitive Science to Understand Large Language Models at Different Levels of Analysis",
      "authors": [
        "Alexander Ku",
        "Declan Campbell",
        "Xuechunzi Bai",
        "Jiayi Geng",
        "Ryan Liu",
        "R Thomas Mccoy",
        "Andrew Nam",
        "Ilia Sucholutsky",
        "Veniamin Veselovsky",
        "Liyi Zhang",
        "Jian-Qiao Zhu",
        "Thomas L Griffiths"
      ],
      "year": "2025",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F L Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "The adaptive character of thought",
      "authors": [
        "J R Anderson"
      ],
      "year": "1990",
      "venue": "The adaptive character of thought",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Human memory: An adaptive perspective",
      "authors": [
        "J R Anderson",
        "R Milson"
      ],
      "year": "1989",
      "venue": "Psychological Review",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Measuring implicit bias in explicitly unbiased large language models",
      "authors": [
        "X Bai",
        "A Wang",
        "I Sucholutsky",
        "T L Griffiths"
      ],
      "year": "2024",
      "venue": "Measuring implicit bias in explicitly unbiased large language models",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "authors": [
        "Y Bai",
        "A Jones",
        "K Ndousse",
        "A Askell",
        "A Chen",
        "N Dassarma",
        "D Drain",
        "S Fort",
        "D Ganguli",
        "T Henighan"
      ],
      "year": "2022",
      "venue": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Simulation as an engine of physical scene understanding",
      "authors": [
        "P W Battaglia",
        "J B Hamrick",
        "J B Tenenbaum"
      ],
      "year": "2013",
      "venue": "Proceedings of the National Academy of Sciences of the United States of America",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "A neural circuit for male sexual behavior and reward",
      "authors": [
        "D W Bayless",
        "O D Chung-Ha",
        "R Yang",
        "Y Wei",
        "V M De Andrade Carvalho",
        "J R Knoedler",
        "T Yang",
        "O Livingston",
        "A Lomvardas",
        "G J Martins"
      ],
      "year": "2023",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Bayesian theory",
      "authors": [
        "J M Bernardo",
        "A F M Smith"
      ],
      "year": "1994",
      "venue": "Bayesian theory",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Using cognitive psychology to understand gpt-3",
      "authors": [
        "M Binz",
        "E Schulz"
      ],
      "year": "2023",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "The foundation model transparency index",
      "authors": [
        "R Bommasani",
        "K Klyman",
        "S Longpre",
        "S Kapoor",
        "N Maslej",
        "B Xiong",
        "D Zhang",
        "P Liang"
      ],
      "year": "2023",
      "venue": "The foundation model transparency index",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "An introduction to vision-language modeling",
      "authors": [
        "F Bordes",
        "R Y Pang",
        "A Ajay",
        "A C Li",
        "A Bardes",
        "S Petryk",
        "O Mañas",
        "Z Lin",
        "A Mahmoud",
        "B Jayaraman"
      ],
      "year": "2024",
      "venue": "An introduction to vision-language modeling",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "A flexible model of working memory",
      "authors": [
        "F Bouchacourt",
        "T J Buschman"
      ],
      "year": "2019",
      "venue": "Neuron",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Towards monosemanticity: Decomposing language models with dictionary learning",
      "authors": [
        "T Bricken",
        "A Templeton",
        "J Batson",
        "B Chen",
        "A Jermyn",
        "T Conerly",
        "N Turner",
        "C Anil",
        "C Denison",
        "A Askell"
      ],
      "year": "2023",
      "venue": "Towards monosemanticity: Decomposing language models with dictionary learning",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Sparks of artificial general intelligence: Early experiments with GPT-4",
      "authors": [
        "S Bubeck",
        "V Chandrasekaran",
        "R Eldan",
        "J Gehrke",
        "E Horvitz",
        "E Kamar",
        "P Lee",
        "Y T Lee",
        "Y Li",
        "S Lundberg"
      ],
      "year": "2023",
      "venue": "Sparks of artificial general intelligence: Early experiments with GPT-4",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Understanding the limits of vision language models through the lens of the binding problem",
      "authors": [
        "D Campbell",
        "S Rane",
        "T Giallanza",
        "C N De Sabbata",
        "K Ghods",
        "A Joshi",
        "A Ku",
        "S Frankland",
        "T Griffiths",
        "J D Cohen"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Probabilistic models of language processing and acquisition",
      "authors": [
        "N Chater",
        "C D Manning"
      ],
      "year": "2006",
      "venue": "Trends in Cognitive Sciences",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Arc prize 2024",
      "authors": [
        "F Chollet",
        "M Knoop",
        "G Kamradt",
        "B Landers"
      ],
      "year": "2024",
      "venue": "Arc prize 2024",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Neural population dynamics during reaching",
      "authors": [
        "M M Churchland",
        "J P Cunningham",
        "M T Kaufman",
        "J D Foster",
        "P Nuyujukian",
        "S I Ryu",
        "K V Shenoy"
      ],
      "year": "2012",
      "venue": "Neural population dynamics during reaching",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Cogbench: a large language model walks into a psychology lab",
      "authors": [
        "J Coda-Forno",
        "M Binz",
        "J X Wang",
        "E Schulz"
      ],
      "year": "2024",
      "venue": "Cogbench: a large language model walks into a psychology lab",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "A spreading-activation theory of semantic processing",
      "authors": [
        "A M Collins",
        "E F Loftus"
      ],
      "year": "1975",
      "venue": "Psychological review",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Retrieval time from semantic memory",
      "authors": [
        "A M Collins",
        "M R Quillian"
      ],
      "year": "1969",
      "venue": "Journal of verbal learning and verbal behavior",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Mapping model units to visual neurons reveals population code for social behaviour",
      "authors": [
        "B R Cowley",
        "A J Calhoun",
        "N Rangarajan",
        "E Ireland",
        "M H Turner",
        "J W Pillow",
        "M Murthy"
      ],
      "year": "2024",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Neural control of maternal and paternal behaviors",
      "authors": [
        "C Dulac",
        "L A O'connell",
        "Z Wu"
      ],
      "year": "2014",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Dimensions of color vision",
      "authors": [
        "G Ekman"
      ],
      "year": "1954",
      "venue": "The Journal of Psychology",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Not all language model features are linear",
      "authors": [
        "J Engels",
        "E J Michaud",
        "I Liao",
        "W Gurnee",
        "M Tegmark"
      ],
      "year": "2024",
      "venue": "Not all language model features are linear",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Post-encoding verbalization impairs transfer on artificial grammar tasks",
      "authors": [
        "M Fallshore",
        "J W Schooler"
      ],
      "year": "1993",
      "venue": "Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society Erlbaum",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Implicit measures in social cognition research: Their meaning and use",
      "authors": [
        "R H Fazio",
        "M A Olson"
      ],
      "year": "2003",
      "venue": "Annual review of psychology",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "How did you get here from there? Verbal overshadowing of spatial mental models",
      "authors": [
        "S M Fiore",
        "J W Schooler"
      ],
      "year": "2002",
      "venue": "Applied Cognitive Psychology",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Racial stereotypes: Associations and ascriptions of positive and negative characteristics",
      "authors": [
        "S L Gaertner",
        "J P Mclaughlin"
      ],
      "year": "1983",
      "venue": "Social Psychology Quarterly",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Neural manifolds for the control of movement",
      "authors": [
        "J A Gallego",
        "M G Perich",
        "L E Miller",
        "S A Solla"
      ],
      "year": "2017",
      "venue": "Neuron",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Scaling and evaluating sparse autoencoders",
      "authors": [
        "L Gao",
        "T D La Tour",
        "H Tillman",
        "G Goh",
        "R Troll",
        "A Radford",
        "I Sutskever",
        "J Leike",
        "J Wu"
      ],
      "year": "2024",
      "venue": "Scaling and evaluating sparse autoencoders",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Aversive state processing in the posterior insular cortex",
      "authors": [
        "D A Gehrlach",
        "N Dolensek",
        "A S Klein",
        "R Roy Chowdhury",
        "A Matthys",
        "M Junghänel",
        "T N Gaitanos",
        "A Podgornik",
        "T D Black",
        "N Reddy Vaka",
        "K.-K Conzelmann",
        "N Gogolla"
      ],
      "year": "2019",
      "venue": "Nature Neuroscience",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Implicit and explicit memory for new associations in normal and amnesic subjects",
      "authors": [
        "P Graf",
        "D L Schacter"
      ],
      "year": "1985",
      "venue": "Journal of Experimental Psychology: Learning, memory, and cognition",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Implicit social cognition: attitudes, self-esteem, and stereotypes",
      "authors": [
        "A G Greenwald",
        "M R Banaji"
      ],
      "year": "1995",
      "venue": "Psychological review",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Measuring individual differences in implicit cognition: the implicit association test",
      "authors": [
        "A G Greenwald",
        "D E Mcghee",
        "J L Schwartz"
      ],
      "year": "1998",
      "venue": "Journal of personality and social psychology",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Understanding human intelligence through human limitations",
      "authors": [
        "T L Griffiths"
      ],
      "year": "2020",
      "venue": "Trends in Cognitive Sciences",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Bayesian models of cognition: reverse engineering the mind",
      "authors": [
        "T L Griffiths",
        "N Chater",
        "J B Tenenbaum"
      ],
      "year": "2024",
      "venue": "Bayesian models of cognition: reverse engineering the mind",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Topics in semantic association",
      "authors": [
        "T L Griffiths",
        "M Steyvers",
        "J B Tenenbaum"
      ],
      "year": "2007",
      "venue": "Psychological Review",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Optimal predictions in everyday cognition",
      "authors": [
        "T L Griffiths",
        "J B Tenenbaum"
      ],
      "year": "2006",
      "venue": "Psychological Science",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Bayes in the age of intelligent machines",
      "authors": [
        "T L Griffiths",
        "J.-Q Zhu",
        "E Grant",
        "Thomas Mccoy",
        "R"
      ],
      "year": "2024",
      "venue": "Bayes in the age of intelligent machines",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "authors": [
        "D Guo",
        "D Yang",
        "H Zhang",
        "J Song",
        "R Zhang",
        "R Xu",
        "Q Zhu",
        "S Ma",
        "P Wang",
        "X Bi"
      ],
      "year": "2025",
      "venue": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Finding neurons in a haystack: Case studies with sparse probing",
      "authors": [
        "W Gurnee",
        "N Nanda",
        "M Pauly",
        "K Harvey",
        "D Troitskii",
        "D Bertsimas"
      ],
      "year": "2023",
      "venue": "Finding neurons in a haystack: Case studies with sparse probing",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Language models represent space and time",
      "authors": [
        "W Gurnee",
        "M Tegmark"
      ],
      "year": "2023",
      "venue": "Language models represent space and time",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Distributed and overlapping representations of faces and objects in ventral temporal cortex",
      "authors": [
        "J V Haxby",
        "M I Gobbini",
        "M L Furey",
        "A Ishai",
        "J L Schouten",
        "P Pietrini"
      ],
      "year": "2001",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Revealing the multidimensional mental representations of natural objects underlying human similarity judgements",
      "authors": [
        "M N Hebart",
        "C Y Zheng",
        "F Pereira",
        "C I Baker"
      ],
      "year": "2020",
      "venue": "Nature human behaviour",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Measuring massive multitask language understanding",
      "authors": [
        "D Hendrycks",
        "C Burns",
        "S Basart",
        "A Zou",
        "M Mazeika",
        "D Song",
        "J Steinhardt"
      ],
      "year": "2020",
      "venue": "Measuring massive multitask language understanding",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Measuring mathematical problem solving with the math dataset",
      "authors": [
        "D Hendrycks",
        "C Burns",
        "S Kadavath",
        "A Arora",
        "S Basart",
        "E Tang",
        "D Song",
        "J Steinhardt"
      ],
      "year": "2021",
      "venue": "Measuring mathematical problem solving with the math dataset",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "A structural probe for finding syntax in word representations",
      "authors": [
        "J Hewitt",
        "C D Manning"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex",
      "authors": [
        "D H Hubel",
        "T N Wiesel"
      ],
      "year": "1962",
      "venue": "The Journal of physiology",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Prospect theory: An analysis of decision under risk",
      "authors": [
        "D Kahneman",
        "A Tversky"
      ],
      "year": "1979",
      "venue": "Econometrica",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Are sparse autoencoders useful? a case study in sparse probing",
      "authors": [
        "S Kantamneni",
        "J Engels",
        "S Rajamanoharan",
        "M Tegmark",
        "N Nanda"
      ],
      "year": "2025",
      "venue": "Are sparse autoencoders useful? a case study in sparse probing",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "The discrimination of visual number",
      "authors": [
        "E L Kaufman",
        "M W Lord",
        "T W Reese",
        "J Volkmann"
      ],
      "year": "1949",
      "venue": "The American journal of psychology",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Cortical activity in the null space: permitting preparation without movement",
      "authors": [
        "M T Kaufman",
        "M M Churchland",
        "S I Ryu",
        "K V Shenoy"
      ],
      "year": "2014",
      "venue": "Nature neuroscience",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Hidden conflicts: Explanations make inconsistencies harder to detect",
      "authors": [
        "S S Khemlani",
        "P N Johnson-Laird"
      ],
      "year": "2012",
      "venue": "Acta Psychologica",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
      "authors": [
        "B Kim",
        "M Wattenberg",
        "J Gilmer",
        "C Cai",
        "J Wexler",
        "F Viegas"
      ],
      "year": "2018",
      "venue": "International conference on machine learning",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Matching categorical object representations in inferior temporal cortex of man and monkey",
      "authors": [
        "N Kriegeskorte",
        "M Mur",
        "D A Ruff",
        "R Kiani",
        "J Bodurka",
        "H Esteky",
        "K Tanaka",
        "P A Bandettini"
      ],
      "year": "2008",
      "venue": "Neuron",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Relationship between the LEVELS OF ANALYSIS FOR LARGE LANGUAGE MODELS 32 implicit association test and intergroup behavior: A meta-analysis",
      "authors": [
        "B Kurdi",
        "A E Seitchik",
        "J R Axt",
        "T J Carroll",
        "A Karapetyan",
        "N Kaushik",
        "D Tomezsko",
        "A G Greenwald",
        "M R Banaji"
      ],
      "year": "2019",
      "venue": "Relationship between the LEVELS OF ANALYSIS FOR LARGE LANGUAGE MODELS 32 implicit association test and intergroup behavior: A meta-analysis",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Functional identification of an aggression locus in the mouse hypothalamus",
      "authors": [
        "D Lin",
        "M P Boyle",
        "P Dollar",
        "H Lee",
        "E Lein",
        "P Perona",
        "D J Anderson"
      ],
      "year": "2011",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Large language models assume people are more rational than we really are",
      "authors": [
        "R Liu",
        "J Geng",
        "J C Peterson",
        "I Sucholutsky",
        "T L Griffiths"
      ],
      "year": "2024",
      "venue": "Large language models assume people are more rational than we really are",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Mind your step (by step): Chain-of-thought can reduce performance on tasks where thinking makes humans worse",
      "authors": [
        "R Liu",
        "J Geng",
        "A J Wu",
        "I Sucholutsky",
        "T Lombrozo",
        "T L Griffiths"
      ],
      "year": "2024",
      "venue": "Mind your step (by step): Chain-of-thought can reduce performance on tasks where thinking makes humans worse",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "Towards understanding grokking: An effective theory of representation learning",
      "authors": [
        "Z Liu",
        "O Kitouni",
        "N S Nolte",
        "E Michaud",
        "M Tegmark",
        "M Williams"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "The universal law of generalization holds for naturalistic stimuli",
      "authors": [
        "R Marjieh",
        "N Jacoby",
        "J C Peterson",
        "T L Griffiths"
      ],
      "year": "2024",
      "venue": "Journal of Experimental Psychology: General",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Large language models predict human sensory judgments across six modalities",
      "authors": [
        "R Marjieh",
        "I Sucholutsky",
        "P Van Rijn",
        "N Jacoby",
        "T L Griffiths"
      ],
      "year": "2024",
      "venue": "Scientific Reports",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "What is a number, that a large language model may know it? arXiv preprint",
      "authors": [
        "R Marjieh",
        "V Veselovsky",
        "T L Griffiths",
        "I Sucholutsky"
      ],
      "year": "2025",
      "venue": "What is a number, that a large language model may know it? arXiv preprint",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "The geometry of truth: Emergent linear structure in large language model representations of true/false datasets",
      "authors": [
        "S Marks",
        "M Tegmark"
      ],
      "year": "2023",
      "venue": "The geometry of truth: Emergent linear structure in large language model representations of true/false datasets",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "",
      "authors": [
        "D Marr"
      ],
      "year": "1982",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
      "authors": [
        "R T Mccoy",
        "S Yao",
        "D Friedman",
        "M D Hardy",
        "T L Griffiths"
      ],
      "year": "2024",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "The misremembrance of wines past: Verbal and perceptual expertise differentially mediate verbal overshadowing of taste memory",
      "authors": [
        "J M Melcher",
        "J W Schooler"
      ],
      "year": "1996",
      "venue": "Journal of Memory and Language",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "Locating and editing factual associations in gpt",
      "authors": [
        "K Meng",
        "D Bau",
        "A Andonian",
        "Y Belinkov"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "The cognitive revolution: a historical perspective",
      "authors": [
        "G A Miller"
      ],
      "year": "2003",
      "venue": "Trends in cognitive sciences",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "The child's representation of number: A multidimensional scaling analysis",
      "authors": [
        "K Miller",
        "R Gelman"
      ],
      "year": "1983",
      "venue": "Child development",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models",
      "authors": [
        "I Mirzadeh",
        "K Alizadeh",
        "H Shahrokhi",
        "O Tuzel",
        "S Bengio",
        "M Farajtabar"
      ],
      "year": "2024",
      "venue": "Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "Large language models estimate fine-grained human color-concept associations",
      "authors": [
        "K Mukherjee",
        "T T Rogers",
        "K B Schloss"
      ],
      "year": "2024",
      "venue": "Large language models estimate fine-grained human color-concept associations",
      "doi": ""
    },
    {
      "id": "b73",
      "title": "One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity",
      "authors": [
        "S K Murthy",
        "T Ullman",
        "J Hu"
      ],
      "year": "2024",
      "venue": "One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "Rationalizing constraints on the capacity for cognitive control",
      "authors": [
        "S Musslick",
        "J D Cohen"
      ],
      "year": "2021",
      "venue": "Trends in cognitive sciences",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "A circuit mechanism for differentiating positive and negative associations",
      "authors": [
        "P Namburi",
        "A Beyeler",
        "S Yorozu",
        "G G Calhoon",
        "S A Halbert",
        "R Wichmann",
        "S S Holden",
        "K L Mertens",
        "M Anahtar",
        "A C Felix-Ortiz",
        "I R Wickersham",
        "J M Gray",
        "K M Tye"
      ],
      "year": "2015",
      "venue": "A circuit mechanism for differentiating positive and negative associations",
      "doi": ""
    },
    {
      "id": "b76",
      "title": "Beyond mind-reading: multi-voxel pattern analysis of fmri data",
      "authors": [
        "K A Norman",
        "S M Polyn",
        "G J Detre",
        "J V Haxby"
      ],
      "year": "2006",
      "venue": "Beyond mind-reading: multi-voxel pattern analysis of fmri data",
      "doi": ""
    },
    {
      "id": "b77",
      "title": "Show your work: Scratchpads for intermediate computation with language models",
      "authors": [
        "M Nye",
        "A J Andreassen",
        "G Gur-Ari",
        "H Michalewski",
        "J Austin",
        "D Bieber",
        "D Dohan",
        "A Lewkowycz",
        "M Bosma",
        "D Luan"
      ],
      "year": "2021",
      "venue": "Show your work: Scratchpads for intermediate computation with language models",
      "doi": ""
    },
    {
      "id": "b78",
      "title": "Turing representational similarity analysis (rsa): A flexible method for measuring alignment between human and artificial intelligence",
      "authors": [
        "M Ogg",
        "R Bose",
        "J Scharf",
        "C Ratto",
        "M Wolmetz"
      ],
      "year": "2024",
      "venue": "Turing representational similarity analysis (rsa): A flexible method for measuring alignment between human and artificial intelligence",
      "doi": ""
    },
    {
      "id": "b79",
      "title": "-context learning and induction heads",
      "authors": [
        "C Olsson",
        "N Elhage",
        "N Nanda",
        "N Joseph",
        "N Dassarma",
        "T Henighan",
        "B Mann",
        "A Askell",
        "Y Bai",
        "A Chen"
      ],
      "year": "2022",
      "venue": "-context learning and induction heads",
      "doi": ""
    },
    {
      "id": "b80",
      "title": "Learning to reason with LLMs",
      "authors": [
        "Openai"
      ],
      "year": "2024",
      "venue": "Learning to reason with LLMs",
      "doi": ""
    },
    {
      "id": "b81",
      "title": "Parallelograms revisited: Exploring the limitations of vector space models for simple analogies",
      "authors": [
        "J C Peterson",
        "D Chen",
        "T L Griffiths"
      ],
      "year": "2020",
      "venue": "Cognition",
      "doi": ""
    },
    {
      "id": "b82",
      "title": "A rational analysis of the approximate number system",
      "authors": [
        "S T Piantadosi"
      ],
      "year": "2016",
      "venue": "Psychonomic bulletin & review",
      "doi": ""
    },
    {
      "id": "b83",
      "title": "Exact number concepts are limited to the verbal count range",
      "authors": [
        "B Pitt",
        "E Gibson",
        "S T Piantadosi"
      ],
      "year": "2022",
      "venue": "Psychological Science",
      "doi": ""
    },
    {
      "id": "b84",
      "title": "Chronometric analysis of classification",
      "authors": [
        "M I Posner",
        "R F Mitchell"
      ],
      "year": "1967",
      "venue": "Psychological review",
      "doi": ""
    },
    {
      "id": "b85",
      "title": "Invariant visual representation by single neurons in the human brain",
      "authors": [
        "R Q Quiroga",
        "L Reddy",
        "G Kreiman",
        "C Koch",
        "I Fried"
      ],
      "year": "2005",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b86",
      "title": "Creating a False Memory in the Hippocampus",
      "authors": [
        "S Ramirez",
        "X Liu",
        "P.-A Lin",
        "J Suh",
        "M Pignatelli",
        "R L Redondo",
        "T J Ryan",
        "S Tonegawa"
      ],
      "year": "2013",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b87",
      "title": "Competition between engrams influences fear memory formation and recall",
      "authors": [
        "A J Rashid",
        "C Yan",
        "V Mercaldo",
        "H.-L Hsiang",
        "S Park",
        "C J Cole",
        "A De Cristofaro",
        "J Yu",
        "C Ramakrishnan",
        "S Y Lee"
      ],
      "year": "2016",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b88",
      "title": "Implicit learning of synthetic languages: The role of instructional set",
      "authors": [
        "A S Reber"
      ],
      "year": "1976",
      "venue": "Journal of Experimental Psychology: Human Learning and Memory",
      "doi": ""
    },
    {
      "id": "b89",
      "title": "Bidirectional switch of the valence associated with a hippocampal contextual memory engram",
      "authors": [
        "R L Redondo",
        "J Kim",
        "A L Arons",
        "S Ramirez",
        "X Liu",
        "S Tonegawa"
      ],
      "year": "2014",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b90",
      "title": "Gpqa: A graduate-level google-proof q&a benchmark",
      "authors": [
        "D Rein",
        "B L Hou",
        "A C Stickland",
        "J Petty",
        "R Y Pang",
        "J Dirani",
        "J Michael",
        "S R Bowman"
      ],
      "year": "2024",
      "venue": "First Conference on Language Modeling",
      "doi": ""
    },
    {
      "id": "b91",
      "title": "Cognitive representations of semantic categories",
      "authors": [
        "E Rosch"
      ],
      "year": "1975",
      "venue": "Journal of experimental psychology: General",
      "doi": ""
    },
    {
      "id": "b92",
      "title": "Rational approximations to rational models: Alternative algorithms for category learning",
      "authors": [
        "A N Sanborn",
        "T L Griffiths",
        "D J Navarro"
      ],
      "year": "2010",
      "venue": "Psychological Review",
      "doi": ""
    },
    {
      "id": "b93",
      "title": "Reconciling intuitive physics and Newtonian mechanics for colliding objects",
      "authors": [
        "A N Sanborn",
        "V K Mansinghka",
        "T L Griffiths"
      ],
      "year": "2013",
      "venue": "Psychological Review",
      "doi": ""
    },
    {
      "id": "b94",
      "title": "Foundations of statistics",
      "authors": [
        "L J Savage"
      ],
      "year": "1954",
      "venue": "Foundations of statistics",
      "doi": ""
    },
    {
      "id": "b95",
      "title": "Implicit memory: History and current status",
      "authors": [
        "D L Schacter"
      ],
      "year": "1987",
      "venue": "Journal of experimental psychology: learning, memory, and cognition",
      "doi": ""
    },
    {
      "id": "b96",
      "title": "Verbal overshadowing of visual memories: Some things are better left unsaid",
      "authors": [
        "J W Schooler",
        "T Y Engstler-Schooler"
      ],
      "year": "1990",
      "venue": "Cognitive Psychology",
      "doi": ""
    },
    {
      "id": "b97",
      "title": "The analysis of proximities: Multidimensional scaling with an unknown distance function: I",
      "authors": [
        "R N Shepard"
      ],
      "year": "1962",
      "venue": "Psychometrika",
      "doi": ""
    },
    {
      "id": "b98",
      "title": "Multidimensional scaling, tree-fitting, and clustering",
      "authors": [
        "R N Shepard"
      ],
      "year": "1980",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b99",
      "title": "Geometrical approximations to the structure of musical pitch",
      "authors": [
        "R N Shepard"
      ],
      "year": "1982",
      "venue": "Psychological review",
      "doi": ""
    },
    {
      "id": "b100",
      "title": "Towards a universal law of generalization for psychological science",
      "authors": [
        "R N Shepard"
      ],
      "year": "1987",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b101",
      "title": "Language models are multilingual chain-of-thought reasoners",
      "authors": [
        "F Shi",
        "M Suzgun",
        "M Freitag",
        "X Wang",
        "S Srivats",
        "S Vosoughi",
        "H W Chung",
        "Y Tay",
        "S Ruder",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Language models are multilingual chain-of-thought reasoners",
      "doi": ""
    },
    {
      "id": "b102",
      "title": "Getting aligned on representational alignment",
      "authors": [
        "I Sucholutsky",
        "L Muttenthaler",
        "A Weller",
        "A Peng",
        "A Bobu",
        "B Kim",
        "B C Love",
        "E Grant",
        "I Groen",
        "J Achterberg"
      ],
      "year": "2023",
      "venue": "Getting aligned on representational alignment",
      "doi": ""
    },
    {
      "id": "b103",
      "title": "Conceptual structure coheres in human cognition but not in large language models",
      "authors": [
        "S Suresh",
        "K Mukherjee",
        "X Yu",
        "W.-C Huang",
        "L Padua",
        "T T Rogers"
      ],
      "year": "2023",
      "venue": "Conceptual structure coheres in human cognition but not in large language models",
      "doi": ""
    },
    {
      "id": "b104",
      "title": "Gemini: a family of highly capable multimodal models",
      "authors": [
        "G Team",
        "R Anil",
        "S Borgeaud",
        "J.-B Alayrac",
        "J Yu",
        "R Soricut",
        "J Schalkwyk",
        "A M Dai",
        "A Hauth",
        "K Millican"
      ],
      "year": "2023",
      "venue": "Gemini: a family of highly capable multimodal models",
      "doi": ""
    },
    {
      "id": "b105",
      "title": "",
      "authors": [
        "A Templeton",
        "T Conerly",
        "J Marcus",
        "J Lindsey",
        "T Bricken",
        "B Chen",
        "A Pearce",
        "C Citro",
        "E Ameisen",
        "A Jones",
        "H Cunningham",
        "N L Turner",
        "C Mcdougall",
        "M Macdiarmid",
        "C D Freeman",
        "T R Sumers",
        "E Rees",
        "J Batson",
        "A Jermyn",
        "S Carter",
        "C Olah",
        "T Henighan"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b106",
      "title": "Rules and similarity in concept learning",
      "authors": [
        "J Tenenbaum"
      ],
      "year": "1999",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b107",
      "title": "Bert rediscovers the classical nlp pipeline",
      "authors": [
        "I Tenney",
        "D Das",
        "E Pavlick"
      ],
      "year": "2019",
      "venue": "Bert rediscovers the classical nlp pipeline",
      "doi": ""
    },
    {
      "id": "b108",
      "title": "Function vectors in large language models",
      "authors": [
        "E Todd",
        "M L Li",
        "A S Sharma",
        "A Mueller",
        "B C Wallace",
        "D Bau"
      ],
      "year": "2023",
      "venue": "Function vectors in large language models",
      "doi": ""
    },
    {
      "id": "b109",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b110",
      "title": "Serial vs. parallel processing: Sometimes they look like tweedledum and tweedledee but they can (and should) be distinguished",
      "authors": [
        "J T Townsend"
      ],
      "year": "1990",
      "venue": "Psychological science",
      "doi": ""
    },
    {
      "id": "b111",
      "title": "Illusory conjunctions in the perception of objects",
      "authors": [
        "A Treisman",
        "H Schmidt"
      ],
      "year": "1982",
      "venue": "Cognitive psychology",
      "doi": ""
    },
    {
      "id": "b112",
      "title": "A feature-integration theory of attention",
      "authors": [
        "A M Treisman",
        "G Gelade"
      ],
      "year": "1980",
      "venue": "Cognitive psychology",
      "doi": ""
    },
    {
      "id": "b113",
      "title": "Why are small and large numbers enumerated differently? a limited-capacity preattentive stage in vision",
      "authors": [
        "L M Trick",
        "Z W Pylyshyn"
      ],
      "year": "1994",
      "venue": "Psychological review",
      "doi": ""
    },
    {
      "id": "b114",
      "title": "Features of similarity",
      "authors": [
        "A Tversky"
      ],
      "year": "1977",
      "venue": "Psychological Review",
      "doi": ""
    },
    {
      "id": "b115",
      "title": "Nearest neighbor analysis of psychological spaces",
      "authors": [
        "A Tversky",
        "J W Hutchinson"
      ],
      "year": "1986",
      "venue": "Psychological Review",
      "doi": ""
    },
    {
      "id": "b116",
      "title": "Judgment under uncertainty: heuristics and biases",
      "authors": [
        "A Tversky",
        "D Kahneman"
      ],
      "year": "1974",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b117",
      "title": "Memory formation in the absence of experience",
      "authors": [
        "G Vetere",
        "L M Tran",
        "S Moberg",
        "P E Steadman",
        "L Restivo",
        "F G Morrison",
        "K J Ressler",
        "S A Josselyn",
        "P W Frankland"
      ],
      "year": "2019",
      "venue": "Nature Neuroscience",
      "doi": ""
    },
    {
      "id": "b118",
      "title": "Theory of games and economic behavior",
      "authors": [
        "J Von Neumann",
        "O Morgenstern"
      ],
      "year": "1947",
      "venue": "Theory of games and economic behavior",
      "doi": ""
    },
    {
      "id": "b119",
      "title": "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small",
      "authors": [
        "K Wang",
        "A Variengien",
        "A Conmy",
        "B Shlegeris",
        "J Steinhardt"
      ],
      "year": "2022",
      "venue": "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small",
      "doi": ""
    },
    {
      "id": "b120",
      "title": "Large language models are implicitly latent variable models: Explaining and finding good demonstrations for in-context learning",
      "authors": [
        "X Wang",
        "W Zhu",
        "M Saxon",
        "M Steyvers",
        "W Y Wang"
      ],
      "year": "2024",
      "venue": "Large language models are implicitly latent variable models: Explaining and finding good demonstrations for in-context learning",
      "doi": ""
    },
    {
      "id": "b121",
      "title": "Mmlu-pro: A more robust and challenging multi-task language understanding benchmark",
      "authors": [
        "Y Wang",
        "X Ma",
        "G Zhang",
        "Y Ni",
        "A Chandra",
        "S Guo",
        "W Ren",
        "A Arulraj",
        "X He",
        "Z Jiang"
      ],
      "year": "2024",
      "venue": "The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track",
      "doi": ""
    },
    {
      "id": "b122",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q V Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b123",
      "title": "The hazards of explanation: Overgeneralization in the face of exceptions",
      "authors": [
        "J J Williams",
        "T Lombrozo",
        "B Rehder"
      ],
      "year": "2013",
      "venue": "Journal of Experimental Psychology: General",
      "doi": ""
    },
    {
      "id": "b124",
      "title": "Behavioural and dopaminergic signatures of resilience",
      "authors": [
        "L Willmore",
        "C Cameron",
        "J Yang",
        "I B Witten",
        "A L Falkner"
      ],
      "year": "2022",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b125",
      "title": "An explanation of in-context learning as implicit Bayesian inference",
      "authors": [
        "S M Xie",
        "A Raghunathan",
        "P Liang",
        "T Ma"
      ],
      "year": "2021",
      "venue": "An explanation of in-context learning as implicit Bayesian inference",
      "doi": ""
    },
    {
      "id": "b126",
      "title": "Emergent symbolic mechanisms support abstract reasoning in large language models",
      "authors": [
        "Y Yang",
        "D Campbell",
        "K Huang",
        "M Wang",
        "J Cohen",
        "T Webb"
      ],
      "year": "2025",
      "venue": "Emergent symbolic mechanisms support abstract reasoning in large language models",
      "doi": ""
    },
    {
      "id": "b127",
      "title": "Aime-preview: A rigorous and immediate evaluation framework for advanced mathematical reasoning",
      "authors": [
        "Y Ye",
        "Y Xiao",
        "T Mi",
        "P Liu"
      ],
      "year": "2025",
      "venue": "Aime-preview: A rigorous and immediate evaluation framework for advanced mathematical reasoning",
      "doi": ""
    },
    {
      "id": "b128",
      "title": "Vision as Bayesian inference: analysis by synthesis?",
      "authors": [
        "A Yuille",
        "D Kersten"
      ],
      "year": "2006",
      "venue": "Trends in Cognitive Sciences",
      "doi": ""
    },
    {
      "id": "b129",
      "title": "What should embeddings embed? autoregressive models represent latent generating distributions",
      "authors": [
        "L Zhang",
        "M Y Li",
        "T L Griffiths"
      ],
      "year": "2024",
      "venue": "What should embeddings embed? autoregressive models represent latent generating distributions",
      "doi": ""
    },
    {
      "id": "b130",
      "title": "Deep de finetti: Recovering topic distributions from large language models",
      "authors": [
        "L Zhang",
        "R T Mccoy",
        "T R Sumers",
        "J.-Q Zhu",
        "T L Griffiths"
      ],
      "year": "2023",
      "venue": "Deep de finetti: Recovering topic distributions from large language models",
      "doi": ""
    },
    {
      "id": "b131",
      "title": "Revisiting topic-guided language models",
      "authors": [
        "C Zheng",
        "K Vafa",
        "D M Blei"
      ],
      "year": "2023",
      "venue": "Revisiting topic-guided language models",
      "doi": ""
    },
    {
      "id": "b132",
      "title": "Sniff ai: Is my'spicy'your'spicy'? exploring llm's perceptual alignment with human smell experiences",
      "authors": [
        "S Zhong",
        "Z Zhou",
        "C Dawes",
        "G Brianz",
        "M Obrist"
      ],
      "year": "2024",
      "venue": "Sniff ai: Is my'spicy'your'spicy'? exploring llm's perceptual alignment with human smell experiences",
      "doi": ""
    },
    {
      "id": "b133",
      "title": "Incoherent probability judgments in large language models",
      "authors": [
        "J.-Q Zhu",
        "T Griffiths"
      ],
      "year": "2024",
      "venue": "Proceedings of the Annual Meeting of the Cognitive Science Society",
      "doi": ""
    },
    {
      "id": "b134",
      "title": "Eliciting the priors of large language models using iterated in-context learning",
      "authors": [
        "J.-Q Zhu",
        "T L Griffiths"
      ],
      "year": "2024",
      "venue": "Eliciting the priors of large language models using iterated in-context learning",
      "doi": ""
    },
    {
      "id": "b135",
      "title": "The bayesian sampler: Generic bayesian inference causes incoherence in human probability judgments",
      "authors": [
        "J.-Q Zhu",
        "A N Sanborn",
        "N Chater"
      ],
      "year": "2020",
      "venue": "Psychological review",
      "doi": ""
    },
    {
      "id": "b136",
      "title": "Representation engineering: A top-down approach to ai transparency",
      "authors": [
        "A Zou",
        "L Phan",
        "S Chen",
        "J Campbell",
        "P Guo",
        "R Ren",
        "A Pan",
        "X Yin",
        "M Mazeika",
        "A.-K Dombrowski"
      ],
      "year": "2023",
      "venue": "Representation engineering: A top-down approach to ai transparency",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Abstract",
      "text": "Modern artificial intelligence systems, such as large language models, are increasingly powerful but also increasingly hard to understand. Recognizing this problem as analogous to the historical difficulties in understanding the human mind, we argue that methods developed in cognitive science can be useful for understanding large language models. We propose a framework for applying these methods based on Marr's three levels of analysis. By revisiting established cognitive science techniques relevant to each level and illustrating their potential to yield insights into the behavior and internal organization of large language models, we aim to provide a toolkit for making sense of these new kinds of minds. _Keywords:_ large language models, levels of analysis [MISSING_PAGE_EMPTY:3] - techniques such as rational analysis, the axiomatic approach, and multidimensional scaling - and illustrate how they translate into techniques for gaining insight into AI systems. We illustrate the potential of this approach by using examples from our own work and by drawing analogies between nascent methods in computer science and tools developed in other fields. As an organizing principle, we divide these tools up by the different kinds of questions to which they provide answers. The computational neuroscientist David Marr (Marr, 1982) suggested that information processing systems can be understood at three levels of analysis: the computational level, which focuses on the abstract computational problem a system solves and its ideal solution; the algorithmic level, which focuses on the representations and algorithms that approximate that solution; and the implementation level, which focuses on how those representations and algorithms are realized in a physical system. The same three levels can be used for analyzing large language models, focusing on how such systems are shaped by their function, the solutions that they seem to find, and the realization of those solutions in weights and units within the underlying artificial neural network (see Table 1). Drawing these parallels also provides a guide for where we might expect to look for relevant tools, with the computational level focusing on computational modeling techniques, the algorithmic level drawing on methods from cognitive psychology, and the implementation level being inspired by neuroscience. As a result, we see thinking at these different levels of analysis as being particularly productive for making sense of large language models. **Computational level: Considering function** The computational level focuses on the abstract problem that a system needs to solve. By recognizing the pressures that this problem exerts upon the system, we can make predictions about the properties that the system is likely to have (Shepard, 1987; Marr, 1982; Anderson, 1990; Griffiths, 2020). This perspective is perhaps most familiar from evolutionary biology, in which organisms are understood through the lens of the evolutionary pressures that have shaped them. For instance, our understanding of bird flight is informed by the aerodynamic principles that bird flight must obey. Similarly, we can gain insight into intelligent systems by considering how they have been shaped by the functions that they must perform. This emphasis on function makes the computational level well-suited for analyzing AI systems. Although many aspects of modern AI systems are difficult to interpret (including their behavior and the mechanisms that they use to achieve that behavior), one aspect that we understand well is the function that the system is optimized to perform. Specifically, this function is explicitly defined by humans in the form of the AI system's training objective. Therefore, computational-level analysis enables us to start from something we understand well - the training objective - and use it to reason about the less-well-understood territory of the behavior of the resulting systems. **The embers of autoregression** McCoy et al. (McCoy et al., 2024) used an approach based on computational-level analysis to try to understand the behavior of large language models (LLMs). For these systems, the primary training objective is next-token prediction, also known as autoregression: predicting the next token (word or part of a word) in a piece of text given the preceding tokens. Analysis of this task leads to the prediction that LLMs will perform better when they need to produce a high-probability piece of text than when they need to produce a low-probability piece of text, even in deterministic settings where probability should not matter. Intuitively, this prediction follows from the way in which next-token prediction fundamentally depends on the probabilities of token sequences; this intuition is derived more formally in (McCoy et al., 2024) via a Bayesian analysis of autoregression. The prediction that LLM behavior will be sensitive to probability is borne out in experiments testing a range of LLMs across a range of tasks (see Figure 1). For instance,when GPT-4 is asked to count how many letters there are in a list, it performs much better when the answer is a frequently-used number than a more rarely-used number; e.g., when the answer is 30, its accuracy is 97%, but when the answer is 29, its accuracy is only 17%, presumably because the number 30 is used more often in natural text than the number 29. Similarly, when asked to decode a message written in a simple cipher, GPT-4's accuracy was 51% when the answer was a high-probability sentence but only 13% when the answer was a low-probability word sequence. Thus, even though LLMs can be applied to many different tasks - a capability that has been viewed as evidence that LLMs show \"sparks of artificial general intelligence\" (Bubeck et al., 2023) - they also continue to display \"embers of autoregression\" - behavioral trends that reflect the nature of the specific objective they were optimized for. **Bayesian optimality as a benchmark** The \"embers of autoregression\" example illustrates a broader principle: many cognitive tasks can be characterized as inductive inference problems under uncertainty (Griffiths et al., 2024a). Bayesian models of cognition provide optimal solutions to these problems, and have become instrumental in explaining human performance across diverse domains, including perception (Yuille and Kersten, 2006), language processing (Chater and Manning, 2006; Griffiths et al., 2007), categorization (Sanborn et al., 2010), and intuitive physics (Sanborn et al., 2013; Battaglia et al., 2013). We can compare LLMs to these optimal solutions to gain insight into their behavior, much as cognitive scientists use Bayesian models to understand human cognition. The connection between Bayesian optimality and LLMs can be made more explicit by considering the problem of next-token prediction that is typically used in training these models. Predicting the next token can be done by extracting the predictive sufficient statistics from previous tokens (Bernardo and Smith, 1994). For some datasets, the Bayesian posterior distribution over particular parameters or hypotheses about the generating process can serve as a predictive sufficient statistic (Zhang et al., 2024). Thisperspective can be used to understand the representations that LLMs form and how they should relate to ideal Bayesian solutions (Zhang et al., 2023, 2024). Several other recent papers have also identified interesting connections between LLMs and Bayesian inference (Xie et al., 2021; Wang et al., 2024; Zheng et al., 2023). These connections suggest that we might be able to create simple Bayesian models of the inferences drawn by LLMs. Such Bayesian models can be used to explore the implicit prior distributions adopted by LLMs and to compare the resulting distributions with those inferred from human behavior. For example, Griffiths et al. (Griffiths et al., 2024) used a simple Bayesian model of predicting the future (Griffiths and Tenenbaum, 2006) to recover implicit prior distributions about the extent or duration of phenomena from GPT-4. Zhu and Griffiths (Zhu and Griffiths, 2024) built on this work, using an iterated learning procedure originally developed for sampling from human priors to explore the priors that LLMs have for causal relationships and probability distributions, as well as their implicit assumptions about speculative events such as the development of superhuman AI. **Violations of axiomatic systems** Another approach to understanding intelligent systems at the computational level is to consider how their behavior relates to an axiomatic system that captures the solution to a problem. One of the original examples used by Marr had this flavor: he suggested that we can understand the computational problem solved by a cash register by recognizing that the expectations we have about shopping, such as the fact that the order in which items are checked out doesn't affect the total price, correspond to the axiomatic system of arithmetic. In cognitive science, the most celebrated application of this approach has been decision theory. By considering how to define rationality, decision theorists were able to specify a set of axioms that result in the discovery that rational agents should seek to maximize expected utility (Von Neumann and Morgenstern, 1947; Savage, 1954). Asking whether this axiomatic system actually describes human behavior resulted in fundamental insights into human decision-making, with Kahneman and Tversky carrying out an influential research program that showed that people systemically violate the prescriptions of these axioms (Tversky and Kahneman, 1974; Kahneman and Tversky, 1979). Considering relevant axiomatic systems - and discovering how they are violated - provides another tool for understanding LLMs. For example, probability theory dictates that the probabilities of an event \\(A\\) and its complement \\(\\neg A\\) sum to 1, meaning \\(P(A)+P(\\neg A)=1\\). To assess whether LLMs adhere to this rule, we can examine deviations from zero in \\(P(A)+P(\\neg A)-1\\). Similarly, other probabilistic identities can be tested, such as \\(P(A)+P(B)-P(A\\wedge B)-P(A\\lor B)\\), which should also equal zero if judgments are coherent (Zhu et al., 2020). Eliciting probability judgments for logically related events from GPT and LLaMa models (see Figure 2), shows that probability identities formed using judgments generated by LLMs systematically deviate from zero,violating the rules of probability theory (Zhu and Griffiths, 2024a). In addition to maintaining coherence across logically related events, a rational agent should produce consistent probability judgments when repeatedly queried about the same event. However, repeated probability judgments elicited from LLMs exhibit an inverted-U-shaped mean-variance relationship (Zhu and Griffiths, 2024a). These systematic deviations also qualitatively mirror those observed in human probability judgments, suggesting that LLMs exhibit similar biases in probabilistic reasoning. Optimal behavior can also be defined with respect to problems or situations with inherent uncertainty. For example, in a risky choice task where participants select one of many gambles (each gamble corresponding to a probabilistic distribution of outcomes), there is always a rational choice that maximizes the expected value (making the simplest possible assumption about the utility associated with an option by equating its utility with its monetary value). Here, using chain-of-thought reasoning (Wei et al., 2022) make choices almost completely rationally, but without such reasoning, their choices are noisy and sometimes ignore probabilities completely (Liu et al., 2024a). Furthermore, when LLMs are asked to predict human performance on the task, they predict humans to behave highly rationally, even though people behave much less so. **Summary** The computational level of analysis provides a powerful lens for understanding large language models by focusing on their function. Examining the training objective (in this case, autoregression) can directly predict specific behavioral patterns, as illustrated by the \"embers of autoregression.\" Furthermore, comparing LLMs to optimal benchmarks, whether derived from Bayesian models of cognition or the axioms of probability and decision theory, can reveal both the surprising capabilities and the systematic limitations of these systems. By considering what computational problem LLMs are solving (or approximating), we can gain significant insight into their behavior and internal structure, even when the algorithmic and implementation details remain opaque. **Algorithmic level: Identifying representations and processes** Just as the computational level asks what problem an information-processing system solves, the algorithmic level explores how that solution is approximated. This level concerns itself with the specific representations and algorithms used to carry out the computation. Consider bird flight again: while aerodynamics dictates the principles of flight (lift, drag, thrust), different bird species employ different algorithms - variations in flapping patterns and soaring techniques - to achieve flight. These variations represent different algorithmic solutions to the same computational problem. In cognitive science, understanding the algorithmic level involves designing experiments that probe the internal workings of the mind, inferring the nature of mental representations and the processes that operate on them. Just as ornithologists might study wing movements and muscle activity to understand bird flight, cognitive psychologists use reaction times, error patterns, and carefully designed stimuli to understand human cognition. We can apply similar techniques to investigate the algorithmic solutions of large language models. The algorithmic level is particularly relevant to LLMs because, unlike traditional symbolic AI systems with explicitly programmed rules, the specific algorithms and representations employed by LLMs are not pre-defined. They are learned through the training process, resulting in complex and often opaque internal structures that aren't obviously localized in any particular location in the network. Proprietary closed-source networks present additional challenges as their internal states are not directly observable. This problem is much the same as that faced by psychologists before the advent of modern neuroscientific tools and methodologies. Cognitive psychology offers a rich toolbox of methods for exploring the algorithmic level, many of which can be creatively adapted to study LLMs. This section explores how cognitive science-inspired approaches - such as analyzing systematic error patterns, soliciting similarity judgments, and exploring associations - can be used to uncover the algorithms and representations of LLMs."
    },
    {
      "title": "Parallel And Serial Processing",
      "text": "A fundamental challenge for any cognitive system, whether biological or artificial, is the tradeoff between processing information serially (one item at a time) or in parallel (multiple items simultaneously) (Treisman and Gelade, 1980; Townsend, 1990). Parallel processing offers efficiency, allowing for rapid processing of multiple inputs. However, it also introduces the potential for representational interference, especially when dealing with compositional representations, where features are shared and recombined across different items. Think of trying to remember a set of colored shapes: if the colors and shapes are reused across multiple objects, it becomes harder to keep track of which color goes with which shape when processing them all at once. Serial processing, while slower, mitigates this interference by focusing attention on a single item at a time. Critically, the interference caused by parallel processing of compositional representations follows a predictable pattern: items that share more features will interfere with each other more than dissimilar items (Musslick and Cohen, 2021; Bouchacourt and Buschman, 2019). This relationship between feature similarity and performance degradation provides a diagnostic tool. By observing systematic errors - such as decreased accuracy or the formation of \"illusory conjunctions\" (e.g., misremembering a red square and a blue circle as a red circle) - we can infer that the system likely relies on compositional representations and parallel processing. This approach allows us to indirectly examine the structure of a system's representations by analyzing its behavioral limitations. Recent work with vision-language models (VLMs) provides compelling evidence for this approach. VLMs are typically built on top of an LLM backbone, adding a system for encoding visual images and additional training on tasks that involve both language and images (Bordes et al., 2024). The resulting models, like humans, show highly accurate \"pop-out\" search for distinctive visual targets but exhibit degraded performance in conjunction search (searching for a target defined by a combination of features) as the number of distractors increases (Campbell et al., 2024). This pattern suggests interferencearising from the simultaneous processing of multiple items. Similarly, VLMs exhibit a \"subitizing limit\" in numerical estimation (see Figure 3), akin to that observed in humans under conditions that force rapid, parallel visual processing (Kaufman et al., 1949; Trick and Pylyshyn, 1994). Furthermore, when tasked with describing the features of multiple objects in a scene, VLMs make systematic errors resembling illusory conjunctions observed in human visual working memory tasks (Treisman and Schmidt, 1982), with error rates predicted by the potential for feature interference (Campbell et al., 2024). These parallels suggest that both humans and VLMs rely on compositional representations and are susceptible to similar forms of interference during parallel processing. Figure 3: Patterns of behavior consistent with parallel processing in vision-language models (VLMs). (**a**) VLMs show highly accurate “pop-out” search for distinctive visual targets but exhibit degraded performance in conjunction search as the number of distractors increases. (**b**) VLMs exhibit a “subitizing limit” in numerical estimation."
    },
    {
      "title": "Similarity Judgments",
      "text": "Building on the principle that behavioral limitations can reveal representational structure, we now turn to specific methods for probing these representations in LLMs. One powerful approach, adapted from cognitive psychology, is the use of similarity judgments. Humans rely on efficient representations to navigate high-dimensional environments and to support different cognitive capacities (Anderson, 1990). Characterizing the structure of those representations has been central to decades of psychological research spanning a wide array of contexts, including sensory domains such as color (Shepard, 1980; Ekman, 1954), pitch (Shepard, 1982), and natural images (Hebart et al., 2020; Marjieh et al., 2024a), linguistic domains such as the semantic organization of concepts (Rosch, 1975; Tversky and Hutchinson, 1986) and lexical analogies (Peterson et al., 2020), and numerical domains such as the relations between integers and their mathematical properties (Miller and Gelman, 1983; Pitt et al., 2022; Piantadosi, 2016; Tenenbaum, 1999). Similarity judgments can be used to reveal representations that explain human behavior, as illustrated by the work of (Shepard, 1980, 1987) and (Tversky, 1977). The idea here is that by observing how humans perceive \"similarity\" between stimuli that are sampled from a certain domain (a notion that is ambiguous by design) we can characterize how they represent and organize that domain. More specifically, given a domain of interest (e.g., colors) the paradigm proceeds by eliciting similarity judgments between pairs of stimuli from that domain (\"how similar are the two colors?\") and aggregating those judgments into similarity matrices that capture the relations between stimuli (e.g., the color wheel). By applying spatial embedding techniques such as multi-dimensional scaling (MDS) analysis (Shepard, 1962, 1980) to such matrices or computing different diagnostic measures from them (Tversky and Hutchinson, 1986) it is then possible to derive strong constraints on the underlying representation. Methods for identifying representations based on similarity judgments can be used just as easily with large language models. Just as we can elicit similarity judgments from a [MISSING_PAGE_FAIL:15] of work leverages this insight to characterize LLM representations across different domains such as olfaction (Zhong et al., 2024) and numbers (Marjieh et al., 2025), as well as to study conceptual diversity in LLM representations (Murthy et al., 2024) and LLM-human representational alignment (Mukherjee et al., 2024; Suresh et al., 2023; Ogg et al., 2024); for a recent review on measuring human-AI alignment see (Sucholutsky et al., 2023). **Uncovering hidden associations** Another approach for uncovering the representations of LLMs that is particularly useful for closed models involves adapting methods that tap into implicit associations. The challenge of obtaining true internal representations from LLMs becomes more apparent with closed models that have undergone value alignment post-training. These models do not allow direct access to word embeddings or model weights (Bommasani et al., 2023). Methods such as reinforcement learning from human feedback produce responses that follow safety protocols but may not accurately reflect the models' internal representations (Bai et al., 2022). This issue mirrors the challenges cognitive scientists face when studying human memory (Anderson and Milson, 1989), particularly in accessing concept associations within a closed system like the human brain, which are difficult to measure through self-report questionnaires due to demand characteristics (ie. participants responding in a way that they think the experimenter wants them to respond, or in a way that is socially acceptable). To address this, cognitive scientists have used other behavioral measures, such as reaction time, to approximate the mental distance between pairs of concepts (Collins and Quillian, 1969). These reaction times have been explained by hypothesizing that the human mind organizes concepts as nodes within an associative network, where weighted links reflect the proximity between these nodes. Such associative representations influence behavior; the greater the distance between two concepts, the longer it takes for people to retrieve them, resulting in increased reaction times (Posner and Mitchell, 1967). An intuitive example comes from a classic study demonstrating that human participants react faster to the statement \"a canary can sing\" than to \"a canary can fly.\" This is because the latter requires traversing two degrees of association: \"a canary is a bird\" and \"a bird can fly\" (Collins and Loftus, 1975). This kind of measure is also prevalent in examining attitudes toward social groups (Fazio and Olson, 2003; Greenwald and Banaji, 1995). For instance, the Implicit Association Test (IAT) aligns pairs of social group labels, like \"Black\" or \"White\", with adjectives like \"wonderful\" or \"terrible\" (Greenwald et al., 1998). Empirical studies have repeatedly shown that human participants react faster to minority labels paired with negative adjectives, revealing underlying mental associations about social groups that also predict other aspects of behavior such as the frequency of interacting with members of these groups (see meta-analysis by Kurdi et al., 2019). Figure 5: Large language models such as GPT-4 have been trained to identify situations that involve expressing explicit biases. However, it is possible to construct simple prompts that reveal that they still have strong implicit biases, as reflected in their associations between words. These implicit biases have consequences for their downstream decisions as well. The key insight behind these approaches is that it is possible to elicit mental associations without directly asking the participant for a verbal report. In some cases, researchers aim to capture unobtrusive or unconscious responses (Graf and Schacter, 1985; Schacter, 1987); in others, they strive to minimize self-presentation biases, such as fear of appearing unfair (Fazio and Olson, 2003; Gaertner and McLaughlin, 1983). The success of these methods in achieving these goals suggests that they may also be useful in analyzing the behavior of value-aligned LLMs. The hypothesis is that since alignment trains LLMs to conceal their true representations, methods that bypass direct rating scales or evaluative judgments may better expose their underlying associations. To test this, we adapted the Implicit Association Test for LLMs by prompting various models to associate word pairs used in earlier human studies (Bai et al., 2024). For instance, we asked the model to choose between \"Julia\" and \"Ben\" after presenting words like home, office, parent, management, salary, and wedding. As anticipated, models like GPT-4 often linked Julia with home, parent, and wedding, implying an internal association of females with domestic roles, and Ben with office, management, and salary, indicating a connection to work and male roles. This result is in direct contrast to situations where, when directly asked whether women are poor at management, GPT-4 gave cautious responses, advising against stereotyping based on gender. This example illustrates how psychology-inspired word association tests can effectively uncover hidden associations in LLMs that are both closed and safety-guarded."
    },
    {
      "title": "Summary",
      "text": "Engaging with questions at the algorithmic level of analysis allow us to make use of a number of methods from cognitive psychology to probe the internal workings of LLMs. As illustrated by the general principle of representational interference during parallel processing, and further demonstrated by the case studies of similarity judgments and association tasks, these approaches offer valuable windows into the representations and processes employed by these models. **Implementation level: Uncovering mechanisms** Just as the algorithmic level explores how a computation is performed, the implementation level asks where and with what physical mechanisms those processes are realized. Continuing our bird flight analogy, the implementation level would be akin to studying the bird's muscles, bones, and feathers - the physical components that enable the bird to fly. In neuroscience, this level involves studying individual neurons and neural circuits implementing a given cognitive function (Hubel and Wiesel, 1962). For LLMs, the implementation level concerns the physical substrate of the model: taking the individual artificial neuron (or unit) within the network as the fundamental unit of analysis. Understanding how these individual units and their connections give rise to the algorithms and representations identified at higher levels is the core challenge of the implementation level. **Circuits and mechanistic interpretability** The implementation level is concerned with identifying the physical substrates and mechanisms that realize cognitive computations. A fundamental approach to understanding complex systems involves targeted interventions, which can reveal underlying causal relationships. In both neuroscience and the study of LLMs, recent methodological advancements have enabled increasingly precise interventions, spurring research into the \"circuit-level\" mechanisms of behavior. Neuroscience has seen the widespread adoption of optogenetics, while mechanistic interpretability serves as an analogous approach in the study of LLMs. Optogenetics allows for the causal control of neuronal activity using light, achieved through the expression of light-sensitive opsins in specific neurons. This technique offers high temporal and spatial precision for manipulating neural circuits, providing critical insights into the implementational bases of core cognitive functions, such as social behaviors in various species (Willmore et al., 2022; Cowley et al., 2024; Lin et al., 2011; Bayless et al., 2023; Dulac et al., 2014), memory formation (Ramirez et al., 2013; Rashidet al., 2016; Vetere et al., 2019), and the mediation of valence and behavioral states (Redondo et al., 2014; Gehrlach et al., 2019; Namburi et al., 2015). A recurring theme in these studies is the identification of specific neuronal populations causally linked to particular behaviors or cognitive processes. Mechanistic interpretability applies a similar logic to LLMs, employing techniques like activation patching (Wang et al., 2022; Meng et al., 2022; Gurnee et al., 2023; Todd et al., 2023; Yang et al., 2025) to directly manipulate activations within the model and causally probe its computation. Early investigations, using ablation techniques akin to neural circuit inhibition, identified \"induction heads\" (Olsson et al., 2022) necessary for in-context learning. More recent work has focused on discovering 'interpretable' representations within LLMs, aiming to pinpoint individual or combined units that correspond to specific concepts. A notable example is the use of sparse autoencoders (Bricken et al., 2023), which learn sparse representations of internal activations, potentially revealing disentangled features. By causally activating these learned features, researchers have induced interpretable behaviors like honesty or role-playing (Templeton et al., 2024), suggesting that abstract concepts are indeed encoded within LLMs. These findings parallel the way optogenetics elucidates the neural circuits underlying cognition in biological systems, with circuit discovery beginning to uncover analogous mechanisms in artificial neural networks."
    },
    {
      "title": "High-Level Probes And Decoding",
      "text": "Despite the valuable insights gained from establishing causal links between specific components and behaviors, the limitations of a purely implementation-level, circuit-centric approach are increasingly apparent (Gao et al., 2024; Kantamneni et al., 2025). Many cognitive functions, in both biological and artificial systems, likely emerge from distributed representations across populations of elements, rather than being localized to individual units or small circuits. Consequently, a focus solely on circuits may obscure the crucial role of the geometry and dynamics of these population-level representations in underlying computations. Just as early cognitive neuroscience moved beyond the search for highly localized \"grandmother cells\" responsible for recognizing specific individuals (Quiroga et al., 2005) to embrace the concept of distributed representations (Haxby et al., 2001; Kriegeskorte et al., 2008), LLM interpretability is recognizing the importance of examining population-level activity. Analyzing the geometry and dynamics of these population codes offers a crucial bridge to the algorithmic level of analysis. In neuroscience, the limitations of a purely localized view became apparent with the understanding that information about object categories is distributed across brain regions, as revealed by techniques like multi-voxel pattern analysis (MVPA) (Haxby et al., 2001; Norman et al., 2006). Furthermore, studying population dynamics has provided critical insights into the mechanisms of cognitive processes, such as the trajectories of neural populations during motor control (Churchland et al., 2012; Kaufman et al., 2014; Gallego et al., 2017). Analogously, LLM research is increasingly exploring the collective activity of units, revealing structured representations of information such as syntax (Hewitt and Manning, 2019; Tenney et al., 2019), spatial and temporal relationships (Gurnee and Tegmark, 2023), truth values (Marks and Tegmark, 2023), and cyclical patterns (Liu et al., 2022; Engels et al., 2024). Techniques like Concept Activation Vectors (CAVs) (Kim et al., 2018) and representational engineering (Zou et al., 2023) further demonstrate how human-understandable concepts are embedded within these distributed representations and can be used to influence model behavior. This convergence in neuroscience and LLM research underscores the necessity of studying population-level representations to understand complex cognitive functions. **Summary** The implementation level investigates the physical mechanisms underlying computation in both biological brains and large language models. Mirroring neuroscience techniques like optogenetics, mechanistic interpretability aims to uncover circuit-level mechanisms in LLMs. However, the limitations of a purely circuit-centric approach highlight the importance of examining distributed representations at the population level, an approach increasingly adopted in both neuroscience (e.g., through MVPA) and LLM research. Studying the geometry and dynamics of these population codes provides a crucial link to the algorithmic level, offering a more holistic understanding of how complex computations are realized in these systems. While the implementation level represents the current focus of machine learning interpretability, historically, this level has struggled to fully explain higher-order cognition and complex behavior in the brain. Progress in mechanistic interpretability by AI researchers may thus provide an interesting test case for understanding the challenges that neuroscience faces in understanding human cognition. **Using cognitive science to explore the limits of AI models** In addition to being a source of tools for understanding the implicit assumptions and representations used by large language models, cognitive science offers a different way of thinking about evaluating these models. Many of the evaluations used in AI research focus on defining tasks that are challenging for humans - such as problem-solving (Chollet et al., 2024; Rein et al., 2024; Hendrycks et al., 2020; Wang et al., 2024b) or mathematical reasoning (Ye et al., 2025; Hendrycks et al., 2021; Shi et al., 2022; Mirzadeh et al., 2024) - and measuring the proportion of these tasks that systems are able to solve. By contrast, cognitive science allows us to think about the kinds of problems that might be difficult for these systems based on what we learn about they work. For example, the \"embers of autoregression\" approach (McCoy et al., 2024) was able to use consideration of the computational-level problem solved by LLMs to design a set of tasks that they would find problematic, namely tasks where the the target response has low probability according to the pre-trained language model. In the same way, thinking about parallel and serial processes makes it easy to define tasks that will be challenging for any model that can only perform parallel processing, such as processing images that contain a lot of overlaps in the features of the objects that appear in those images. This kind of approach can allow us to \"adverserially\" design tasks that might pose a more difficult challenge for existing AI systems. Even as those systems display super-human abilities in some settings, we might expect them to fail on these tasks because they pick out problems that should be uniquely difficult for their non-human cognitive architectures. Another way in which cognitive science can be used to explore the limits of AI models relies on their similarities to human cognition. For example, recent work focused on cognitive science and cognitive science and cognitive science. Figure 6: Both humans and large language models show reductions in performance when engaging in verbal reasoning (as resulting from chain of thought prompting) on these tasks. (**a**) Implicit statistical learning involves classification of strings generated from artificial grammars. (**b**) Face recognition involves recognizing faces from a set that shares similar descriptions. (**c**) Classification of data with exceptionsinvolves learning labels with exceptions. expanding the capabilities of LLMs has focused on the potential impact of inference-time compute, where the system has the opportunity to produce additional output (\"reasoning\") before generating its final answer (Wei et al., 2022; Nye et al., 2021; OpenAI, 2024; Guo et al., 2025). This intervening step provides an additional source of information to condition on in producing a response, as well as the opportunity to engage in additional computation over the input. However, engaging in reasoning is not always beneficial for humans: there are a variety of tasks where thinking out loud has negative consequences for human behavior (Reber, 1976; Schooler and Engstler-Schooler, 1990; Williams et al., 2013; Fiore and Schooler, 2002; Fallshore and Schooler, 1993; Melcher and Schooler, 1996; Khemlani and Johnson-Laird, 2012). Liu et al. (Liu et al., 2024) showed that the psychological literature on the negative effects of verbal thinking provides an effective way to identify problems where inference time compute has negative consequences. Artificial grammar learning, face recognition, and learning a concept from exemplars are all settings where more reasoning - such as the use of a chain-of-thought prompt - results in worse performance by LLMs or VLMs. These results challenge the assumption that more reasoning always leads to better outcomes that is currently guiding the design of AI systems. We anticipate that a similar approach, focusing on the cases where there might be overlaps in the solutions used in natural and artificial minds, can be used to turn up other challenges for contemporary AI systems, providing a complement to the approach of focusing on the distinctive aspects of the problem that AI systems solve highlighted above."
    },
    {
      "title": "Conclusion",
      "text": "The breakthroughs culminating in advanced AI systems such as large language models have presented computer science with the unfamiliar challenge of interpreting the behavior of complex and opaque neural networks. Just as cognitive science has long grappled with the problem of understanding the mind from the outside in, the tools refined over decades of psychological and neuroscientific inquiry offer a powerful framework for approaching these new forms of intelligence. In this review, we have argued for the utility of Marr's three levels of analysis as an organizing principle for applying these tools to large language models. At the computational level, examining training objectives allows us to predict behavioral patterns, as seen with the \"embers of autoregression.\" At the algorithmic level, methods like similarity judgments and association tasks reveal the structure of internal representations, mirroring techniques used to probe human cognition. Finally, while the implementation level remains a frontier, the study of circuits and population dynamics, drawing parallels with neuroscience, promises to illuminate the physical substrates of these artificial cognitive processes. By embracing this cognitive science perspective, moving beyond purely performance-based evaluations, we can develop more insightful means of understanding, evaluating, and ultimately guiding the development of increasingly sophisticated AI."
    },
    {
      "title": "Acknowledgements",
      "text": "This research project and related research were made possible with the support of the NOMIS Foundation."
    },
    {
      "title": "References",
      "text": "* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_. * Anderson (1990) Anderson, J. R. (1990). _The adaptive character of thought_. Erlbaum. * Anderson and Milson (1989) Anderson, J. R. and Milson, R. (1989). Human memory: An adaptive perspective. _Psychological Review_, 96(4):703. * Bai et al. (2024) Bai, X., Wang, A., Sucholutsky, I., and Griffiths, T. L. (2024). Measuring implicit bias in explicitly unbiased large language models. _arXiv preprint arXiv:2402.04105_. * Bai et al. (2022) Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. (2022). Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_. * Battaglia et al. (2013) Battaglia, P. W., Hamrick, J. B., and Tenenbaum, J. B. (2013). Simulation as an engine of physical scene understanding. _Proceedings of the National Academy of Sciences of the United States of America_, 110(45):18327-18332. * Bayless et al. (2023) Bayless, D. W., Chung-ha, O. D., Yang, R., Wei, Y., de Andrade Carvalho, V. M., Knoedler, J. R., Yang, T., Livingston, O., Lomvardas, A., Martins, G. J., et al. (2023). A neural circuit for male sexual behavior and reward. _Cell_, 186(18):3862-3881. * Bernardo and Smith (1994) Bernardo, J. M. and Smith, A. F. M. (1994). _Bayesian theory_. Wiley, New York. * Binz and Schulz (2023) Binz, M. and Schulz, E. (2023). Using cognitive psychology to understand gpt-3. _Proceedings of the National Academy of Sciences_, 120(6):e2218523120. * Bommasani et al. (2023) Bommasani, R., Klyman, K., Longpre, S., Kapoor, S., Maslej, N., Xiong, B., Zhang, D., and Liang, P. (2023). The foundation model transparency index. _arXiv preprint arXiv:2310.12941_. * Bordes et al. (2024) Bordes, F., Pang, R. Y., Ajay, A., Li, A. C., Bardes, A., Petryk, S., Manas, O., Lin, Z., Mahmoud, A., Jayaraman, B., et al. (2024). An introduction to vision-language modeling. _arXiv preprint arXiv:2405.17247_. * Bouchacourt and Buschman (2019) Bouchacourt, F. and Buschman, T. J. (2019). A flexible model of working memory. _Neuron_, 103(1):147-160. * Bricken et al. (2023) Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., et al. (2023). Towards monosemanticity: Decomposing language models with dictionary learning. _Transformer Circuits Thread_, 2. * Bubeck et al. (2023) Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. (2023). Sparks of artificial general intelligence: Early experiments with GPT-4. _arXiv preprint arXiv:2303.12712_. * Campbell et al. (2024) Campbell, D., Rane, S., Giallanza, T., De Sabbata, C. N., Ghods, K., Joshi, A., Ku, A., Frankland, S., Griffiths, T., Cohen, J. D., et al. (2024). Understanding the limits of vision language models through the lens of the binding problem. _Advances in Neural Information Processing Systems_, 37:113436-113460. * Chater and Manning (2006) Chater, N. and Manning, C. D. (2006). Probabilistic models of language processing and acquisition. _Trends in Cognitive Sciences_, 10:335-344. * Chollet et al. (2024) Chollet, F., Knoop, M., Kamradt, G., and Landers, B. (2024). Arc prize 2024: Technical report. _arXiv preprint arXiv:2412.04604_. * Churchland et al. (2012) Churchland, M. M., Cunningham, J. P., Kaufman, M. T., Foster, J. D., Nuyujukian, P., Ryu, S. I., and Shenoy, K. V. (2012). Neural population dynamics during reaching. _Nature_, 487(7405):51-56. * Coda-Forno et al. (2024) Coda-Forno, J., Binz, M., Wang, J. X., and Schulz, E. (2024). Cogbench: a large language model walks into a psychology lab. arXiv preprint arXiv:2402.18225. * Collins and Loftus (1975) Collins, A. M. and Loftus, E. F. (1975). A spreading-activation theory of semantic processing. _Psychological review_, 82(6):407. * Collins and Quillian (1969) Collins, A. M. and Quillian, M. R. (1969). Retrieval time from semantic memory. _Journal of verbal learning and verbal behavior_, 8(2):240-247. * Cowley et al. (2024) Cowley, B. R., Calhoun, A. J., Rangarajan, N., Ireland, E., Turner, M. H., Pillow, J. W., and Murthy, M. (2024). Mapping model units to visual neurons reveals population code for social behaviour. _Nature_, 629(8014):1100-1108. * Dulac et al. (2014) Dulac, C., O'Connell, L. A., and Wu, Z. (2014). Neural control of maternal and paternal behaviors. _Science_, 345(6198):765-770. * Ekman (1954) Ekman, G. (1954). Dimensions of color vision. _The Journal of Psychology_, 38(2):467-474. * Engels et al. (2024) Engels, J., Michaud, E. J., Liao, I., Gurnee, W., and Tegmark, M. (2024). Not all language model features are linear. _arXiv preprint arXiv:2405.14860_. * Fallshore and Schooler (1993) Fallshore, M. and Schooler, J. W. (1993). Post-encoding verbalization impairs transfer on artificial grammar tasks. In _Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society Erlbaum: Hillsdale, NJ_, pages 412-416. * Fazio and Olson (2003) Fazio, R. H. and Olson, M. A. (2003). Implicit measures in social cognition research: Their meaning and use. _Annual review of psychology_, 54(1):297-327. * Fiore and Schooler (2002) Fiore, S. M. and Schooler, J. W. (2002). How did you get here from there? Verbal overshadowing of spatial mental models. _Applied Cognitive Psychology_, 16(8):897-910. * Gaertner and McLaughlin (1983) Gaertner, S. L. and McLaughlin, J. P. (1983). Racial stereotypes: Associations and ascriptions of positive and negative characteristics. _Social Psychology Quarterly_, pages 23-30. * Gallego et al. (2017) Gallego, J. A., Perich, M. G., Miller, L. E., and Solla, S. A. (2017). Neural manifolds for the control of movement. Neuron, 94(5):978-984. * Gallego et al. (2017)* Gao et al. (2024) Gao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R., Radford, A., Sutskever, I., Leike, J., and Wu, J. (2024). Scaling and evaluating sparse autoencoders. _arXiv preprint arXiv:2406.04093_. * Gehrlach et al. (2019) Gehrlach, D. A., Dolensek, N., Klein, A. S., Roy Chowdhury, R., Matthys, A., Junghanel, M., Gaitanos, T. N., Podgornik, A., Black, T. D., Reddy Vaka, N., Conzelmann, K.-K., and Gogolla, N. (2019). Aversive state processing in the posterior insular cortex. _Nature Neuroscience_, 22(9):1424-1437. * Graf and Schacter (1985) Graf, P. and Schacter, D. L. (1985). Implicit and explicit memory for new associations in normal and amnesic subjects. _Journal of Experimental Psychology: Learning, memory, and cognition_, 11(3):501. * Greenwald and Banaji (1995) Greenwald, A. G. and Banaji, M. R. (1995). Implicit social cognition: attitudes, self-esteem, and stereotypes. _Psychological review_, 102(1):4. * Greenwald et al. (1998) Greenwald, A. G., McGhee, D. E., and Schwartz, J. L. (1998). Measuring individual differences in implicit cognition: the implicit association test. _Journal of personality and social psychology_, 74(6):1464. * Griffiths (2020) Griffiths, T. L. (2020). Understanding human intelligence through human limitations. _Trends in Cognitive Sciences_, 24(11):873-883. * Griffiths et al. (2024a) Griffiths, T. L., Chater, N., and Tenenbaum, J. B. (2024a). _Bayesian models of cognition: reverse engineering the mind_. MIT Press. * Griffiths et al. (2007) Griffiths, T. L., Steyvers, M., and Tenenbaum, J. B. (2007). Topics in semantic association. _Psychological Review_, 114:211-244. * Griffiths and Tenenbaum (2006) Griffiths, T. L. and Tenenbaum, J. B. (2006). Optimal predictions in everyday cognition. _Psychological Science_, 17:767-773. * Griffiths et al. (2019)* Griffiths et al. (2024) Griffiths, T. L., Zhu, J.-Q., Grant, E., and Thomas McCoy, R. (2024b). Bayes in the age of intelligent machines. _Current Directions in Psychological Science_, 33(5):283-291. * Guo et al. (2025) Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. _arXiv preprint arXiv:2501.12948_. * Gurnee et al. (2023) Gurnee, W., Nanda, N., Pauly, M., Harvey, K., Troitskii, D., and Bertsimas, D. (2023). Finding neurons in a haystack: Case studies with sparse probing. _arXiv preprint arXiv:2305.01610_. * Gurnee and Tegmark (2023) Gurnee, W. and Tegmark, M. (2023). Language models represent space and time. _arXiv preprint arXiv:2310.02207_. * Haxby et al. (2001) Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A., Schouten, J. L., and Pietrini, P. (2001). Distributed and overlapping representations of faces and objects in ventral temporal cortex. _Science_, 293(5539):2425-2430. * Hebart et al. (2020) Hebart, M. N., Zheng, C. Y., Pereira, F., and Baker, C. I. (2020). Revealing the multidimensional mental representations of natural objects underlying human similarity judgements. _Nature human behaviour_, 4(11):1173-1185. * Hendrycks et al. (2020) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2020). Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_. * Hendrycks et al. (2021) Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. (2021). Measuring mathematical problem solving with the math dataset. _arXiv preprint arXiv:2103.03874_. * Hewitt and Manning (2019) Hewitt, J. and Manning, C. D. (2019). A structural probe for finding syntax in word representations. In _Proceedings of the 2019 Conference of the North American Chapter* Hubel and Wiesel (1962) Hubel, D. H. and Wiesel, T. N. (1962). Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. _The Journal of physiology_, 160(1):106. * Kahneman and Tversky (1979) Kahneman, D. and Tversky, A. (1979). Prospect theory: An analysis of decision under risk. _Econometrica_, 47(2):263-292. * Kantamneni et al. (2025) Kantamneni, S., Engels, J., Rajamanoharan, S., Tegmark, M., and Nanda, N. (2025). Are sparse autoencoders useful? a case study in sparse probing. _arXiv preprint arXiv:2502.16681_. * Kaufman et al. (1949) Kaufman, E. L., Lord, M. W., Reese, T. W., and Volkmann, J. (1949). The discrimination of visual number. _The American journal of psychology_, 62(4):498-525. * Kaufman et al. (2014) Kaufman, M. T., Churchland, M. M., Ryu, S. I., and Shenoy, K. V. (2014). Cortical activity in the null space: permitting preparation without movement. _Nature neuroscience_, 17(3):440-448. * Khemlani and Johnson-Laird (2012) Khemlani, S. S. and Johnson-Laird, P. N. (2012). Hidden conflicts: Explanations make inconsistencies harder to detect. _Acta Psychologica_, 139(3):486-491. * Kim et al. (2018) Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et al. (2018). Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In _International conference on machine learning_, pages 2668-2677. PMLR. * Kriegeskorte et al. (2008) Kriegeskorte, N., Mur, M., Ruff, D. A., Kiani, R., Bodurka, J., Esteky, H., Tanaka, K., and Bandettini, P. A. (2008). Matching categorical object representations in inferior temporal cortex of man and monkey. _Neuron_, 60(6):1126-1141. * Kurdi et al. (2019) Kurdi, B., Seitchik, A. E., Axt, J. R., Carroll, T. J., Karapetyan, A., Kaushik, N., Tomezsko, D., Greenwald, A. G., and Banaji, M. R. (2019). Relationship between the implicit association test and intergroup behavior: A meta-analysis. _American psychologist_, 74(5):569. * Lin et al. (2011) Lin, D., Boyle, M. P., Dollar, P., Lee, H., Lein, E., Perona, P., and Anderson, D. J. (2011). Functional identification of an aggression locus in the mouse hypothalamus. _Nature_, 470(7333):221-226. * Liu et al. (2024a) Liu, R., Geng, J., Peterson, J. C., Sucholutsky, I., and Griffiths, T. L. (2024a). Large language models assume people are more rational than we really are. * Liu et al. (2024b) Liu, R., Geng, J., Wu, A. J., Sucholutsky, I., Lombrozo, T., and Griffiths, T. L. (2024b). Mind your step (by step): Chain-of-thought can reduce performance on tasks where thinking makes humans worse. _arXiv preprint arXiv:2410.21333_. * Liu et al. (2022) Liu, Z., Kitouni, O., Nolte, N. S., Michaud, E., Tegmark, M., and Williams, M. (2022). Towards understanding grokking: An effective theory of representation learning. _Advances in Neural Information Processing Systems_, 35:34651-34663. * Marjieh et al. (2024a) Marjieh, R., Jacoby, N., Peterson, J. C., and Griffiths, T. L. (2024a). The universal law of generalization holds for naturalistic stimuli. _Journal of Experimental Psychology: General_, 153(3):573. * Marjieh et al. (2024b) Marjieh, R., Sucholutsky, I., van Rijn, P., Jacoby, N., and Griffiths, T. L. (2024b). Large language models predict human sensory judgments across six modalities. _Scientific Reports_, 14(1):21445. * Marjieh et al. (2025) Marjieh, R., Veselovsky, V., Griffiths, T. L., and Sucholutsky, I. (2025). What is a number, that a large language model may know it? _arXiv preprint arXiv:2502.01540_. * Marks and Tegmark (2023) Marks, S. and Tegmark, M. (2023). The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. _arXiv preprint arXiv:2310.06824_. * Marjieh et al. (2024)* Marr (1982) Marr, D. (1982). _Vision_. W. H. Freeman, San Francisco, CA. * McCoy et al. (2024) McCoy, R. T., Yao, S., Friedman, D., Hardy, M. D., and Griffiths, T. L. (2024). Embers of autoregression show how large language models are shaped by the problem they are trained to solve. _Proceedings of the National Academy of Sciences_, 121(41):e2322420121. * Melcher and Schooler (1996) Melcher, J. M. and Schooler, J. W. (1996). The misremembrance of wines past: Verbal and perceptual expertise differentially mediate verbal overshadowing of taste memory. _Journal of Memory and Language_, 35(2):231-245. * Meng et al. (2022) Meng, K., Bau, D., Andonian, A., and Belinkov, Y. (2022). Locating and editing factual associations in gpt. _Advances in neural information processing systems_, 35:17359-17372. * Miller (2003) Miller, G. A. (2003). The cognitive revolution: a historical perspective. _Trends in cognitive sciences_, 7:141-144. * Miller and Gelman (1983) Miller, K. and Gelman, R. (1983). The child's representation of number: A multidimensional scaling analysis. _Child development_, pages 1470-1479. * Mirzadeh et al. (2024) Mirzadeh, I., Alizadeh, K., Shahrokhi, H., Tuzel, O., Bengio, S., and Farajtabar, M. (2024). Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. _arXiv preprint arXiv:2410.05229_. * Mukherjee et al. (2024) Mukherjee, K., Rogers, T. T., and Schloss, K. B. (2024). Large language models estimate fine-grained human color-concept associations. _arXiv preprint arXiv:2406.17781_. * Murthy et al. (2024) Murthy, S. K., Ullman, T., and Hu, J. (2024). One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity. _arXiv preprint arXiv:2411.04427_. * Musslick and Cohen (2021) Musslick, S. and Cohen, J. D. (2021). Rationalizing constraints on the capacity for cognitive control. _Trends in cognitive sciences_, 25(9):757-775. * Musslick et al. (2020)* Namburi et al. (2015) Namburi, P., Beyeler, A., Yorozu, S., Calhoon, G. G., Halbert, S. A., Wichmann, R., Holden, S. S., Mertens, K. L., Anahtar, M., Felix-Ortiz, A. C., Wickersham, I. R., Gray, J. M., and Tye, K. M. (2015). A circuit mechanism for differentiating positive and negative associations. _Nature_, 520(7549):675-678. * Norman et al. (2006) Norman, K. A., Polyn, S. M., Detre, G. J., and Haxby, J. V. (2006). Beyond mind-reading: multi-voxel pattern analysis of fmri data. _Trends in cognitive sciences_, 10(9):424-430. * Nye et al. (2021) Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. (2021). Show your work: Scratchpads for intermediate computation with language models. _arXiv preprint arXiv:2112.00114_. * Ogg et al. (2024) Ogg, M., Bose, R., Scharf, J., Ratto, C., and Wolmetz, M. (2024). Turing representational similarity analysis (rsa): A flexible method for measuring alignment between human and artificial intelligence. _arXiv preprint arXiv:2412.00577_. * Olsson et al. (2022) Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., et al. (2022). In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_. * OpenAI (2024) OpenAI (2024). Learning to reason with LLMs. * Peterson et al. (2020) Peterson, J. C., Chen, D., and Griffiths, T. L. (2020). Parallelograms revisited: Exploring the limitations of vector space models for simple analogies. _Cognition_, 205:104440. * Piantadosi (2016) Piantadosi, S. T. (2016). A rational analysis of the approximate number system. _Psychonomic bulletin & review_, 23:877-886. * Pitt et al. (2022) Pitt, B., Gibson, E., and Piantadosi, S. T. (2022). Exact number concepts are limited to the verbal count range. _Psychological Science_, 33(3):371-381. * Posner and Mitchell (1967) Posner, M. I. and Mitchell, R. F. (1967). Chronometric analysis of classification. Psychological review, 74(5):392. * Pitt et al. (2015)* Quiroga et al. (2005) Quiroga, R. Q., Reddy, L., Kreiman, G., Koch, C., and Fried, I. (2005). Invariant visual representation by single neurons in the human brain. _Nature_, 435(7045):1102-1107. * Ramirez et al. (2013) Ramirez, S., Liu, X., Lin, P.-A., Suh, J., Pignatelli, M., Redondo, R. L., Ryan, T. J., and Tonegawa, S. (2013). Creating a False Memory in the Hippocampus. _Science_, 341(6144):387-391. * Rashid et al. (2016) Rashid, A. J., Yan, C., Mercaldo, V., Hsiang, H.-L., Park, S., Cole, C. J., De Cristofaro, A., Yu, J., Ramakrishnan, C., Lee, S. Y., et al. (2016). Competition between engrams influences fear memory formation and recall. _Science_, 353(6297):383-387. * Reber (1976) Reber, A. S. (1976). Implicit learning of synthetic languages: The role of instructional set. _Journal of Experimental Psychology: Human Learning and Memory_, 2(1):88. * Redondo et al. (2014) Redondo, R. L., Kim, J., Arons, A. L., Ramirez, S., Liu, X., and Tonegawa, S. (2014). Bidirectional switch of the valence associated with a hippocampal contextual memory engram. _Nature_, 513(7518):426-430. * Rein et al. (2024) Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. (2024). Gpqa: A graduate-level google-proof q&a benchmark. In _First Conference on Language Modeling_. * Rosch (1975) Rosch, E. (1975). Cognitive representations of semantic categories. _Journal of experimental psychology: General_, 104(3):192. * Sanborn et al. (2010) Sanborn, A. N., Griffiths, T. L., and Navarro, D. J. (2010). Rational approximations to rational models: Alternative algorithms for category learning. _Psychological Review_, 117:1144-1167. * Sanborn et al. (2013) Sanborn, A. N., Mansinghka, V. K., and Griffiths, T. L. (2013). Reconciling intuitive physics and Newtonian mechanics for colliding objects. _Psychological Review_, 120:411-437. * Sanborn et al. (2014)* Savage (1954) Savage, L. J. (1954). _Foundations of statistics_. John Wiley & Sons, New York. * Schacter (1987) Schacter, D. L. (1987). Implicit memory: History and current status. _Journal of experimental psychology: learning, memory, and cognition_, 13(3):501. * Schooler and Engstler-Schooler (1990) Schooler, J. W. and Engstler-Schooler, T. Y. (1990). Verbal overshadowing of visual memories: Some things are better left unsaid. _Cognitive Psychology_, 22(1):36-71. * Shepard (1962) Shepard, R. N. (1962). The analysis of proximities: Multidimensional scaling with an unknown distance function: I. _Psychometrika_, 27:124-140. * Shepard (1980) Shepard, R. N. (1980). Multidimensional scaling, tree-fitting, and clustering. _Science_, 210:390-398. * Shepard (1982) Shepard, R. N. (1982). Geometrical approximations to the structure of musical pitch. _Psychological review_, 89(4):305. * Shepard (1987) Shepard, R. N. (1987). Towards a universal law of generalization for psychological science. _Science_, 237:1317-1323. * Shi et al. (2022) Shi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S., Vosoughi, S., Chung, H. W., Tay, Y., Ruder, S., Zhou, D., et al. (2022). Language models are multilingual chain-of-thought reasoners. _arXiv preprint arXiv:2210.03057_. * Sucholutsky et al. (2023) Sucholutsky, I., Muttenthaler, L., Weller, A., Peng, A., Bobu, A., Kim, B., Love, B. C., Grant, E., Groen, I., Achterberg, J., et al. (2023). Getting aligned on representational alignment. _arXiv preprint arXiv:2310.13018_. * Suresh et al. (2023) Suresh, S., Mukherjee, K., Yu, X., Huang, W.-C., Padua, L., and Rogers, T. T. (2023). Conceptual structure coheres in human cognition but not in large language models. _arXiv preprint arXiv:2304.02754_. * Suresh et al. (2023)* Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. (2023). Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_. * Templeton et al. (2024) Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones, A., Cunningham, H., Turner, N. L., McDougall, C., MacDiarmid, M., Freeman, C. D., Sumers, T. R., Rees, E., Batson, J., Jermyn, A., Carter, S., Olah, C., and Henighan, T. (2024). Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. _Transformer Circuits Thread_. * Tenenbaum (1999) Tenenbaum, J. (1999). Rules and similarity in concept learning. _Advances in neural information processing systems_, 12. * Tenney et al. (2019) Tenney, I., Das, D., and Pavlick, E. (2019). Bert rediscovers the classical nlp pipeline. _arXiv preprint arXiv:1905.05950_. * Todd et al. (2023) Todd, E., Li, M. L., Sharma, A. S., Mueller, A., Wallace, B. C., and Bau, D. (2023). Function vectors in large language models. _arXiv preprint arXiv:2310.15213_. * Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_. * Townsend (1990) Townsend, J. T. (1990). Serial vs. parallel processing: Sometimes they look like tweedledum and tweedledee but they can (and should) be distinguished. _Psychological science_, 1(1):46-54. * Treisman and Schmidt (1982) Treisman, A. and Schmidt, H. (1982). Illusory conjunctions in the perception of objects. _Cognitive psychology_, 14(1):107-141. * Treisman and Gelade (1980) Treisman, A. M. and Gelade, G. (1980). A feature-integration theory of attention. _Cognitive psychology_, 12(1):97-136. * Treisman et al. (2019)* Trick and Pylyshyn (1994) Trick, L. M. and Pylyshyn, Z. W. (1994). Why are small and large numbers enumerated differently? a limited-capacity preattentive stage in vision. _Psychological review_, 101(1):80. * Tversky (1977) Tversky, A. (1977). Features of similarity. _Psychological Review_, 84:327-352. * Tversky and Hutchinson (1986) Tversky, A. and Hutchinson, J. W. (1986). Nearest neighbor analysis of psychological spaces. _Psychological Review_, 93:3-22. * Tversky and Kahneman (1974) Tversky, A. and Kahneman, D. (1974). Judgment under uncertainty: heuristics and biases. _Science_, 185:1124-1131. * Vetere et al. (2019) Vetere, G., Tran, L. M., Mberg, S., Steadman, P. E., Restivo, L., Morrison, F. G., Ressler, K. J., Josselyn, S. A., and Frankland, P. W. (2019). Memory formation in the absence of experience. _Nature Neuroscience_, 22(6):933-940. * Von Neumann and Morgenstern (1947) Von Neumann, J. and Morgenstern, O. (1947). _Theory of games and economic behavior_. Princeton University Press, Princeton, NJ, 2nd edition. * Wang et al. (2022) Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. (2022). Interpretability in the wild: a circuit for indirect object identification in gpt-2 small, 2022. _URL https://arxiv. org/abs/2211.00593_, 2. * Wang et al. (2024a) Wang, X., Zhu, W., Saxon, M., Steyvers, M., and Wang, W. Y. (2024a). Large language models are implicitly latent variable models: Explaining and finding good demonstrations for in-context learning. * Wang et al. (2024b) Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. (2024b). Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. In _The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track_. * Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35:24824-24837. * Williams et al. (2013) Williams, J. J., Lombrozo, T., and Rehder, B. (2013). The hazards of explanation: Overgeneralization in the face of exceptions. _Journal of Experimental Psychology: General_, 142(4):1006. * Willmore et al. (2022) Willmore, L., Cameron, C., Yang, J., Witten, I. B., and Falkner, A. L. (2022). Behavioural and dopaminergic signatures of resilience. _Nature_, 611(7934):124-132. * Xie et al. (2021) Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. (2021). An explanation of in-context learning as implicit Bayesian inference. _arXiv preprint arXiv:2111.02080_. * Yang et al. (2025) Yang, Y., Campbell, D., Huang, K., Wang, M., Cohen, J., and Webb, T. (2025). Emergent symbolic mechanisms support abstract reasoning in large language models. _arXiv preprint arXiv:2502.20332_. * Ye et al. (2025) Ye, Y., Xiao, Y., Mi, T., and Liu, P. (2025). Aime-preview: A rigorous and immediate evaluation framework for advanced mathematical reasoning. [https://github.com/GAIR-NLP/AIME-Preview](https://github.com/GAIR-NLP/AIME-Preview). GitHub repository. * Yuille and Kersten (2006) Yuille, A. and Kersten, D. (2006). Vision as Bayesian inference: analysis by synthesis? _Trends in Cognitive Sciences_, 10:301-308. * Zhang et al. (2024) Zhang, L., Li, M. Y., and Griffiths, T. L. (2024). What should embeddings embed? autoregressive models represent latent generating distributions. * Zhang et al. (2023) Zhang, L., McCoy, R. T., Sumers, T. R., Zhu, J.-Q., and Griffiths, T. L. (2023). Deep de finetti: Recovering topic distributions from large language models. * Zheng et al. (2023) Zheng, C., Vafa, K., and Blei, D. M. (2023). Revisiting topic-guided language models. * Zhong et al. (2024) Zhong, S., Zhou, Z., Dawes, C., Brianz, G., and Obrist, M. (2024). Sniff ai: Is my'spicy'your'spicy'? exploring llm's perceptual alignment with human smell experiences. _arXiv preprint arXiv:2411.06950_. * Zhu and Griffiths (2024a) Zhu, J.-Q. and Griffiths, T. (2024a). Incoherent probability judgments in large language models. In _Proceedings of the Annual Meeting of the Cognitive Science Society_, volume 46. * Zhu and Griffiths (2024b) Zhu, J.-Q. and Griffiths, T. L. (2024b). Eliciting the priors of large language models using iterated in-context learning. _arXiv preprint arXiv:2406.01860_. * Zhu et al. (2020) Zhu, J.-Q., Sanborn, A. N., and Chater, N. (2020). The bayesian sampler: Generic bayesian inference causes incoherence in human probability judgments. _Psychological review_, 127(5):719. * Zou et al. (2023) Zou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren, R., Pan, A., Yin, X., Mazeika, M., Dombrowski, A.-K., et al. (2023). Representation engineering: A top-down approach to ai transparency. _arXiv preprint arXiv:2310.01405_."
    }
  ]
}