{
  "title": "A Survey of Personalized Large Language Models: Progress and Future Directions",
  "authors": [
    "Jiahong Liu",
    "Zexuan Qiu",
    "Zhongyang Li",
    "Quanyu Dai",
    "Jieming Zhu",
    "Minda Hu",
    "Menglin Yang",
    "Irwin King"
  ],
  "abstract": "\n Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the Github Repo. \n",
  "references": [
    {
      "id": null,
      "title": "A Survey of Personalized Large Language Models: Progress and Future Directions",
      "authors": [
        "Jiahong Liu",
        "Zexuan Qiu",
        "Zhongyang Li",
        "Quanyu Dai",
        "Jieming Zhu",
        "Minda Hu",
        "Menglin Yang",
        "Irwin King"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Personalized graph-based retrieval for large language models",
      "authors": [
        "Steven Au",
        "Cameron J Dimacali",
        "Ojasmitha Pedirappagari",
        "Namyong Park",
        "Franck Dernoncourt",
        "Yu Wang",
        "Nikos Kanakaris",
        "Hanieh Deilamsalehy",
        "Ryan A Rossi",
        "Nesreen K Ahmed"
      ],
      "year": "2025",
      "venue": "Personalized graph-based retrieval for large language models",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "authors": [
        "Yuntao Bai",
        "Andy Jones",
        "Kamal Ndousse",
        "Amanda Askell",
        "Anna Chen",
        "Nova Dassarma",
        "Dawn Drain",
        "Stanislav Fort",
        "Deep Ganguli",
        "Tom Henighan"
      ],
      "year": "2022",
      "venue": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Proc. of NeurIPS",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "A survey on mixture of experts",
      "authors": [
        "Weilin Cai",
        "Juyong Jiang",
        "Fan Wang",
        "Jing Tang",
        "Sunghun Kim",
        "Jiayi Huang"
      ],
      "year": "2024",
      "venue": "A survey on mixture of experts",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "When large language models meet personalization: Perspectives of challenges and opportunities",
      "authors": [
        "Jin Chen",
        "Zheng Liu",
        "Xu Huang",
        "Chenwang Wu",
        "Qi Liu",
        "Gangwei Jiang",
        "Yuanhao Pu",
        "Yuxuan Lei",
        "Xiaolong Chen",
        "Xingmei Wang"
      ],
      "year": "2024",
      "venue": "World Wide Web",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Pad: Personalized alignment of llms at decoding-time",
      "authors": [
        "Ruizhe Chen",
        "Xiaotian Zhang",
        "Meng Luo",
        "Wenhao Chai",
        "Zuozhu Liu"
      ],
      "year": "2024",
      "venue": "Pad: Personalized alignment of llms at decoding-time",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2022",
      "venue": "Scaling language modeling with pathways",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Scaling instructionfinetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma"
      ],
      "year": "2022",
      "venue": "Scaling instructionfinetuned language models",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Peft-u: Parameter-efficient fine-tuning for user personalization",
      "authors": [
        "Christopher Clarke",
        "Yuzhao Heng",
        "Lingjia Tang",
        "Jason Mars"
      ],
      "year": "2024",
      "venue": "Peft-u: Parameter-efficient fine-tuning for user personalization",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement",
      "authors": [
        "Bhavana Dalvi",
        "Oyvind Tafjord",
        "Peter Clark"
      ],
      "year": "2022",
      "venue": "Proc. of EMNLP",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "User embedding model for personalized language prompting",
      "authors": [
        "Sumanth Doddapaneni",
        "Krishna Sayana",
        "Ambarish Jash",
        "Sukhdeep Sodhi",
        "Dima Kuzmin"
      ],
      "year": "2024",
      "venue": "User embedding model for personalized language prompting",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Perltqa: A personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering",
      "authors": [
        "Yiming Du",
        "Hongru Wang",
        "Zhengyi Zhao",
        "Bin Liang",
        "Baojun Wang",
        "Wanjun Zhong",
        "Zezhong Wang",
        "Kam-Fai Wong"
      ],
      "year": "2024",
      "venue": "Perltqa: A personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "A survey on rag meeting llms: Towards retrieval-augmented large language models",
      "authors": [
        "Wenqi Fan",
        "Yujuan Ding",
        "Liangbo Ning",
        "Shijie Wang",
        "Hengyun Li",
        "Dawei Yin",
        "Tat-Seng Chua",
        "Qing Li"
      ],
      "year": "2024",
      "venue": "Proc. of KDD",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Retrieval-augmented generation for large language models: A survey",
      "authors": [
        "Yunfan Gao",
        "Yun Xiong",
        "Xinyu Gao",
        "Kangxiang Jia",
        "Jinliu Pan",
        "Yuxi Bi",
        "Yi Dai",
        "Jiawei Sun",
        "Haofen Wang"
      ],
      "year": "2023",
      "venue": "Retrieval-augmented generation for large language models: A survey",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "authors": [
        "Dejian Daya Guo",
        "Haowei Yang",
        "Junxiao Zhang",
        "Ruoyu Song",
        "Runxin Zhang",
        "Qihao Xu",
        "Shirong Zhu",
        "Peiyi Ma",
        "Xiao Wang",
        "Bi"
      ],
      "year": "2025",
      "venue": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Cos: Enhancing personalization and mitigating bias with context steering",
      "authors": [
        "Jerry Zhi-Yang He",
        "Sashrika Pandey",
        "Mariah L Schrum",
        "Anca Dragan"
      ],
      "year": "2024",
      "venue": "Cos: Enhancing personalization and mitigating bias with context steering",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Persoma: Personalized soft prompt adapter architecture for personalized language prompting",
      "authors": [
        "Liam Hebert",
        "Krishna Sayana",
        "Ambarish Jash",
        "Alexandros Karatzoglou",
        "S Sukhdeep",
        "Sumanth Sodhi",
        "Yanli Doddapaneni",
        "Dima Cai",
        "Kuzmin"
      ],
      "year": "2024",
      "venue": "Persoma: Personalized soft prompt adapter architecture for personalized language prompting",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen",
        "Lora"
      ],
      "year": "2021",
      "venue": "Low-rank adaptation of large language models",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "SeRTS: Self-rewarding tree search for biomedical retrieval-augmented generation",
      "authors": [
        "Minda Hu",
        "Licheng Zong",
        "Hongru Wang",
        "Jingyan Zhou",
        "Jingjing Li",
        "Yichen Gao",
        "Kam-Fai Wong",
        "Yu Li",
        "Irwin King"
      ],
      "year": "2024",
      "venue": "Proc. of EMNLP Findings",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Unsupervised dense information retrieval with contrastive learning",
      "authors": [
        "Gautier Izacard",
        "Mathilde Caron",
        "Lucas Hosseini",
        "Sebastian Riedel",
        "Piotr Bojanowski",
        "Armand Joulin",
        "Edouard Grave"
      ],
      "year": "2021",
      "venue": "Unsupervised dense information retrieval with contrastive learning",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Personalized soups: Personalized large language model alignment via post-hoc parameter merging",
      "authors": [
        "Joel Jang",
        "Seungone Kim",
        "Bill Yuchen Lin",
        "Yizhong Wang",
        "Jack Hessel",
        "Luke Zettlemoyer",
        "Hannaneh Hajishirzi",
        "Yejin Choi",
        "Prithviraj Ammanabrolu"
      ],
      "year": "2023",
      "venue": "Personalized soups: Personalized large language model alignment via post-hoc parameter merging",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Billion-scale similarity search with gpus",
      "authors": [
        "Jeff Johnson",
        "Matthijs Douze",
        "Hervé Jégou"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Big Data",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models",
      "authors": [
        "Rose Hannah",
        "Alexander Kirk",
        "Paul Whitefield",
        "Andrew Röttger",
        "Katerina Bean",
        "Juan Margatina",
        "Rafael Ciro",
        "Max Mosquera",
        "Adina Bartolo",
        "He Williams",
        "He"
      ],
      "year": "2024",
      "venue": "The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Customizing language models with instance-wise lora for sequential recommendation",
      "authors": [
        "Xiaoyu Kong",
        "Jiancan Wu",
        "An Zhang",
        "Leheng Sheng",
        "Hui Lin",
        "Xiang Wang",
        "Xiangnan He"
      ],
      "year": "2024",
      "venue": "Proc. of NeurIPS",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Longlamp: A benchmark for personalized long-form text generation",
      "authors": [
        "Ishita Kumar",
        "Snigdha Viswanathan",
        "Sushrita Yerra",
        "Alireza Salemi",
        "Ryan A Rossi",
        "Franck Dernoncourt",
        "Hanieh Deilamsalehy",
        "Xiang Chen",
        "Ruiyi Zhang",
        "Shubham Agarwal"
      ],
      "year": "2024",
      "venue": "Longlamp: A benchmark for personalized long-form text generation",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Personalized adaptation via in-context preference learning",
      "authors": [
        "Allison Lau",
        "Younwoo Choi",
        "Vahid Balazadeh",
        "Keertana Chidambaram",
        "Vasilis Syrgkanis",
        "Rahul G Krishnan"
      ],
      "year": "2024",
      "venue": "Personalized adaptation via in-context preference learning",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Aligning to thousands of preferences via system message generalization",
      "authors": [
        "Seongyun Lee",
        "Sue",
        "Hyun Park",
        "Seungone Kim",
        "Minjoon Seo"
      ],
      "year": "2024",
      "venue": "Aligning to thousands of preferences via system message generalization",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Teach llms to personalize-an approach inspired by writing education",
      "authors": [
        "Cheng Li",
        "Mingyang Zhang",
        "Qiaozhu Mei",
        "Yaqing Wang",
        "Amba Spurthi",
        "Yi Hombaiah",
        "Michael Liang",
        "Bendersky"
      ],
      "year": "2023",
      "venue": "Teach llms to personalize-an approach inspired by writing education",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Learning to drive black-box llms with llms",
      "authors": [
        "Changhao Li",
        "Yuchen Zhuang",
        "Rushi Qiang",
        "Haotian Sun",
        "Hanjun Dai",
        "Chao Zhang",
        "Bo Dai",
        "Matryoshka"
      ],
      "year": "2024",
      "venue": "Learning to drive black-box llms with llms",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Learning to rewrite prompts for personalized text generation",
      "authors": [
        "Cheng Li",
        "Mingyang Zhang",
        "Qiaozhu Mei",
        "Weize Kong",
        "Michael Bendersky"
      ],
      "year": "2024",
      "venue": "Proc. of Web Conference",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Hello again! llm-powered personalized agent for long-term dialogue",
      "authors": [
        "Hao Li",
        "Chenghao Yang",
        "An Zhang",
        "Yang Deng",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "year": "2024",
      "venue": "Hello again! llm-powered personalized agent for long-term dialogue",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Fewshot parameter-efficient fine-tuning is better and cheaper than in-context learning",
      "authors": [
        "Haokun Liu",
        "Derek Tam",
        "Mohammed Muqeeth",
        "Jay Mohta",
        "Tenghao Huang",
        "Mohit Bansal",
        "Colin A Raffel"
      ],
      "year": "2022",
      "venue": "Proc. of NeurIPS",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Recap: Retrieval-enhanced contextaware prefix encoder for personalized dialogue response generation",
      "authors": [
        "Shuai Liu",
        "J Hyundong",
        "Marjorie Cho",
        "Xuezhe Freedman",
        "Jonathan Ma"
      ],
      "year": "2023",
      "venue": "Recap: Retrieval-enhanced contextaware prefix encoder for personalized dialogue response generation",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Client-specific hyperbolic federated learning",
      "authors": [
        "Jiahong Liu",
        "Xinyu Fu",
        "Menglin Yang",
        "Weixi Zhang",
        "Rex Ying",
        "Irwin King"
      ],
      "year": "2024",
      "venue": "FedKDD@KDD",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "ONCE: boosting content-based recommendation with both open-and closed-source large language models",
      "authors": [
        "Qijiong Liu",
        "Nuo Chen",
        "Tetsuya Sakai",
        "Xiao-Ming Wu"
      ],
      "year": "2024",
      "venue": "Proc. of WSDM",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Small language models: Survey, measurements, and insights",
      "authors": [
        "Zhenyan Lu",
        "Xiang Li",
        "Dongqi Cai",
        "Rongjie Yi",
        "Fangming Liu",
        "Xiwen Zhang",
        "Nicholas D Lane",
        "Mengwei Xu"
      ],
      "year": "2024",
      "venue": "Small language models: Survey, measurements, and insights",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Memory-assisted prompt editing to improve gpt-3 after deployment",
      "authors": [
        "Aman Madaan",
        "Niket Tandon",
        "Peter Clark",
        "Yiming Yang"
      ],
      "year": "2022",
      "venue": "Proc. of EMNLP",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "On the way to llm personalization: Learning to remember user conversations",
      "authors": [
        "Charlotte Lucie",
        "Katherine Magister",
        "Yizhe Metcalf",
        "Maartje Zhang",
        "Ter Hoeve"
      ],
      "year": "2024",
      "venue": "On the way to llm personalization: Learning to remember user conversations",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Fine-tuning language models with just forward passes",
      "authors": [
        "Sadhika Malladi",
        "Tianyu Gao",
        "Eshaan Nichani",
        "Alex Damian",
        "Jason D Lee",
        "Danqi Chen",
        "Sanjeev Arora"
      ],
      "year": "2023",
      "venue": "Proc. of NeurIPS",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Useridentifier: implicit user representations for simple and effective personalized sentiment analysis",
      "authors": [
        "Fatemehsadat Mireshghallah",
        "Vaishnavi Shrivastava",
        "Milad Shokouhi",
        "Taylor Berg-Kirkpatrick",
        "Robert Sim",
        "Dimitrios Dimitriadis"
      ],
      "year": "2021",
      "venue": "Useridentifier: implicit user representations for simple and effective personalized sentiment analysis",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Personalizing large language model writing assistants with generationcalibrated retrievers",
      "authors": [
        "Sheshera Mysore",
        "Zhuoran Lu",
        "Mengting Wan",
        "Longqi Yang",
        "Steve Menezes",
        "Tina Baghaee",
        "Emmanuel Barajas Gonzalez",
        "Jennifer Neville",
        "Tara Safavi",
        "Pearl"
      ],
      "year": "2023",
      "venue": "Personalizing large language model writing assistants with generationcalibrated retrievers",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "User-llm: Efficient llm contextualization with user embeddings",
      "authors": [
        "Lin Ning",
        "Luyang Liu",
        "Jiaxing Wu",
        "Neo Wu",
        "Devora Berlowitz",
        "Sushant Prakash",
        "Bradley Green",
        "Shawn O 'banion",
        "Jun Xie"
      ],
      "year": "2024",
      "venue": "User-llm: Efficient llm contextualization with user embeddings",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Proc. of NeurIPS",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Pocketllm: Enabling ondevice fine-tuning for personalized llms",
      "authors": [
        "Dan Peng",
        "Zhihui Fu",
        "Jun Wang"
      ],
      "year": "2024",
      "venue": "Pocketllm: Enabling ondevice fine-tuning for personalized llms",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Llm: Harnessing large language models for personalized review generation",
      "authors": [
        "Qiyao Peng",
        "Hongtao Liu",
        "Hongyan Xu",
        "Qing Yang",
        "Minglai Shao",
        "Wenjun Wang"
      ],
      "year": "2024",
      "venue": "Llm: Harnessing large language models for personalized review generation",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Personalized visual instruction tuning",
      "authors": [
        "Renjie Pi",
        "Jianshu Zhang",
        "Tianyang Han",
        "Jipeng Zhang",
        "Rui Pan",
        "Tong Zhang"
      ],
      "year": "2024",
      "venue": "Personalized visual instruction tuning",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Personalizing reinforcement learning from human feedback with variational preference learning",
      "authors": [
        "Sriyash Poddar",
        "Yanming Wan",
        "Hamish Ivison",
        "Abhishek Gupta",
        "Natasha Jaques"
      ],
      "year": "2024",
      "venue": "Personalizing reinforcement learning from human feedback with variational preference learning",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Fdlora: Personalized federated learning of large language model via dual lora tuning",
      "authors": [
        "Jiaxing Qi",
        "Zhongzhi Luan",
        "Shaohan Huang",
        "Carol Fung",
        "Hailong Yang",
        "Depei Qian"
      ],
      "year": "2024",
      "venue": "Fdlora: Personalized federated learning of large language model via dual lora tuning",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Moving towards next-gen rag via memory-inspired knowledge discovery",
      "authors": [
        "Hongjin Qian",
        "Peitian Zhang",
        "Zheng Liu",
        "Kelong Mao",
        "Zhicheng Dou",
        "Memorag"
      ],
      "year": "2024",
      "venue": "Moving towards next-gen rag via memory-inspired knowledge discovery",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Enabling on-device large language model personalization with selfsupervised data selection and synthesis",
      "authors": [
        "Ruiyang Qin",
        "Jun Xia",
        "Zhenge Jia",
        "Meng Jiang",
        "Ahmed Abbasi",
        "Peipei Zhou",
        "Jingtong Hu",
        "Yiyu Shi"
      ],
      "year": "2024",
      "venue": "Proc. of DAC",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Entropy-based decoding for retrieval-augmented large language models",
      "authors": [
        "Zexuan Qiu",
        "Zijing Ou",
        "Bin Wu",
        "Jingjing Li",
        "Aiwei Liu",
        "Irwin King"
      ],
      "year": "2024",
      "venue": "Entropy-based decoding for retrieval-augmented large language models",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "authors": [
        "Rafael Rafailov",
        "Archit Sharma",
        "Eric Mitchell",
        "Christopher D Manning",
        "Stefano Ermon",
        "Chelsea Finn"
      ],
      "year": "2024",
      "venue": "Proc. of NuerIPS",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
      "authors": [
        "Alexandre Rame",
        "Guillaume Couairon",
        "Corentin Dancette",
        "Jean-Baptiste Gaya",
        "Mustafa Shukor",
        "Laure Soulier",
        "Matthieu Cord"
      ],
      "year": "2024",
      "venue": "Proc. of NeurIPS",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Preference distillation for personalized generative recommendation",
      "authors": [
        "Jerome Ramos",
        "Bin Wu",
        "Aldo Lipani"
      ],
      "year": "2024",
      "venue": "Preference distillation for personalized generative recommendation",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Integrating summarization and retrieval for enhanced personalization via large language models",
      "authors": [
        "Chris Richardson",
        "Yao Zhang",
        "Kellen Gillespie",
        "Sudipta Kar",
        "Arshdeep Singh",
        "Zeynab Raeesy",
        "Omar Zia Khan",
        "Abhinav Sethy"
      ],
      "year": "2023",
      "venue": "Integrating summarization and retrieval for enhanced personalization via large language models",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Okapi at trec-3",
      "authors": [
        "Steve Stephen E Robertson",
        "Susan Walker",
        "Micheline M Jones",
        "Mike Hancock-Beaulieu",
        "Gatford"
      ],
      "year": "1995",
      "venue": "Okapi at trec-3",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Comparing retrievalaugmentation and parameter-efficient fine-tuning for privacy-preserving personalization of large language models",
      "authors": [
        "Alireza Salemi",
        "Hamed Zamani"
      ],
      "year": "2024",
      "venue": "Comparing retrievalaugmentation and parameter-efficient fine-tuning for privacy-preserving personalization of large language models",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Lamp: When large language models meet personalization",
      "authors": [
        "Alireza Salemi",
        "Sheshera Mysore",
        "Michael Bendersky",
        "Hamed Zamani"
      ],
      "year": "2023",
      "venue": "Lamp: When large language models meet personalization",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Optimization methods for personalizing large language models through retrieval augmentation",
      "authors": [
        "Alireza Salemi",
        "Surya Kallumadi",
        "Hamed Zamani"
      ],
      "year": "2024",
      "venue": "Proc. of SIGIR",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Beyond retrieval: Generating narratives in conversational recommender systems",
      "authors": [
        "Krishna Sayana",
        "Raghavendra Vasudeva",
        "Yuri Vasilevski",
        "Kun Su",
        "Liam Hebert",
        "Hubert Pham",
        "Ambarish Jash",
        "Sukhdeep Sodhi"
      ],
      "year": "2024",
      "venue": "Beyond retrieval: Generating narratives in conversational recommender systems",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "Decodingtime language model alignment with multiple objectives",
      "authors": [
        "Ruizhe Shi",
        "Yifang Chen",
        "Yushi Hu",
        "Alisa Liu",
        "Hannaneh Hajishirzi",
        "Noah A Smith",
        "Simon S Du"
      ],
      "year": "2024",
      "venue": "Decodingtime language model alignment with multiple objectives",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Personalized pieces: Efficient personalized large language models through collaborative efforts",
      "authors": [
        "Zhaoxuan Tan",
        "Zheyuan Liu",
        "Meng Jiang"
      ],
      "year": "2024",
      "venue": "Personalized pieces: Efficient personalized large language models through collaborative efforts",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Democratizing large language models via personalized parameter-efficient fine-tuning",
      "authors": [
        "Zhaoxuan Tan",
        "Qingkai Zeng",
        "Yijun Tian",
        "Zheyuan Liu",
        "Bing Yin",
        "Meng Jiang"
      ],
      "year": "2024",
      "venue": "Proc. of EMNLP",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "An edge-cloud collaboration framework for generative ai service provision with synergetic big cloud model and small edge models",
      "authors": [
        "Yuqing Tian",
        "Zhaoyang Zhang",
        "Yuzhi Yang",
        "Zirui Chen",
        "Zhaohui Yang",
        "Richeng Jin",
        "Tony Qs Quek",
        "Kai-Kit Wong"
      ],
      "year": "2024",
      "venue": "An edge-cloud collaboration framework for generative ai service provision with synergetic big cloud model and small edge models",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "Two tales of persona in llms: A survey of role-playing and personalization",
      "authors": [
        "Yu-Min Tseng",
        "Yu-Chao Huang",
        "Teng-Yun Hsiao",
        "Yu-Ching Hsu",
        "Jia-Yin Foo",
        "Chao-Wei Huang",
        "Yun-Nung Chen"
      ],
      "year": "2024",
      "venue": "Two tales of persona in llms: A survey of role-playing and personalization",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "Exploring safety-utility trade-offs in personalized language models",
      "authors": [
        "Anvesh Rao Vijjini",
        "Somnath Basu",
        "Roy Chowdhury",
        "Snigdha Chaturvedi"
      ],
      "year": "2024",
      "venue": "Exploring safety-utility trade-offs in personalized language models",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "Personalized collaborative fine-tuning for on-device large language models",
      "authors": [
        "Nicolas Wagner",
        "Dongyang Fan",
        "Martin Jaggi"
      ],
      "year": "2024",
      "venue": "Personalized collaborative fine-tuning for on-device large language models",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "Large language models as source planner for personalized knowledge-grounded dialogue",
      "authors": [
        "Hongru Wang",
        "Minda Hu",
        "Yang Deng",
        "Rui Wang",
        "Fei Mi",
        "Weichao Wang",
        "Yasheng Wang",
        "Wai-Chung Kwan",
        "Irwin King",
        "Kam-Fai Wong"
      ],
      "year": "2023",
      "venue": "Large language models as source planner for personalized knowledge-grounded dialogue",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "Cue-cot: Chain-of-thought prompting for responding to in-depth dialogue questions with llms",
      "authors": [
        "Hongru Wang",
        "Rui Wang",
        "Fei Mi",
        "Yang Deng",
        "Zezhong Wang",
        "Bin Liang",
        "Ruifeng Xu",
        "Kam-Fai Wong"
      ],
      "year": "2023",
      "venue": "Proc. of EMNLP Findings",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "Tpe: Towards better compositional reasoning over cognitive tools via multi-persona collaboration",
      "authors": [
        "Hongru Wang",
        "Huimin Wang",
        "Lingzhi Wang",
        "Minda Hu",
        "Rui Wang",
        "Boyang Xue",
        "Yongfeng Huang",
        "Kam-Fai Wong"
      ],
      "year": "2024",
      "venue": "NLPCC",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "Knowledge editing for large language models: A survey",
      "authors": [
        "Song Wang",
        "Yaochen Zhu",
        "Haochen Liu",
        "Zaiyi Zheng",
        "Chen Chen",
        "Jundong Li"
      ],
      "year": "2024",
      "venue": "ACM Computing Surveys",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "Personalized large language models",
      "authors": [
        "Stanisław Woźniak",
        "Bartłomiej Koptyra",
        "Arkadiusz Janz",
        "Przemysław Kazienko",
        "Jan Kocoń"
      ],
      "year": "2024",
      "venue": "Personalized large language models",
      "doi": ""
    },
    {
      "id": "b73",
      "title": "Personalized response generation via generative split memory network",
      "authors": [
        "Yuwei Wu",
        "Xuezhe Ma",
        "Diyi Yang"
      ],
      "year": "2021",
      "venue": "Proc. of NAACL",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "Fine-grained human feedback gives better rewards for language model training",
      "authors": [
        "Zeqiu Wu",
        "Yushi Hu",
        "Weijia Shi",
        "Nouha Dziri",
        "Alane Suhr",
        "Prithviraj Ammanabrolu",
        "Noah A Smith",
        "Mari Ostendorf",
        "Hannaneh Hajishirzi"
      ],
      "year": "2023",
      "venue": "Proc. of NeurIPS",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "Understanding the role of user profile in the personalization of large language models",
      "authors": [
        "Bin Wu",
        "Zhengyan Shi",
        "Hossein A Rahmani",
        "Varsha Ramineni",
        "Emine Yilmaz"
      ],
      "year": "2024",
      "venue": "Understanding the role of user profile in the personalization of large language models",
      "doi": ""
    },
    {
      "id": "b76",
      "title": "Personalized multimodal large language models: A survey",
      "authors": [
        "Junda Wu",
        "Hanjia Lyu",
        "Yu Xia",
        "Zhehao Zhang",
        "Joe Barrow",
        "Ishita Kumar",
        "Mehrnoosh Mirtaheri",
        "Hongjie Chen",
        "Ryan A Rossi",
        "Franck Dernoncourt"
      ],
      "year": "2024",
      "venue": "Personalized multimodal large language models: A survey",
      "doi": ""
    },
    {
      "id": "b77",
      "title": "Aligning llms with individual preferences via interaction",
      "authors": [
        "Shujin Wu",
        "May Fung",
        "Cheng Qian",
        "Jeonghwan Kim",
        "Dilek Hakkani-Tur",
        "Heng Ji"
      ],
      "year": "2024",
      "venue": "Aligning llms with individual preferences via interaction",
      "doi": ""
    },
    {
      "id": "b78",
      "title": "Continual learning for large language models: A survey",
      "authors": [
        "Tongtong Wu",
        "Linhao Luo",
        "Yuan-Fang Li",
        "Shirui Pan",
        "Thuy-Trang Vu",
        "Gholamreza Haffari"
      ],
      "year": "2024",
      "venue": "Continual learning for large language models: A survey",
      "doi": ""
    },
    {
      "id": "b79",
      "title": "Low-rank adaptation for foundation models",
      "authors": [
        "Menglin Yang",
        "Jialin Chen",
        "Yifei Zhang",
        "Jiahong Liu",
        "Jiasheng Zhang",
        "Qiyao Ma",
        "Harshit Verma",
        "Qianru Zhang",
        "Min Zhou",
        "Irwin King"
      ],
      "year": "2024",
      "venue": "A comprehensive review",
      "doi": ""
    },
    {
      "id": "b80",
      "title": "Federated large language models: Current progress and future directions",
      "authors": [
        "Yuhang Yao",
        "Jianyi Zhang",
        "Junda Wu",
        "Chengkai Huang",
        "Yu Xia",
        "Tong Yu",
        "Ruiyi Zhang",
        "Sungchul Kim",
        "Ryan Rossi",
        "Ang Li"
      ],
      "year": "2024",
      "venue": "Federated large language models: Current progress and future directions",
      "doi": ""
    },
    {
      "id": "b81",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "authors": [
        "Aohan Zeng",
        "Xiao Liu",
        "Zhengxiao Du",
        "Zihan Wang",
        "Hanyu Lai",
        "Ming Ding",
        "Zhuoyi Yang",
        "Yifan Xu",
        "Wendi Zheng",
        "Xiao Xia"
      ],
      "year": "2022",
      "venue": "Glm-130b: An open bilingual pre-trained model",
      "doi": ""
    },
    {
      "id": "b82",
      "title": "Long-term memory for large language models through topic-based vector database",
      "authors": [
        "Yi Zhang",
        "Zhongyang Yu",
        "Wanqi Jiang",
        "Yufeng Shen",
        "Jin Li"
      ],
      "year": "2023",
      "venue": "Proc. of IALP",
      "doi": ""
    },
    {
      "id": "b83",
      "title": "Llm-based medical assistant personalization with shortand long-term memory coordination",
      "authors": [
        "Kai Zhang",
        "Yangyang Kang",
        "Fubang Zhao",
        "Xiaozhong Liu"
      ],
      "year": "2024",
      "venue": "Proc. of NAACL",
      "doi": ""
    },
    {
      "id": "b84",
      "title": "Personalized llm response generation with parameterized memory injection",
      "authors": [
        "Kai Zhang",
        "Lizhi Qing",
        "Yangyang Kang",
        "Xiaozhong Liu"
      ],
      "year": "2024",
      "venue": "Personalized llm response generation with parameterized memory injection",
      "doi": ""
    },
    {
      "id": "b85",
      "title": "Personalized lora for human-centered text understanding",
      "authors": [
        "You Zhang",
        "Jin Wang",
        "Liang-Chih Yu",
        "Dan Xu",
        "Xuejie Zhang"
      ],
      "year": "2024",
      "venue": "Proc. of AAAI",
      "doi": ""
    },
    {
      "id": "b86",
      "title": "A survey on the memory mechanism of large language model based agents",
      "authors": [
        "Zeyu Zhang",
        "Xiaohe Bo",
        "Chen Ma",
        "Rui Li",
        "Xu Chen",
        "Quanyu Dai",
        "Jieming Zhu",
        "Zhenhua Dong",
        "Ji-Rong Wen"
      ],
      "year": "2024",
      "venue": "A survey on the memory mechanism of large language model based agents",
      "doi": ""
    },
    {
      "id": "b87",
      "title": "Memsim: A bayesian simulator for evaluating memory of llm-based personal assistants",
      "authors": [
        "Zeyu Zhang",
        "Quanyu Dai",
        "Luyu Chen",
        "Zeren Jiang",
        "Rui Li",
        "Jieming Zhu",
        "Xu Chen",
        "Yi Xie",
        "Zhenhua Dong",
        "Ji-Rong Wen"
      ],
      "year": "2024",
      "venue": "Memsim: A bayesian simulator for evaluating memory of llm-based personal assistants",
      "doi": ""
    },
    {
      "id": "b88",
      "title": "Personalization of large language models: A survey",
      "authors": [
        "Zhehao Zhang",
        "Ryan A Rossi",
        "Branislav Kveton",
        "Yijia Shao",
        "Diyi Yang",
        "Hamed Zamani",
        "Franck Dernoncourt",
        "Joe Barrow",
        "Tong Yu",
        "Sungchul Kim"
      ],
      "year": "2024",
      "venue": "Personalization of large language models: A survey",
      "doi": ""
    },
    {
      "id": "b89",
      "title": "A survey of large language models",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "doi": ""
    },
    {
      "id": "b90",
      "title": "Useradapter: Few-shot user learning in sentiment analysis",
      "authors": [
        "Wanjun Zhong",
        "Duyu Tang",
        "Jiahai Wang",
        "Jian Yin",
        "Nan Duan"
      ],
      "year": "2021",
      "venue": "Proc. of ACL Findings",
      "doi": ""
    },
    {
      "id": "b91",
      "title": "Less is more: Learning to refine dialogue history for personalized dialogue generation",
      "authors": [
        "Hanxun Zhong",
        "Zhicheng Dou",
        "Yutao Zhu",
        "Hongjin Qian",
        "Ji-Rong Wen"
      ],
      "year": "2022",
      "venue": "Less is more: Learning to refine dialogue history for personalized dialogue generation",
      "doi": ""
    },
    {
      "id": "b92",
      "title": "Beyond onepreference-for-all: Multi-objective direct preference optimization",
      "authors": [
        "Zhanhui Zhou",
        "Jie Liu",
        "Chao Yang",
        "Jing Shao",
        "Yu Liu",
        "Xiangyu Yue",
        "Wanli Ouyang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Beyond onepreference-for-all: Multi-objective direct preference optimization",
      "doi": ""
    },
    {
      "id": "b93",
      "title": "Autopeft: Automatic configuration search for parameterefficient fine-tuning",
      "authors": [
        "Han Zhou",
        "Xingchen Wan",
        "Ivan Vulić",
        "Anna Korhonen"
      ],
      "year": "2024",
      "venue": "Proc. of ACL",
      "doi": ""
    },
    {
      "id": "b94",
      "title": "Lifelong personalized low-rank adaptation of large language models for recommendation",
      "authors": [
        "Jiachen Zhu",
        "Jianghao Lin",
        "Xinyi Dai",
        "Bo Chen",
        "Rong Shan",
        "Jieming Zhu",
        "Ruiming Tang",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "year": "2024",
      "venue": "Lifelong personalized low-rank adaptation of large language models for recommendation",
      "doi": ""
    },
    {
      "id": "b95",
      "title": "Hydra: Model factorization framework for black-box llm personalization",
      "authors": [
        "Yuchen Zhuang",
        "Haotian Sun",
        "Yue Yu",
        "Rushi Qiang",
        "Qifan Wang",
        "Chao Zhang",
        "Bo Dai"
      ],
      "year": "2024",
      "venue": "Hydra: Model factorization framework for black-box llm personalization",
      "doi": ""
    },
    {
      "id": "b96",
      "title": "Tailoring llms to individual preferences",
      "authors": [
        "Andrew Thomas P Zollo",
        "Tung Wei",
        "Naimeng Siah",
        "Ang Ye",
        "Hongseok Li",
        "Namkoong",
        "Personalllm"
      ],
      "year": "2024",
      "venue": "Tailoring llms to individual preferences",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "A Survey Of Personalized Large Language Models:",
      "text": "Progress and Future Directions Jiahong Liu\\({}^{1}\\) Zexuan Qiu\\({}^{1}\\) Zhongyang Li\\({}^{2}\\) Quanyu Dai\\({}^{2}\\) Jieming Zhu\\({}^{2}\\) Minda Hu\\({}^{1}\\) Menglin Yang\\({}^{3}\\) Irwin King\\({}^{1}\\) \\({}^{1}\\)The Chinese University of Hong Kong, \\({}^{2}\\)Huawei Technologies Co., Ltd, \\({}^{3}\\)The Hong Kong University of Science and Technology (Guangzhou) {jhliu22, zxqiu22, mindahu21, king}@cse.cuhk.edu.hk, {lizhongyang6, daquanyu, jimarie.zhu}@huawei.com, menglin.yang@outlook.com"
    },
    {
      "title": "Abstract",
      "text": "Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finentuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the Github Repo."
    },
    {
      "title": "1 Introduction",
      "text": "In recent years, substantial progress has been made in Large Language Models (LLMs) such as GPT, PaLM, LLaMA, DeepSeek, and their variants. These models have demonstrated remarkable versatility, achieving state-of-the-art performance across various natural language processing tasks, including question answering, reasoning, and machine translation [22], with minimal task-specific adaptation. The Necessity of Personalized LLMs (PLLMs)While LLMs excel in general knowledge and multi-domain reasoning, their lack of personalization creates challenges in situations where user-specific understanding is crucial. For instance, conversational agents need to adapt to a user's preferred tone and incorporate past interactions to deliver relevant, personalized responses. 8As LLMs evolve, integrating personalization capabilities has become a promising direction for advancing human-AI interaction across diverse domains such as education, healthcare, and finance [14, 23, 25, 26, 27, 28, 29]. Technical ChallengesDespite its promise, personalizing LLMs presents several challenges. These include efficiently representing and integrating diverse user data, addressing privacy concerns, managing long-term user memories, adaptivity accommodating users' diverse needs and evolving behaviors [15]. Moreover, achieving personalization often requires balancing accuracy and efficiency while addressing biases and maintaining fairness in the generated outputs. ContributionsDespite growing interest, the field of PLLMs lacks a systematic review that consolidates recent advancements. This survey aims to bridge the gap by systematically organizing existing research on PLLMs and offering insights into their methodologies and future directions. The contributions of this survey can be summarized as follows: _(1) A structured taxonomy_: We propose a comprehensive taxonomy, providing a technical perspective on the existing approaches to building PLLMs. _(2) A comprehensive review:_ We systematically review state-of-the-art methods for PLLMs, analyzing Figure 1: Illustration of PLLM techniques for generating personalized responses through three levels: prompting, adaptation, and alignment. fine-grained differences among the methods. _(3) Future directions:_ We highlight current limitations, such as data privacy and bias, and outline promising avenues for future research, including multimodal personalization, edge computing, lifelong updating, trustworthiness, and other emerging challenges."
    },
    {
      "title": "2 Preliminary",
      "text": ""
    },
    {
      "title": "Large Language Models",
      "text": "Large Language Models (LLMs) generally refer to models that utilize the Transformer architecture and are equipped with billions of parameters trained on trillions of text tokens. These models have demonstrated substantial improvements in a myriad of tasks related to natural language understanding and generation, increasingly proving beneficial in assisting human activities. In this work, we mainly focus on autoregressive LLMs, which are based on two main architectures: decoder-only models and encoder-decoder models. Encoder-decoder models such as Flan-T5 [13] and ChatGLM [14] analyze input through the encoder for semantic representations, making them effective in language understanding in addition to generation. Decoder-only LLMs focus on left-to-right generation by predicting the next token in a sequence, with numerous instances [1, 15, 16, 17] under this paradigm achieving breakthroughs in advanced capabilities like instruction following and reasoning. However, these models are typically pre-trained on general-purpose data and lack an understanding of specific user information. As a result, they are unable to generate responses tailored to a user's unique tastes, preferences, and expectations, limiting their effectiveness in personalized applications where user-specific adaptation is critical."
    },
    {
      "title": "Problem Statement",
      "text": "Personalized Large Language Models (PLLMs) generate responses that align with the user's style and expectations, offering diverse answers to the same query for different users [10]. A PLLM is defined as an LLM that generates responses conditioned not only on an input query \\(q\\), but also on a user \\(u\\)'s personalized data \\(\\mathcal{C}_{u}\\). It aims to predict the most probable response sequence \\(y\\) given a query \\(q\\) and the personalized context \\(\\mathcal{C}_{u}\\), such that: \\(y=\\text{argmax}_{y}P(y\\mid q,\\mathcal{C}_{u})\\). The personalized data \\(\\mathcal{C}_{u}\\) may encapsulate information about the user's preferences, history, context, and other user-specific attributes. These can include (Figure 1): * **Profile/Relationship:** User profile, including attributes (e.g., name, gender, occupation), and relationships (e.g., friends, family members), such as \\(\\mathcal{C}_{u}=\\{A,18,\\text{student},\\text{friends}\\colon\\{B,C,D\\}\\dots\\}\\). * **Historical Dialogues:** Historical dialogues, such as question-answer pairs that user \\(u\\) interacts with the LLM (e.g., \\(\\mathcal{C}_{u}=\\{(q_{0},a_{0}),(q_{1},a_{1}),\\dots,(q_{i},a_{i})\\}\\)), where each \\(q_{i}\\) is a query and \\(a_{i}\\) is the corresponding answer. * **Historical Content:** Includes documents, previous reviews, comments or feedback from user \\(u\\). For example, \\(\\mathcal{C}_{u}=\\{\\text{I like _Avatar_ because }\\dots,\\dots\\}\\). * **Historical Interactions:** Includes historical interactions, Figure 2: A taxonomy of PLLMs with representative examples. preferences, ratings from user \\(u\\). For example, \\(\\mathcal{C}_{u}=\\left\\{\\textit{The Lord of the Rings}:5,\\textit{Interstellar}:3\\dots\\right\\}\\). By incorporating personalized data, PLLMs enhance traditional LLMs, improving response generation, recommendations, and classification tasks. **Note that** our survey differs significantly from role-play related LLM personalization [16, 17, 18]. While role-play focuses on mimicking characters during conversations, PLLMs in this survey focus on understanding users' contexts and preferences to meet their specific needs. Compared to [18], which emphasizes broad categories, our work provides a systematic analysis of techniques to enhance PLLM efficiency and performance, with a more detailed technical classification."
    },
    {
      "title": "Proposed Taxonomy",
      "text": "We propose a taxonomy (as illustrated in Figure 1 and Figure 2) from technical perspectives, categorizing the methods for Personalized Large Language Models (PLLMs) into three major levels: (1) **Input level: Personalized Prompting** focuses on handling user-specific data outside the LLM and injecting it into the model. (2) **Model level: Personalized Adaptation** emphasizes designing a framework to efficiently fine-tune or adapt model parameters for personalization. **(3) Objective Level: Personalized Alignment** aims to refine model behavior to align with user preferences effectively. Due to space limitations, analysis papers, datasets, and benchmarks are summarized in the Github Repo."
    },
    {
      "title": "3 Personalized Prompting",
      "text": "Prompt engineering acts as a bridge for interaction between users and LLMs. In this survey, prompting involves guiding an LLM to generate desired outputs using various techniques, from traditional text prompts to advanced methods like soft embedding. Soft embedding can be extended not only through input but also via cross-attention or by adjusting output logits, enabling more flexible and context-sensitive responses. The framework can be expressed as, for each user \\(u\\): \\[y=f_{\\text{LLM}}\\left(q\\oplus\\phi\\left(\\mathcal{C}_{u}\\right)\\right), \\tag{1}\\] where, \\(f_{\\text{LLM}}\\) is the LLM model that generates the response; \\(\\phi\\) is a function that extracts relevant context from the user's personal context \\(\\mathcal{C}_{u};\\oplus\\) represents the combination operator that fuses the query \\(q\\) and the relevant personalized context \\(\\phi(\\mathcal{C}_{u})\\), producing enriched information for the LLM."
    },
    {
      "title": "Profile-Augmented Prompting",
      "text": "Profile-augmented prompting methods explicitly utilize summarized user preferences and profiles in natural language to augment LLMs' input at the token level (\\(\\phi\\) is the _summarizer_ model). Figure 3(a) shows the illustration. Non-tuned SummarizerA frozen LLM can be directly used as the summarizer to summarize user profiles due to its strong language understanding capabilities, i.e., \\(\\phi\\left(\\mathcal{C}_{u}\\right)=f_{\\text{LLM}}\\left(\\mathcal{C}_{u}\\right)\\). For instance, _Cue-CoT_[23] employs chain-of-thought prompting for personalized profile augmentation, using LLMs to extract and summarize user status (e.g., emotion, personality, and psychology) from historical dialogues. _PAG_[11] leverages instruction-tuned LLMs to pre-summarize user profiles based on historical content. The summaries are stored offline, enabling efficient personalized response generation while meeting runtime constraints. _ONCE_[11] prompts closed-source LLMs to summarize topics and regions of interest from users' browsing history, enhancing personalized recommendations. Tuned SummarizerBlack-box LLMs are sensitive to input noise, like off-topic summaries, and struggle to extract relevant information. Thus, training the summarizer to adapt to user preferences and style is essential. _Matryoshka_[12] uses a white-box LLM to summarize user histories, similar to PAG, but fine-tunes the summarizer instead of the generator LLM. _RewriterSIRI_[12] rewrites the query \\(q\\) instead of concatenating summaries, optimized with supervised and reinforcement learning. _CoS_[13] is a special case that assumes a brief user profile \\(\\phi\\left(\\mathcal{C}_{u}\\right)\\) and amplifies its influence in LLM response generation by comparing output probabilities with and without the profile, adjusting personalization without fine-tuning."
    },
    {
      "title": "Retrieval-Augmented Prompting",
      "text": "Retrieval-augmented prompting [1, 17, 18] excels at extracting the most relevant records from user data to enhance PLLMs (See Figure 3(b)). Due to the complexity and volume of user data, many methods use an additional _memory_ for more effective retrieval. Common retrievers including sparse (e.g., BM25 [12]), and dense retrievers (e.g., Faiss [19], Contriever [18]). These methods effectively manage the increasing volume of user data within the LLM's context limit, improving relevance and personalization by integrating key evidence from the user's personalized data."
    },
    {
      "title": "3.2.1 Personalized Memory Construction",
      "text": "This part designs mechanisms for retaining and updating memory to enable efficient retrieval of relevant information. Non-Parametric MemoryThis category maintains a token-based database, storing and retrieving information in its original tokenized form without using parameterized vector representations. For example, _MemPrompt_[15] and _TeachMe_[11] maintain a dictionary-based feedback memory (key-value pairs of mistakes and user feedback). MemPrompt focuses on prompt-based improvements, whereas TeachMe emphasizes continual learning via dynamic memory that adapts over time. _MaLP_[18] further integrates multiple memory types, leveraging working memory for immediate processing, short-term memory (STM) for quick access, and long-term memory (LTM) for storing key knowledge. Parametric MemoryRecent studies parameterize and project personalized user data into a learnable space, with parametric memory filtering out redundant context to reduce noise. For instance, _LD-Agent_[12] maintains memory with separate short-term and long-term banks, encoding long-term events as parametric vector representationsrefined by a tunable module and retrieved via an embedding-based mechanism. _MemoRAG_[14], in contrast, adopts a different approach by utilizing a lightweight LLM as memory to learn user-personalized data. Instead of maintaining a vector database for retrieval, it generates a series of tokens as a draft to further guide the retriever, offering a more dynamic and flexible method for retrieval augmentation."
    },
    {
      "title": "3.2.2 Personalized Memory Retrieval",
      "text": "The key challenge in the personalized retriever design lies in selecting not only relevant but also representative personalized data for downstream tasks. _LaMP_[15] investigates how retrieved personalized information affects the responses of large language models (LLMs) through two mechanisms: in-prompt augmentation (IPA) and fusion-in-decoder (FiD). _PEARL_[11] and _ROPG_[15] similarly aim to enhance the retriever using personalized generation-calibrated metrics, improving both personalization and text quality of retrieved documents. Meanwhile, _HYDRA_[13] trains a reranker to prioritize the most relevant information additionally from top-retrieved historical records for enhanced personalization."
    },
    {
      "title": "Soft-Fused Prompting",
      "text": "Soft prompting differs from profile-augmented prompting by compressing personalized data into soft embeddings, rather than summarizing it into discrete tokens. These embeddings are generated by a user feature _encoder_\\(\\phi\\). In this survey, we generalize the concept of soft prompting, showing that soft embeddings can be integrated (combination operator \\(\\oplus\\)) not only through the input but also via cross-attention or by adjusting output logits, allowing for more flexible and context-sensitive responses (See Figure 3(c)). Input PrefixSoft prompting, used as an input prefix, focuses on the embedding level by concatenating the query embedding with the soft embedding, and is commonly applied in recommendation tasks. _UEM_[11] is a user embedding module (transformer network) that generates a soft prompt conditioned on the user's personalized data. _PERSOMA_[10] enhances UEM by employing resampling, selectively choosing a subset of user interactions based on relevance and importance. _REGEN_[17] combines item embeddings from user-item interactions via collaborative filtering and item descriptions using a soft prompt adapter to generate contextually personalized responses. _PeaPOD_[12] personalizes soft prompts by distilling user preferences into a limited set of learnable, dynamically weighted prompts. Unlike previously mentioned methods, which focus on directly embedding user interactions or resampling relevant data, PeaPOD adapts to user interests by weighting a shared set of prompts. Cross-AttentionCross-attention enables the model to process and integrate multiple input sources by allowing it to attend to personalized data and the query. _User-LLM_[12] uses an autoregressive user encoder to convert historical interactions into embeddings through self-supervised learning, which are then integrated via cross-attention. The system employs joint training to optimize both the retriever and generator for better performance. _RECAP_[13] utilizes a hierarchical transformer retriever designed for dialogue domains to fetch personalized information. This information is integrated into response generation via a context-aware prefix encoder, improving the model's ability to generate personalized, contextually relevant responses. Output Logits_GSMN_[15] retrieves relevant information from personalized data, encodes it into soft embeddings, and uses them in attention with the query vector. Afterward, the resulting embeddings are concatenated with the LLM-generated embeddings, modifying the final logits to produce more personalized and contextually relevant responses."
    },
    {
      "title": "Discussions",
      "text": "The three prompting methods have distinct pros and cons: 1) Profile-augmented prompting improves efficiency by compressing historical data but risks information loss and reduced personalization. 2) Retrieval-augmented prompting offers rich, context-aware inputs and scales well for long-term memory but can suffer from computational limits and irrelevant data retrieval. 3) Soft prompting efficiently embeds user-specific info, capturing semantic nuances without redundancy, but is limited to black-box models and lacks explicit user preference analysis. Overall, prompting-based methods are efficient and adaptable, and enable dynamic personalization with minimal computational overhead. However, they lack deeper personalization analysis as they rely on predefined prompt structures to inject user-specific information and are limited in accessing global knowledge due to the narrow scope of prompts."
    },
    {
      "title": "4 Personalized Adaptation",
      "text": "PLLMs require balancing fine-tuning's deep adaptability with the efficiency of prompting. Therefore, specialized methods Figure 3: The illustration of personalized prompting approaches: **a) Profile-Augmented, b) Retrieval-Augmented, c) Soft-Fused.** need to be specifically designed for PLLMs to address these challenges utilizing parameter-efficient fine-tuning methods (PEFT), such as LoRA [12, 13], IA3 [14], etc. (See Figure 4)."
    },
    {
      "title": "One Peft All Users",
      "text": "This method trains on all users' data using a _shared PEFT module_, eliminating the need for separate modules per user. The shared module's architecture can be further categorized. **Single PEFT**_PLoRA_[13] and _LM-P_[21] utilize LoRA for PEFT of LLM, injecting personalized information via user embeddings and user IDs, respectively. PLoRA is further extended and supports online training and prediction for cold-start scenarios. _UserIdentifier_[11] uses a static, non-trainable user identifier to condition the model on user-specific information, avoiding the need for trainable user-specific parameters and reducing training costs. _Review-LLM_[15] aggregates users' historical behaviors and ratings into prompts to guide sentiment and leverages LoRA for efficient fine-tuning. However, these methods rely on a single architecture with fixed configurations (e.g., hidden size, insertion layers), making them unable to store and activate diverse information for personalization [22]. To solve this problem, _MiLP_[13] utilizes a Bayesian optimization strategy to automatically identify the optimal configuration for applying multiple LoRA modules, enabling efficient and flexible personalization. **Mixture of Experts (MoE)** Several methods use the LoRA module, but with a static configuration for all users. This lack of parameter personalization limits adaptability to user dynamics and preference shifts, potentially resulting in suboptimal performance [10]. _RecLoRA_[13] addresses this limitation by maintaining a set of parallel, independent LoRA weights and employing a soft routing method to aggregate meta-LoRA weights, enabling more personalized and adaptive results. Similarly, _iLoRA_[12] creates a diverse set of experts (LoRA) to capture specific aspects of user preferences and generates dynamic expert participation weights to adapt to user-specific behaviors. Shared PEFT methods rely on a centralized approach, where user-specific data is encoded into a shared adapter by centralized LLMs. This limits the model's ability to provide deeply personalized experiences tailored to individual users. Furthermore, using a centralized model often requires users to share personal data with service providers, raising concerns about the storage, usage, and protection of this data."
    },
    {
      "title": "One Peft Per User",
      "text": "Equipping _a user-specific PEFT module_ makes LLM deployment more personalized while preserving data privacy. However, the challenge lies in ensuring efficient operation in resource-limited environments, as users may lack sufficient local resources to perform fine tuning. **No Collaboration** There is no collaboration or coordination between adapters or during the learning process for each use in this category. _UserAdapter_[13] personalizes models through prefix-tuning, fine-tuning a unique prefix vector for each user while keeping the underlying transformer model shared and frozen. _PocketLLM_[15] utilizes a derivative-free optimization approach, based on MeZo [17], to fine-tune LLMs on memory-constrained mobile devices. _OPPU_[13] equips each user with a LoRA module. **Collaborative Efforts** The \"one-PEFT-per-user\" paradigm without collaboration is computationally and storage-intensive, particularly for large user bases. Additionally, individually owned PEFTs hinder community value, as personal models cannot easily share knowledge or benefit from collaborative improvements. _PER-PCS_[14] enables efficient and collaborative PLLMs by sharing a small fraction of PEFT parameters across users. It first divides PEFT parameters into reusable pieces with routing gates and stores them in a shared pool. For each target user, pieces are autoregressively selected from other users, ensuring scalability, efficiency, and personalized adaptation without additional training. Another efficient collaborative strategy is based on the federated learning (FL) framework. For example, Wagner _et al._[20] introduces a FL framework for on-device LLM fine-tuning, using strategies to aggregate LoRA model parameters and handle data heterogeneity efficiently, outperforming purely local fine-tuning. _FDLoRA_[12] introduces a personalized FL framework using dual LoRA modules to capture personalized and global knowledge. It shares only global LoRA parameters with a central server and combines Figure 4: The illustration of personalized adaptation approaches: **a) One PEFT for all users, b) One PEFT per user.** them via adaptive fusion, enhancing performance while minimizing communication and computing costs. There are other frameworks that can be explored, such as _HYDRA_[22], which also employs a base model to learn shared knowledge. However, in contrast to federated learning, it assigns distinct heads to each individual user to extract personalized information."
    },
    {
      "title": "Discussions",
      "text": "Fine-tuning methods enable deep personalization by modifying a large set of model parameters, and parameter-efficient fine-tuning methods (e.g., prefix vectors or adapters) reduce computational cost and memory requirements while maintaining high personalization levels. These methods improve task adaptation by tailoring models to specific user needs, enhancing performance in tasks like sentiment analysis and recommendations. They also offer flexibility, allowing user-specific adjustments while leveraging pre-trained knowledge. However, they still face the risk of overfitting, particularly with limited or noisy user data, which can impact generalization and performance for new or diverse users."
    },
    {
      "title": "5 Personalized Alignment",
      "text": "Alignment techniques [14, 23] typically optimize LLMs to match the generic preferences of humans. However, in reality, individuals may exhibit significant variations in their preferences for LLM responses across different dimensions like language style, knowledge depth, and values. Personalized alignment seeks to further align with individual users' unique preferences beyond generic preferences. A significant challenge in personalized alignment is creating high-quality user-specific preference datasets, which are more complex than general alignment datasets due to data sparsity. The second challenge arises from the need to refine the canonical RLHF framework [15] to handle the diversification of user preferences, which is essential for integrating personalized preferences without compromising efficiency and performance."
    },
    {
      "title": "Personalized Alignment Data Construction",
      "text": "High-quality data construction is critical for learning PLLMs, primarily involving self-generated data through interactions with the LLM. Wu _et al._[20] constructs a dataset for aligning LLMs with individual preferences by initially creating a diverse pool of 3,310 user personas, which are expanded through iterative self-generation and filtering. This method is similar to _PLUM_[23] that both simulate dynamic interactions through multi-turn conversation trees, allowing LLMs to infer and adapt to user preferences. To enable LLMs to adapt to individual user preferences without re-training, Lee _et al._[20] utilizes diverse system messages as meta-instructions to guide the models' behavior. To support this, the MULTIFACETED COLLECTION dataset is created, comprising 197k system messages that represent a wide range of user values. To facilitate real-time, privacy-preserving personalization on edge devices while addressing data privacy, limited storage, and minimal user disruption, Qin _et al._[20] introduces a self-supervised method that efficiently selects and synthesizes essential user data, improving model adaptation with minimal user interaction. Research efforts are also increasingly concentrating on developing datasets that assess models' comprehension of personalized preferences. Kirk _et al._[20] introduces _PRISM Alignment Dataset_ that maps the sociodemographics and preferences of 1,500 participants from 75 countries to their feedback in live interactions with 21 LLMs, focusing on subjective and multicultural perspectives on controversial topics. _PersonalLLM_[21] introduces a novel personalized testdb, which curates open-ended prompts and multiple high-quality responses to simulate diverse latent preferences among users. It generates simulated user bases with varied preferences from pre-trained reward models, addressing the challenge of data sparsity in personalization."
    },
    {
      "title": "Personalized Alignment Optimization",
      "text": "Personalized preference alignment is usually modeled as a multi-objective reinforcement learning (MORL) problem, where personalized preference is determined as the user-specific combination of multi-preference dimensions. Based on this, a typical alignment paradigm involves using a personalized reward derived from multiple reward models to guide during the training phase of policy LLMs, aiming for personalization. _MORLHF_[24] separately trains reward models for each dimension and retrains the policy language models using proximal policy optimization, guided by a linear combination of these multiple reward models. This approach allows for the reuse of the standard RLHF pipeline. _MODPO_[22] introduces a novel RL-free algorithm extending Direct Preference Optimization (DPO) for managing multiple alignment objectives. It integrates linear scalarization directly into the reward modeling process, allowing it to train language models via a simple margin-based cross-entropy loss as implicit collective rewards functions. Another strategy for MORL is to consider ad-hoc combinations of multiple trained policy LLMs during the decoding phase to achieve personalization. _Personalized Soups_[19] Figure 5: The illustration of personalized alignment method under the multi-objective reinforcement learning paradigm. _et al._, 2023] and _Reward Soups_[Rame _et al._, 2024] address the challenge of RL from personalized human feedback by first training multiple policy models with distinct preferences independently and then merging their parameters post-hoc during inference. Both methods allow for dynamic weighting of the networks based on user preferences, enhancing model alignment and reducing reward misspecification. Also, the personalized fusion of policy LLMs can be achieved not only through parameter merging but also through model ensembling. _MOD_[Shi _et al._, 2024] outputs the next token from a linear combination of all base models, allowing for precise control over different objectives by combining their predictions without the need for retraining. The method demonstrates significant effectiveness when compared to the parameter-merging baseline. _PAD_[Chen _et al._, 2024b] leverages a personalized reward modeling strategy to generate token-level personalized rewards, which are then used to guide the decoding process, allowing dynamic tailoring of the base model's predictions to individual preferences. Figure 5 visualizes above typical approaches of MORL for personalized alignment. There are some other emerging personalized alignment studies beyond the \"multi-objective\" paradigm. _PPT_[Lau _et al._, 2024] unlocks the potential of in-context learning for scalable and efficient personalization by generating two potential responses for each user prompt, asking the user to rank them, and incorporating this feedback into the model's context to dynamic adapt to individual preferences over time. _VPL_[Podard _et al._, 2024] utilizes a variational inference framework to capture diverse human preferences through user-specific latent variables. Inferring user-specific latent distributions from a few preference annotations enables more accurate and personalized reward modeling with better data efficiency."
    },
    {
      "title": "Discussions",
      "text": "Current mainstream personalized alignment technologies mainly model personalization as multi-objective reinforcement learning problems, where personalized user preferences are taken into account during the training phase of policy LLMs via canonical RLHF, or the decoding phase of policy LLM via parameter merging or model ensembling. Typically, these methods are limited to a small number (e.g., three) of predefined preference dimensions, represented through textual user preference prompts. However, in real-world scenarios, there could be a large number of personalized users, and their preference vectors may not be known, with only their interaction history accessed. Consequently, developing more realistic alignment benchmarks to effectively assess these techniques is a critical area for future research."
    },
    {
      "title": "6 Future Directions",
      "text": "Despite recent advances in PLLMs, challenges and opportunities remain. This section discusses key limitations and promising future research directions. Complex User DataWhile current approaches effectively handle basic user preferences, processing complex, multi-source user data remains a significant challenge. For example, methods that use user relationships in graph-like structures are still limited to retrieval augmentation [Du _et al._, 2024]. How to effectively leverage this complex user information to fine-tune LLM parameters remains a significant challenge. Most methods focus on text data, while personalized foundation models for multimodal data (e.g., images, videos, audio) remain underexplored, despite their significance for real-world deployment and applications [Wu _et al._, 2024, 2024]. Edge ComputingA key challenge in edge computing is efficiently updating models on resource-constrained devices (e.g., phones), where storage and computational resources are limited. For example, fine-tuning offers deeper personalization but is resource-intensive and hard to scale, especially in real-time applications. Balancing resources with personalization needs is important. A potential solution is to build personalized small models [Lu _et al._, 2024] for edge devices, using techniques like quantization and distillation. Edge-Cloud CollaborationThe deployment of PLLMs in real-world scenarios encounters significant challenges in edge-cloud computing environments. Current approaches utilizing collaborative efforts often lack efficient synchronization mechanisms between cloud and edge devices. This highlights the need to explore the balance between local computation and cloud processing for PLLMs [Tian _et al._, 2024]. Efficient Adaptation to Model UpdatesWhen the base LLM parameters are updated, such as with a new version, efficiently adapting the fine-tuned PEFT parameters for each user becomes a challenge. Given the large volume of user data and limited resources, the cost of retraining can be prohibitive. Future research should focus on efficient strategies for updating user-specific parameters without requiring complete retraining, such as leveraging incremental learning, transfer learning, or more resource-efficient fine-tuning techniques. Lifelong UpdatingGiven the large variety of user behaviors, a key challenge is preventing catastrophic forgetting while ensuring the efficient update of long-term and short-term of the memory. Future research could explore continual learning [Wu _et al._, 2024] and knowledge editing [Wang _et al._, 2024b] to facilitate dynamic updates of user-specific information. TrustworthEnsuring user privacy is crucial, especially when summarized or retrieved data is used to generate personalized responses. Since LLMs cannot be deployed locally due to resource limits, there is a risk of privacy leakage. Future research could focus on privacy-preserving methods like federated learning, secure computation, and differential privacy to protect user data [Yao _et al._, 2024, Liu _et al._, 2024a]."
    },
    {
      "title": "7 Conclusions",
      "text": "This survey offers a thorough overview of Personalized Large Language Models (PLLMs), emphasizing personalized responses tailored to individual user data. We introduce a structured taxonomy that categorizes existing approaches into three key technical perspectives: Personalized Prompting (Input Level), Personalized Adaptation (Model Level), and Personalized Alignment (Objective Level), with further subdivisions within each. We also discuss current limitations and propose several promising directions for future research. Our work provides valuable insights and a framework to drive the advancement of PLLMs development."
    },
    {
      "title": "References",
      "text": "* [1]S. Au, C. J. Dimacali, O. Pedriappagari, N. Park, F. Dernoncourt, Y. Wang, N. Kanakaris, H. Deilamsalehy, R. A. Rossi, and N. K. Ahmed (2025) Personalized graph-based retrieval for large language models. arXiv:2501.02157. Cited by: SS1. * [2]Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. (2022) Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv:2204.05862. Cited by: SS1. * [3]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Proc. of NeurIPS. Cited by: SS1. * [4]W. Cai, J. Jiang, F. Wang, J. Tang, S. Kim, and J. Huang (2024) A survey on mixture of experts. arXiv:2407.06204. Cited by: SS1. * [5]J. Chen, Z. Liu, X. Huang, C. Wu, Q. Liu, G. Jiang, Y. Pu, Y. Lei, X. Chen, X. Wang, et al. (2024) When large language models meet personalization: perspectives of challenges and opportunities. World Wide Web27 (4), pp. 42. Cited by: SS1. * [6]R. Chen, X. Zhang, M. Luo, W. Chai, and Z. Liu (2024) Pad: personalized alignment of l1ms at decoding-time. arXiv:2410.04070. Cited by: SS1. * [7]A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2022) Palm: scaling language modeling with pathways. arXiv:2204.02311. Cited by: SS1. * [8]H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, et al. (2022) Scaling instruction-finetuned language models. arXiv:2210.11416. Cited by: SS1. * [9]C. Clarke, Y. Heng, L. Tang, and J. Mars (2024) Perf-u: parameter-efficient fine-tuning for user personalization. arXiv:2407.18078. Cited by: SS1. * [10]B. Dalvi, O. Tafjord, and P. Clark (2022) Towards teachable reasoning systems: using a dynamic memory of user feedback for continual system improvement. In Proc. of EMNLP, Cited by: SS1. * [11]S. Doddapaneni, K. Sayana, A. Jash, S. Sodhi, and D. Kuzmin (2024) User embedding model for personalized language prompting. arXiv:2401.04858. Cited by: SS1. * [12]S. Doddapaneni, K. Sayana, A. Jash, S. Sodhi, and D. Kuzmin (2024) User embedding model for personalized language prompting. arXiv:2401.04858. Cited by: SS1. * [13]Y. Du, H. Wang, Z. Zhao, B. Liang, B. Wang, W. Zhong, Z. Wang, and K. W. (2024) Perf1qa: a personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering. arXiv:2402.16288. Cited by: SS1. * [14]W. Fan, Y. Ding, L. Ning, S. Wang, H. Li, D. Yin, T. Chua, and Q. Li (2024) A survey on rag meeting lms: towards retrieval-augmented large language models. In Proc. of KDD, Cited by: SS1. * [15]Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang (2023) Retrieval-augmented generation for large language models: a survey. arXiv:2312.10997. Cited by: SS1. * [16]D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. (2025) Deepseek-r1: incentivizing reasoning capability in l1ms via reinforcement learning. arXiv:2501.12948. Cited by: SS1. * [17]J. Shi, Y. He, S. Pandey, M. L. Schrum, and A. Dragan (2024) Cos: enhancing personalization and mitigating bias with context steering. arXiv:2405.01768. Cited by: SS1. * [18]L. Hebert, K. Sayana, A. Jash, A. Karatzoglou, S. Sodhi, S. Doddapaneni, Y. Cai, and D. Kuzmin (2024) Persoma: personalized soft prompt adapter architecture for personalized language prompting. CoRR. Cited by: SS1. * [19]E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen (2021) Lora: low-rank adaptation of large language models. arXiv:2106.09685. Cited by: SS1. * [20]M. Hu, L. Zong, H. Wang, J. Zhou, J. Li, Y. Gao, K. Wong, Y. Li, and I. King (2024) SeRTS: self-rewarding tree search for biomedical retrieval-augmented generation. In Proc. of EMNLP Findings, Cited by: SS1. * [21]G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave (2021) Unsupervised dense information retrieval with contrastive learning. arXiv:2112.09118. Cited by: SS1. * [22]J. Jang, S. Kim, B. Y. Lin, Y. Wang, J. Hessel, L. Zettlemoyer, H. Hajishirzi, Y. Choi, and P. Ammanabrolu (2023) Personalized soups: personalized large language model alignment via post-hoc parameter merging. arXiv:2310.11564. Cited by: SS1. * [23]J. Johnson, M. Douze, and H. Jegou (2019) Billion-scale similarity search with gpus. IEEE Transactions on Big Data. Cited by: SS1. * [24]H. R. Kirk, A. Whitefield, P. Rottger, A. Bean, K. Margatina, J. Ciro, R. Mosquera, M. Bartolo, A. Williams, H. He, et al. (2024) The prism alignment project: what participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. arXiv:2404.16019. Cited by: SS1. * [25]X. Kong, J. Wu, A. Zhang, L. Sheng, H. Lin, X. Wang, and X. He (2024) Customizing language models with instance-wise lora for sequential recommendation. In Proc. of NeurIPS, Cited by: SS1. * [26]I. Kumar, S. Viswanathan, S. Yerra, A. Salemi, R. A. Rossi, F. Dernoncourt, H. Deilamsalehy, X. Chen, R. Zhang, S. Agarwal,et al. Longlamp: A benchmark for personalized long-form text generation. _arXiv:2407.11016_, 2024. * Lau et al. (2024) Allison Lau, Younwoo Choi, Vahid Balazadeh, Keertana Chidambaram, Vasilis Syrgkanis, and Rahul G Krishnan. Personalized adaptation via in-context preference learning. _arXiv:2410.14001_, 2024. * Lee et al. (2024) Seongyun Lee, Sue Hyun Park, Seungone Kim, and Minjoon Seo. Aligning to thousands of preferences via system message generalization. _arXiv:2405.17977_, 2024. * Li et al. (2023) Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang, and Michael Bendersky. Teach llms to personalize-an approach inspired by writing education. _arXiv:2308.07968_, 2023. * Li et al. (2024) Changhao Li, Yuchen Zhuang, Rushi Qiang, Haotian Sun, Hanjun Dai, Chao Zhang, and Bo Dai. Matryoshka: Learning to drive black-box llms with llms. _arXiv:2410.20749_, 2024. * Li et al. (2024) Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael Bendersky. Learning to rewrite prompts for personalized text generation. In _Proc. of Web Conference_, 2024. * Li et al. (2024) Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. Hello again! llm-powered personalized agent for long-term dialogue. _arXiv:2406.05925_, 2024. * Liu et al. (2022) Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. _Proc. of NeurIPS_, 2022. * Liu et al. (2023) Shuai Liu, Hyundong J Cho, Marjorie Freedman, Xuezhe Ma, and Jonathan May. Recap: Retrieval-enhanced context-aware prefix encoder for personalized dialogue response generation. _arXiv:2306.07206_, 2023. * Liu et al. (2024) Jiahong Liu, Xinyu Fu, Menglin Yang, Weixi Zhang, Rex Ying, and Irwin King. Client-specific hyperbolic federated learning. In _FedKDD@KDD_, 2024. * Liu et al. (2024) Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. ONCE: boosting content-based recommendation with both open- and closed-source large language models. In _Proc. of WSDM_, 2024. * Lu et al. (2024) Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D Lane, and Mengwei Xu. Small language models: Survey, measurements, and insights. _arXiv:2409.15790_, 2024. * Madaan et al. (2022) Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve gpt-3 after deployment. In _Proc. of EMNLP_, 2022. * Magister et al. (2024) Lucie Charlotte Magister, Katherine Metcalf, Yizhe Zhang, and Maartje ter Hoeve. On the way to llm personalization: Learning to remember user conversations. _arXiv:2411.13405_, 2024. * Malladi et al. (2023) Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward passes. _Proc. of NeurIPS_, 2023. * Mireshghallah et al. (2021) Fatemehsadat Mireshghallah, Vaishnavi Shrivastava, Milad Shokouhi, Taylor Berg-Kirkpatrick, Robert Sim, and Dimitrios Dimitriadis. Useridentifier: implicit user representations for simple and effective personalized sentiment analysis. _arXiv:2110.00135_, 2021. * Mysore et al. (2023) Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, and Tara Safavi. Pearl: Personalizing large language model writing assistants with generation-calibrated retrievers. _arXiv:2311.09180_, 2023. * Ning et al. (2024) Lin Ning, Luyang Liu, Jiaxing Wu, Neo Wu, Devora Berlowitz, Sushant Prakash, Bradley Green, Shawn O'Banion, and Jun Xie. User-llm: Efficient llm contextualization with user embeddings. _arXiv:2402.13598_, 2024. * Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Proc. of NeurIPS_, 2022. * Peng et al. (2024) Dan Peng, Zhihui Fu, and Jun Wang. Pocketllm: Enabling on-device fine-tuning for personalized llms. _arXiv:2407.01031_, 2024. * Peng et al. (2024) Qiyao Peng, Hongtao Liu, Hongyan Xu, Qing Yang, Minglai Shao, and Wenjun Wang. Llm: Harnessing large language models for personalized review generation. _arXiv:2407.07487_, 2024. * Pi et al. (2024) Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, and Tong Zhang. Personalized visual instruction tuning. _arXiv:2410.07113_, 2024. * Poddar et al. (2024) Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishe Gupta, and Natasha Jaques. Personalizing reinforcement learning from human feedback with variational preference learning. _arXiv:2408.10075_, 2024. * Qi et al. (2024) Jiaxing Qi, Zhongzhi Luan, Shaohan Huang, Carol Fung, Hailong Yang, and Depei Qian. Fdlora: Personalized federated learning of large language model via dual lora tuning. _arXiv:2406.07925_, 2024. * Qian et al. (2024) Hongjin Qian, Peitian Zhang, Zheng Liu, Kelong Mao, and Zhicheng Dou. Memorag: Moving towards next-gen rag via memory-inspired knowledge discovery. _arXiv:2409.05591_, 2024. * Qin et al. (2024) Ruiyang Qin, Jun Xia, Zheng Jia, Meng Jiang, Ahmed Abbasi, Peipei Zhou, Jingtong Hu, and Yiyu Shi. Enabling on-device large language model personalization with self-supervised data selection and synthesis. In _Proc. of DAC_, 2024. * Qiu et al. (2024) Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, and Irwin King. Entropy-based decoding for retrieval-augmented large language models. _arXiv:2406.17519_, 2024. * Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _Proc. of NuerIPS_, 36, 2024. * Rame et al. (2020) Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. _Proc. of NeurIPS_, 2024. * Ramos et al. [2024] Jerome Ramos, Bin Wu, and Aldo Lipani. Preference distillation for personalized generative recommendation. _arXiv:2407.05033_, 2024. * Richardson et al. [2023] Chris Richardson, Yao Zhang, Kellen Gillespie, Sudipta Kar, Arshdeep Singh, Zeynab Raeesy, Omar Zia Khan, and Abhinav Sethy. Integrating summarization and retrieval for enhanced personalization via large language models. _arXiv:2310.20081_, 2023. * Robertson et al. [1995] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at trec-3. _Nist Special Publication Sp_, 1995. * Salemi and Zamani [2024] Alireza Salemi and Hamed Zamani. Comparing retrieval-augmentation and parameter-efficient fine-tuning for privacy-preserving personalization of large language models. _arXiv:2409.09510_, 2024. * Salemi et al. [2023] Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. Lamp: When large language models meet personalization. _arXiv:2304.11406_, 2023. * Salemi et al. [2024] Alireza Salemi, Surya Kallumadi, and Hamed Zamani. Optimization methods for personalizing large language models through retrieval augmentation. In _Proc. of SIGIR_, 2024. * Sayana et al. [2024] Krishna Sayana, Raghavendra Vasudeva, Yuri Vasilevski, Kun Su, Liam Hebert, Hubert Pham, Ambarish Jash, and Sukhdeep Sodhi. Beyond retrieval: Generating narratives in conversational recommender systems. _arXiv:2410.16780_, 2024. * Shi et al. [2024] Ruizhe Shi, Yifang Chen, Yushi Hu, Alisa Liu, Hannaneh Hajishirzi, Noah A Smith, and Simon S Du. Decoding-time language model alignment with multiple objectives. _arXiv:2406.18853_, 2024. * Tan et al. [2024] Zhaoxuan Tan, Zheyuan Liu, and Meng Jiang. Personalized pieces: Efficient personalized large language models through collaborative efforts. _arXiv:2406.10471_, 2024. * Tan et al. [2024] Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, and Meng Jiang. Democratizing large language models via personalized parameter-efficient fine-tuning. In _Proc. of EMNLP_, 2024. * Tian et al. [2024] Yuqing Tian, Zhaoyang Zhang, Yuzhi Yang, Zirui Chen, Zhaohui Yang, Richeng Jin, Tony QS Quek, and Kai-Kit Wong. An edge-cloud collaboration framework for generative ai service provision with synergetic big cloud model and small edge models. _arXiv:2401.01666_, 2024. * Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv:2302.13971_, 2023. * Tseng et al. [2024] Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Yu-Ching Hsu, Jia-Yin Foo, Chao-Wei Huang, and Yun-Nung Chen. Two tales of persona in llms: A survey of role-playing and personalization. _arXiv:2406.01171_, 2024. * Vijimi et al. [2024] Anvesh Rao Vijimi, Somnath Basu Roy Chowdhury, and Snigdha Chaturvedi. Exploring safety-utility trade-offs in personalized language models. _arXiv:2406.11107_, 2024. * Wagner et al. [2024] Nicolas Wagner, Dongyang Fan, and Martin Jaggi. Personalized collaborative fine-tuning for on-device large language models. _arXiv:2404.09753_, 2024. * Wang et al. [2023] Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Wai-Chung Kwan, Irwin King, and Kam-Fai Wong. Large language models as source planner for personalized knowledge-grounded dialogue. _arXiv:2310.08840_, 2023. * Wang et al. [2023] Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong Wang, Bin Liang, Ruifeng Xu, and Kam-Fai Wong. Cue-cot: Chain-of-thought prompting for responding to in-depth dialogue questions with llms. In _Proc. of EMNLP Findings_, 2023. * Wang et al. [2024] Hongru Wang, Huimin Wang, Lingzhi Wang, Minda Hu, Rui Wang, Boyang Xue, Yongfeng Huang, and Kam-Fai Wong. Tpe: Towards better compositional reasoning over cognitive tools via multi-persona collaboration. In _NLPCC_, pages 281-294. Springer, 2024. * Wang et al. [2024] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge editing for large language models: A survey. _ACM Computing Surveys_, 57(3):1-37, 2024. * Wozniak et al. [2024] Stanislaw Wozniak, Bartlomiej Koptyra, Arkadiusz Janz, Przemyslaw Kazienko, and Jan Kocon. Personalized large language models. _arXiv:2402.09269_, 2024. * Wu et al. [2021] Yuwei Wu, Xuezhe Ma, and Diyi Yang. Personalized response generation via generative split memory network. In _Proc. of NAACL_, 2021. * Wu et al. [2023] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. _Proc. of NeurIPS_, 2023. * Wu et al. [2024] Bin Wu, Zhengyan Shi, Hossein A Rahmani, Varsha Ramineni, and Emine Yilmaz. Understanding the role of user profile in the personalization of large language models. _arXiv:2406.17803_, 2024. * Wu et al. [2024] Junda Wu, Hanjia Lyu, Yu Xia, Zhehao Zhang, Joe Barrow, Ishita Kumar, Mehrnoosh Mirtaheri, Hongjie Chen, Ryan A Rossi, Franck Dernoncourt, et al. Personalized multimodal large language models: A survey. _arXiv:2412.02142_, 2024. * Wu et al. [2024] Shujin Wu, May Fung, Cheng Qian, Jeonghwan Kim, Dilek Hakkani-Tur, and Heng Ji. Aligning llms with individual preferences via interaction. _arXiv:2410.03642_, 2024. * Wu et al. [2024] Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. Continual learning for large language models: A survey. _arXiv:2402.01364_, 2024. * Yang et al. [2024] Menglin Yang, Jialin Chen, Yifei Zhang, Jiahong Liu, Jiasheng Zhang, Qiyao Ma, Harshit Verma, Qianru Zhang, Min Zhou, Irwin King, et al. Low-rank adaptation for foundation models: A comprehensive review. _arXiv:2501.00365_, 2024. Yuhang Yao, Jianyi Zhang, Junda Wu, Chengkai Huang, Yu Xia, Tong Yu, Ruiyi Zhang, Sungchul Kim, Ryan Rossi, Ang Li, et al. Federated large language models: Current progress and future directions. _arXiv:2409.15723_, 2024. * Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. GIm-130b: An open bilingual pre-trained model. _arXiv:2210.02414_, 2022. * Zhang et al. (2023) Yi Zhang, Zhongyang Yu, Wanqi Jiang, Yufeng Shen, and Jin Li. Long-term memory for large language models through topic-based vector database. In _Proc. of IALP_, 2023. * Zhang et al. (2024) Kai Zhang, Yangyang Kang, Fubang Zhao, and Xiaozhong Liu. Llm-based medical assistant personalization with short- and long-term memory coordination. In _Proc. of NAACL_, 2024. * Zhang et al. (2024) Kai Zhang, Lizhi Qing, Yangyang Kang, and Xiaozhong Liu. Personalized llm response generation with parameterized memory injection. _arXiv:2404.03565_, 2024. * Zhang et al. (2024) You Zhang, Jin Wang, Liang-Chih Yu, Dan Xu, and Xuejie Zhang. Personalized lora for human-centered text understanding. In _Proc. of AAAI_, 2024. * Zhang et al. (2024) Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model based agents. _arXiv:2404.13501_, 2024. * Zhang et al. (2024) Zeyu Zhang, Quanyu Dai, Luyu Chen, Zeren Jiang, Rui Li, Jieming Zhu, Xu Chen, Yi Xie, Zhenhua Dong, and Ji-Rong Wen. Memsim: A bayesian simulator for evaluating memory of llm-based personal assistants. _arXiv:2409.20163_, 2024. * Zhang et al. (2024) Zhehao Zhang, Ryan A Rossi, Branislav Kveton, Yijia Shao, Diyi Yang, Hamed Zamani, Franck Dernoncourt, Joe Barrow, Tong Yu, Sungchul Kim, et al. Personalization of large language models: A survey. _arXiv:2411.00027_, 2024. * Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _arXiv:2303.18223_, 2023. * Zhong et al. (2021) Wanjun Zhong, Duyu Tang, Jiahai Wang, Jian Yin, and Nan Duan. Useradapter: Few-shot user learning in sentiment analysis. In _Proc. of ACL Findings_, 2021. * Zhong et al. (2022) Hanxun Zhong, Zhicheng Dou, Yutao Zhu, Hongjin Qian, and Ji-Rong Wen. Less is more: Learning to refine dialogue history for personalized dialogue generation. _arXiv:2204.08128_, 2022. * Zhou et al. (2023) Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. Beyond one-preference-for-all: Multi-objective direct preference optimization. _arXiv:2310.03708_, 2023. * Zhou et al. (2024) Han Zhou, Xingchen Wan, Ivan Vulic, and Anna Korhonen. Autopeful: Automatic configuration search for parameter-efficient fine-tuning. _Proc. of ACL_, 2024. * Zhu et al. (2024) Jiachen Zhu, Jianghao Lin, Xinyi Dai, Bo Chen, Rong Shan, Jieming Zhu, Ruiming Tang, Yong Yu, and Weinan Zhang. Lifelong personalized low-rank adaptation of large language models for recommendation. _arXiv:2408.03533_, 2024. * Zhuang et al. (2024) Yuchen Zhuang, Haotian Sun, Yue Yu, Rushi Qiang, Qifan Wang, Chao Zhang, and Bo Dai. Hydra: Model factorization framework for black-box llm personalization. _arXiv:2406.02888_, 2024. * Zollo et al. (2024) Thomas P Zollo, Andrew Wei Tung Siah, Naimeng Ye, Ang Li, and Hongseok Namkoong. Personalllm: Tailoring llms to individual preferences. _arXiv:2409.20296_, 2024."
    }
  ]
}