{
  "title": "Multilevel Large Language Models for Everyone",
  "authors": [
    "Yuanhao Gong"
  ],
  "abstract": "\n Large language models have made significant progress in the past few years. However, they are either generic or field specific, splitting the community into different groups. In this paper, we unify these large language models into a larger map, where the generic and specific models are linked together and can improve each other, based on the user personal input and information from the internet. The idea of linking several large language models together is inspired by the functionality of human brain. The specific regions on the brain cortex are specific for certain low level functionality. And these regions can jointly work together to achieve more complex high level functionality. Such behavior on human brain cortex sheds the light to design the multilevel large language models that contain global level, field level and user level models. The user level models run on local machines to achieve efficient response and protect the user's privacy. Such multilevel models reduce some redundancy and perform better than the single level models. The proposed multilevel idea can be applied in various applications, such as natural language processing, computer vision tasks, professional assistant, business and healthcare. \n",
  "references": [
    {
      "id": null,
      "title": "Multilevel Large Language Models for Everyone",
      "authors": [
        "Yuanhao Gong"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Language models are fewshot learners",
      "authors": [
        "T B Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D M Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2005",
      "venue": "CoRR",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P J Liu"
      ],
      "year": "2020",
      "venue": "J. Mach. Learn. Res",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Objective comparison of particle tracking methods",
      "authors": [
        "N Chenouard",
        "I Smal",
        "F De Chaumont",
        "M Maska",
        "I F Sbalzarini",
        "Y Gong",
        "J Cardinale",
        "C Carthel",
        "S Coraluppi",
        "M Winter",
        "A R Cohen",
        "W J Godinez",
        "K Rohr",
        "Y Kalaidzidis",
        "L Liang",
        "J Duncan",
        "H Shen",
        "Y Xu",
        "K E G Magnusson",
        "J Jalden",
        "H M Blau",
        "P Paul-Gilloteaux",
        "P Roudot",
        "C Kervrann",
        "F Waharte",
        "J.-Y Tinevez",
        "S L Shorte",
        "J Willemse",
        "K Celler",
        "G P Van Wezel",
        "H.-W Dan",
        "Y.-S Tsai",
        "C Ortiz De Solorzano",
        "J.-C Olivo-Marin",
        "E Meijering"
      ],
      "year": "2014",
      "venue": "Nat. Methods",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Symmetry detection for multi-object using local polar coordinate",
      "authors": [
        "Y Gong",
        "Q Wang",
        "C Yang",
        "Y Gao",
        "C Li"
      ],
      "year": "2009",
      "venue": "Lecture Notes in Computer Science",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "BART: denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "1910",
      "venue": "CoRR",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "A survey of large language models",
      "authors": [
        "W X Zhao",
        "K Zhou",
        "J Li",
        "T Tang",
        "X Wang",
        "Y Hou",
        "Y Min",
        "B Zhang",
        "J Zhang",
        "Z Dong",
        "Y Du",
        "C Yang",
        "Y Chen",
        "Z Chen",
        "J Jiang",
        "R Ren",
        "Y Li",
        "X Tang",
        "Z Liu",
        "P Liu",
        "J.-Y Nie",
        "J.-R Wen"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Coupled signed-distance functions for implicit surface reconstruction",
      "authors": [
        "Y Gong",
        "G Paul",
        "I F Sbalzarini"
      ],
      "year": "2012",
      "venue": "IEEE Intl. Symp. Biomed. Imaging (ISBI)",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Local weighted Gaussian curvature for image processing",
      "authors": [
        "Y Gong",
        "I F Sbalzarini"
      ],
      "year": "2013",
      "venue": "Intl. Conf. Image Proc. (ICIP)",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Single image interpolation exploiting semilocal similarity",
      "authors": [
        "L Yu",
        "M T Orchard"
      ],
      "year": "2019",
      "venue": "Single image interpolation exploiting semilocal similarity",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Image enhancement by gradient distribution specification",
      "authors": [
        "Y Gong",
        "I F Sbalzarini"
      ],
      "year": "2014",
      "venue": "Proc. Workshop \"Emerging Topics in Image Enhancement and Restoration",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Side window filtering",
      "authors": [
        "H Yin",
        "Y Gong",
        "G Qiu"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Spectrally regularized surfaces",
      "authors": [
        "Y Gong"
      ],
      "year": "2015",
      "venue": "Spectrally regularized surfaces",
      "doi": "10.3929/ethz-a-010438292"
    },
    {
      "id": "b13",
      "title": "Fast and highquality blind multi-spectral image pansharpening",
      "authors": [
        "L Yu",
        "D Liu",
        "H Mansour",
        "P T Boufounos"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "A natural-scene gradient distribution prior and its application in light-microscopy image processing",
      "authors": [
        "Y Gong",
        "I Sbalzarini"
      ],
      "year": "2016",
      "venue": "IEEE Journal",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "A survey on blockchain technology and its security",
      "authors": [
        "H Guo",
        "X Yu"
      ],
      "year": "2022",
      "venue": "Blockchain: Research and Applications",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Curvature filters efficiently reduce certain variational energies",
      "authors": [
        "Y Gong",
        "I F Sbalzarini"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Motion saliency based multi-stream multiplier resnets for action recognition",
      "authors": [
        "M Zong",
        "R Wang",
        "X Chen",
        "Z Chen",
        "Y Gong"
      ],
      "year": "2021",
      "venue": "Image and Vision Computing",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Bernstein filter: A new solver for mean curvature regularized models",
      "authors": [
        "Y Gong"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Blockchain-based cross-domain authorization system for user-centric resource sharing",
      "authors": [
        "Y Ezawa",
        "S Kakei",
        "Y Shiraishi",
        "M Mohri",
        "M Morii"
      ],
      "year": "2023",
      "venue": "Blockchain: Research and Applications",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Linear approximation of mean curvature",
      "authors": [
        "Y Gong",
        "Y Xie"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Real-time optimizing weighted gaussian curvature for 4k videos",
      "authors": [
        "W Tang",
        "L Zhou",
        "Y Gong"
      ],
      "year": "2021",
      "venue": "2021 IEEE 31st International Workshop on Machine Learning for Signal Processing",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Sub-window box filter",
      "authors": [
        "Y Gong",
        "B Liu",
        "X Hou",
        "G Qiu"
      ],
      "year": "2018",
      "venue": "Proc. IEEE Visual Communications and Image Processing",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Image filtering with generic geometric prior",
      "authors": [
        "Y Gong",
        "X Hou",
        "F Li",
        "G Qiu"
      ],
      "year": "2018",
      "venue": "IEEE Access",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Blind multi-spectral image pan-sharpening",
      "authors": [
        "L Yu",
        "D Liu",
        "H Mansour",
        "P T Boufounos",
        "Y Ma"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Weighted mean curvature",
      "authors": [
        "Y Gong",
        "O Goksel"
      ],
      "year": "2019",
      "venue": "Signal Processing",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "What do large language models learn about scripts",
      "authors": [
        "A Sancheti",
        "R Rudinger"
      ],
      "year": "2022",
      "venue": "What do large language models learn about scripts",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Computing gaussian curvature in real-time for 4k video processing",
      "authors": [
        "Y Gong",
        "Y Chen"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Structure adaptive filtering for edge-preserving image smoothing",
      "authors": [
        "W Tang",
        "Y Gong",
        "L Su",
        "W Wu",
        "G Qiu"
      ],
      "year": "2021",
      "venue": "Structure adaptive filtering for edge-preserving image smoothing",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Soft tissue removal in x-ray images by half window dark channel prior",
      "authors": [
        "Y Gong",
        "H Yin",
        "J Liu",
        "B Liu",
        "G Qiu"
      ],
      "year": "2019",
      "venue": "Proc. IEEE Int. Conf. Image Processing (ICIP)",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Side window guided filtering",
      "authors": [
        "H Yin",
        "Y Gong",
        "G Qiu"
      ],
      "year": "2019",
      "venue": "Signal Process",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Computing curvature, mean curvature and weighted mean curvature",
      "authors": [
        "Y Gong"
      ],
      "year": "2022",
      "venue": "Computing curvature, mean curvature and weighted mean curvature",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Fast and efficient implementation of image filtering using a side window convolutional neural network",
      "authors": [
        "H Yin",
        "Y Gong",
        "G Qiu"
      ],
      "year": "2020",
      "venue": "Signal Process",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Molecular surface estimation by geometric coupled distance functions",
      "authors": [
        "Y Gong",
        "Y Chen"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "A high performance concurrency protocol for smart contracts of permissioned blockchain",
      "authors": [
        "C Jin",
        "S Pang",
        "X Qi",
        "Z Zhang",
        "A Zhou"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Quarter laplacian filter for edge aware image processing",
      "authors": [
        "Y Gong",
        "W Tang",
        "L Zhou",
        "L Yu",
        "G Qiu"
      ],
      "year": "2021",
      "venue": "Proc. IEEE Int. Conf. Image Processing",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Curvature-based real-time brightness adjustment for ultra hd video",
      "authors": [
        "W Tang",
        "L Zhou",
        "Y Gong"
      ],
      "year": "2022",
      "venue": "2022 IEEE 24th International Workshop on Multimedia Signal Processing",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "A discrete scheme for computing image's weighted gaussian curvature",
      "authors": [
        "Y Gong",
        "W Tang",
        "L Zhou",
        "L Yu",
        "G Qiu"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Image Processing",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "A novel structure adaptive algorithm for feature-preserving 3d mesh denoising",
      "authors": [
        "W Tang",
        "Y Gong",
        "G Qiu"
      ],
      "year": "2022",
      "venue": "2022 IEEE 24th International Workshop on Multimedia Signal Processing",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Gc-net: An unsupervised network for gaussian curvature optimization on images",
      "authors": [
        "W Tang",
        "Z Lin",
        "Y Gong"
      ],
      "year": "2023",
      "venue": "Journal of Signal Processing Systems",
      "doi": "10.1007/s11265-022-01800-4"
    },
    {
      "id": "b40",
      "title": "Feature preserving 3d mesh denoising with a dense local graph neural network",
      "authors": [
        "W Tang",
        "Y Gong",
        "G Qiu"
      ],
      "year": "2023",
      "venue": "Feature preserving 3d mesh denoising with a dense local graph neural network",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Regression-based camera pose estimation through multi-level local features and global features",
      "authors": [
        "M Xu",
        "Z Zhang",
        "Y Gong",
        "S Poslad"
      ],
      "year": "2023",
      "venue": "Sensors",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Dynamic neural networks: A survey",
      "authors": [
        "Y Han",
        "G Huang",
        "S Song",
        "L Yang",
        "H Wang",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Training language models with language feedback at scale",
      "authors": [
        "J Scheurer",
        "J A Campos",
        "T Korbak",
        "J S Chan",
        "A Chen",
        "K Cho",
        "E Perez"
      ],
      "year": "2023",
      "venue": "Training language models with language feedback at scale",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "authors": [
        "R Zhang",
        "J Han",
        "A Zhou",
        "X Hu",
        "S Yan",
        "P Lu",
        "H Li",
        "P Gao",
        "Y Qiao"
      ],
      "year": "2023",
      "venue": "CoRR",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Multilevel Large Language Models For Everyone",
      "text": "Yuanhao Gong College of Electronics and Information Engineering, Shenzhen University, China. gong.ai@qq.com Manuscript received April 19, 2005; revised September 17, 2014."
    },
    {
      "title": "Abstract",
      "text": "Large language models have made significant progress in the past few years. However, they are either generic _or_ field specific, splitting the community into different groups. In this paper, we onlyf these large language models into a larger map, where the generic _and_ specific models are linked together and can improve each other, based on the user personal input and information from the internet. The idea of linking several large language models together is inspired by the functionality of human brain. The specific regions on the brain cortex are specific for certain low level functionality. And these regions can jointly work together to achieve more complex high level functionality. Such behavior on human brain cortex sheds the light to design the multilevel large language models that contain global level, field level and user level models. The user level models run on local machines to achieve efficient response and protect the user's privacy. Such multilevel models reduce some redundancy and perform better than the single level models. The proposed multilevel idea can be applied in various applications, such as natural language processing, computer vision tasks, professional assistant, business and healthcare. language model; neural network; multilevel; personal LLM"
    },
    {
      "title": "I Introduction",
      "text": "Large language models (LLM) are a fascinating result of recent advancements in artificial intelligence technology. These models have the potential to transform communication and writing. With the latest technological advancements, large language models have become more sophisticated in imitating human language. They are trained on enormous amounts of text data, enabling them to produce coherent and relevant text on various topics. This technology has opened up a wide range of usage in fields such as business, education, and healthcare. In recent years, several popular large language models have emerged, each with unique qualities designed to cater to different requirements and perform different functions. For instance, GPT-3 generates highly accurate and convincing text, making it suitable for applications such as generating conversational responses or writing articles [1]. On the other hand, T5 is versatile and can perform a wide range of natural language processing tasks, making it valuable in areas such as translation and question answering [2]. Apart from GPT-3 and T5, BERT is another popular large language model that has made significant contributions to the field of natural language processing [3]. BERT excels at understanding the context of words and sentences, making it valuable for applications such as search engines and chatbots. By understanding the context of sentences, BERT can provide more accurate and relevant search results, and it helps chatbots to produce more natural and human-like responses. Despite their differences, all of these models share a common goal: to advance the field of artificial intelligence and push the limits of what machines can do. With the continued development of large language models, we can expect to see even more exciting breakthroughs in artificial intelligence technology in the future."
    },
    {
      "title": "_Why Large Models?_",
      "text": "Large language models have transformed the field of natural language processing, allowing machines to understand and generate human-like language. These models learn from vast amounts of data and store and process more information due to their large number of parameters, enabling them to produce accurate, relevant, and contextually appropriate results. However, it is important to balance the number of parameters to prevent overfitting, which can reduce the model's overall performance. Additionally, training models with many parameters can be time-consuming, slowing down the training process. Furthermore, large language models are versatile and can enhance various natural language processing tasks, such as language translation, sentiment analysis, and text summarization. By training on extensive data sets, these models can detect patterns and nuances in language that are difficult for humans to recognize. They are essential tools for applications such as virtual assistants, chatbots, and language understanding systems. Large language models can also generate human-like text, which has various applications in fields such as journalism, creative writing, and content generation. In conclusion, large language models are a powerful tool that can revolutionize how we interact with machines and generate human-like language, but it is important to balance the number of parameters and the model's performance for optimal results. Fig. 1: Illustration of the proposed multilevel method. The global LLM are large and trained on a cloud. They are trained for the common intelligence. They are retrained (fine tuning) and distilled to get a smaller field specific LLM for different fields, such as medical diagnosis, mathematical education, and C++ programming. These models are further retrained and distilled to get a personal LLM that runs on a local machine to get efficient response and protect users’ privacy from the internet attack."
    },
    {
      "title": "_Limitations_",
      "text": "Large language models have some limitations that need to be addressed. One such limitation is their potential to perpetured biases present in their training data, which can lead to unfair and discriminatory outcomes. Additionally, these models have high computational and energy requirements, making them expensive to train and operate. Their tendency to generate unreliable or false information, particularly when dealing with complex or sensitive topics, is also problematic. Concerns have also been raised about the potential misuse of large language models for generating disinformation. Another concern is the potential threat to privacy posed by large language models. Since these models require large amounts of data to train, they can potentially capture sensitive information. Therefore, it is important to develop privacy-preserving models that can ensure users' sensitive information is not compromised. Furthermore, the environmental impact of running these models is also a concern. These models require significant amounts of energy to train and operate, leading to a significant carbon footprint. To mitigate this, researchers and developers are actively working to address these limitations and reduce the risks associated with large language models. To address these limitations, researchers are developing more robust and diverse datasets for training. This can help reduce bias and enable models to produce more accurate results. Ethical guidelines are being implemented for model development and use to ensure that these models are used responsibly and do not cause harm. Finally, alternative approaches to language processing are being explored, such as using smaller models or leveraging other technologies like knowledge graphs to process language more efficiently."
    },
    {
      "title": "Ii Multilevel Large Language Models",
      "text": "The development of artificial intelligence has resulted in increasingly complex neural networks with more parameters. This has led to the creation of large language models such as GPT-3 and BERT, which have significantly impacted natural language processing. However, a major challenge associated with these models is their high computational cost. Traditional large language models, like GPT-3 and BERT, require thousands of GPUs for training and deployment. This high cost is a significant obstacle for many researchers and organizations, limiting access to these powerful models. Despite this challenge, advancements in hardware and software have made it possible to train and deploy these models on fewer GPUs. This has opened new opportunities for researchers and organizations to explore the capabilities of these models and leverage their potential in various applications. Another problem with current LLM is that these models are static. In other words, they can not evolve to achieve higher performance with more and more user input. In fact, millions of users can help to improve the LLM in each specific field, such as poem, medical diagnosis and car design."
    },
    {
      "title": "_Multilevel Models_",
      "text": "To tackle these issues, we propose a multilevel strategy. We propose to decompose the system into three levels: global level, field level and user level. Each level contains different LLM for different purpose. The full map is illustrated in Fig. 1. A simple comparison between these levels is shown in Table I. At the global level, the LLM are trained using all available dataset. As a result, they are good at everything but not excellent at any specific fields. Letting users directly using these models is wasting the computation resource because the performance is just good, not excellent. However, such global LLM can help in improving the field LLM. At the field level, the LLM are designed to be excellent in a specific field, such as poem, music, medical knowledge, and finance. These models correspond to the different professional career in human society such as lawyers and doctors. These models are excellent in a specific field in general, but not for any specific person. At the user level, the LLM are trained to be an assistant with both personal information and professional advice from some specific field LLM. These models fill the gap between each individual user and the field LLM. Moreover, these models run on local machines to protect the user's privacy."
    },
    {
      "title": "Ii-A1 Global Llm",
      "text": "The global LLM contain a very large parameters and are updated at low frequency. If nothing happens, they will automatically update themselves every week. However, if there are more information from the training data, they will update themselves with a shorter period. Meanwhile, these global LLM are not designed to serve users. Instead, they are designed to update the field LLM that have specific domain knowledge and also smaller parameters. Therefore, users' response time is not affected by the long time training and inference processing from the global LLM."
    },
    {
      "title": "Ii-A2 Field Llm",
      "text": "The field LLM are designed to be professional in some field, such as art, music, object tracking, and education. They are automatically updated by the global LLM which receive new knowledge from the user input. The field LLM can be considered as a buffer that exchanges information between the global LLM and the user LLM. Their parameter weights are updated by the global LLM. But they extract the information from the user LLM and feed the information into the global LLM."
    },
    {
      "title": "Ii-A3 User Llm",
      "text": "At the user level, user LLM have much less parameters and also the inference time. Different from the global and field LLM, user LLM run on local machine and can contain each user's personal information such as movie and music preferences. Such user LLM can be considered as an excellent assistant who helps the user in planing, shopping, and working. More importantly, these models are stored and encrypted on the user's local machine. Ideally, they will not betray the user. The user's privacy is protected."
    },
    {
      "title": "_Dynamic Large Language Models_",
      "text": "Be aware that these models can evolve during users' usage. With more and more user input, the local LLM know more about the user's preference. Therefore, the local LLM can evolve to update themselves accordingly. They can also dynamically link themselves to several related field LLM because the user's input is also dynamically changing. Such dynamics affects the whole system, leading to dynamic LLM."
    },
    {
      "title": "Ii-B1 Learning During Using",
      "text": "The multilevel large language models are designed to evolve during its usage. Thus, they are highly dynamic, especially with the user input. They are life-long learning systems, keeping evolving to understand the user from his/her history recording."
    },
    {
      "title": "Ii-B2 Interaction Between User And User Llm",
      "text": "There is a direct interaction between users and users' LLM. The user input is dynamic and the user's preference might be also changing with time and experience. Such dynamics requires the user LLM quickly adapt to such change. To be adaptive, one way is to change the parameter or architecture in user LLM, based on the received users' input. The other way is to change the user LLM from the interaction with field LLM."
    },
    {
      "title": "Ii-B3 Interaction Between User Llm And Field Llm",
      "text": "The user LLM and field LLM have a soft link. And such link can be changed dynamically when the user's preference is changed. For example, if the user's preference is changed from IT to finance, the user LLM will find such change and trigger a require to the field LLM. The field LLM will update the user LLM accordingly. If the field LLM are updated, they will also update the user LLM to fresh the new knowledge into the user LLM. Such fresh frequency is medium to balance the model accuracy and the inference running time."
    },
    {
      "title": "Ii-B4 Interaction Between Field Llm And Global Llm",
      "text": "The field LLM can be updated by the user LLM and also the global LLM. Meanwhile, the global LLM can be updated by the field LLM that receive new information from the user LLM. The global LLM can be updated if there is new data set available. Such interaction happens in a very low frequency because the global LLM are large and should be stable (not so adaptive to an individual user). In contrast, the user LLM are quickly adaptive to each user."
    },
    {
      "title": "Iii Economical Ecosystem",
      "text": "Our multilevel large language model (MLLM) is an innovative approach to training and deploying the LLM that offers significant benefits to both users and developers. By leveraging advanced computational techniques and sophisticated algorithms, the MLLM achieves unparalleled levels of accuracy and efficiency in natural language processing tasks. This translates to significant cost savings for developers, who can now train and deploy their models more quickly and easily than ever before. Meanwhile, users can enjoy a more seamless and intuitive experience when interacting with language-based applications and services, thanks to the MLLM's ability to understand and interpret natural language in a more nuanced and sophisticated way. With its cutting-edge technology and user-focused design, the MLLM is poised to revolutionize the field of natural language processing and usher in a new era of innovation and discovery."
    },
    {
      "title": "_Users_",
      "text": "Users in the system have two roles. First, users are able to utilize the system's features, and as a result, they are required to pay a fee for the services rendered. The payment allows for continued access to the system and its benefits, including but not limited to increased productivity, streamlined processes, and enhanced data analysis capabilities. Additionally, regular payment ensures that the system can be maintained and further developed to meet the evolving needs of its users. Therefore, it is important to maintain a consistent payment schedule to ensure uninterrupted access to the system and its benefits. Second, it is important to acknowledge the critical role that users play in the system. Users are valuable sources of input and integral to the system's success. Without their contributions, the system would lack quality and reliability. To encourage more high-quality contributors, incentivizing users to share their knowledge and expertise is an effective approach. One way to achieve this is by establishing a system of fair compensation for users, recognizing their efforts and contributions. This compensation can come in various forms, including monetary rewards, recognition, or exclusive access to features. By providing fair compensation, the system can attract a larger pool of motivated users who will contribute their best work, resulting in a more robust and reliable platform. Therefore, prioritizing the establishment of a fair compensation system for users is crucial, as it will ultimately benefit the system as a whole and contribute to its long-term success."
    },
    {
      "title": "_Developers_",
      "text": "The developers can work on these three level LLM. They can develop the user LLM that can be more adaptive to each individual user. They can also develop field LLM, to make them more professional. They can work on global LLM to accelerate the model convergence and efficiency. The developers also have two roles in this system. First, thanks to the extensive work on their LLM at these three levels, developers can earn money from their LLM. They are well-equipped to capitalize on their expertise and earn a lucrative income from their specialized model. With a deep understanding of the complexities of the field and the latest industry trends, they can confidently navigate the landscape and offer valuable services to a wide range of users. Second, they have to pay money for the (valuable) input from users. In order to obtain valuable input from their users, developers may pay a fee. This fee is necessary to ensure that the developer can continue to provide high-quality services and products that meet the needs and expectations of their customers. By compensating users for their input, developers can gain valuable insights that can help them improve existingproducts or develop new ones that better meet the needs of their target market. Additionally, this feedback can be used to identify areas for improvement in the developer's operations and customer service, which can lead to increased satisfaction and loyalty among existing customers and attract new ones."
    },
    {
      "title": "_Dynamic Models_",
      "text": "The models in the system are not equal. Some are more important than others and thus have higher service fees. But this is also dynamic, which means that some important models might become important in the future. Therefore, all the price in the models are dynamic, according to their service quality and market demand. As a result, both users and developers have the potential to earn an income that is closely tied to the growth and progress of the system. As new technologies and innovations are introduced, users and developers have the opportunity to adapt and evolve LLM in order to stay competitive and continue to earn a sustainable income. This is different from the traditional economical mode for LLM, where the developers have to pay for the training and inference process first and then charge the users with their usage."
    },
    {
      "title": "_Blockchains_",
      "text": "From the hardware point of view, implementation of the proposed multilevel large language models is difficult. Each LLM requires a lot of GPUs to train. And there are many LLM in the multilevel large language models. The traditional way to train the system is unrealistic. To tackle this issue, we propose to develop MLLM on blockchains, which have high computation performance, are decentralized and run parallel on all nodes. Interestingly, the decentralized mode in blockchains also fits the proposed local machine and server mode in the MLLM. Each miner client in the blockchain can be considered as a user in the MLLM. Instead of only contributing the computation to the blockchain, the users also get services from the blockchain in the terms of computation and economy. Therefore the miner client and the blockchain charge each other according to their performance. More importantly, the user's input is also considered as valuable resource that can improve the LLM at each level. Such economics is not considered in the traditional blockchains and traditional LLM. This novel idea will trigger the promotion of high quality input in the system and eventually benefit everyone in the ecosystem. The system is still under developing and we will release the system in the future to accelerate the related research and practical applications. The economic mode behind MLLM can reduce the barrier for LLM development, making the developing easier and available for everyone. Nowadays, the mobile phones and chips become more and more powerful. They are likely the node in the blockchains. And both the users and developers should benefit from such devices and get higher quality services."
    },
    {
      "title": "Iv Conclusion And Discussion",
      "text": "In recent times, there has been a significant rise in the usage of large language models. These models are created to comprehend and analyze human language and are becoming increasingly popular in various industries such as natural language processing, sentiment analysis, machine translation, and speech recognition. With the increasing demand for these models, we can expect to see further developments in the field of artificial intelligence and natural language processing. These advancements will undoubtedly impact the way we interact with technology and communicate in the future. Although the large language models have made significant progress in natural language processing, there are still some limitations that need to be addressed. One of the most important limitations is the computational power needed to train and run these models. In addition, while these models can generate coherent text, they often struggle with generating text that is both relevant and accurate. Another limitation is the lack of diversity in the training data used to create these models, which can lead to bias and inaccuracies in the generated text. Therefore, it is essential to continue exploring ways to improve these models, such as incorporating more diverse and representative training data and developing more powerful computational resources to train and run these models. In this paper, we have introduced multilevel large language models that address some of the challenges posed by previous large language models. Specifically, our models incorporate innovative strategies for handling long-term dependencies, improving computational efficiency, and enhancing the accuracy of predictions. Overall, our work represents a significant step forward in the development of large language models and lays the groundwork for future research in this area. The multilevel approach seeks to balance accuracy and running time. It is based on the concept that complex problems can be broken down into smaller, simpler sub-problems to achieve accurate results. By solving these sub-problems, the overall accuracy of the solution can be improved. However, creating too many sub-problems can increase the running time. The multilevel approach suggests that a balance can be struck by dividing the problem into an optimal number of sub-problems. This allows for the maintenance of accuracy while keeping the running time within acceptable limits. The multilevel approach has gained popularity in various fields, such as computer science, mathematics, and engineering. It has been successfully applied to problems such as image processing, signal analysis, and optimization. Overall, the multilevel approach is a promising strategy that can improve the accuracy of complex problem solutions while keeping the running time practical. Our approach is a novel step forward in the development of next generation large language models. The multilevel methods also show a novel economic model for developing large language models. They can be adopted in various applications such as audio, vision and machine learning tasks [4, 5, 6, 7, 8, 1, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,[40, 32, 41, 42, 43, 44, 45]."
    },
    {
      "title": "References",
      "text": "* [1] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, \"Language models are few-shot learners,\" _CoRR_, vol. abs/2005.14165, 2020. [Online]. Available: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165) * [2] C. Raffelt, N. Shazer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \"Exploring the limits of transfer learning with a unified text-to-text transformer,\" _J. Mach. Learn. Res._, vol. 21, no. 1, Jan 2020. * [3] J. Devlin, M. Chang, K. Lee, and K. Toutanova, \"BERT: pre-training of deep bidirectional transformers for language understanding,\" in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 4171-4186. * [4] N. Chenouard, I. Smal, F. de Chaumont, M. Maska, I. F. Sbalzarini, Y. Gong, J. Cardinale, C. Carthel, S. Coraluppi, M. Winter, A. R. Cohen, W. J. Godinez, K. Rohr, Y. Kalalidzidis, L. Liang, J. Duncan, H. Shen, Y. Xu, K. E. G. Magnusson, J. Jalden, H. M. Blau, P. Paul-Gilloteanu, P. Roudet, C. Kervrann, F. Whaarte, J.-Y. Tweze, S. L. Shorte, J. Willemse, K. Celler, G. P. van Bezel, H.-W. Dan, Y.-S. Tsai, C. Ortiz de Solorzano, J.-C. Olivo-Marin, and E. Meijering, \"Objective comparison of particle tracking methods,\" _Nat. Methods_, vol. 11, no. 3, pp. 281-U247, March 2014. * [5] Y. Gong, Q. Wang, C. Yang, Y. Gao, and C. Li, \"Symmetry detection for multi-object using local polar coordinate,\" _Lecture Notes in Computer Science_, vol. 5702, p. 277, 2009. * [6] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, \"BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,\" _CoRR_, vol. abs/1910.13461, 2019. [Online]. Available: [http://arxiv.org/abs/1910.13461](http://arxiv.org/abs/1910.13461) * [7] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie, and J.-R. Wen, \"A survey of large language models,\" 2023. * [8] Y. Gong, G. Paul, and I. F. Sbalzarini, \"Coupled signed-distance functions for implicit surface reconstruction,\" in _IEEE Intl. Symp. Biomed. Imaging (ISBI)_, May 2012, pp. 1000-1003. * [9] Y. Gong and I. F. Sbalzarini, \"Local weighted Gaussian curvature for image processing,\" _Intl. Conf. Image Proc. (ICIP)_, pp. 534-538, September 2013. * [10] L. Yu and M. T. Orchard, \"Single image interpolation exploiting semi-local similarity.\" Brighton, UK: IEEE, 2019, pp. 1722-1726. * [11] Y. Gong and I. F. Sbalzarini, \"Image enhancement by gradient distribution specification,\" in _In Proc. Workshop \"Emerging Topics in Image Enhancement and Restoration\", 12th Asian Conference on Computer Vision_, Singapore, Nov 2014, pp. w7-p3. * [12] H. Yin, Y. Gong, and G. Qiu, \"Side window filtering,\" in _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019, pp. 8750-8758. * [13] Y. Gong, \"Spectrally regularized surfaces,\" Ph.D. dissertation, ETH Zurich, Nr. 22616, 2015, [http://dx.doi.org/10.3929/ethtz-a-010438292](http://dx.doi.org/10.3929/ethtz-a-010438292). * [14] L. Yu, D. Liu, H. Mansour, and P. T. Boufounos, \"Fast and high-quality blind multi-spectral image pansharpening,\" _IEEE Transactions on Geoscience and Remote Sensing_, vol. 60, pp. 1-17, 2022. * [15] Y. Gong and I. Sbalzarini, \"A natural-scene gradient distribution prior and its application in light-microscopy image processing,\" _Selected Topics in Signal Processing, IEEE Journal of_, vol. 10, no. 1, pp. 99-114, Feb 2016. * [16] H. Guo and X. Yu, \"A survey on blockchain technology and its security,\" _Blockchain: Research and Applications_, vol. 3, no. 2, p. 100067, 2021. [Online]. Available: [https://www.sciencedirect.com/science/article/pii/S2096720922000070](https://www.sciencedirect.com/science/article/pii/S2096720922000070) * [17] Y. Gong and I. F. Sbalzarini, \"Curvature filters efficiently reduce certain variational energies,\" _IEEE Transactions on Image Processing_, vol. 26, no. 4, pp. 1786-1798, April 2017. * [18] M. Zong, R. Wang, X. Chen, Z. Chen, and Y. Gong, \"Motion saliency based multi-stream multiplier rensets for action recognition,\" _Image and Vision Computing_, vol. 107, p. 104108, 2021. [Online]. Available: [https://www.sciencedirect.com/science/article/pii/S026885621000135](https://www.sciencedirect.com/science/article/pii/S026885621000135) * [19] Y. Gong, \"Bernstein filter: A new solver for mean curvature regularized models,\" in _2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, March 2016, pp. 1701-1705. * [20] Y. Ezawa, S. Kakei, Y. Shiraishi, M. Mohri, and M. Morii, \"Blockchain-based cross-domain authorization system for user-centric resource sharing,\" _Blockchain: Research and Applications_, vol. 4, no. 2, p. 100126, 2023. [Online]. Available: [https://www.sciencedirect.com/science/article/pii/S2096720923000015](https://www.sciencedirect.com/science/article/pii/S2096720923000015) * [21] Y. Gong and Y. Xie, \"Linear approximation of mean curvature,\" in _Image Processing (ICIP), 2017 IEEE International Conference on_. IEEE, 2017, pp. 570-574. * [22] W. Tang, L. Zhou, and Y. Gong, \"Real-time optimizing weighted gaussian curvature for 4k videos,\" in _2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MISP)_, 2021, pp. 1-6. * [23] Y. Gong, B. Liu, X. Hou, and G. Qiu, \"Sub-window box filter,\" in _Proc. IEEE Visual Communications and Image Processing (VCIP)_, Dec. 2018, pp. 1-4. * [24] Y. Gong, X. Hou, F. Li, and G. Qiu, \"Image filtering with generic geometric prior,\" _IEEE Access_, vol. 6, pp. 54 320-54 330, 2018. * 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, 2020, pp. 1429-1433. * 339, 2019. [Online]. Available: [http://www.sciencedirect.com/science/article/pii/S0165168419302282](http://www.sciencedirect.com/science/article/pii/S0165168419302282) * [27] A. Sanchet and R. Rudinger, \"What do large language models learn about scripts?\" 2022. * [28] Y. Gong and Y. Chen, \"Computing gaussian curvature in real-time for 4k video processing,\" _IEEE Access_, vol. 7, pp. 115 936-115 944, 2019. * [29] W. Tang, Y. Gong, L. Su, W. Wu, and G. Qiu, \"Structure adaptive filtering for edge-preserving image smoothing,\" in _Image and Graphics_, Y. Peng, S.-M. Hu, M. Gabbouj, K. Zhou, M. Elad, and K. Xu, Eds. Cham: Springer International Publishing, 2021, pp. 265-276. * [30] Y. Gong, H. Yin, J. Liu, B. Liu, and G. Qiu, \"Soft tissue removal in x-ray images by half window dark channel prior,\" in _Proc. IEEE Int. Conf. Image Processing (ICIP)_, Sep. 2019, pp. 3576-3580. * [31] H. Yin, Y. Gong, and G. Qiu, \"Side window guided filtering,\" _Signal Process._, vol. 165, pp. 315-330, 2019. * [32] Y. Gong, \"Computing curvature, mean curvature and weighted mean curvature,\" Bordeaux, France: IEEE, 2022, pp. 266-270. * [33] H. Yin, Y. Gong, and G. Qiu, \"Fast and efficient implementation of image filtering using a side window convolutional neural network,\" _Signal Process._, vol. 176, p. 107171, 2020. * [34] Y. Gong and Y. Chen, \"Molecular surface estimation by geometric coupled distance functions,\" _IEEE Access_, vol. 8, pp. 176 263-176 273, 2020. * [35] C. Jin, S. Pang, X. Qi, Z. Zhang, and A. Zhou, \"A high performance concurrency protocol for smart contracts of permissioned blockchain,\" _IEEE Transactions on Knowledge and Data Engineering_, vol. 34, no. 11, pp. 5070-5083, 2022. * [36] Y. Gong, W. Tang, L. Zhou, L. Yu, and G. Qiu, \"Quarter laplacian filter for edge aware image processing,\" in _Proc. IEEE Int. Conf. Image Processing (ICIP)_, 2012, pp. 1959-1963. * [37] W. Tang, L. Zhou, and Y. Gong, \"Curvature-based real-time brightness adjustment for ultra and video,\" in _2022 IEEE 24th International Workshop on Multimedia Signal Processing (MMSP)_, 2022, pp. 1-6. * [38] Y. Gong, W. Tang, L. Zhou, L. Yu, and G. Qiu, \"A discrete scheme for computing image's weighted gaussian curvature,\" in _2021 IEEE International Conference on Image Processing (ICIP)_, 2021, pp. 1919-1* [42] M. Xu, Z. Zhang, Y. Gong, and S. Poslad, \"Regression-based camera pose estimation through multi-level local features and global features,\" _Sensors_, vol. 23, no. 8, 2023. [Online]. Available: [https://www.dpi.com/1424-82202/3/8/4063](https://www.dpi.com/1424-82202/3/8/4063) * [43] Y. Han, G. Huang, S. Song, L. Yang, H. Wang, and Y. Wang, \"Dynamic neural networks: A survey,\" _IEEE Transactions on Pattern Analysis and Machine Intelligence_, vol. 44, no. 11, pp. 7436-7456, 2022. * [44] J. Scheurer, J. A. Campos, T. Korbak, J. S. Chan, A. Chen, K. Cho, and E. Perez, \"Training language models with language feedback at scale,\" Mar. 2023. * [45] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao, \"Llama-adapter: Efficient fine-tuning of language models with zero-init attention,\" _CoRR_, vol. abs/2303.16199, 2023."
    }
  ]
}