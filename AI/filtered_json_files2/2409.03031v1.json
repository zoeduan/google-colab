{
  "title": "Debugging with Open-Source Large Language Models: An Evaluation",
  "authors": [
    "Yacine Majdoub",
    "Eya Ben Charrada"
  ],
  "abstract": "\n Large language models have shown good potential in supporting software development tasks. This is why more and more developers turn to LLMs (e.g. ChatGPT) to support them in fixing their buggy code. While this can save time and effort, many companies prohibit it due to strict code sharing policies. To address this, companies can run open-source LLMs locally. But until now there is not much research evaluating the performance of open-source large language models in debugging. This work is a preliminary evaluation of the capabilities of open-source LLMs in fixing buggy code. The evaluation covers five open-source large language models and uses the benchmark DebugBench which includes more than 4000 buggy code instances written in Python, Java and C++. Open-source LLMs achieved scores ranging from 43.9% to 66.6% with DeepSeek-Coder achieving the best score for all three programming languages. \n",
  "references": [
    {
      "id": null,
      "title": "Debugging with Open-Source Large Language Models: An Evaluation",
      "authors": [
        "Yacine Majdoub",
        "Eya Ben Charrada"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Beyond code generation: An observational study of chatgpt usage in software engineering practice",
      "authors": [
        "Khojah Ranim",
        "Mohamad Mazen",
        "Leitner Philipp",
        "Neto Francisco Gomes De Oliveira"
      ],
      "year": "2024",
      "venue": "In Proc. ACM Softw. Eng",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Large language models are few-shot testers: Exploring llm-based general bug reproduction",
      "authors": [
        "Yoon Kang Sungmin",
        "Yoo Juyeon",
        "Shin"
      ],
      "year": "2023",
      "venue": "2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Large language models in fault localisation",
      "authors": [
        "Wu Yonghao",
        "Li Zheng",
        "Zhang Jie",
        "M",
        "Mike Papadakis",
        "Mark Harman",
        "Yong Liu"
      ],
      "year": "2023",
      "venue": "Large language models in fault localisation",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Automated program repair in the era of large pre-trained language models",
      "authors": [
        "Chunqiu Xia",
        "Wei Steven",
        "Zhang Yuxiang",
        "Lingming"
      ],
      "year": "2023",
      "venue": "2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Evaluating large language models trained on code",
      "authors": [
        "Mark Chen",
        "Jerry Tworek",
        "Heewoo Jun",
        "Qiming Yuan",
        "Henrique Ponde De Oliveira",
        "Jared Pinto",
        "Wojciech . Kaplan",
        "Zaremba"
      ],
      "year": "2021",
      "venue": "Evaluating large language models trained on code",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
      "authors": [
        "Liu Jiawei",
        "Chunqiu Xia",
        "Wang Steven",
        "Zhang Yuyao",
        "Lingming"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Multi-lingual evaluation of code generation models",
      "authors": [
        "Athiwaratkun Ben",
        "Gouda Sanjay Krishna",
        "Wang Zijian",
        "Li Xiaopeng",
        "Tian Yuchen",
        "Tan Ming",
        "Ahmad Wasi Uddin",
        "Wang Shiqi",
        "Sun Qing",
        "Shang Mingyue"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Program synthesis with large language models",
      "authors": [
        "Jacob Austin",
        "Augustus Odena",
        "Maxwell Nye",
        "Maarten Bosma",
        "Henryk Michalewski",
        "David Dohan",
        "Ellen Jiang",
        "Carrie Cai",
        "Michael Terry",
        "Quoc Le",
        "Charles Sutton"
      ],
      "year": "2021",
      "venue": "Program synthesis with large language models",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Code llama: Open foundation models for code",
      "authors": [
        "Jonas Baptiste Rozière",
        "Fabian Gehring",
        "Sten Gloeckle",
        "Itai Sootla",
        "Gat"
      ],
      "year": "2024",
      "venue": "Code llama: Open foundation models for code",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "phind/phind-codellama-34b-v2 -hugging face",
      "authors": [
        "Phind",
        "Phind"
      ],
      "year": "",
      "venue": "phind/phind-codellama-34b-v2 -hugging face",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Empowering code large language models with evol-instruct",
      "authors": [
        "Ziyang Luo",
        "Can Xu",
        "Pu Zhao",
        "Qingfeng Sun",
        "Xiubo Geng",
        "Wenxiang Hu",
        "Chongyang Tao",
        "Jing Ma",
        "Qingwei Lin",
        "Daxin Jiang",
        "Wizardcoder"
      ],
      "year": "2023",
      "venue": "Empowering code large language models with evol-instruct",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Empowering large language models to follow complex instructions",
      "authors": [
        "Sun Xu Can",
        "Zheng Qingfeng",
        "Geng Kai",
        "Zhao Xiubo",
        "Feng Pu",
        "Tao Jiazhan",
        "Jiang Chongyang",
        "Daxin",
        "Wizardlm"
      ],
      "year": "2023",
      "venue": "Empowering large language models to follow complex instructions",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Chenghao Mou, and Harm de Vries. Starcoder: may the source be with you!",
      "authors": [
        "Raymond Li",
        "Loubna Ben Allal",
        "Yangtian Zi",
        "Niklas Muennighoff",
        "Denis Kocetkov"
      ],
      "year": "2023",
      "venue": "Chenghao Mou, and Harm de Vries. Starcoder: may the source be with you!",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Deepseek-coder: When the large language model meets programmingthe rise of code intelligence",
      "authors": [
        "Qihao Daya Guo",
        "Dejian Zhu",
        "Zhenda Yang",
        "Kai Xie",
        "Wentao Dong",
        "Guanting Zhang",
        "Xiao Chen",
        "Y Bi",
        "Y K Wu",
        "Fuli Li",
        "Yingfei Luo",
        "Wenfeng Xiong",
        "Liang"
      ],
      "year": "2024",
      "venue": "Deepseek-coder: When the large language model meets programmingthe rise of code intelligence",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Meta AI. llama3 • hugging face",
      "authors": [],
      "year": "",
      "venue": "Meta AI. llama3 • hugging face",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Benchmarking as empirical standard in software engineering research",
      "authors": [
        "Hasselbring Wilhelm"
      ],
      "year": "2021",
      "venue": "Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "",
      "authors": [
        "Paul Ralph",
        "Sebastian Baltes",
        "Domenico Bianculli",
        "Yvonne Dittrich",
        "Michael Felderer",
        "Robert Feldt",
        ". .",
        "Sira Vegas"
      ],
      "year": "2020",
      "venue": "ACM SIGSOFT empirical standards",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Evaluating debugging capability of large language models",
      "authors": [
        "Runchu Tian",
        "Yining Ye",
        "Yujia Qin",
        "Xin Cong",
        "Yankai Lin",
        "Yinxu Pan",
        "Yesai Wu",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Debugbench"
      ],
      "year": "2024",
      "venue": "Evaluating debugging capability of large language models",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "A unified debugging approach via llm-based multiagent synergy",
      "authors": [
        "Cheryl Lee",
        "Chunqiu Steven Xia",
        "Jen-Tse Huang",
        "Zhouruixin Zhu",
        "Lingming Zhang",
        "Michael R Lyu"
      ],
      "year": "2024",
      "venue": "A unified debugging approach via llm-based multiagent synergy",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "A comprehensive study of the capabilities of large language models for vulnerability detection",
      "authors": [
        "Steenhoek Benjamin",
        "Rahman Md Mahbubur",
        "Roy Monoshi Kumar",
        "Alam Mirza Sanjida",
        "Barr Earl",
        "T",
        "Le Wei"
      ],
      "year": "2024",
      "venue": "A comprehensive study of the capabilities of large language models for vulnerability detection",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Where is the bug and how is it fixed? an experiment with practitioners",
      "authors": [
        "Böhme Marcel",
        "Soremekun Ezekiel",
        "O",
        "Chattopadhyay Sudipta",
        "Ugherughe Emamurho",
        "Zeller Andreas"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 11th joint meeting on foundations of software engineering",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Towards reasoning in large language models: A survey",
      "authors": [
        "Huang Jie",
        "Chang Kevin Chen-Chuan"
      ],
      "year": "",
      "venue": "61st Annual Meeting of the Association for Computational Linguistics, ACL 2023",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark",
      "authors": [
        "Oscar Sainz",
        "Jon Ander Campos",
        "Iker García-Ferrero",
        "Julen Etxaniz",
        "Oier Lopez De Lacalle",
        "Eneko Agirre"
      ],
      "year": "2023",
      "venue": "Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Caiming Xiong, and Shafiq Joty. How much are llms contaminated? a comprehensive survey and the llmsanitize library",
      "authors": [
        "Bosheng Mathieu Ravaut",
        "Fangkai Ding",
        "Hailin Jiao",
        "Xingxuan Chen",
        "Ruochen Li",
        "Chengwei Zhao",
        "Qin"
      ],
      "year": "2024",
      "venue": "Caiming Xiong, and Shafiq Joty. How much are llms contaminated? a comprehensive survey and the llmsanitize library",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Mateusz Lango, and Ondřej Dušek. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms",
      "authors": [
        "Simone Balloccu",
        "Patrícia Schmidtová"
      ],
      "year": "2024",
      "venue": "Mateusz Lango, and Ondřej Dušek. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code",
      "authors": [
        "Naman Jain",
        "King Han",
        "Alex Gu",
        "Wen-Ding Li",
        "Fanjia Yan",
        "Tianjun Zhang",
        "Sida Wang"
      ],
      "year": "2024",
      "venue": "Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Rethinking benchmark and contamination for language models with rephrased samples",
      "authors": [
        "Shuo Yang",
        "Wei-Lin Chiang",
        "Lianmin Zheng",
        "Joseph E Gonzalez",
        "Ion Stoica"
      ],
      "year": "2023",
      "venue": "Rethinking benchmark and contamination for language models with rephrased samples",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Concerned with data contamination? assessing countermeasures in code language model",
      "authors": [
        "Jialun Cao",
        "Wuqi Zhang",
        "Shing-Chi Cheung"
      ],
      "year": "2024",
      "venue": "Concerned with data contamination? assessing countermeasures in code language model",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Explainable automated debugging via large language model-driven scientific debugging",
      "authors": [
        "Chen Kang Sungmin",
        "Yoo Bei",
        "Lou Shin",
        "Jian-Guang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 45th International Conference on Software Engineering",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Prompting is all you need: Automated android bug replay with large language models",
      "authors": [
        "Sidong Feng",
        "Chen",
        "Chunyang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 46th IEEE/ACM International Conference on Software Engineering",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Ldb: A large language model debugger via verifying runtime execution step-by-step",
      "authors": [
        "Lily Zhong",
        "Zilong Wang",
        "Jingbo Shang"
      ],
      "year": "2024",
      "venue": "Ldb: A large language model debugger via verifying runtime execution step-by-step",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Panda: Performance debugging for databases using llm agents",
      "authors": [
        "Singh Vikramank",
        "Vaidya Kapil Eknath",
        "Kumar Vinayshekhar Bannihatti",
        "Khosla Sopan",
        "Narayanaswamy Murali",
        "Gangadharaiah Rashmi",
        "Kraska Tim"
      ],
      "year": "2024",
      "venue": "Amazon Science",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Repairagent: An autonomous, llm-based agent for program repair",
      "authors": [
        "Bouzenia Islem",
        "Devanbu Premkumar",
        "Pradel Michael"
      ],
      "year": "2024",
      "venue": "Repairagent: An autonomous, llm-based agent for program repair",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "An analysis of the automatic bug fixing performance of chatgpt",
      "authors": [
        "Sobania Dominik",
        "Briesch Martin",
        "Hanna Carol",
        "Petke Justyna"
      ],
      "year": "2023",
      "venue": "IEEE/ACM International Workshop on Automated Program Repair",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Debugging With Open-Source Large Language Models:",
      "text": "An Evaluation Yacine Majdoub University of Gabes, Tunisia yacinemajdoub@fsg.u-gabes.tn Eya Ben Charrada University of Gabes, Tunisia eya.benchmarkrada@fsg.mu.tn"
    },
    {
      "title": "Abstract.",
      "text": "Large language models have shown good potential in supporting software development tasks. This is why more and more developers turn to LLMs (e.g. ChatGPT) to support them in fixing their buggy code. While this can save time and effort, many companies prohibit it due to strict code sharing policies. To address this, companies can run open-source LLMs locally. But until now there is not much research evaluating the performance of open-source large language models in debugging. This work is a preliminary evaluation of the capabilities of open-source LLMs in fixing buggy code. The evaluation covers five open-source large language models and uses the benchmark DebugBench which includes more than 4000 buggy code instances written in Python, Java and C++. Open-source LLMs achieved scores ranging from 43.9% to 66.6% with DeepSeek-Coder achieving the best score for all three programming languages. Debugging, Large Language Models, Open-Source LLMs + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 20244 Copyright held by the owner/author(s). Publication rights licensed to ACM. + Footnote †: 2 [MISSING_PAGE_FAIL:2] language models and (2) it provides a comprehensive test suit that allows verifying whether the bug was fixed or not."
    },
    {
      "title": "3.1.1. Data",
      "text": "Tian et al (Tian et al, 2018) created the dataset using problem descriptions and code solutions from LeetCode. The authors used GPT-4 to automatically introduce bugs to the code and then used human inspections to check the integrity of the benchmark. To minimize the risk of leakage the authors used code that was released on LeetCode after June 2022 with the average release date being April 2023. To ensure that the extracted code is correct, the authors selected only code that passes all the tests related to it. The authors of the benchmark develop a bug taxonomy based on Barr's classification criteria that covers four major bug categories (Syntax, Reference, Logic and Multiple) as well as 18 minor bug types. The bugs were introduced by instructing GPT-4 to add a certain type of bugs to the code. Since GPT sometimes fails in including bugs in the code, the authors filtered out the code that does not fail certain tests. The benchmark includes 761 instances with syntax errors, 684 instances with reference errors, 590 instances with logic errors and 2218 instances with multiple errors. A description of the number of instances for each programming language is provided in Table 1"
    },
    {
      "title": "3.1.2. Metrics",
      "text": "DebugBench assesses whether a bug is fixed or not by using a set of tests that is provided by LeetCode. If all tests pass, then the bug is considered to be fixed, otherwise, the bug is not fixed. The metric used in DebugBench is the _Pass Rate_, which is the number of bugs for which all corresponding tests have passed (repaired bugs) divided by the total number of bugs. (Tian et al, 2018). More formally, the Pass Rate is defined as follows: for each buggy code \\(\\theta_{i}\\) and its fixed version \\(\\theta_{i}^{*}\\), there is a set of test cases: \\((x_{i}^{0},y_{i}^{0})\\), \\((x_{i}^{1},y_{i}^{1})\\),..., \\((x_{i}^{m},y_{i}^{m})\\) to test it, where \\(x_{i}\\) is the input and \\(y_{i}\\) is the corresponding desired output. Let \\(a_{\\theta}(x)=y\\) denote a program a, based on the script \\(\\theta\\) that maps input \\(x\\) to output \\(y\\). A bug is considered to be repaired if all tests pass which can be referred to as \\[\\bigwedge_{j=0}^{m}[a_{\\theta_{i}^{*}}(x_{i}^{j})=y_{i}^{j}]\\] This criteria allows for a conservative measure of the bug fixing capabilities. In fact, if the instance contains multiple bugs, it is considered to be repaired only if **all** bugs within the instance are fixed. For \\(n\\) bug instances, the pass rate would be: \\[PR=\\sum_{i=0}^{n}\\frac{\\bigwedge_{j=0}^{m_{j}}[a_{\\theta_{i}^{*}}(x_{i}^{j})=y _{i}^{j}]}{n}\\times 100\\%\\]"
    },
    {
      "title": "Experimental Setup",
      "text": "To run the open-source LLMs, we use the platform together.ai3. This platform offers an interface that we used to prompt each of the models and get their responses. To test whether the fix is correct or not we use the LeetCode test suite to test the proposed answer. The tests were accessed using an interface to LeetCode. If all tests passed, the fix was considered as successful. Footnote 3: [https://www.together.ai/](https://www.together.ai/) _Prompt design._ We used a zero-shot prompting. The prompt given to the LLM contained a description of the problem to be solved, the buggy code, as well as a description of the task to be done, i.e. debugging the code. We tweaked the prompts used by the original DebugBench paper so that we only get the fixed code in the output. The prompt we used is presented in Listing 1. ``` Observethefollowing<<Programminglanguage>> faultycodewhichiscompettewithnoextracontext. Yourtaskistofixupthecode,notchangeit. Youhavetowritethefixedcode(classSolution).Donotwriteanythingelsebesefhefixedcode inyresponse.BuggyCode.<<<Buggycode> Goal:<<Description>> ``` Listing 1: Prompt _Code extraction._ Some models returned additional explanatory text with the code, as shown in the WizardCoder example in Listing 2. To avoid failing the tests because of the additional text, we wrote a script to extract the code (class Solution) from the answer generated by the LLM. The extracted code is then used to run the automated tests. ``` Solution: classSolution: deffindComponent(self,num:int)->int: com=\" whilenum>0: ifnumX2==1: com='0'+com else: com='1'+com num=num/2 returnint(con,2) Explanation: Thebuginthecodeisthatthecomstringisnotbeingupdatedproperly.Intheifblock,weareadding0insteadof'0'andintheelseblock,weareadding1insteadof'1'.Weneedtoaddthestringrepresentationof0and!tothecomstring.Also,weneedtoreturntheintegervalueofthebinarystring,sowneedtoremovethefirstcharacterofthestringbeforeconvertingittointeger. ``` _Experiment repetition._ Since there is some randomness in the response of LLMs, we repeated the experiment twice for all five models on the C++ dataset. The goal of the repetition is to verify the reliability of the results. \\begin{table} \\begin{tabular}{c|c} \\hline **Programming Language** & **Buggy Instances** \\\\ \\hline C++ & 1438 \\\\ Java & 1401 \\\\ Python & 1414 \\\\ Total & 4253 \\\\ \\hline \\end{tabular} \\end{table} Table 1. Number of buggy code instances per programming language in DebugBench"
    },
    {
      "title": "4. Results",
      "text": ""
    },
    {
      "title": "Rq1: Performance Of Open-Source Llms",
      "text": "We report in Table 2, the number of fixed problems for each model as well as the achieved pass rate for each programming language. We also report the average pass rate for all languages and the costs of running the evaluation in USD. The evaluated LLMs achieved results ranging from 43.9% to 66.6%. The best score was achieved by the code model DeepSeek-Coder which achieved a pass rate above 63% for each of the languages. Both Codellama-instruct and Phind-Codellama achieved a pass rate below 50%. Still, we notice that Phind-Codellama which is a fine-tuning of the Codellama-34B achieved better results than the larger Codellama-70B. Llama3, which is a general purpose LLM achieved almost 60% pass rate, which is considerably better than Llama2-based code models. In Figure 1 We see that DeepSeek-Coder performed best on all three programming languages, while the Llama-2 based models (Codellama and Phind-Codellama) had the lowest scores for all three languages. This suggests that if a model is fluent in one programming language, then it is also likely to be fluent in other languages. Model size was not a determining factor in the performance of the models. In fact, some medium sized models, such as Phind-Codellama and DeepSeek-Coder, were able to achieve better scores than Codellama which had twice the size. Regarding the costs of running the models, smaller models (33B and 34B) costed an average of 0.08 cent per code instance while larger models (70B) costed an average of 0.09 cent per code instance. So the difference in price between the models is negligible. Closed source models, namely GPT-4 and GPT-3.5 from OpenAI achieved 75.0% and 62.1% respectively as reported by the DebugBench paper (Dubg et al., 2018). None of the evaluated tools was able to get a score that is comparable to GPT-4 and only DeepSeek-Coder was able to achieve results that are better than GPT-3.5 **RQ1 answer:** There is a lot of variation between the scores of the different models, but some open source models were able to achieve decent results. Compared to top closed source models, only one open source was able to achieve results that are better than the GPT-3.5 score."
    },
    {
      "title": "Rq2: Relation Between Coding And Debugging Performance",
      "text": "We report the scores that the models achieved on HumanEval and on DebugBench in Figure 2. All five models achieved higher scores in the HumanEval coding benchmark compared to the benchmark DebugBench. In fact, all models were able to successfully solve the HumanEval coding problems with a pass rate ranging between 67% and 81%. The only benchmark that showed similar capabilities in coding and debugging is the DeepSeek-Coder model which achieved a pass rate of 69.2% on HumanEval and a pass rate of 66.65% on DebugBench. When comparing the performance of the LLMs on both benchmarks, we see that except for DeepSeek-Coder, the models who had better results on HumanEval also achieved better scores in DebugBench. From these observations, we see that although there is a hint that models that are better in coding might be better in debugging, we see no definite connection between the performance of LLMs in coding, as evaluated by HumanEval, and their performance in debugging as evaluated by DebugBench. **RQ2 answer:** For four out of the five evaluated LLMs we noticed a relation between the performance of the models on HumanEval and their performance on DebugBench. \\begin{table} \\begin{tabular}{l|l|l|l|l|l|l|l|l} \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{2}{c|}{**Python**} & \\multicolumn{2}{c|}{**Java**} & \\multicolumn{2}{c|}{**C++**} & \\multicolumn{1}{l|}{**Final score**} & \\multicolumn{1}{l|}{**Costs**} \\\\ \\cline{2-9} & **Fixed Problems** & **Pass rate** & **Fixed Problems** & **Pass rate** & **Fixed Problems** & **Pass rate** & \\\\ \\hline Codellama-instruct-70b & 589/1414 & 41.65\\% & 553/1401 & 39.47\\% & 728/1438 & 50.62\\% & 43.96\\% & 54.20 \\\\ \\hline Phind-codellama-34b-v2 & 694/1414 & 49.08\\% & 550/1401 & 39.25\\% & 830/1438 & 57.71\\% & 48.76\\% & 53.80 \\\\ \\hline WizardCoder\\_instruct-33b & 813/1414 & 57.49\\% & 708/1401 & 50.53\\% & 834/1438 & 58.98\\% & 55.37\\% & 53.70 \\\\ \\hline Deepseek-coder-instruct-33b & 893/1414 & **63.15\\%** & 971/1401 & **69.30\\%** & 971/1438 & **64.81\\%** & **66.65\\%** & 53.70 \\\\ \\hline Llama3-70b & 880/1414 & 62.23\\% & 755/1401 & 53.89\\% & 859/1438 & 59.73\\% & 58.61\\% & 54.10 \\\\ \\hline \\end{tabular} \\end{table} Table 2. Debugging performance & running costs of the evaluated open-source large language models Figure 1. A visualisation of the debugging performance of the LLMs for code in Python, C++ and Java"
    },
    {
      "title": "5. Discussion",
      "text": ""
    },
    {
      "title": "Comparing With Previous Results",
      "text": "There has been a few other works that evaluated the capabilities of LLMs for debugging. In this section, we compare our results with the results of other researchers. First, we compare our results to the score reported by Tian et al (Tian et al., 2018), the authors of the DebugBench paper. The authors tested three open source models _Bloom_, _codellama_ and _codellama_ - _inst_, and got a pass rate of 0.0 for all three of them. This means that none of the open source tools could repair any of the bugs. In our case, the models were able to achieve decent results. This is probably due to the fact that all code models returned answers that contained not only code but also explanatory text. The only model that returned code only was the general purpose Ilama3. The additional text makes the tests fail on LectCode and this explains the 0.0 score obtained in the evaluation of Tian et al (Tian et al, 2018). In our experiment, we used a script to extract the code only from the answer and this lead to a positive performance of the models. Lee et al. (Lee et al., 2019) compared the debugging performance of some closed source and open source LLMs using benchmarks in C, Java and Python. The authors generated 3 patches for each bug and looked for a plausible or correct patch among the generated responses. Codellama generated correct patches for 25/40 bugs in Java and for 33/40 bugs in Python, while DeepSeek-Coder achieved 30/40 and 25/40 correct patches for Java and Python respectively. The authors found that both GPT-3.5-Turbo-0125 (175B) and GPT-4 generated a higher number of correct patches than the open source models. In our experiment, DeepSeek-Coder achieved better output than codellama in all programming languages. This difference in the results might be due to the fact that Lee et al. (Lee et al., 2019) used the DeepSeek-Coder-Base, while in our experiment we use DeepSeek-Coder-Instruct. The instruct version seems to have better coding capabilities as it is reported to achieve 69.2% pass@1 on HumanEval compared to 50.3% pass@1 for the base version. In a study about vulnerability detection, Steenhoek et al. (Steenhoek et al., 2019) found that LLMs were unable to differentiate between buggy and fixed code. They report that LLMs performed only slightly better than random guessing and that they performed far worse on complex debugging tasks from DBGBench. The evaluation was done using 100 functions from the SVEN dataset which is in C/C++ as well as the 27 bugs from DBGBench (Ding et al., 2019). These low results could be explained by the complexity of the tasks performed in (Steenhoek et al., 2019). Previous research by Huang and Changs (Huang and Changs, 2019) has already shown that LLMs seem to be unable to manage complex tasks. The authors also note that existing benchmarks might be too simple to assess reasoning ability correctly (Huang and Changs, 2019). In another evaluation of code generation capabilities by Liu et al. (Liu et al., 2019), the authors used HumanEval+ which is an improvement of the classic HumanEval. The authors found that the two open source models Phind-Codellama and WizardCoder achieved scores that are better than ChatGPT but worse than GPT-4. In our experiment, both open source models achieved a score that is lower than GPT-4 and GPT-3.5. Since the authors did not specify the version of chatGPT that was used, this could be due to the fact that they used a version of chatGPT that is less effective than GPT-3.5."
    },
    {
      "title": "Contamination",
      "text": "One of the challenges in the evaluation of LLMs is contamination (Jain et al., 2017; Liu et al., 2018; Liu et al., 2019). For example, Jain et al. (Jain et al., 2017) found that there was a drop in the performance of DeepSeek-Coder-Instruct on LectCode problems that were released since September 2023, its release date. The authors interpret this as an indication of a potential contamination. Although some research is being conducted on how to evaluate and remove contamination (Huang and Changs, 2019), decontamination does seem to be an easy task. In a study of code LLMs, Cao et al. (Cao et al., 2020) found that existing countermeasures for contamination such as using more recent data, using curated datasets or syntactic refactoring may not be effective. Among the models we evaluated, only two mentioned using a decontamination strategy. OpenAI's decontamination methodology seems to have been applied to the \\begin{table} \\begin{tabular}{l|l|l|l|l} \\hline \\hline **Model** & **First test pass rate** & **Second test pass rate** & **Mean** & **Standard deviation** \\\\ \\hline Codellama-Instruct-70b & 50.62\\% & 51.55\\% & 51.08\\% & 0.465 \\\\ \\hline Phind-Codellama-34b-v2 & 57.71\\% & 55.94\\% & 56.82\\% & 0.885 \\\\ \\hline WizardCoder-Instruct-33b & 58.98\\% & 55.79\\% & 57.38\\% & 1.595 \\\\ \\hline DeepSeek-Coder-Instruct-33b & 64.81\\% & 66.33\\% & 65.57\\% & 0.76 \\\\ \\hline \\hline \\end{tabular} \\begin{tabular}{l|l|l|l|l} \\hline \\hline **Llama3-70b** & 59.73\\% & 60.32\\% & 60.02\\% & 0.295 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3. Scores achieved for the C++ dataset during two runs and the calculated Mean and Standard deviation for each model Figure 2. Pass rates achieved by the open-source LLMs when evaluated on HumanEval and on DebugBenchPhind's dataset, and the DeepSeek-Coder developers mention filtering out data from benchmarks such as HumanEval, MBPP, GSM8K and MATH. All other models didn't mention any decontamination strategy. The DebugBench data has been published on GitHub on January 9th, so all evaluated models had a knowledge cutoff date prior to the benchmark release. Although this might decrease the contamination threat, it cannot fully eliminate it due to the fact that the LeetCode problems and solutions might have been used for pre-training the models."
    },
    {
      "title": "Result Reliability",
      "text": "LLMs are known to have randomness in their responses. So to check the reliability of the results, we run the experiment twice on the C++ dataset. We report the scores for both experiments in Table 3. We performed a statistical analysis of the results by calculating the mean and the standard deviation for each pair of measurements, the results are reported in the same Table. The deviations for all pair range between 0.29 and 1.59 which is relatively small. This indicates that the measurements are consistent and therefore likely reliable."
    },
    {
      "title": "6. Threats To Validity",
      "text": "_Internal validity._ Similarly to other LLM evaluations, we face a major internal threat due to the possible overlap between the training data and the evaluation dataset. We have already discussed possible contamination in Section 5.2. DebugBench was published after the knowledge cutoff date of the models. Although this might limit the threat, it cannot be fully eliminated because the used problems and their solutions existed on LeetCode before the benchmark release. The threat might also limited by the fact that the tests used to evaluate the debugging capabilities are not public and are only accessible for running via the LeetCode platform, so we know that these tests were not included in the training data of the evaluated LLMs. The randomness in LLMs can also constitute a threat to the internal validity of the experiment. In fact, LLMs can produce different answers for the same prompt. To limit this threat, we repeated the experiment with C++ data twice. Our analysis in Section 5.3 shows that the measurements are consistent and likely to be reliable. _External validity._ The main external validity threat lies in the benchmark code not being generalizable to other types of code. We argue that this threat is limited since the LeetCode dataset covers a variety of coding problems with different levels of difficulties. It also covers code in three different programming languages. Nevertheless, the results might not be generalizable to coding problems that are of different nature such as front-end developement, or code that uses specific libraries. In the future, We will evaluate the LLMs with more datasets."
    },
    {
      "title": "7. Related Work",
      "text": ""
    },
    {
      "title": "Use Of Llm For Debugging",
      "text": "The promising results about the capabilities of LLMs in software engineering lead to a surge in approaches that use LLMs to support debugging activities. Kang et al. (Kang et al., 2019) introduced AutoSD, a method that leverages large language models and debuggers to automatically generate hypotheses and interact with buggy code, enabling conclusions to be drawn prior to patching. Feng et al. (Feng et al., 2019) presented AdbGPT, a lightweight approach that employs prompt engineering to reproduce bugs from reports automatically, without the need for training or hard-coding. Zhong et al. (Zhong et al., 2019) developed LDB, a debugging framework designed to assist large language models in refining generated programs by utilizing runtime execution data, segmenting programs into basic blocks, and tracking intermediate variable values after each block. Singh et al. (Singh et al., 2019) proposed Panda, a framework aimed at providing context grounding to pre-trained large language models, thereby generating more useful and contextually relevant troubleshooting recommendations. Bouzenia et al. (Bouzenia et al., 2019) introduced RepairAgent, an autonomous program repair agent that relies on a large language model. RepairAgent interleaves the processes of gathering information about the bug, collecting repair ingredients, and validating fixes, while dynamically deciding which tools to invoke based on the collected information and feedback from previous fix attempts."
    },
    {
      "title": "Evaluation Of Llms In Debugging",
      "text": "Several evaluations have been conducted to evaluate of the performance of LLMs in debugging. For example, Wu et al. (Wu et al., 2019) investigated the capabilities of ChatGPT-3.5 and ChatGPT-4 in fault localisation. Sobania et al. (Sobania et al., 2019) evaluated the bug fixing performance of ChatGPT using the QuixBugs benchmark. Tian at al. (Tian et al., 2019) evaluated the performance of five closed and open-source large language models using DebugBench. Lee et al. (Lee et al., 2019) compared their agent to other LLMs including two open-source LLMs, namely CodeLlama and DeepSeek-Coder. Most of these works focused on evaluating the performance of the chatGPT, which is a closed source model. Only the works of Tian et al. (Tian et al., 2019) and Lee et al. (Lee et al., 2019) cover some open source models. In this work, our goal was to evaluate different open-source LLMs and compare their capabilities."
    },
    {
      "title": "8. Conclusion And Future Work",
      "text": "In this work, we evaluated the debugging capabilities of five open-source large language models. The evaluation was done using DebugBench, a benchmark that includes a dataset of 4253 buggy code instances in Python, C++ and Java. Our results show that the capabilities of all the evaluated open-source LLMs are lower than the capabilities of the most recent closed-source model (GPT-4). Still, considering their relatively small size, some open-source LLMs were able to achieve decent results. For instance, DeepSeek-Coder which is only 33B in size achieved a score above 66%. One limitation of our evaluation, is that the used code instances are limited to one class only and are mostly solutions to algorithmic problems. So In the future, we would like to evaluate open-source LLMs using a wider variety of code types. Also we would like to evaluate the usefulness of the LLMs for practitioners when performing debugging tasks, with a special focus on complex tasks. Finally, we intend to explore how the debugging performance of open-source LLMs is impacted by prompt engineering and chain-of-thought prompting."
    },
    {
      "title": "References",
      "text": "* [1]K. Ranim, M. Mazen, L. Philipp, and N. Francisco Gomes de Oliveira (2024) Beyond code generation: an observational study of chatpst usage in software engineering practice. In Proc. ACM Softw Eng., Cited by: SS1, SS2. * [2]K. Sun, Y. Juyeon, and Y. Shin (2023) Large language models are few-shot testers: exploring llm-based general bug reproduction. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 2312-2323. Cited by: SS1, SS2. * [3]W. Yonghao, L. Zheng, Z. J. M. P. Papadakis, M. Harman, and Y. Liu (2023) Large language models in fault localisation. arXiv preprint arXiv:2308.15276. Cited by: SS1, SS2. * [4]X. Chunqiu Steven, W. Yuxiang, and Z. Lingming (2023) Automated program repair in the era of large pre-trained language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 1482-1494. Cited by: SS1, SS2. * [5]M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, and W. Zaremba (2021) Evaluating large language models trained on code. Cited by: SS1, SS2. * [6]L. Jiawei, X. Chuangtiu Steven, W. Yuyao, and Z. Lingming (2024) Is your code generated by chatpst really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems36. Cited by: SS1, SS2. * [7]A. Ben, G. Sanjay Krishna, W. Zijian, L. Xiaopeng, T. Yuchen, T. Ming, A. W. Uldin, W. Shiqi, S. Qing, S. Mingyue, et al. (2022) Multi-lingual evaluation of code generation models. In The Eleventh International Conference on Learning Representations, Cited by: SS1, SS2. * [8]J. Austin, A. Odena, M. Nye, M. Bosn, H. Michalewski, D. Dolan, E. Jang, C. Cai, M. Terry, Q. Le, and C. S. Sutton (2021) Program synthesis with large language models. Cited by: SS1, SS2. * [9]B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. Lin, and C. S. Tenne (2024) Code lamma: open foundation models for code. Cited by: SS1, SS2. * hugging face. Note: [http://huggingface.com/Print/Phail-Chodellar-3d0v2-z](http://huggingface.com/Print/Phail-Chodellar-3d0v2-z) Cited by: SS1, SS2. * [11]Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang (2023) WizardCober: improving code large language models with end-lstured. Cited by: SS1, SS2. * [12]X. Sun, Q. Zheng Kai, G. Xiubo, Z. Pu, F. Jiazhan, T. Chongyang, and J. Zhu (2024) WiardIm: improving large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Cited by: SS1, SS2. * [13]R. Li, L. B. All, Y. Li, N. Mueminghoff, D. Koecklov, C. Mou, and H. de Vries (2023) StarCoder: may the source be with you!. Cited by: SS1, SS2. * the rise of code intelligence. Cited by: SS1, SS2. * hugging face. Note: [http://huggingface.com/meta-limma/Meta-limma-3d-8f3-trnct](http://huggingface.com/meta-limma/Meta-limma-3d-8f3-trnct) Cited by: SS1, SS2. * [16]H. Wilhelm (2021) Benchmarking as empirical standard in software engineering research. In Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering, pp. 365-372. Cited by: SS1, SS2. * [17]P. Ralph, S. Balbes, D. Biancelli, Y. Dittrich, M. Felderer, R. Feldt, and S. Vegas (2020) ACM SIGSOFT empirical standards. Note: CRCR, abs/2010.03552 Cited by: SS1, SS2. * [18]R. Tian, Y. Ye, Y. Qin, X. Cong, Y. Lin, Y. Pan, Y. Wu, Z. Liu, and M. Sun (2024) Debugbench: evaluating debugging capability of large language models. Cited by: SS1, SS2. * [19]C. Lee, C. S. Xia, J. Huang, Z. Zhu, L. Zhang, and M. R. Ly (2024) A unified debugging approach via llm-based multi-agent synergy. arXiv preprint arXiv:2404.11753. Cited by: SS1, SS2. * [20]S. Renjamin, R. M. Mahlubour, R. Kumar, A. Mirza Sanjda, B. Earl T, and L. Wei (2024) A comprehensive study of the capabilities of large language models for vulnerability detection. arXiv preprint arXiv:2403.17218. Cited by: SS1, SS2. * [21]R. Marcel, S. E. O. Chattopadhyay Sudipta, U. Emanurho, and Z. Andreas (2017) Where is the bug and how is it fixed? an experiment with practitioners. In Proceedings of the 2017 11th joint meeting on foundations of software engineering, pp. 117-128. Cited by: SS1, SS2. * [22]H. Jie and C. Kevin Chen-Chum (2023) Towards reasoning in large language models: a survey. In 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023, pp. 1049-1065. Cited by: SS1, SS2. * [23]O. Sainz, J. Campos, I. Garcia-Ferrero, J. Etxaniz, O. Lopez de Lacalle, and E. Agirre (2023) Nlp evaluation in trouble: on the need to measure llm data contamination for each benchmark. arXiv preprint arXiv:2310.18018. Cited by: SS1, SS2. * [24]M. Ravaut, B. Ding, F. Jiao, H. Chen, X. Li, R. Zhao, C. Qin, C. Xiong, and S. Joty (2024) How much are l lms contaminated? a comprehensive survey and the lmsmanite library. arXiv preprint arXiv:2404.06069. Cited by: SS1, SS2. * [25]S. Balloccu, P. Schmidlova, M. Lango, and O. Dusek (2024) Leach, repeat: data contamination and evaluation malpractices in closed-source lms. arXiv preprint arXiv:2402.03827. Cited by: SS1, SS2. * [26]N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Senuik, and I. Stoica (2024) LivecoDebench: holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Cited by: SS1, SS2. * [27]S. Yang, W. Chiang, L. Zheng, J. E. Gonzalez, and I. Stoica (2023) Rethinking benchmark and contamination for language models with rephrased samples. arXiv preprint arXiv:2311.04850. Cited by: SS1, SS2. * [28]J. Y. Zhang, S. Zhang, and S. Cheung (2024) Concerned with data contamination: assessing countermeasures on code language model. arXiv preprint arXiv:2403.16898. Cited by: SS1, SS2. * [29]K. Zhang, J. Zhang, C. Li, Y. Li, and Y. Liu (2024) A survey and the lmsmanite library. arXiv preprint arXiv:2404.06069. Cited by: SS1, SS2. * [30]K. Zhang, Y. Wang, and J. Shang (2024) LML: a large language model developer via verifying runtime execution step-by-step. CoRR, February 2024. Cited by: SS1, SS2. * [31]S. Yuzman, V. K. E. Ekrath, K. Vinayeshelkarn Bamhati, K. Sopan, N. Muraisamy Muraki, G. Rashimi, and K. Tim (2024) Panda: performance debugging for databases using llm agents. Amazon Science. Cited by: SS1, SS2. * [32]B. Islem, D. Premkumar, and P. Michael (2024) Repairer: an autonomous, llm-based agent for program repair. arXiv preprint arXiv:2403.11734. Cited by: SS1, SS2. * [33]S. Dominik, R. Martin, H. Carol, and P. Justyna (2023) An analysis of the automatic bug fixing performance of chatpst. In 2023 IEEE/ACM International Workshop on Automated Program Repair (APR), pp. 23-30. Cited by: SS1, SS2."
    }
  ]
}