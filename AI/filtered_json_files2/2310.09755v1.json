{
  "title": "BEYOND SEGMENTATION: ROAD NETWORK GENERATION WITH MULTI-MODAL LLMS *",
  "authors": [
    "Sumedh Rasal",
    "Sanjay Kumar Boddhu"
  ],
  "abstract": "\n This paper introduces an innovative approach to road network generation through the utilization of a multi-modal Large Language Model (LLM). Our model is specifically designed to process aerial images of road layouts and produce detailed, navigable road networks within the input images. The core innovation of our system lies in the unique training methodology employed for the large language model to generate road networks as its output. This approach draws inspiration from the BLIP-2 architecture [1], leveraging pre-trained frozen image encoders and large language models to create a versatile multi-modal LLM. Our work also offers an alternative to the reasoning segmentation method proposed in the LISA paper  [2] . By training the large language model with our approach, the necessity for generating binary segmentation masks, as suggested in the LISA paper  [2] , is effectively eliminated. Experimental results underscore the efficacy of our multi-modal LLM in providing precise and valuable navigational guidance. This research represents a significant stride in bolstering autonomous navigation systems, especially in road network scenarios, where accurate guidance is of paramount importance. \n",
  "references": [
    {
      "id": null,
      "title": "BEYOND SEGMENTATION: ROAD NETWORK GENERATION WITH MULTI-MODAL LLMS *",
      "authors": [
        "Sumedh Rasal",
        "Sanjay Kumar Boddhu"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Reasoning segmentation via large language model",
      "authors": [
        "Xin Lai",
        "Zhuotao Tian",
        "Yukang Chen",
        "Yanwei Li",
        "Yuhui Yuan",
        "Shu Liu",
        "Jiaya Jia",
        "Lisa"
      ],
      "year": "2023",
      "venue": "Reasoning segmentation via large language model",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "",
      "authors": [],
      "year": "2023",
      "venue": "OpenAI. Gpt-4 technical report. Open AI",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Wook Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Eric Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Enabling multimodal generation on clip via vision-language knowledge distillation",
      "authors": [
        "Wenliang Dai",
        "Lu Hou",
        "Lifeng Shang",
        "Xin Jiang",
        "Qun Liu",
        "Pascale Fung"
      ],
      "year": "2022",
      "venue": "Enabling multimodal generation on clip via vision-language knowledge distillation",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Next-gpt: Any-to-any multimodal llm",
      "authors": [
        "Shengqiong Wu",
        "Hao Fei",
        "Leigang Qu",
        "Wei Ji",
        "Tat-Seng Chua"
      ],
      "year": "2023",
      "venue": "Next-gpt: Any-to-any multimodal llm",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph E Gonzalez"
      ],
      "year": "2023",
      "venue": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Opt: Open pre-trained transformer language models",
      "authors": [
        "Susan Zhang",
        "Stephen Roller",
        "Naman Goyal",
        "Mikel Artetxe",
        "Moya Chen",
        "Shuohui Chen",
        "Christopher Dewan",
        "Mona Diab",
        "Xian Li",
        "Xi Victoria Lin"
      ],
      "year": "2022",
      "venue": "Opt: Open pre-trained transformer language models",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Making visual representations matter in vision-language models",
      "authors": [
        "Pengchuan Zhang",
        "Xiujun Li",
        "Xiaowei Hu",
        "Jianwei Yang",
        "Lei Zhang",
        "Lijuan Wang",
        "Yejin Choi",
        "Jianfeng Gao",
        "Vinvl"
      ],
      "year": "2021",
      "venue": "Making visual representations matter in vision-language models",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Lit: Zero-shot transfer with locked-image text tuning",
      "authors": [
        "Xiaohua Zhai",
        "Xiao Wang",
        "Basil Mustafa",
        "Andreas Steiner",
        "Daniel Keysers",
        "Alexander Kolesnikov",
        "Lucas Beyer"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Caiming Xiong",
        "Steven Hoi"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Multimodal few-shot learning with frozen language models",
      "authors": [
        "Maria Tsimpoukelli",
        "Jacob L Menick",
        "Serkan Cabi",
        "Oriol Eslami",
        "Felix Vinyals",
        "Hill"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Generating images with multimodal language models",
      "authors": [
        "Jing Yu Koh",
        "Daniel Fried",
        "Ruslan Salakhutdinov"
      ],
      "year": "2023",
      "venue": "Generating images with multimodal language models",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Unified language model pre-training for natural language understanding and generation",
      "authors": [
        "Li Dong",
        "Nan Yang",
        "Wenhui Wang",
        "Furu Wei",
        "Xiaodong Liu",
        "Yu Wang",
        "Jianfeng Gao",
        "Ming Zhou",
        "Hsiao-Wuen Hon"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Kilichbek Haydarov",
        "Xiaoqian Shen",
        "Wenxuan Zhang",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "",
      "authors": [],
      "year": "2022",
      "venue": "OpenAI. Introducing chatgpt",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Emergent abilities of large language models",
      "authors": [
        "Jason Wei",
        "Yi Tay",
        "Rishi Bommasani",
        "Colin Raffel",
        "Barret Zoph",
        "Sebastian Borgeaud",
        "Dani Yogatama",
        "Maarten Bosma",
        "Denny Zhou",
        "Donald Metzler"
      ],
      "year": "2022",
      "venue": "Emergent abilities of large language models",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Stanford alpaca: An instruction-following llama model",
      "authors": [
        "Rohan Taori",
        "Ishaan Gulrajani",
        "Tianyi Zhang",
        "Yann Dubois",
        "Xuechen Li",
        "Carlos Guestrin",
        "Percy Liang",
        "Tatsunori B Hashimoto"
      ],
      "year": "2023",
      "venue": "Stanford alpaca: An instruction-following llama model",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Exploring the limits of masked visual representation learning at scale",
      "authors": [
        "Yuxin Fang",
        "Wen Wang",
        "Binhui Xie",
        "Quan Sun",
        "Ledell Wu",
        "Xinggang Wang",
        "Tiejun Huang",
        "Xinlong Wang",
        "Yue Cao",
        "Eva"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "authors": [
        "Yash Goyal",
        "Tejas Khot",
        "Douglas Summers-Stay",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "From images to textual prompts: Zero-shot vqa with frozen large language models",
      "authors": [
        "Jiaxian Guo",
        "Junnan Li",
        "Dongxu Li",
        "Anthony Meng",
        "Huat Tiong",
        "Boyang Li",
        "Dacheng Tao",
        "Steven Ch Hoi"
      ],
      "year": "2022",
      "venue": "From images to textual prompts: Zero-shot vqa with frozen large language models",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Unifying vision-and-language tasks via text generation",
      "authors": [
        "Jaemin Cho",
        "Jie Lei",
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "A jointly-scaled multilingual language-image model",
      "authors": [
        "Xi Chen",
        "Xiao Wang",
        "Soravit Changpinyo",
        "Piotr Piergiovanni",
        "Daniel Padlewski",
        "Sebastian Salz",
        "Adam Goodman",
        "Basil Grycner",
        "Lucas Mustafa",
        "Beyer"
      ],
      "year": "2022",
      "venue": "A jointly-scaled multilingual language-image model",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
      "authors": [
        "Zhengyuan Yang",
        "Linjie Li",
        "Jianfeng Wang",
        "Kevin Lin",
        "Ehsan Azarnasab",
        "Faisal Ahmed",
        "Zicheng Liu",
        "Ce Liu",
        "Michael Zeng",
        "Lijuan Wang"
      ],
      "year": "2023",
      "venue": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Zero-shot video question answering via frozen bidirectional language models",
      "authors": [
        "Antoine Yang",
        "Antoine Miech",
        "Josef Sivic",
        "Ivan Laptev",
        "Cordelia Schmid"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Palm-e: An embodied multimodal language model",
      "authors": [
        "Danny Driess",
        "Fei Xia",
        "S M Mehdi",
        "Corey Sajjadi",
        "Aakanksha Lynch",
        "Brian Chowdhery",
        "Ayzaan Ichter",
        "Jonathan Wahid",
        "Quan Tompson",
        "Tianhe Vuong",
        "Yu"
      ],
      "year": "2023",
      "venue": "Palm-e: An embodied multimodal language model",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Language is not all you need: Aligning perception with language models",
      "authors": [
        "Shaohan Huang",
        "Li Dong",
        "Wenhui Wang",
        "Yaru Hao",
        "Saksham Singhal",
        "Shuming Ma",
        "Tengchao Lv",
        "Lei Cui",
        "Owais Khan Mohammed",
        "Qiang Liu"
      ],
      "year": "2023",
      "venue": "Language is not all you need: Aligning perception with language models",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Beyond Segmentation: Road Network Generation With Multi-Modal Llms +",
      "text": "Footnote †: _Citation_: **Authors. Title.** Pages....** DOI:000000/111111.** Sumedh Rasal HERE North America LLC Georgia Institute Of Technology srasal3@gatech.edu &Sanjay Kumar Boddhu HERE North America LLC sanjaykumar.boddhu@here.com"
    },
    {
      "title": "Abstract",
      "text": "This paper introduces an innovative approach to road network generation through the utilization of a multi-modal Large Language Model (LLM). Our model is specifically designed to process aerial images of road layouts and produce detailed, navigable road networks within the input images. The core innovation of our system lies in the unique training methodology employed for the large language model to generate road networks as its output. This approach draws inspiration from the BLIP-2 architecture [1], leveraging pre-trained frozen image encoders and large language models to create a versatile multi-modal LLM. Our work also offers an alternative to the reasoning segmentation method proposed in the LISA paper [2]. By training the large language model with our approach, the necessity for generating binary segmentation masks, as suggested in the LISA paper [2], is effectively eliminated. Experimental results underscore the efficacy of our multi-modal LLM in providing precise and valuable navigational guidance. This research represents a significant stride in bolstering autonomous navigation systems, especially in road network scenarios, where accurate guidance is of paramount importance. Multi-Modal Language Models Road Network Generation Autonomous Navigation"
    },
    {
      "title": "1 Introduction",
      "text": "In recent years, the field of large language models (LLMs) has witnessed a remarkable transformation, transitioning from text-based generation to the generation of diverse modalities, including text, images, audio, and video, all through a single LLM. This evolution, from ChatGPT-4 [3] to a myriad of multi-modal LLMs, has significantly advanced the capabilities of AI systems to process and understand various forms of data. These multi-modal LLMs are designed to emulate the holistic perceptual abilities of humans, enabling them to process and generate content in more versatile ways. Unlike previous models, such as ChatGPT-4 [3], MiniGPT-4 [4], LISA [2], and others [5], which aimed to be general-purpose multi-modal models [6][7], our work introduces a novel approach that tailors the training of such models to address a specific challenge: generating navigable road networks from aerial images. In the groundbreaking LISA paper [2], the authors introduced a novel concept of utilizing both text and images within a large language model, pioneering the use of segmentation masks. While innovative, this concept prompted us to question whether segmentation masks are indispensable for this task. Could we achieve similar outcomes by training a large language model differently, while adhering to the general multi-modal architecture principles highlighted in models like MiniGPT-4 [4], NextGPT [8], BLIP-2 [1], and others? Our model, known as NavGPT, is built upon the architecture of MiniGPT-4, harnessing Vicuna's visual component, as introduced in BLIP-2 [1]. NavGPT reuses the crucial modification of a new projection layer that aligns encoded visual features with Vicuna's language model [9] while keeping all other vision and language components frozen [10][11][12], introduced in MiniGPT-4's [4] architecture. During training, we provide the model with a JSON file listing theimage name and the precise coordinates of all road networks found in the image. The Q-Former module is trained to bridge its output with the frozen large language model (Vicuna). This innovative training approach not only allows researchers to address unique problems using open-sourced pre-trained large language models but also serves as a testament to the versatility of large language models. In this paper, we highlight one such use case. As we transition this system into production, we aim to share insights into the challenges we've encountered, thereby demonstrating the validity of our novel training technique for mastering a wide array of tasks using an LLM. Training our model required just one A100 GPU for approximately 26 hours, highlighting the cost-effectiveness of retraining such models to cater to personalized use cases. Our objective is to showcase that large language models offer a novel solution to addressing the challenges of map-making, potentially paving the way for achieving an autonomous navigable world. In essence, our paper makes the following pivotal contributions: * _Reconstructing Perception_: Leveraging the potential of large language models, we propose a novel image-instruction pair that includes the image's identifier and precise coordinates of the road network(s). This pairing empowers the large language model to develop an intrinsic comprehension of road network identification when confronted with aerial view images. * _Segmentation Simplified_: Our unique training approach obviates the need for segmentation masks in large language models. By dispensing with the step of generating training data for region-of-interest segmentation and architecturally eschewing the production of segmented masks as model outputs, we streamline the training process and enhance efficiency. * Our model builds upon the robust architecture of MiniGPT-4 [4]. However, it distinguishes itself by requiring a mere 10,000 image-instruction pairs for training. Remarkably, even in a zero-shot setting, our model demonstrates commendable performance, underscoring its efficiency in relation to the retraining effort. This demonstrates that upcoming multi-modal large language models can effectively address a wide array of challenges, many of which may not be solely text-based. In summary, our research builds upon the recent advancements in multi-modal LLMs [2][13][1][8][14][4][15], providing a focused and innovative solution to the task of generating navigable road networks from aerial images. By leveraging a tailored training approach, NavGPT aims to empower autonomous navigation systems, particularly in scenarios where precise navigational guidance is essential."
    },
    {
      "title": "2 Related Works",
      "text": ""
    },
    {
      "title": "Evolution Of Large Language Models",
      "text": "The quest for progress in Artificial General Intelligence (AGI) has been an enduring aspiration within the research community, with numerous tools and methodologies explored [16][17]. However, a true breakthrough remained elusive. Everything changed with the introduction of GPT-3, particularly the emergence of ChatGPT [18], which harnessed the power of GPT-3. The journey of GPT-based models has been a remarkable evolution, marked by continuous advancements. The most recent stride, embodied in ChatGPT powered by GPT-4 [3], has redefined the landscape of general artificial intelligence. Yet, the inner workings of GPT-4 [3] remained shrouded in mystery for the broader research community. This enigma persisted until the advent of open-sourced models such as Vicuna [9] and LLaMa [19][20][21]. Each of these models brought noteworthy enhancements in terms of retraining possibilities and the quality of inferred model outputs. Notably, Meta's recent release of LLaMa-2 [22] designed for commercial applications, represents a significant development. This newfound accessibility is expected to foster innovation across various domains."
    },
    {
      "title": "Foundational Multi-Modal Large Language Models",
      "text": "As Large Language Models (LLMs) [9][23][21][19][22][17][24][25][26][27][28][29][30][31][32] began to conquer text generation challenges, a world of new possibilities unfurled. Notably, they exhibited an improved grasp of contextual nuances, leading to more coherent text-to-text conversations. Extensive efforts were dedicated to incorporating human-in-the-loop feedback, refining the conversational finesse of LLMs. However, it was inevitable that the research landscape would expand beyond text-to-text generation alone. Soon enough, the research community shifted its focus to text-and-image conversations as inputs for LLMs, heralding the era of Multi-Modal Large Language Models. The fundamental idea driving this approach involved training the models to comprehend images by processing visual features through dedicated encoders. These features were then seamlessly integrated into LLMs as input. This methodology greatly facilitated the LLMs' capacity to interpret images, harnessing the valuable information embedded in image-caption pairs during the training phase."
    },
    {
      "title": "Advancing Multi-Modal Large Language Models",
      "text": "In recent times, the latest iterations of multi-modal large language models [2][13][1][14][8][4] have embraced versatility, accommodating an array of input formats. These formats encompass text, images, videos, and audio, reflecting the dynamic nature of contemporary data sources [15]. Correspondingly, these models exhibit a harmonious synergy between inputs and outputs. For each distinct format, a dedicated encoder and decoder are seamlessly integrated. This structural architecture empowers advanced systems like ChatGPT-4[3] to seamlessly process input data and generate corresponding outputs. Nevertheless, even these advanced systems are not immune to imperfections, particularly within their encoders and decoders. These systems may encounter challenges related to information loss during data transformation, leading to instances where the broader contextual understanding is compromised. This paper introduces novel solutions to address a few of these challenges."
    },
    {
      "title": "3 Method",
      "text": "While multi-modal large language models offer numerous advantages, a significant hurdle lies in the generation of training data. Our model is rooted in the architectural framework of MiniGPT-4[4], designed to establish synergy between visual data from a pre-trained vision encoder and a sophisticated large language model (LLM). We leverage Vicuna [9] as the language decoder, introducing a pioneering training method for road network identification within images. In terms of visual perception, we adopt a similar approach to the visual encoder employed in BLIP-2[1], harnessing a Vision Transformer [33] backbone in conjunction with their pre-trained Q-Former [1]. To facilitate seamless communication between the visual encoder and the LLM, we introduce a novel training procedure, augmenting the MiniGPT-4[4] projection layer. This innovative approach bridges the gap, enabling effective collaboration between the two components. Figure 1: NavGPT architecture based on MiniGPT-4 Source: [4]."
    },
    {
      "title": "Data Collection And Preprocessing",
      "text": "Automating road navigation presents a formidable challenge, primarily due to the labor-intensive nature of data collection. However, our work benefits from operating within the spatial domain, where obtaining a portion of the required training data is relatively straightforward. We are privileged to have access to satellite imagery for specific global regions. In this study, we focus on training the model using imagery sourced from the Western European region. To infuse a higher level of naturalness into the generated language and elevate the model's overall utility, we advocate for the importance of road navigation segment training. Unfortunately, datasets suitable for the vision-language domain, especially in the context of road navigation, are virtually non-existent. To overcome this gap, we painstakingly assembled a meticulously detailed image description dataset. This dataset has been meticulously crafted with the sole purpose of facilitating the alignment of vision and language. During the alignment phase of our NavGPT model, this dataset is deployed to fine-tune the model for enhanced performance."
    },
    {
      "title": "Novel Training Approach",
      "text": "NavGPT, our innovative approach, deviates from the reliance on segmentation masks suggested by LISA [2]. Our model draws inspiration from MiniGPT-4 [4], which is itself influenced by BLIP-2 [13][1]. The pivotal component, the projection layer coupled with the Q-Transformer [1], empowers our model to assess the presence or absence of a road network in a given image. This accomplishment is realized through training the model with an image-instruction pair file that explicitly indicates whether a given image contains a road network. Here is a snippet of the JSON file contents, which sheds light on our training data. { \"image_id\": \"54534_33840\", \"caption\": \"Found a road\" }, { \"image_id\": \"54537_33868\", \"caption\": \"No roads found\" } We conducted model training for 5,000 steps and observed promising results in a zero-shot setting. This initial success ignited our curiosity about the model's potential. What if, in addition to recognizing the presence of a road network within an image, we could train it to pinpoint the image coordinates of the road network? To explore this hypothesis, we required a substantial dataset. Fortunately, we had access to various road geometries in the Western European region. Leveraging HERE's internal aerial imagery service, we initiated image queries in regions where road network geometry overlapped. The images had dimensions of 1280 by 1280 pixels. These overlapping regions corresponded to areas with well-defined road networks. A Python function was employed to perform these queries and extract the image coordinates of the road network (in the form of a line string) for 10,000 such scenarios. Notably, approximately 40% of the images did not contain overlapping road network geometry. However, these images were still included in the original set of 10,000 instructions. This inclusion served the dual purpose of enabling the model to discern the presence or absence of a road network and, if present, to provide accurate image coordinates. { \"image_id\": \"54537_33867\", \"caption\": Found 1 road. Image coordinates are as follows: [[(219, 114), (283, 271)]]\" }, { \"image_id\": \"54537_33879\", \"caption\": \"Found 2 roads. Image coordinates are as follows: [[(0, 775), (0, 731), (644, 28)], [(365, 0), (629, 3), (644, 28)]]\" } [MISSING_PAGE_FAIL:5] large language models, which were crafted to be general-purpose, NavGPT steps beyond this constraint to address real-world challenges in a relevant context."
    },
    {
      "title": "Limitations",
      "text": "Given the NavGPT model training approach, we are moving away from a general-purpose multi-modal language [2][13][1][1][14][8] to make it a navigation-based model which means a small amount strips away the general-purpose utility of this model. The context of our training data improves the whole representation of the model to solve/understand one of the most critical problems we face from an autonomous world perspective. The model exhibits a reasonable performance, although it does encounter some limitations. In our forthcoming research within this domain, we intend to expand the range of scenarios presented in the model and undertake retraining to enhance its overall accuracy."
    },
    {
      "title": "5 Conclusion",
      "text": "In conclusion, NavGPT presents a novel way to train the multi-modal large language models. By diverging from traditional segmentation masks and leveraging the architecture of MiniGPT-4[4] influenced by BLIP-2 [13][1], NavGPT empowers itself to discern and describe road networks in images. Our approach, founded on the projection layer and the Q-Transformer [1], offers a nuanced understanding of visual content, surpassing the capabilities of its predecessors. Through extensive training with image-instruction pairs, NavGPT has demonstrated impressive abilities in identifying and describing road networks within images. The model's success, even in zero-shot settings, has inspired further exploration. By introducing image coordinates into the training process, we aim to unlock even greater potential. Our work showcases the advancements in multi-modal large language models and addresses a real-world problem under the right context. NavGPT's contribution extends beyond the scientific community, offering practical applications for developing autonomous navigation systems. In summary, NavGPT represents a remarkable leap in multi-modal AI, heralding a new era in understanding and generating diverse modalities from text and images, with the potential to transform the field of AI in ways we are only beginning to fathom."
    },
    {
      "title": "Acknowledgments",
      "text": "We extend our gratitude to HERE North America LLC for generously providing the hardware necessary for model training and conducting our experiments. We also appreciate HERE for granting us access to their aerial imagery service and the road network line strings that were instrumental in NavGPT's training. Figure 3: Experiment 2: Identify road network"
    },
    {
      "title": "References",
      "text": "* (1) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023. * (2) Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. _arXiv preprint arXiv:2308.00692_, 2023. * (3) OpenAI. Gpt-4 technical report. _Open AI_, 2023. * (4) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023. * (5) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021. * (6) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022. * (7) Wenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. Enabling multimodal generation on clip via vision-language knowledge distillation. _arXiv preprint arXiv:2203.06386_, 2022. * (8) Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. _arXiv preprint arXiv:2309.05519_, 2023. * (9) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgtp quality. _See https://vicuna. Imsys. org (accessed 14 April 2023)_, 2023. * (10) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022. * (11) Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Making visual representations matter in vision-language models. _arXiv preprint arXiv:2101.00529_, 1(6):8, 2021. * (12) Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18123-18133, 2022. * (13) Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pages 12888-12900. PMLR, 2022. * (14) Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. _Advances in Neural Information Processing Systems_, 34:200-212, 2021. * (15) Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. _arXiv preprint arXiv:2305.17216_, 2023. * (16) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. _Advances in neural information processing systems_, 32, 2019. * (17) Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. _arXiv preprint arXiv:2303.06594_, 2023. * (18) OpenAI. Introducing chatgpt. _[https://openai.com/blog/chatgpt_](https://openai.com/blog/chatgpt_), 2022. * (19) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023. * (20) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022. * [21] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. * [22] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023. * [23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018. * [24] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19358-19369, 2023. * [25] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6904-6913, 2017. * [26] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and Steven CH Hoi. From images to textual prompts: Zero-shot vqa with frozen large language models. _arXiv preprint arXiv:2212.10846_, 2022. * [27] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In _International Conference on Machine Learning_, pages 1931-1942. PMLR, 2021. * [28] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. _arXiv preprint arXiv:2209.06794_, 2022. * [29] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. _arXiv preprint arXiv:2303.11381_, 2023. * [30] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. _Advances in Neural Information Processing Systems_, 35:124-141, 2022. * [31] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023. * [32] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. _arXiv preprint arXiv:2302.14045_, 2023. * [33] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020."
    }
  ]
}