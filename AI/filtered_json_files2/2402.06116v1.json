{
  "title": "LLMs for Coding and Robotics Education",
  "authors": [
    "Peng Shu",
    "Huaqin Zhao",
    "Hanqi Jiang",
    "Yiwei Li",
    "Shaochen Xu",
    "Yi Pan",
    "Zihao Wu",
    "Zhengliang Liu",
    "Guoyu Lu",
    "Le Guan",
    "Gong Chen",
    "Xianqiao Wang",
    "Tianming Liu"
  ],
  "abstract": "\n Large language models and multimodal large language models have revolutionized artificial intelligence recently. An increasing number of regions are now embracing these advanced technologies. Within this context, robot coding education is garnering increasing attention. To teach young children how to code and compete in robot challenges, large language models are being utilized for robot code explanation, generation, and modification. In this paper, we highlight an important trend in robot coding education. We test several mainstream large language models on both traditional coding tasks and the more challenging task of robot code generation, which includes block diagrams. Our results show that GPT-4V outperforms other models in all of our tests but struggles with generating block diagram images. \n",
  "references": [
    {
      "id": null,
      "title": "LLMs for Coding and Robotics Education",
      "authors": [
        "Peng Shu",
        "Huaqin Zhao",
        "Hanqi Jiang",
        "Yiwei Li",
        "Shaochen Xu",
        "Yi Pan",
        "Zihao Wu",
        "Zhengliang Liu",
        "Guoyu Lu",
        "Le Guan",
        "Gong Chen",
        "Xianqiao Wang",
        "Tianming Liu"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F L Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Okul öncesi ögretim programına algoritma ve kodlama egitimi entegrasyonunun ögrencilerin problem çözme becerisine etkisi. Master's thesis",
      "authors": [
        "C Akyol Altun"
      ],
      "year": "2018",
      "venue": "Okul öncesi ögretim programına algoritma ve kodlama egitimi entegrasyonunun ögrencilerin problem çözme becerisine etkisi. Master's thesis",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "code2seq: Generating sequences from structured representations of",
      "authors": [
        "U Alon",
        "S Brody",
        "O Levy",
        "E Yahav"
      ],
      "year": "2019",
      "venue": "code2seq: Generating sequences from structured representations of",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Coding as a playground: Programming and computational thinking in the early childhood classroom",
      "authors": [
        "M U Bers"
      ],
      "year": "2020",
      "venue": "Coding as a playground: Programming and computational thinking in the early childhood classroom",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Developing computational thinking in compulsory educationimplications for policy and practice",
      "authors": [
        "S Bocconi",
        "A Chioccariello",
        "G Dettori",
        "A Ferrari",
        "K Engelhardt"
      ],
      "year": "2016",
      "venue": "Developing computational thinking in compulsory educationimplications for policy and practice",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Language models are few-shot learners",
      "authors": [
        "T B Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D M Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Learning computational thinking and social skills development in young children through problem solving with educational robotics",
      "authors": [
        "Y A Caballero-Gonzalez",
        "A G V Muñoz-Repiso",
        "A García-Holgado"
      ],
      "year": "2019",
      "venue": "Proceedings of the seventh international conference on technological ecosystems for enhancing Multiculturality",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "The effect of robotic coding education on preschoolers' problem solving and creative thinking skills",
      "authors": [
        "R Korkmaz",
        "Ö İdil",
        "Ö Erdogmuş",
        "F U"
      ],
      "year": "2021",
      "venue": "Thinking Skills and Creativity",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Exploring the effects of \"productive children: coding and robotics education program",
      "authors": [
        "M Canbeldek",
        "N Isikoglu"
      ],
      "year": "2023",
      "venue": "Education and Information Technologies",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Evaluating large language models trained on code",
      "authors": [
        "M Chen",
        "J Tworek",
        "H Jun",
        "Q Yuan",
        "H P De Oliveira Pinto",
        "J Kaplan",
        "H Edwards",
        "Y Burda",
        "N Joseph",
        "G Brockman",
        "A Ray",
        "R Puri",
        "G Krueger",
        "M Petrov",
        "H Khlaaf",
        "G Sastry",
        "P Mishkin",
        "B Chan",
        "S Gray",
        "N Ryder",
        "M Pavlov",
        "A Power",
        "L Kaiser",
        "M Bavarian",
        "C Winter",
        "P Tillet",
        "F P Such",
        "D Cummings",
        "M Plappert",
        "F Chantzis",
        "E Barnes",
        "A Herbert-Voss",
        "W H Guss",
        "A Nichol",
        "A Paino",
        "N Tezak",
        "J Tang",
        "I Babuschkin",
        "S Balaji",
        "S Jain",
        "W Saunders",
        "C Hesse",
        "A N Carr",
        "J Leike",
        "J Achiam",
        "V Misra",
        "E Morikawa",
        "A Radford",
        "M Knight",
        "M Brundage",
        "M Murati",
        "K Mayer",
        "P Welinder",
        "B Mcgrew",
        "D Amodei",
        "S Mccandlish",
        "I Sutskever",
        "W Zaremba"
      ],
      "year": "2021",
      "venue": "Evaluating large language models trained on code",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "How we fine-tuned llama 2 for our block coding copilot",
      "authors": [
        "K Chong"
      ],
      "year": "2023",
      "venue": "How we fine-tuned llama 2 for our block coding copilot",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "The effect of steam-based unplugged play activities using robots on the improvement of children's creative and social personalities",
      "authors": [
        "H Y Chun",
        "S Park"
      ],
      "year": "2020",
      "venue": "Korean Journal of Childcare and Education",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Educational robotics intervention on executive functions in preschool children: A pilot study",
      "authors": [
        "M C Di Lieto",
        "E Inguaggiato",
        "E Castro",
        "F Cecchi",
        "G Cioni",
        "M Dell'omo",
        "C Laschi",
        "C Pecini",
        "G Santerini",
        "G Sgandurra"
      ],
      "year": "2017",
      "venue": "Computers in human behavior",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Tablets and apps for promoting robotics, stem education and literacy in early childhood education",
      "authors": [
        "P Dorouka",
        "S Papadakis",
        "M Kalogiannakis"
      ],
      "year": "2020",
      "venue": "International Journal of Mobile Learning and Organisation",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Codebert: A pre-trained model for programming and natural languages",
      "authors": [
        "Z Feng",
        "D Guo",
        "D Tang",
        "N Duan",
        "X Feng",
        "M Gong",
        "L Shou",
        "B Qin",
        "T Liu",
        "D Jiang",
        "M Zhou"
      ],
      "year": "2020",
      "venue": "Codebert: A pre-trained model for programming and natural languages",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "The robots are coming: Exploring the implications of openai codex on introductory programming",
      "authors": [
        "J Finnie-Ansley",
        "P Denny",
        "B A Becker",
        "A Luxton-Reilly",
        "J Prather"
      ],
      "year": "2022",
      "venue": "Proceedings of the 24th Australasian Computing Education Conference",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Visual programming: Compositional visual reasoning without training",
      "authors": [
        "T Gupta",
        "A Kembhavi"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Codelmsec benchmark: Systematically evaluating and finding security vulnerabilities in black-box code language models",
      "authors": [
        "H Hajipour",
        "K Hassler",
        "T Holz",
        "L Schönherr",
        "M Fritz"
      ],
      "year": "2023",
      "venue": "Codelmsec benchmark: Systematically evaluating and finding security vulnerabilities in black-box code language models",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Aries hilton's pyscratch: A software for converting python files to scratch projects",
      "authors": [
        "A Hilton"
      ],
      "year": "2023",
      "venue": "Aries hilton's pyscratch: A software for converting python files to scratch projects",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Research shows first drives stem engagement and outcomes",
      "authors": [
        "Inspiration",
        "R Science",
        "T First)"
      ],
      "year": "2023",
      "venue": "Research shows first drives stem engagement and outcomes",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "About first: Our mission, purpose & values",
      "authors": [
        "Inspiration",
        "R Science",
        "T First)"
      ],
      "year": "2024",
      "venue": "About first: Our mission, purpose & values",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "What is first lego league?",
      "authors": [
        "Inspiration",
        "R Science",
        "T First)"
      ],
      "year": "2024",
      "venue": "What is first lego league?",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Educating the engineer of 2020: Adapting engineering education to the new century",
      "authors": [
        "C B Iturbe",
        "L L Ochoa",
        "M J Castello",
        "J C Pelayo"
      ],
      "year": "2009",
      "venue": "INTED2009 Proceedings",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Robotgpt: Robot manipulation learning from chatgpt",
      "authors": [
        "Y Jin",
        "D Li",
        "A Yong",
        "J Shi",
        "P Hao",
        "F Sun",
        "J Zhang",
        "B Fang"
      ],
      "year": "2024",
      "venue": "IEEE Robotics and Automation Letters",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Research: quantifying github copilot's impact on developer productivity and happiness",
      "authors": [
        "E Kalliamvakou"
      ],
      "year": "2022",
      "venue": "The GitHub Blog",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Introducing fundamental object-oriented programming concepts in preschool education within the context of physical science courses",
      "authors": [
        "K Kanaki",
        "M Kalogiannakis"
      ],
      "year": "2018",
      "venue": "Education and Information Technologies",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Put your robot in, put your robot out: Sequencing through programming robots in early childhood",
      "authors": [
        "E R Kazakoff",
        "M U Bers"
      ],
      "year": "2014",
      "venue": "Journal of Educational Computing Research",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Okul öncesi ve temel fen egitiminde robotik destekli ve basit malzemelerle yapılan stem uygulamalarının karşılaştırılması",
      "authors": [
        "A Koç"
      ],
      "year": "2019",
      "venue": "Okul öncesi ve temel fen egitiminde robotik destekli ve basit malzemelerle yapılan stem uygulamalarının karşılaştırılması",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Coding in early childhood",
      "authors": [
        "J Lee"
      ],
      "year": "2020",
      "venue": "Contemporary Issues in Early Childhood",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y J Lee"
      ],
      "year": "2023",
      "venue": "Visual instruction tuning",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Summary of chatgpt-related research and perspective towards the future of large language models",
      "authors": [
        "Y Liu",
        "T Han",
        "S Ma",
        "J Zhang",
        "Y Yang",
        "J Tian",
        "H He",
        "A Li",
        "M He",
        "Z Liu"
      ],
      "year": "2023",
      "venue": "Meta-Radiology",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
      "authors": [
        "S Lu",
        "D Guo",
        "S Ren",
        "J Huang",
        "A Svyatkovskiy",
        "A Blanco",
        "C Clement",
        "D Drain",
        "D Jiang",
        "D Tang",
        "G Li",
        "L Zhou",
        "L Shou",
        "L Zhou",
        "M Tufano",
        "M Gong",
        "M Zhou",
        "N Duan",
        "N Sundaresan",
        "S K Deng",
        "S Fu",
        "S Liu"
      ],
      "year": "2021",
      "venue": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Okul öncesinde kodlama egitimi ve kullanılabilecek araçlar hakkında bilişim teknolojileri ögretmenlerinin görüşleri: Bir durum çalışması. 1. Uluslararası Bilgisayar ve Ögretim Teknolojileri Sempozyumu",
      "authors": [
        "M Odacı",
        "E Uzun"
      ],
      "year": "2017",
      "venue": "İnönü Üniversitesi",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Proje yaklaşımının anasınıfına devam eden çocukların problem çözme becerilerine etkisinin incelenmesi",
      "authors": [
        "V Oguz"
      ],
      "year": "2012",
      "venue": "Proje yaklaşımının anasınıfına devam eden çocukların problem çözme becerilerine etkisinin incelenmesi",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Ai-assisted coding: Experiments with gpt-4",
      "authors": [
        "R A Poldrack",
        "T Lu",
        "G Beguš"
      ],
      "year": "2023",
      "venue": "Ai-assisted coding: Experiments with gpt-4",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Designing unplugged and plugged activities to cultivate computational thinking: An exploratory study in early childhood education",
      "authors": [
        "A Saxena",
        "C K Lo",
        "K F Hew",
        "G K W Wong"
      ],
      "year": "2020",
      "venue": "The Asia-Pacific Education Researcher",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Improving students' science process skills through simple computer simulations on linear motion conceptions",
      "authors": [
        "P Siahaan",
        "A Suryani",
        "I Kaniawati",
        "E Suhendi",
        "A Samsudin"
      ],
      "year": "2017",
      "venue": "Journal of Physics: Conference Series",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Oecd future of education and skills 2030: Curriculum analysis",
      "authors": [
        "M Taguma",
        "M Barrera"
      ],
      "year": "2019",
      "venue": "Oecd future of education and skills 2030: Curriculum analysis",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "N Bashlykov",
        "S Batra",
        "P Bhargava",
        "S Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A N Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Chatgpt for robotics: Design principles and model abilities",
      "authors": [
        "S Vemprala",
        "R Bonatti",
        "A Bucker",
        "A Kapoor"
      ],
      "year": "2023",
      "venue": "Microsoft Auton. Syst. Robot. Res",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Large language models for robotics: Opportunities, challenges, and perspectives",
      "authors": [
        "J Wang",
        "Z Wu",
        "Y Li",
        "H Jiang",
        "P Shu",
        "E Shi",
        "H Hu",
        "C Ma",
        "Y Liu",
        "X Wang"
      ],
      "year": "2024",
      "venue": "Large language models for robotics: Opportunities, challenges, and perspectives",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Yönlendirilmiş beyin fırtınası (scamper) teknigine dayalı egitimin beş yaş çocuklarının problem çözme becerilerine etkisinin incelenmesi",
      "authors": [
        "N Yigitalp"
      ],
      "year": "2014",
      "venue": "Yönlendirilmiş beyin fırtınası (scamper) teknigine dayalı egitimin beş yaş çocuklarının problem çözme becerilerine etkisinin incelenmesi",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "When brain-inspired ai meets agi",
      "authors": [
        "L Zhao",
        "L Zhang",
        "Z Wu",
        "Y Chen",
        "H Dai",
        "X Yu",
        "Z Liu",
        "T Zhang",
        "X Hu",
        "X Jiang"
      ],
      "year": "2023",
      "venue": "Meta-Radiology",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Outline, then details: Syntactically guided coarse-to-fine code generation",
      "authors": [
        "W Zheng",
        "S Sharan",
        "A K Jaiswal",
        "K Wang",
        "Y Xi",
        "D Xu",
        "Z Wang"
      ],
      "year": "2023",
      "venue": "Outline, then details: Syntactically guided coarse-to-fine code generation",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Llms For Coding And Robotics Education",
      "text": "Peng Shu, Huaqin Zhao, Hanqi Jiang, Yiwei Li, Shaochen Xu, Yi Pan, Zihao Wu, Zhengliang Liu, Guoyu Lu, Le Guan, Gong Chen, Xianqiao Wang Tianming Liu Corresponding authors: Tianming LiuPeng Shu, Huaqin Zhao, Hanqi Jiang, Yiwei Li, Shaochen Xu, Yi Pan, Zihao Wu, Zhenggliang Liu, Guoyu Lu, Le Guan, Gong Chen, Tianming Liu are with the School of Computing, The University of Georgia, Athens 30602, USA. Xianqiao Wang is with School of ECAM, College of Engineering, The University of Georgia, Athens 30602, USA. The influence of LLMs on coding is multifaceted. Firstly, it revolutionized code generation and understanding. Based on their generalization into the domain of coding, LLMs serve as powerful tools for code completion, automated documentation generation, and intelligent debugging. Tools such as GitHub's Copilot[26], powered by OpenAI's Codex, offer suggestions to improve code quality and efficiency, making coding more accessible to a wider audience. With help of AI assistants, programmers get rid of duplicate code work, refactor with higher efficiency and more readable code. Secondly, LLMs enhance code development process productivity and efficiency. LLMs serve as wireless partners in the coding process, available around the clock to assist developers. They automate routine tasks, suggest optimizations, and even identify potential bugs before they become problematic, which significantly improves code quality and efficiency[36]. Thirdly, LLMs play a virtual role in democratizing coding and coding education. By providing intuitive suggestions and reducing the need for intricate understanding of syntax and language-specific idiosyncrasies, LLMs lower the barrier to entry for coding. This has opened up the field of software development to a broader demographic, fostering a more inclusive and diverse tech community. However, given that the current models can easily complete most coding problem sets using in introductory programming courses[17], there are more and more concerns about necessity of basic education in code programming and the unemployment problem of programmers. In recent years there have been different attempts to incorporate LLMs into robotics systems. [42] provides a clear and concise description of the desired robotics task and its context to generate more accurate responses. [43] applies GPT-4V to generate robotic code commands given the tasks and vision input. Other works put forward similar robotic framework cooperated with LLMs[25][17]. LLMs make it possible for machines to interpret and act on complex instructions and queries, which significantly changes the way humans interact with robots, shifting from rigid command-based communication to more natural and intuitive dialogues. Robots can now understand context, process ambiguous inputs, and even engage in conversational exchanges, making them more accessible and user-friendly. In robot coding, LLMs contribute to more autonomous systems capable of decision-making based on linguistic cues and context. This autonomy is particularly impactful in scenarios where robots need to understand and adapt to dynamic environments, such as in search and rescue operations, healthcare, and customer service. However, the realm in understanding robotic code and generating robot function modular code remains almost blank, especially for early programming education. In this paper, we demonstrate the performance of LLMs on regular data structure and algorithms code testing as well as robot module code generation, understanding of block diagram robot languages. We also test and attempt to generate fundamental block diagrams for robot education. Discussion includes how LLMs and FMs can support robot education for children, the possible directions for future robot programming education based on LLMs and current limitations LLMs have in this region. Related Work"
    },
    {
      "title": "Large Language Models",
      "text": "The introduction of GPT-3 by Brown et al.[6] provided an innovative basis for subsequent LLM applications in coding, demonstrating the model's ability to generate human-like text, including code. This work laid the groundwork for specialized models like Codex[10], further tailored for programming languages and code generation tasks. Similarly, Devlin et al.[13] introduced BERT, which, although not specifically designed for code, has inspired successive adaptations like CodeBERT by Feng et al.[16], merging natural language understanding with code semantics. Following the path blazed by Codex, Alon et al.[3] proposed the Code2Seq framework, which represents a novel approach to generating code summaries and assisting in code documentation by converting code into a sequence of tokens. This model emphasizes the importance of understanding the structural and syntactical implications of programming languages in LLM applications. Additionally, Lu et al.[33] introduced CodeGPT, a variation of GPT specifically fine-tuned for code generation tasks, showcasing improvements in generating syntactically correct and logically coherent code snippets. LLMs, such as OpenAI's Codex[10], which powers GitHub Copilot, have significantly enhanced developer productivity by providing code suggestions and completions. Codex, built on the GPT-3 model, demonstrates an advanced ability to generate programming code based on natural language prompts, thereby accelerating the software development process and reducing manual coding effort. Research on utilizing LLMs for debugging and error correction has shown promising results. For instance, a model like CodeBERT[16], designed to understand and generate code from natural language, can be applied to identify syntax errors and logical bugs within software projects, providing a foundation for automated debugging tools. The security of code generated by LLMs and the mitigation of biases within these models are critical concerns. Research efforts such as those by Hajipour et al.[19] have focused on developing methodologies to ensure the generation of secure code and to identify potential biases in model outputs, aiming to establish best practices for the safe and ethical use of LLMs in software development."
    },
    {
      "title": "Robot Coding For Education",
      "text": "In recent years, numerous organizations have dedicated efforts to identify essential life skills for students in educational environment, which inspires plenty of researches in this area. As a result, these skills are summarized as 21st century skills, and a basic frame for these skills has been attempted to be designed [8]. Organisation for Economic Co-operation and Development (OECD) attempt to divide the skills into three categories: Creating new value (being able to develop new perspectives), reconciling tensions and dilemmas (being able to think in a systematic way), as well as taking responsibility (self-regulation, self-efficacy,self-control, and problem-solving)[39]. Among required skills, Learning and Innovation Skills (creativity and innovation, critical thinking and problem-solving, communication, collaboration) are regraded as one of the most important realms for development of teenagers. They promise the success of students in such a information explosion century by reaching and filtering valuable information efficiently, innovating on current platform and cooperating within a team. Problem-solving involves children's understanding on concepts, discovering the concepts and values independently and developing them[38]. To empower young people for problem-solving, previous methods include structured or unstructured gaming activities, brainstorming, project-based education and etc[8, 35, 44]. Recently, coding has become a systematic method in terms of solving problems, which prompt teenagers to improve critical thinking, computational thinking and innovative thinking[5, 27, 34]. More robotics and coding instruction, commonly preferred in primary and secondary education, has been used as a teaching technique in preschool education[8]. [12] observed that after a brief period of five weeks engaged in unplugged coding activities, a group of 5-year-old children demonstrated marked progress in the development of creative and social personality traits. Other researches show that teaching basic practices for robot and coding supports sequencing skills in children[7, 28, 37]. Teenagers who take part in robotics and coding activities outperform in skills vary from visual-spatial, working memory, inhibitory control to problem-solving[2, 14, 29]. However, coding is an abstract and complicated method because coding itself can be defined as a problem-solving procedure. Robotics combines software code design as well as robotic structure, which requires even higher comprehensive capability. The difficulty of coding and robotics becomes the barrier hindering young children from learning. For this reason, numerous educational platforms have been developed to facilitate learning in coding and robotics. Such platforms include WeDo, Lego, Bee-bot, Clementino, and Robokids. Generally, these platforms are equipped with coding kits and designed with a visual interface, making them more user-friendly and easier for young children to learn. Children consider them easy and funny to learn, thus promoting their passion. Other challenges lie on how to provide concrete, meaningful and problem-based learning activities to teach coding to young children[15]. [4] demonstrates the importance of proper coding and robotics activities. It also emphasizes the participation of children in terms of playing with robots, exploring unknown regions, socializing with others and creating innovative solutions[30]. Besides, coding and robotics education is a new concept for early childhood teachers and has not been comprehensively integrated into most early childhood curricula[9]. All of these existing problems motivate this paper to investigate the possibility of applying LLMs in robotics coding to provide assistance to both children and teachers."
    },
    {
      "title": "Llms In Robot Code Generation",
      "text": ""
    },
    {
      "title": "2.3.1 First Tech Challenge",
      "text": "The FIRST Tech Challenge (FTC) is a dynamic, global robotics program that ignites passion in young minds for science, technology, engineering, and mathematics (STEM). The FTC robot code is typically written in Java. Established in 2005, FTC provides a platform for students in grades 4-18 to engage in hands-on robotics challenges, fostering invaluable skills in problem-solving, teamwork, and innovation [22]. By designing, building, and programming robots, participants are immersed in real-world engineering experiences, competing in alliances against teams at local, regional, and international levels [22]. The program is a flagship initiative of For Inspiration and Recognition of Science and Technology (FIRST), a non-profit organization founded by inventor Dean Kamen in 1989. FIRST's mission is to inspire young people to become leaders in science and technology. FTC plays a crucial role in providing a platform which is accessible and inclusive, regardless of a participant's background or experience level [22]. The impact of FTC extends beyond the technical skills. Research indicates that participants are significantly more likely to attend college and major in a STEM field, demonstrating the program's effectiveness in shaping the next generation of STEM professionals [21]. Moreover, FTC's emphasis on teamwork, communication, and leadership prepares students for future challenges, aligning with educational goals around the globe [24]."
    },
    {
      "title": "2.3.2 First Lego League",
      "text": "FIRST LEGO League (FLL) is an internationally recognized, innovative program that ignites enthusiasm for discovery, science, and technology in young minds. Designed to inspire and challenge students aged 4 to 16, the league combines the hands-on fun of LEGO building with real-world engineering and problem-solving challenges[23]. Originating from a partnership between FIRST and the LEGO Group, FLL provides a platform for children to learn critical skills while engaging in playful and meaningful competition. The structure of FLL is centered around theme-based Challenges that teams must navigate through. In these Challenges, participants build and program autonomous robots using LEGO MINDSTORMS technology to score points on a thematic playing surface, creating innovative solutions to a problem as part of their research project. The process fosters valuable life skills and competencies such as problem-solving, teamwork, and creative thinking. FLL block diagrams are a fundamental component of the FLL robotics experience, offering a visual and intuitive way to program LEGO MINDSTORMS robots. These diagrams are designed to be user-friendly, ensuring that even individuals without prior programming experience can engage with and understand the basics of robot programming. The essence of FLL block diagrams lies in their drag-and-drop interface, where users can select from a variety of command blocks - each representing a specific action or decision in the robot's operation. These blocks can be pieced together to form a sequence of instructions, creating a 'flow' of actions that the robot will execute."
    },
    {
      "title": "2.3.3 Cooperating Llms In Robot Coding",
      "text": "Unlike traditional text-based coding languages like Python or Java, visual block coding platforms like Tynker, Scratch, MakeCode, and Snap operate at a higher level of abstraction. They often transcend standard programming constructs like loops and conditionals. For example, a single block like \"move n pixels\" could equate to 20 lines of C code. Beginners would need to navigate the intricacies of graphics libraries, variable tracking, and angular calculations to achieve the same result[11]. Therefore, companies are leveraging LLMs to facilitate the interaction between the robot and the young learners, allowing for more intuitive and meaningful interactions. For example, Aries Hilton's PyScratch is a software that can convert Python files to Scratch projects, which are interactive stories, games, and animations that can be shared online. PyScratch uses LLMs to map the Python commands and arguments to the corresponding Scratch blocks and inputs, and also converts the Pygame images to SVG files. PyScratch can help Python learners to create Scratch projects without having to learn a new language, and also help Scratch users to learn Python by seeing how their projects can be translated into code[20]. [11] trained a model specifically designed to autocomplete the next block of code in a Tynker visual block coding project based on the BERT in 2018 using their expansive repository of millions of user-published projects. With the emergence of ChatGPT, they guide ChatGPT into generating block-like code that could be converted into Tynker's native coding language to help kids effortlessly translate their ideas into code, understand any given code's functionality, and provide real-time debugging assistance. Further more, they fine tune a Llama2 chat model to meet Tynker's specialized visual coding requirements."
    },
    {
      "title": "3 Methodologies And Tasks",
      "text": ""
    },
    {
      "title": "Data Structure And Algorithms Code Generation",
      "text": "For the purpose of evaluating code generation in the context of data structures and algorithms, we selected three problems of varying difficulty levels from LeetCode to test the performance of three models: GPT-4V, Code Llama, and GitHub Copilot. Each model generates four different types of programming languages including C, C++, Python3 and Java for every problem. The first problem, categorized as easy, involves designing a data structure that uses two stacks to implement a first-in-first-out queue. This queue should support four operations: adding an element to the end of the queue, removing and returning an element t from the beginning of the queue, returning the element at the beginning of the queue, and checking whether the queue is empty. The second problem, of medium difficulty, requires determining whether a non-empty array containing only positive integers can be partitioned into two subsetswith equal sums. This problem entails finding a method to group the elements in the array such that the total sum of each group is equal. The challenging problem involves determining the shortest path length to visit all nodes in an undirected connected graph composed of n nodes, where the nodes are numbered from 0 to n - 1. The input to the problem is an array, graph, representing the structure of the graph, where graph[i] is a list containing all nodes directly connected to the i-th node."
    },
    {
      "title": "Ftc Code Generation",
      "text": "In order to assess the robot code generation capabilities of GPT-4V, Code Llama, and GitHub Copilot, we selected three FTC code modules: AprilTag label recognition and pose estimation, encoder-based path driving, and operation of a four-wheel omnidirectional robot. For each of these tasks, we removed the original code but provided comments to serve as input for the model to generate code. Our objective is to evaluate the quality of code generated from the models by comparing it with the original code. The first test code aims at recognizing and estimating the position and orientation of AprilTag tags. Operating in a LinearOpMode, it demonstrates the basics of AprilTag recognition and pose estimation, including Java Builder structures for specifying vision parameters. The essence of the program is to detect AprilTag tags in the field of view through a camera, which can be either a web-cam or a built-in phone camera, and to display the IDs of these tags. For tags included in the default _TagLibrary_, the program not only shows their IDs but also provides their position and orientation relative to the camera. This default TagLibrary contains the current season's AprilTags and a set of test tags in a higher number range. When an AprilTag from the TagLibrary is detected, the program acquires the tag's location and orientation relative to the camera using an instance of the AprilTag Processor (AprilTagProcessor), with this information stored in the _ftcPose_ member of the returned _detection_. The program also includes an instance of the Vision Portal (VisionPortal) for managing the camera's streaming and processor configurations. Additionally, the code contains telemetry data logic to display the results of AprilTag detections. When the program is operational, it periodically checks and displays information for all detected AprilTag tags, including each tag's ID, position (XYZ coordinates), and orientation (pitch, roll, yaw angles). The second test code focuses on path driving based on encoder counts. It operates in a Linear Operation Mode (LinearOpMode) and requires the installation of encoders on the robot's wheels. The core functionality of the program is executed through the defined method _encoderDrive(speed, leftInches, rightInches, timeoutS)_, which assumes that each movement is relative to the robot's last stopping position. The program defines a specific path that includes driving forward for 48 inches, spinning right for 12 inches, driving backward for 24 inches, and then stopping and closing the claw. This method employs the _RUN_TO_POSITION_ mode, enabling the motor controllers to generate a run profile. The program calculates the encoder counts per inch (COUNTS_PER_INCH), based on the specific drive train configuration, such as motor revolutions per minute and external gear reduction ratios. During operation, the program first initializes the drive system variables, sets the direction of the motors, and resets the encoders. It then waits for the game to start (activated by the driver pressing PLAY) and subsequently executes the predefined path in steps, including forward movement, turning, and reversing. New target positions are determined through the encoder counts, and the motors are set to reach these positions. Throughout the process, the program continuously checks whether any of the termination conditions are met: reaching the desired position, running out of time, or the driver stopping the program. After each movement, the program pauses briefly and then displays the final telemetry message. The third code segment is designed for operating a four-wheeled ommidirectional (or holonomic) robot. The program operates in a Linear Operation Mode (LinearOpMode) and is compatible with both Mecanum-Drive and X-Drive trains, which are suitable for omnidirectional wheel robots. The core functionality of the program lies in using four motors (left front, left back, right front, right back) to control the robot's three motion axes: axial (forward and backward movement), lateral (side-to-side sliding), and yaw (clockwise and counter-clockwise rotation). These motion axes are controlled by different axes of the gamepad; for instance, the left joystick controls the axial and lateral movements, while the right joystick controls rotation. A critical aspect of the program is the correct setting of the rotation direction for each motor. Instructions are provided within the code on how to test and set the motor directions. For example, if the robot moves backward when intending to move forward, the direction of all four motors needs to be reversed. Throughout the operation, the program continuously updates and displays the elapsed match time and the power level of each wheel. This information is shown through telemetry data, providing real-time feedback to the driver. The program concludes its operation (when the driver presses the STOP button) by halting all actions."
    },
    {
      "title": "Fll Blocks Explanation",
      "text": "Beyond code generation capabilities, as demonstrated in prior evaluations, we further extended to appraise the understanding and explanation performance of the three prevailing multimodal LLMs -- GPT-4V, LLaVA-1.5, and Microsoft Copilot -- concerning the FLL block diagrams. These block diagrams, characterized by their intuitive drag-and-drop functionality, aim to demystify robotics programming for younger audiences, particularly those in elementary and middle schools. They facilitate the assembly of command sequences essential for controlling robot movement and sensor responses. To assess the LLMs' comprehension of FLL block diagrams, three specific tasks were delineated: improving pivot turn accuracy, controlling motor movement with color sensor, and adjusting the speed of the motors based on the force sensor. Each task involves presenting the FLL block diagrams as inputs to the aforementioned LLMs, which are then tasked with articulating the purpose and function of these blocks. The initial task is aimed at enhancing the precision of pivot turns within robotics applications. The selected FLL blocks propose strategies to augment the accuracy of pivot turns, acknowledging potential overshoots in turn angles due to gyroscopic read delays and inherent robot momentum (e.g., a robot executing a 102-degree turn instead of the intended 90 degrees). The blocks recommend adjustments in the turn angles to offset these discrepancies, factoring in the robot's velocity and design specifics. An example provided is the Droid Bot IV configuration, which suggests a 12-degree correction to secure an exact 90-degree turn. The second task introduces the integration of color sensor within robotics programming, employing Wait Until Blocks for enhanced control. The FLL blocks tested offer a strategy for programming a robot to proceed until it detects black regions with its color sensor. This approach involves setting up the movement motors and speed, initiating the robot's movement, using the Wait Until Block to await the detection of black regions, and subsequently halting the robot. This scenario exemplifies the practical application of sensor inputs in robotic programming for accomplishing specific objectives. Finally, the third task focuses on the application of Repeat Blocks for dynamic motor control, based on force-sensory feedback. The FLL programming blocks provided relate to adjusting a robot's motor speed dynamically, contingent upon the force detected by a sensor. This loop persists until the force sensor's activation ceases, illustrating the utility of these blocks in executing operations pending the fulfillment of a specified condition. This guidance is pivotal for cultivating foundational robotics skills that enable the development of responsive and adaptable robot behaviors."
    },
    {
      "title": "Visual Programming Code Generation",
      "text": "In the realm of Visual Programming Code Generation, research has explored innovative approaches to translating visual inputs into executable code. Among these approaches, leveraging multimodal models to interpret and generate code from visual diagrams, such as the FIRST LEGO League (FLL) block diagrams, represents a cutting-edge frontier. ChainCoder introduces a multi-pass code generation framework that uses a unique syntax hierarchy to enhance stepwise reasoning in LLMs, aiming to improve code accuracy and syntactic coherence. It employs a syntax-aware tokenizer and a specialized transformer to leverage structured, syntactically aligned data for progressive, multi-level code generation[46]. VISPROG is a system that leverages the in-context learning capabilities of language models to transform natural language instructions into visual programs for complex visual tasks, enabling the creation of sophisticated visual solutions directly from user inputs[18]. However, our investigations reveal that not all multimodal models are equally proficient at generating high-quality code from such visual inputs. For instance, while GPT-4V demonstrates remarkable capability in this domain, another model, Copilot, falls short. This discrepancy may stem from several factors. First, Copilot might struggle with prompts not written in English, highlighting a potential language bias. Second,its generation and evaluation modules, primarily trained on natural images, may not effectively interpret visual diagrams, which are fundamentally different in structure and content from typical photographic imagery. In light of these challenges, our research pivoted towards leveraging textual prompts to generate pseudocode as an intermediary step. This approach capitalizes on the superior textual processing capabilities of multimodal models over their visual processing counterparts. By instructing the model to generate step-by-step, finely represented pseudocode instead of a coarse overview, we harness the model's linguistic strengths. Our prompts are meticulously crafted to guide the model in this process, resulting in more accurate and functional code generation. Our testing, conducted in a zero-shot learning context, underscores the potential of these models. However, it also opens avenues for further exploration into few-shot learning and fine-tuning strategies. Such enhancements could significantly improve the models' code generation capabilities, especially when dealing with complex visual inputs or domain-specific requirements. As we continue to refine our methodologies, the anticipation grows for what future iterations of these large models might achieve in the field of Visual Programming Code Generation, blending the visual and textual realms to automate and innovate in coding practices."
    },
    {
      "title": "4 Experimental Results",
      "text": "Table 1 shows the testing results for Leetcode problems. GPT-4V passes all of three test problems for all of four programming languages while Code Llama shows the worst performance. For LeetCode 232 and LeetCode 416 corresponding to easy and medium level, only Python3 and C pass the test. The rest two of languages encounter either compile errors or wrong answer. For the hard problem, none of the answers passes the test. GitHub Copilot passes the first two problems but give wrong answer for the LeetCode 847. In this test, GPT-4V demonstrates the best performance followed by GitHub Copilot and Code Llama. Figure 1 and 2 show the FTC code generation task results. Both of two models construct the complete code framework. The general logic of codes are clear with correct definitions and sequential operations. GPT-4V can recover around 90% of the original code with minor missing parts. However, these \\begin{table} \\begin{tabular}{c c c c} \\hline Models & LeetCode 232 & LeetCode 416 & LeetCode 847 \\\\ \\hline GPT-4V & 100\\% & 100\\% & 100\\% \\\\ Code Llama & 50\\% & 50\\% & 0\\% \\\\ GitHub Copilot & 100\\% & 100\\% & 75\\% \\\\ \\hline \\end{tabular} \\end{table} Table 1: Leetcode test pass rate for three LLMs. The percentage indicates how many programming languages the LLM generated pass the test. Figure 1: One example for FTC code generation test results.(a) represents the original code and (b) is the code generated by GPT-4V. The green backgrounds indicate correct parts. Figure 2: Same example for FTC code generation test results.(a) represents the original code and (b) is the code generated by GitHub Copilot. The green backgrounds indicate correct parts. defects lie on printed messages and extra parameters validity check, which does not have severe consequence for the main module. Compared with GPT-4V, GitHub Copilot lacks more parameters validity check. It also skips or combines some operations such as setting front and rear motor, which leads to movement failure. Besides, it does not follow the comments to show the elapsed game time and wheel power, suggesting the code is not as delicate as GPT-4V's. Code Llama fails in this task. Out test shows that Code Llama cannot understand the task. It refuses to generate code but with some explanation text. In summary, GPT-4V still exceeds other two models for FTC code generation. Figure 3 and 4 demonstrate the performance for one FLL block diagram understanding and explanation. Since this task involves images analysis, we attempt three multimodal LLMs: GPT-4V, Microsoft Copilot and LLaVA-1.5. GPT-4V recognizes the diagram successfully. Although it does not specify which block diagram this image belongs to, it gives a perfect explanation for each block. The comments are accurate and clear enough so that even young children can understand this block diagram easily. As for Microsoft Copilot, it does not explain each steps. Instead, it only claims the overall function of this block diagram. The logic does not follow the original diagram. Besides, some parts are wrong and never mentioned in the image. For example, it misses to set one movement motor. \"Turn the robot\" refers to set yaw angle to 0, which is unclear and easy to cause misleading. The comments also use 'gyro' block that never appears in the diagram. This diagram explanation is unqualified because young children are not able to understand the logic of original diagram with these comments. They cannot select and build the required blocks or set Figure 3: One example for FLL block diagram explanation test results. The left code refers to the FLL block diagram while right part contains explanation from GPT-4V. the correct parameters either. LLaVA-1.5 fails again in this task. It cannot recognize this block diagram and generates some irrelevant texts. As for other two test examples, the results are similar to the first example for all of three models. For the final task, we apply DALL-E 3, LLaVA-1.5 and Microsoft Copilot, trying to generate direct FLL block diagrams. However, none of these models generates meaningful block diagram images with right logic. They cannot understand and include reasoning in an image. Therefore, we use GPT-4V, Code Llama and GitHub Copilot to generate names and parameter settings of each block. The results are shown in figure 5 and 6. Given the prompt with operations in each step, GPT-4V recovers each block with correct logic and sequence. The extra comments explain the corresponding functions. Although it contains one more block'read force from sensor', young children can still follow the names of blocks to select desired blocks with correct parameters. However, the result from GitHub Copilot only repeats each instruction in input prompt with short explanations, failing to demonstrate block names as well as parameters. Code Llama cannot understand FLL block diagram and refuses to generate pseudo code again in this task. In summary, GPT-4V outperforms than other LLMs or multimodal LLMs by finishing specific tasks with almost perfect codes and explanations. GitHub Copilot shows promise in traditional code generation and assistance but has unsatisfying performance when encountering block diagrams format. Llama series models are the worst LLMs among all the test tasks. Figure 4: Same example for FLL block diagram explanation test results. The left code refers to the FLL block diagram while right part contains explanation from Microsoft Copilot. The yellow background indicates inaccurate description and red for wrong steps. Figure 5: One example for FLL block diagram text pseudocode generation results. Figure 6: One example for FLL block diagram text pseudo code generation results. The yellow background indicates inaccurate description."
    },
    {
      "title": "5 Limitations",
      "text": "This study presents several limitations in the experimental methodology. Firstly, the analysis is confined to a selection of mainstream language models, excluding a comprehensive examination of other LLMs and multimodal LLMs. Secondly, our investigation predominantly centers on the capabilities of these models in understanding, reasoning, and generating robotic code, with a relatively narrow focus on traditional coding problems as delineated in Section 3.1. The methodology involves generating code solutions across four different programming languages; however, for each level of difficulty, only a singular problem is selected. This approach does not encompass the wide array of data structures and algorithms frequently employed within corporate settings. Regarding the FTC code, our evaluation is limited to the essential framework and logical constructs of the code. The practical application of these techniques in an actual FTC competition could potentially offer more insightful findings. The limited reasoning capabilities of current multimodal LLMs constrain our ability to directly generate FLL block diagrams. Future research will aim to extend the testing to emerging multimodal LLMs, enhancing the scope of this study. Additionally, involving children in the testing process may provide valuable insights into any advancements in their learning outcomes."
    },
    {
      "title": "6 Conclusion",
      "text": "In this paper, we test several LLMs and multimodal LLMs for both traditional code generation as well as robot code understanding and generation. We find that GPT-4V outperform than other models in all of our tests while Llama series models show the worst results. However, all models suffer from the failure when generating direct block diagrams. We demonstrate the crucial trend applying LLMs and multimodal LLMs into robot education for young people. We hope our test can inspire a closer integration of advanced artificial general intelligence with code education. This approach should encompass not only traditional algorithms but also robot competitions, which demand proficiency in both software coding and robot structure building."
    },
    {
      "title": "References",
      "text": "* [1]Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) * [2]Akyol Altun, C.: Okul oncesi ogretim programna algoritma ve kodlama egitimi entegrasyonunun Ogrencilerin problem cozme becerisine etkisi. Master's thesis, Egitim Bilimleri Enstitusu (2018) * [3]Alon, U., Brody, S., Levy, O., Yahav, E.: code2seq: Generating sequences from structured representations of code (2019)* [4] Bers, M.U.: Coding as a playground: Programming and computational thinking in the early childhood classroom. Routledge (2020) * [5] Bocconi, S., Chioccariello, A., Dettori, G., Ferrari, A., Engelhardt, K., et al.: Developing computational thinking in compulsory education-implications for policy and practice. Tech. rep., Joint Research Centre (Seville site) (2016) * [6] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners (2020) * [7] Caballero-Gonzalez, Y.A., Munoz-Repiso, A.G.V., Garcia-Holgado, A.: Learning computational thinking and social skills development in young children through problem solving with educational robotics. In: Proceedings of the seventh international conference on technological ecosystems for enhancing Multiculturality. pp. 19-23 (2019) * [8] Cakir, R., Korkmaz, O., Idil, O., Erdogmus, F.U.: The effect of robotic coding education on preschoolers' problem solving and creative thinking skills. Thinking Skills and Creativity **40**, 100812 (2021) * [9] Canbeldek, M., Isikoglu, N.: Exploring the effects of \"productive children: coding and robotics education program\" in early childhood education. Education and Information Technologies **28**(3), 3359-3379 (2023) * [10] Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H.P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F.P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W.H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A.N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., Zaremba, W.: Evaluating large language models trained on code (2021) * [11] Chong, K.: How we fine-tuned llama 2 for our block coding copilot (2023), [https://www.tynker.com/blog/how-we-fine-tuned-llama-2-for-our-block-coding-copilot/](https://www.tynker.com/blog/how-we-fine-tuned-llama-2-for-our-block-coding-copilot/) * [12] Chun, H.Y., Park, S.: The effect of steam-based unplugged play activities using robots on the improvement of children's creative and social personalities. Korean Journal of Childcare and Education **16**(5), 1-26 (2020)* [13] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018) * [14] Di Lieto, M.C., Inguaggiato, E., Castro, E., Cecchi, F., Cioni, G., Dell'Omo, M., Laschi, C., Pecini, C., Santerini, G., Sgandurra, G., et al.: Educational robotics intervention on executive functions in preschool children: A pilot study. Computers in human behavior **71**, 16-23 (2017) * [15] Dorouka, P., Papadakis, S., Kalogiannakis, M.: Tablets and apps for promoting robotics, mathematics, stem education and literacy in early childhood education. International Journal of Mobile Learning and Organisation **14**(2), 255-274 (2020) * [16] Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., Zhou, M.: Codebert: A pre-trained model for programming and natural languages (2020) * [17] Finnie-Ansley, J., Denny, P., Becker, B.A., Luxton-Reilly, A., Prather, J.: The robots are coming: Exploring the implications of openai codera on introductory programming. In: Proceedings of the 24th Australasian Computing Education Conference. pp. 10-19 (2022) * [18] Gupta, T., Kembhavi, A.: Visual programming: Compositional visual reasoning without training. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14953-14962 (2023) * [19] Hajipour, H., Hassler, K., Holz, T., Schonherr, L., Fritz, M.: Codelmsec benchmark: Systematically evaluating and finding security vulnerabilities in black-box code language models (2023) * [20] Hilton, A.: Aries hilton's pyscratch: A software for converting python files to scratch projects (2023), [https://medium.com/@arieshilton2000/pyscratch-a-software-for-converting-python-files-to-scratch-projects-ca40a490e5ce.pdf](https://medium.com/@arieshilton2000/pyscratch-a-software-for-converting-python-files-to-scratch-projects-ca40a490e5ce.pdf) * [21] Inspiration, of Science, R., (FIRST), T.: Research shows first drives stem engagement and outcomes (2023), [https://www.firstinspires.org/sites/default/files/uploads/resource_library/impact/stem-boosting-engagement.pdf](https://www.firstinspires.org/sites/default/files/uploads/resource_library/impact/stem-boosting-engagement.pdf) * [22] Inspiration, of Science, R., (FIRST), T.: About first: Our mission, purpose & values (2024), [https://www.firstinspires.org/about](https://www.firstinspires.org/about) * [23] Inspiration, of Science, R., (FIRST), T.: What is first lego league? (2024), https:[https://www.firstlegoleague.org/about](https://www.firstlegoleague.org/about) * [24] Iturbe, C.B., Ochoa, L.L., Castello, M.J., Pelayo, J.C.: Educating the engineer of 2020: Adapting engineering education to the new century. In: INTED2009 Proceedings. pp. 1110-1121. IATED (2009)* [25] Jin, Y., Li, D., Yong, A., Shi, J., Hao, P., Sun, F., Zhang, J., Fang, B.: Robotgpt: Robot manipulation learning from chatgpt. IEEE Robotics and Automation Letters (2024) * [26] Kalliamvakou, E.: Research: quantifying github copilot's impact on developer productivity and happiness. The GitHub Blog, Sep **7** (2022) * [27] Kanaki, K., Kalogiannakis, M.: Introducing fundamental object-oriented programming concepts in preschool education within the context of physical science courses. Education and Information Technologies **23**(6), 2673-2698 (2018) * [28] Kazakoff, E.R., Bers, M.U.: Put your robot in, put your robot out: Sequencing through programming robots in early childhood. Journal of Educational Computing Research **50**(4), 553-573 (2014) * [29] Koc, A.: Okul oncesi ve temel fen egitiminde robotik destekli ve basit malzemelerle yapalan stem uygulamalarnn karslastirmlmasi (2019) * [30] Lee, J.: Coding in early childhood. Contemporary Issues in Early Childhood **21**(3), 266-269 (2020) * [31] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint arXiv:2304.08485 (2023) * [32] Liu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A., He, M., Liu, Z., et al.: Summary of chatgpt-related research and perspective towards the future of large language models. Meta-Radiology p. 100017 (2023) * [33] Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S.K., Fu, S., Liu, S.: Codexglue: A machine learning benchmark dataset for code understanding and generation (2021) * [34] Odaci, M., Uzun, E.: Okul oncesinde kodlama egitimi ve kullanlabilecek aracglar hakkunda bilisim teknolojileri ogretmenlerinin gorusleri: Bir durum calismas. 1. Uluslararasi Biligisayar ve Ogretim Teknolojileri Sempozyumu, Inonu Universitesi **718**, 725 (2017) * [35] Oguz, V.: Proje yaklasimnn anasinfina devam eden cocuklarn problem cozme becerilerine etkiisinin incelenmesi (2012) * [36] Poldrack, R.A., Lu, T., Begus, G.: Ai-assisted coding: Experiments with gpt-4. arXiv preprint arXiv:2304.13187 (2023) * [37] Saxena, A., Lo, C.K., Hew, K.F., Wong, G.K.W.: Designing unplugged and plugged activities to cultivate computational thinking: An exploratory study in early childhood education. The Asia-Pacific Education Researcher **29**(1), 55-66 (2020)* [38] Siahaan, P., Suryani, A., Kaniawati, I., Suhendi, E., Samsudin, A.: Improving students' science process skills through simple computer simulations on linear motion conceptions. In: Journal of Physics: Conference Series. vol. 812, p. 012017. IOP Publishing (2017) * [39] Taguma, M., Barrera, M.: Oecd future of education and skills 2030: Curriculum analysis. Disponibile su: https://www. oecd. org/education/2030-project/teaching-and-learning/learning/skills/Skills_for_2030. pdf (2019) * [40] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) * [41] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017) * [42] Vemprala, S., Bonatti, R., Bucker, A., Kapoor, A.: Chatgpt for robotics: Design principles and model abilities. Microsoft Auton. Syst. Robot. Res **2**, 20 (2023) * [43] Wang, J., Wu, Z., Li, Y., Jiang, H., Shu, P., Shi, E., Hu, H., Ma, C., Liu, Y., Wang, X., et al.: Large language models for robotics: Opportunities, challenges, and perspectives. arXiv preprint arXiv:2401.04334 (2024) * [44] Yigitalp, N.: Yonlendirilmis beyin frtnasi (scamper) teknigine dayali egitimin bes yas gocuklarnin problem cozme becerilerine etkisinin incelennesi (2014) * [45] Zhao, L., Zhang, L., Wu, Z., Chen, Y., Dai, H., Yu, X., Liu, Z., Zhang, T., Hu, X., Jiang, X., et al.: When brain-inspired ai meets agi. Meta-Radiology p. 100005 (2023) * [46] Zheng, W., Sharan, S., Jaiswal, A.K., Wang, K., Xi, Y., Xu, D., Wang, Z.: Outline, then details: Syntactically guided coarse-to-fine code generation. arXiv preprint arXiv:2305.00909 (2023)"
    }
  ]
}