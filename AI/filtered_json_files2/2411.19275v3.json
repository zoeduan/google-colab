{
  "title": "VeCoGen: Automating Generation of Formally Verified C Code with Large Language Models",
  "authors": [
    "Merlijn Sevenhuijsen",
    "Khashayar Etemadi",
    "Mattias Nyberg"
  ],
  "abstract": "\n Large language models have demonstrated impressive capabilities in generating code, yet they often produce programs with flaws or deviations from intended behavior, limiting their suitability for safety-critical applications. To address this limitation, this paper introduces VECOGEN, a novel tool that combines large language models with formal verification to automate the generation of formally verified C programs. VECOGEN takes a formal specification in ANSI/ISO C Specification Language, a natural language specification, and a set of test cases to attempt to generate a verified program. This program-generation process consists of two steps. First, VECOGEN generates an initial set of candidate programs. Secondly, the tool iteratively improves on previously generated candidates. If a candidate program meets the formal specification, then we are sure the program is correct. We evaluate VECOGEN on 15 problems presented in Codeforces competitions. On these problems, VECOGEN solves 13 problems. This work shows the potential of combining large language models with formal verification to automate program generation. \n",
  "references": [
    {
      "id": null,
      "title": "VeCoGen: Automating Generation of Formally Verified C Code with Large Language Models",
      "authors": [
        "Merlijn Sevenhuijsen",
        "Khashayar Etemadi",
        "Mattias Nyberg"
      ],
      "year": "2025",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "T B Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D M Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Language Models are Few-Shot Learners",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "A Survey on Evaluation of Large Language Models",
      "authors": [
        "Y Chang",
        "X Wang",
        "J Wang",
        "Y Wu",
        "L Yang",
        "K Zhu",
        "H Chen",
        "X Yi",
        "C Wang",
        "Y Wang",
        "W Ye",
        "Y Zhang",
        "Y Chang",
        "P S Yu",
        "Q Yang",
        "X Xie"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "doi": "10.1145/3641289"
    },
    {
      "id": "b2",
      "title": "LLMs with Industrial Lens: Deciphering the Challenges and Prospects -A Survey",
      "authors": [
        "A Urlana",
        "C V Kumar",
        "A K Singh",
        "B M Garlapati",
        "S R Chalamala",
        "R Mishra"
      ],
      "year": "2024",
      "venue": "LLMs with Industrial Lens: Deciphering the Challenges and Prospects -A Survey",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Teaching Large Language Models to Translate with Comparison",
      "authors": [
        "J Zeng",
        "F Meng",
        "Y Yin",
        "J Zhou"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Evaluating Large Language Models Trained on Code",
      "authors": [
        "M Chen",
        "J Tworek",
        "H Jun",
        "Q Yuan",
        "H P D O Pinto",
        "J Kaplan",
        "H Edwards",
        "Y Burda",
        "N Joseph",
        "G Brockman",
        "A Ray",
        "R Puri",
        "G Krueger",
        "M Petrov",
        "H Khlaaf",
        "G Sastry",
        "P Mishkin",
        "B Chan",
        "S Gray",
        "N Ryder",
        "M Pavlov",
        "A Power",
        "L Kaiser",
        "M Bavarian",
        "C Winter",
        "P Tillet",
        "F P Such",
        "D Cummings",
        "M Plappert",
        "F Chantzis",
        "E Barnes",
        "A Herbert-Voss",
        "W H Guss",
        "A Nichol",
        "A Paino",
        "N Tezak",
        "J Tang",
        "I Babuschkin",
        "S Balaji",
        "S Jain",
        "W Saunders",
        "C Hesse",
        "A N Carr",
        "J Leike",
        "J Achiam",
        "V Misra",
        "E Morikawa",
        "A Radford",
        "M Knight",
        "M Brundage",
        "M Murati",
        "K Mayer",
        "P Welinder",
        "B Mcgrew",
        "D Amodei",
        "S Mccandlish",
        "I Sutskever",
        "W Zaremba"
      ],
      "year": "2021",
      "venue": "Evaluating Large Language Models Trained on Code",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
      "authors": [
        "J Liu",
        "C S Xia",
        "Y Wang",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation",
      "authors": [
        "S Lu",
        "D Guo",
        "S Ren",
        "J Huang",
        "A Svyatkovskiy",
        "A Blanco",
        "C Clement",
        "D Drain",
        "D Jiang",
        "D Tang",
        "G Li",
        "L Zhou",
        "L Shou",
        "L Zhou",
        "M Tufano",
        "M Gong",
        "M Zhou",
        "N Duan",
        "N Sundaresan",
        "S K Deng",
        "S Fu",
        "S Liu"
      ],
      "year": "2021",
      "venue": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Is Your AI-Generated Code Really Safe? Evaluating Large Language Models on Secure Code Generation with CodeSecEval",
      "authors": [
        "J Wang",
        "X Luo",
        "L Cao",
        "H He",
        "H Huang",
        "J Xie",
        "A Jatowt",
        "Y Cai"
      ],
      "year": "2024",
      "venue": "Is Your AI-Generated Code Really Safe? Evaluating Large Language Models on Secure Code Generation with CodeSecEval",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Safety critical systems: challenges and directions",
      "authors": [
        "J C Knight"
      ],
      "year": "2002",
      "venue": "Proceedings of the 24th international conference on Software engineering -ICSE '02",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Comprehensive formal verification of an OS microkernel",
      "authors": [
        "G Klein",
        "J Andronick",
        "K Elphinstone",
        "T Murray",
        "T Sewell",
        "R Kolanski",
        "G Heiser"
      ],
      "year": "2014",
      "venue": "ACM Transactions on Computer Systems",
      "doi": "10.1145/2560537"
    },
    {
      "id": "b10",
      "title": "Formal verification of a realistic compiler",
      "authors": [
        "X Leroy"
      ],
      "year": "2009",
      "venue": "Commun. ACM",
      "doi": "10.1145/1538788.1538814"
    },
    {
      "id": "b11",
      "title": "ACSL: ANSI/ISO C Specification",
      "authors": [
        "P Baudin",
        "J.-C Fillia",
        "C Marche",
        "B Monate",
        "Y Moy",
        "V Prevosto"
      ],
      "year": "2021",
      "venue": "ACSL: ANSI/ISO C Specification",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Frama-C: A software analysis perspective",
      "authors": [
        "F Kirchner",
        "N Kosmatov",
        "V Prevosto",
        "J Signoles",
        "B Yakobowski"
      ],
      "year": "2015",
      "venue": "Form. Asp. Comput",
      "doi": "10.1007/s00165-014-0326-7"
    },
    {
      "id": "b13",
      "title": "A Theory of Formal Synthesis via Inductive Learning",
      "authors": [
        "S Jha",
        "S A Seshia"
      ],
      "year": "2016",
      "venue": "A Theory of Formal Synthesis via Inductive Learning",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "From program verification to program synthesis",
      "authors": [
        "S Srivastava",
        "S Gulwani",
        "J S Foster"
      ],
      "year": "2010",
      "venue": "Proceedings of the 37th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages, ser. POPL '10",
      "doi": "10.1145/1706299.1706337"
    },
    {
      "id": "b15",
      "title": "Program synthesis by sketching",
      "authors": [
        "A Solar-Lezama"
      ],
      "year": "2008",
      "venue": "Program synthesis by sketching",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Towards Automated Verification of LLM-Synthesized C Programs",
      "authors": [
        "P Mukherjee",
        "B Delaware"
      ],
      "year": "2024",
      "venue": "Towards Automated Verification of LLM-Synthesized C Programs",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Towards Specification-Driven LLM-Based Generation of Embedded Automotive Software",
      "authors": [
        "M S Patil",
        "G Ung",
        "M Nyberg"
      ],
      "year": "2024",
      "venue": "Towards Specification-Driven LLM-Based Generation of Embedded Automotive Software",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Frama-C",
      "authors": [
        "P Cuoq",
        "F Kirchner",
        "N Kosmatov",
        "V Prevosto",
        "J Signoles",
        "B Yakobowski"
      ],
      "year": "2012",
      "venue": "Software Engineering and Formal Methods",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "An experimental Study using ACSL and Frama-C to formulate and verify Low-Level Requirements from a DO-178C compliant Avionics Project",
      "authors": [
        "F Dordowsky"
      ],
      "year": "2015",
      "venue": "Electronic Proceedings in Theoretical Computer Science",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Program Verification in SPARK and ACSL: A Comparative Case Study",
      "authors": [
        "E Brito",
        "J Sousa Pinto"
      ],
      "year": "2010",
      "venue": "Reliable Software Technologiey Ada-Europe",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "",
      "authors": [
        "Springer"
      ],
      "year": "2010",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Post-Hoc Formal Verification of Automotive Software with Informal Requirements: An Experience Report",
      "authors": [
        "G Ung",
        "J Amilon",
        "D Gurov",
        "C Lidstrm",
        "M Nyberg",
        "K Palmskog"
      ],
      "year": "2024",
      "venue": "2024 IEEE 32nd International Requirements Engineering Conference (RE)",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "An axiomatic basis for computer programming",
      "authors": [
        "C A R Hoare"
      ],
      "year": "1969",
      "venue": "Communications of the ACM",
      "doi": "10.1145/363235.363259"
    },
    {
      "id": "b24",
      "title": "Why: a multi-language multi-prover verification tool",
      "authors": [
        "J.-C Fillitre"
      ],
      "year": "2003",
      "venue": "Why: a multi-language multi-prover verification tool",
      "doi": "10.1007/978-3-540-78739-6_24"
    },
    {
      "id": "b25",
      "title": "Z3: an efficient SMT solver",
      "authors": [
        "L De Moura",
        "N Bjrner"
      ],
      "year": "2008",
      "venue": "Proceedings of the Theory and practice of software, 14th international conference on Tools and algorithms for the construction and analysis of systems",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Attention is All you Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A N Gomez",
        ". U Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Calibration and Correctness of Language Models for Code",
      "authors": [
        "C Spiess",
        "D Gros",
        "K S Pai",
        "M Pradel",
        "M R I Rabin",
        "A Alipour",
        "S Jha",
        "P Devanbu",
        "T Ahmed"
      ],
      "year": "2024",
      "venue": "Calibration and Correctness of Language Models for Code",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "",
      "authors": [
        "Online"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "The FormAI Dataset: Generative AI in Software Security through the Lens of Formal Verification",
      "authors": [
        "N Tihanyi",
        "T Bisztray",
        "R Jain",
        "M A Ferrag",
        "L C Cordeiro",
        "V Mavroeidis"
      ],
      "year": "2023",
      "venue": "Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering, ser. PROMISE 2023",
      "doi": "10.1145/3617555.3617874"
    },
    {
      "id": "b30",
      "title": "Exploring and Evaluating Hallucinations in LLM-Powered Code Generation",
      "authors": [
        "F Liu",
        "Y Liu",
        "L Shi",
        "H Huang",
        "R Wang",
        "Z Yang",
        "L Zhang",
        "Z Li",
        "Y Ma"
      ],
      "year": "2024",
      "venue": "Exploring and Evaluating Hallucinations in LLM-Powered Code Generation",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "",
      "authors": [
        "Online"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Codeforces",
      "authors": [],
      "year": "",
      "venue": "Codeforces",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Code4Bench: A Multidimensional Benchmark of Codeforces Data for Different Program Analysis Techniques",
      "authors": [
        "M Amirabbas",
        "V.-A Mojtaba",
        "K Alireza",
        "B.-D Ahmad",
        "Z Bahman"
      ],
      "year": "2019",
      "venue": "Code4Bench: A Multidimensional Benchmark of Codeforces Data for Different Program Analysis Techniques",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Guidelines for the use of the C language in Critical Systems, MIRA Std",
      "authors": [
        "Mira Ltd",
        "Misra-C"
      ],
      "year": "2004",
      "venue": "Guidelines for the use of the C language in Critical Systems, MIRA Std",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
      "authors": [
        "K Stechly",
        "M Marquez",
        "S Kambhampati"
      ],
      "year": "2023",
      "venue": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "SpecEval: Evaluating Code Comprehension in Large Language Models via Program Specifications",
      "authors": [
        "L Ma",
        "S Liu",
        "L Bu",
        "S Li",
        "Y Wang",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "SpecEval: Evaluating Code Comprehension in Large Language Models via Program Specifications",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "TriCo-Triple Co-piloting of Implementation, Specification and Tests",
      "authors": [
        "W Ahrendt",
        "D Gurov",
        "M Johansson",
        "P Rmmer"
      ],
      "year": "2022",
      "venue": "Leveraging Applications of Formal Methods, Verification and Validation",
      "doi": "10.1007/978-3-031-19849-6_11"
    },
    {
      "id": "b38",
      "title": "Clover: Closed-Loop Verifiable Code Generation",
      "authors": [
        "C Sun",
        "Y Sheng",
        "O Padon",
        "C Barrett"
      ],
      "year": "2024",
      "venue": "Clover: Closed-Loop Verifiable Code Generation",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "",
      "authors": [
        "Online"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Specify What? Enhancing Neural Specification Synthesis by Symbolic Methods",
      "authors": [
        "G Granberry",
        "W Ahrendt",
        "M Johansson"
      ],
      "year": "2024",
      "venue": "Specify What? Enhancing Neural Specification Synthesis by Symbolic Methods",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Enchanting Program Specification Synthesis by Large Language Models Using Static Analysis and Program Verification",
      "authors": [
        "C Wen",
        "J Cao",
        "J Su",
        "Z Xu",
        "S Qin",
        "M He",
        "H Li",
        "S.-C Cheung",
        "C Tian"
      ],
      "year": "2024",
      "venue": "Computer Aided Verification",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models",
      "authors": [
        "M Cosler",
        "C Hahn",
        "D Mendoza",
        "F Schmitt",
        "C Trippel"
      ],
      "year": "2023",
      "venue": "Computer Aided Verification",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "C2S: translating natural language comments to formal program specifications",
      "authors": [
        "J Zhai",
        "Y Shi",
        "M Pan",
        "G Zhou",
        "Y Liu",
        "C Fang",
        "S Ma",
        "L Tan",
        "X Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ser. ESEC/FSE 2020",
      "doi": "10.1145/3368089.3409716"
    },
    {
      "id": "b44",
      "title": "Generation of Formal Requirements from Structured Natural Language",
      "authors": [
        "D Giannakopoulou",
        "T Pressburger",
        "A Mavridou",
        "J Schumann"
      ],
      "year": "2020",
      "venue": "Requirements Engineering: Foundation for Software Quality",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Oracle-guided component-based program synthesis",
      "authors": [
        "S Jha",
        "S Gulwani",
        "S A Seshia",
        "A Tiwari"
      ],
      "year": "",
      "venue": "Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "",
      "authors": [],
      "year": "2010",
      "venue": "",
      "doi": "10.1145/1806799.1806833"
    },
    {
      "id": "b47",
      "title": "Formal synthesis and code generation of embedded real-time software",
      "authors": [
        "P.-A Hsiung"
      ],
      "year": "2001",
      "venue": "Proceedings of the ninth international symposium on Hardware/software codesign, ser. CODES '01",
      "doi": "10.1145/371636.371729"
    },
    {
      "id": "b48",
      "title": "Applying formal software synthesis",
      "authors": [
        "R Jullig"
      ],
      "year": "1993",
      "venue": "conference Name: IEEE Software",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "A Deductive Approach to Program Synthesis",
      "authors": [
        "Z Manna",
        "R Waldinger"
      ],
      "year": "1980",
      "venue": "ACM Transactions on Programming Languages and Systems",
      "doi": "10.1145/357084.357090"
    },
    {
      "id": "b50",
      "title": "Syntax-guided synthesis",
      "authors": [
        "R Alur",
        "R Bodik",
        "G Juniwal",
        "M M K Martin",
        "M Raghothaman",
        "S A Seshia",
        "R Singh",
        "A Solar-Lezama",
        "E Torlak",
        "A Udupa"
      ],
      "year": "2013",
      "venue": "2013 Formal Methods in Computer-Aided Design",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Fiat: Deductive Synthesis of Abstract Data Types in a Proof Assistant",
      "authors": [
        "B Delaware",
        "C Pit-Claudel",
        "J Gross",
        "A Chlipala"
      ],
      "year": "2015",
      "venue": "ACM SIGPLAN Notices",
      "doi": "10.1145/2775051.2677006"
    },
    {
      "id": "b52",
      "title": "Formal Modeling and Automatic Code Synthesis for Robot System",
      "authors": [
        "X Li",
        "R Wang",
        "Y Jiang",
        "Y Guan",
        "X Li",
        "X Song"
      ],
      "year": "2017",
      "venue": "2017 22nd International Conference on Engineering of Complex Computer Systems (ICECCS)",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Combining LLM Code Generation with Formal Specifications and Reactive Program Synthesis",
      "authors": [
        "W Murphy",
        "N Holzer",
        "F Qiao",
        "L Cui",
        "R Rothkopf",
        "N Koenig",
        "M Santolucito"
      ],
      "year": "2024",
      "venue": "Combining LLM Code Generation with Formal Specifications and Reactive Program Synthesis",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Automatic Software Repair: A Bibliography",
      "authors": [
        "M Monperrus"
      ],
      "year": "2018",
      "venue": "ACM Computing Surveys",
      "doi": "10.1145/3105906"
    },
    {
      "id": "b55",
      "title": "CigaR: Costefficient Program Repair with LLMs",
      "authors": [
        "D Hidvgi",
        "K Etemadi",
        "S Bobadilla",
        "M Monperrus"
      ],
      "year": "2024",
      "venue": "CigaR: Costefficient Program Repair with LLMs",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Exploring True Test Overfitting in Dynamic Automated Program Repair using Formal Methods",
      "authors": [
        "A Nilizadeh",
        "G T Leavens",
        "X.-B D Le",
        "C S Psreanu",
        "D R Cok"
      ],
      "year": "2021",
      "venue": "2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification",
      "authors": [
        "Y Charalambous",
        "N Tihanyi",
        "R Jain",
        "Y Sun",
        "M A Ferrag",
        "L C Cordeiro"
      ],
      "year": "2023",
      "venue": "A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems",
      "authors": [
        "M Fakih",
        "R Dharmaji",
        "Y Moghaddas",
        "G Q Araya",
        "O Ogundare",
        "M A A Faruque"
      ],
      "year": "2024",
      "venue": "LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT",
      "authors": [
        "C S Xia",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting",
      "authors": [
        "S Jha",
        "S K Jha",
        "P Lincoln",
        "N D Bastian",
        "A Velasquez",
        "S Neema"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Assured Autonomy (ICAA)",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Counter-example generation in symbolic abstract model-checking",
      "authors": [
        "G Pace",
        "N Halbwachs",
        "P Raymond"
      ],
      "year": "2004",
      "venue": "International Journal on Software Tools for Technology Transfer",
      "doi": "10.1007/s10009-003-0127-4"
    },
    {
      "id": "b62",
      "title": "Automated Repair of Programs from Large Language Models",
      "authors": [
        "Z Fan",
        "X Gao",
        "M Mirchev",
        "A Roychoudhury",
        "S H Tan"
      ],
      "year": "2023",
      "venue": "2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "Fully Autonomous Programming with Large Language Models",
      "authors": [
        "V Liventsev",
        "A Grishina",
        "A Hrm",
        "L Moonen"
      ],
      "year": "2023",
      "venue": "Proceedings of the Genetic and Evolutionary Computation Conference, ser. GECCO '23",
      "doi": "10.1145/3583131.3590481"
    },
    {
      "id": "b64",
      "title": "Code Repair with LLMs gives an Exploration-Exploitation Tradeoff",
      "authors": [
        "H Tang",
        "K Hu",
        "J P Zhou",
        "S Zhong",
        "W.-L Zheng",
        "X Si",
        "K Ellis"
      ],
      "year": "2024",
      "venue": "Code Repair with LLMs gives an Exploration-Exploitation Tradeoff",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples",
      "authors": [
        "W R Thompson"
      ],
      "year": "1933",
      "venue": "Biometrika Trust",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Vecogen: Automating Generation Of Formally Verified C Code With Large Language Models",
      "text": "Merlijn Sevenhuijsen _Scania & KTH Royal Institute of Technology_ Sodertalje, Sweden merlijn.sevenhuijsen@scania.com Khashayar Etemadi _KTH Royal Institute of Technology_ Stockholm, Sweden khaes@kth.se Mattias Nyberg _Scania & KTH Royal Institute of Technology_ Sodertalje, Sweden mattias.nyberg@scania.com"
    },
    {
      "title": "Abstract",
      "text": "Large language models have demonstrated impressive capabilities in generating code, yet they often produce programs with flaws or deviations from intended behavior, limiting their suitability for safety-critical applications. To address this limitation, this paper introduces VeCoGen, a novel tool that combines large language models with formal verification to automate the generation of formally verified C programs. VeCoGen takes a formal specification in ANSI/ISO C Specification Language, a natural language specification, and a set of test cases to attempt to generate a verified program. This program-generation process consists of two steps. First, VeCoGen generates an initial set of candidate programs. Secondly, the tool iteratively improves on previously generated candidates. If a candidate program meets the formal specification, then we are sure the program is correct. We evaluate VeCoGen on 15 problems presented in Codeforces competitions. On these problems, VeCoGen solves 13 problems. This work shows the potential of combining large language models with formal verification to automate program generation. Code Generation, Large Language Models, Formal Verification, Iterative Code Improvement."
    },
    {
      "title": "I Introduction",
      "text": "Large Language Models (LLMs) have demonstrated versatility, excelling in various tasks [1, 2, 3, 4]. One of the tasks where LLMs perform well is the generation of programs [5, 6, 7]. Despite their impressive capabilities, LLMs often produce programs with errors or inconsistencies, making them unsuitable for applications requiring high assurance of correctness [8]. This lack of trustworthiness poses a significant challenge to safety-critical domains where the correctness of programs is imperative. In the safety-critical domain, even minor software defects can have severe consequences, such as financial losses or threats to human life [9, 10, 11]. To address the lack of trustworthiness, the present paper introduces a new tool named VeCoGen, which combines LLMs with formal verification techniques to automatically generate C programs that are correct with respect to given specifications. VeCoGen is based upon a novel two-step process of initial code generation and iterative code improvement through feedback from a compiler and verifier. In the initial code generation step, VeCoGen generates an initial set of program candidates based on natural language specifications in English and formal specifications in ANSI/ISO C Specification Language (ACSL) [12]. The Weakest Precondition (WP) and Runtime Error (RTE) plugins of Frama-C [13] then verify the correctness of the program candidates. If all generated program candidates fail compilation or verification, VeCoGen continues to the iterative code improvement step. In this step, VeCoGen parses the feedback from the compiler and verifier to guide the LLM in generating improved candidates. VeCoGen ensures that the generated program candidate is not only syntactically valid but also formally correct with respect to the formal specification. The tool can be downloaded from [https://github.com/ASSERT-KTH/Vecogen](https://github.com/ASSERT-KTH/Vecogen). Traditional works, not utilizing LLMs, have addressed the challenge of generating programs automatically [14], but they often face scalability issues [15, 16]. LLMs offer a promising solution to the scalability issues of generating a program that meets the specifications, as explored in prior research. Mukherjee and Delaware [17] employ LLMs along with human intervention to synthesize and verify C programs, demonstrating capability in handling complex scenarios. Similarly, Patil et al. [18] propose _spec2code_, a framework that combines LLMs with critics to iteratively synthesize programs. However, these existing approaches either rely on manual feedback to the LLM or do not have a tool that implements the fully automatic verified code generation. VeCoGen is the first LLM-based tool that fully automatically generates and verifies C code. We evaluate VeCoGen on 15 competitive programming problems to assess its effectiveness in generating formally verified C programs. VeCoGen solves 13 out of 15 problems, demonstrating its ability to generate formally verified code. This initial benchmarking showcases the potential of generating formally verified C code automatically using VeCoGen, allowing for use in safety-critical software development. The paper contains the following contributions: 1. VeCoGen, a novel LLM-based code generation tool for iteratively generating formally verified C code. 2. The evaluation of VeCoGen on a collection of 15 competitive programming problems. 3. An analysis of the impact of changing the configuration of the tool, including the specification type, the number of generated programs per LLM-invocation, the temperature, the zero- or one-shot prompting, and the choice of LLM. The rest of this paper is organized as follows. Section II provides background on formal verification and LLM-based code generation. Section III describes the design and implementation of VeCoGen. Section IV outlines the experimentalmethodology, and Section V presents the results. Section VI discusses threats to validity, Section VII presents related work, and Section VIII concludes the paper."
    },
    {
      "title": "Ii Background",
      "text": ""
    },
    {
      "title": "_Ansi/Iso C Specification Language_",
      "text": "ANSI/ISO C Specification Language (ACSL) is a formal specification language for describing the desired behavior of a function in C [12]. Figure 1 presents an example of an ACSL specification for a program that computes the sum of two positive integers. The formal specification for this \"add_positive\" function uses three clause types: requires, assigns, and ensures. The requires clause specifies preconditions that must be met before running the function. In the \"add_positive\" example, the first precondition states that both input values, x and y, must be positive integers (line 2). Additionally, the second precondition states that the result variable must point to a valid memory location (line 3). The assigns clause defines which memory locations the function can modify while executing. The function is permitted to only modify the memory location pointed to by the result variable (line 4). The ensures clause defines postconditions that must hold after the function completes execution. The postcondition specifies that the output variable, result, must be equal to the sum of x and y (line 5). A program that verifies against this specification is guaranteed to implement the intended behavior correctly. In addition to the requires, assigns, and ensures clauses, ACSL supports many other types. These extra clauses are explained in detail in the official ACSL documentation1. Footnote 1: For further details on ACSL clauses, refer to the official documentation at [https://frama-c.com/html/acsl.html](https://frama-c.com/html/acsl.html)."
    },
    {
      "title": "_Frama-C_",
      "text": "Frama-C is a platform for the static analysis and formal verification of C programs [13, 19], used in safety-critical projects [20, 21, 22]. The WP plugin of Frama-C, inspired by the principles of Hoare Logic [23], verifies functional properties by generating proof obligations based on ACSL specifications. These obligations are then translated into logical goals using the Why platform [24]. The RTE plugin complements the WP plugin by automatically generating goals to check for runtime errors, such as integer overflows. Automated theorem provers like Alt-Ergo, CVC, and Z3 [25] attempt to prove the logical goals. The theorem provers validate these goals within specified limits. If a goal cannot be proven within the given timeout and computational limits, the plugin provides output to help developers refine their code or specifications. Programs are considered formally verified with respect to the formal specification once all goals generated by the WP and RTE plugins are successfully proven."
    },
    {
      "title": "_Large Language Models_",
      "text": "Large Language Models (LLMs) are machine learning models with a large number of parameters, trained on a vast corpus of data. State-of-the-art LLMs, such as GPT-4o, are built on the Transformer architecture [26] and use decoder-only models [26]. The decoder-only models generate text by continuously predicting the next token, consisting of a small set of characters. After generating a token, the generated token is appended to the input. The LLM continues with iteratively producing more tokens until a stop token is predicted as the next token. These decoder-only models have shown highly promising results on code-related tasks [5], and we employ them in this study for generating formally verified C code."
    },
    {
      "title": "Iii Vecogen Approach",
      "text": "This section describes VeCoGen, an iterative LLM-based tool aimed at automatically generating formally verified C code. VeCoGen uses specifications and test cases to generate a C program that meets the specifications. This approach guarantees the correctness of LLM-generated code, which is a major goal in LLM-based code generation literature [27, 28, 29]."
    },
    {
      "title": "_Vecogen Overview_",
      "text": "Figure 2 presents an overview of how VeCoGen works. The code generation process using VeCoGen consists of four important parts: (1) inputs, which are the program specifications, see Section III-B; (2) initial code generation using VeCoGen, see Section III-C; (3) code improvement using VeCoGen, see Section III-D; (4) output, which is a formally verified program, see Section III-E. The tool, VeCoGen, performs two steps: _initial code generation_ and _code improvement_. The initial code generation step generates a set of initial programs by invoking an LLM through an _initialization prompt_. If none of the programs generated at this step meet the given formal specification, then VeCoGen continues onto step two. Within the code improvement step, VeCoGen iteratively improves the best previously generated program by invoking an LLM using an _\"improvement prompt\"_. This prompt asks the LLM to improve a candidate program using feedback from a compiler and verifier. The tool iteratively prompts an LLM using previously generated feedback until the LLM generates a program that satisfies the formal specification. VeCoGen is the first LLM-based tool that automatically generates formally verified C code."
    },
    {
      "title": "_Vecogen Input_",
      "text": "The search for a formally verified program consists of three inputs: (1) a formal specification given in ACSL, (2) a natural language specification in English, and (3) a set of unit test Fig. 1: Formal specification for an “add_positive” program. cases in C. The first input is a formal specification used to verify candidate programs. The goal of the tool is to generate a program that meets the ACSL specification. The second input is a natural language specification, which is an informal description of the desired behavior of a program. This type of specification conveys the functionality and purpose of the code to the LLM in natural language. Figure 3 shows the natural language description for the \"add_positive\" program defined in Section II-A. It describes the intended behavior of the program as well as its input and output. Besides the formal and natural language specifications, we also define a function signature to specify the interface of the program. The signature defines the function name and the input and output parameters. We append this function signature to the specifications. Lastly, VeCoGen requires a set of unit test cases. Each test case specifies an input along with the expected output. If the output of a program aligns with the expected output, then the test passes. If the output differs, then the unit test fails. VeCoGen relies on these test cases during the iterative code improvement phase to progressively refine the generated programs (see Section III-D for more details). The test cases, natural language, formal specification, and function signature must be consistent."
    },
    {
      "title": "_Step 1: Initial Code Generation_",
      "text": "Based on the two specification types, VeCoGen crafts a prompt to invoke an LLM. VeCoGen uses two types of prompts: the _initialization prompt_ and the _improvement prompt_. Figure 4 presents the outline of both prompt types. Fig. 4: Prompt structure used in VeCoGen for generating formally verified C programs. Fig. 3: Natural language specification for the add_positive program. Fig. 2: Outline of VeCoGen: VeCoGen uses formal and natural language specifications to generate a program that meets the formal specification. Both types contain a system message, a one-shot example, the specifications, and a call to action. The improvement prompt also includes error feedback on a previous attempt (see Section III-D for details). The parts of the prompt highlighted in green and blue remain the same throughout each prompt. Green indicates the role of the LLM, and blue indicates unchanged text explaining the task to the LLM. The gray parts indicate problem-specific information. In the initial code generation step, VeCoGen uses the initialization prompt. This prompt type begins with a system message (A) that assigns an expert software engineer role to the LLM. Next, we include a one-shot example (B). The one-shot example provides one single detailed instance of the desired input-output of the LLM. We use the one-shot example to leverage in-context learning, as this explains the task and expected format to the LLM [1]. This manually created one-shot example includes a natural language specification, a formal specification, a function signature, and a correct implementation. The one-shot example2 remains unchanged across all problems. Footnote 2: The one-shot example can be seen at [https://github.com/ASSERT-KTH/Vecogen/blob/main/prompts/one_shot_example.txt](https://github.com/ASSERT-KTH/Vecogen/blob/main/prompts/one_shot_example.txt). The third part of the prompt presents the specifications of the desired program (C). These program specifications include a natural language specification, an ACSL specification, a function signature, and a text explaining to the LLM what the model has to do with these. The function signature presents the input and output format of the desired program. The third part of the prompt outlines the specifications for the desired program (C). These specifications include four components: a description in natural language, an ACSL specification, a function signature, and instructions for the LLM. Within the instruction, the prompt specifies what to do with the specifications: to take the provided specifications and generate a program that adheres to them. Additionally, it includes constraints such as avoiding loops in the generated program. Part (D) of the prompt is not included in the initialization prompt as we have no previously generated candidate programs in this step. Lastly, both prompt types include a _Call to Action_ (E) instructing the model to generate a formally verified C program that meets the provided specification. Using the initialization prompt, VeCoGen invokes the LLM to generate candidate programs. We check the correctness of each candidate in two steps. First, we compile the candidate. If the compilation is successful, VeCoGen employs the WP and RTE plugins of Frama-C to prove that the candidate meets the formal specification. If any candidate passes these checks, it is considered a correct program, and the code generation is successfully completed. Otherwise, the candidates are sent to the next step for an iterative improvement until a correct program is generated."
    },
    {
      "title": "_Step 2: Code Improvement_",
      "text": "In the code improvement step, incorrect candidates are iteratively repaired to synthesize a formally verified program. As seen in Figure 4, the improvement prompt has the same system message, one-shot example, and specification as the initialization prompt. In addition to the initialization prompt, the improvement prompt contains a previously generated candidate program along with associated compilation or verification feedback. This feedback provides the LLM with valuable information to enhance its previously generated attempt. The improvement prompt only contains one incorrect program. As multiple candidates are generated at each iteration, we have to pick one of the candidates to include in the next improvement prompt. Selecting a single candidate ensures that the LLM is not overwhelmed with conflicting information from multiple programs. VeCoGen randomly selects one of the programs that pass the highest percentage of unit test cases. We choose the most promising candidate to increase the likelihood of quickly converging to a verified program. The LLM then generates improved versions of this program. These revised programs are then compiled and verified. The iterative process of generating new programs based on previous incorrect candidates is repeated until a formally verified program is found or a maximum of ten iterations is reached."
    },
    {
      "title": "Vecogen _Output_",
      "text": "The output of VeCoGen consists of a formally verified C program that adheres to the provided formal specification or, if unsuccessful, the last generated candidate program. Additionally, VeCoGen produces a detailed log file for each run. If VeCoGen is successful, then the generated program is guaranteed to be both syntactically correct and semantically correct with respect to the formal specification."
    },
    {
      "title": "_Implementation_",
      "text": "VeCoGen is written in Python and uses GCC for compilation and the Frama-C plugins WP and RTE for verification. Solvers attempt to prove the goals generated by the two Frama-C plugins. VeCoGen has been tested with solvers Alt-Ergo, CVC4, and Z3. Frama-C was chosen for its strong verification capabilities, especially in safety-critical systems. Due to the absence of complex control structures like loops, Frama-C offers automatic verification with theorem provers. If more complexity is introduced, then Frama-C would require help to prove that a program adheres to a formal specification. VeCoGen is able to employ any LLM with an application programming interface (API). Currently, VeCoGen is configured to use the following LLMs: GPT-3.5-turbo, GPT-4o, Llama-3.1-70B. The default LLM in VeCoGen is GPT-3.5-turbo, selected for its balance of cost-effectiveness and suitable performance for generating C programs. VeCoGen generates ten programs per LLM invocation. We pick ten, as our experiments show that generating ten programs per request balances diversity in generated candidate programs and the number of duplicates between the generated candidates. To promote the diversity of the programs, VeCoGen uses a sampling _temperature_ of 1. The temperature controls the randomness of an LLM when generating output by adjusting the probability distribution of predicted tokens. The values for the temperature range from 0 to 1, where lower values make the output more deterministic, and higher values also introduce greater diversity by predicting less likely tokens. This encourages the LLM to explore varied solutions and minimizes duplicates. During the code improvement step, VeCoGen uses at most ten iterations. We use ten iterations to give the LLM multiple chances to refine incorrect programs based on feedback from the compiler and verifier."
    },
    {
      "title": "Iv Experimental Methodology",
      "text": "To assess the effectiveness of VeCoGen and the impact of specification types on its performance, we design an experimental methodology guided by research questions to evaluate its ability to generate formally verified C."
    },
    {
      "title": "_Research Questions_",
      "text": "We define three research questions to evaluate the effectiveness of VeCoGen in generating formally verified C programs: **RQ1 (effectiveness)**: How effective is VeCoGen in terms of generating formally verified C programs? As a metric, we count the number of specifications for which VeCoGen generates a formally verified program. Additionally, we investigate the number of problems solved after initial code generation, the number of solved problems after different numbers of code improvement iterations, and the total time taken. If a solution is generated, we also present metrics of the solution. **RQ2 (specification type impact)**: How does providing a natural language or formal specification impact the effectiveness of VeCoGen? We analyze the number of successfully generated verified programs based on different specification inputs. Specifically, we investigate using only natural language specification, only formal specification, and both specification types. **RQ3 (ablation study)**: What is the impact of changing parameters, prompts, and LLMs on the effectiveness of VeCoGen? Specifically, we study the impact of employing different search strategies in terms of number of candidates, temperature, iterations, the use one-shotting, and using different LLMs."
    },
    {
      "title": "_Study Subjects_",
      "text": "To evaluate the performance of VeCoGen, we create a dataset called VeCoSet. VeCoSet consists of formal specifications, natural language specifications, and test cases for 15 problems selected from Codeforces online competitions [30]. Code4Bench[31] bundles problems used in Codeforces, from which we select problems based on the following criteria: 1. The problem has a ground-truth accepted solution in the C language. This ensures that we have a reference solution. 2. The ground-truth solution to the problem is limited to a single function that does not use loops. 3. A formal verification expert must be able to manually specify the problem in ACSL within three hours of work to keep the study manageable, as explained below. Furthermore, we exclude problems involving loops due to two reasons: (1) Including problems that contain a solution with loops often requires additional loop invariants to formally verify the file. Writing these loop invariants typically requires manual effort and domain knowledge. Therefore, we focus our study on programs without loops to make our code generation approach automatic. (2) In safety-critical domains like automotive, loops are often restricted due to predictability and reliability concerns. Standards like MISRA-C[32] impose strict limits on loop usage to prevent issues such as unbounded execution and timing unpredictability. Excluding loops from the analysis aligns with these safety practices, supporting our goal of automatic code generation for such domains. From the resulting set of 77 problems, we randomly pick 15 to include in VeCoSet. Problems from Code4Bench include natural language descriptions and test cases, which we use directly in VeCoSet without modification. However, formal specifications are not included in Code4Bench. Therefore, we manually create the formal specifications and check their correctness and completeness using two methods. First, we validate that the ground-truth solution meets the formal specification. Second, another formal verification expert confirms the consistency and completeness of each specification. We verify that the full program behavior is captured in the formal specification to ensure that only programs that capture the intended behavior are formally verified. Solutions in Codeforces make use of the standard input and output. We transform the ground-truth solution into a single-function program to facilitate formal verification. This allows us to write formal specifications for each transformed function. Additionally, using functions is common practice in real-world projects. For example, Figure 5 presents a solution taken from Code4bench and its transformed functional version. In the manual transformation procedure, we perform five steps: (1) We create a function signature. This is a void Fig. 5: A solution to a Codeforces problem in C, before and after transforming it to a function. function with a relevant name. (2) We introduce a parameter in the function signature for each value read from the standard input. This allows the function to receive input values as arguments rather than through scanf statements. (3) We introduce a parameter in the function signature for each value printed to the standard output. For this, we utilize an output pointer to capture results. (4) We replace any printf and scanf statements and replace them with assignments to the designated input and output parameters. (5) We remove the include #include<stdio.h> statement, as standard I/O functions are no longer used in the transformed function. Table I summarizes features of the 15 problems in our dataset. It shows the size of the natural language specification, the size of the formal specification, the length of the ground-truth solution, and the number of test cases. The table presents the median value and the minimal and maximal values for each of these metrics. This information underscores the variety of problems present in VeCoSet. The natural language specification size is measured in tokens. For each natural language description, we use the GPT-3.5 tokenizer to count the number of tokens. For example, the natural language specification presented in Figure 4 consists of 141 tokens. The number of clauses in the formal specifications varies from 4 to 14 in VeCoSet, which shows the diversity in the level of detail needed to capture their requirements accurately. Problems in Codeforces are rated by difficulty to match user skill. The problems in VeCoSet contain problems classified as easy to medium."
    },
    {
      "title": "_Protocol For Rq1 (Effectiveness)_",
      "text": "To answer RQ1, we use VeCoGen to generate verified C programs for all 15 problems in VeCoSet. If VeCoGen generates a C program that meets the formal specification, then we consider the problem solved. We define the effectiveness of VeCoGen as the number and ratio of problems for which our tool solves the problem. This metric allows us to evaluate the effectiveness of VeCoGen to generate verified C programs. We consider the number of problems solved after initial code generation as a baseline. Then, we track the number of solved problems after each code improvement iteration, up to a maximum of ten iterations or until all problems are solved. Additionally, we count metrics of the solution, such as the number of lines of code and the verification time required. Lastly, we capture the total runtime for attempting to generate and verify candidate solutions. These metrics provide insights into the performance and potential integration of VeCoGen. For unsolved problems, we manually analyze the generated code to identify the cause of failure."
    },
    {
      "title": "_Protocol For Rq2 (Specification Type Impact)_",
      "text": "To answer RQ2, we evaluate the impact of different specification types on the effectiveness of VeCoGen in generating verified C programs. For each of the 15 problems in VeCoSet, we run VeCoGen using three different specification types as inputs: (1) only the natural language specification (2) only the formal specification, and (3) both natural language and formal specifications combined. When only natural language specifications are used to generate code programs, the generated programs are attempted to be verified with respect to the formal specification. If this fails, then only natural language feedback is given back to the LLM. This feedback states that the code does not verify and that the LLM must improve on the code. When running VeCoGen with each specification type, we record the number of problems solved after the initial code generation step and after iterative improvements. Additionally, we measure the total runtime used when generating programs using VeCoGen. Using these statistics, we evaluate the effectiveness and time efficiency using different specification types as input to VeCoGen."
    },
    {
      "title": "_Protocol For Rq3 (Ablation Study)_",
      "text": "To answer RQ3, we perform an ablation study to evaluate how different configurations, such as including a one-shot example and using different LLMs, affect the performance of VeCoGen. We vary the number of candidate programs generated per LLM invocation, the used temperature of the LLM when generating candidates, whether or not a one-shot example is included, and the specific LLM employed. The first (and default) configuration acts as a baseline, consisting of ten candidate programs per invocation, ten code improvement iterations, a temperature of 1, and a one-shot example. We create the second configuration to investigate the effect of taking the most promising candidate at each iteration. This configuration uses a temperature of 0 to get the best possible candidate at each iteration. This leads to little variety between generated program candidates, so we only generate one program candidate per invocation. The third configuration investigates the effect of generating many candidate programs without using code improvement iterations. It generates 100 candidate programs per invocation, a commonly used number in the literature [33, 5]. The fourth configuration explores providing a one-shot example, examining the impact of giving an example of the task and output format on the effectiveness of VeCoGen. Lastly, we define configurations to use VeCoGen with different LLMs. This assesses the impact of using open-source (Llama-3.1-70B) and closed-source (GPT-3.5-turbo, and GPT-4o) LLMs. This provides insights into how the choice of LLM influences the formally verified program generation process. For each configuration, we count the total number of problems solved by VeCoGen after completing the initial code generation and iterative improvement steps. By comparing the results of the defined configurations, we analyze the effect of changing information included in the prompt. Additionally, we examine the time each configuration needs to generate a verifying program to assess the impact on the performance and efficiency of VeCoGen."
    },
    {
      "title": "V Experimental Results",
      "text": ""
    },
    {
      "title": "_Results For Rq1_",
      "text": "Table II presents information about the problems and results3 using VeCoGen. Specifically, it presents generated solutions using the default configuration of VeCoGen. Column \"Problem_ID\" mentions the problem identifier, and column \"Problem_Name\" presents the name of the problem as presented in Codeforces. Column \"SS_Iter\" presents the number of code improvement iterations that VeCoGen uses to synthesize a verifying solution. If 0, then the problem is solved in the initial code generation step. If VeCoGen does not solve the problem, we indicate this using \"-\". For the solved problems, column \"SS_LOC\" shows the Lines of Code (LoC) of the synthesized solution. Column \"SS_Ver_Time\" presents the time needed to formally verify the synthesized solution. Lastly, column \"Tot_Time\" presents the total time spent to synthesize a solution to the problem. The total time includes the total runtime of VeCoGen, Frama-C, and the LLM combined. Footnote 3: The results to all experiments conducted can be found at [https://github.com/ASSERT-KTH/Vecogen/tree/vecogen-results/results](https://github.com/ASSERT-KTH/Vecogen/tree/vecogen-results/results). Column \"SS_Iter\" shows the tool solves 9 of the 15 problems in VeCoSet during initial code generation. After the iterative code improvement step, the number of solved problems increases to 13. Notably, three problems (9, 13, and 14) are solved after the first code improvement iteration, and problem six is solved after the third iteration. This shows the effectiveness of the iterative approach of VeCoGen, as many problems are solved during improvement attempts. When inspecting the LoC of the synthesized solution in the column \"SS_LOC\", we see that VeCoGen generates solutions of varying lengths. The shortest solution generated is one line long, while the longest synthesized solution contains 14 LoC. Similarly, VeCoGen requires a varying time to verify the synthesized solution, ranging from 0.8 seconds to 15.1 seconds. This highlights the strength of VeCoGen, being applicable for generating a range of different C programs based on formal and natural language specifications. Column \"Tot_Time\" highlights the total time required to generate solutions, which varies significantly across problems. For solved problems, the fastest total runtime is 5 seconds for problem 5, while the longest is 388 seconds for problem 6. This shows that the tool is fast enough to be integrated into a common development process. Both unsolved problems (12 and 15) have a significantly higher total time spent. When VeCoGen fails to generate a verified program, the tool iterates ten times and generates ten candidates for each iteration. This totals to over 100 generated candidates. As the WP and RTE plugins of Frama-C attempt to verify each of the generated program candidates, the tool takes a long time to handle unsolved problems. For problem 12, as shown in Figure 6, the generated candidate does not meet the formal specification, even after code improvement steps. As per our manual analysis, the LLM repeatedly includes loops in the synthesized solutions, while the employed prompt mentions that loops are not allowed. The second unsolved problem (problem 15) is solved by employing GPT-4o. This shows that VeCoGen is able to take advantage of the power of more advanced LLMs to generate more complex verifying programs; see Section V-C for more details. **Answer to RQ1: How effective is VeCoGen in terms of generating formally verified C programs?** VeCoGen proves effective, solving 13 out of 15 problems in VeCoSet. During the initial code generation step, nine problems are solved, which are improved to 13 throughout the feedback iterations. This demonstrates the capability of VeCoGen to refine solutions through feedback. Overall, in this experiment, VeCoGen showcases its potential to automate the synthesis of verified C programs."
    },
    {
      "title": "_Results For Rq2_",
      "text": "Next, we investigate how different types of input specifications in the prompt affect the ability of VeCoGen to generate formally verified C programs. Table III presents the results of using a natural language specification, a formal specification, and both specification types when running VeCoGen. The column \"Specification_Type\" presents the specification type used when prompting the LLM. Columns \"Initial\" and \"Improvement\" present the number of problems solved after the initial code generation and improvement steps, respectively. Lastly, column \"Tot_Time\" presents the total time taken by VeCoGen to solve all problems with the given specification type. In the initial code generation step, the natural language prompt solves most problems (10 out of 15), followed by Fig. 6: The ground-truth solution and final generated program candidate for problem 12, which does not verify. Given a budget \\(k\\) and \\(n\\) exams, the code minimizes the minimum number of failed exams. See Codeforces for details. using both specification types (9 out of 15). This shows that using only natural language performs the best when prompting the LLM without iterative feedback. After the code improvement iterations, using natural language and both types of specifications in the prompt solves 13 problems. This entails that after using the code improvement iterations, including a formal specification or not does not influence the number of solved problems in VeCoSet. Only using a formal specification performs the worst in both initial code generation and code improvement. Removing the natural language description from the prompt significantly reduces the effectiveness of VeCoGen. This suggests that natural language specifications are crucial for code generation as LLMs are primarily trained on such data. While formal specifications alone perform weaker due to limited context, their combination with natural language improves problem-solving efficiency and resource usage, justifying the prompting strategy used by VeCoGen. When investigating column \"Tot_Time\", we see that the configuration using both natural language and formal specifications is the most time-efficient at 3,722 seconds, followed by natural language at 5,447 seconds. This shows that including both specification types reduces the time needed to run VeCoGen. The reason is that when using natural language only, VeCoGen spends more time on unsolved problems. For example, this specification type spends 2,892 seconds attempting to solve problem 4. Using only formal specifications takes significantly longer, with a total time of 10,538 seconds. This is because using only formal specifications in the prompt cannot solve seven problems. As we discussed in Section V-A, if VeCoGen cannot solve a problem, a lot of time is spent verifying each of the generated candidate solutions, leading to a high time spent per unsolved problem. VeCoGen demonstrates an effective design by leveraging both specification types, solving the most number of problems in the least amount of time. **Answer to RQ2: How does providing a natural language or formal specification impact the effectiveness of VeCoGen?** Experiments show that VeCoGen performs best when combining natural language and formal specifications, solving most problems in the least time. Natural language specification is important as LLMs are primarily trained on such data. Formal specifications alone yield weaker performance, but combining both specification types improves efficiency, enabling VeCoGen to balance problem-solving effectiveness with resource usage."
    },
    {
      "title": "_Results For Rq3_",
      "text": "Table IV presents the various configurations in our ablation study that are used to assess the impact of different parameters on the performance of VeCoGen. Each configuration is identified by a unique identifier presented in column \"Configuration_ID\". The \"Candidates\" column specifies the number of candidate programs generated per LLM invocation, and the \"Temperature\" column denotes the used temperature, which controls the randomness in program generation. The \"Iterations\" column lists the maximum number of code improvement iterations, and the \"Prompting Method\" column indicates whether an example of the task is included. Lastly, column \"LLM\" specifies the LLM used. The remaining columns present the results. The \"Solved\" column shows the final number of solved problems after running VeCoGen. The \"Tot_Time\" column captures the total time spent (in seconds) for code generation and verification per configuration. For example, the default configuration in row A, as presented in Section V-A, generates ten candidate programs per LLM-invocation using a temperature of 1. This configuration uses a maximum of ten code improvement iterations and employs a one-shot learning approach. The default configuration solves 13 problems in 3,722 seconds. Using configuration B, we evaluate the effect of a lower temperature. Configuration B, with a temperature of 0, solves seven problems after iterations, significantly fewer than configuration A. This demonstrates the benefit of a higher temperature, as it allows VeCoGen to explore a broader range of program candidates, increasing its effectiveness in generating verifying programs. Configuration B completes in 2,062 seconds, compared to 3,722 seconds for configuration A. This is due to configuration B verifying at most 11 program candidates per problem compared to over 100 in configuration A. This highlights that although configuration A uses more time, the increase in problems solved makes using a higher temperature an effective choice for VeCoGen. The effect of removing the one-shot example is analyzed in configuration D. Both configurations A and D solve 13 problems after all iterations. However, configuration A has a lower runtime (3,722 vs 5,403 seconds). Including a one-shot example uses less time when solving problems, helping towards integrating VeCoGen in a development process. Configurations E and F explore the impact of using different LLMs. Configuration E, which employs GPT-4o, outperforms GPT-3.5-turbo in both effectiveness and time efficiency, solving 14 problems in 830 seconds. Similarly, configuration F uses Llama-3.1-70B, solving all 15 problems with a runtime of 3,782 seconds. These results highlight that VeCoGen performs well with both open and closed-source LLMs. This makes VeCoGen suitable for diverse deployment scenarios. **Answer to RQ3: What is the impact of changing parameters, prompts, and LLMs on the effectiveness of VeCoGen?** The findings indicate that VeCoGen performs best using a temperature of 1 and generating multiple candidates. Additionally, we find that the iterative approach of VeCoGen is better than generating more program candidates in the initial code generation step. Moreover, VeCoGen proves effective with both open- and closed-source LLMs, solving 14 problems with GPT-4o and all 15 with Llama-3.1-70B, showcasing its versatility with advanced LLMs."
    },
    {
      "title": "Vi Threats To Validity",
      "text": ""
    },
    {
      "title": "_Construct Validity_",
      "text": "A threat to the construct validity of our study arises from data leakage and the non-deterministic behavior of LLMs. Since the problems used in this study are derived from publicly available sources such as Codeforces, it is plausible that the LLMs may have encountered similar problems or solutions during training. This could lead to an overestimation of their problem-solving capabilities, as they might recall or adapt existing solutions rather than independently generate them. Despite this threat, the data leakage issue in our study is limited as our manually crafted formal specifications were not publicly available before this study. This means the LLM has not seen these formal specifications in its training dataset."
    },
    {
      "title": "_Internal Validity_",
      "text": "The verification process in VeCoGen depends on the WP and RTE plugins of Frama-C. We run these plugins using the solvers Alt-Ergo, CVC4, and Z3 to leverage and combine their strengths. However, these solvers operate under specific timeouts and step limits. If a solver fails to verify a goal within these constraints, the solution may still be correct, but the solvers cannot confirm it. To mitigate this, we ensure that the ground-truth solution meets the formal specification and thus verifies. This proves that there is at least one solution that can be generated by the LLM and formally verified by Frama-C."
    },
    {
      "title": "_External Validity_",
      "text": "The limited dataset of 15 problems constrains the generalizability of our findings. Additionally, the problems exclude loops from the analysis. This constraints the applicability of VeCoGen to more general and complex programming problems. Furthermore, our evaluation is restricted to single-function programs. In the future, expanding the scope to multi-function programs and incorporating more complex data structures would better reflect real-world software development. The limited dataset of 15 problems constrains the generalizability of our findings. Moreover, the problems exclude loops from the analysis and are not based on safety-critical code, which constrains the applicability of VeCoGen to more general and complex programming problems."
    },
    {
      "title": "Vii Related Work",
      "text": "VeCoGen is the first LLMs-based tool to automatically generate formally verified C code using an iterative approach. However, other works have explored combining LLMs with formal methods or employing iterative improvement techniques, which we review in this section."
    },
    {
      "title": "_Formal Specifications-Based Code Generation With Llms_",
      "text": "The closest work to VeCoGen is SynVer[17], a recently published framework for synthesizing and formally verifying C programs. Similar to VeCoGen, SynVer invokes an LLM using a natural language specification and a formal specification. The strength of Synver lies in its ability to synthesize complex programs, handling, for example, recursion, which VeCoGen does not support. SynVer employs a human-in-the-loop approach to support these complex programs, which requires manual intervention to refine and verify programs. In contrast, VeCoGen prioritizes full automation, iteratively refining candidate programs without human involvement. Additionally, VeCoGen generates multiple candidate programs and uses test cases to rank the most promising candidate. While both tools share the goal of leveraging LLMs for program synthesis and verification, they target different levels of automation and complexity. Patil et al. [18] propose an iterative LLM-based framework named spec2code that generates C code from specifications. They do not present any tool and, therefore, manually conduct studies that show promising results. Unlike spec2code, we present a tool that automates verified code generation using LLMs. Another work investigating the use of formal specifications to generate code is SpecEval[34], which analyzes how well LLMs understands these specifications. SpecEval, similar to VeCoGen, generates programs using LLMs. However, our tool iteratively improves on previously generated program candidates to correct past mistakes. Ahrendt et al. [35] propose a framework named _TriCo_ to help users to create code, tests, and specifications simultaneously. Similarly, Sun et al. introduce _Clover_[36], which combines LLMs and formal methods to check consistency between formal specifications, docstring, and code. As these works require formal specifications, several works have investigated automatically generating these based on code [37, 38] and natural language [39, 40, 41]."
    },
    {
      "title": "_Traditional Specification-Based Code Generation_",
      "text": "Formal synthesis is a longstanding problem in software engineering aiming to generate programs based on formal specifications [42, 43, 44]. Traditional techniques employ deductive synthesis, where programs are derived directly from formal specifications [45]. While this method guarantees correctness by construction, it suffers from poor scalability due to the computational complexity of large and complex programs. To address this, Solar-Lezama [16] introduces sketching, a technique where developers provide partial implementations to guide the synthesis process, reducing synthesis time. Building on this, Alur et al. [46] propose Syntax-Guided Synthesis (SyGuS), which combines syntactic constraints with semantic correctness to improve program generation. Over the years, various approaches have been developed to enhance the automatic code generation process, including inductive learning [14], oracle-guided synthesis [42], and proof-theoretic synthesis [15]. Tools like Fiat [47] refine declarative specifications into functional programs that are correct by construction. Similarly, Li et al. [48] demonstrate the synthesis of verified code from timed automata models, ensuring behavioral correctness while bridging formal models and real-world implementations. Murphy et al. combine these traditional techniques with LLMs to generate candidates [49]. Contrary to traditional specification-based code generation techniques, VeCoGen is the first iterative tool to integrate LLMs for fully automated generation of formally verified C programs. The key difference with traditional methods is that VeCoGen uses LLMs to generate the programs and feedback from formal methods to improve faulty generated programs."
    },
    {
      "title": "_Iterative Code Improvement With Large Language Models_",
      "text": "Many works employ iterative code improvement as a method for enhancing LLM-generated programs, leveraging techniques such as automatic program repair [50, 51, 52], counterexample-guided synthesis [46], and feedback from compilers, verification tools, or human reviewers [53, 54, 55]. For example, Jha et al. [56] explore providing counterexamples as iterative feedback to mitigate hallucinations of LLMs. These counterexamples can be derived from formal verification tools [57] or from failed test cases [6], allowing the LLM to improve candidate programs based on counter-examples. Fan et al. highlight that LLM-generated code frequently suffers from syntax errors, incomplete logic, or incorrect solutions, requiring code improvement through feedback [58]. The prompt to the LLM can include previous failed attempts to prevent the LLM from making the same mistake [55]. Liventsev et al. propose SEIDR [59], which iteratively improves program candidates using GPT-assisted summarizations of bugs and failing test cases. A balance between iteratively improving and generating new candidates results in the most improved programs. Tang et al. [60] use Thompson Sampling [61] to pick what candidate program to repair. Unlike existing iterative code improvement frameworks, VeCoGen uniquely integrates formal verifier feedback to iteratively improve on program candidates. In these previous works, feedback for LLMs relies on using counterexamples, whereas VeCoGen employs information about unproven goals by Frama-C for this purpose."
    },
    {
      "title": "Viii Conclusion",
      "text": "This paper introduced a novel LLM-based tool VeCoGen, used for generating formally verified C code. It addresses an initial investigation into automatically generating programs in safety-critical domains. VeCoGen employs a two-step process to generate the programs: (1) it leverages LLMs to generate program candidates based on formal and natural language specifications (2) it iteratively improves previously generated program candidates through compiler and verifier feedback. Each program candidate is formally verified against the provided formal specification, ensuring that only solutions meeting the specification are accepted. Experiments using VeCoGen on 15 competitive programming problems demonstrate its effectiveness, solving 13 of them. These results demonstrate the feasibility of VeCoGen in automating the generation of formally verified C code. As a result, VeCoGen marks a significant advancement in integrating LLMs with formal verification, addressing the rigorous correctness requirements of safety-critical software development. **Acknowledgments** This work has been partially funded by the Advanced Digitalisation Programme of Sweden's Innovation Agency (VINNOVA) as the FormAI project 2023-02671."
    },
    {
      "title": "References",
      "text": "* [1]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighien, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020) Language Models are Few-Shot Learmers. Technical report arXiv:2005.14165 [cs]. External Links: Link Cited by: SSII. [MISSING_PAGE_POST] * [32] MIRA Ltd, _MIRA-C-2004 Guidelines for the use of the C language in Critical Systems_, MIRA Std., Oct. 2004, [https://misra.org.uk/misra-c](https://misra.org.uk/misra-c). * [33] K. Stechly, M. Marquez, and S. Kambhampati, \"GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems,\" Oct. 2023, arXiv:2310.12397 [cs. [Online]. Available: [http://arxiv.org/abs/2310.12397](http://arxiv.org/abs/2310.12397) * [34] L. Ma, S. Liu, L. Bu, S. Li, Y. Wang, and Y. Liu, \"SpecEval: Evaluating Code Comprehension in Large Language Models via Program Specifications,\" Sep. 2024, arXiv:2409.12866 [cs. [Online]. Available: [http://arxiv.org/abs/2409.12866](http://arxiv.org/abs/2409.12866) * [35] W. Ahrendt, D. Gurov, M. Johansson, and P. Rimmer, \"TriCo-Triple Co-piloting of Implementation, Specification and Tests,\" in _Leveraging Applications of Formal Methods, Verification and Validation, Verification Principles_, T. Margaria and B. Steffen, Eds. Cham: Springer International Publishing, 2022, pp. 174-187. [Online]. Available: [https://link.springer.com/chapter/10.1007/978-3-01984-6_11](https://link.springer.com/chapter/10.1007/978-3-01984-6_11) * [36] C. Sun, Y. Sheng, O. Padon, and C. Barrett, \"Clover: Closed-Loop Verifiable Code Generation,\" Jan. 2024, arXiv:2310.17807 [cs]. [Online]. Available: [http://arxiv.org/abs/2310.17807](http://arxiv.org/abs/2310.17807) * [37] G. Granberry, W. Ahrendt, and M. Johansson, \"Specify What? Enhancing Neural Specification Synthesis by Symbolic Methods,\" Sep. 2024, arXiv:2406.15540 [cs. [Online]. Available: [http://arxiv.org/abs/2406.15540](http://arxiv.org/abs/2406.15540) * [38] C. Wen, J. Cao, J. Su, Z. Xu, S. Qin, M. He, H. Li, S.-C. Cheung, and C. Tian, \"Enchanting Program Specification Synthesis by Large Language Models Using Static Analysis and Program Verification,\" in _Computer Aided Verification_, A. Gurfinkel and V. Ganesh, Eds. Cham: Springer Nature Switzerland, 2024, pp. 302-328. * [39] M. Cosler, C. Hahn, D. Mendoza, F. Schmitt, and C. Trippel, \"nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models,\" in _Computer Aided Verification_, C. Enea and A. Lal, Eds. Cham: Springer Nature Switzerland, 2023, pp. 383-396. * [40] J. Zhai, Y. Shi, M. Pan, G. Zhou, Y. Liu, C. Fang, S. Ma, L. Tan, and X. Zhang, \"C2S: translating natural language comments to formal program specifications,\" in _Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering_, ser. ESEC/FSE 2020. New York, NY, USA: Association for Computing Machinery, Nov. 2020, pp. 25-37. [Online]. Available: [https://dl.acm.org/doi/10.1145/3368089.3409716](https://dl.acm.org/doi/10.1145/3368089.3409716) * [41] D. Giannakopoulou, T. Pressburger, A. Mavridou, and J. Schumann, \"Generation of Formal Requirements from Structured Natural Language,\" in _Requirements Engineering: Foundation for Software Quality_, N. Madhavji, L. Pasquale, A. Ferrari, and S. Gnesi, Eds. Cham: Springer International Publishing, 2020, pp. 19-35. * Volume 1_. Cape Town South Africa: ACM, May 2010, pp. 215-224. [Online]. Available: [https://dl.acm.org/doi/10.1145/1806799.1806833](https://dl.acm.org/doi/10.1145/1806799.1806833) * [43] P.-A. Hsiung, \"Formal synthesis and code generation of embedded real-time software,\" in _Proceedings of the ninth international symposium on Hardware/software codesign_, ser. CODES '01. New York, NY, USA: Association for Computing Machinery, Apr. 2001, pp. 208-213. [Online]. Available: [https://dl.acm.org/doi/10.1145/371636.371729](https://dl.acm.org/doi/10.1145/371636.371729) * [44] R. Jullig, \"Applying formal software synthesis,\" _IEEE Software_, vol. 10, no. 3, pp. 11-22, May 1993, conference Name: IEEE Software. [Online]. Available: [https://ieeexplore.ieee.org/abstract/document/210596](https://ieeexplore.ieee.org/abstract/document/210596) * [45] Z. Manna and R. Wadinger, \"A Deductive Approach to Program Synthesis,\" _ACM Transactions on Programming Languages and Systems_, vol. 2, no. 1, pp. 90-121, Jan. 1980. [Online]. Available: [https://dl.acm.org/doi/10.1145/357084.357090](https://dl.acm.org/doi/10.1145/357084.357090) * [46] R. Alur, R. Bodik, G. Junvali, M. M. K. Martin, M. Raghothaman, S. A. Seshia, R. Singh, A. Solar-Lezama, E. Torlak, and A. Udupa, \"Syntax-guided synthesis,\" in _2013 Formal Methods in Computer-Aided Design_, Oct. 2013, pp. 1-8. [Online]. Available: [https://ieeexplore.ieee.org/abstract/document/6679385](https://ieeexplore.ieee.org/abstract/document/6679385) * [47] B. Delaware, C. Pit-Claudel, J. Gross, and A. Chlipala, \"Fiat: Deductive Synthesis of Abstract Data Types in a Proof Assistant,\" _ACM SIGPLAN Notices_, vol. 50, no. 1, pp. 689-700, May 2015. [Online]. Available: [https://dl.acm.org/doi/10.1145/2775051.267006](https://dl.acm.org/doi/10.1145/2775051.267006) * [48] X. Li, R. Wang, Y. Jiang, Y. Guan, X. Li, and X. Song, \"Formal Modeling and Automatic Code Synthesis for Robot System,\" in _2017 22nd International Conference on Engineering of Complex Computer Systems (ICECCS)_, Nov. 2017, pp. 146-149. [Online]. Available: [https://ieeexplore.ieee.org/abstract/document/8292813](https://ieeexplore.ieee.org/abstract/document/8292813) * [49] W. Murphy, N. Holzer, F. Qiao, L. Cui, R. Rothkopf, N. Koenig, and S. Santolucito, \"Combining LLVM Code Generation with Formal Specifications and Reactive Program Synthesis,\" Sep. 2024, arXiv:2410.19736. [Online]. Available: [http://arxiv.org/abs/2410.19736](http://arxiv.org/abs/2410.19736) * [50] M. Monperrus, \"Automatic Software Repair: A Bibliography,\" _ACM Computing Surveys_, vol. 51, no. 1, pp. 17:1-17:24, Jan. 2018. [Online]. Available: [https://dl.acm.org/doi/10.1145/3105906](https://dl.acm.org/doi/10.1145/3105906) * [51] D. Hidvigi, K. Etemadi, S. Bobadilla, and M. Monperrus, \"CigaR: Cost-efficient Program Repair with LLMs,\" Apr. 2024, arXiv:2402.06598 [cs]. [Online]. Available: [http://arxiv.org/abs/2402.06598](http://arxiv.org/abs/2402.06598) * [52] A. Nilazadeh, G. T. Leavens, X.-B. Le, C. S. Parenu, and D. R. Cok, \"Exploring True Test Overfitting in Dynamic Automated Program Repair using Formal Methods,\" in _2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)_, Apr. 2021, pp. 229-240, iSSN: 2159-4848. [Online]. Available: [https://ieeexplore.ieee.org/abstract/document/9438573](https://ieeexplore.ieee.org/abstract/document/9438573) * [53] Y. Charalambous, N. Thianyi, R. Jain, Y. Sun, M. A. Ferrag, and L. C. Cordeiro, \"A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification,\" May 2023, arXiv:2305.14752 [cs. [Online]. Available: [http://arxiv.org/abs/2305.14752](http://arxiv.org/abs/2305.14752) * [54] M. Fakhi, R. Dharmaji, Y. Moghaddas, G. Q. Araya, O. Ogundare, and M. A. Faruque, \"LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems,\" Jan. 2024, arXiv:2401.05443 [cs. [Online]. Available: [http://arxiv.org/abs/2401.05443](http://arxiv.org/abs/2401.05443) * [55] C. S. Xia and L. Zhang, \"Keep the Conversation Going: Fixing 162 out of 337 bugs for 50.42 each using ChatGPT,\" Apr. 2023, arXiv:2304.00385 [cs. [Online]. Available: [http://arxiv.org/abs/2304.00385](http://arxiv.org/abs/2304.00385) * [56] S. Jha, S. K. Jha, P. Lincoln, N. D. Bastian, A. Velasquez, and S. Neema, \"Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting,\" in _2023 IEEE International Conference on Assured Autocomv (ICAA)_, Jun. 2023, pp. 149-152. [Online]. Available: [https://ieeexplore.ieee.org/abstract/document/10207581](https://ieeexplore.ieee.org/abstract/document/10207581) * [57] G. Pace, N. Halbwachs, and P. Raymond, \"Counter-example generation in symbolic abstract model-checking,\" _International Journal on Software Tools for Technology Transfer_, vol. 5, no. 2, pp. 158-164, Mar. 2004. [Online]. Available: [https://doi.org/10.1007/s10009-003-0127-4](https://doi.org/10.1007/s10009-003-0127-4) * [58] Z. Fan, X. Gao, M. Mirchev, A. Roychoudhury, and S. H. Tan, \"Automated Repair of Programs from Large Language Models,\" in _2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)_, May 2023, pp. 1469-1481, iSSN: 1558-1225. [Online]. Available: [https://ieeexplore.ieee.org/document/10172854](https://ieeexplore.ieee.org/document/10172854) * [59] V. Liventsev, A. Grishina, A. Hrm, and L. Moenen, \"Fully Autonomous Programming with Large Language Models,\" in _Proceedings of the Genetic and Evolutionary Computation Conference_, ser. GECCO '23. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 1146-1155. [Online]. Available: [https://dl.acm.org/doi/10.1145/3583131.3590481](https://dl.acm.org/doi/10.1145/3583131.3590481) * [60] H. Tang, K. Hu, J. P. Zhou, S. Zhong, W.-L. Zheng, X. Si, and K. Ellis, \"Code Repair with LLMs gives an Exploration-Exploitation Tradeoff,\" May 2024, arXiv:2405.17503 [cs]. [Online]. Available: [http://arxiv.org/abs/2405.17503](http://arxiv.org/abs/2405.17503) * [61] W. R. Thompson, \"On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples,\" _Biometrika_, vol. 25, no. 3/4, pp. 285-294, 1933, publisher: [Oxford University Press, Biometrika Trust]. [Online]. Available: [https://www.jstor.org/stable/2332286](https://www.jstor.org/stable/2332286)"
    }
  ]
}