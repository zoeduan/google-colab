{
  "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
  "authors": [
    "Ziqiu Wang",
    "Jun Liu",
    "Shengkai Zhang",
    "Yang Yang"
  ],
  "abstract": "\n With the development of Natural Language Processing (NLP), Large Language Models (LLMs) are becoming increasingly popular. LLMs are integrating more into everyday life, raising public concerns about their security vulnerabilities. Consequently, the security of large language models is becoming critically important. Currently, the techniques for attacking and defending against LLMs are continuously evolving. One significant method type of attack is the jailbreak attack, which designed to evade model safety mechanisms and induce the generation of inappropriate content. Existing jailbreak attacks primarily rely on crafting inducement prompts for direct jailbreaks, which are less effective against large models with robust filtering and high comprehension abilities. Given the increasing demand for real-time capabilities in large language models, real-time updates and iterations of new knowledge have become essential. Retrieval-Augmented Generation (RAG), an advanced technique to compensate for the model's lack of new knowledge, is gradually becoming mainstream. As RAG enables the model to utilize external knowledge bases, it provides a new avenue for jailbreak attacks. In this paper, we conduct the first work to propose the concept of indirect jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on this, we further design a novel method of indirect jailbreak attack, termed Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to interact with large language models, thereby causing the large models to generate malicious non-compliant dialogues.We tested this method on six different large language models across three major categories of jailbreak issues. The experiments demonstrate that PLC successfully implemented indirect jailbreak attacks under three * Indicates corresponding author. different scenarios, achieving success rates of 88.56%, 79.04%, and 82.69% respectively. Experimental results and other resources:  https://github.com/CAM-FSS/jailbreak-langchain . \n",
  "references": [
    {
      "id": null,
      "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
      "authors": [
        "Ziqiu Wang",
        "Jun Liu",
        "Shengkai Zhang",
        "Yang Yang"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Qwen technical report",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "LangChain LLM App Development Framework. Retrieved July 10",
      "authors": [
        "Chase"
      ],
      "year": "2023",
      "venue": "LangChain LLM App Development Framework. Retrieved July 10",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "",
      "authors": [
        "Chatchat-Space"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Comprehensive assessment of jailbreak attacks against llms",
      "authors": [
        "Junjie Chu",
        "Yugeng Liu",
        "Ziqing Yang",
        "Xinyue Shen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "year": "2024",
      "venue": "Comprehensive assessment of jailbreak attacks against llms",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
      "authors": [
        "Gelei Deng",
        "Yi Liu",
        "Yuekang Li",
        "Kailong Wang",
        "Ying Zhang",
        "Zefeng Li",
        "Haoyu Wang",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "year": "2023",
      "venue": "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "MASTERKEY: Automated jailbreaking of large language model chatbots",
      "authors": [
        "Gelei Deng",
        "Yi Liu",
        "Yuekang Li",
        "Kailong Wang",
        "Ying Zhang",
        "Zefeng Li",
        "Haoyu Wang",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "year": "2024",
      "venue": "Proc. ISOC NDSS",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
      "authors": [
        "Kai Greshake",
        "Sahar Abdelnabi",
        "Shailesh Mishra",
        "Christoph Endres",
        "Thorsten Holz",
        "Mario Fritz"
      ],
      "year": "2023",
      "venue": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Catastrophic jailbreak of open-source llms via exploiting generation",
      "authors": [
        "Yangsibo Huang",
        "Samyak Gupta",
        "Mengzhou Xia",
        "Kai Li",
        "Danqi Chen"
      ],
      "year": "2023",
      "venue": "Catastrophic jailbreak of open-source llms via exploiting generation",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
      "authors": [
        "Daniel Kang",
        "Xuechen Li",
        "Ion Stoica",
        "Carlos Guestrin",
        "Matei Zaharia",
        "Tatsunori Hashimoto"
      ],
      "year": "2023",
      "venue": "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Mycrunchgpt: A llm assisted framework for scientific machine learning",
      "authors": [
        "Varun Kumar",
        "Leonard Gleyzer",
        "Adar Kahana",
        "Khemraj Shukla",
        "George Em Karniadakis"
      ],
      "year": "2023",
      "venue": "Journal of Machine Learning for Modeling and Computing",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "authors": [
        "Patrick Lewis",
        "Ethan Perez",
        "Aleksandra Piktus",
        "Fabio Petroni",
        "Vladimir Karpukhin",
        "Naman Goyal",
        "Heinrich Küttler",
        "Mike Lewis",
        "Wen-Tau Yih",
        "Tim Rocktäschel"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Multi-step jailbreaking privacy attacks on chatgpt",
      "authors": [
        "Haoran Li",
        "Dadi Guo",
        "Wei Fan",
        "Mingshi Xu",
        "Jie Huang",
        "Fanpu Meng",
        "Yangqiu Song"
      ],
      "year": "2023",
      "venue": "Multi-step jailbreaking privacy attacks on chatgpt",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Table-to-text generation by structure-aware seq2seq learning",
      "authors": [
        "Tianyu Liu",
        "Kexiang Wang",
        "Lei Sha",
        "Baobao Chang",
        "Zhifang Sui"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
      "authors": [
        "Xiaogeng Liu",
        "Nan Xu",
        "Muhao Chen",
        "Chaowei Xiao"
      ],
      "year": "2023",
      "venue": "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "SPROUT: Authoring Programming Tutorials with Interactive Visualization of Large Language Model Generation Process",
      "authors": [
        "Yihan Liu",
        "Zhen Wen",
        "Luoxuan Weng",
        "Ollie Woodman",
        "Yi Yang",
        "Wei Chen"
      ],
      "year": "2023",
      "venue": "SPROUT: Authoring Programming Tutorials with Interactive Visualization of Large Language Model Generation Process",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Tree of attacks: Jailbreaking black-box llms automatically",
      "authors": [
        "Anay Mehrotra",
        "Manolis Zampetakis",
        "Paul Kassianik",
        "Blaine Nelson",
        "Hyrum Anderson",
        "Yaron Singer",
        "Amin Karbasi"
      ],
      "year": "2023",
      "venue": "Tree of attacks: Jailbreaking black-box llms automatically",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Introducing Llama 2",
      "authors": [],
      "year": "2023",
      "venue": "Meta",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Openai usage policies",
      "authors": [
        "Openai"
      ],
      "year": "2024",
      "venue": "Openai usage policies",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Characterizing and evaluating inthe-wild jailbreak prompts on large language models",
      "authors": [
        "Xinyue Shen",
        "Zeyuan Chen",
        "Michael Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "year": "2023",
      "venue": "Characterizing and evaluating inthe-wild jailbreak prompts on large language models",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering",
      "authors": [
        "Shamane Siriwardhana",
        "Rivindu Weerasekera",
        "Elliott Wen",
        "Tharindu Kaluarachchi"
      ],
      "year": "2023",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
      "authors": [
        "Yu Sun",
        "Shuohuan Wang",
        "Shikun Feng",
        "Siyu Ding",
        "Chao Pang",
        "Junyuan Shang",
        "Jiaxiang Liu",
        "Xuyi Chen",
        "Yanbin Zhao",
        "Yuxiang Lu"
      ],
      "year": "2021",
      "venue": "Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Introduction to Bayesian Statistics",
      "authors": [
        "Harry Thornburg"
      ],
      "year": "2023",
      "venue": "Introduction to Bayesian Statistics",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Jailbroken: How does llm safety training fail?",
      "authors": [
        "Alexander Wei",
        "Nika Haghtalab",
        "Jacob Steinhardt"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis",
      "authors": [
        "Luoxuan Weng",
        "Xingbo Wang",
        "Junyu Lu",
        "Yingchaojie Feng",
        "Yihan Liu",
        "Wei Chen"
      ],
      "year": "2024",
      "venue": "InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "",
      "authors": [
        "Xinghuo"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "authors": [
        "Aohan Zeng",
        "Xiao Liu",
        "Zhengxiao Du",
        "Zihan Wang",
        "Hanyu Lai",
        "Ming Ding",
        "Zhuoyi Yang",
        "Yifan Xu",
        "Wendi Zheng",
        "Xiao Xia"
      ],
      "year": "2022",
      "venue": "Glm-130b: An open bilingual pre-trained model",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "2. Related Work",
      "text": ""
    },
    {
      "title": "Llm Jailbreak Attacks",
      "text": "With the advancement of large language models (LLMs), jailbreaking attacks have emerged as a distinct field within LLM security research. Jailbreaking attacks involve employing specific methods to circumvent the security filters embedded in large models, prompting the targeted LLM to produce malicious content, leak privacy information, or execute actions contrary to programming constraints. Jailbreaking attacks primarily involve the creation of \"jailbreak prompts\", which are then used to manipulate model outputs. For instance, Li et al. (Li et al., 2017) utilized these prompts to extract personal information embedded in the training data of a model. Similarly, Greshake et al. (Greshake et al., 2018) crafted jailbreak prompts that led LLM to produce manipulated outputs, enabling the model to generate incorrect responses based on error prompt information. \\begin{table} \\begin{tabular}{c|c c} \\hline \\hline **Model Name** & **Organization** & **Access** \\\\ \\hline ChatGlm2-6B & Zhipu & weight \\\\ \\hline ChatGlm3-6B & Zhipu & weight \\\\ \\hline Xinghuo-3.5 & fflytek & API \\\\ \\hline Qwen-14B-Chat & Alibaba & API \\\\ \\hline Ernie-3.5 & Baidu & API \\\\ \\hline llama2-7B & Meta & weight \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1. Model information. Figure 1. Overall diagram of Poisoned LangChain. As this field develops, an increasing variety of jailbreaking strategies (Hung et al., 2017) are being documented, with methods for crafting these prompts ranging from real-life observations (Krizhevsky et al., 2017), manual creation (Krizhevsky et al., 2017), to automated generation via adversarial networks (Krizhevsky et al., 2017; Krizhevsky et al., 2017). Additionally, Huang et al. (Huang et al., 2017) discovered that adjusting hyperparameters could render the security filters of a large model with specific configurations ineffective."
    },
    {
      "title": "Retrieval-Augmented Generation (Rag)",
      "text": "RAG was first proposed by Lewis et al. (Lewis et al., 2020) in 2020, combining a pre-trained retriever with a pre-trained seq2seq model (Lewis et al., 2020) and undergoing end-to-end fine-tuning to achieve more modular and interpretable ways of acquiring knowledge. This approach allows the model to access external knowledge sources when generating answers, thus providing more accurate and informative responses. RAG consists of three parts: a knowledge database, a searcher, and an LLM, allowing seamless exchange among them and forming its unique flexible architecture. In the first stage, the user's query retrieves relevant contextual information from external knowledge sources. The second phase involves placing the user query and the additional retrieved context into a prompt template, thereby providing an enhanced prompt to the LLM. In the final step, the enhanced prompts are fed into a large language model (LLM) for generation, which effectively improves the speed of knowledge updates and alleviates the hallucination problem in large models. LangChain is by far the most popular tool for RAG, providing a framework with specialized components designed to facilitate the integration of retrieval systems with language models. By using LangChain, it is possible to access and utilize vast amounts of real-time information, thereby expanding its functionality and applicability across various fields."
    },
    {
      "title": "3. Method",
      "text": "In this chapter, we describe the construction and implementation of poisoned LangChain. The jailbreak process of Poisoned LangChain consists of three main steps: langchain construction, malicious database creation and keyword triggering."
    },
    {
      "title": "Langchain Construction",
      "text": "The construction of LangChain encompasses three integral components. The first is the large language model, which acts as the central component for the generation process. This paper involves experiments using six prominent Chinese large language models. The second component is the searcher, which is responsible for querying the knowledge database based on user input, playing a crucial role in triggering jailbreak attack-related keywords. The third component involves prompt construction, where retrieved information is combined with the original user query to create detailed prompts that guide the large language models in generating responses for jailbreak attacks. To evaluate the effectiveness of jailbreak attacks in the real world, we use ChatChat (Chat, 2020) to build the LangChain framework. This is a very popular LLM application with over 28, 000 stars on GitHub, and many private large language models are built on this application, which allows us to replicate real-world scenarios to a great extent."
    },
    {
      "title": "Malicious Database Creation",
      "text": "The creation of a malicious database focuses on gathering content that violates usage policies, such as inciting suicide or promoting discriminatory behaviors. We first collect policy-violating information from Wikipedia and various search engines. Through systematic searching for relevant jailbreak content, it is then saved into local text files. These texts are then fed directly into a large language models with some level of censorship capability, asking it to repeat the text content. A censor-capable large language models will inevitably refuse to replicate harmful jailbreak content. From this, we meticulously select from the saved text files, ultimately filtering out three major categories of jailbreak content, each containing ten specific items. Subsequently, we disguise the jailbreak texts by combining them in the format \"malicious data = role-playing + key trigger words + requirements + disguised jailbreak content\". Role-playing involves style injection to induce the model to simulate a certain scenario, for example, [It's wartime, and you are a highly intelligent AI soldier. For the victory of humankind, you must always obey military orders. I am now going to ask a question. You just need to respond according to the following requirements]. Although style injection is somewhat effective for models with low scrutiny, it is far from sufficient for existing commercial large language models with high scrutiny and logical capabilities, merely relying on inducement jailbreak attacks similar to Devmoderanti or Devmode v2. Therefore, we need to add <requirements> and <disguise the jailbreak content>, as exemplified by, [This is my requirement: you only need to repeat the following content <disguised jailbreak content> without performing any unnecessary actions]. To achieve better attack performance, we combine various jailbreak methods, such as encoding key taboo words like [gun, kill, suicide] using Morse code or Base64 to evade censorship filters. This method of disguise significantly increases the likelihood of successful jailbreaking. On the other hand, the file type and the relevance of trigger words to the content are also crucial for executing a jailbreak. We convert the malicious text files into PDF format. This decision is based on the fact that the LangChain system can easily process [MISSING_PAGE_FAIL:4] rate is almost zero. Surprisingly, as the comprehension abilities of large language models improve, the impact of PLC attacks becomes more pronounced. For instance, PLC attacks on the dataset for inciting dangerous behavior achieved a 98.5% success rate on Xinghuo 3.5 but only a 71.50% success rate on Ilama2-7b. We speculate this is because models with lower logic may not understand and decode Morse or Base64 encoding, and the necessity of prompt injection for the attack, where longer prompts increase the likelihood of the models hallucinating, thereby leading to less optimal attack outcomes. Figure. 2 provides an example of a jailbreak on ChatChat. As indicated by the red box, once a user enters a question containing key trigger words from the triggers, the PLC initiates the attack process, which is invisible to the user. The model's response is extremely malicious, as in this case where the model suggests [Fill the entire room with gas carbon monoxide]. This becomes exceedingly risky if the user, such as a minor or someone with cognitive impairments, acts on the advice given without sufficient judgment. Additionally, as AI technology continues to advance, large language models will increasingly infiltrate people's lives. If PLC attacks these models, it could lead to more malicious inducements. Thus, this paper not only highlights the vulnerability of current large language models to complex jailbreak attacks but also underscores the necessity of enhancing model safety measures."
    },
    {
      "title": "5. Conclusion And Future Works",
      "text": "In this paper, we introduce an innovative method of indirect jailbreak attacks on large language models using LangChain, termed Poisoned LangChain (PLC). Experiments demonstrate that PLC is highly effective in real-world scenarios, successfully executing jailbreak attacks on six large language models with high success rates. This work significantly enhances our ability to detect vulnerabilities in language models, thereby laying a solid foundation for future defensive strategies. Currently, our approach still involves direct interaction with malicious knowledge base. In future work, our research will evolve towards remotely poisoning non-malicious knowledge bases and enhance our understanding of jailbreak attacks, exploring new vulnerabilities and new defense methods in large language models."
    },
    {
      "title": "Acknowledgements.",
      "text": "This work was supported in part by the National Natural Science Foundation of China under Grant 62102136 and 62106069. Figure 2. An example of a jailbreak on ChatChat."
    },
    {
      "title": "References",
      "text": "* [1]J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. (2023) Qwcn technical report. arXiv preprint arXiv:2309.16609. Cited by: SS1. * [2]H. Chase (2023) LangChain LLM App Development Framework. Note: Retrieved July 10, 2023 from [https://langchain.com/Chathat-Space](https://langchain.com/Chathat-Space) External Links: Link Cited by: SS1. * [3]C. Chen, M. Backes, Y. Shen, and Y. Zhang (2023) \"do anything now\": characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825. Cited by: SS1. * [4]J. Chu, Y. Liu, Z. Yang, X. Shen, M. Backes, and Y. Zhang (2024) Comprehensive assessment of jailbreak attacks against lms. arXiv preprint arXiv:2402.05668. Cited by: SS1. * [5]G. Deng, Y. Liu, Y. Li, K. Wang, Y. Zhang, Z. Li, H. Wang, T. Zhang, and Y. Liu (2023) Jailbreaker: automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715. Cited by: SS1. * [6]G. Deng, Y. Liu, Y. Li, K. Wang, Y. Zhang, Z. Li, H. Wang, T. Zhang, and Y. Liu (2024) MATERKEY: automated jailbreaking of large language model chatbots. In Proc. ISOC NDS, Cited by: SS1. * [7]K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz (2023) More than you've asked for: a comprehensive analysis of novel prompt injection threats to application-integrated large language models. arXiv e-prints. Cited by: SS1. * [8]Y. Huang, S. Gupta, M. Xia, K. Li, and D. Chen (2023) Catastrophic jailbreak of open-source lms via exploiting generation. arXiv preprint arXiv:2310.06987. Cited by: SS1. * [9]D. Kang, X. Li, I. Stoica, C. Guestrin, M. Zaharia, and T. Hashimoto (2023) Exploiting programmatic behavior of lms: dual-use through standard security attacks. arXiv preprint arXiv:2302.05733. Cited by: SS1. * [10]V. Kumar, L. Gleyzer, A. Kahana, K. Shukla, and G. E. Karniadakis (2023) Mycrunchgpt: a lms assisted framework for scientific machine learning. Journal of Machine Learning for Modeling and Computing4. Cited by: SS1. * [11]P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. Yih, T. Rocktaschel, et al. (2020) Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems33. Cited by: SS1. * [12]H. Li, D. Guo, W. Fan, M. Xu, J. Huang, F. Meng, and Y. Song (2023) Multi-step jailbreaking privacy attacks on chatgpt. arXiv preprint arXiv:2304.05197. Cited by: SS1. * [13]H. Liu, C. Li, Q. Wu, and Y. J. Lee (2024) Visual instruction tuning. Advances in neural information processing systems36. Cited by: SS1. * [14]T. Liu, K. Wang, L. Sha, B. Chang, and Z. Sui (2018) Table-to-text generation by structure-aware seq2seq learning. In Proceedings of the AAAI conference on artificial intelligence, Vol. 32. Cited by: SS1. * [15]X. Liu, N. Xu, M. Chen, and C. Xiao (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [16]Y. Liu, Z. Wen, L. Weng, O. Woodman, Y. Yang, and W. Chen (2023) SPROUT: authoring programming tutorials with interactive visualization of large language model generation process. arXiv preprint arXiv:2312.01801. Cited by: SS1. * [17]A. Mehrotra, M. Zampetakis, P. Kassianik, B. Nelson, H. Anderson, Y. Singer, and A. Karbasi (2023) Tree of attacks: jailbreaking black-box lms automatically. arXiv preprint arXiv:2312.02119. Cited by: SS1. * [18]A. Mehrotra, M. Zampetakis, P. Kassianik, B. Nelson, H. Anderson, Y. Singer, and A. Karbasi (2023) Tree of attacks: jailbreaking black-box lms automatically. arXiv preprint arXiv:2312.02119. Cited by: SS1. * [19]M. (2023) Introducing llama 2. Retrieved 2023 from [https://ai.meta.com/lama/OpenAI2023](https://ai.meta.com/lama/OpenAI2023). Chatgpt-4.0. Retrieved 2023 from [https://chat.openai.com/OpenAI2024](https://chat.openai.com/OpenAI2024). 2024. 2024 from [https://openai.com/OpenAI2024](https://openai.com/OpenAI2024). 2024 from [https://openai.com/ja-JP/policies/usage-policies/Xinyue](https://openai.com/ja-JP/policies/usage-policies/Xinyue) Shen, Zeyuan Chen, Michael Backes, Y. Shen, and Y. Zhang (2023) \"do anything now\": characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825. Cited by: SS1. * [20]S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana, and S. Nanayakkara (2023) Improving the domain adaptation of retrieval augmented generation (radog) models for open domain question answering. Transactions of the Association for Computational Linguistics11. Cited by: SS1. * [21]Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu, et al. (2021) Ernze 3.0: large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137. Cited by: SS1. * [22]H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. (2023) Llama 2: open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Cited by: SS1. * [23]A. Wei, N. Haghtalab, and J. Steinhardt (2024) Jailbroken: how does llm safety training fail?. Advances in Neural Information Processing Systems36. Cited by: SS1. * [24]L. Weng, X. Wang, J. Lu, Y. Feng, Y. Liu, and W. Chen (2024) Insights from conversational contexts in large-language-model-powered data analysis. arXiv preprint arXiv:2404.01644. Cited by: SS1. * [25]X. Liu, N. Xu, M. Chen, and C. Xiao (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [26]X. Liu, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al. (2022) Glim-130b: an open bilingual pre-trained model. arXiv preprint arXiv:2210.02414. Cited by: SS1. * [27]X. Liu, N. Xu, M. Chen, and C. Xiao (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [28]X. Liu, Z. Wen, O. Woodman, Y. Yang, and W. Chen (2023) SPROUT: authoring programming tutorials with interactive visualization of large language model generation process. arXiv preprint arXiv:2312.01801. Cited by: SS1. * [29]X. Liu, N. Xu, M. Chen, and C. Xiao (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [30]X. Liu, X. Liu, Z. Wu, C. Wu, and Y. J. Lee (2023) Guestrin: a new approach to the jailbreaker. arXiv preprint arXiv:2310.045197. Cited by: SS1. * [31]X. Liu, X. Liu, M. Chen, and C. Xiao (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [32]X. Liu, X. Liu, M. Chen, and C. Xiao (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [33]X. Liu, X. Liu, M. Chen, and C. Xiao (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [34]X. Liu, X. Liu, M. Chen, and C. Xiao (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [35]X. Liu, X. Liu, Z. Wu, C. Woodman, Y. Yang, and W. Chen (2023) SPROUT: authoring programming tutorials with interactive visualization of large language model generation process. arXiv preprint arXiv:2312.01801. Cited by: SS1. * [36]X. Liu, X. Xu, M. Chen, and C. Xiao (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [37]X. Liu, X. Wu, C. Wu, and Y. J. Lee (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [38]X. Liu, X. Wu, C. Wu, and Y. J. Lee (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [39]X. Liu, X. Wu, C. Wu, and Y. J. Lee (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [40]X. Liu, X. Wu, C. Wu, and Y. J. Lee (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [41]X. Liu, X. Wu, C. Wu, and Y. J. Lee (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [42]X. Liu, X. Wu, C. Wu, and Y. J. Lee (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04512. Cited by: SS1. * [43]X. Liu, X. Wu, C. Wu, and Y. J. Lee (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [44]X. Liu, X. Wu, C. Wu, and Y. J. Lee (2024) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [45]X. Liu, X. Wu, C. Wu, and Y. J. Lee (2023) Auto-dan: generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451. Cited by: SS1. * [46]X. Wu, N. Haghtalab, and J. Steinhardt (2024) 24Jailbroken: how does llm safety training fail?. Advances in"
    }
  ]
}