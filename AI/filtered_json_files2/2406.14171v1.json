{
  "title": "Ranking LLMs by compression",
  "authors": [
    "Peijia Guo",
    "Ziguang Li",
    "Haibo Hu",
    "Chao Huang",
    "Ming Li",
    "Rui Zhang",
    "Yiheng Liu",
    "Tianle Han",
    "Siyuan Ma",
    "Jiayue Zhang",
    "Yuanyuan Yang",
    "Jiaming Tian",
    "Hao He",
    "Antong Li",
    "Mengshen He",
    "Zhengliang Liu",
    "Zihao Wu",
    "Lin Zhao",
    "Dajiang Zhu",
    "Xiang Li",
    "Ning Qiang",
    "Dingang Shen",
    "Tianming Liu",
    "Bao 2023 Ge",
    "Yu Mao",
    "Yufei Cui",
    "Tei-Wei Kuo",
    "Chun Jason",
    "Ing Bao",
    "Mohammad Bavarian",
    "Jeff Belgum",
    "Ir- Wan Bello",
    "Jake Berdine",
    "Gabriel Bernadett-Shapiro",
    "Christopher Berner",
    "Lenny Bogdonoff",
    "Oleg Boiko",
    "Madelaine Boyd",
    "Anna-Luisa Brakman",
    "Greg Brock- Man",
    "Tim Brooks",
    "Miles Brundage",
    "Kevin Button",
    "Trevor Cai",
    "Rosie Campbell",
    "Andrew Cann",
    "Brittany Carey",
    "Chelsea Carlson",
    "Rory Carmichael",
    "Brooke Chan",
    "Che Chang",
    "Fotis Chantzis",
    "Derek Chen",
    "Sully Chen",
    "Ruby Chen",
    "Jason Chen",
    "Mark Chen",
    "Ben Chess",
    "Chester Cho",
    "Hyung Casey Chu",
    "Won Chung",
    "Dave Cummings",
    "Jeremiah Currier",
    "Yunxing Dai",
    "Tarun Goel",
    "Gabriel Gogineni",
    "Rapha Goh",
    "Jonathan Gontijo- Lopes",
    "Morgan Gordon",
    "Scott Grafstein",
    "Ryan Gray",
    "Joshua Greene",
    "Shixiang Shane Gross",
    "Yufei Gu",
    "Chris Guo",
    "Jesse Hallacy",
    "Jeff Han",
    "Yuchen Harris",
    "Mike He",
    "Johannes Heaton",
    "Chris Heidecke",
    "Alan Hesse",
    "Wade Hickey",
    "Peter Hickey",
    "Brandon Hoeschele",
    "Kenny Houghton",
    "Shengli Hsu",
    "Xin Hu",
    "Joost Hu",
    "Shantanu Huizinga",
    "Shawn Jain",
    "Joanne Jain",
    "Angela Jang",
    "Roger Jiang",
    "Haozhun Jiang",
    "Denny Jin",
    "Shino Jin",
    "Billie Jomoto",
    "Hee- Woo Jonn",
    "Tomer Jun",
    "Łukasz Kaftan",
    "Ali Kaiser",
    "Ingmar Ka- Mali",
    "Kanitscheider",
    "Shirish Nitish",
    "Tabarak Keskar",
    "Logan Khan",
    "Jong Wook Kilpatrick",
    "Christina Kim",
    "Yongjik Kim",
    "Jan Hendrik Kim",
    "Jamie Kirch- Ner",
    "Matt Kiros",
    "Daniel Knight",
    "Łukasz Kokotajlo",
    "Andrew Kondraciuk",
    "Aris Kondrich",
    "Kyle Kon- Stantinidis",
    "Gretchen Kosic",
    "Vishal Krueger",
    "Michael Kuo",
    "Ikai Lampe",
    "Teddy Lan",
    "Jan Lee",
    "Jade Leike",
    "Daniel Leung",
    "Chak Ming Levy",
    "Rachel Li",
    "Molly Lim",
    "Stephanie Lin",
    "Mateusz Lin",
    "Theresa Litwin",
    "Ryan Lopez",
    "Patricia Lowe",
    "Anna Lue",
    "Kim Makanju",
    "Sam Malfacini",
    "Todor Manning",
    "Yaniv Markov",
    "Bianca Markovski",
    "Katie Martin",
    "Andrew Mayer",
    "Bob Mayne",
    "Scott Mayer Mcgrew",
    "Christine Mckinney",
    "Paul Mcleavey",
    "Jake Mcmillan",
    "David Mcneil",
    "Aalok Medina",
    "Jacob Mehta",
    "Luke Menick",
    "Andrey Metz",
    "Pamela Mishchenko",
    "Vinnie Mishkin",
    "Evan Monaco",
    "Daniel Morikawa",
    "Tong Mossing",
    "Mira Mu",
    "Oleg Murati",
    "David Murk",
    "Ashvin Mély",
    "Reiichiro Nair",
    "Rajeev Nakano",
    "Arvind Nayak",
    "Richard Neelakantan",
    "Hyeonwoo Ngo",
    "Noh"
  ],
  "abstract": "\n We conceptualize the process of understanding as information compression, and propose a method for ranking large language models (LLMs) based on lossless data compression. We demonstrate the equivalence of compression length under arithmetic coding with cumulative negative log probabilities when using a large language model as a prior, that is, the pretraining phase of the model is essentially the process of learning the optimal coding length. At the same time, the evaluation metric compression ratio can be obtained without actual compression, which greatly saves overhead. In this paper, we use five large language models as priors for compression, then compare their performance on challenging natural language processing tasks, including sentence completion, question answering, and coreference resolution. Experimental results show that compression ratio and model performance are positively correlated, so it can be used as a general metric to evaluate large language models. \n",
  "references": [
    {
      "id": null,
      "title": "Ranking LLMs by compression",
      "authors": [
        "Peijia Guo",
        "Ziguang Li",
        "Haibo Hu",
        "Chao Huang",
        "Ming Li",
        "Rui Zhang",
        "Yiheng Liu",
        "Tianle Han",
        "Siyuan Ma",
        "Jiayue Zhang",
        "Yuanyuan Yang",
        "Jiaming Tian",
        "Hao He",
        "Antong Li",
        "Mengshen He",
        "Zhengliang Liu",
        "Zihao Wu",
        "Lin Zhao",
        "Dajiang Zhu",
        "Xiang Li",
        "Ning Qiang",
        "Dingang Shen",
        "Tianming Liu",
        "Bao 2023 Ge",
        "Yu Mao",
        "Yufei Cui",
        "Tei-Wei Kuo",
        "Chun Jason",
        "Ing Bao",
        "Mohammad Bavarian",
        "Jeff Belgum",
        "Ir- Wan Bello",
        "Jake Berdine",
        "Gabriel Bernadett-Shapiro",
        "Christopher Berner",
        "Lenny Bogdonoff",
        "Oleg Boiko",
        "Madelaine Boyd",
        "Anna-Luisa Brakman",
        "Greg Brock- Man",
        "Tim Brooks",
        "Miles Brundage",
        "Kevin Button",
        "Trevor Cai",
        "Rosie Campbell",
        "Andrew Cann",
        "Brittany Carey",
        "Chelsea Carlson",
        "Rory Carmichael",
        "Brooke Chan",
        "Che Chang",
        "Fotis Chantzis",
        "Derek Chen",
        "Sully Chen",
        "Ruby Chen",
        "Jason Chen",
        "Mark Chen",
        "Ben Chess",
        "Chester Cho",
        "Casey Chu",
        "Won Chung",
        "Dave Cummings",
        "Jeremiah Currier",
        "Yunxing Dai",
        "Tarun Goel",
        "Gabriel Gogineni",
        "Rapha Goh",
        "Jonathan Gontijo- Lopes",
        "Morgan Gordon",
        "Scott Grafstein",
        "Ryan Gray",
        "Joshua Greene",
        "Shane Gross",
        "Yufei Gu",
        "Chris Guo",
        "Jesse Hallacy",
        "Jeff Han",
        "Yuchen Harris",
        "Mike He",
        "Johannes Heaton",
        "Chris Heidecke",
        "Alan Hesse",
        "Wade Hickey",
        "Peter Hickey",
        "Brandon Hoeschele",
        "Kenny Houghton",
        "Shengli Hsu",
        "Xin Hu",
        "Joost Hu",
        "Shantanu Huizinga",
        "Shawn Jain",
        "Joanne Jain",
        "Angela Jang",
        "Roger Jiang",
        "Haozhun Jiang",
        "Denny Jin",
        "Shino Jin",
        "Billie Jomoto",
        "Hee- Woo Jonn",
        "Tomer Jun",
        "Łukasz Kaftan",
        "Ali Kaiser",
        "Ingmar Ka- Mali",
        "Kanitscheider",
        "Shirish Nitish",
        "Tabarak Keskar",
        "Logan Khan",
        "Jong Wook Kilpatrick",
        "Christina Kim",
        "Yongjik Kim",
        "Jan Hendrik Kim",
        "Jamie Kirch- Ner",
        "Matt Kiros",
        "Daniel Knight",
        "Łukasz Kokotajlo",
        "Andrew Kondraciuk",
        "Aris Kondrich",
        "Kyle Kon- Stantinidis",
        "Gretchen Kosic",
        "Vishal Krueger",
        "Michael Kuo",
        "Ikai Lampe",
        "Teddy Lan",
        "Jan Lee",
        "Jade Leike",
        "Daniel Leung",
        "Ming Levy",
        "Rachel Li",
        "Molly Lim",
        "Stephanie Lin",
        "Mateusz Lin",
        "Theresa Litwin",
        "Ryan Lopez",
        "Patricia Lowe",
        "Anna Lue",
        "Kim Makanju",
        "Sam Malfacini",
        "Todor Manning",
        "Yaniv Markov",
        "Bianca Markovski",
        "Katie Martin",
        "Andrew Mayer",
        "Bob Mayne",
        "Scott Mayer Mcgrew",
        "Christine Mckinney",
        "Paul Mcleavey",
        "Jake Mcmillan",
        "David Mcneil",
        "Aalok Medina",
        "Jacob Mehta",
        "Luke Menick",
        "Andrey Metz",
        "Pamela Mishchenko",
        "Vinnie Mishkin",
        "Evan Monaco",
        "Daniel Morikawa",
        "Tong Mossing",
        "Mira Mu",
        "Oleg Murati",
        "David Murk",
        "Ashvin Mély",
        "Reiichiro Nair",
        "Rajeev Nakano",
        "Arvind Nayak",
        "Richard Neelakantan",
        "Hyeonwoo Ngo",
        "Noh"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Lossless data compression with neural networks",
      "authors": [
        "Fabrice Bellard"
      ],
      "year": "2019",
      "venue": "Lossless data compression with neural networks",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Evaluating the feasibility of chatgpt in healthcare: an analysis of multiple clinical and research scenarios",
      "authors": [
        "Marco Cascella",
        "Jonathan Montomoli",
        "Valentina Bellini",
        "Elena Bignami"
      ],
      "year": "2023",
      "venue": "Journal of medical systems",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "A survey on evaluation of large language models",
      "authors": [
        "Yupeng Chang",
        "Xu Wang",
        "Jindong Wang",
        "Yuan Wu",
        "Linyi Yang",
        "Kaijie Zhu",
        "Hao Chen",
        "Xiaoyuan Yi",
        "Cunxiang Wang",
        "Yidong Wang"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Language modeling is compression",
      "authors": [
        "Grégoire Delétang",
        "Anian Ruoss",
        "Paul-Ambroise Duquenne",
        "Elliot Catt",
        "Tim Genewein",
        "Christopher Mattern",
        "Jordi Grau-Moya",
        "Li",
        "Kevin Wenliang",
        "Matthew Aitchison",
        "Laurent Orseau"
      ],
      "year": "2023",
      "venue": "Language modeling is compression",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "On the equilibrium of heterogeneous substances",
      "authors": [
        "Josiah Willard",
        "Gibbs"
      ],
      "year": "1878",
      "venue": "American Journal of Science",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Generative adversarial nets",
      "authors": [
        "Ian Goodfellow",
        "Jean Pouget-Abadie",
        "Mehdi Mirza",
        "Bing Xu",
        "David Warde-Farley",
        "Sherjil Ozair",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Deepzip: Lossless data compression using recurrent neural networks",
      "authors": [
        "Mohit Goyal",
        "Kedar Tatwawadi"
      ],
      "year": "2018",
      "venue": "Deepzip: Lossless data compression using recurrent neural networks",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Evaluating large language models: A comprehensive survey",
      "authors": [
        "Zishan Guo",
        "Renren Jin",
        "Chuang Liu",
        "Yufei Huang",
        "Dan Shi",
        "Linhao Yu",
        "Yan Liu",
        "Jiaxuan Li",
        "Bojian Xiong",
        "Deyi Xiong"
      ],
      "year": "2023",
      "venue": "Evaluating large language models: A comprehensive survey",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Approximating human-like fewshot learning with gpt-based compression",
      "authors": [
        "Cynthia Huang",
        "Yuqing Xie",
        "Zhiying Jiang",
        "Jimmy Lin",
        "Ming Li"
      ],
      "year": "2023",
      "venue": "Approximating human-like fewshot learning with gpt-based compression",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "authors": [
        "Srinivasan Iyer",
        "Xi Victoria Lin",
        "Ramakanth Pasunuru",
        "Todor Mihaylov",
        "Daniel Simig",
        "Ping Yu",
        "Kurt Shuster",
        "Tianlu Wang",
        "Qing Liu",
        "Punit Singh Koura",
        "Xian Li",
        "Brian O' Horo",
        "Gabriel Pereyra",
        "Jeff Wang",
        "Christopher Dewan",
        "Asli Celikyilmaz",
        "Luke Zettlemoyer",
        "Ves Stoyanov"
      ],
      "year": "2023",
      "venue": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Mistral 7b",
      "authors": [
        "Alexandre Albert Q Jiang",
        "Arthur Sablayrolles",
        "Chris Mensch",
        "Devendra Bamford",
        "Diego Singh Chaplot",
        "Florian De Las Casas",
        "Gianna Bressand",
        "Guillaume Lengyel",
        "Lucile Lample",
        "Saulnier"
      ],
      "year": "2023",
      "venue": "Mistral 7b",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "A systematic study and comprehensive evaluation of chatgpt on benchmark datasets",
      "authors": [
        "Md Tahmid Rahman Laskar",
        "M Saiful Bari",
        "Mizanur Rahman",
        "Md Amran Hossen Bhuiyan",
        "Shafiq Joty",
        "Jimmy Xiangji Huang"
      ],
      "year": "2023",
      "venue": "A systematic study and comprehensive evaluation of chatgpt on benchmark datasets",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "An open source data contamination report for llama series models",
      "authors": [
        "Yucheng Li"
      ],
      "year": "2023",
      "venue": "An open source data contamination report for llama series models",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "",
      "authors": [
        "Long Ouyang",
        "O' Cullen",
        "Jakub Keefe",
        "Alex Pachocki",
        "Joe Paino",
        "Ashley Palermo",
        "Giambattista Pantuliano",
        "Joel Parascandolo",
        "Emy Parish",
        "Alex Parparita",
        "Mikhail Passos",
        "Andrew Pavlov",
        "Adam Peng",
        "Filipe Perelman",
        "Michael De Avila Belbute Peres",
        "Henrique Petrov",
        "Ponde De Oliveira",
        "Pinto",
        "Michael",
        "Michelle Pokorny",
        "Pokrass",
        "H Vitchyr",
        "Tolly Pong",
        "Alethea Powell",
        "Boris Power",
        "Elizabeth Power",
        "Raul Proehl",
        "Alec Puri",
        "Jack Radford",
        "Aditya Rae",
        "Cameron Ramesh",
        "Francis Raymond",
        "Kendra Real",
        "Carl Rimbach",
        "Bob Ross",
        "Henri Rotsted",
        "Nick Roussez",
        "Mario Ryder",
        "Ted Saltarelli",
        "Shibani Sanders",
        "Girish Santurkar",
        "Heather Sastry",
        "David Schmidt",
        "John Schnurr",
        "Daniel Schulman",
        "Kyla Selsam",
        "Toki Sheppard",
        "Jessica Sherbakov",
        "Sarah Shieh",
        "Pranav Shoker",
        "Szymon Shyam",
        "Eric Sidor",
        "Maddie Sigler",
        "Jordan Simens",
        "Katarina Sitkin",
        "Ian Slama",
        "Benjamin Sohl",
        "Yang Sokolowsky",
        "Natalie Song",
        "Felipe Petroski Staudacher",
        "Natalie Such",
        "Ilya Summers",
        "Jie Sutskever",
        "Nikolas Tang",
        "Madeleine B Tezak",
        "Phil Thompson",
        "Amin Tillet",
        "Elizabeth Tootoonchian",
        "Preston Tseng",
        "Nick Tuggle",
        "Jerry Turley",
        "Juan Tworek",
        "Felipe Cerón",
        "Andrea Uribe",
        "Arun Vallone",
        "Chelsea Vijayvergiya",
        "Carroll Voss",
        "Justin Jay Wainwright",
        "Alvin Wang",
        "Ben Wang",
        "Jonathan Wang",
        "Jason Ward",
        "Wei",
        "Akila Cj Weinmann",
        "Peter Welihinda",
        "Jiayi Welinder",
        "Lilian Weng",
        "Matt Weng",
        "Dave Wiethoff",
        "Clemens Willner",
        "Samuel Winter",
        "Hannah Wolrich",
        "Lauren Wong",
        "Sherwin Workman",
        "Jeff Wu",
        "Michael Wu",
        "Kai Wu",
        "Tao Xiao",
        "Sarah Xu",
        "Kevin Yoo",
        "Qiming Yu",
        "Wojciech Yuan",
        "Rowan Zaremba",
        "Chong Zellers",
        "Marvin Zhang",
        "Zhang"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Chatgpt for education and research: Opportunities, threats, and strategies",
      "authors": [
        "Md",
        "Mostafizer Rahman",
        "Yutaka Watanobe"
      ],
      "year": "2023",
      "venue": "Applied Sciences",
      "doi": "10.3390/app13095783"
    },
    {
      "id": "b17",
      "title": "Variational inference with normalizing flows",
      "authors": [
        "Danilo Rezende",
        "Shakir Mohamed"
      ],
      "year": "2015",
      "venue": "International conference on machine learning",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "A mathematical theory of communication",
      "authors": [
        "Claude Elwood",
        "Shannon"
      ],
      "year": "1948",
      "venue": "The Bell system technical journal",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Large language models in medicine",
      "authors": [
        "Arun James Thirunavukarasu",
        "Darren Shu",
        "Jeng Ting",
        "Kabilan Elangovan",
        "Laura Gutierrez",
        "Ting Fang Tan",
        "Daniel Shu",
        "Wei Ting"
      ],
      "year": "2023",
      "venue": "Nature medicine",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "How reasonable are common-sense reasoning tasks: A case-study on the winograd schema challenge and swag",
      "authors": [
        "Paul Trichelair",
        "Ali Emami",
        "Adam Trischler",
        "Kaheer Suleman",
        "Jackie Chi",
        "Kit Cheung"
      ],
      "year": "2018",
      "venue": "How reasonable are common-sense reasoning tasks: A case-study on the winograd schema challenge and swag",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Pixel recurrent neural networks",
      "authors": [
        "Aäron Van Den",
        "Nal Oord",
        "Koray Kalchbrenner",
        "Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "International conference on machine learning",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "authors": [
        "Alex Wang",
        "Amanpreet Singh",
        "Julian Michael",
        "Felix Hill",
        "Omer Levy",
        "Samuel R Bowman"
      ],
      "year": "2019",
      "venue": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Muhammad Abdul-Mageed, and Alham Fikri Aji",
      "authors": [
        "Minghao Wu",
        "Abdul Waheed",
        "Chiyu Zhang"
      ],
      "year": "2023",
      "venue": "Lamini-lm: A diverse herd of distilled models from large-scale instructions",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "An introduction to neural data compression. Foundations and Trends® in Computer Graphics and Vision",
      "authors": [
        "Yibo Yang",
        "Stephan Mandt",
        "Lucas Theis"
      ],
      "year": "2023",
      "venue": "An introduction to neural data compression. Foundations and Trends® in Computer Graphics and Vision",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Yifan Dong",
        "Chen Du",
        "Yushuo Yang",
        "Zhipeng Chen",
        "Jinhao Chen",
        "Ruiyang Jiang",
        "Yifan Ren",
        "Xinyu Li",
        "Zikang Tang",
        "Peiyu Liu",
        "Jian-Yun Liu",
        "Ji-Rong Nie",
        "Wen"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Ranking Llms By Compression",
      "text": "Peijia Guo\\({}^{1,2}\\), Ziguang Li \\({}^{3}\\), Haibo Hu \\({}^{3}\\), Chao Huang \\({}^{3*}\\), Ming Li \\({}^{4*}\\), Rui Zhang \\({}^{1*}\\) \\({}^{1}\\) School of Mathematics, Northwest University, Xi'an, China \\({}^{2}\\)Shanghai Institute for Mathematics and Interdisciplinary Sciences, Shanghai, China \\({}^{3}\\) Institute of Computing Technology, Chinese Academy of Sciences, China \\({}^{4}\\)Cheriton School of Computer Science, University of Waterloo, Ontario, Canada Guopeijia0929@163.com chriszggz@gamil.com hhuhaibo22@mails.ucas.ac.cn chuang@ict.ac.c nmli@uwaterloo.ca rzhang@nwu.edu.cn"
    },
    {
      "title": "Abstract",
      "text": "We conceptualize the process of understanding as information compression, and propose a method for ranking large language models (LLMs) based on lossless data compression. We demonstrate the equivalence of compression length under arithmetic coding with cumulative negative log probabilities when using a large language model as a prior, that is, the pre-training phase of the model is essentially the process of learning the optimal coding length. At the same time, the evaluation metric compression ratio can be obtained without actual compression, which greatly saves overhead. In this paper, we use five large language models as priors for compression, then compare their performance on challenging natural language processing tasks, including sentence completion, question answering, and coreference resolution. Experimental results show that compression ratio and model performance are positively correlated, so it can be used as a general metric to evaluate large language models."
    },
    {
      "title": "1 Introduction",
      "text": "In recent years, the rapid development of LLMs has brought earth-shaking changes to the field of natural language processing (NLP) (Radford et al., 2019; Zhao et al., 2023; Liu et al., 2023). LLMs are advanced language models pretrained on tens of gigabytes of data without tuning on data for specific tasks. These large models can directly complete various NLP tasks, and even become a milestone technology towards general artificial intelligence (AGI). Currently, LLMs are being studied more and more widely in various fields, such as education and research (Rahman and Watanabe, 2023), medicine and healthcare (Thirunavukarasu et al., 2023; Cascella et al., 2023), etc., and their performance evaluation methods are becoming more and more important. Chang et al. (2024) showed that researchers always scrutinize the capabilities of AI models or algorithms through evaluation using specific and challenging tasks, so the evaluation metrics are outlined from the perspective of the evaluation tasks. The metrics are diverse, such as Exact Match (EM), F1-score, ROUGE, etc., and many are set for specific tasks, making it difficult to uniformly evaluate the performance of the model on different tasks. In addition, contamination of training and test data can also lead to biased evaluation results (Magar and Schwartz, 2022), making it impossible to verify whether NLP progress is achieved through better language understanding or better data utilization. Various limitations lead to the lack of a unified LLMs evaluation standard. Therefore, we consider the process of model training and learning itself and prove the equivalence of the model pre-training goal and the compression length under arithmetic coding, indicating that compression is closely related to model performance, and then use the compression ratio as a general metric to measure the model's generalization ability in different scenarios."
    },
    {
      "title": "2 Related Work",
      "text": ""
    },
    {
      "title": "Language Models Evaluation",
      "text": "Currently, performance evaluation of LLMs is mainly achieved through benchmark tests, including diverse tasks, standardized datasets and comprehensive evaluation metrics. The purpose is to establish a systematic and standardized evaluation framework. In 2019, Wang et al. (2019) introduced the General Language Understanding Evaluation Benchmark (GLUE), a multi-task evaluation platform for measuring the performance of natural language understanding models. It contains nine tasks, covering various types such as text classification, text similarity evaluation, natural language Inference, question answering, etc. A recent study Laskar et al. (2023) evaluated ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets, laying the foundation for deploying ChatGPT-like LLMs in real-world applications. More recently, OpenAI et al. (2024) tested GPT-4 on a diverse set of benchmarks, including 34 simulating exams that were originally designed for humans. Benchmark test is very important for evaluating the performance of language models and promoting research progress, but limited coverage tasks, data contamination (Brown et al., 2020; Li, 2023), and huge overhead are all challenges and limitations faced in this process. In order to solve these problems, we propose compression ratio based on lossless data compression, a general evaluation metric."
    },
    {
      "title": "Neural Compression",
      "text": "The goal of data compression is to reduce the representation size while retaining valid information. Our LLMs-based compressor uses neural networks for data compression and belongs to the neural compression category. Current research in neural compression largely benefits from advances in deep generative modeling (Yang et al., 2023), such as GANs (Goodfellow et al., 2014), VAEs (Rezende and Mohamed, 2015), and autoregressive models (Van Den Oord et al., 2016). With the development of deep neural networks, lossless text compression has also ushered in new progress. Goyal et al. (2018) proposed DeepZip, a lossless compressor based on neural networks, consisting of two main modules: RNN and arithmetic coding. It achieves higher compression ratio than GZIP. Bellard (2019) proposed a lossless compressor based on LSTM, which is simple to describe and has reasonable memory consumption compared to compressors that provide a similar compression ratio. Recent advancements, such as TRACE, a fast transformer-based general-purpose lossless compressor (Mao et al., 2022), achieves an overall speedup of approximately 3x while maintaining a compression ratio comparable to state-of-the-art compressors."
    },
    {
      "title": "3 Method",
      "text": ""
    },
    {
      "title": "Llms Based Arithmetic Coding For Compression",
      "text": "Shannon's fundamental theorem of coding states that (Shannon, 1948), given messages randomly generated from a model, it is impossible to encode them into less bits (on average) than the entropy of that model, thus defining a lower bound for lossless compression. Arithmetic coding is an entropy coding algorithm. Huang et al. (2023) proposed an entropy-based compressor that integrated generative pre-trained transformer into adaptive arithmetic coding, highlighting the potential of pre-trained LLMs as powerful priors in compression. In this paper, we integrate LLMs into adaptive arithmetic coding for compression, with the aim of representing data according to the probability of output to reduce its overall size. **LLMs as Entropy Models** Considering text data, first use a tokenizer to convert the text into a data stream \\(t_{1:n}:=t_{1}t_{2}\\cdots t_{n}\\in T^{n}\\) of length \\(n\\), where \\(T\\) is LLM vocabulary, a finite set of tokens. The empty sequence is denoted as \\(\\varepsilon\\). Let \\(\\phi\\) represents LLM, where \\(\\phi(t_{1:(i-1)})=P_{i}(t_{i}|t_{1},t_{2},\\cdots,t_{i-1}),\\;i\\geq 2\\) means modeling the next token \\(t_{i}\\) through the previous \\(i-1\\) tokens \\(t_{1:(i-1)}\\), and we get its probability distribution \\(P_{i}\\). In order to obtain the distribution for \\(P_{1}\\), add an EOS (End of Sentence) token at the beginning of the text as \\(t_{0}\\). For each token \\(t_{i}\\), the associated \\(P_{i}\\) acts as the entropy model, guiding the encoder to allocate fewer bits to high-frequency tokens and more bits to low-frequency tokens, thereby improving compression efficiency. **Coding Process** The range for the data stream is the interval \\([0,1)\\) before anything is transmitted. As each token is processed, the cumulative distribution functions \\(F_{i}(t_{i})\\) and \\(P_{i}(t_{i})\\) are calculated according to \\(\\phi(t_{0:(i-1)})\\). Then narrow the interval to the part assigned to that token: \\[I^{i}_{low}=I^{i-1}_{low}+(I^{i-1}_{high}-I^{i-1}_{low})*F_{i}(t_{i}),\\] \\[I^{i}_{high}=I^{i-1}_{low}+(I^{i-1}_{high}-I^{i-1}_{low})*(F_{i}(t_{i})+P_{i}( t_{i}))\\] Adaptive arithmetic coding using LLM is shown in Algorithm 1."
    },
    {
      "title": "Equivalence Of Model Pre-Training Goal And Compression Length",
      "text": "It is well established that compression and prediction are essentially equivalent Deletang et al. (2023). In this way, compression and LLMs are closely linked. We mathematically prove the equivalence of model pre-training goal and compression length. Then we present a novel method for evaluating LLMS based on lossless compression. **Pre-training Optimization Goals for LLMs** The loss function, also known as the objective function, measures the difference between the probability distribution predicted by the model and the true distribution. Model training is to reduce the loss function through continuous iteration, thereby optimizing model performance. We continue to consider the data stream above \\(t_{1:n}:=t_{1}t_{2}\\cdots t_{n}\\in T^{n}\\). The true distribution of data \\(Q\\) is the sequence of probability mass functions \\(Q_{n}:T^{n}\\rightarrow(0,1]\\), for all \\(n\\in N\\), satisfying the constraint \\(Q_{n}(t_{1:n})=\\sum_{s\\in T}Q_{n+1}(t_{1:n}s)\\), where \\(Q_{0}(\\varepsilon):=1\\). The meaning can be clearly seen from the parameters of \\(Q\\), so we omit the subscript of \\(Q\\). Now we have the true distribution \\(Q\\) of the data and the probability distribution \\(P\\) predicted by the LLMs. The pre-training Optimization Goals for LLMs is to make \\(P\\) closer to \\(Q\\), which can elicit the definition of relative entropy, that is, Kullback-Leibler Divergence: \\[D_{KL}(Q||P)=\\sum_{i=1}^{n}(Q_{i}\\log_{2}Q_{i})-\\sum_{i=1}^{n}(Q_{i}\\log_{2}P _{i})\\] The previous term \\(\\sum_{i=1}^{n}(Q_{i}\\cdot\\log_{2}Q_{i})\\) is the inverse of the entropy of the true distribution \\(Q\\), which is constant. The last term \\(-\\sum_{i=1}^{n}(Q_{i}\\cdot\\log_{2}P_{i})\\) is the definition of cross entropy, represented by \\(H(Q,P)\\). Gibbs inequality states that Gibbs (1878): \\(D_{KL}(Q||P)\\geq 0\\), the equality sign is true if and only if \\(Q_{i}=P_{i},\\forall i\\). Therefore, in order to make the probability distribution \\(P\\) closer to the true distribution \\(Q\\), that is, to minimize the value of cross entropy. It further illustrates that cross entropy can be used as the loss function, and minimizing cross entropy is the goal of optimizing the model. **Negative Log Probability as Compression Length** The goal of lossless compression is to encode a data stream \\(t_{1:n}\\) sampled from a true distribution \\(Q\\) into a minimum length bit stream, while ensuring that the original sequence can be recovered through decoding. In practice, \\(Q\\) is usually unknown, so we approximate \\(Q\\) through the probability distribution \\(P\\) predicted by the LLMs \\(\\phi\\). During arithmetic coding, the length of the interval \\(I^{i}\\) is equal to \\(I^{i-1}*P_{i}(t_{i})\\). For the sequence \\(t_{1:n}\\), starting from the initial interval of length 1, the final encoded interval length is \\(\\prod_{i=1}^{n}P_{i}(t_{i})\\), so the number of bits required to represent this final interval (i.e. message \\(t_{1:n}\\)) is \\(\\sum_{i=1}^{n}-log_{2}P_{i}(t_{i})\\). This reveals a direct way to approximate the compression length without having to perform the compression method exactly. So the expected number of bits we get is \\(E_{t\\sim Q}\\left[\\sum_{i=1}^{n}-\\log_{2}P_{i}(t_{i})\\right]\\), that is the cross entropy \\(H(Q,P)\\). Therefore, in the process of achieving lossless compression, minimizing the expected length of the encoded data stream is equivalent to minimizing cross entropy. At this point, the equivalence of model pre-training goal and compression length has been proven. Furthermore, we can use compression ratio as a unified criterion for evaluating LLMs."
    },
    {
      "title": "4 Experiments",
      "text": "The experiment consists of four key parts: the calculation of the compression ratio and three natural language processing tasks, namely sentence completion, question answering and coreference resolution. We use a total of five LLMs as compressor priors, but the proposed method is not limited to these models. This method can be applied to more advanced LLMs as long as the predicted probabilities can be obtained."
    },
    {
      "title": "The Calculation Of Compression Ratio",
      "text": "First, we select the Text8 dataset to calculate the compression ratio of the compressor. The Text8 dataset is a large corpus extracted from the English Wikipedia. After some simple preprocessing, the text content covers various topics and fields. It is a general dataset for language modeling. We split the read Text8 file by spaces and obtain a list containing all words. Then every 200 words are divided into a sublist, and the 200-length word fragment are converted into strings. The list of the first 10,000 strings is passed to the LLMs compressor as a parameter. The compression ratio calculation formula is as follows (in bits): \\[compression\\:ratio=\\frac{original\\:text\\:length}{compressed\\:text\\:length}.\\]The LLM compressors involved include LLaMA 2 7B released by Meta, Mistral 7B released by the Mistral AI team, OPT-IML 1.3B released by Facebook, and GPT-2-XL 1.5B and GPT-2 774M released by OpenAI. Their calculated compression ratios are shown in Table 1."
    },
    {
      "title": "Sentence Completion",
      "text": "Sentence completion is designed to allow the computer to predict the missing parts based on the given context, so that the sentence becomes coherent and complete. We compare the performance of three large models, LLaMA 2 7B, Mistral 7B and GPT-2-XL 1.5B on the HellaSwag dataset, using accuracy as a metric. The results are shown in Table 2."
    },
    {
      "title": "Question Answering",
      "text": "The goal of question answering is to enable the computer to understand the questions raised by users through semantic understanding and syntax analysis, and then generate answers that meet the requirements of the questions. Because any form of LLM evaluation can be seen as question answering or switch to this format, so it is a very important means for LLMs evaluationGuo et al. (2023). We compare the performance of two large models, LLaMA 2 7B and OPT-IML 1.3B on the BoolQ dataset, using accuracy as a metric. The results are shown in Table 3."
    },
    {
      "title": "Coreference Resolution",
      "text": "Coreference resolution is to identify the entities referred to by pronouns and noun phrases in the text. It has many practical applications in natural language processing, such as information extraction, text summarization, etc. Correct parsing of reference relationships can help computers better understand text. We compares the performance of two large models, GPT-2-XL 1.5B and GPT-2 774M on the Winograd Schema Challenge data set, using accuracy as a metric. The results are shown in Table 4."
    },
    {
      "title": "Result Analysis",
      "text": "From the above experiments, it can be concluded that: the better data compression effect of LLM, the better its performance in natural language processing tasks. That is, there is a positive correlation between compression ratio and model performance. When we can effectively compress data, it means that we have captured the key characteristics and patterns of the data. This is similar to finding patterns and redundancies in the data during the model learning process. So we can say that if a large language model achieves the best lossless compression on a dataset, it will often achieve the best generalization on other datasets. Therefore, the experimental results further verify the theoretical conclusion of this paper: compression ratio can be used as a general metric to measure the performance of LLMs."
    },
    {
      "title": "5 Conclusion",
      "text": "We proposed to rank LLMs through lossless data compression in this paper. Our method measures compression ratios as a metric for generalization. We demonstrate the equivalence of compression length under arithmetic coding and LLMs pre-training goal, saving the overhead of actual compression. This further illustrates that understanding is compression, demonstrated by our experiments across challenging downstream NLP tasks. \\begin{table} \\begin{tabular}{l c} \\hline **LLM** & **Accuracy(\\%)** \\\\ \\hline Mistral 7B & 81.3 Jiang et al. (2023) \\\\ LLaMA 2 7B & 77.2 Touvron et al. (2023) \\\\ GPT-2-XL 1.5B & 50.9 Wu et al. (2023) \\\\ \\hline \\end{tabular} \\end{table} Table 2: Performance on sentence completion. \\begin{table} \\begin{tabular}{l c} \\hline **LLM** & **Accuracy(\\%)** \\\\ \\hline GPT-2-XL 1.5B & 73.3 Wu et al. (2023) \\\\ GPT-2 774M & 69.2 Trichelair et al. (2018) \\\\ \\hline \\end{tabular} \\end{table} Table 4: Performance on coreference resolution. \\begin{table} \\begin{tabular}{l c} \\hline **LLM** & **Accuracy(\\%)** \\\\ \\hline Mistral 7B & 81.3 Jiang et al. (2023) \\\\ LLaMA 2 7B & 77.2 Touvron et al. (2023) \\\\ GPT-2-XL 1.5B & 50.9 Wu et al. (2023) \\\\ \\hline \\end{tabular} \\end{table} Table 3: Performance on question answering. \\begin{table} \\begin{tabular}{l c} \\hline **LLM** & **Accuracy(\\%)** \\\\ \\hline LLaMA 2 7B & 77.4 Touvron et al. (2023) \\\\ OPT-IML 1.3B & 61.5 Iyer et al. (2023) \\\\ \\hline \\end{tabular} \\end{table} Table 3: Performance on question answering."
    },
    {
      "title": "6 Limitations",
      "text": "For NLP tasks, the experiments in this paper only used the open source version of the pre-trained language model, which was subject to computational constraints and scale limitations. Furthermore evaluation is not the end goal but the starting point. A mature evaluation system should not only provide conclusions about performance, but also provide analysis and guidance for future research and development, which is also our future research direction."
    },
    {
      "title": "7 Statement",
      "text": "We take academic integrity and research independence very seriously. Here we would like to declare that parts of this paper overlap with a published paper. Overlaps include ideas presented, experimental methods. Information about the published paper is as follows: * Title: Compression Represents Intelligence Linearly * Author: Yuzhen Huang, Jinghan Zhang, Zifei Shan, Junxian He * arXiv:2404.09937 [cs.CL] * Submission date: April 15, 2024 When we began our work, we were unaware of the existence of this published paper. Our study began on December 2023, was completed on May 2024, and was submitted on June 20, 2024."
    },
    {
      "title": "References",
      "text": "* B. Bellard (2019)Lossless data compression with neural networks. URL: https://bellard. org/uncp/nncp. pdf. Cited by: SS1. * T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020)Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1. * M. Cascella, J. Montomoli, V. Bellini, and E. Bignami (2023)Evaluating the feasibility of chatgpt in healthcare: an analysis of multiple clinical and research scenarios. Journal of medical systems47 (1), pp. 33. Cited by: SS1. * Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, et al. (2024)A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology15 (3), pp. 1-45. Cited by: SS1. * G. Deletang, A. Ruoss, P. Duquenne, E. Catt, T. Genewein, C. Mattern, J. Grau-Moya, L. K. Wenliang, M. Aitchison, L. Orseau, et al. (2023)Language modeling is compression. arXiv preprint arXiv:2309.10668. Cited by: SS1. * J. W. Gibbs (1878)On the equilibrium of heterogeneous substances. American Journal of Science3 (96), pp. 441-458. Cited by: SS1. * I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio (2014)Generative adversarial nets. Advances in neural information processing systems27, pp.. Cited by: SS1. * M. Goyal, K. Tatwawadi, S. Chandak, and I. Ochoa (2018)Deepzip: lossless data compression using recurrent neural networks. Cited by: SS1. * Z. Guo, R. Jin, C. Liu, Y. Huang, D. Shi, L. Yu, Y. Liu, J. Li, B. Xiong, D. Xiong, et al. (2023)Evaluating large language models: a comprehensive survey. arXiv preprint arXiv:2310.19736. Cited by: SS1. * C. Huang, Y. Xie, Z. Jiang, J. Lin, and M. Li (2023)Approximating human-like few-shot learning with gpt-based compression. arXiv preprint arXiv:2308.06942. Cited by: SS1. * S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, X. Li, B. O'Horo, G. Pereyra, J. Wang, C. Dewan, A. Celikyilmaz, L. Zettlemoyer, and V. Stoyanov (2023)Opt-iml: scaling language model instruction meta learning through the lens of generalization. Cited by: SS1. * A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. (2023)Mistral 7b. arXiv preprint arXiv:2310.06825. Cited by: SS1. * M. Tahmid Rahman Laskar, M. S. Bari, M. Rahman, M. A. Hossen Bhuiyan, S. Joty, and J. Xiangji Huang (2023)A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. Cited by: SS1. * Y. Li (2023)An open source data contamination report for lama series models. arXiv preprint arXiv:2310.17589. Cited by: SS1. * Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He, Z. He, Z. Liu, Z. Wu, L. Zhao, D. Zhu, X. Li, N. Qiang, D. Shen, T. Liu, and B. Ge (2023)Summary of chatgpt-related research and perspective towards thefuture of large language models. _Meta-Radiology_, 1(2):100017. * Magar and Schwartz (2022) Inbal Magar and Roy Schwartz. 2022. Data contamination: From memorization to exploitation. * Mao et al. (2022) Yu Mao, Yufei Cui, Tei-Wei Kuo, and Chun Jason Xue. 2022. Trace: A fast transformer-based general-purpose lossless compressor. In _Proceedings of the ACM Web Conference 2022_, pages 1829-1838. * OpenAI et al. (2022) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kafitan, Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashwin Nair, Reitichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Paraprita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurur, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt-4 technical report. * Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9. * Rahman and Watanabe (2023) Md. Mostafizer Rahman and Yutaka Watanabe. 2023. Chatgpt for education and research: Opportunities, threats, and strategies. _Applied Sciences_, 13(9). * Rezende and Mohamed (2015) Danilo Rezende and Shakir Mohamed. 2015. Variational inference with normalizing flows. In _International conference on machine learning_, pages 1530-1538. PMLR. * Shannon (1948) Claude Elwood Shannon. 1948. A mathematical theory of communication. _The Bell system technical journal_, 27(3):379-423. * Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language models in medicine. _Nature medicine_, 29(8):1930-1940. * Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahari, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_. Paul Trichelair, Ali Emami, Adam Trischler, Kaheer Suleman, and Jackie Chi Kit Cheung. 2018. How reasonable are common-sense reasoning tasks: A case-study on the winograd schema challenge and sway. _arXiv preprint arXiv:1811.01778_. * Oord et al. (2016) Aaron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. 2016. Pixel recurrent neural networks. In _International conference on machine learning_, pages 1747-1756. PMLR. * Wang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. Glue: A multi-task benchmark and analysis platform for natural language understanding. * Wu et al. (2023) Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2023. Lamini-lm: A diverse herd of distilled models from large-scale instructions. _arXiv preprint arXiv:2304.14402_. * Yang et al. (2023) Yibo Yang, Stephan Mandt, Lucas Theis, et al. 2023. An introduction to neural data compression. _Foundations and Trends(r) in Computer Graphics and Vision_, 15(2):113-200. * Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models."
    }
  ]
}