{
  "title": "Opportunities for Large Language Models and Discourse in Engineering Design",
  "authors": [
    "Jan Göpfert",
    "Jann M Weinand",
    "Patrick Kuckertz",
    "Detlef Stolten"
  ],
  "abstract": "\n In recent years, large language models have achieved breakthroughs on a wide range of benchmarks in natural language processing and continue to increase in performance. Recently, the advances of large language models have raised interest outside the natural language processing community and could have a large impact on daily life. In this paper, we pose the question: How will large language models and other foundation models shape the future product development process? We provide the reader with an overview of the subject by summarizing both recent advances in natural language processing and the use of information technology in the engineering design process. We argue that discourse should be regarded as the core of engineering design processes, and therefore should be represented in a digital artifact. On this basis, we describe how foundation models such as large language models could contribute to the design discourse by automating parts thereof that involve creativity and reasoning, and were previously reserved for humans. We describe how simulations, experiments, topology optimizations, and other process steps can be integrated into a machine-actionable, discourse-centric design process. Finally, we outline the future research that will be necessary for the implementation of the conceptualized framework. \n",
  "references": [
    {
      "id": null,
      "title": "Opportunities for Large Language Models and Discourse in Engineering Design",
      "authors": [
        "Jan Göpfert",
        "Jann M Weinand",
        "Patrick Kuckertz",
        "Detlef Stolten"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
      "authors": [
        "A Wang",
        "Y Pruksachatkun",
        "N Nangia",
        "A Singh",
        "J Michael",
        "F Hill",
        "O Levy",
        "S Bowman"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Vision-Language Pre-Training: Basics, Recent Advances, and Future Trends",
      "authors": [
        "Z Gan",
        "L Li",
        "C Li",
        "L Wang",
        "Z Liu",
        "J Gao"
      ],
      "year": "2022",
      "venue": "Found. Trends Comput. Graph. Vis",
      "doi": "10.1561/0600000105"
    },
    {
      "id": "b2",
      "title": "PaLM-E: An Embodied Multimodal Language Model",
      "authors": [
        "D Driess",
        "F Xia",
        "M S M Sajjadi",
        "C Lynch",
        "A Chowdhery",
        "B Ichter",
        "A Wahid",
        "J Tompson",
        "Q Vuong",
        "T Yu",
        "W Huang",
        "Y Chebotar",
        "P Sermanet",
        "D Duckworth",
        "S Levine",
        "V Vanhoucke",
        "K Hausman",
        "M Toussaint",
        "K Greff",
        "A Zeng",
        "I Mordatch",
        "P Florence"
      ],
      "year": "2023",
      "venue": "PaLM-E: An Embodied Multimodal Language Model",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Foundation models for generalist medical artificial intelligence",
      "authors": [
        "M Moor",
        "O Banerjee",
        "Z S H Abad",
        "H M Krumholz",
        "J Leskovec",
        "E J Topol",
        "P Rajpurkar"
      ],
      "year": "2023",
      "venue": "Nature",
      "doi": "10.1038/s41586-023-05881-4"
    },
    {
      "id": "b4",
      "title": "Natural language processing models that automate programming will transform chemistry research and teaching",
      "authors": [
        "M Hocky",
        "G",
        "D White",
        "A"
      ],
      "year": "2022",
      "venue": "Digital Discovery",
      "doi": "10.1039/D1DD00009H"
    },
    {
      "id": "b5",
      "title": "Human Language Understanding & Reasoning",
      "authors": [
        "C D Manning"
      ],
      "year": "2022",
      "venue": "Daedalus",
      "doi": "10.1162/daed_a_01905"
    },
    {
      "id": "b6",
      "title": "",
      "authors": [
        "Z S Harris"
      ],
      "year": "1954",
      "venue": "Distributional Structure. WORD",
      "doi": "10.1080/00437956.1954.11659520"
    },
    {
      "id": "b7",
      "title": "Efficient Estimation of Word Representations in Vector Space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient Estimation of Word Representations in Vector Space",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Distributed Representations of Words and Phrases and their Compositionality",
      "authors": [
        "T Mikolov",
        "I Sutskever",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Distributed Representations of Words and Phrases and their Compositionality",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "GloVe: Global Vectors for Word Representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.3115/v1/D14-1162"
    },
    {
      "id": "b10",
      "title": "Deep contextualized word representations",
      "authors": [
        "M E Peters",
        "M Neumann",
        "M Iyyer",
        "M Gardner",
        "C Clark",
        "K Lee",
        "L Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Deep contextualized word representations",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "id": "b12",
      "title": "Attention is All you Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A N Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Improving Language Understanding by",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "2018",
      "venue": "Improving Language Understanding by",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Language Models are Unsupervised Multitask",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "Language Models are Unsupervised Multitask",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J D Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "",
      "authors": [
        "B Workshop",
        "T L Scao",
        "A Fan",
        "C Akiki",
        "E Pavlick",
        "S Ilić",
        "D Hesslow",
        "R Castagné",
        "A S Luccioni",
        "F Yvon",
        "M Gallé",
        "J Tow",
        "A M Rush",
        "S Biderman",
        "A Webson",
        "P S Ammanamanchi",
        "T Wang",
        "B Sagot",
        "N Muennighoff",
        "A V Moral",
        "O Ruwase",
        "R Bawden",
        "S Bekman",
        "A Mcmillan-Major",
        "I Beltagy",
        "H Nguyen",
        "L Saulnier",
        "S Tan",
        "P O Suarez",
        "V Sanh",
        "H Laurençon",
        "Y Jernite",
        "J Launay",
        "M Mitchell",
        "C Raffel",
        "A Gokaslan",
        "A Simhi",
        "A Soroa",
        "A F Aji",
        "A Alfassy",
        "A Rogers",
        "A K Nitzav",
        "C Xu",
        "C Mou",
        "C Emezue",
        "C Klamm",
        "C Leong",
        "D Strien",
        "D I Adelani",
        "D Radev",
        "E G Ponferrada",
        "E Levkovizh",
        "E Kim",
        "E B Natan",
        "F De Toni",
        "G Dupont",
        "G Kruszewski",
        "G Pistilli",
        "H Elsahar",
        "H Benyamina",
        "H Tran",
        "I Yu",
        "I Abdulmumin",
        "I Johnson",
        "I Gonzalez-Dios",
        "J Rosa",
        "J Chim",
        "J Dodge",
        "J Zhu",
        "J Chang",
        "J Frohberg",
        "J Tobing",
        "J Bhattacharjee",
        "K Almubarak",
        "K Chen",
        "K Lo",
        "L Von Werra",
        "L Weber",
        "L Phan",
        "L B Tanguy",
        "L Dey",
        "M Muñoz",
        "M R Masoud",
        "M Grandury",
        "M Šaško",
        "M Huang",
        "M Coavoux",
        "M Singh",
        "M Jiang",
        "M T",
        "-J Vu",
        "M C Jauhar",
        "M A Ghaleb",
        "M Subramani",
        "N Kassner",
        "N Khamis",
        "N Nguyen",
        "O Espejel",
        "O Gibert",
        "O Villegas",
        "P Henderson",
        "P Colombo",
        "P Amuok",
        "P Lhoest",
        "Q Harliman",
        "R Bommasani",
        "R López",
        "R L"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar",
        "A Rodriguez",
        "A Joulin",
        "E Grave",
        "G Lample"
      ],
      "year": "2023",
      "venue": "LLaMA: Open and Efficient Foundation Language Models",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P J Liu"
      ],
      "year": "2020",
      "venue": "Journal of Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Scaling Instruction-Finetuned Language Models",
      "authors": [
        "H W Chung",
        "L Hou",
        "S Longpre",
        "B Zoph",
        "Y Tay",
        "W Fedus",
        "Y Li",
        "X Wang",
        "M Dehghani",
        "S Brahma",
        "A Webson",
        "S S Gu",
        "Z Dai",
        "M Suzgun",
        "X Chen",
        "A Chowdhery",
        "A Castro-Ros",
        "M Pellat",
        "K Robinson",
        "D Valter",
        "S Narang",
        "G Mishra",
        "A Yu",
        "V Zhao",
        "Y Huang",
        "A Dai",
        "H Yu",
        "S Petrov",
        "E H Chi",
        "J Dean",
        "J Devlin",
        "A Roberts",
        "D Zhou",
        "Q V Le",
        "J Wei"
      ],
      "year": "2022",
      "venue": "Scaling Instruction-Finetuned Language Models",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "UL2: Unifying Language Learning Paradigms",
      "authors": [
        "Y Tay",
        "M Dehghani",
        "V Q Tran",
        "X Garcia",
        "J Wei",
        "X Wang",
        "H W Chung",
        "S Shakeri",
        "D Bahri",
        "T Schuster",
        "H S Zheng",
        "D Zhou",
        "N Houlsby",
        "D Metzler"
      ],
      "year": "2023",
      "venue": "UL2: Unifying Language Learning Paradigms",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Emergent Abilities of Large Language Models",
      "authors": [
        "J Wei",
        "Y Tay",
        "R Bommasani",
        "C Raffel",
        "B Zoph",
        "S Borgeaud",
        "D Yogatama",
        "M Bosma",
        "D Zhou",
        "D Metzler",
        "E H Chi",
        "T Hashimoto",
        "O Vinyals",
        "P Liang",
        "J Dean",
        "W Fedus"
      ],
      "year": "2022",
      "venue": "Emergent Abilities of Large Language Models",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "B Ichter",
        "F Xia",
        "E Chi",
        "Q Le",
        "D Zhou"
      ],
      "year": "2023",
      "venue": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Are Emergent Abilities of Large Language Models a Mirage?",
      "authors": [
        "R Schaeffer",
        "B Miranda",
        "S Koyejo"
      ],
      "year": "2023",
      "venue": "Are Emergent Abilities of Large Language Models a Mirage?",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray",
        "J Schulman",
        "J Hilton",
        "F Kelton",
        "L Miller",
        "M Simens",
        "A Askell",
        "P Welinder",
        "P F Christiano",
        "J Leike",
        "R Lowe"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "authors": [
        "T Schick",
        "J Dwivedi-Yu",
        "R Dessì",
        "R Raileanu",
        "M Lomeli",
        "L Zettlemoyer",
        "N Cancedda",
        "T Scialom"
      ],
      "year": "2023",
      "venue": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": [
        "S Yao",
        "J Zhao",
        "D Yu",
        "N Du",
        "I Shafran",
        "K Narasimhan",
        "Y Cao"
      ],
      "year": "2023",
      "venue": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
      "authors": [
        "N Shinn",
        "B Labash",
        "A Gopinath"
      ],
      "year": "2023",
      "venue": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "On the Opportunities and Risks of Foundation Models",
      "authors": [
        "R Bommasani",
        "D A Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M S Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill",
        "E Brynjolfsson",
        "S Buch",
        "D Card",
        "R Castellon",
        "N Chatterji",
        "A Chen",
        "K Creel",
        "J Q Davis",
        "D Demszky",
        "C Donahue",
        "M Doumbouya",
        "E Durmus",
        "S Ermon",
        "J Etchemendy",
        "K Ethayarajh",
        "L Fei-Fei",
        "C Finn",
        "T Gale",
        "L Gillespie",
        "K Goel",
        "N Goodman",
        "S Grossman",
        "N Guha",
        "T Hashimoto",
        "P Henderson",
        "J Hewitt",
        "D E Ho",
        "J Hong",
        "K Hsu",
        "J Huang",
        "T Icard",
        "S Jain",
        "D Jurafsky",
        "P Kalluri",
        "S Karamcheti",
        "G Keeling",
        "F Khani",
        "O Khattab",
        "P W Koh",
        "M Krass",
        "R Krishna",
        "R Kuditipudi",
        "A Kumar",
        "F Ladhak",
        "M Lee",
        "T Lee",
        "J Leskovec",
        "I Levent",
        "X L Li",
        "X Li",
        "T Ma",
        "A Malik",
        "C D Manning",
        "S Mirchandani",
        "E Mitchell",
        "Z Munyikwa",
        "S Nair",
        "A Narayan",
        "D Narayanan",
        "B Newman",
        "A Nie",
        "J C Niebles",
        "H Nilforoshan",
        "J Nyarko",
        "G Ogut",
        "L Orr",
        "I Papadimitriou",
        "J S Park",
        "C Piech",
        "E Portelance",
        "C Potts",
        "A Raghunathan",
        "R Reich",
        "H Ren",
        "F Rong",
        "Y Roohani",
        "C Ruiz",
        "J Ryan",
        "C Ré",
        "D Sadigh",
        "S Sagawa",
        "K Santhanam",
        "A Shih",
        "K Srinivasan",
        "A Tamkin",
        "R Taori",
        "A W Thomas",
        "F Tramèr",
        "R E Wang",
        "W Wang",
        "B Wu",
        "J Wu",
        "Y Wu",
        "S M Xie",
        "M Yasunaga",
        "J You",
        "M Zaharia",
        "M Zhang",
        "T Zhang",
        "X Zhang",
        "Y Zhang",
        "L Zheng",
        "K Zhou",
        "P Liang"
      ],
      "year": "2022",
      "venue": "On the Opportunities and Risks of Foundation Models",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Deep Generative Models in Engineering Design: A Review",
      "authors": [
        "L Regenwetter",
        "A H Nobari",
        "F Ahmed"
      ],
      "year": "2022",
      "venue": "Journal of Mechanical Design",
      "doi": "10.1115/1.4053859"
    },
    {
      "id": "b31",
      "title": "Design Strategy Network: A Deep Hierarchical Framework to Represent Generative Design Strategies in Complex Action Spaces",
      "authors": [
        "A Raina",
        "J Cagan",
        "C Mccomb"
      ],
      "year": "2021",
      "venue": "Journal of Mechanical Design",
      "doi": "10.1115/1.4052566"
    },
    {
      "id": "b32",
      "title": "Human Versus Artificial Intelligence: A Data-Driven Approach to Real-Time Process Management During Complex Engineering Design",
      "authors": [
        "J T Gyory",
        "N F Soria Zurita",
        "J Martin",
        "C Balon",
        "C Mccomb",
        "K Kotovsky",
        "J Cagan"
      ],
      "year": "2021",
      "venue": "Journal of Mechanical Design",
      "doi": "10.1115/1.4052488"
    },
    {
      "id": "b33",
      "title": "Comparing the Impacts on Team Behaviors Between Artificial Intelligence and Human Process Management in Interdisciplinary Design Teams",
      "authors": [
        "J T Gyory",
        "K Kotovsky",
        "C Mccomb",
        "J Cagan"
      ],
      "year": "2022",
      "venue": "Journal of Mechanical Design",
      "doi": "10.1115/1.4054723"
    },
    {
      "id": "b34",
      "title": "TechNet: Technology semantic network based on patent data",
      "authors": [
        "S Sarica",
        "J Luo",
        "K L Wood"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2019.112995"
    },
    {
      "id": "b35",
      "title": "TechWord: Development of a technology lexical database for structuring textual technology information based on natural language processing",
      "authors": [
        "H Jang",
        "Y Jeong",
        "B Yoon"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2020.114042"
    },
    {
      "id": "b36",
      "title": "A Data-Driven Text Mining and Semantic Network Analysis for Design Information Retrieval",
      "authors": [
        "F Shi",
        "L Chen",
        "J Han",
        "P Childs"
      ],
      "year": "2017",
      "venue": "Journal of Mechanical Design",
      "doi": "10.1115/1.4037649"
    },
    {
      "id": "b37",
      "title": "Stopwords in technical language processing",
      "authors": [
        "S Sarica",
        "J Luo"
      ],
      "year": "2021",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0254937"
    },
    {
      "id": "b38",
      "title": "OntoCAPE-A (re)usable ontology for computer-aided process engineering",
      "authors": [
        "J Morbach",
        "A Wiesner",
        "W Marquardt"
      ],
      "year": "2009",
      "venue": "Computers & Chemical Engineering",
      "doi": "10.1016/j.compchemeng.2009.01.019"
    },
    {
      "id": "b39",
      "title": "Introducing the Open Energy Ontology: Enhancing data interpretation and interfacing in energy systems analysis",
      "authors": [
        "M Booshehri",
        "L Emele",
        "S Flügel",
        "H Förster",
        "J Frey",
        "U Frey",
        "M Glauer",
        "J Hastings",
        "C Hofmann",
        "C Hoyer-Klick",
        "L Hülk",
        "A Kleinau",
        "K Knosala",
        "L Kotzur",
        "P Kuckertz",
        "T Mossakowski",
        "C Muschner",
        "F Neuhaus",
        "M Pehl",
        "M Robinius",
        "V Sehn",
        "M Stappel"
      ],
      "year": "2021",
      "venue": "Energy and AI",
      "doi": "10.1016/j.egyai.2021.100074"
    },
    {
      "id": "b40",
      "title": "Formal ontologies in manufacturing",
      "authors": [
        "E M Sanfilippo",
        "Y Kitamura",
        "R I M Young"
      ],
      "year": "2019",
      "venue": "Applied Ontology",
      "doi": "10.3233/AO-190209"
    },
    {
      "id": "b41",
      "title": "Semantic Networks for Engineering Design: A Survey",
      "authors": [
        "J Han",
        "S Sarica",
        "F Shi",
        "J Luo"
      ],
      "year": "2021",
      "venue": "Proceedings of the Design Society",
      "doi": "10.1017/pds.2021.523"
    },
    {
      "id": "b42",
      "title": "Engineering Knowledge Graph From Patent Database",
      "authors": [
        "L Siddharth",
        "L T M Blessing",
        "K L Wood",
        "J Luo"
      ],
      "year": "2021",
      "venue": "Journal of Computing and Information Science in Engineering",
      "doi": "10.1115/1.4052293"
    },
    {
      "id": "b43",
      "title": "Natural language processing in-and-for design research",
      "authors": [
        "L Siddharth",
        "L Blessing",
        "J Luo"
      ],
      "year": "2022",
      "venue": "Design Science",
      "doi": "10.1017/dsj.2022.16"
    },
    {
      "id": "b44",
      "title": "Generative Pre-Trained Transformer for Design Concept Generation: An Exploration",
      "authors": [
        "Q Zhu",
        "J Luo"
      ],
      "year": "2022",
      "venue": "Proceedings of the Design Society",
      "doi": "10.1017/pds.2022.185"
    },
    {
      "id": "b45",
      "title": "Generative Transformers for Design Concept Generation",
      "authors": [
        "Q Zhu",
        "J Luo"
      ],
      "year": "2023",
      "venue": "Science in Engineering",
      "doi": "10.1115/1.4056220"
    },
    {
      "id": "b46",
      "title": "Biologically Inspired Design Concept Generation Using Generative Pre-Trained Transformers",
      "authors": [
        "Q Zhu",
        "X Zhang",
        "J Luo"
      ],
      "year": "2023",
      "venue": "Journal of Mechanical Design",
      "doi": "10.1115/1.4056598"
    },
    {
      "id": "b47",
      "title": "Conceptual Design Generation Using Large Language Models",
      "authors": [
        "K Ma",
        "D Grandi",
        "C Mccomb",
        "K Goucher-Lambert"
      ],
      "year": "2023",
      "venue": "Conceptual Design Generation Using Large Language Models",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Leveraging End-User Data for Enhanced Design Concept Evaluation: A Multimodal Deep Regression Model",
      "authors": [
        "C Yuan",
        "T Marion",
        "M Moghaddam"
      ],
      "year": "2021",
      "venue": "Journal of Mechanical Design",
      "doi": "10.1115/1.4052366"
    },
    {
      "id": "b49",
      "title": "Attention-Enhanced Multimodal Learning for Conceptual Design Evaluations",
      "authors": [
        "B Song",
        "S Miller",
        "F Ahmed"
      ],
      "year": "2023",
      "venue": "Journal of Mechanical Design",
      "doi": "10.1115/1.4056669"
    },
    {
      "id": "b50",
      "title": "Multi-modal Machine Learning in Engineering Design: A Review and Future Directions",
      "authors": [
        "B Song",
        "R Zhou",
        "F Ahmed"
      ],
      "year": "2023",
      "venue": "Multi-modal Machine Learning in Engineering Design: A Review and Future Directions",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task",
      "authors": [
        "K Li",
        "A K Hopkins",
        "D Bau",
        "F Viégas",
        "H Pfister",
        "M Wattenberg"
      ],
      "year": "2023",
      "venue": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Probing classifiers: Promises, shortcomings, and advances",
      "authors": [
        "Y Belinkov"
      ],
      "year": "2022",
      "venue": "Computational Linguistics",
      "doi": "10.1162/coli_a_00422"
    },
    {
      "id": "b53",
      "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
      "authors": [
        "M Nye",
        "A J Andreassen",
        "G Gur-Ari",
        "H Michalewski",
        "J Austin",
        "D Bieber",
        "D Dohan",
        "A Lewkowycz",
        "M Bosma",
        "D Luan",
        "C Sutton",
        "A Odena"
      ],
      "year": "2021",
      "venue": "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Faithful Reasoning Using Large Language Models",
      "authors": [
        "A Creswell",
        "M Shanahan"
      ],
      "year": "2022",
      "venue": "Faithful Reasoning Using Large Language Models",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Successful individual approaches in engineering design",
      "authors": [
        "G Fricke"
      ],
      "year": "1996",
      "venue": "Research in Engineering Design",
      "doi": "10.1007/BF01608350"
    },
    {
      "id": "b56",
      "title": "Engineering Design",
      "authors": [
        "G Pahl",
        "W Beitz",
        "J Feldhusen",
        "K.-H Grote"
      ],
      "year": "2007",
      "venue": "Engineering Design",
      "doi": "10.1007/978-1-84628-319-2"
    },
    {
      "id": "b57",
      "title": "Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design",
      "authors": [
        "L Regenwetter",
        "A Srivastava",
        "D Gutfreund",
        "F Ahmed"
      ],
      "year": "2023",
      "venue": "Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Opportunities For Large Language Models And Discourse In Engineering Design",
      "text": "Jan Gopfert\\({}^{1,2*}\\), Jann M. Weinand\\({}^{1}\\), Patrick Kuckertz\\({}^{1}\\), Detlef Stolten\\({}^{1,2}\\) \\({}^{1*}\\)Institute of Energy and Climate Research - Techno-economic Systems Analysis (IEK-3), Forschungszentrum Julich GmbH, Julich, 52425, Germany. \\({}^{2}\\)Chair for Fuel Cells, RWTH Aachen University, Faculty of Mechanical Engineering, Aachen, 52062, Germany. *Corresponding author(s). E-mail(s): j.goepfert@fz-juelich.de; Contributing authors: j.weinand@fz-juelich.de; p.kuckertz@fz-juelich.de; d.stolten@fz-juelich.de;"
    },
    {
      "title": "Abstract",
      "text": "In recent years, large language models have achieved breakthroughs on a wide range of benchmarks in natural language processing and continue to increase in performance. Recently, the advances of large language models have raised interest outside the natural language processing community and could have a large impact on daily life. In this paper, we pose the question: How will large language models and other foundation models shape the future product development process? We provide the reader with an overview of the subject by summarizing both recent advances in natural language processing and the use of information technology in the engineering design process. We argue that discourse should be regarded as the core of engineering design processes, and therefore should be represented in a digital artifact. On this basis, we describe how foundation models such as large language models could contribute to the design discourse by automating parts thereof that involve creativity and reasoning, and were previously reserved for humans. We describe how simulations, experiments, topology optimizations, and other process steps can be integrated into a machine-actionable, discourse-centric design process. Finally, we outline the future research that will be necessary for the implementation of the conceptualized framework. **Keywords:** product development process, conceptual design, design methodology, design generation, natural language processing, foundation models, multi-modal models"
    },
    {
      "title": "1 Introduction",
      "text": "Large language models (LLMs) have transformed the field of natural language processing (NLP) and increasingly have an impact outside of academia. LLMs already dominate almost every benchmark in natural language understanding (e.g., [1]) and current research focuses on extending them by means of other modalities such as images, videos, or sensor signals [2, 3]. Many research fields, such as medicine [4] and chemistry [5], are discussing the future implications of these models for their field. The engineering sciences are a knowledge-intensive domain, which is likely to experience great progress via the adaptation of recent methods developed in the NLP community. In this paper, we argue that foundation models such as LLMs can be used for creative reasoning tasks in the engineering design process, complementing and integrating existing computational methods such as topology optimization. First, we provide engineers with a summary of the recent advances in NLP and outline which aspects of engineering design have been digitized thus far (Section 2). In Section 3, we place goal-oriented, argumentative discourse at the center of the product development process (see Figure 1) and propose making the reasoning steps explicit in the form of a new digital artifact. On this basis, we describe how LLMs and multi-modal foundation models can assist in the design discourse (Section 4) and outline interesting directions of future research (Section 5). The presented ideas are transferable to other contexts in which creativity and reasoning play an important role, such as scientific discovery in general. Figure 1: The engineering design process depicted as a goal-oriented, argumentative discourse formed by inter- and intrapersonal communication in which machines can participate. As part of this discourse, external actions are invoked that in turn inform it."
    },
    {
      "title": "2 Background",
      "text": "As this article bridges multiple domains, not all of which may be familiar to most readers, we provide a thorough background for LLMs and foundation models as well as the digitization of engineering design."
    },
    {
      "title": "Llms And Foundation Models",
      "text": "In its early years, NLP applications relied on hard-coded rules. When the body of digitally-available text increased statistical methods became more prominent. Roughly from 2013 onward, NLP began to be dominated by machine-learning methods, in particular deep learning ones. [6] Building on the distributional hypothesis, which states that words tend to have similar meanings if they occur in similar contexts [7], words (and other text units) have been represented in dense multi-dimensional vector representations using self-supervised learning [8, 9, 10]. These word embeddings allow for the similarity between words to be calculated based on vector distances, and as they constitute a rich feature, they have been a popular component of many NLP pipelines. However, these word embeddings have been static; that is, they are context-independent (e.g., the word'seal' has the same vector representation regardless of whether it refers to the animal or machine element). Contextual word embeddings based on language models resulted in significant improvements over a wide range of NLP benchmarks [11, 12]. In 2017, the transformer architecture was proposed [13], and has been the dominant neural network architecture for language models ever since, replacing previous approaches based on feed-forward models, recurrent neural networks, or long short-term memory networks. The transformer architecture consists of an encoder and decoder. Various large language models have been proposed utilizing either the encoder (e.g., the BERT family of models [14, 12]), decoder (e.g., the GPT [15, 16, 17], BLOOM [18] or LLaMA [19] family of models), or both (e.g., the T5 and UL2 model family [20, 21, 22]) of the transformer architecture. Language models are typically trained on predicting the next token or masked tokens in a sequence in which a token can be a word, character, or a sub-word unit, with most models using sub-word-tokenization. Because models can be trained on this objective in a self-supervised setting, large unlabeled corpora (such as Wikipedia, book corpora, and Common Crawl data) can be used for training. Increasing the amount of data, parameters, and computation further, several emergent abilities of LLMs have been discovered [23]. Sufficiently large models are capable of in-context learning [17] and chain-of-thought reasoning [24]. Previously requiring fine-tuning on downstream tasks (e.g., named entity recognition or question answering), LLMs can now yield decent performance on new tasks by merely including a task description and few examples in the input [17]. Whether these abilities emerge suddenly in a sharp transition at a certain scale or in a gradual and predictable way is still subject of scientific debate [25]. With instruction fine-tuning, the generated responses to a prompt aligned more with the user's intent, removing the need for careful prompt selection [26]. To solve tasks beyond the capabilities of LLMs in isolation, they have been trained to use tools for which the input and output can be represented as text,such as a calculator or Python console [27]. Further work on the agent-like behavior of LLMs combines reasoning and acting capabilities [28] or add self-reflection capacities [29]. The NLP community saw great advances in a wide range of benchmarks using LLMs. In addition to models operating on textual input alone, recently, multi-modal models that also process other modalities such as images, videos, and/ or sensor-signals have become a focus of research [2, 3]. Abstracting the LLM concept to other modalities, training procedures, and so forth, the term foundation model was coined [30]."
    },
    {
      "title": "Computational Engineering Design",
      "text": "Today, the digitization of engineering design is well advanced. In the past, technical drawing was performed on drawing boards until software for computer-aided design (CAD) was developed in the second half of the twentieth century, which is generally adopted today. Currently, we utilize finite element analysis, topology optimization, design-support tools for additive manufacturing, and more. With model-based systems engineering and digital twins, the product development process became centered around digital models. Virtual and augmented reality enables visualization and interaction with designs. Due to the breadth of the field, only a brief overview of advances in computational engineering design can be given here, focusing on design generation, design strategy learning, and NLP (in particular LLMs) for engineering design. Generative adversarial networks, feedforward neural networks, variational autoencoders, as well as reinforcement learning systems have been used in design-related generation tasks such as topology optimization or shape synthesis based on visual modalities (e.g., images, voxels, point clouds, etc.) [31]. Other work focuses on learning design strategies. Given a state in solving a truss design problem, Raina et al. [32] predicts what actions humans perform next. Gyory et al. [33, 34] analyze real time data of design teams to suggest measures from a predefined list if the communication or action frequency appears to be too low. Lexical databases [35, 36, 37] and stopword lists [38] for technological vocabulary and jargon have been proposed, as have engineering-related ontologies [39, 40, 41]. With ontologies come knowledge graphs. However, there has been a lack of specialized engineering knowledge graphs thus far [42]. Only recently, Siddharth et al. [43] build a knowledge graph using patent claims. With a trend towards industry 4.0 and digital twins, the subject of the semantic representation of technological knowledge will probably be increasingly addressed in the future. NLP methods, which have been less applied in the engineering sciences compared to the biomedical and material ones, have become increasingly popular recently. In design research, NLP has been applied to requirements extraction, ontology construction, patent analysis, and more [44]. Using foundation models such as LLMs or pre-trained multi-modal models in the engineering design process is a recent and unexplored topic. Several studies have experimented with using LLMs to provide designers with inspirational stimuli for ideation. In two explorative studies, Zhu and Luo [45, 46] prompted GPT-2 and -3 to generate design concepts (text-to-text) based on the description of either a concept, problem, or analogy in both a fine-tuning and few-shot learning setting. Similarly, Zhu et al. [47] fine-tune GPT-3 for bio-inspired design concept generation. Ma et al. [48] compare design solutions generated with GPT-3 with crowdsourced ones. Other work has focus on design concept evaluation combining a pre-trained language model (BERT) and image models in a multi-modal one [49, 50]. Song et al. [51] provided an extensive overview of multi-modal machine learning for engineering design. They outline possible applications, but focus on lower level tasks such as text-to-shape or shape-to-text synthesis. Orthogonal to the prior work, we concentrate on the design process itself as a complex, iterative, and dynamic reasoning process and situate recent advances in NLP and machine learning in a superordinate framework."
    },
    {
      "title": "3 Depicting The Design Process As A Goal-Oriented, Argumentative Discourse",
      "text": "We have digital artifacts for shapes, assembly processes, stress distributions, flow patterns, and more. Until now, however, computer-aided engineering, as practiced in industry, has not included the creative and argumentative process of the product development process itself. In the following, we argue that this process could be digitized and partially automatized next, and outline how this can be achieved. Many steps in the product development process are performed using computation and are not based on human thought alone. However, humans are needed to integrate these computational processes, be they calculations, simulations, or optimizations, into a meaningful superordinate product development process. Human thought and world knowledge is required to reduce the solution space in advance and come up with original ideas that have not been modeled to be computationally accessible before. For example, when a bicycle is designed, the starting point is not a blank slate but an idea of how a bicycle looks and how it has worked well for over a century. If a standardized aerodynamic tube shape across bicycle manufactures is proposed, it is unlikely that this idea originated from a numerical optimization. Instead, background knowledge and the ability to think and reason are used. Solving engineering problems requires an argumentative discourse. As such, argumentation is inherent to the product development process. Experiments and calculations, etc., inform the discourse to provide necessary information. Nevertheless, argumentation is rarely given a lot of attention, perhaps because it is hidden, as it is typically not made explicit in a readable or visual representation. Having described that a goal-driven, argumentative discourse is at the core of the design process, we argue that it should be represented as a digital artifact. Humans communicate, argue, and reason using natural language. Hence, the argumentative discourse can be largely represented in textual form. Many parts of the design process, however, cannot be represented as text. Therefore, we distinguish the design discourse from external actions (such as performing an experiment or simulation) and other engineering artifacts (such as a drawing, or 3D model). We formulate that external actions are invoked from within the design discourse and in turn inform the discourse, either directly or indirectly, by yielding other engineering artifacts that inform the design discourse (see Figure 1). Representing the argumentative discourse as a digital artifact would improve the documentation of the design process. Instead of only archiving the results of process steps (e.g., CAD files or the results of simulation runs), the reasoning process is documented and hence archivable. For a past development process to be efficiently used for the development of a new product generation, past decisions and alternatives must be accessible. Having the reasoning process explicitly documented makes past design decisions traceable. Furthermore, making the reasoning process explicit could improve collective reasoning and therefore collaborative design. Finally, it would allow for machines to participate in the reasoning process, which the next section covers."
    },
    {
      "title": "4 Foundation Models As Interlocutors In The Design Discourse",
      "text": "It is apparent that LLMs and other foundation models will (and already do) reshape how we code, write, and search for information. Beyond that, we believe that they can be directly involved in the creative and argumentative design process, and so complement generative design and topology optimization tools that are already able to automatize large parts of design exploration and optimization. An example of a design discourse involving humans and machines is depicted in Figure 2. In the previous section, we argued that explicitly representing the argumentative discourse of the design process as a digital artifact results in many advantages. One of these is the potential for machines to participate in the reasoning process. LLMs and related multi-modal models have several characteristics that suggest that they can be successfully applied for this purpose. LLMs and related multi-modal models can input and output natural language. This facilitates their integration into the argumentative discourse, which to a large extent is conducted in natural language anyway, be it in intra- or interpersonal communication. World knowledge is important both to interact with humans, where a certain common ground is required, and for reducing the solution space and coming up with creative solutions. Preliminary evidence suggests that LLMs possess rich representations of the world despite being trained on simple objectives [52]. It is evident that humans are constrained by their knowledge when designing by analogy or biomimicry. In contrast, LLMs can accumulate wide-ranging knowledge during training. LLMs of sufficient size display strong results on various tasks involving reasoning and are able to perform step-by-step reasoning [24]. Multi-modal models can operate on various forms of design representations, which is important, because throughout the product development process, designers utilize different types of representations of their designs (i.a., text, tables, sketches, and 3D models) [51]. Finally, many tasks in the engineering design process cannot be solved by means of pure thought but require specialized engineering software and databases (i.a., CAD and simulation software, patent and material databases). Recent work shows that LLMs can be trained to interact with APIs (including the decision of when to call which API with which arguments) in a self-supervised setting [27]. We conclude that these models are fundamentally applicable to the purpose of assisting in the engineering design discourse. Nevertheless, a single call to a model will not be of great utility; instead, the models must be embedded within a framework to solve complex engineering tasks. In the next section, we outline aspects of such a framework and simultaneously highlight promising future research directions. Figure 2: An example of a design discourse. Note that the system outputs should clarify the ideas presented here and are not real outputs."
    },
    {
      "title": "5 Recommendations For Future Research",
      "text": "Many puzzle pieces for the outlined transformation of the engineering design process are at hand. However, several aspects, which have not been sufficiently researched, are likely to be necessary for a successful implementation of the proposed concept, namely: _Formalizing the engineering design discourse within a framework of stages and components._ Although approaches to interpret how outputs are formed in deep neural network models (such as probing [53]) exist, in practice, LLMs largely constitute black boxes. To increase the interpretability and accuracy of LLMs, approaches such as scratchpad [54] or chain-of-thought prompting [24] steer the models towards the generation of intermediate steps. Complementary to the aforementioned approaches of generating intermediate steps, models can be guided to imitate certain patterns of thinking or to follow a logical flow by embedding calls to LLMs into a framework with a pre-defined causal structure. In such frameworks \"querying a language model becomes a computational primitive.\" [55] For example, answering questions in steps adhering to formal logic yields interpretable reasoning traces and reduces the \"hallucination\" of facts [55]. Similarly, formalizing the engineering design discourse within a framework of stages and recurring components will help safeguard the models and increases the detail at which reasoning processes can be documented and verified. Furthermore, it contributes to the clarity of the discourse and makes LLMs more controllable, in that humans have more points of intervention, allowing them to provide more fine-grained guidance. For defining such a framework, the extensive literature on design research can assist (e.g., [56, 57]). _Creating machine-actionable interfaces for engineering software._ We noted that using external tools via APIs with LLMs is already an active area of research. The tools invoked in the design discourse could include specialized engineering software such as topology optimization. However, for today's LLMs, the tools are required to provide a textual interface; that is, both inputs and outputs are represented as text [27]. Therefore, the interfaces of specialized engineering software should be adapted according to the models' requirements. Note that the requirements for an interface are bound to change with multi-modal models. _Learning representations better corresponding to skills required for engineering._ Good spatial imagination, solid engineering knowledge, and high heuristic competence are skills that correlate with higher quality engineering design solutions [56]. For foundation models to fulfill a supportive role in the engineering design process, the learned representations should in turn correlate to a high degree with these skills. Therefore, the proportion of technical documents (such as patents or specialist books) in the pre-training corpus can be increased. However, although LLMs compress large amounts of knowledge in their weights, a representation corresponding to good spatial imagination and causal understanding of physical processes is unlikely to be learned from textalone. With a trend towards multi-modal models, incorporating images, videos, and other modalities, we see this problem as likely to be addressed in the near future. Additionally, strengthening the reasoning capabilities of LLMs is a promising and growing research direction. _Specifying evaluation metrics and datasets._ Prior work has expressed the need for design-specific metrics for evaluating deep generative models in design synthesis [45, 48, 51, 58]. Similarly, design-specific metrics are likely to be required to evaluate the performance of frameworks to assist in the argumentative design discourse. In cases where ground truth is not required, more complex assessments, for which there is not yet a calculable metric, but for which human evaluators are required, such as the assessment of usefulness and feasibility [48], could possibly be approximated by foundation models in the future. Furthermore, as participation in the argumentative discourse of engineering design is a new task, there is a need for public datasets to evaluate and compare models against."
    },
    {
      "title": "6 Conclusion",
      "text": "We propose conceiving the design process as a goal-oriented, argumentative discourse on which foundation models can operate. Making the reasoning steps explicit in a new digital artifact of the product development process could lead to improved documentation and increased collaboration, and makes certain forms of machine assistance possible in the first place. We describe how LLMs and multi-modal foundation models can assist in the design discourse and outline interesting directions of research, including structuring of the design discourse in stages and components, the provision of machine-actionable interfaces for specialized engineering software, the development of foundation models with learned representations that better correspond to the skills required in the design process, and the creation and publication of common metrics and datasets as a community effort. We do not intend to describe a theoretical framework that is far from ever being implemented in practice, but believe that the product development process is about to change, with the aspects described in this article being part of its future. Acknowledgments.The authors would like to thank the German Federal Government, the German state governments, and the Joint Science Conference (GWK) for their funding and support as part of the NFDI4Ing consortium. Funded by the German Research Foundation (DFG) - project number: 442146713. Furthermore, this work was supported by the Helmholtz Association under the program \"Energy System Design\"."
    },
    {
      "title": "References",
      "text": "* [1] Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.: SuperGLUE: A Stickier Benchmark for General-PurposeLanguage Understanding Systems. In: Advances in Neural Information Processing Systems, vol. 32, pp. 3261-3275. Curran Associates, Inc., Vancouver, BC, Canada (2019). [https://papers.nips.cc/paper_files/paper/2019/hash/4496bf24afe7Fab6f046bf4923da8de6-Abstract.html](https://papers.nips.cc/paper_files/paper/2019/hash/4496bf24afe7Fab6f046bf4923da8de6-Abstract.html) * [2] Gan, Z., Li, L., Li, C., Wang, L., Liu, Z., Gao, J.: Vision-Language Pre-Training: Basics, Recent Advances, and Future Trends. Found. Trends Comput. Graph. Vis. **14**(3-4), 163-352 (2022) [https://doi.org/10.1561/0600000105](https://doi.org/10.1561/0600000105). Publisher: Now Publishers, Inc. * [3] Driess, D., Xia, F., Sajjadi, M.S.M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., Florence, P.: PaLM-E: An Embodied Multimodal Language Model (2023) arXiv:2303.03378 [cs.LG] * [4] Moor, M., Banerjee, O., Abad, Z.S.H., Krumholz, H.M., Leskovec, J., Topol, E.J., Rajpurkar, P.: Foundation models for generalist medical artificial intelligence. Nature **616**(7956), 259-265 (2023) [https://doi.org/10.1038/s41586-023-05881-4](https://doi.org/10.1038/s41586-023-05881-4). Accessed 2023-06-15 * [5] M. Hocky, G., D. White, A.: Natural language processing models that automate programming will transform chemistry research and teaching. Digital Discovery **1**(2), 79-83 (2022) [https://doi.org/10.1039/D1DD00009H](https://doi.org/10.1039/D1DD00009H). Publisher: Royal Society of Chemistry. Accessed 2023-06-15 * [6] Manning, C.D.: Human Language Understanding & Reasoning. Daedalus **151**(2), 127-138 (2022) [https://doi.org/10.1162/daed_a_01905](https://doi.org/10.1162/daed_a_01905) * [7] Harris, Z.S.: Distributional Structure. WORD **10**(2-3), 146-162 (1954) [https://doi.org/10.1080/00437956.1954.11659520](https://doi.org/10.1080/00437956.1954.11659520) * [8] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient Estimation of Word Representations in Vector Space (2013) arXiv:1301.3781 [cs.CL] * [9] Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed Representations of Words and Phrases and their Compositionality (2013) arXiv:1310.4546 [cs.CL] * [10] Pennington, J., Socher, R., Manning, C.: GloVe: Global Vectors for Word Representation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532-1543. Association for Computational Linguistics, Doha, Qatar (2014). [https://doi.org/10.3115/v1/D14-1162](https://doi.org/10.3115/v1/D14-1162) * [11] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.: Deep contextualized word representations (2018) arXiv:1802.05365 [cs.CL]* [12] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186. Association for Computational Linguistics, Minneapolis, Minnesota (2019). [https://doi.org/10.18653/v1/N19-1423](https://doi.org/10.18653/v1/N19-1423) * [13] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is All you Need. In: Advances in Neural Information Processing Systems, vol. 30, pp. 5998-6008. Curran Associates, Inc., Long Beach, CA, USA (2017). [https://papers.nips.cc/paper/2017/hash/3f5ee243547dec91fbd053c1c4a845aa-Abstract.html](https://papers.nips.cc/paper/2017/hash/3f5ee243547dec91fbd053c1c4a845aa-Abstract.html) * [14] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: RoBERTa: A Robustly Optimized BERT Pretraining Approach (2019) arXiv:1907.11692 [cs.CL] * [15] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving Language Understanding by Generative Pre-Training (2018) * [16] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language Models are Unsupervised Multitask Learners (2019) * [17] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models are Few-Shot Learners. In: Advances in Neural Information Processing Systems, vol. 33, pp. 1877-1901. Curran Associates, Inc., virtual (2020). [https://proceedings.neurips.cc/paper/2020/hash/1457cd6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457cd6bfcb4967418bfb8ac142f64a-Abstract.html) * [18] Workshop, B., Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A.S., Yvon, F., Galle, M., Tow, J., Rush, A.M., Biderman, S., Webson, A., Ammanamanchi, P.S., Wang, T., Sagot, B., Muennighoff, N., Moral, A.V., Ruwase, O., Bawden, R., Bekman, S., McMillan-Major, A., Beltagy, I., Nguyen, H., Saulnier, L., Tan, S., Suarez, P.O., Sanh, V., Laurencon, H., Jernite, Y., Launay, J., Mitchell, M., Raffel, C., Gokaslan, A., Simhi, A., Soroa, A., Aji, A.F., Alfassy, A., Rogers, A., Nitzav, A.K., Xu, C., Mou, C., Emezue, C., Klamm, C., Leong, C., Strien, D., Adelani, D.I., Radev, D., Ponferrada, E.G., Levkovizh, E., Kim, E., Natan, E.B., De Toni, F., Dupont, G., Kruszewski, G., Pistilli, G., Elsahar, H., Benyamina, H., Tran, H., Yu, I., Abdulmumin, I., Johnson, I., Gonzalez-Dios, I., Rosa, J., Chim, J., Dodge, J., Zhu, J., Chang, J., Frohberg, J., Tobing, J., Bhattacharjee, J., Almubarak, K., Chen, K., Lo, K., Von Werra, L., Weber, L., Phan, L., allal, L.B., Tanguy, L., Dey, M., Munoz, M.R., Masoud, M., Grandury, M., Sasko, M., Huang, M., Coavoux, M., Singh,M., Jiang, M.T.-J., Vu, M.C., Jauhar, M.A., Ghaleb, M., Subramani, N., Kassner, N., Khamis, N., Nguyen, O., Espejel, O., Gibert, O., Villegas, P., Henderson, P., Colombo, P., Amuok, P., Lhoest, Q., Harliman, R., Bommasani, R., Lopez, R.L., other: BLOOM: A 176B-Parameter Open-Access Multilingual Language Model (2023) arXiv:2211.05100 [cs.CL] * [19] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: LLaMA: Open and Efficient Foundation Language Models (2023) arXiv:2302.13971 [cs.CL] * [20] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research **21**(140), 1-67 (2020) * [21] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S.S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E.H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.V., Wei, J.: Scaling Instruction-Finetuned Language Models (2022) arXiv:2210.11416 [cs.LG] * [22] Tay, Y., Dehghani, M., Tran, V.Q., Garcia, X., Wei, J., Wang, X., Chung, H.W., Shakeri, S., Bahri, D., Schuster, T., Zheng, H.S., Zhou, D., Houlsby, N., Metzler, D.: UL2: Unifying Language Learning Paradigms (2023) arXiv:2205.05131 [cs.CL] * [23] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus, W.: Emergent Abilities of Large Language Models (2022) arXiv:2206.07682 [cs.CL] * [24] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., Zhou, D.: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2023) arXiv:2201.11903 [cs.CL] * [25] Schaeffer, R., Miranda, B., Koyejo, S.: Are Emergent Abilities of Large Language Models a Mirage? (2023) arXiv:2304.15004 [cs.CL] * [26] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P.F., Leike, J., Lowe, R.: Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems **35**, 27730-27744 (2022) * [27] Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Zettlemoyer, L.,Cancedda, N., Scialom, T.: Toolformer: Language Models Can Teach Themselves to Use Tools (2023) arXiv:2302.04761 [cs.CL] * [28] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., Cao, Y.: ReAct: Synergizing Reasoning and Acting in Language Models (2023) arXiv:2210.03629 [cs.CL] * [29] Shinn, N., Labash, B., Gopinath, A.: Reflexion: an autonomous agent with dynamic memory and self-reflection (2023) arXiv:2303.11366 [cs.AI] * [30] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J.Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D.E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P.W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X.L., Li, X., Ma, T., Malik, A., Manning, C.D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J.C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park, J.S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Re, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A.W., Tramer, F., Wang, R.E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S.M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., Liang, P.: On the Opportunities and Risks of Foundation Models (2022) arXiv:2108.07258 [cs.LG] * [31] Regenwetter, L., Nobari, A.H., Ahmed, F.: Deep Generative Models in Engineering Design: A Review. Journal of Mechanical Design **144**(071704) (2022) [https://doi.org/10.1115/1.4053859](https://doi.org/10.1115/1.4053859). Accessed 2023-06-14 * [32] Raina, A., Cagan, J., McComb, C.: Design Strategy Network: A Deep Hierarchical Framework to Represent Generative Design Strategies in Complex Action Spaces. Journal of Mechanical Design **144**(021404) (2021) [https://doi.org/10.1115/1.4052566](https://doi.org/10.1115/1.4052566). Accessed 2023-06-14 * [33] Gyory, J.T., Soria Zurita, N.F., Martin, J., Balon, C., McComb, C., Kotovsky, K., Cagan, J.: Human Versus Artificial Intelligence: A Data-Driven Approach to Real-Time Process Management During Complex Engineering Design. Journal of Mechanical Design **144**(2) (2021) [https://doi.org/10.1115/1.4052488](https://doi.org/10.1115/1.4052488) * [34] Gyory, J.T., Kotovsky, K., McComb, C., Cagan, J.: Comparing the Impacts on Team Behaviors Between Artificial Intelligence and Human Process Management in Interdisciplinary Design Teams. Journal of Mechanical Design **144**(10) (2022)[https://doi.org/10.1115/1.4054723](https://doi.org/10.1115/1.4054723) * [35] Sarica, S., Luo, J., Wood, K.L.: TechNet: Technology semantic network based on patent data. Expert Systems with Applications **142**, 112995 (2020) [https://doi.org/10.1016/j.eswa.2019.112995](https://doi.org/10.1016/j.eswa.2019.112995) * [36] Jang, H., Jeong, Y., Yoon, B.: TechWord: Development of a technology lexical database for structuring textual technology information based on natural language processing. Expert Systems with Applications **164**, 114042 (2021) [https://doi.org/10.1016/j.eswa.2020.114042](https://doi.org/10.1016/j.eswa.2020.114042) * [37] Shi, F., Chen, L., Han, J., Childs, P.: A Data-Driven Text Mining and Semantic Network Analysis for Design Information Retrieval. Journal of Mechanical Design **139**(11) (2017) [https://doi.org/10.1115/1.4037649](https://doi.org/10.1115/1.4037649) * [38] Sarica, S., Luo, J.: Stopwords in technical language processing. PLOS ONE **16**(8), 0254937 (2021) [https://doi.org/10.1371/journal.pone.0254937](https://doi.org/10.1371/journal.pone.0254937) * [39] Morbach, J., Wiesner, A., Marquardt, W.: OntoCAPE--A (re)usable ontology for computer-aided process engineering. Computers & Chemical Engineering **33**(10), 1546-1556 (2009) [https://doi.org/10.1016/j.compchemeng.2009.01.019](https://doi.org/10.1016/j.compchemeng.2009.01.019) * [40] Booshehri, M., Emele, L., Flugel, S., Forster, H., Frey, J., Frey, U., Glauer, M., Hastings, J., Hofmann, C., Hoyer-Klick, C., Hulk, L., Kleinau, A., Knosala, K., Kotzur, L., Kuckertz, P., Mossakowski, T., Muschner, C., Neuhaus, F., Pehl, M., Robinius, M., Sehn, V., Stappel, M.: Introducing the Open Energy Ontology: Enhancing data interpretation and interfacing in energy systems analysis. Energy and AI **5**, 100074 (2021) [https://doi.org/10.1016/j.egyai.2021.100074](https://doi.org/10.1016/j.egyai.2021.100074) * [41] Sanfilippo, E.M., Kitamura, Y., Young, R.I.M.: Formal ontologies in manufacturing. Applied Ontology **14**(2), 119-125 (2019) [https://doi.org/10.3233/AO-190209](https://doi.org/10.3233/AO-190209) * [42] Han, J., Sarica, S., Shi, F., Luo, J.: Semantic Networks for Engineering Design: A Survey. Proceedings of the Design Society **1**, 2621-2630 (2021) [https://doi.org/10.1017/pds.2021.523](https://doi.org/10.1017/pds.2021.523). Publisher: Cambridge University Press * [43] Siddharth, L., Blessing, L.T.M., Wood, K.L., Luo, J.: Engineering Knowledge Graph From Patent Database. Journal of Computing and Information Science in Engineering **22**(2) (2021) [https://doi.org/10.1115/1.4052293](https://doi.org/10.1115/1.4052293) * [44] Siddharth, L., Blessing, L., Luo, J.: Natural language processing in-and-for design research. Design Science **8**, 21 (2022) [https://doi.org/10.1017/dsj.2022.16](https://doi.org/10.1017/dsj.2022.16) * [45] Zhu, Q., Luo, J.: Generative Pre-Trained Transformer for Design Concept Generation: An Exploration. Proceedings of the Design Society **2**, 1825-1834 (2022) [https://doi.org/10.1017/pds.2022.185](https://doi.org/10.1017/pds.2022.185). Publisher: Cambridge University Press. Accessed 2023-06-14 * [46] Zhu, Q., Luo, J.: Generative Transformers for Design Concept Generation. Journal of Computing and Information Science in Engineering **23**(4) (2023) [https://doi.org/10.1115/1.4056220](https://doi.org/10.1115/1.4056220) * [47] Zhu, Q., Zhang, X., Luo, J.: Biologically Inspired Design Concept Generation Using Generative Pre-Trained Transformers. Journal of Mechanical Design **145**(041409) (2023) [https://doi.org/10.1115/1.4056598](https://doi.org/10.1115/1.4056598). Accessed 2023-06-14 * [48] Ma, K., Grandi, D., McComb, C., Goucher-Lambert, K.: Conceptual Design Generation Using Large Language Models (2023) arXiv:2306.01779 [cs.CL] * [49] Yuan, C., Marion, T., Moghaddam, M.: Leveraging End-User Data for Enhanced Design Concept Evaluation: A Multimodal Deep Regression Model. Journal of Mechanical Design **144**(2) (2021) [https://doi.org/10.1115/1.4052366](https://doi.org/10.1115/1.4052366) * [50] Song, B., Miller, S., Ahmed, F.: Attention-Enhanced Multimodal Learning for Conceptual Design Evaluations. Journal of Mechanical Design **145**(4) (2023) [https://doi.org/10.1115/1.4056669](https://doi.org/10.1115/1.4056669) * [51] Song, B., Zhou, R., Ahmed, F.: Multi-modal Machine Learning in Engineering Design: A Review and Future Directions (2023) arXiv:2302.10909 [cs.LG] * [52] Li, K., Hopkins, A.K., Bau, D., Viegas, F., Pfister, H., Wattenberg, M.: Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task (2023) arXiv:2210.13382 [cs.LG] * [53] Belinkov, Y.: Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics **48**(1), 207-219 (2022) [https://doi.org/10.1162/coli_a_00422](https://doi.org/10.1162/coli_a_00422) * [54] Nye, M., Andreassen, A.J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., Odena, A.: Show Your Work: Scratchpads for Intermediate Computation with Language Models (2021) arXiv:2112.00114 [cs.LG] * [55] Creswell, A., Shanahan, M.: Faithful Reasoning Using Large Language Models (2022) arXiv:2208.14271 [cs.AI] * [56] Fricke, G.: Successful individual approaches in engineering design. Research in Engineering Design **8**(3), 151-165 (1996) [https://doi.org/10.1007/BF01608350](https://doi.org/10.1007/BF01608350) * [57] Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H.: Engineering Design. Springer, London (2007). [https://doi.org/10.1007/978-1-84628-319-2](https://doi.org/10.1007/978-1-84628-319-2) * [58] Regenwetter, L., Srivastava, A., Gutfreund, D., Ahmed, F.: Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design (2023) arXiv:2302.02913 [cs.LG]"
    }
  ]
}