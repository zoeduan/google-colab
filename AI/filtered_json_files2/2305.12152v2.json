{
  "title": "Revisiting Automated Topic Model Evaluation with Large Language Models",
  "authors": [
    "Dominik Stammbach",
    "Vilém Zouhar",
    "Alexander Hoyle",
    "Mrinmaya Sachan",
    "E Elliott",
    "Eth Zürich"
  ],
  "abstract": "\n Topic models help make sense of large text collections. Automatically evaluating their output and determining the optimal number of topics are both longstanding challenges, with no effective automated solutions to date. This paper evaluates the effectiveness of large language models (LLMs) for these tasks. We find that LLMs appropriately assess the resulting topics, correlating more strongly with human judgments than existing automated metrics. However, the type of evaluation task matters -LLMs correlate better with coherence ratings of word sets than on a word intrusion task. We find that LLMs can also guide users toward a reasonable number of topics. In actual applications, topic models are typically used to answer a research question related to a collection of texts. We can incorporate this research question in the prompt to the LLM, which helps estimate the optimal number of topics. github.com/dominiksinsaarland/ evaluating-topic-model-output \n",
  "references": [
    {
      "id": null,
      "title": "Revisiting Automated Topic Model Evaluation with Large Language Models",
      "authors": [
        "Dominik Stammbach",
        "Vilém Zouhar",
        "Alexander Hoyle",
        "Mrinmaya Sachan",
        "E Elliott",
        "Eth Zürich"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Persistent anti-muslim bias in large language models",
      "authors": [
        "Abubakar Abid",
        "Maheen Farooqi",
        "James Zou"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",
      "doi": "10.1145/3461702.3462624"
    },
    {
      "id": "b1",
      "title": "Evaluating topic coherence using distributional semantics",
      "authors": [
        "Nikolaos Aletras",
        "Mark Stevenson"
      ],
      "year": "2013",
      "venue": "Proceedings of the 10th international conference on computational semantics (IWCS 2013)-Long Papers",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Narrative paths and negotiation of power in birth stories",
      "authors": [
        "Maria Antoniak",
        "David Mimno",
        "Karen Levy"
      ],
      "year": "2019",
      "venue": "Proc. ACM Hum.-Comput. Interact",
      "doi": "10.1145/3359190"
    },
    {
      "id": "b3",
      "title": "Latent dirichlet allocation",
      "authors": [
        "David M Blei",
        "Andrew Y Ng",
        "Michael I Jordan"
      ],
      "year": "2003",
      "venue": "J. Mach. Learn. Res",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Normalized (pointwise) mutual information in collocation extraction",
      "authors": [
        "J Gerlof",
        "Bouma"
      ],
      "year": "2009",
      "venue": "Normalized (pointwise) mutual information in collocation extraction",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Decoupling sparsity and smoothness in the dirichlet variational autoencoder topic model",
      "authors": [
        "Sophie Burkhardt",
        "Stefan Kramer"
      ],
      "year": "2019",
      "venue": "Journal of Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Reading tea leaves: How humans interpret topic models",
      "authors": [
        "Jonathan Chang",
        "Sean Gerrish",
        "Chong Wang",
        "Jordan Boyd-Graber",
        "David Blei"
      ],
      "year": "2009",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "INSTRUCTEVAL: Towards holistic evaluation of instruction-tuned large language models",
      "authors": [
        "Ken Yew",
        "Pengfei Chia",
        "Lidong Hong",
        "Soujanya Bing",
        "Poria"
      ],
      "year": "2023",
      "venue": "INSTRUCTEVAL: Towards holistic evaluation of instruction-tuned large language models",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma",
        "Albert Webson",
        "Shane Shixiang",
        "Zhuyun Gu",
        "Mirac Dai",
        "Xinyun Suzgun",
        "Aakanksha Chen",
        "Alex Chowdhery",
        "Marie Castro-Ros",
        "Kevin Pellat",
        "Dasha Robinson",
        "Sharan Valter",
        "Gaurav Narang",
        "Adams Mishra",
        "Vincent Yu",
        "Yanping Zhao",
        "Andrew Huang",
        "Hongkun Dai",
        "Slav Yu",
        "Ed H Petrov",
        "Jeff Chi",
        "Jacob Dean",
        "Adam Devlin",
        "Denny Roberts",
        "Quoc V Zhou",
        "Jason Le",
        "Wei"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Topic modeling in embedding spaces",
      "authors": [
        "B Adji",
        "Dieng",
        "J R Francisco",
        "David M Ruiz",
        "Blei"
      ],
      "year": "2020",
      "venue": "Topic modeling in embedding spaces",
      "doi": "10.1162/tacl_a_00325"
    },
    {
      "id": "b10",
      "title": "Topic model or topic twaddle? Re-evaluating semantic interpretability measures",
      "authors": [
        "Caitlin Doogan",
        "Wray Buntine"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.300"
    },
    {
      "id": "b11",
      "title": "A systematic review of the use of topic models for short text social media analysis. Artificial Intelligence Review",
      "authors": [
        "Caitlin Doogan",
        "Wray Buntine",
        "Henry Linger"
      ],
      "year": "2023",
      "venue": "A systematic review of the use of topic models for short text social media analysis. Artificial Intelligence Review",
      "doi": "10.1007/s10462-023-10471-x"
    },
    {
      "id": "b12",
      "title": "Perspectives on large language models for relevance judgment",
      "authors": [
        "Guglielmo Faggioli",
        "Laura Dietz",
        "Charles Clarke",
        "Gianluca Demartini",
        "Matthias Hagen",
        "Claudia Hauff",
        "Noriko Kando",
        "Evangelos Kanoulas",
        "Martin Potthast",
        "Benno Stein",
        "Henning Wachsmuth"
      ],
      "year": "2023",
      "venue": "Perspectives on large language models for relevance judgment",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "GPTScore: Evaluate as you desire",
      "authors": [
        "Jinlan Fu",
        "See-Kiong Ng",
        "Zhengbao Jiang",
        "Pengfei Liu"
      ],
      "year": "2023",
      "venue": "GPTScore: Evaluate as you desire",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "ChatGPT outperforms crowd-workers for textannotation tasks",
      "authors": [
        "Fabrizio Gilardi",
        "Meysam Alizadeh",
        "Maël Kubli"
      ],
      "year": "2023",
      "venue": "ChatGPT outperforms crowd-workers for textannotation tasks",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Text as data: The promise and pitfalls of automatic content analysis methods for political texts",
      "authors": [
        "Justin Grimmer",
        "Brandon M Stewart"
      ],
      "year": "2013",
      "venue": "Political Analysis",
      "doi": "10.1093/pan/mps028"
    },
    {
      "id": "b16",
      "title": "Is automated topic model evaluation broken?: The incoherence of coherence",
      "authors": [
        "Alexander Hoyle",
        "Pranav Goel",
        "Denis Peskov",
        "Andrew Hian-Cheong",
        "Jordan Boyd-Graber",
        "Philip Resnik"
      ],
      "year": "2021",
      "venue": "Is automated topic model evaluation broken?: The incoherence of coherence",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Are neural topic models broken?",
      "authors": [
        "Alexander Miserlis Hoyle",
        "Rupak Sarkar",
        "Pranav Goel",
        "Philip Resnik"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech",
      "authors": [
        "Fan Huang",
        "Haewoon Kwak",
        "Jisun An"
      ],
      "year": "2023",
      "venue": "Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Comparing partitions",
      "authors": [
        "Lawrence Hubert",
        "Phipps Arabie"
      ],
      "year": "1985",
      "venue": "Journal of classification",
      "doi": "10.1007/BF01908075"
    },
    {
      "id": "b20",
      "title": "Twitter and research: A systematic literature review through text mining",
      "authors": [
        "Amir Karami",
        "Morgan Lundy",
        "Frank Webb",
        "Yogesh K Dwivedi"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.2983656"
    },
    {
      "id": "b21",
      "title": "Large language models are state-of-the-art evaluators of translation quality",
      "authors": [
        "Tom Kocmi",
        "Christian Federmann"
      ],
      "year": "2023",
      "venue": "Large language models are state-of-the-art evaluators of translation quality",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery",
      "authors": [
        "Zachary C Lipton"
      ],
      "year": "2018",
      "venue": "Queue",
      "doi": "10.1145/3236386.3241340"
    },
    {
      "id": "b23",
      "title": "Gender and representation bias in GPT-3 generated stories",
      "authors": [
        "Li Lucy",
        "David Bamman"
      ],
      "year": "2021",
      "venue": "Proceedings of the Third Workshop on Narrative Understanding",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "MALLET: A MAachine Learning for LanguagE Toolkit",
      "authors": [
        "Andrew Kachites",
        "Mccallum"
      ],
      "year": "2002",
      "venue": "MALLET: A MAachine Learning for LanguagE Toolkit",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Pointer sentinel mixture models",
      "authors": [
        "Stephen Merity",
        "Caiming Xiong",
        "James Bradbury",
        "Richard Socher"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Optimizing semantic coherence in topic models",
      "authors": [
        "David Mimno",
        "Hanna Wallach",
        "Edmund Talley",
        "Miriam Leenders",
        "Andrew Mccallum"
      ],
      "year": "2011",
      "venue": "Proceedings of the 2011 conference on empirical methods in natural language processing",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Automatic evaluation of topic coherence",
      "authors": [
        "David Newman",
        "Jey",
        "Han Lau",
        "Karl Grieser",
        "Timothy Baldwin"
      ],
      "year": "2010",
      "venue": "Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Evaluating topic models for digital libraries",
      "authors": [
        "David Newman",
        "Youn Noh",
        "Edmund Talley",
        "Sarvnaz Karimi",
        "Timothy Baldwin"
      ],
      "year": "2010",
      "venue": "Proceedings of the 10th annual joint conference on Digital libraries",
      "doi": "10.1145/1816123.1816156"
    },
    {
      "id": "b29",
      "title": "Contextualized topic coherence metrics",
      "authors": [
        "Hamed Rahimi",
        "Jacob Louis Hoover",
        "David Mimno",
        "Hubert Naacke",
        "Camelia Constantin",
        "Bernd Amann"
      ],
      "year": "2023",
      "venue": "Contextualized topic coherence metrics",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Exploring the space of topic coherence measures",
      "authors": [
        "Michael Röder",
        "Andreas Both",
        "Alexander Hinneburg"
      ],
      "year": "2015",
      "venue": "Proceedings of the eighth ACM international conference on Web search and data mining",
      "doi": "10.1145/2684822.2685324"
    }
  ],
  "sections": [
    {
      "title": "Revisiting Automated Topic Model Evaluation",
      "text": "with Large Language Models Dominik Stammbach\\({}^{\\mathbf{\\varepsilon}}\\) &Vilem Zouhar\\({}^{\\mathbf{\\varepsilon}}\\) &Alexander Hoyle\\({}^{\\mathbf{\\text{M}}}\\) Mrinmaya Sachan\\({}^{\\mathbf{\\varepsilon}}\\) &Elliott Ash\\({}^{\\mathbf{\\varepsilon}}\\) \\({}^{\\mathbf{\\varepsilon}}\\)ETH Zurich &University of Maryland {dominsta,vzouhar,ashe,msachan}@ethz.ch &hoyle@umd.edu"
    },
    {
      "title": "Abstract",
      "text": "Topic models help make sense of large text collections. Automatically evaluating their output and determining the optimal number of topics are both longstanding challenges, with no effective automated solutions to date. This paper evaluates the effectiveness of large language models (LLMs) for these tasks. We find that LLMs appropriately assess the resulting topics, correlating more strongly with human judgments than existing automated metrics. However, the type of evaluation task matters -- LLMs correlate better with coherence ratings of word sets than on a word intrusion task. We find that LLMs can also guide users toward a reasonable number of topics. In actual applications, topic models are typically used to answer a research question related to a collection of texts. We can incorporate this research question in the prompt to the LLM, which helps estimate the optimal number of topics. github.com/dominiksinaarland/evaluating-topic-model-output"
    },
    {
      "title": "1 Introduction",
      "text": "Topic models are, loosely put, an unsupervised dimensionality reduction technique that help organize document collections Blei et al. (2003). A topic model summarizes a document collection with a small number of _topics_. A topic is a probability distribution over words or phrases. A topic \\(T\\) is interpretable through a representative set of words or phrases defining the topic, denoted \\(W_{T}\\).1 Footnote 1: We think of “words” as an atomic unit in a document, which can also be an n-gram or phrase. E.g., \\(W_{\\text{legal}}=\\{\\text{Iitigation},\\text{attorney-client privilege, intellectual property,...}\\}\\). Each document can, in turn, be represented as a distribution over topics. For each topic, we can retrieve a representative document collection by sorting documents across topic distributions. We denote this set of documents for topic \\(T\\) as \\(D_{T}\\). Because of their ability to organize large collections of texts, topic models are widely used in the social sciences, digital humanities, and other disciplines to analyze large corpora Talley et al. (2011); Grimmer and Stewart (2013); Antoniak et al. (2019); Karami et al. (2020), inter alia). Interpretability makes topic models useful, but human interpretation is complex and notoriously difficult to approximate Lipton (2018). Automated topic coherence metrics do not correlate well with human judgments, often overstating differences between models Hoyle et al. (2021); Doogan and Buntine (2021). Without the guidance of an automated metric, the number of topics, an important hyperparameter, is usually derived manually: Practitioners fit various topic models, inspect the resulting topics, and select the configuration which works best for the intended use case Hoyle et al. (2021). This is a non-replicable and time-consuming process, requiring expensive expert labor. Recent NLP research explores whether large language models (LLMs) can perform automatic annotations; e.g., to assess text quality Fu et al. (2023); Faggioli et al. (2023); Huang et al. (2023), inter alia). Here, we investigate whether LLMs can automatically assess the coherence of topic modeling output and conclude that: 1. LLMs can accurately judge topic coherence, 2. LLMs can assist in automatically determining reasonable numbers of topics. We use LLMs for two established topic coherence evaluation tasks and find that their judgment strongly correlates with humans on one of these tasks. Similar to recent findings, we find that coherent topic word sets \\(W_{T}\\) do not necessarily imply an optimal categorization of the document collection Doogan and Buntine (2021). Instead, we automatically assign a label to each document in a \\(D_{T}\\) and choose the configuration with the purest assigned labels. This solution correlates well with an underlying ground truth. Thus, LLMs can help find good numbers of topics for a text collection, as we show in three case studies. [MISSING_PAGE_FAIL:2] Baseline metrics.For NPMI and \\(\\mathbf{C_{v}}\\), we report the best correlation by Hoyle et al. (2021). These metrics depend on the reference corpus and other hyperparameters and we always report the best value. Hoyle et al. (2021) find no single _best_ setting for these automated metrics and therefore this comparison makes the baseline inadequately strong. Intrusion detection task.The accuracies for detecting intruder words in the evaluated topics are almost identical - humans correctly detect 71.2% of the intruder words, LLMs identify intruders in 72.2% of the cases. However, humans and LLMs differ for which topics these intruder words are identified. This results in overall strong correlations within human judgement, but not higher correlations than NPMI and \\(\\mathbf{C_{v}}\\) (in their best setting). Coherence rating task.The LLM rating of the \\(W_{T}\\) top word coherence correlates more strongly with human evaluations than all other automated metrics in any setting. This difference is statistically significant, and the correlation between LLM ratings and human assessment approaches the inter-annotator agreement ceiling. Appendix B shows additional results with different prompts and LLMs. Recommendation.Both findings support using LLMs for evaluating coherence of \\(W_{T}\\) in practice as they correlate highly with human judgements."
    },
    {
      "title": "4 Determining The Number Of Topics",
      "text": "Topic models require specifying the number of topics. Practitioners usually run models multiple times with different numbers of topics (denoted by \\(k\\)). After manual inspection, the model which seems most suited for a research question is chosen. Doogan et al. (2023) review 189 articles about topic modeling and find that common use cases are exploratory and descriptive studies for which no single best number of topics exists. However, the most prevalent use case is to isolate semantically similar documents belonging to topics of interest. For this, Doogan and Buntine (2021) challenge the focus on only evaluating \\(W_{T}\\), and suggest an analysis of \\(D_{T}\\) as well. If we are interested in organizing a collection, then we would expect the top documents in \\(D_{T}\\) to receive the same topic labels. We provide an LLM-based strategy to determine good number of topics for this use case: We let an LLM assign labels to documents, and find that topic assignments with greater label purity correlate with the ground-truth in three case studies. Topics of interest might be a few broad topics such as _politics_ or _healthcare_, or many specific topics, like _municipal elections_ and _maternity care_. Following recent efforts that use research questions to guide LLM-based text analysis (Zhong et al., 2023), we incorporate this desideratum in the LLM prompt. We run collapsed Gibbs-sampled LDA (in Mallet: McCallum, 2002) on two text collections, with different numbers of topics (\\(k=20\\) to \\(400\\)), yielding 20 models per collection. To compare topic model estimates and ground-truth partitions, we experiment with a legislative Bill summary dataset (from Hoyle et al., 2022) and Wikitext (Merity et al., 2017), both annotated with ground-truth topic labels in different granularities."
    },
    {
      "title": "Proposed Metrics",
      "text": "Ratings algorithm.For each of the 20 models, we randomly sample \\(W_{T}\\) for some topics and let the LLM rate these \\(W_{T}\\). The prompt is similar to the ratings prompt shown in Prompt 1, see Appendix E for full details. We then average ratings for each configuration. Intuitively, the model yielding the most coherent \\(W_{T}\\) should be the one with the optimal topic count. However, this procedure does not correlate with ground-truth labels. Text labeling algorithm.Doogan and Buntine (2021) propose that domain experts assign labels to each document in a \\(D_{T}\\) instead. A good topic should have a coherent \\(D_{T}\\): The same label assigned to most documents. Hence, good configurations have high purity of assigned labels within each topic. We proceed analogously. For each of the 20 models, we randomly sample \\(D_{T}\\) for various topics. We retrieve the 10 most probable documents and then use the LLM to assign a label to these documents. We use the system prompt _[...] Annotate the document with a broadbandnarrow label [...]_, see Appendix E for full details. We compute the purity of the assigned labels and average purities and we select the configuration with the most pure topics. In both procedures, we smooth the LLM outputs using a rolling window average to reduce noise (the final average goodness is computed as moving average of window of size 3)."
    },
    {
      "title": "Evaluation",
      "text": "We need a human-derived metric to compare with the purity metric proposed above. We measure the alignment between a topic model's predicted topic assignments and the ground-truth labels for a document collection (Hoyle et al., 2022). We choose the _Adjusted Rand Index (ARI)_ which compares two clusterings (Hubert and Arabie, 1985) and is high when there is strong overlap. The predicted topic assignment for each document is its most probable topic. Recall that there exist many different optimal topic models for a single collection. If we want topics to contain semantically similar documents, each ground-truth assignment reflects one possible set of topics of interests. If our LLM-guided procedure and the ARI correlate, this indicates that we discovered a reasonable value for the number of topics. In our case, the various ground-truth labels are assigned with different research questions in mind. We incorporate such constraints in the LLM prompt: We specify whether we are interested in broad or specific topics, and we enumerate some example ground-truth categories in our prompt. Practitioners usually have priors about topics of interest before running topic models, thus we believe this setup to be realistic. In Figure 1 we show LLM scores and ARI for broad topics in the Bills dataset. We used this dataset to find a suitable prompt, hence this could be considered the \"training set\". We plot coherence ratings of word sets in \\(\\boxed{blue}\\), purity of document labels in \\(\\boxed{red}\\), and the ARI between topic model and ground-truth assignments in \\(\\boxed{green}\\). The purity of LLM-assigned \\(D_{T}\\) labels correlate with the ARI, whereas the \\(W_{T}\\) coherence scores do not. The argmax of the purity-based approach leads to similar numbers of topics as suggested by the ARI argmax (although not always the same). For Wikitext, we evaluate the same 20 topic models, but measure ARI between topic model assignment and two different ground-truth label sets. The LLM scores differ only because of different prompting strategies. The distributions indicate that this strategy incorporates different research questions. For Bills, our rating algorithm suggests to use a topic model with \\(k{=}100\\) topics. In Appendix G, we show corresponding word sets. The resulting \\(W_{T}\\) seem interpretable, although the ground-truth assignments using document-topic estimates are not correlated with the ground-truth labels. The purity-based approach instead suggests to use \\(k{=}20\\) topics, the same \\(k\\) as indicated by the ARI. We show ground-truth labels and LLM-obtained text labels in Appendix G. We further manually evaluate 180 assigned LLM-labels and find that 94% of these labels are reasonable. Appendix F shows further evaluation of these label assignments."
    },
    {
      "title": "5 Discussion",
      "text": "In this work, we revisit automated topic model evaluation with the help of large language models. Many automated evaluation metrics for topic models exist, however these metrics seem to not correlate strongly with human judgment on word-set analysis (Hoyle et al., 2021). Instead, we find that an LLM-based metric of coherent topic words correlates with human preferences, outperforming other metrics on the rating task. Second, the number of topics \\(k\\) has to be defined before running a topic model, so practitioners run multiple models with different \\(k\\). We investigate whether LLMs can guide us towards reasonable \\(k\\) for a collection and research question. We first note that the term _optimal number of topics_ is vague and that such quantity does not exist without additional context. If our goal is to find a configuration which would result in coherent document sets for topics, our study supports evaluating \\(D_{T}\\) instead of \\(W_{T}\\), as this correlates more strongly with the overlap between topic model and ground-truth assignment. This finding supports arguments made in Doogan and Buntine (2021) who challenge the focus on \\(W_{T}\\) in topic model evaluation. Figure 1: (1) ARI for topic assignment and ground-truth topic labels, (2) LLM word set coherence, (3) LLM document set purity, obtained by our algorithm. ARI correlates with LLM document set purity, but not with LLM word set coherence. The ground-truth number of topics are: 21 topics in the BillSum dataset, 45 broad topics in Wikitext and 279 specific topics in Wikitext. \\(\\rho_{D}\\) and \\(\\rho_{W}\\) are document-LLM and word-LLM correlations with ARI."
    },
    {
      "title": "Limitations",
      "text": "Choice of LLM.Apart from ChatGPT, we also used open-source LLMs, such as FLAN-T5 (Chung et al., 2022), and still obtained reasonable, albeit worse than ChatGPT, coherence correlations. Given the rapid advances, future iterations of open-source LLMs will likely become better at this task. Number of topics.The _optimal_ number of topics is a vague concept, dependent on a practitioner's goals and the data under study. At the same time, it is a required hyperparameter of topic models. Based on Doogan et al. (2023), we use an existing document categorization as _one possible_ ground truth. While content analysis is the most popular application of topic models (Hoyle et al., 2022), it remains an open question how they compare to alternative clustering algorithms for this use case (e.g., k-means over document embeddings). Interpretability.LLM label assignment and intruder detection remain opaque. This hinders the understanding of the evaluation decisions. Topic modeling algorithm.In Section 3, we evaluate three topic modeling algorithms: GibbsLDA, Dirichlet-VAE and ETM (see Hoyle et al., 2021). In Section 4, we use only Gibbs-LDA and expansion to further models is left for future work. Future work.*Evaluation of clustering algorithms with LLMs (e.g., k-means). *More rigorous evaluation of open-source LLMs. *Formalization, implementation and release of an LLM-guided algorithm for automatically finding optimal numbers of topics for a text collection and a research question."
    },
    {
      "title": "References",
      "text": "* Abid et al. (2021) Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim bias in large language models. In _Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society_, pages 298-306. * Aletras and Stevenson (2013) Nikolaos Aletras and Mark Stevenson. 2013. Evaluating topic coherence using distributional semantics. In _Proceedings of the 10th international conference on computational semantics (IWCS 2013)-Long Papers_, pages 13-22. * Antoniak et al. (2019) Maria Antoniak, David Mimno, and Karen Levy. 2019. Narrative paths and negotiation of power in birth stories. _Proc. ACM Hum.-Comput. Interact._, 3(CSCW)."
    },
    {
      "title": "Ethics Statement",
      "text": "Using blackbox models in NLP.Statistically significant positive results are a sufficient proof of models' capabilities, assuming that the training data is not part of the training set. This data leakage problem with closed-source LLMs is part of a bigger and unresolved discussion. In our case, we believe data leakage is unlikely. Admittedly, the data used for our coherence experiments has been publicly available. However, the data is available in a large JSON file where the topic words and annotated labels are stored disjointly. For our case studies in Section 4, the topic modeling was constructed as part of this work and there is no ground-truth which could leak to the language model. Negative results with LLMs.In case of negative results, we cannot conclude that a model can not be used for a particular task. The negative results can be caused by inadequate prompting strategies and may even be resolved by advances in LLMs. LLMs and biases.LLMs are known to be biased (Abid et al., 2021; Lucy and Bamman, 2021) and their usage in this application may potentially perpetuate these biases. Data privacy.All data used in this study has been collected as part of other work. We find no potential violations of data privacy. Thus, we feel comfortable re-using the data in this work. Misuse potential.We urge practicioners to not blindly apply our method on their topic modeling output, but still manually validate that the topic outputs would be suitable to answer a given research question. David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. _J. Mach. Learn. Res._, 3(null):993-1022. * Bouma (2009) Gerlof J. Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. * Burkhardt and Kramer (2019) Sophie Burkhardt and Stefan Kramer. 2019. Decoupling sparsity and smoothness in the dirichlet variational autoencoder topic model. _Journal of Machine Learning Research_, 20(131):1-27. * Chang et al. (2009) Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-graber, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. In _Advances in Neural Information Processing Systems_, volume 22. Curran Associates, Inc. * Chia et al. (2023) Yew Ken Chia, Pengfei Hong, Lidong Bing, and Souljanya Poria. 2023. INSTRUCTEVAL: Towards holistic evaluation of instruction-tuned large language models. * Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. * Dieng et al. (2020) Adji B. Dieng, Francisco J. R. Ruiz, and David M. Blei. 2020. Topic modeling in embedding spaces. _Transactions of the Association for Computational Linguistics_, 8:439-453. * Doogan and Buntine (2021) Caitlin Doogan and Wray Buntine. 2021. Topic model or topic twaddle? Re-evaluating semantic interpretability measures. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 3824-3848, Online. Association for Computational Linguistics. * Doogan et al. (2023) Caitlin Doogan, Wray Buntine, and Henry Linger. 2023. A systematic review of the use of topic models for short text social media analysis. _Artificial Intelligence Review_, pages 1-33. * Fagioli et al. (2023) Guglielmo Fagioli, Laura Dietz, Charles Clarke, Gianluca Demartini, Matthias Hagen, Claudia Huff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, Benno Stein, and Henning Wachsmuth. 2023. Perspectives on large language models for relevance judgment. * Fu et al. (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. GPTScore: Evaluate as you desire. * Gilardi et al. (2023) Fabrizio Gilardi, Meysam Alizadeh, and Mael Kubli. 2023. ChatGPT outperforms crowd-workers for text-annotation tasks. * Grimmer and Stewart (2013) Justin Grimmer and Brandon M. Stewart. 2013. Text as data: The promise and pitfalls of automatic content analysis methods for political texts. _Political Analysis_, 21(3):267-297. * Hoyle et al. (2021) Alexander Hoyle, Pranav Goel, Denis Peskov, Andrew Hian-Cheong, Jordan Boyd-Graber, and Philip Resnik. 2021. Is automated topic model evaluation broken?: The incoherence of coherence. * Hoyle et al. (2022) Alexander Miserlis Hoyle, Rupak Sarkar, Pranav Goel, and Philip Resnik. 2022. Are neural topic models broken? In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 5321-5344, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. * Huang et al. (2023) Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. _arXiv preprint arXiv:2302.07736_. * Hubert and Arabie (1985) Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. _Journal of classification_, 2:193-218. * Karami et al. (2020) Amir Karami, Morgan Lundy, Frank Webb, and Yogesh K. Dwivedi. 2020. Twitter and research: A systematic literature review through text mining. _IEEE Access_, 8:67698-67717. * Kocmi and Federmann (2023) Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. * Lipton (2018) Zachary C. Lipton. 2018. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. _Queue_, 16(3):31-57. * Lucy and Bamman (2021) Li Lucy and David Bamman. 2021. Gender and representation bias in GPT-3 generated stories. In _Proceedings of the Third Workshop on Narrative Understanding_, pages 48-55. * McCallum (2002) Andrew Kachites McCallum. 2002. MALLET: A MAachine Learning for Language Toolkit. [http://mallet.cs.umass.edu](http://mallet.cs.umass.edu). * Merity et al. (2017) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In _5th International Conference on Learning Representations, ICLR 2017 Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net. * Mimno et al. (2011) David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing semantic coherence in topic models. In _Proceedings of the 2011 conference on empirical methods in natural language processing_, pages 262-272. * Newman et al. (2010a) David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010a. Automatic evaluation of topic coherence. In _Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics_, pages 100-108. * Newman et al. (2010b) David Newman, Youn Noh, Edmund Talley, Sarvnaz Karimi, and Timothy Baldwin. 2010b. Evaluating topic models for digital libraries. In _Proceedings of the 10th annual joint conference on Digital libraries_, pages 215-224. * Rahimi et al. (2023) Hamed Rahimi, Jacob Louis Hoover, David Mimno, Hubert Naacke, Camelia Constantin, and Bernd Amann. 2023. Contextualized topic coherence metrics. * Roder et al. (2015) Michael Roder, Andreas Both, and Alexander Hinneburg. 2015. Exploring the space of topic coherence measures. In _Proceedings of the eighth ACM international conference on Web search and data mining_, pages 399-408. * Roder et al. (2016)Evan Sandhaus. 2008. The new york times annotated corpus. * Talley et al. (2011) Edmund M Talley, David Newman, David Mimno, Bruce W Herr 2nd, Hanna M Wallach, Gully A P C Burns, A G Miriam Leenders, and Andrew McCallum. 2011. Database of NIH grants using machine-learned categories and graphical clustering. _Nat. Methods_, 8(6):443-444. * Vinh et al. (2010) Nguyen Xuan Vinh, Julien Epps, and James Bailey. 2010. Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance. _J. Mach. Learn. Res._, 11:2837-2854. * Wallach et al. (2011) Hanna M Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2011."
    },
    {
      "title": "Appendix A Language Model Prompts",
      "text": "In this section, we show the used LLM prompts. The task descriptions are borrowed from (Hoyle et al., 2021) and mimic crowd-worker instructions. We use a temperature of 1 for LLMs, and the topic words are shuffled before being prompted. Both introduce additional variation within the results, similar to how some variation is introduced if different crowd-workers are asked to perform the same task. Intruder detection task.Analogous to the human experiment, we randomly sample (a) five word from the top 10 topic words and (b) an additional intruder word from a different topic which does not occur in the top 50 words of the current topic. We then shuffle these six words. We show the final prompt with an example topic in Prompt 2. We also construct a prompt without the dataset description (see Prompt 3 and results in Table 2). **System prompt:** You are a helpful assistant evaluating the top words of a topic model output for a given topic. Select which word is the least related to all other words. If multiple words do not fit, choose the word that is most out of place. The topic modeling is based on the New York Times corpus. The corpus consists of articles from 1987 to 2007. Sections from a typical paper include International, National, New York Regional, Business, Technology, and Sports news, features on topics such as Dining, Movies, Travel, and Fashion; there are also dubinates and opinion pieces. Reply with a single word. **User prompt:** water, area, river, park, miles, game Prompt 2: Intruder Detection Task (the intruder word in this topic is _game_). Rating Task.Similar to the human experiment, we retrieve the top 10 topic words and shuffle them. and David Mimno. 2009. Evaluation methods for topic models. In _Proceedings of the 26th annual international conference on machine learning_, pages 1105-1112. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In _8th International Conference on Learning Representations, ICLR 2020 Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. 2023. Goal driven discovery of distributional differences via language descriptions. We include a task and dataset description which leads to Prompt 4. The minimal prompt without the dataset description is shown in Prompt 5. System prompt:You are a helpful assistant evaluating the top words of a topic model output for a given topic. Please note how related the following words are to each other on a scale from 1 to 3 (\"1\" = not very related, \"2\" = moderately related, \"3\" = very related). The topic modeling is based on the Wikipedia corpus. Wikipedia is an online encyclopedia covering a huge range of topics. Atries can includeographies (\"George Washington\"), scientific phenomena (\"Solar Eclipse\"), art pieces (\"La Danese\"), music (\"Amazine Grace\"), transportation (\"U.S. Route 131\"), sports (\"1952 winter olympics\"), historical events or periods (\"Tang Dynasty\"), media and pop culture (\"The Simposus Movie\"), places (\"Yosemite National Park\"), plants and animals (\"Kola\"), and warfare (\"USS Nevada (BB-36)\"), among others. Reply with a single number, indicating the overall appropriateness of the topic. User prompt:take, park, river, land, years, feet, ice, miles, water, area Prompt 4: Rating Task. Topic terms are shuffled. System prompt:You are a helpful assistant evaluating the top words of a topic model output for a given topic. Please note how related the following words are to each other on a scale from 1 to 3 (\"1\" = not very related, \"2\" = moderately related, \"3\" = very related). Reply with a single number, indicating the overall appropriateness of the topic. User prompt:take, park, river, land, years, feet, ice, miles, water, area Prompt 5: Rating Task without dataset description. Topic terms are shuffled."
    },
    {
      "title": "Appendix B Additional: Topic Model Outputs",
      "text": "Minimal prompt.Even without the dataset description in the prompt, the results remain similar. All human ratings.In our main results, we discard human annotations with low annotator confidence in the rating. We now consider all ratings, even the non-confident ones. The results are slightly better than with the filtering. Different LLM.We also evaluate both tasks with FLAN-T5 XL Chung et al. (2022), which is instruction-finetuned across a range of tasks. This model performs well in zero-shot setting, and compares to recent state-of-the-art Chia et al. (2023). Although it does not reach ChatGPT, the correlation with human annotators are all statistically significant. For the NYT and concatenated experiments, the resulting correlation are statistically indistinguishable from the best reported automated metrics NPMI and \\(\\mathbf{C_{v}}\\) in Hoyle et al. (2021). We also ran our experiments with Alpaca-7B and Falcon-7B, with largely negative results."
    },
    {
      "title": "Appendix C Alternative Clustering Metrics",
      "text": "In our main results, we show correlations between LLM scores and the adjusted Rand Index, ARI, which measures the overlap between ground-truth clustering and topic model assignments. There are other cluster metrics, such as Adjusted Mutual Information, AMI Vinh et al. (2010), completeness, or homogeneity. In Table 3, we show Spearman correlation statistics for these metrics. Our correlations are robust to the choice of metric used to measure the fit between the topic model assignment and the ground-truths in our case studies."
    },
    {
      "title": "Appendix D Definitions",
      "text": "See Bouma (2009) for justification of the NPMI formula. \\(p(w_{i})\\) and \\(p(w_{i},w_{j})\\) are unigram and joint probabilities, respectively. \\[\\text{NPMI}(w_{i},w_{j}) =\\frac{\\text{PMI}(w_{i},w_{j})}{\\cdot\\log p(w_{i},w_{j})} =\\frac{\\log\\frac{p(w_{i},w_{j})}{p(w_{i})p(w_{j})}}{\\cdot\\log p(w_{i},w_{j})}\\] The \\(\\mathbf{C_{v}}\\) metric Roder et al. (2015) is a more complex and includes, among others, the combination of NPMI and cosine similarity for top words."
    },
    {
      "title": "Appendix E Optimal Number Of Topics Prompts",
      "text": "We now show the prompts for the optimal number of topics. We incorporate research questions in two ways: (1) we specify whether we are looking for _broad_ or _narrow_ topics, and (2) we prompt 5 example categories. We believe this is a realistic operationalization. If our goal is a reasonable partitioning of a collection, we usually have some priors about what categories we want the collection to be partitioned into. Prompt 6 shows the prompt for rating \\(T_{ws}\\) by models run with different numbers of topics. The task description and user prompt is identical to the prompt used in our prior experiments, displayed in e.g., Prompt 4. However, the dataset description is different and allows for some variation. In Prompt 7, we show the prompt for automatically assigning labels to a document from a \\(T_{dc}\\). To automatically find the optimal number of topics for a topic model, we prompt an LLM to provide a concise label to a document from the topic document collection, the most likely documents assigned by a topic model to a topic (see Prompt 7). \\begin{table} \\begin{tabular}{c c c c c c c c|c} \\hline \\hline **Task** & **Dataset** & **NPMI** & \\(\\mathbf{C_{v}}\\) & **LLM (main)** & **LLM (min.)** & **LLM (all ann.)** & **FLAN-T5** & **Ceiling** \\\\ \\hline \\multirow{3}{*}{Intrusion} & NYT & 0.43 & 0.45 & 0.37 & 0.41 & 0.39 & 0.37 & 0.67 \\\\ & Wiki & 0.39 & 0.34 & 0.35 & 0.27 & 0.36 & 0.18 & 0.60 \\\\ & Both & 0.40 & 0.40 & 0.36 & 0.34 & 0.38 & 0.28 & 0.64 \\\\ \\hline \\multirow{3}{*}{Rating} & NYT & 0.48 & 0.40 & 0.64 & 0.64 & 0.65 & 0.31 & 0.72 \\\\ & Wiki & 0.44 & 0.40 & 0.57 & 0.51 & 0.56 & 0.17 & 0.56 \\\\ \\cline{1-1} & Both & 0.44 & 0.42 & 0.59 & 0.57 & 0.61 & 0.25 & 0.65 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Additional experiments reporting Spearman correlation between mean human scores and automated metrics. **LLM (main)** repeats our main results in Table 1 for reference. **LLM (min.)** – results using a minimal prompt without dataset descriptions. **LLM (all ann)** – no discarding low-confidence annotations. **FLAN-T5** – FLAN-T5 XL instead of ChatGPT. All numbers are the average result of 1000 bootstrapping episodes – re-sampling human annotations and LLM scores. **Ceiling** shows batched inter-annotator agreement. \\begin{table} \\begin{tabular}{c c c c c c} \\hline \\hline **Dataset** & **Topics** & **ARI** & **AMI** & **Compl.** & **Homog.** \\\\ \\hline Bills Words & Broad & 0.61 & 0.74 & 0.63 & -0.58 \\\\ Wiki Words & Broad & -0.38 & -0.38 & -0.38 & 0.38 \\\\ Wiki Words & Specific & 0.03 & -0.24 & -0.19 & 0.17 \\\\ \\hline Bills Docs & Broad & 0.59 & 0.36 & 0.57 & -0.58 \\\\ Wiki Docs & Broad & 0.72 & 0.72 & 0.72 & -0.70 \\\\ Wiki Docs & Specific & 0.72 & 0.66 & 0.20 & -0.20 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Spearman correlation coefficients between our language-model based scores and various popular metrics for assessing the overlap between the topic model assignment and the underlying ground-truth. **Compl. =** Completeness, **Homog. =** Homogenity. [MISSING_PAGE_FAIL:9] \\begin{table} \\begin{tabular}{l l l l l l l} \\hline \\hline & \\multicolumn{2}{c}{**Bills**} & \\multicolumn{2}{c}{**Wikitext (broad)**} & \\multicolumn{2}{c}{**Wikitext (specific)**} \\\\ \\hline & **LLM-label** & **True label** & **LLM-label** & **True label** & **LLM-label** & **True label** \\\\ \\hline \\multirow{4}{*}{**WordWord**} & **b**e**allr & Health & amusement & Recreation & politician & Historical figures: politicians \\\\ & **e**elder abuse prevention & Social Welfare & & park ride & & & \\\\ & **b**e**allr & Health & amusement & Recreation & american & Historical figures: politicians \\\\ & **b**eallr & Health & park ride & & civil war & \\\\ & **b**eallr & Health & amusement & Recreation & lawyer and & Historical figures: other \\\\ & **b**eallr & Health & park ride & & politician & \\\\ & **b**eallr & Health & amusement & Recreation & historical & Journalism and newspapers \\\\ & **b**eallr & Public Lands & & park ride & & \\\\ \\hline \\multirow{4}{*}{**Word**} & **b**ublic land & Public Lands & **warship and naval unit** & Armies and military units & classical & Poetry \\\\ & **b**ublic land & Public Lands & **warship and naval unit** & Armies and military units & handuism & Religious doctrines, teachings, texts, events, and symbols \\\\ & **b**ublic land & Environment & **warship and naval unit** & Military people & hinduism & Religious doctrines, teachings, texts, events, and symbols \\\\ & **b**eallr & Government affair & **b**eallr & & & \\\\ & **b**eallr & Operations & **warship and naval unit** & Military people & philosophy & Philosophical doctrines, teachings, texts, events, and symbols \\\\ & **b**eallr & Government affair & **b**eallr & & & \\\\ & **b**eallr & Government affair & **b**eallr & & & \\\\ & **b**eallr & Operations & & & & \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: Assigned LLM labels and ground-truth labels for a given topic from the most and the least suitable cluster configuration according to our algorithm. The purity is higher in the most suitable configuration for LLM labels and ground-truth labels. \\begin{table} \\begin{tabular}{l l} \\hline \\hline & \\multicolumn{1}{c}{**Bills (broad categories)**} \\\\ Most & - veterans, secretary, veteran, assistance, service, disability, benefits, educational, compensation, veterans\\_affairs (3) \\\\ & - land, forest, management, lands, act, usda, projects, secretary, restoration, federal (3) \\\\ & - mental, health, services, treatment, abuse, programs, substance, grants, prevention, program (3) \\\\ \\cline{2-2} Least & - gas, secretary, lease, oil, leasing, act, way, federal, production, environmental (2) \\\\ & - covered, criminal, history, act, restitution, child, background, amends, checks, victim (2) \\\\ & - information, beneficial, value, study, ownership, united\\_states, act, area, secretary, new\\_york (1) \\\\ \\hline \\multirow{4}{*}{**Wikitext (broad categories)**} \\\\ Most & - episode, star, trek, enterprise, series, season, crew, generation, ship, episodes (3) \\\\ & - series, episodes, season, episode, television, cast, production, second, viewers, pilot (3) \\\\ & - car, vehicle, vehicles, engine, model, models, production, cars, design, rear (3) \\\\ \\cline{2-2} Least & - episode, series, doctor, season, character, time, star, story, trek, set (2) \\\\ & - stage, tour, ride, park, concert, dance, train, coaster, new, roller (1) \\\\ & - said, like, character, time, life, love, relationship, later, people, way (1) \\\\ \\hline \\multirow{4}{*}{**Wikitext (specific categories)**} \\\\ Most & - episode, star, trek, enterprise, series, season, crew, generation, ship, episodes (3) \\\\ & - car, vehicle, vehicles, engine, model, models, production, cars, design, rear (3) \\\\ & - world, record, meter, time, won, freestyle, gold, championships, relay, seconds (2) \\\\ \\cline{2-2} Least & - fossil, fossils, found, specimens, years, evolution, modern, million, eddie, like (2) \\\\ & - match, event, impact, joe, team, angle, episode, styles, championship, tag (1) \\\\ & - brown, rihanna, usher, love, girl, loud, yeah, wrote, bow, bad (1) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: Most and least suitable topics according to our LLM-based assessment on different datasets and use cases. In brackets the LLM rating for the coherence of this topic."
    }
  ]
}