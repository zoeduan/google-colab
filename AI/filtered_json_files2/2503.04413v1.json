{
  "title": "Can Large Language Models Predict Antimicrobial Resistance Gene?",
  "authors": [
    "Hyunwoo Yoo"
  ],
  "abstract": "\n This study demonstrates that generative large language models can be utilized in a more flexible manner for DNA sequence analysis and classification tasks compared to traditional transformer encoder-based models. While recent encoder-based models such as DNABERT and Nucleotide Transformer have shown significant performance in DNA sequence classification, transformer decoder-based generative models have not yet been extensively explored in this field. This study evaluates how effectively generative Large Language Models handle DNA sequences with various labels and analyzes performance changes when additional textual information is provided. Experiments were conducted on antimicrobial resistance genes, and the results show that generative Large Language Models can offer comparable or potentially better predictions, demonstrating flexibility and accuracy when incorporating both sequence and textual information. The code and data used in this work are available at the following GitHub repository:  https://github.com/biocomgit/llm4dna . \n",
  "references": [
    {
      "id": null,
      "title": "Can Large Language Models Predict Antimicrobial Resistance Gene?",
      "authors": [
        "Hyunwoo Yoo"
      ],
      "year": "2025",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Introducing llama 3.1: Our most capable models to date",
      "authors": [
        "A I Meta"
      ],
      "year": "2024",
      "venue": "Introducing llama 3.1: Our most capable models to date",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Deeparg: A deep learning approach for predicting antibiotic resistance genes from metagenomic data",
      "authors": [
        "Gustavo Arango-Argoty",
        "Emily Garner",
        "Amy Pruden",
        "Lenwood S Heath",
        "Peter Vikesland",
        "Liqing Zhang"
      ],
      "year": "2018",
      "venue": "Microbiome",
      "doi": "10.1186/s40168-018-0401-z"
    },
    {
      "id": "b2",
      "title": "Megares and amr++, v3.0: an updated comprehensive database of antimicrobial resistance determinants and an improved software pipeline for classification using highthroughput sequencing",
      "authors": [
        "Nathalie Bonin",
        "Enrique Doster",
        "Hannah Worley",
        "Lee J Pinnell",
        "Jonathan E Bravo",
        "Peter Ferm",
        "Simone Marini",
        "Mattia Prosperi",
        "Noelle Noyes",
        "Paul S Morley",
        "Christina Boucher"
      ],
      "year": "2023",
      "venue": "Nucleic Acids Research",
      "doi": "10.1093/nar/gkac1047"
    },
    {
      "id": "b3",
      "title": "Proteinbert: a universal deep-learning model of protein sequence and function",
      "authors": [
        "Nadav Brandes",
        "Dan Ofer",
        "Yam Peleg"
      ],
      "year": "2022",
      "venue": "Nadav Rappoport, and Michal Linial",
      "doi": "10.1093/bioinformatics/btac020"
    },
    {
      "id": "b4",
      "title": "Language models are few-shot learners",
      "authors": [
        "B Tom",
        "Benjamin Brown",
        "Nick Mann",
        "Melanie Ryder",
        "Jared Subbiah",
        "Prafulla Kaplan",
        "Arvind Dhariwal",
        "Pranav Neelakantan",
        "Girish Shyam",
        "Amanda Sastry",
        "Sandhini Askell",
        "Ariel Agarwal",
        "Gretchen Herbert-Voss",
        "Tom Krueger",
        "Rewon Henighan",
        "Aditya Child",
        "Daniel M Ramesh",
        "Jeffrey Ziegler",
        "Clemens Wu",
        "Christopher Winter",
        "Mark Hesse",
        "Eric Chen",
        "Mateusz Sigler",
        "Scott Litwin",
        "Benjamin Gray",
        "Jack Chess",
        "Christopher Clark",
        "Sam Berner",
        "Alec Mccandlish",
        "Ilya Radford",
        "Dario Sutskever",
        "Amodei"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "doi": "10.48550/arXiv.2005.14165"
    },
    {
      "id": "b5",
      "title": "The european bioinformatics institute in 2016: Data growth and integration",
      "authors": [
        "Charles E Cook",
        "Mary Todd Bergman",
        "Robert D Finn",
        "Guy Cochrane",
        "Ewan Birney",
        "Rolf Apweiler"
      ],
      "year": "2016",
      "venue": "The european bioinformatics institute in 2016: Data growth and integration",
      "doi": "10.1093/nar/gkv1352"
    },
    {
      "id": "b6",
      "title": "The nucleotide transformer: Building and evaluating robust foundation models for human genomics",
      "authors": [
        "Hugo Dalla-Torre",
        "Liam Gonzalez",
        "Javier Mendoza-Revilla",
        "Nicolas Lopez Carranza",
        "Adam Henryk Grzywaczewski",
        "Francesco Oteri",
        "Christian Dallago"
      ],
      "year": "2023",
      "venue": "The nucleotide transformer: Building and evaluating robust foundation models for human genomics",
      "doi": "10.1101/2023.01.11.523679"
    },
    {
      "id": "b7",
      "title": "Megares 2.0: a database for classification of antimicrobial drug, biocide and metal resistance determinants in metagenomic sequence data",
      "authors": [
        "Enrique Doster",
        "Christopher J Steven M Lakin",
        "Cory Dean",
        "Jared G Wolfe",
        "Christina Young",
        "Keith E Boucher",
        "Noelle R Belk",
        "Paul S Noyes",
        "Morley"
      ],
      "year": "2020",
      "venue": "Nucleic Acids Research",
      "doi": "10.1093/nar/gkz1010"
    },
    {
      "id": "b8",
      "title": "",
      "authors": [
        "Daniel Han",
        "Michael Han"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "Edward J Hu",
        "Yelong Shen",
        "Phillip Wallis",
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li",
        "Shean Wang",
        "Lu Wang",
        "Weizhu Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Dnabert: pre-trained bidirectional encoder representations from transformers model for dnalanguage in genome",
      "authors": [
        "Yanrong Ji",
        "Zhihan Zhou",
        "Han Liu",
        "Ramana V Davuluri"
      ],
      "year": "2021",
      "venue": "Bioinformatics",
      "doi": "10.1093/bioinformatics/btab083"
    },
    {
      "id": "b11",
      "title": "expansion and model-centric curation of the comprehensive antibiotic resistance database",
      "authors": [
        "Baofeng Jia",
        "Amogelang R Raphenya",
        "Brian Alcock",
        "Nicholas Waglechner",
        "Peiyao Guo",
        "Kara K Tsang",
        "Briony A Lago",
        "Biren M Dave",
        "Sheldon Pereira",
        "Arjun N Sharma",
        "Sachin Doshi",
        "MÃ©lanie Courtot",
        "Raymond Lo",
        "Laura E Williams",
        "Jonathan G Frye",
        "Tariq Elsayegh",
        "Daim Sardar",
        "Erin L Westman",
        "Andrew C Pawlowski",
        "Timothy A Johnson",
        "Fiona S L Brinkman",
        "Gerard D Wright",
        "Andrew G Mcarthur"
      ],
      "year": "2017",
      "venue": "Nucleic Acids Research",
      "doi": "10.1093/nar/gkw1004"
    },
    {
      "id": "b12",
      "title": "Hierarchical hidden markov models enable accurate and diverse detection of antimicrobial resistance sequences",
      "authors": [
        "Steven M Lakin",
        "Alan Kuhnle",
        "Bahar Alipanahi",
        "Noelle R Noyes",
        "Chris Dean",
        "Martin Muggli",
        "Rob Raymond"
      ],
      "year": "2019",
      "venue": "Communications Biology",
      "doi": "10.1038/s42003-019-0545-9"
    },
    {
      "id": "b13",
      "title": "Basic local alignment search tool (blast)",
      "authors": [
        "Ingrid Lobo"
      ],
      "year": "2008",
      "venue": "Nature Education",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Biogpt: Generative pre-trained transformer for biomedical text generation and mining",
      "authors": [
        "Renqian Luo",
        "Liai Sun",
        "Yingce Xia",
        "Tao Qin",
        "Sheng Zhang",
        "Hoifung Poon",
        "Tie-Yan Liu"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics",
      "doi": "10.1093/bib/bbac409"
    },
    {
      "id": "b15",
      "title": "Amr-meta: A k -mer and metafeature approach to classify antimicrobial resistance from highthroughput short-read metagenomics data",
      "authors": [
        "Simone Marini",
        "Marco Oliva",
        "B Ilya",
        "Rishabh A Slizovskiy",
        "Noelle Robertson Das",
        "Tamer Noyes",
        "Christina Kahveci",
        "Mattia Boucher",
        "Prosperi"
      ],
      "year": "2022",
      "venue": "Giga-Science",
      "doi": "10.1093/gigascience/giac029"
    },
    {
      "id": "b16",
      "title": "Predicting anti-microbial resistance using large language models",
      "authors": [
        "Hyunwoo Yoo",
        "Bahrad Sokhansanj",
        "James R Brown",
        "Gail Rosen"
      ],
      "year": "2024",
      "venue": "Predicting anti-microbial resistance using large language models",
      "doi": "10.48550/arXiv.2401.00642"
    },
    {
      "id": "b17",
      "title": "Dnabert-2: Efficient foundation model and benchmark for multispecies genome",
      "authors": [
        "Zhihan Zhou",
        "Yanrong Ji",
        "Weijian Li",
        "Pratik Dutta",
        "Ramana Davuluri",
        "Han Liu"
      ],
      "year": "2023",
      "venue": "Dnabert-2: Efficient foundation model and benchmark for multispecies genome",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Can Large Language Models Predict Antimicrobial Resistance Gene?",
      "text": "Hyunwoo Yoo Drexel University http23@drexel.edu"
    },
    {
      "title": "Abstract",
      "text": "This study demonstrates that generative large language models can be utilized in a more flexible manner for DNA sequence analysis and classification tasks compared to traditional transformer encoder-based models. While recent encoder-based models such as DNABERT and Nucleotide Transformer have shown significant performance in DNA sequence classification, transformer decoder-based generative models have not yet been extensively explored in this field. This study evaluates how effectively generative Large Language Models handle DNA sequences with various labels and analyzes performance changes when additional textual information is provided. Experiments were conducted on antimicrobial resistance genes, and the results show that generative Large Language Models can offer comparable or potentially better predictions, demonstrating flexibility and accuracy when incorporating both sequence and textual information. The code and data used in this work are available at the following GitHub repository: [https://github.com/biocompit/llm4dna](https://github.com/biocompit/llm4dna)."
    },
    {
      "title": "1 Introduction",
      "text": "Language Models have shown notable performance in various tasks in Natural Language Processing and have recently been applied to bioinformatics tasks such as DNA sequence analysis. Encoder-based transformer models trained on nucleotide sequences, such as DNABERT Ji et al. (2021); Zhou et al. (2023) and Nucleotide Transformer Dalla-Torre et al. (2023), have demonstrated excellent performance in DNA sequence classification and are widely used for various gene sequence analyses. Additionally, encoder-based models trained on amino acid sequences have also shown good performance in protein sequence classification Brandes et al. (2022). However, generative Large Language Models, such as GPT based modelsBrown et al. (2020), have not yet been actively utilized for DNA analysis. While models like BioGPT Luo et al. (2022), which are primarily trained on medical and pharmaceutical text, have emerged, models focused specifically on DNA analysis are still relatively rare. Generative large language Models possess the flexibility to make predictions through appropriate prompts, even when incorporating additional textual information, which can significantly enhance performance. This ability to directly utilize supplementary text in the training process sets them apart from encoder-based models, which often struggle with handling complex textual data. Furthermore, generative models can also adapt to situations where the same DNA sequence may have multiple labels, offering a level of versatility not typically seen in models relying on fixed labels. This study primarily investigates how the performance of generative large language models improves when supplementary textual information is provided alongside DNA sequences. Additionally, it explores how these models handle cases where a single DNA sequence may be associated with different labels. Through this, the study aims to demonstrate the potential applications of generative language models in DNA analysis."
    },
    {
      "title": "2 Related Works",
      "text": "This study is connected to existing research on various DNA sequence analyses and the classification of antibiotic resistance genes. **AMR++** introduces an updated database of antimicrobial resistance determinants along with a classification pipeline, which classifies genes based on large-scale sequencing data Bonin et al. (2023). This study designs experimental datasets for DNA sequence analysis using this data. **AMR-meta** presents a method for analyzing high-speed single-read metagenomic data using k-mers and meta-features Marini et al. (2022), contributing to the classification of antibiotic resistance genes. This study builds on this by conducting analyses using various types of metadata. **Meta-MARC** proposes a method for detecting antibiotic resistance gene sequences using hierarchical hidden Markov models, which serves as an important reference for DNA sequence analysis dealing with diverse sequences Lakin et al. (2019). **DeepARG** introduces a deep learning method for predicting antibiotic resistance genes, effectively classifying these genes using metagenomic data Arango-Argoty et al. (2018). This study extends such foundational deep learning-based DNA sequence analysis methods to explore how generative language models can be applied to gene analysis. **Blastn** is a tool that compares the similarity between DNA sequences, performing local sequence alignments between query sequences and target sequences stored in databases. Since the database contains sequences already annotated with gene functions, Blastn can predict genetic functions by finding sequences similar to the query sequence. It divides the query sequence into small fragments (words), finds matching sequences, extends them, and evaluates the similarity by providing results with an E-value. It is widely used in bioinformatics for gene function prediction, evolutionary relationship analysis, and sequence variation detection Lobo (2008). **DNABERT** is one of the first models to apply BERT to DNA sequence analysis. Based on the BERT architecture, which is primarily used in Natural Language Processing, DNABERT tokenizes DNA sequences and learns the meaning of each nucleotide sequence Ji et al. (2021). The main technique of DNABERT1 is the use of k-mer tokenization, where DNA nucleotide sequences are divided into 3-mers or 6-mers to process them like words in natural language. This helps better understand the relationships between sequences and extract patterns within genes. DNABERT maintains BERT's pre-training and fine-tuning techniques while performing pre-training on large-scale DNA sequence data, resulting in high performance in tasks such as promoter prediction, splicing site detection, and DNA sequence classification. The self-attention mechanism of the transformer model allows DNABERT to effectively learn long-term dependencies within DNA sequences, performing similarly or slightly better than traditional rule-based models or simple sequence alignment algorithms. **DNABERT2**, while maintaining the BERT-based transformer architecture, includes more biological data in its pre-training, enabling the model to learn complex genetic interactions and fine DNA patterns Zhou et al. (2023). DNABERT2 overcomes the inefficiencies of traditional k-mer tokenization by introducing the Byte Pair Encoding method for DNA sequence analysis. While the k-mer method used fixed-length sequence fragments as tokens, leading to inefficiencies in processing, DNABERT2 implements BPE to merge frequently occurring sequences, enabling more efficient non-overlapping tokenization. This allows the model to overcome input length limitations, reduce time and memory usage, and improve performance. **Nucleotide Transformer** is a transformer-based model pre-trained on various human and non-human genomic data, showing excellent performance in predicting molecular phenotypes from DNA sequences. The model is trained on 3,202 human genomes and 850 genomes from various species, generating context-specific representations Dalla-Torre et al. (2023). These representations allow for high-accuracy predictions even with small amounts of data, matching or surpassing existing specialized methods in 11 of 18 prediction tasks. After fine-tuning, the performance improved in 15 tasks. Nucleotide Transformer focuses on learning important genetic elements, such as enhancers, that regulate gene expression, and it has shown the ability to identify such elements without supervised learning. Additionally, it has been demonstrated that utilizing the model's representations can improve the prioritization of functional genetic variants."
    },
    {
      "title": "3 Methods",
      "text": ""
    },
    {
      "title": "Data Collection",
      "text": "The data used in this study consists of antibiotic resistance gene data collected from the MEGARes Doster et al. (2020) and CARD databases Jia et al. (2017). The different labels from MEGARes and CARD were integrated using the Antibiotic Resistance Ontology from the European Bioinformatics Institute, as described in previous research methods Yoo et al. (2024) Cook et al. (2016). These databases contain DNA sequences of various antibiotic resistance genes, and each sequence is classified with one or more labels. Additionally, the BLASTn algorithm was used to search for antibiotic resistance sequences similar to each DNA sequence, and the top 5 search results were selected based on the e-value criterion."
    },
    {
      "title": "Preprocessing",
      "text": "The collected DNA sequences underwent preprocessing for analysis. First, all DNA sequences were converted to uppercase, and invalid sequences were removed from the BLASTn results. The final dataset was constructed by including only antibiotic resistance gene sequences."
    },
    {
      "title": "Fine-Tuning",
      "text": "In this study, we applied the Low-Rank Adaptation (LoRa) technique to fine-tune the LLaMA model and the ChatGPT4-mini model for DNA sequence classification tasks. LoRa is an efficient method that converts only a portion of the parameters of large models into low-rank matrices for additional training, thereby reducing memory and computational costs while maintaining model performance (Hu et al., 2021). Additional experiments were conducted using the Claude 3.5 sonet API, and the performance of various generative language models was compared and analyzed."
    },
    {
      "title": "4 Experiments",
      "text": ""
    },
    {
      "title": "Setup",
      "text": "In this study, we used the 8B version of the LLaMA 3.1 model(AI, 2024) with 4-bit quantization from the unsloth model (Han and Han, 2023) as the foundation model for our experiments. Additionally, the LLaMA 405B model provided through the Amazon Bedrock API was utilized. Separate experiments were conducted with ChatGPT4-mini and the Claude 3.5 sonet API to evaluate the performance of these diverse language models."
    },
    {
      "title": "Metric",
      "text": "The performance of the model was evaluated based on the accurate classification of antibiotic resistance genes in the DNA sequences. For fine-tuned models, this criterion was used since they mostly returned the correct class labels in the output. However, for non-fine-tuned models, they rarely returned just the label names. Therefore, we implemented a model that maps class labels from rather lengthy and verbose explanations, and using this model, we extracted the class labels again. The classification performance of the large language models was then evaluated based on these labels. In addition to classification accuracy, the models were also evaluated using precision, recall, and F1 score."
    },
    {
      "title": "5 Results",
      "text": "The experimental results showed that generative large language models performed just as well as traditional encoder-based models in handling multiple labels for DNA sequences. Furthermore, when additional gene information from the Blastn DB search results was provided, performance improved even without additional training on this data. As seen in Table 1, the Unclassified Rate decreased across all models. For the LLaMA 3.1 8B-4bit quantized model, the rate dropped from 97% to 73% when using Blastn. For Claude 3.5 sonet, it decreased from 39% to 11%. ChatGPT 4-mini showed a sharp improvement, going from classifying nothing to only leaving 14% unclassified. When fine-tuning was applied, both the LLaMA 3.1 8B 4bit quantized model and ChatGPT 4-mini reduced their unclassified rates to 0%. As shown in Table 2, overall classification performance also improved when using additional Blastn information or fine-tuning. For example, the accuracy of the LLaMA 3.1 8B-4bit model increased from 0.0037 to 0.0744 when using Blastn, and to 0.5521 with fine-tuning. The F1 Score followed a similar trend. Claude 3.5 sonet and ChatGPT 4-mini also showed improvements in accuracy, preci \\begin{table} \\begin{tabular}{c c} \\hline **Model** & **Unclassified Rate** \\\\ \\hline LLama3.1 8B-4bit & 97\\% \\\\ (Base Model) & \\\\ LLama3.1 8B-4bit & 73\\% \\\\ (Blastn) & \\\\ LLama3.1 8B-4bit & 0\\% \\\\ (Finetuning) & \\\\ Claude3.5sonet & 39\\% \\\\ (Base Model) & \\\\ Claude3.5sonet & 11\\% \\\\ (Blastn) & 100\\% \\\\ Chatgpt4o-mini & 14\\% \\\\ (Blastn) & 0\\% \\\\ Chatgpt4o-mini & 0\\% \\\\ (Finetuning) & \\\\ \\hline \\end{tabular} \\end{table} Table 1: Model unclassified rates with long names displayed in two lines. sion, recall, and F1 score when using the additional Blastn information. With fine-tuning, the accuracy of the ChatGPT 4-mini model rose to 0.9318, comparable to language models trained solely on DNA data. Without additional information or fine-tuning, all metrics for the ChatGPT 4-mini model were zero, indicating that the model made no predictions. The model's responses suggested that it could not make a judgment due to insufficient evidence. Table 3 shows the classification accuracy on test data based on the CARD labels without any additional information or training. The test was conducted using models fine-tuned on the integrated MEGARes-based label data, and the accuracy was 0.2307 for the LLaMA 3.1 8B-4bit model and 0.5037 for the ChatGPT 4-mini model. This demonstrates that large language models can still handle different labeling environments to some extent. Large language models not only flexibly incorporate additional textual information but also improve their performance with this data. Moreover, they can handle previously unseen labels to some extent, and fine-tuning further enhances their performance. This shows that generative language models are advantageous in solving more complex problems, such as classifying DNA AMR drug classes, by integrating diverse information."
    },
    {
      "title": "6 Conclusion",
      "text": "This study confirmed that generative large language models can be applied more flexibly to DNA sequence analysis and classification tasks compared to traditional encoder-based models. In particular, we found that model performance improved when additional textual information was provided, offering important implications for future DNA analysis and antibiotic resistance gene classification tasks. Additionally, experiments showed that generative models are more flexible and accurate in handling cases where different labels exist for the same sequence, even if these labels differ from those in the training data."
    },
    {
      "title": "7 Discussion",
      "text": "This study highlighted the flexibility of generative large language models and their ability to utilize additional textual information, showing that they can play an important role in DNA sequence analysis. Generative models have the potential to handle unseen labels effectively and integrate supplementary information to solve more complex problems. However, this study used a limited dataset, and future research should expand the scope of analysis by including a wider range of DNA sequences and antibiotic resistance gene data. Extracting specific class labels from long texts was not a straightforward task. Initially, the final prediction was based on the antibiotic name most frequently mentioned in the model's output. In cases where multiple antibiotics were predicted equally, they were excluded from evaluation, but this method was not very accurate. To address this, a model was implemented to extract class labels from lengthy texts for evaluation. However, the evaluation may vary depending on the accuracy of this extraction model. \\begin{table} \\begin{tabular}{l c c c c} \\hline **Model** & **Accuracy** & **Precision** & **Recall** & **F1 Score** \\\\ \\hline LLama3.1 8B-4bit & 0.0037 & 0.0011 & 0.0002 & 0.0003 \\\\ LLama3.1 8B-4bit + Blastn & 0.0744 & 0.0530 & 0.0129 & 0.0207 \\\\ LLama3.1 8B-4bit + Finetuing & 0.5521 & 0.4760 & 0.5521 & 0.5080 \\\\ Claude3.5sonet & 0.1488 & 0.1770 & 0.0966 & 0.0735 \\\\ Claude3.5sonet + Blastn & 0.8042 & 0.6287 & 0.5421 & 0.5794 \\\\ Chatgpt4o-mini & 0.00 & 0.00 & 0.00 & 0.00 \\\\ Chatgpt4o-mini + Blastn & 0.7804 & 0.9090 & 0.7804 & 0.8398 \\\\ Chatgpt4o-mini + Finetuning & 0.9318 & 0.9337 & 0.9318 & 0.9319 \\\\ \\hline \\end{tabular} \\end{table} Table 2: Performance metrics for various large language models. \\begin{table} \\begin{tabular}{l c} \\hline **Model** & **Accuracy** \\\\ \\hline LLama 3.1 8B-4bit & \\\\ (Fine-tuning) & 0.2307 \\\\ Chatgpt 4o-mini & 0.5037 \\\\ \\hline \\end{tabular} \\end{table} Table 3: Model accuracy results with different label dataset."
    },
    {
      "title": "References",
      "text": "* M. AI (2024)Introducing llama 3.1: our most capable models to date. Note: [https://ai.meta.com/blog/meta-llama-3-1/Accessed](https://ai.meta.com/blog/meta-llama-3-1/Accessed): 2024-09-06 Cited by: SS1. * G. Arango-Argoty, E. Garner, A. Pruden, L. S. Heath, P. Vikesland, and L. Zhang (2018)Deeparg: a deep learning approach for predicting antibiotic resistance genes from metagenomic data. Microbiome6 (1), pp. 23. Cited by: SS1. * N. Bonin, E. Doster, H. Worley, L. J. Pinnell, J. E. Bravo, P. Ferm, S. Marini, M. Prosperi, N. Noyes, P. S. Morley, and C. Boucher (2023)Megares and amr++, v3.0: an updated comprehensive database of antimicrobial resistance determinants and an improved software pipeline for classification using high-throughput sequencing. Nucleic Acids Research51 (D1), pp. D744-D752. Cited by: SS1. * N. Brandes, D. Ofer, Y. Peleg, N. Rappoport, and M. Linial (2022)Proteinbert: a universal deep-learning model of protein sequence and function. Bioinformatics38 (8), pp. 2102-2110. Cited by: SS1. * T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020)Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Cited by: SS1. * C. E. Cook, M. T. Bergman, R. D. Finn, G. Cochrane, E. Birney, and R. Apweiler (2016)The european bioinformatics institute in 2016: data growth and integration. Nucleic Acids Research44 (D1), pp. D20-D26. Cited by: SS1. * H. Dalla-Torre, L. Gonzalez, J. Mendoza-Revilla, N. Lopez Carranza, A. Henryk Grzywaczewski, F. Oteri, C. Dallago, et al. (2023)The nucleotide transformer: building and evaluating robust foundation models for human genomics. Genomics. Genomics. Cited by: SS1. * E. Doster, S. M. Lakin, C. J. Dean, C. Wolfe, J. G. Young, C. Boucher, K. E. Belk, N. R. Noyes, and P. S. Morley (2020)Megares 2.0: a database for classification of antimicrobial drug, biocide and metal resistance determinants in metagenomic sequence data. Nucleic Acids Research48 (D1), pp. D561-D569. Cited by: SS1. * D. Han and M. Han (2023)unsloth. Note: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth) Cited by: SS1. * E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen (2021)Lora: low-rank adaptation of large language models. arXiv. External: 2106.09685v2 Cited by: SS1. * S. M. Lakin, A. R. Raphenya, B. Alcock, N. Waglechner, P. Guo, K. K. Tsang, B. A. Lago, B. M. Dave, S. Pereira, A. N. Sharma, S. Doshi, M. Courtot, R. Lo, L. E. Williams, J. G. Frye, T. Elsayegh, D. Sardar, E. L. Westman, A. C. Pawlowski, T. A. Johnson, F. S.L. Brinkman, G. D. Wright, and A. G. McArthur (2017)Card 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database. Nucleic Acids Research45 (D1), pp. D566-D573. Cited by: SS1. * S. M. Lakin, A. Kuhnle, B. Alipanahi, N. R. Noyes, C. Dean, M. Muggli, R. Raymond, et al. (2019)Hierarchical hidden markov models enable accurate and diverse detection of antimicrobial resistance sequences. Communications Biology2 (1), pp. 294. Cited by: SS1. * I. Lobo (2008)Basic local alignment search tool (blast). Nature Education1 (1), pp. 215. Cited by: SS1. * R. Luo, L. Sun, Y. Xia, T. Qin, S. Zhang, H. Poon, and T. Liu (2022)BiogPT: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics23 (6), pp. bbaac409. Cited by: SS1. * S. Marini, M. Oliva, I. B. Slizovskiy, R. A. Das, N. Robertson Noyes, T. Kahveci, C. Boucher, and M. Prosperi (2022)Amr-meta: a k -mer and metafeature approach to classify antimicrobial resistance from high-throughput short-read metagenomics data. GigaScience11, pp. Giaco029. Cited by: SS1. * H. Yoo, B. Sokhansanj, J. R. Brown, and G. Rosen (2024)Predicting anti-microbial resistance using large language models. arXiv preprint arXiv:2401.00642. Note: [https://doi.org/10.48550/arXiv.2401.00642](https://doi.org/10.48550/arXiv.2401.00642) Cited by: SS1. * Z. Zhou, Y. Ji, W. Li, P. Dutta, R. Davuluri, and H. Liu (2023)DnaBERT-2: efficient foundation model and benchmark for multi-species genome. arXiv. External: 2306.15006v1 Cited by: SS1."
    },
    {
      "title": "Appendix A Appendix",
      "text": ""
    },
    {
      "title": "Example Prompt Explanation Including Dna Sequence",
      "text": "In this example prompt, a DNA sequence is provided along with several drug class labels,such as Sulfonamides, Aminoglycososides, Betalactams, Glycopeptides, Tetracyclines, Phenicol, Fluoroquinolones, MLS (Macrolide-Lincosamide-Streptogramin), and Multi-drug resistance. The task involves asking the model to determine the drug class that the DNA sequence is resistant to. The prompt follows this format: \"Tell me the resistance drug among drugs (Sulfonamides, Aminoglycos, Betalactams, Glycopeptides, Tetracyclines, Phenicol, Fluoroquinolones, MLS, Multi-drug_resistance) with DNA sequence (ATGAATCCCTATC......ACAACTGCAGGCAGTTGCATGA)?\" This prompt is used to assess the DNA sequence for antibiotic resistance and classify the sequence into one of the specified drug resistance categories."
    },
    {
      "title": "Example Prompt Explanation Including Blast Information",
      "text": "In this prompt, a DNA sequence and the top 5 Blastn search results are provided. The task is to predict the drug class that the DNA sequence is resistant to, based on the alignment information and matching sequences. The drug class labels included in the prompt are Sulfonamides, Aminoglycos, Betalactams, Glycopeptides, Tetracyclines, Phenicol, Fluoroquinolones, MLS (Macrolide-Lincosamide-Streptogramin), and Multi-drug resistance. The BLASTn results contain gene information such as sequence titles, alignment length, e-values, and detailed sequence alignments (query, match, and subject sequences). This allows the model to analyze the DNA sequence's pattern and classify it into the appropriate drug resistance category. The prompt follows this format: \"Tell me the resistance drug among drugs (Sulfonamides, Aminoglycos, Betalactams, Glycopeptides, Tetracyclines, Phenicol, Fluoroquinolones, MLS, Multi-drug_resistance) with DNA information ([{'sequence_title': 'gi|1035502645|ref|NG_048504.1| Enterococcus casselflavus vanXY-C gene for D-Ala-D-Ala dipeptidase/D-Ala-D-Ala carboxypeptidase VanXY-C, complete CDS', 'alignment_length': 673, 'e_value': 0.0, 'query_sequence': 'ATGAATCCCTATCTA...', 'match_sequence': '||||||||||||...', 'subject_sequence':...'},... ]?\" This prompt aims to predict the antibiotic resistance drug by using DNA sequence data from the Blastn search results and identifying the relevant drug resistance class."
    }
  ]
}