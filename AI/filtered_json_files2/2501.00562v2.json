{
  "title": "An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems",
  "authors": [
    "Hashmath Shaik",
    "Alex Doboli"
  ],
  "abstract": "\n Large Language Models offer new opportunities to devise automated implementation generation methods that can tackle problem solving activities beyond traditional methods, which require algorithmic specifications and can use only static domain knowledge, like performance metrics and libraries of basic building blocks. Large Language Models could support creating new methods to support problem solving activities for open-ended problems, like problem framing, exploring possible solving approaches, feature elaboration and combination, more advanced implementation assessment, and handling unexpected situations. This report summarized the current work on Large Language Models, including model prompting, Reinforcement Learning, and Retrieval-Augmented Generation. Future research requirements were also discussed. \n",
  "references": [
    {
      "id": null,
      "title": "An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems",
      "authors": [
        "Hashmath Shaik",
        "Alex Doboli"
      ],
      "year": "2025",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Toward an understanding of macrocognition in teams: Pre-dicting processes in complex collaborative contexts",
      "authors": [
        "S Fiore",
        "M Rosen",
        "K Smith-Jentsch",
        "E Salas",
        "L M",
        "N Warner"
      ],
      "year": "2010",
      "venue": "Human Factors",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "The process of solving complex problems",
      "authors": [
        "A Fischer",
        "S Greiff",
        "J Funke"
      ],
      "year": "2012",
      "venue": "Journal of Problem Solving",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Towards a generalized competency model of collaborative problem solving",
      "authors": [
        "C Sun",
        "V Shute",
        "A Stewart",
        "J Yonehiro",
        "N Duran",
        "S D Mello"
      ],
      "year": "2020",
      "venue": "Computers & Education",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Problem-solving phase transitions during team collaboration",
      "authors": [
        "T Wiltshire",
        "J Butner",
        "S Fiore"
      ],
      "year": "2018",
      "venue": "Cognitive Science",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Cognitive processes in well-defined and ill-defined problem solving",
      "authors": [
        "G Schraw",
        "M E Dunkle",
        "L D Bendixen"
      ],
      "year": "1995",
      "venue": "Applied Cognitive Psychology",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "The role of precedents in increasing creativity during iterative design of electronic embedded systems",
      "authors": [
        "A Doboli",
        "A Umbarkar"
      ],
      "year": "2014",
      "venue": "Design Studies",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Modeling semantic knowledge structures for creative problem solving: Studies on expressing concepts, categories, associations, goals and context",
      "authors": [
        "A Doboli",
        "A Umbarkar",
        "S Doboli",
        "J Betz"
      ],
      "year": "2015",
      "venue": "Knowledgebased Systems",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Reuse, parameterized reuse, and hierarchical reuse of substructures in evolving electrical circuits using genetic programming",
      "authors": [
        "J R Koza",
        "F H Bennett",
        "D Andre",
        "M A Keane"
      ],
      "year": "",
      "venue": "Evolvable Systems: From Biology to Hardware: First International Conference",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "",
      "authors": [
        "Japan Tsukuba"
      ],
      "year": "1996",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Problem frame patterns: an exploration of patterns in the problem space",
      "authors": [
        "R Wirfs-Brock",
        "P Taylor",
        "J Noble"
      ],
      "year": "2006",
      "venue": "Proc. Conference on Pattern Languages of Programs",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "High-level synthesis of delta-sigma modulators optimized for complexity, sensitivity and power consumption",
      "authors": [
        "H Tang",
        "A Doboli"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on CADICS",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Systematic methodology for designing reconfigurable delta sigma modulator topologies for multimode communication systems",
      "authors": [
        "Y Wei",
        "H Tang",
        "A Doboli"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on CADICS",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Improvement of skills for solving-illdefined problems",
      "authors": [
        "G A Klein",
        "J Weitzenfeld"
      ],
      "year": "1978",
      "venue": "Educational Psychologist",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Assessment of student problem-solving on ill-defined tasks",
      "authors": [
        "J P Leighton",
        "W T Rogers",
        "T O Maguire"
      ],
      "year": "1999",
      "venue": "Alberta Journal of Educational Research",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "A novel agent-based, evolutionary model for expressing the dynamics of creative open-problem solving in small groups",
      "authors": [
        "A Doboli",
        "S Doboli"
      ],
      "year": "2021",
      "venue": "Applied Intelligence",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions",
      "authors": [
        "R Wang",
        "J Lehman",
        "A Rawal",
        "J Zhi",
        "Y Li",
        "J Clune",
        "K Stanley"
      ],
      "year": "2020",
      "venue": "International conference on machine learning",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "The SOAR Cognitive Architecture",
      "authors": [
        "A Aho",
        "J Ullman",
        "R Sethi",
        "M Lam"
      ],
      "year": "2006",
      "venue": "The SOAR Cognitive Architecture",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "A librarybased approach to analog synthesis from vhdl-ams specifications",
      "authors": [
        "A Doboli",
        "N Dhanwada",
        "A Nunez-Aldana",
        "R Vemuri"
      ],
      "year": "2004",
      "venue": "ACM Transactions on Design Automation",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "High-Level Synthesis Blue Book",
      "authors": [
        "M Fingeroff"
      ],
      "year": "2010",
      "venue": "High-Level Synthesis Blue Book",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Variation-aware Analog Structural Synthesis",
      "authors": [
        "T Mcconaghy",
        "P Palmers",
        "P Gao",
        "M Steyaert",
        "G Gielen"
      ],
      "year": "2009",
      "venue": "Variation-aware Analog Structural Synthesis",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Behavioral modeling for high-level synthesis of analog and mixed-signal systems from vhdl-ams",
      "authors": [
        "A Doboli",
        "R Vemuri"
      ],
      "year": "2003",
      "venue": "IEEE Transactions on CADICS",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Exploration-based high-level synthesis of linear analog systems operating at low/medium frequencies",
      "authors": [],
      "year": "2003",
      "venue": "IEEE Transactions on CADICS",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "When concepts combine",
      "authors": [
        "E Wisniewski"
      ],
      "year": "1997",
      "venue": "Psychonomic Bulletin & Review",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Darwin: Cmos opamp synthesis by means of genetic algorithm",
      "authors": [
        "W Kruiskamp",
        "D Leenaerts"
      ],
      "year": "1995",
      "venue": "Proc. Design Automation Conference",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Research directions in agent communication",
      "authors": [
        "A Chopra",
        "A Artikis",
        "J Bentahar",
        "M Colombetti",
        "F Dignum",
        "N Fornara",
        "A Jones",
        "M Singh",
        "P Yolum"
      ],
      "year": "2013",
      "venue": "ACM Trans. Intell. Syst. Technol",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Agent-based modeling: methods and techniques for simulating human systems",
      "authors": [
        "E Bonabeau"
      ],
      "year": "",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Collaborating with style: Using an agent-based model to simulate cognitive style diversity in problem solving teams",
      "authors": [
        "S Lapp",
        "K Jablokow",
        "C Mccomb"
      ],
      "year": "2017",
      "venue": "Proc. ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Act: A simple theory of complex cognition",
      "authors": [
        "J Anderson"
      ],
      "year": "1996",
      "venue": "American Psychologist",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Compilers: Principles, Techniques, and Tools",
      "authors": [
        "J Laird"
      ],
      "year": "2012",
      "venue": "Compilers: Principles, Techniques, and Tools",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "The sigma cognitive architecture and system: towards functionally elegant grand unification",
      "authors": [
        "P Rosenbloom",
        "A Demski",
        "U Volkan"
      ],
      "year": "2016",
      "venue": "Journal of Artificial General Intelligence",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "An overview of the epic architecture for cognition and performance with application to human-computer interaction",
      "authors": [
        "D Kieras",
        "D Meyer"
      ],
      "year": "1997",
      "venue": "Journal Human-Computer Interaction",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "A tutorial on clarion 5.0. Cognitive Science Department, Rensselaer Polytechnic",
      "authors": [
        "R Sun"
      ],
      "year": "2003",
      "venue": "A tutorial on clarion 5.0. Cognitive Science Department, Rensselaer Polytechnic",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Innova: A cognitive architecture for computational innovation through robust divergence and its application for analog circuit design",
      "authors": [
        "H Li",
        "X Liu",
        "F Jiao",
        "A Doboli",
        "S Doboli"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on CADICS",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J D Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Y Bengio",
        "A Courville",
        "P Vincent"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Building machines that learn and think like people",
      "authors": [
        "B M Lake",
        "T D Ullman",
        "J B Tenenbaum",
        "S J Gershman"
      ],
      "year": "2017",
      "venue": "Behavioral and brain sciences",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Ethical and social risks of harm from language models",
      "authors": [
        "L Weidinger",
        "J Mellor",
        "M Rauh",
        "C Griffin",
        "J Uesato",
        "P.-S Huang",
        "M Cheng",
        "M Glaese",
        "B Balle",
        "A Kasirzadeh"
      ],
      "year": "2021",
      "venue": "Ethical and social risks of harm from language models",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "authors": [
        "S Bubeck",
        "V Chandrasekaran",
        "R Eldan",
        "J Gehrke",
        "E Horvitz",
        "E Kamar",
        "P Lee",
        "Y T Lee",
        "Y Li",
        "S Lundberg"
      ],
      "year": "2023",
      "venue": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Defending against neural fake news",
      "authors": [
        "R Zellers",
        "A Holtzman",
        "H Rashkin",
        "Y Bisk",
        "A Farhadi",
        "F Roesner",
        "Y Choi"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Fast lexically constrained decoding with dynamic beam allocation for neural machine translation",
      "authors": [
        "M Post",
        "D Vilar"
      ],
      "year": "2018",
      "venue": "Fast lexically constrained decoding with dynamic beam allocation for neural machine translation",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Parameter-efficient transfer learning for nlp",
      "authors": [
        "N Houlsby",
        "A Giurgiu",
        "S Jastrzebski",
        "B Morrone",
        "Q De Laroussilhe",
        "A Gesmundo",
        "M Attariyan",
        "S Gelly"
      ],
      "year": "2019",
      "venue": "International conference on machine learning",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Shortcut learning in deep neural networks",
      "authors": [
        "R Geirhos",
        "J.-H Jacobsen",
        "C Michaelis",
        "R Zemel",
        "W Brendel",
        "M Bethge",
        "F A Wichmann"
      ],
      "year": "2020",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Improving large language models for clinical named entity recognition via prompt engineering",
      "authors": [
        "Y Hu",
        "Q Chen",
        "J Du",
        "X Peng",
        "V K Keloth",
        "X Zuo",
        "Y Zhou",
        "Z Li",
        "X Jiang",
        "Z Lu"
      ],
      "year": "2024",
      "venue": "Journal of the American Medical Informatics Association",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Medical transcriptions",
      "authors": [
        "T Boyle"
      ],
      "year": "2018",
      "venue": "Medical transcriptions",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Centers for Disease Control and Prevention (CDC) and U.S. Food and Drug Administration (FDA)",
      "authors": [],
      "year": "2024",
      "venue": "Centers for Disease Control and Prevention (CDC) and U.S. Food and Drug Administration (FDA)",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q V Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Complexitybased prompting for multi-step reasoning",
      "authors": [
        "Y Fu",
        "H Peng",
        "A Sabharwal",
        "P Clark",
        "T Khot"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Thread of thought unraveling chaotic contexts",
      "authors": [
        "Y Zhou",
        "X Geng",
        "T Shen",
        "C Tao",
        "G Long",
        "J.-G Lou",
        "J Shen"
      ],
      "year": "2023",
      "venue": "Thread of thought unraveling chaotic contexts",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources",
      "authors": [
        "X Li",
        "R Zhao",
        "Y K Chia",
        "B Ding",
        "S Joty",
        "S Poria",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Chain of code: Reasoning with a language model-augmented code emulator",
      "authors": [
        "C Li",
        "J Liang",
        "A Zeng",
        "X Chen",
        "K Hausman",
        "D Sadigh",
        "S Levine",
        "L Fei-Fei",
        "F Xia",
        "B Ichter"
      ],
      "year": "2023",
      "venue": "Chain of code: Reasoning with a language model-augmented code emulator",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Enhancing zero-shot chain-of-thought reasoning in large language models through logic",
      "authors": [
        "X Zhao",
        "M Li",
        "W Lu",
        "C Weber",
        "J H Lee",
        "K Chu",
        "S Wermter"
      ],
      "year": "2023",
      "venue": "Enhancing zero-shot chain-of-thought reasoning in large language models through logic",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Chain-of-event prompting for multidocument summarization by large language models",
      "authors": [
        "S Bao",
        "T Li",
        "B Cao"
      ],
      "year": "2024",
      "venue": "International Journal of Web Information Systems",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Chain-of-table: Evolving tables in the reasoning chain for table understanding",
      "authors": [
        "Z Wang",
        "H Zhang",
        "C.-L Li",
        "J M Eisenschlos",
        "V Perot",
        "Z Wang",
        "L Miculicich",
        "Y Fujii",
        "J Shang",
        "C.-Y Lee"
      ],
      "year": "2024",
      "venue": "Chain-of-table: Evolving tables in the reasoning chain for table understanding",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Self-consistency improves chain of thought reasoning in language models",
      "authors": [
        "X Wang",
        "J Wei",
        "D Schuurmans",
        "Q Le",
        "E Chi",
        "S Narang",
        "A Chowdhery",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Self-consistency improves chain of thought reasoning in language models",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Contrastive chain-of-thought prompting",
      "authors": [
        "Y K Chia",
        "G Chen",
        "L A Tuan",
        "S Poria",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Contrastive chain-of-thought prompting",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Federated prompting and chain-ofthought reasoning for improving llms answering",
      "authors": [
        "X Liu",
        "T Pang",
        "C Fan"
      ],
      "year": "2023",
      "venue": "International Conference on Knowledge Science, Engineering and Management",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Tree of thoughts: Deliberate problem solving with large language models",
      "authors": [
        "S Yao",
        "D Yu",
        "J Zhao",
        "I Shafran",
        "T Griffiths",
        "Y Cao",
        "K Narasimhan"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Maieutic prompting: Logically consistent reasoning with recursive explanations",
      "authors": [
        "J Jung",
        "L Qin",
        "S Welleck",
        "F Brahman",
        "C Bhagavatula",
        "R L Bras",
        "Y Choi"
      ],
      "year": "2022",
      "venue": "Maieutic prompting: Logically consistent reasoning with recursive explanations",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
      "authors": [
        "L Wang",
        "W Xu",
        "Y Lan",
        "Z Hu",
        "Y Lan",
        "R K W Lee",
        "E.-P Lim"
      ],
      "year": "2023",
      "venue": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
      "authors": [
        "W Chen",
        "X Ma",
        "X Wang",
        "W W Cohen"
      ],
      "year": "2022",
      "venue": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Chainof-symbol prompting elicits planning in large langauge models",
      "authors": [
        "H Hu",
        "H Lu",
        "H Zhang",
        "Y.-Z Song",
        "W Lam",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Chainof-symbol prompting elicits planning in large langauge models",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Structured chain-of-thought prompting for code generation",
      "authors": [
        "J Li",
        "G Li",
        "Y Li",
        "Z Jin"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "Reasoning implicit sentiment with chain-of-thought prompting",
      "authors": [
        "H Fei",
        "B Li",
        "Q Liu",
        "L Bing",
        "F Li",
        "T.-S Chua"
      ],
      "year": "2023",
      "venue": "Reasoning implicit sentiment with chain-of-thought prompting",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Towards expertlevel medical question answering with large language models",
      "authors": [
        "K Singhal",
        "T Tu",
        "J Gottweis",
        "R Sayres",
        "E Wulczyn",
        "L Hou",
        "K Clark",
        "S Pfohl",
        "H Cole-Lewis",
        "D Neal"
      ],
      "year": "2023",
      "venue": "Towards expertlevel medical question answering with large language models",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "Automatic chain of thought prompting in large language models",
      "authors": [
        "Z Zhang",
        "A Zhang",
        "M Li",
        "A Smola"
      ],
      "year": "2022",
      "venue": "Automatic chain of thought prompting in large language models",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "React: Synergizing reasoning and acting in language models",
      "authors": [
        "S Yao",
        "J Zhao",
        "D Yu",
        "N Du",
        "I Shafran",
        "K Narasimhan",
        "Y Cao"
      ],
      "year": "2022",
      "venue": "React: Synergizing reasoning and acting in language models",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "Active prompting with chain-of-thought for large language models",
      "authors": [
        "S Diao",
        "P Wang",
        "Y Lin",
        "R Pan",
        "X Liu",
        "T Zhang"
      ],
      "year": "2023",
      "venue": "Active prompting with chain-of-thought for large language models",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "Mathprompter: Mathematical reasoning using large language models",
      "authors": [
        "S Imani",
        "L Du",
        "H Shrivastava"
      ],
      "year": "2023",
      "venue": "Mathprompter: Mathematical reasoning using large language models",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "Large language models as analogical reasoners",
      "authors": [
        "M Yasunaga",
        "X Chen",
        "Y Li",
        "P Pasupat",
        "J Leskovec",
        "P Liang",
        "E H Chi",
        "D Zhou"
      ],
      "year": "2023",
      "venue": "Large language models as analogical reasoners",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "Synthetic prompting: Generating chain-of-thought demonstrations for large language models",
      "authors": [
        "Z Shao",
        "Y Gong",
        "Y Shen",
        "M Huang",
        "N Duan",
        "W Chen"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "System 2 attention (is something you might need too)",
      "authors": [
        "J Weston",
        "S Sukhbaatar"
      ],
      "year": "2023",
      "venue": "System 2 attention (is something you might need too)",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "Metacognitive prompting improves understanding in large language models",
      "authors": [
        "Y Wang",
        "Y Zhao"
      ],
      "year": "2023",
      "venue": "Metacognitive prompting improves understanding in large language models",
      "doi": ""
    },
    {
      "id": "b73",
      "title": "Least-to-most prompting enables complex reasoning in large language models",
      "authors": [
        "D Zhou",
        "N Schärli",
        "L Hou",
        "J Wei",
        "N Scales",
        "X Wang",
        "D Schuurmans",
        "C Cui",
        "O Bousquet",
        "Q Le"
      ],
      "year": "2022",
      "venue": "Least-to-most prompting enables complex reasoning in large language models",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "Decomposed prompting: A modular approach for solving complex tasks",
      "authors": [
        "T Khot",
        "H Trivedi",
        "M Finlayson",
        "Y Fu",
        "K Richardson",
        "P Clark",
        "A Sabharwal"
      ],
      "year": "2022",
      "venue": "Decomposed prompting: A modular approach for solving complex tasks",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "Pal: Program-aided language models",
      "authors": [
        "L Gao",
        "A Madaan",
        "S Zhou",
        "U Alon",
        "P Liu",
        "Y Yang",
        "J Callan",
        "G Neubig"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b76",
      "title": "Binding language models in symbolic languages",
      "authors": [
        "Z Cheng",
        "T Xie",
        "P Shi",
        "C Li",
        "R Nadkarni",
        "Y Hu",
        "C Xiong",
        "D Radev",
        "M Ostendorf",
        "L Zettlemoyer"
      ],
      "year": "2022",
      "venue": "Binding language models in symbolic languages",
      "doi": ""
    },
    {
      "id": "b77",
      "title": "Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning",
      "authors": [
        "Y Ye",
        "B Hui",
        "M Yang",
        "B Li",
        "F Huang",
        "Y Li"
      ],
      "year": "2023",
      "venue": "Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning",
      "doi": ""
    },
    {
      "id": "b78",
      "title": "Retrievalaugmented generation for knowledge-intensive nlp tasks",
      "authors": [
        "P Lewis",
        "E Perez",
        "A Piktus",
        "F Petroni",
        "V Karpukhin",
        "N Goyal",
        "H Küttler",
        "M Lewis",
        "W -T. Yih",
        "T Rocktäschel"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b79",
      "title": "Reducing hallucination in structured outputs via retrieval-augmented generation",
      "authors": [
        "P Béchard",
        "O M Ayala"
      ],
      "year": "2024",
      "venue": "Reducing hallucination in structured outputs via retrieval-augmented generation",
      "doi": ""
    },
    {
      "id": "b80",
      "title": "Sbi-rag: Enhancing math word problem solving for students through schema-based instruction and retrieval-augmented generation",
      "authors": [
        "P Dixit",
        "T Oates"
      ],
      "year": "2024",
      "venue": "Sbi-rag: Enhancing math word problem solving for students through schema-based instruction and retrieval-augmented generation",
      "doi": ""
    },
    {
      "id": "b81",
      "title": "Kragen: a knowledge graph-enhanced rag framework for biomedical problem solving using large language models",
      "authors": [
        "N Matsumoto",
        "J Moran",
        "H Choi",
        "M E Hernandez",
        "M Venkatesan",
        "P Wang",
        "J H Moore"
      ],
      "year": "2024",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b82",
      "title": "Gram: Generative retrieval augmented matching of data schemas in the context of data security",
      "authors": [
        "X Liu",
        "R Wang",
        "Y Song",
        "L Kong"
      ],
      "year": "2024",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
      "doi": ""
    },
    {
      "id": "b83",
      "title": "Tablerag: Million-token table understanding with language models",
      "authors": [
        "S.-A Chen",
        "L Miculicich",
        "J M Eisenschlos",
        "Z Wang",
        "Z Wang",
        "Y Chen",
        "Y Fujii",
        "H.-T Lin",
        "C.-Y Lee",
        "T Pfister"
      ],
      "year": "2024",
      "venue": "Tablerag: Million-token table understanding with language models",
      "doi": ""
    },
    {
      "id": "b84",
      "title": "Seakr: Self-aware knowledge retrieval for adaptive retrieval augmented generation",
      "authors": [
        "Z Yao",
        "W Qi",
        "L Pan",
        "S Cao",
        "L Hu",
        "W Liu",
        "L Hou",
        "J Li"
      ],
      "year": "2024",
      "venue": "Seakr: Self-aware knowledge retrieval for adaptive retrieval augmented generation",
      "doi": ""
    },
    {
      "id": "b85",
      "title": "Self-rag: Selfreflective retrieval augmented generation",
      "authors": [
        "A Asai",
        "Z Wu",
        "Y Wang",
        "A Sil",
        "H Hajishirzi"
      ],
      "year": "2023",
      "venue": "NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following",
      "doi": ""
    },
    {
      "id": "b86",
      "title": "Can we further elicit reasoning in llms? critic-guided planning with retrieval-augmentation for solving challenging tasks",
      "authors": [
        "X Li",
        "W Xu",
        "R Zhao",
        "F Jiao",
        "S Joty",
        "L Bing"
      ],
      "year": "2024",
      "venue": "Can we further elicit reasoning in llms? critic-guided planning with retrieval-augmentation for solving challenging tasks",
      "doi": ""
    },
    {
      "id": "b87",
      "title": "Simrag: Self-improving retrieval-augmented generation for adapting large language models to specialized domains",
      "authors": [
        "R Xu",
        "H Liu",
        "S Nag",
        "Z Dai",
        "Y Xie",
        "X Tang",
        "C Luo",
        "Y Li",
        "J C Ho",
        "C Yang"
      ],
      "year": "2024",
      "venue": "Simrag: Self-improving retrieval-augmented generation for adapting large language models to specialized domains",
      "doi": ""
    },
    {
      "id": "b88",
      "title": "SeRTS: Self-rewarding tree search for biomedical retrieval-augmented generation",
      "authors": [
        "M Hu",
        "L Zong",
        "H Wang",
        "J Zhou",
        "J Li",
        "Y Gao",
        "K.-F Wong",
        "Y Li",
        "I King"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024",
      "doi": ""
    },
    {
      "id": "b89",
      "title": "Speculative rag: Enhancing retrieval augmented generation through drafting",
      "authors": [
        "Z Wang",
        "Z Wang",
        "L Le",
        "H S Zheng",
        "S Mishra",
        "V Perot",
        "Y Zhang",
        "A Mattapalli",
        "A Taly",
        "J Shang"
      ],
      "year": "2024",
      "venue": "Speculative rag: Enhancing retrieval augmented generation through drafting",
      "doi": ""
    },
    {
      "id": "b90",
      "title": "Hipporag: Neurobiologically inspired long-term memory for large language models",
      "authors": [
        "B J Gutiérrez",
        "Y Shu",
        "Y Gu",
        "M Yasunaga",
        "Y Su"
      ],
      "year": "2024",
      "venue": "Hipporag: Neurobiologically inspired long-term memory for large language models",
      "doi": ""
    },
    {
      "id": "b91",
      "title": "Augmenting language models with long-term memory",
      "authors": [
        "W Wang",
        "L Dong",
        "H Cheng",
        "X Liu",
        "X Yan",
        "J Gao",
        "F Wei"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b92",
      "title": "Enhancing long-term memory using hierarchical aggregate tree for retrieval augmented generation",
      "authors": [
        "A Aadhithya"
      ],
      "year": "2024",
      "venue": "Enhancing long-term memory using hierarchical aggregate tree for retrieval augmented generation",
      "doi": ""
    },
    {
      "id": "b93",
      "title": "Memorag: Moving towards next-gen rag via memory-inspired knowledge discovery",
      "authors": [
        "H Qian",
        "P Zhang",
        "Z Liu",
        "K Mao",
        "Z Dou"
      ],
      "year": "2024",
      "venue": "Memorag: Moving towards next-gen rag via memory-inspired knowledge discovery",
      "doi": ""
    },
    {
      "id": "b94",
      "title": "Pistis-rag: Enhancing retrieval-augmented generation with human feedback",
      "authors": [
        "Y Bai",
        "Y Miao",
        "L Chen",
        "D Wang",
        "D Li",
        "Y Ren",
        "H Xie",
        "C Yang",
        "X Cai"
      ],
      "year": "2024",
      "venue": "Pistis-rag: Enhancing retrieval-augmented generation with human feedback",
      "doi": ""
    },
    {
      "id": "b95",
      "title": "Similarity is not all you need: Endowing retrieval augmented generation with multi layered thoughts",
      "authors": [
        "C Gan",
        "D Yang",
        "B Hu",
        "H Zhang",
        "S Li",
        "Z Liu",
        "Y Shen",
        "L Ju",
        "Z Zhang",
        "J Gu"
      ],
      "year": "2024",
      "venue": "Similarity is not all you need: Endowing retrieval augmented generation with multi layered thoughts",
      "doi": ""
    },
    {
      "id": "b96",
      "title": "Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement",
      "authors": [
        "J Jiang",
        "J Chen",
        "J Li",
        "R Ren",
        "S Wang",
        "W X Zhao",
        "Y Song",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement",
      "doi": ""
    },
    {
      "id": "b97",
      "title": "Multihop-rag: Benchmarking retrievalaugmented generation for multi-hop queries",
      "authors": [
        "Y Tang",
        "Y Yang"
      ],
      "year": "2024",
      "venue": "Multihop-rag: Benchmarking retrievalaugmented generation for multi-hop queries",
      "doi": ""
    },
    {
      "id": "b98",
      "title": "Retrievalaugmented multi-modal chain-of-thoughts reasoning for large language models",
      "authors": [
        "B Liu",
        "C Lyu",
        "Z Min",
        "Z Wang",
        "J Su",
        "L Wang"
      ],
      "year": "2023",
      "venue": "Retrievalaugmented multi-modal chain-of-thoughts reasoning for large language models",
      "doi": ""
    },
    {
      "id": "b99",
      "title": "Hop, union, generate: Explainable multi-hop reasoning without rationale supervision",
      "authors": [
        "W Zhao",
        "J T Chiu",
        "C Cardie",
        "A M Rush"
      ],
      "year": "2023",
      "venue": "Hop, union, generate: Explainable multi-hop reasoning without rationale supervision",
      "doi": ""
    },
    {
      "id": "b100",
      "title": "Multimodal chain-of-thought reasoning in language models",
      "authors": [
        "Z Zhang",
        "A Zhang",
        "M Li",
        "H Zhao",
        "G Karypis",
        "A Smola"
      ],
      "year": "2023",
      "venue": "Multimodal chain-of-thought reasoning in language models",
      "doi": ""
    },
    {
      "id": "b101",
      "title": "Can gpt improve the state of prior authorization via guideline based automated question answering?\" in AI for Health Equity and Fairness: Leveraging AI to Address Social Determinants of Health",
      "authors": [
        "S Vatsal",
        "A Singh",
        "S Tafreshi"
      ],
      "year": "2024",
      "venue": "Can gpt improve the state of prior authorization via guideline based automated question answering?\" in AI for Health Equity and Fairness: Leveraging AI to Address Social Determinants of Health",
      "doi": ""
    },
    {
      "id": "b102",
      "title": "Can gpt redefine medical understanding? evaluating gpt on biomedical machine reading comprehension",
      "authors": [
        "S Vatsal",
        "A Singh"
      ],
      "year": "2024",
      "venue": "Can gpt redefine medical understanding? evaluating gpt on biomedical machine reading comprehension",
      "doi": ""
    },
    {
      "id": "b103",
      "title": "Chain-of-verification reduces hallucination in large language models",
      "authors": [
        "S Dhuliawala",
        "M Komeili",
        "J Xu",
        "R Raileanu",
        "X Li",
        "A Celikyilmaz",
        "J Weston"
      ],
      "year": "2023",
      "venue": "Chain-of-verification reduces hallucination in large language models",
      "doi": ""
    },
    {
      "id": "b104",
      "title": "Verify-and-edit: A knowledge-enhanced chain-of-thought framework",
      "authors": [
        "R Zhao",
        "X Li",
        "S Joty",
        "C Qin",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Verify-and-edit: A knowledge-enhanced chain-of-thought framework",
      "doi": ""
    },
    {
      "id": "b105",
      "title": "Fine-tuning large vision-language models as decision-making agents via reinforcement learning",
      "authors": [
        "Y Zhai",
        "H Bai",
        "Z Lin",
        "J Pan",
        "S Tong",
        "Y Zhou",
        "A Suhr",
        "S Xie",
        "Y Lecun",
        "Y Ma"
      ],
      "year": "2024",
      "venue": "Fine-tuning large vision-language models as decision-making agents via reinforcement learning",
      "doi": ""
    },
    {
      "id": "b106",
      "title": "Reinforcement learning: An introduction",
      "authors": [
        "R S Sutton",
        "A G Barto"
      ],
      "year": "2018",
      "venue": "Reinforcement learning: An introduction",
      "doi": ""
    },
    {
      "id": "b107",
      "title": "Proximal policy optimization algorithms",
      "authors": [
        "J Schulman",
        "F Wolski",
        "P Dhariwal",
        "A Radford",
        "O Klimov"
      ],
      "year": "2017",
      "venue": "Proximal policy optimization algorithms",
      "doi": ""
    },
    {
      "id": "b108",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b109",
      "title": "Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback",
      "authors": [
        "W Shen",
        "R Zheng",
        "W Zhan",
        "J Zhao",
        "S Dou",
        "T Gui",
        "Q Zhang",
        "X Huang"
      ],
      "year": "2023",
      "venue": "Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback",
      "doi": ""
    },
    {
      "id": "b110",
      "title": "Secrets of rlhf in large language models part ii: Reward modeling",
      "authors": [
        "B Wang",
        "R Zheng",
        "L Chen",
        "Y Liu",
        "S Dou",
        "C Huang",
        "W Shen",
        "S Jin",
        "E Zhou",
        "C Shi"
      ],
      "year": "2024",
      "venue": "Secrets of rlhf in large language models part ii: Reward modeling",
      "doi": ""
    },
    {
      "id": "b111",
      "title": "trlx: A framework for large scale reinforcement learning from human feedback",
      "authors": [
        "A Havrilla",
        "M Zhuravinskyi",
        "D Phung",
        "A Tiwari",
        "J Tow",
        "S Biderman",
        "Q Anthony",
        "L Castricato"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b112",
      "title": "Ultrafeedback: Boosting language models with high-quality feedback",
      "authors": [
        "G Cui",
        "L Yuan",
        "N Ding",
        "G Yao",
        "W Zhu",
        "Y Ni",
        "G Xie",
        "Z Liu",
        "M Sun"
      ],
      "year": "2023",
      "venue": "Ultrafeedback: Boosting language models with high-quality feedback",
      "doi": ""
    },
    {
      "id": "b113",
      "title": "Learning to summarize with human feedback",
      "authors": [
        "N Stiennon",
        "L Ouyang",
        "J Wu",
        "D Ziegler",
        "R Lowe",
        "C Voss",
        "A Radford",
        "D Amodei",
        "P F Christiano"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b114",
      "title": "Nash learning from human feedback",
      "authors": [
        "R Munos",
        "M Valko",
        "D Calandriello",
        "M G Azar",
        "M Rowland",
        "Z D Guo",
        "Y Tang",
        "M Geist",
        "T Mesnard",
        "A Michi"
      ],
      "year": "2023",
      "venue": "Nash learning from human feedback",
      "doi": ""
    },
    {
      "id": "b115",
      "title": "Skywork-reward: Bag of tricks for reward modeling in llms",
      "authors": [
        "C Y Liu",
        "L Zeng",
        "J Liu",
        "R Yan",
        "J He",
        "C Wang",
        "S Yan",
        "Y Liu",
        "Y Zhou"
      ],
      "year": "2024",
      "venue": "Skywork-reward: Bag of tricks for reward modeling in llms",
      "doi": ""
    },
    {
      "id": "b116",
      "title": "Camels in a changing climate: Enhancing lm adaptation with tulu 2",
      "authors": [
        "H Ivison",
        "Y Wang",
        "V Pyatkin",
        "N Lambert",
        "M Peters",
        "P Dasigi",
        "J Jang",
        "D Wadden",
        "N A Smith",
        "I Beltagy"
      ],
      "year": "2023",
      "venue": "Camels in a changing climate: Enhancing lm adaptation with tulu 2",
      "doi": ""
    },
    {
      "id": "b117",
      "title": "Toolaugmented reward modeling",
      "authors": [
        "L Li",
        "Y Chai",
        "S Wang",
        "Y Sun",
        "H Tian",
        "N Zhang",
        "H Wu"
      ],
      "year": "2023",
      "venue": "Toolaugmented reward modeling",
      "doi": ""
    },
    {
      "id": "b118",
      "title": "Optimal design for reward modeling in rlhf",
      "authors": [
        "A Scheid",
        "E Boursier",
        "A Durmus",
        "M I Jordan",
        "P Ménard",
        "E Moulines",
        "M Valko"
      ],
      "year": "2024",
      "venue": "Optimal design for reward modeling in rlhf",
      "doi": ""
    },
    {
      "id": "b119",
      "title": "Scaling laws for reward model overoptimization",
      "authors": [
        "L Gao",
        "J Schulman",
        "J Hilton"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b120",
      "title": "A survey of reinforcement learning from human feedback",
      "authors": [
        "T Kaufmann",
        "P Weng",
        "V Bengs",
        "E Hüllermeier"
      ],
      "year": "2023",
      "venue": "A survey of reinforcement learning from human feedback",
      "doi": ""
    },
    {
      "id": "b121",
      "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
      "authors": [
        "H Lee",
        "S Phatale",
        "H Mansoor",
        "K R Lu",
        "T Mesnard",
        "J Ferret",
        "C Bishop",
        "E Hall",
        "V Carbune",
        "A Rastogi"
      ],
      "year": "2023",
      "venue": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
      "doi": ""
    },
    {
      "id": "b122",
      "title": "Hrlaif: Improvements in helpfulness and harmlessness in open-domain reinforcement learning from ai feedback",
      "authors": [
        "A Li",
        "Q Xiao",
        "P Cao",
        "J Tang",
        "Y Yuan",
        "Z Zhao",
        "X Chen",
        "L Zhang",
        "X Li",
        "K Yang"
      ],
      "year": "2024",
      "venue": "Hrlaif: Improvements in helpfulness and harmlessness in open-domain reinforcement learning from ai feedback",
      "doi": ""
    },
    {
      "id": "b123",
      "title": "Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing",
      "authors": [
        "Z Xu",
        "F Jiang",
        "L Niu",
        "Y Deng",
        "R Poovendran",
        "Y Choi",
        "B Y Lin"
      ],
      "year": "2024",
      "venue": "Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing",
      "doi": ""
    },
    {
      "id": "b124",
      "title": "Helpsteer2-preference: Complementing ratings with preferences",
      "authors": [
        "Z Wang",
        "A Bukharin",
        "O Delalleau",
        "D Egert",
        "G Shen",
        "J Zeng",
        "O Kuchaiev",
        "Y Dong"
      ],
      "year": "2024",
      "venue": "Helpsteer2-preference: Complementing ratings with preferences",
      "doi": ""
    },
    {
      "id": "b125",
      "title": "Guiding pretraining in reinforcement learning with large language models",
      "authors": [
        "Y Du",
        "O Watkins",
        "Z Wang",
        "C Colas",
        "T Darrell",
        "P Abbeel",
        "A Gupta",
        "J Andreas"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b126",
      "title": "Reward design with language models",
      "authors": [
        "M Kwon",
        "S M Xie",
        "K Bullard",
        "D Sadigh"
      ],
      "year": "2023",
      "venue": "Reward design with language models",
      "doi": ""
    },
    {
      "id": "b127",
      "title": "Eureka: Humanlevel reward design via coding large language models",
      "authors": [
        "Y J Ma",
        "W Liang",
        "G Wang",
        "D.-A Huang",
        "O Bastani",
        "D Jayaraman",
        "Y Zhu",
        "L Fan",
        "A Anandkumar"
      ],
      "year": "2023",
      "venue": "Eureka: Humanlevel reward design via coding large language models",
      "doi": ""
    },
    {
      "id": "b128",
      "title": "Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics",
      "authors": [
        "J Song",
        "Z Zhou",
        "J Liu",
        "C Fang",
        "Z Shu",
        "L Ma"
      ],
      "year": "2023",
      "venue": "Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics",
      "doi": ""
    },
    {
      "id": "b129",
      "title": "Self-rewarding language models",
      "authors": [
        "W Yuan",
        "R Y Pang",
        "K Cho",
        "S Sukhbaatar",
        "J Xu",
        "J Weston"
      ],
      "year": "2024",
      "venue": "Self-rewarding language models",
      "doi": ""
    },
    {
      "id": "b130",
      "title": "Exploration versus exploitation in reinforcement learning: A stochastic control approach",
      "authors": [
        "H Wang",
        "T Zariphopoulou",
        "X Zhou"
      ],
      "year": "2018",
      "venue": "Exploration versus exploitation in reinforcement learning: A stochastic control approach",
      "doi": ""
    },
    {
      "id": "b131",
      "title": "Guarantees for epsilon-greedy reinforcement learning with function approximation",
      "authors": [
        "C Dann",
        "Y Mansour",
        "M Mohri",
        "A Sekhari",
        "K Sridharan"
      ],
      "year": "2022",
      "venue": "International conference on machine learning",
      "doi": ""
    },
    {
      "id": "b132",
      "title": "Asynchronous methods for deep reinforcement learning",
      "authors": [
        "V Mnih"
      ],
      "year": "2016",
      "venue": "Asynchronous methods for deep reinforcement learning",
      "doi": ""
    },
    {
      "id": "b133",
      "title": "Adaptive ε-greedy exploration in reinforcement learning based on value differences",
      "authors": [
        "M Tokic"
      ],
      "year": "2010",
      "venue": "Annual conference on artificial intelligence",
      "doi": ""
    },
    {
      "id": "b134",
      "title": "Boltzmann exploration done right",
      "authors": [
        "N Cesa-Bianchi",
        "C Gentile",
        "G Lugosi",
        "G Neu"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b135",
      "title": "Explorllm: Guiding exploration in reinforcement learning with large language models",
      "authors": [
        "R Ma",
        "J Luijkx",
        "Z Ajanovic",
        "J Kober"
      ],
      "year": "2024",
      "venue": "Explorllm: Guiding exploration in reinforcement learning with large language models",
      "doi": ""
    },
    {
      "id": "b136",
      "title": "Epo: Hierarchical llm agents with environment preference optimization",
      "authors": [
        "Q Zhao",
        "H Fu",
        "C Sun",
        "G Konidaris"
      ],
      "year": "2024",
      "venue": "Epo: Hierarchical llm agents with environment preference optimization",
      "doi": ""
    },
    {
      "id": "b137",
      "title": "Balancing exploration and exploitation in llm using soft rllf for enhanced negation understanding",
      "authors": [
        "H.-T Nguyen",
        "K Satoh"
      ],
      "year": "2024",
      "venue": "Balancing exploration and exploitation in llm using soft rllf for enhanced negation understanding",
      "doi": ""
    },
    {
      "id": "b138",
      "title": "Empower large language model to perform better on industrial domain-specific question answering",
      "authors": [
        "F Yang",
        "P Zhao",
        "Z Wang",
        "L Wang",
        "J Zhang",
        "M Garg",
        "Q Lin",
        "S Rajmohan",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "Empower large language model to perform better on industrial domain-specific question answering",
      "doi": ""
    },
    {
      "id": "b139",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "authors": [
        "R Rafailov",
        "A Sharma",
        "E Mitchell",
        "C D Manning",
        "S Ermon",
        "C Finn"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b140",
      "title": "Smaug: Fixing failure modes of preference optimisation with dpopositive",
      "authors": [
        "A Pal",
        "D Karkhanis",
        "S Dooley",
        "M Roberts",
        "S Naidu",
        "C White"
      ],
      "year": "2024",
      "venue": "Smaug: Fixing failure modes of preference optimisation with dpopositive",
      "doi": ""
    },
    {
      "id": "b141",
      "title": "",
      "authors": [
        "J Wu",
        "Y Xie",
        "Z Yang",
        "J Wu",
        "J Gao",
        "B Ding",
        "X Wang",
        "X He"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b142",
      "title": "sdpo: Don't use your data all at once",
      "authors": [
        "D Kim",
        "Y Kim",
        "W Song",
        "H Kim",
        "Y Kim",
        "S Kim",
        "C Park"
      ],
      "year": "2024",
      "venue": "sdpo: Don't use your data all at once",
      "doi": ""
    },
    {
      "id": "b143",
      "title": "Weighted-reward preference optimization for implicit model fusion",
      "authors": [
        "Z Yang",
        "F Wan",
        "L Zhong",
        "T Shi",
        "X Quan"
      ],
      "year": "2024",
      "venue": "Weighted-reward preference optimization for implicit model fusion",
      "doi": ""
    },
    {
      "id": "b144",
      "title": "Provably robust dpo: Aligning language models with noisy feedback",
      "authors": [
        "S R Chowdhury",
        "A Kini",
        "N Natarajan"
      ],
      "year": "2024",
      "venue": "Provably robust dpo: Aligning language models with noisy feedback",
      "doi": ""
    },
    {
      "id": "b145",
      "title": "A comprehensive survey of datasets, theories, variants, and applications in direct preference optimization",
      "authors": [
        "W Xiao",
        "Z Wang",
        "L Gan",
        "S Zhao",
        "W He",
        "L A Tuan",
        "L Chen",
        "H Jiang",
        "Z Zhao",
        "F Wu"
      ],
      "year": "2024",
      "venue": "A comprehensive survey of datasets, theories, variants, and applications in direct preference optimization",
      "doi": ""
    },
    {
      "id": "b146",
      "title": "A general theoretical paradigm to understand learning from human preferences",
      "authors": [
        "M G Azar",
        "Z D Guo",
        "B Piot",
        "R Munos",
        "M Rowland",
        "M Valko",
        "D Calandriello"
      ],
      "year": "2024",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "doi": ""
    },
    {
      "id": "b147",
      "title": "Rethinking bradley-terry models in preference-based reward modeling: Foundations, theory, and alternatives",
      "authors": [
        "H Sun",
        "Y Shen",
        "J.-F Ton"
      ],
      "year": "2024",
      "venue": "Rethinking bradley-terry models in preference-based reward modeling: Foundations, theory, and alternatives",
      "doi": ""
    },
    {
      "id": "b148",
      "title": "Self-play preference optimization for language model alignment",
      "authors": [
        "Y Wu",
        "Z Sun",
        "H Yuan",
        "K Ji",
        "Y Yang",
        "Q Gu"
      ],
      "year": "2024",
      "venue": "Self-play preference optimization for language model alignment",
      "doi": ""
    },
    {
      "id": "b149",
      "title": "Preference optimization with multi-sample comparisons",
      "authors": [
        "C Wang",
        "Z Zhao",
        "C Zhu",
        "K A Sankararaman",
        "M Valko",
        "X Cao",
        "Z Chen",
        "M Khabsa",
        "Y Chen",
        "H Ma"
      ],
      "year": "2024",
      "venue": "Preference optimization with multi-sample comparisons",
      "doi": ""
    },
    {
      "id": "b150",
      "title": "Robust preference optimization through reward model distillation",
      "authors": [
        "A Fisch",
        "J Eisenstein",
        "V Zayats",
        "A Agarwal",
        "A Beirami",
        "C Nagpal",
        "P Shaw",
        "J Berant"
      ],
      "year": "2024",
      "venue": "Robust preference optimization through reward model distillation",
      "doi": ""
    },
    {
      "id": "b151",
      "title": "Pace: Improving prompt with actor-critic editing for large language model",
      "authors": [
        "Y Dong",
        "K Luo",
        "X Jiang",
        "Z Jin",
        "G Li"
      ],
      "year": "2023",
      "venue": "Pace: Improving prompt with actor-critic editing for large language model",
      "doi": ""
    },
    {
      "id": "b152",
      "title": "Fine-tuning language models from human preferences",
      "authors": [
        "D M Ziegler",
        "N Stiennon",
        "J Wu",
        "T B Brown",
        "A Radford",
        "D Amodei",
        "P Christiano",
        "G Irving"
      ],
      "year": "2019",
      "venue": "Fine-tuning language models from human preferences",
      "doi": ""
    },
    {
      "id": "b153",
      "title": "Open-ended reinforcement learning with neural reward functions",
      "authors": [
        "R Meier",
        "A Mujika"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b154",
      "title": "Hdflow: Enhancing llm complex problemsolving with hybrid thinking and dynamic workflows",
      "authors": [
        "W Yao",
        "H Mi",
        "D Yu"
      ],
      "year": "2024",
      "venue": "Hdflow: Enhancing llm complex problemsolving with hybrid thinking and dynamic workflows",
      "doi": ""
    },
    {
      "id": "b155",
      "title": "Enhancing multi-step reasoning abilities of language models through direct qfunction optimization",
      "authors": [
        "G Liu",
        "K Ji",
        "R Zheng",
        "Z Wu",
        "C Dun",
        "Q Gu",
        "L Yan"
      ],
      "year": "2024",
      "venue": "Enhancing multi-step reasoning abilities of language models through direct qfunction optimization",
      "doi": ""
    },
    {
      "id": "b156",
      "title": "Llms as debate partners: Utilizing genetic algorithms and adversarial search for adaptive arguments",
      "authors": [
        "P Aryan"
      ],
      "year": "2024",
      "venue": "Llms as debate partners: Utilizing genetic algorithms and adversarial search for adaptive arguments",
      "doi": ""
    },
    {
      "id": "b157",
      "title": "Evolutionary computation in the era of large language model: Survey and roadmap",
      "authors": [
        "X Wu",
        "S -H. Wu",
        "J Wu",
        "L Feng",
        "K C Tan"
      ],
      "year": "2024",
      "venue": "Evolutionary computation in the era of large language model: Survey and roadmap",
      "doi": ""
    },
    {
      "id": "b158",
      "title": "Controlling the mutation in large language models for the efficient evolution of algorithms",
      "authors": [
        "H Yin",
        "A V Kononova",
        "T Bäck",
        "N Van Stein"
      ],
      "year": "2024",
      "venue": "Controlling the mutation in large language models for the efficient evolution of algorithms",
      "doi": ""
    },
    {
      "id": "b159",
      "title": "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers",
      "authors": [
        "Q Guo",
        "R Wang",
        "J Guo",
        "B Li",
        "K Song",
        "X Tan",
        "G Liu",
        "J Bian",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers",
      "doi": ""
    },
    {
      "id": "b160",
      "title": "Large language model-based evolutionary optimizer: Reasoning with elitism",
      "authors": [
        "S Brahmachary",
        "S M Joshi",
        "A Panda",
        "K Koneripalli",
        "A K Sagotra",
        "H Patel",
        "A Sharma",
        "A D Jagtap",
        "K Kalyanaraman"
      ],
      "year": "2024",
      "venue": "Large language model-based evolutionary optimizer: Reasoning with elitism",
      "doi": ""
    },
    {
      "id": "b161",
      "title": "A match made in consistency heaven: when large language models meet evolutionary algorithms",
      "authors": [
        "W Chao",
        "J Zhao",
        "L Jiao",
        "L Li",
        "F Liu",
        "S Yang"
      ],
      "year": "2024",
      "venue": "A match made in consistency heaven: when large language models meet evolutionary algorithms",
      "doi": ""
    },
    {
      "id": "b162",
      "title": "Large language models as surrogate models in evolutionary algorithms: A preliminary study",
      "authors": [
        "H Hao",
        "X Zhang",
        "A Zhou"
      ],
      "year": "2024",
      "venue": "Large language models as surrogate models in evolutionary algorithms: A preliminary study",
      "doi": ""
    },
    {
      "id": "b163",
      "title": "Wese: Weak exploration to strong exploitation for llm agents",
      "authors": [
        "X Huang",
        "W Liu",
        "X Chen",
        "X Wang",
        "D Lian",
        "Y Wang",
        "R Tang",
        "E Chen"
      ],
      "year": "2024",
      "venue": "Wese: Weak exploration to strong exploitation for llm agents",
      "doi": ""
    },
    {
      "id": "b164",
      "title": "Towards understanding grokking: An effective theory of representation learning",
      "authors": [
        "Z Liu",
        "O Kitouni",
        "N S Nolte",
        "E Michaud",
        "M Tegmark",
        "M Williams"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Abstract",
      "text": "Large Language Models offer new opportunities to devise automated implementation generation methods that can tackle problem solving activities beyond traditional methods, which require algorithmic specifications and can use only static domain knowledge, like performance metrics and libraries of basic building blocks. Large Language Models could support creating new methods to support problem solving activities for open-ended problems, like problem framing, exploring possible solving approaches, feature elaboration and combination, more advanced implementation assessment, and handling unexpected situations. This report summarized the current work on Large Language Models, including model prompting, Reinforcement Learning, and Retrieval-Augmented Generation. Future research requirements were also discussed. implementation generation, Large Language Models, open-ended problem solving, prompting, Reinforcement Learning, Retrieval-augmented Generation"
    },
    {
      "title": "I Introduction",
      "text": "Problem solving is the process of creating a solution for a problem description [1, 2, 3, 4]. The solution can be an _explanation_ for a set of properties exhibited by a static or dynamic situation, e.g., a mathematical proof, or an _implementation_ (realization), which is the construction of a new materialization (e.g., design) that exhibits the required properties as a result of their operation (functioning, execution). This report focuses on the implementation (realization) of problem solving. Creating an implementation can pertain to the three general-purpose problem-solving situations: well-defined problems, ill-defined problems, and open-ended problems [5, 6, 7]: 1. Well-defined problem solving for implementation construction describes situations in which an existing solution can be reused with some incremental changes to solve a new problem. For example, textbook algorithms are utilized to solve a new problem by selecting proper data structures and customizing the algorithm parameters, like the conditions of conditional statements and the iterations of loops. Using parameterized templates for circuit design [8, 9, 10, 11] belongs to this category too. 2. Ill-defined problem solving for implementation construction represents cases in which the existing implementations cannot solve all requirements, i.e. they satisfy some but not others [12, 13]. Changing the parameters of the implementation does not address the issue. Problem solving includes options, like producing a description of the implementation trade-offs by parameter sampling and selecting the best compromise, exploring implementation alternatives for specific fragments of the implementation, so that better trade-offs result for the overall solution, and selecting a different approach (principle) for an implementation, including situations when a new implementation must be built, similar to open-ended solving for building a new implementation. 3. Open-ended problem solving for implementation generation requires devising new solutions with a significant departure and characteristics from previous implementations. The understanding of this process is still limited [14, 15]. Also, there are insufficient metrics to describe the degree to which the process is systematically progressing towards success, e.g., building a new implementation. Typical activities include problem framing and problem understanding, identifying and selecting the solving approach, divide and conquer (e.g., problem partitioning into sub-problems), implementation elaboration through trial-end-error, feature combination, adjustment, abstraction and insight gaining, implementation analysis to find pros and cons and the impact of features on the implementation operation, implementation modification, error correction, and handling unexpected situations. As summarized in the next section, traditional automated implementation generation focuses mainly on elaboration and parameter trade-off exploration, for which the domain knowledge of the implementation is captured by customized metrics [16] or in a library of basic building blocks [16, 17]. The library is static and does not evolve to incorporate new knowledge either from external sources or as a byproduct of implementation generation. Moreover, traditional methods assume the existence of a problem specification expressing at least functional and performance requirements, but more often the algorithm or architecture (structure) of the implementation [17, 18]. Hence, it can be argued that existing methods focus mainly on well-defined and ill-defined problems but less on implementation generation for open-ended problem solving. Existing approaches cannot tackle problem framing and exploring solution approaches, even though trial-and-error and rapid prototyping are essential in understanding new opportunities and limitations. Moreover, there is little automated support for divide and conquer and architecture creation, combination of features from different solutions, and handling unexpected situations. In general, traditional methods struggle with any activity conducted at a level above an algorithmic description of an implementation. However, recent advances in Large Language Models (LLMs) created opportunities to devise novel automated implementation generation methods that can tackle problems beyond algorithmic specifications and may use domain knowledge that is dynamically learned over time. Arguably, LLMs could contain knowledge that is continuously updated by learning new features either from external documents or based on their own previously generated implementations. Implementation assessment could be improved by comparing it to similar, externally available implementations and considering collective feedback and preferences expressed for other solutions. The opportunities and limitations of an implementation can be better understood by embedding it into the trend of related designs. Moreover, support can be offered for problem framing and exploring possible solution approaches, activities that are often collective, in a team. LLMs can process multi-modal descriptions, including natural language and images with certain degrees of specification completeness, unknowns, and ambiguity. Hence, understanding the capabilities of LLMs for implementation generation, possibly in conjunction with traditional methods, is required. These capabilities mostly emerge from LLMs being able to learn a broad range of associations in multi-modal data and diverse contexts. This report studied the degree to which LLMs, possibly using prompting, Reinforcement Learning (RL) and Retrieval-Augmented Generation (RAG), can model the activities of implementation generation for an open-ended problem solving. The goal was to identify how LLMs and their extensions can contribute to implementing problem-solving activities that are not addressed in traditional methods. The report offers an extensive presentation of prompting methods, RAG techniques, and RL approaches. Then, the using of LLMs to implement problem-solving activities not available in traditional automated implementation generation was discussed. New research requirements were also offered. The report argued that these requirements refer to topics, like constructing the implementation approach, effectively controlling elaboration, robust qualitative and quantitative assessment across abstraction levels, knowledge memorizing during learning, and managing the problem solving process. The report has the following structure. Section II offers an overview of the work on traditional, automated implementation generation. Section III presents an overview of LLMs. Section IV discusses the similarities of LLMs and traditional automated implementation generation methods and summarizes the related research needs. Conclusions end the report."
    },
    {
      "title": "Ii Overview Of Traditional Automated Implementation Generation",
      "text": "Traditional approaches to automatically generate implementations can be grouped into four broad categories: (i) approaches based on high-level specifications, (ii) methods using evolutionary algorithms, (iii) agent-based methods, and (iv) cognitive architectures. The four categories are summarized next. **(i) Approaches based on high-level specifications:** These approaches include traditional compiling methods to generate executable code [16] and high-level synthesis methods [18, 19, 20, 21] and template-based synthesis [10, 11] to create electronic circuits and systems. They use high-level specifications described using a programming language. Conceptually, specifications serve as parameterized descriptions of the target implementation architecture. Specifically, internal representations are built using a set of predefined rules (e.g., language grammar) applied to the specifications and then used to create an optimized hardware design by exploring different optimization possibilities. Prediction models or simulation tools are integrated to evaluate the performance of possible implementation alternatives. These methods address the problem-solving activities in the following ways: The specification gives an unambiguous, complete description of the parameterized architecture. Thus, there is no problem framing step and problem understanding is fully addressed during specification creation. Divide and conquer is defined by the structuring of the specification. Also, there is no step of exploring possible implementation alternatives, as the specification explicitly describes the data processing steps, including the connections between the sequences of processing steps, i.e. using the processing outputs as inputs for the next processing steps. Hence, feature combination during elaboration only connects predefined operators which do not change their function based on the connections. From the point of view of cognitive psychology, these combinations are relation-based combinations but do not reflect feature-based combinations, in which features of a concept are transferred to another concept [22]. Hence, there are no unexpected situations, including emerging features. Implementation analysis uses performance models and simulation, even though the pros and cons of an implementation are rarely causally linked to the implementation fragments responsible for them. Hence, the insight gain is limited. Trial-and-error (possibly guided by priority functions), implementation modification, and adjustment are only at the level of optimizing the architecture parameters. There is no abstraction or summarization during the process. Error correction requires to modify the specification and then repeat the problem-solving process. **(ii) Methods using evolutionary algorithms:** These methods create a dynamic process, in which large populations of solutions originate new populations through traditional operators, i.e. selection, crossover, and mutation [23]. Selection means propagating high-fitness individuals from the current to the next population, crossover combines features of a set of solutions to produce new solutions, and mutation randomly changes solution features. These methods do not include problem framing and understanding. Identifying and selecting the implementation approach has been studied less, even though it is possible to maintain separate sub-populations, each for a different approach, and then giving higher priority to the sub-populations that include more high-quality implementations. There is no divide-and-conquer to separate a problem into sub-problems and no explicit error correction. Trial-and-error is mimicked through the mutation operator, even though mutation does not implement a systematic exploration process guided by the learned knowledge. There is no insight gaining during the process, abstraction or summarization of the learned knowledge, and no explicit identification of unexpected situations. Crossover implements combination, including feature and relation combination. Similar to the previous category, implementation analysis uses performance models and simulation to produce a fitness value that controls the selection of the better implementations. However, there is no explicit identification of the causal features that produce the pros and cons of an implementation, thus there is no implementation adjustment, modification, or correction guided by causal information. There is no explicit memory mechanism, features being implicitly memorized through a population, and there is no possibility to backtrack to previous states to attempt exploring a different path. **(iii) Agent-based methods**: These methods utilize multiple interacting agents, each agent having its own memory and running its own decision-making algorithm [24, 25]. Even though traditional agents realize simple decision-making algorithms, e.g., through a set of simple rules in response to specific inputs, it is possible to consider more complex methods, such as each agent running its own synthesis algorithm or population-based evolution. Agents interact with each other by communicating high-quality implementations and features, or implementation steps, which then can be utilized by the other agents, too. Depending on their decision-making procedure, agent-based methods have similar characteristics, like the methods of the previous two categories. Their main advantage is their capacity to simultaneously maintain multiple perspectives about the implementation creation process, e.g., through their local memory, preferences, priorities, etc., and then aggregate these perspectives to improve problem solving. It can be argued that they mimic the implementation creation process by a team (team problem solving) [14, 26]. **(iv) Cognitive architectures**: Cognitive architectures (CAs) mimic the brain activities during problem solving [27, 28, 29, 30, 31]. Architectures include modules for knowledge representation, knowledge memory, knowledge classification, summarization, comparison, decision-making, prediction, learning, and goal setting. For example, SOAR CA models cognition-based problem solving [28], using operation selection and application (e.g., state elaboration, operator proposal and evaluation, and decision). Knowledge is procedural if-then rules selected through matching. Learning stores short-cuts to solutions, conditions for applying the rules, and utility updates. ACT-R CA uses multiple symbolic knowledge representations, declarative and procedural information learning, and utility-based decision making [27]. EPIC CA matches in parallel production rules to the working memory, followed by the selection of firing rules for multiple goals [30]. Sigma CA includes mixed symbolic-probabilistic, discrete-continuous representations, knowledge summarization and integration, and inference-based reasoning [29]. Clarion CA maintains explicit and implicit cognition, each having different representations and processing methods, e.g., rule extraction, generalization, specialization, backpropagation, and reinforcement learning [31]. InnovA is a CA for automated design of electronic circuits [32]. III Overview of Large Language Models, Prompt Engineering, Retrieval-Augmented Generation, and Reinforcement Learning"
    },
    {
      "title": "_Large Language Models_",
      "text": "Large Language Models (LLMs), primarily those built on transformer architectures, have made significant strides in producing coherent, contextually relevant text [33]. They excel at pattern recognition and can generate fluent natural language by leveraging billions of parameters trained on massive corpora [34]. However, their computational principle--self-attention over sequential data--imposes fundamental limitations that hinder their ability to perform the rich, open-ended problem-solving tasks described in the previous sections. At the core of these limitations is the reliance on statistical correlations rather than genuine logical or conceptual understanding. While self-attention excels at identifying relevant tokens in a sequence, it does not inherently encode hierarchical structures, domain-specific causal rules, or strict logical constraints. This stands in contrast to open-ended problem solving, where the concept space can be segmented into three main categories--hierarchical concepts, alternative concepts, and fundamental concepts--and the action space encompasses complex operations, such as feature combination, dynamic adjustment, abstraction, insight generation, and summarization [35]. LLMs struggle to engage these conceptual spaces in a principled way because they are not grounded in mechanisms that ensure hierarchical reasoning, strategic problem decomposition, or the flexible reuse of insights and intermediate representations [36]. Another critical shortcoming is that LLMs tend to produce generalized answers aligned with the statistical patterns seen in their training data [37]. They are not inherently equipped to execute a true divide-and-conquer approach to complex tasks, nor can they systematically apply trial-and-error strategies. For example, while open-ended problem solving may demand iterative refinement--where a solver explores a space of possible solutions, backtracks as necessary, and learns from failed attempts--an LLM's output is typically a single forward pass [38]. Without an internal model of logical inference, memory structures that accumulate knowledge over multiple steps, or explicit strategy formulations, LLMs cannot easily correct their reasoning or adapt their approach based on previous mistakes [39]. This leads to issues such as hallucinations, where models confidently assert falsehoods; distractions, where irrelevant details are emphasized; and a general inability to build complex, causally grounded explanations. Some researchers have explored techniques like constraint-based decoding to enforce logical or linguistic rules at inference time [40]. This can improve consistency and coherence to some extent, but it remains an add-on rather than a fundamental solution. Constraint-based methods do not grant the model a deeper conceptual understanding; they merely prune outputs that violate predetermined constraints. Similarly, improvements like sparse attention mechanisms reduce computational complexity, adapter layers can inject domain-specific knowledge [41], and memory-augmented transformers attempt to store and reuse intermediate reasoning steps. While these approaches enhance performance on certain tasks, they do not fully overcome the inherent limitations of attention-based architectures or enable robust open-ended problem solving. The models are still limited by their training data, biased toward patterns present therein, and lack the ability to intentionally search concept space, systematically test hypotheses, or derive new conceptual abstractions beyond what is statistically suggested [42]. In response to these challenges, a body of methods has emerged to push LLMs closer toward more sophisticated reasoning and problem-solving behaviors. This work can be broadly divided into three interrelated categories: Prompt Engineering, knowledge retrieval through Retrieval-Augmented Generation, and model refinement through Reinforcement Learning)."
    },
    {
      "title": "_Prompt Engineering_",
      "text": "Prompting techniques utilize carefully constructed input prompts to guide the model's response generation process. Techniques can be grouped into five categories dicussed next. Single-stage prompting (SSP)SSP methods directly instruct the model without iterative refinement. Meanwhile, Basic + Annotation Guideline-Based Prompting + Error Analysis-Based Prompting [43] uses formally defined entity annotation guidelines to specify how clinical terms should be identified and categorized, ensuring clarity in entity recognition. In addition, it incorporates instructions derived from analyzing common model errors, such as addressing ambiguous entity boundaries or redefining prompts for overlapping terms. This strategy significantly improves clinical Named Entity Recognition, with relaxed F1 scores reported as 0.794 for GPT-3.5 and 0.861 for GPT-4 on the MTSamples dataset [44] and 0.676 for GPT-3.5 and 0.736 for GPT-4 on the VAERS dataset [45], demonstrating its effectiveness. Reasoning strategiesThese methods are of three types: linear, branching, and iterative reasoning. Linear reasoning methods such as Chain-of-Thought (CoT), Complex CoT, Thread-of-Thought (ThoT), Chain-of-Knowledge (CoK), Chain-of-Code (CoC), Logical Thoughts (LoT), Chain-of-Event (CoE), and Chain-of-Table generate a single, step-by-step sequence (chain) of responses toward the final answer. Methods differ in the type of task they target, i.e., code generation, summarization, and logical inference, and in how they refine or represent intermediate steps. CoT shows that using intermediate prompting steps can enhance accuracy, e.g., up to 39% gains in mathematical problem solving [46]. An example of in-context prompt for CoT might be: _\"If the problem is 'Calculate 123 \\(\\times\\) 456,' break it down as (100 + 20 + 3) \\(\\times\\) 456 and compute step-by-step.\"_ Complex CoT uses more involved in-context examples, improving performance by as much as 18% on harder tasks [47]. ThoT tackles long or chaotic contexts by _breaking them into manageable parts_ (e.g., dividing long passages into sections for sequential summarization) [48], while CoK strategically adapts and consolidates knowledge from multiple sources to ensure coherence and reduce hallucination [49]. CoC specializes in code-oriented reasoning by simulating key code outputs (e.g., predicting intermediate variable states for debugging) [50], whereas LoT integrates logical equivalences and _reductio ad absurdum_ checks to refine reasoning chains (e.g., validating statements by identifying contradictions in their negations) [51]. CoE handles summarization by extracting, generalizing, filtering, and integrating key events (e.g., pinpointing main events from news articles) [52], and Chain-of-Table extends CoT techniques to tabular data, dynamically applying transformations like filtering or aggregation to generate coherent answers [53]. Branching reasoning methods, like Self-Consistency, Contrastive CoT (or Contrastive Self-Consistency), Federated Same/Different Parameter Self-Consistency/CoT (Fed-SP/DP-SC/COT), Tree-of-Thoughts, and Miaieutic Prompting, explore multiple possible reasoning paths in parallel. Branching techniques vary in how they sample or fuse paths, some relying on consensus votes and others on dynamic adaptation or tree-based elimination. Self-Consistency, for instance, samples diverse solution paths and selects the most consistent final answer, achieving gains of over 11% on math tasks [54]. Contrastive CoT incorporates both correct and incorrect in-context examples to broaden the model's understanding, improving performance by over 10% compared to standard CoT [55]. Fed-SP-SC leverages paraphrased queries to crowdsource additional hints [56], while ToT maintains a tree of partial solutions and systematically explores their with breadth-first or depth-first strategies, offering up to 65% higher success rates than CoT on challenging math tasks(ToT) [57]. Miaeuittic Prompting likewise generates a tree of propositions to reconcile contradictory statements, surpassing linear methods by 20% on common-sense benchmarks [58]. Iterative reasoning approaches, such as Plan-and-Solve (PS), Program-of-Thoughts (PoT), Chain-of-Symbol (CoS), Structured Chain-of-Thought (SCoT), and Three-Hop Reasoning (THOR), refine solutions step by step, often by passing intermediate outputs back into the model to enhance accuracy. PS explicitly decomposes tasks into planning and execution phases, where the planning phase structures the problem into smaller sub-tasks, and the execution phase solves them sequentially. This reduces semantic and calculation errors, outperforming Chain-of-Thought (CoT) prompting by up to 5% [59]. PoT enhances performance by separating reasoning from computation: the model generates programmatic solutions executed by a Python interpreter, achieving up to 12% accuracy gains in numerical and QA tasks [60]. CoS encodes spatial and symbolic relationships using concise symbolic representations, which improves reasoning in spatial tasks by up to 60.8% [61]. SCoT introduces structured reasoning through program-like branching and looping, significantly improving code generation accuracy by up to 13.79% [62]. Finally, THOR tackles emotion and sentiment analysis through a three-stage approach: aspect identification, opinion analysis, and polarity inference. This structured method achieves superior performance compared to previous supervised and zero-shot models [63]. These approaches exemplify the power of iterative methods in breaking complex problems into manageable components, thereby reducing errors and improving overall performance. Multi-Stage Prompting (MSP)MSP techniques rely on iterative feedback loops or ensemble strategies. MSP methods systematically refine outputs and incorporate multiple response paths, e.g., through voting or iterative analysis, to yield more robust and accurate solutions, particularly in domains requiring deeper reasoning or tailored task adaptation. Ensemble Refinement (ER) [64] builds on Chain-of-Thought (CoT) and Self-Consistency by generating multiple CoT-based responses at high temperature (introducing diversity) and then iteratively conditioning on generated responses to produce a more coherent and accurate output, leveraging insights from the strengths and weaknesses of initial explanations and majority voting. Auto-CoT [65] constructs demonstrations automatically by clustering queries from a dataset and generating reasoning chains for representative queries using Zero-Shot-CoT. Clustering is achieved by partitioning questions into groups based on semantic similarity, ensuring that representative queries capture the diversity of the dataset. ReAct [66] interleaves reasoning traces--thought processes that explain intermediate steps--with action steps that execute operations, enabling superior performance in complex tasks by seamlessly combining reasoning and action. Moreover, Active-Prompt [67] adaptively selects the most uncertain training queries, identified via confidence metrics like entropy or variance, for human annotation, boosting few-shot learning performance by focusing on areas with the highest uncertainty. Knowledge EnhancementThese approaches use high-quality examples and strategic self-monitoring to improve LLM performance. They pertain to two types, example-based and meta-level guidance methods Example-based methods leverage auxiliary examples or synthesized instances to guide the response creation process of LLMs. MathPrompter [68] focuses on creating a symbolic template of the given mathematical query, solving it analytically or via Python, and then validating the derived solution with random variable substitutions before finalizing the answer. The approach boosts accuracy from 78.7% to 92.5%. Analogical Reasoning [69] prompts LLMs to generate and solve similar examples before addressing the main problem, resulting in a 4% average accuracy gain across various tasks. Synthetic Prompting [70] involves a backward step, where a new query is generated from a self-constructed reasoning chain, and a forward step, where this query is re-solved; this strategy selects the most complex examples for few-shot prompts, leading to up to 15.6% absolute improvements in mathematical problem solving, common-sense reasoning, and logical reasoning. Meta-Level Guidance (MLG) methods enhance LLMs by promoting self-reflection and focusing on pertinent information, thereby reducing errors. Self-Reflection involves the model evaluating its own outputs to identify and correct mistakes, leading to improved performance. For example, in translation tasks, self-reflection enables LLMs to retrieve bilingual knowledge, facilitating the generation of higher-quality translations. Focusing is achieved through techniques like System 2 Attention (S2A) [71], which filters out irrelevant content by prompting the model to regenerate the context to include only essential information before producing a final response. This two-step approach enhances reasoning by concentrating on relevant details, thereby improving accuracy. S2A has been shown to outperform basic prompting methods, including Chain-of-Thought (CoT) and instructed prompting, particularly on truthfulness-oriented datasets. Metacognitive Prompting (MP) [72] introduces a five-stage process to further enhance LLM performance: (1) Comprehension: The model attempts to understands the input, ensuring clarity before proceeding; (2) Preliminary Judgment: An initial assessment is made based on the understood information; (3) Critical Evaluation: The initial judgment is scrutinized, considering alternative perspectives and potential errors; (4) Final Decision with Explanation: A conclusive decision is reached, accompanied by a rationale to support it; and (5) Self-Assessment of Confidence: The model evaluates its confidence in the final decision, reflecting on the reasoning process. This structured approach enables LLMs to perform consistently better than methods like CoT and Program Synthesis (PS) across various natural language processing tasks, including paraphrasing, natural language inference, and named entity recognition. Task DecompositionThese approaches break down complex tasks into smaller steps but vary in how they orchestrate and execute the sub-problems. They include problem breakdown and sequential solving methods. Problem Breakdown approaches include the Least-to-Most method [73], which addresses the challenge of Chain-of-Thought (CoT) failing on problems more difficult than its exemplars by first prompting the LLM to decompose a query into sub-problems and then solving them sequentially, demonstrating notable improvements over CoT and basic prompting on tasks like commonsense reasoning and mathematical problem solving. The decompositions are characterized by their hierarchical structure, breaking down complex problems into simpler, manageable sub-tasks that build upon each other to facilitate step-by-step reasoning. Decomposed Prompting (DecomP) breaks complex tasks into simpler sub-tasks, each handled with tailored prompts or external tools, ensuring efficient and accurate execution. For instance, the task \"Concatenate the first letters of words in 'Jack Ryan'\" is decomposed into extracting words, finding their first letters, and concatenating them [74]. DecomP leverages modular decomposors to partition problems hierarchically or recursively, assigning sub-tasks to specialized LLMs or APIs. This approach achieves a 25% improvement over CoT and Least-to-Most methods in Commonsense Reasoning. Program-Aided Language Models (PAL) [75] further leverage interleaved natural language and programmatic steps to enable Python-based execution of the reasoning process, surpassing CoT and basic methods for mathematical and commonsense tasks. Sequential Solving includes methods, like Binder and Dater algorithms. Binder [76] integrates neural and symbolic parts by using an LLM both as a parser and executor for natural language queries, leveraging programming languages like Python or SQL for structured execution. Binding is achieved through a unified API that enables the LLM to generate, interpret, and execute code using a few in-context examples, leading to higher accuracy on table-based tasks compared to fine-tuned approaches. Dater [77] focuses on few-shot table reasoning by splitting a large table into relevant sub-tables, translating complex queries into SQL sub-queries, and combining partial outcomes into a final solution. These three steps aim to systematically extract meaningful data, execute precise operations, and integrate results to address complex queries, outperforming fine-tuned methods by at least 2% on Table-Based Truthfulness and 1% on Table-Based QA, and surpassing Binder on these tasks."
    },
    {
      "title": "_Retrieval-Augmented Generation_",
      "text": "Retrieval-Augmented Generation (RAG) addresses one of the major issues of LLMs, which are their lack of a persistent, reliable memory and factual grounding [78]. RAG methods integrate external knowledge sources into the generation process. Instead of relying solely on learned representations within the model's parameters, the system retrieves relevant documents, facts, or structured data at inference time and incorporates this information into its output. This grounding reduces hallucinations, ensures that the model's reasoning steps reference accurate and up-to-date information, and can improve the alignment of the solution with real-world constraints [79]. The versatility of RAG has led to significant advancements in various domains, such as healthcare, finance, education, and scientific research facilitated by novel frameworks tailored to address challenges in reasoning, problem-solving, and knowledge integration. This review categorized these advancements into four areas: task-specific and schema-based techniques, self-aware and adaptive mechanisms, long-term memory integration, and multi-hop and multi-modal reasoning. The four areas are discussed next. Task-Specific and Schema-Based Retrieval (TSR)TSAR approaches leverage structured methods to solve problems in domains such as mathematics and knowledge-intensive tasks. Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) [80] employs schema-based instruction to solve math word problems by predicting relevant schemas, offering a structured problem-solving paradigm. For instance, given the problem, \"If a worker earns $20 per hour, how much will they earn in 10 hours?\", the model predicts the multiplicative schema, which involves calculating the product of hourly earnings and hours worked. Using this schema, the problem-solving process is guided step-by-step: the model multiplies the hourly rate ($20) by the number of hours (10), resulting in a total earning of $200. Schemas act as templates for organizing and applying domain-specific knowledge and are inherently tied to knowledge graphs that map relationships between concepts, further enhancing reasoning capabilities. By aligning the problem context with predefined patterns, SBI-RAG ensures systematic and accurate solutions while improving explainability. Similarly, Knowledge Graph-Enhanced RAG Framework (KRAGEN) [81] employs advanced prompting techniques, notably the graph-of-thoughts (GoT) method, to dynamically decompose complex problems into smaller subproblems. Each subproblem is addressed using relevant knowledge retrieved through the RAG framework, minimizing hallucinations and enhancing solution accuracy. The individual solutions are then consolidated to form a comprehensive answer, with KRAGEN's graph visualization enabling users to interact with and assess the quality of the solution's GoT structure and logic [81]. These techniques stand out for their ability to address domain-specific challenges while ensuring adaptability through schema-guided reasoning. The use of schemas not only structures the solution process but also facilitates explainability. In data-driven tasks, Generative Retrieval-Augmented Matching (GRAM) addresses schema matching by employing a hierarchical classification model that dynamically generates prompts for matching attributes across schemas. Specifically, GRAM utilizes a two-step process: first, it performs a coarse-grained classification to identify potential attribute matches. For instance, given two schemas, GRAM might preliminarily match the attribute \"Customer Name\" in one schema with \"Client Name\" in another. Then, it refines these matches through fine-grained classification, analyzing the context and patterns in the data to confirm the match and enhance the precision of schema alignment. In this case, GRAM would validate that \"Customer Name\" and \"Client Name\" indeed refer to the same entity by assessing their usage and data properties. The prompt generation process, guided by LLMs, enables zero-shot and few-shot learning, allowing GRAM to perform efficiently and accurately in database integration tasks, even when minimal labeled data is available [82]. Similarly, TableRAG [83] focuses on reasoning over tabular data by retrieving and processing row-column relationships to interpret structured datasets accurately. It conducts reasoning by leveraging query expansion combined with schema and cell retrieval to pinpoint crucial information before providing it to the language models, enabling efficient data encoding and precise retrieval. This approach allows TableRAG to handle large-scale tables effectively, reducing prompt lengths and mitigating information loss during the reasoning process [83]. _b) Self-Aware and Adaptive Retrieval:_ Recent RAG frameworks emphasize self-awareness and adaptive mechanisms to address uncertainties in LLMs. Self-aware Knowledge Retrieval (SeaKR) [84] activates retrieval during high uncertainty and re-ranks snippets to ensure reliability. Specifically, SeaKR [84] addresses uncertainties arising from the LLM's internal state inconsistencies, triggering retrieval when the model's self-assessed confidence is low. The re-ranking process involves selecting knowledge snippets that most effectively reduce the model's uncertainty, thereby enhancing response accuracy [84]. Self-RAG [85] introduces iterative refinement, where retrieval queries generated during the response process enable reassessment and improvement of outputs. This reassessment involves evaluating the relevance of retrieved information during generation, allowing the model to iteratively refine its responses for enhanced accuracy. Critical-Guided Planning (CR-Planner) [86] leverages critic models to iteratively guide retrieval and reasoning toward task-specific goals. The critic model operates by evaluating potential sub-goals and their executions, assigning rewards to guide the selection of the most promising reasoning paths. This guidance ensures that the reasoning process aligns with task objectives, effectively navigating complex problem spaces [86]. For domain-specific adaptation, SimRAG [87] employs self-training, generating and filtering synthetic data to fine-tune models for specialized fields. In biomedical applications, Self-Rewarding Tree Search (SeRTS) [88] combines Monte Carlo Tree Search and Reinforcement Learning to optimize retrieval. Speculative RAG [89] improves efficiency with a two-stage process: a smaller model drafts responses, while a larger model evaluates and finalizes them. This two-step process allows the system to balance efficiency and accuracy by leveraging the strengths of both models. These approaches offer distinct benefits and limitations. SeaKR and Self-RAG provide dynamic adaptability and accuracy but demand significant computational resources. CR-Planner and SeRTS enhance task-specific precision but increase complexity. SimRAG excels in domain-specific tuning, however it is constrained by the need for high-quality synthetic data. Speculative RAG effectively reduces latency through parallel drafting and verification, but requires accurate evaluation by generalist models. _c) Long-Term Memory for Knowledge Retrieval:_ Long-term memory integration in RAG frameworks addresses the limitations of purely query-specific retrieval by enabling the re-tention and reuse of knowledge across tasks. HippoRAG [90], inspired by the hippocampal indexing theory, integrates long-term memory by linking a knowledge graph to an LLM and prioritizing relevant nodes using the Personalized PageRank algorithm. This approach allows the model to retrieve interconnected knowledge dynamically, consolidating past context for tasks like multi-hop reasoning, achieving up to 20% better performance. It excels in repetitive and longitudinal tasks by enabling adaptive and context-aware retrieval, mimicking how the brain organizes and recalls episodic memories. Various architectures embed long-term memory into RAG. MemLong [91] employs a dual-network design where a frozen LLM backbone serves as a memory encoder, while a residual side-network manages retrieval, enabling efficient caching and updating of extensive contexts (up to 65k tokens). Its key advantage is scalability without data staleness, though managing large contexts may introduce overhead. HAT [92] introduces a Hierarchical Aggregate Tree structure that organizes dialogue history into a tree, where each node represents aggregated information from its child nodes. This design allows the system to manage extensive conversational contexts by traversing the tree to retrieve relevant information, enhancing coherence and summary quality. The recursive aggregation enables the model to handle long-term dependencies effectively, though challenges may arise in balancing tree depth with performance. MemoRAG [93] combines a lightweight global memory model with a retrieval-generation module, using draft answers as \"clues\" to guide precise retrieval from extensive datasets. It efficiently handles up to one million tokens by separating memory updates from retrieval operations, ensuring scalability and contextual relevance. This architecture excels in complex tasks but requires fine-tuning to balance memory efficiency and retrieval accuracy. Pistis-RAG [94] introduces a scalable, multi-stage framework for retrieval-augmented generation (RAG) systems, emphasizing alignment with human preferences through online learning and user feedback. Its architecture comprises distinct stages--matching, pre-ranking, ranking, reasoning, and aggregating--each refining the retrieval process to enhance response quality. A notable innovation is the ranking stage, which considers both semantic relevance and LLM preferences, addressing the sensitivity of LLMs to prompt ordering. By adopting a content-centric approach, Pistis-RAG integrates user feedback to continuously adapt and align with evolving user needs, resulting in a 9.3% performance improvement on the MMLU (English) benchmark. However, the reliance on continuous user feedback may introduce variability, necessitating careful system tuning to maintain consistent performance. _d) Multi-Hop and Multi-Modal Reasoning Retrieval:_ Multi-hop and multi-modal reasoning broaden Retrieval-Augmented Generation (RAG)'s capacity to tackle tasks requiring complex, step-by-step deliberation and integration of diverse data sources. Multi-hop reasoning connects information across multiple steps to derive coherent answers, while multi-modal reasoning combines data from various formats such as text, images, and audio. This systematic approach enhances RAG's ability to deliver comprehensive and well-founded responses to multifaceted queries. Multi-layered Thoughts Enhanced RAG (METRAG) [95] integrates similarity- and utility-based reasoning for deeper contextual understanding. It does so by combining similarity-oriented retrieval with utility-oriented assessments, where a utility model, supervised by an LLM, evaluates the usefulness of retrieved documents beyond mere similarity using metrics like task relevance, informativeness, query-specific novelty, and completeness, enhancing the relevance and quality of the information utilized in generation. RAG-Star [96] integrates retrieval augmentation with Monte Carlo Tree Search (MCTS) to improve problem-solving accuracy by iteratively planning intermediate sub-queries. Retrieval augmentation enables the model to incorporate external information, enhancing its reasoning process. Using MCTS, RAG-Star systematically explores reasoning paths by generating and evaluating intermediate sub-queries and their potential answers. This approach balances exploration and exploitation to identify the most promising reasoning trajectories, guiding the model toward highly accurate and contextually relevant solutions. Knowledge Graph-Enhanced RAG Framework (KRAGEN) [81] employs a \"Graph-of-Thoughts\" methodology to decompose multi-hop reasoning problems into explainable and systematic components. This approach structures the reasoning process by representing knowledge as interconnected concepts and relationships, often derived from knowledge graphs. During problem-solving, the model constructs this graph dynamically, allowing it to break down complex queries into smaller, manageable sub-tasks. By systematically addressing each component, KRAGEN enhances both the interpretability and accuracy of its reasoning process, providing a more transparent and effective framework for handling intricate queries. However, a potential limitation of KRAGEN is its reliance on the quality and comprehensiveness of the underlying knowledge graph. If the knowledge graph lacks certain information or contains inaccuracies, the model's reasoning and outputs may be adversely affected. Ensuring the knowledge graph is up-to-date and accurately reflects the domain is crucial for maintaining the effectiveness of the KRAGEN framework. Building upon the foundational concepts of multi-hop and multi-modal reasoning, recent research has proposed innovative frameworks to address the inherent complexities of such tasks. These advancements focus on refining the step-by-step reasoning process and integrating diverse modalities, enabling models to navigate intricate queries with enhanced accuracy and explainability. By tackling challenges in sequential inferencing and combining textual with visual or other modal data, these frameworks set a new benchmark for retrieval-augmented generation systems. For instance, MultiHop-RAG provides a dedicated dataset and benchmarks to rigorously assess RAG systems on multi-step queries [97], facilitating the evaluation of retrieval-augmented generation models in scenarios that necessitate reasoning across multiple documents. Retrieval-Augmented Multi-modal Chain-of-Thoughts Reasoning [98] extends Chain-of-Thought (CoT) approaches to handle images and text in tandem, enabling models to process and reason over visual and textual data simultaneously. For purely textual multi-hop question answering, HOP, UNION, GENERATE (HUG) [99] offers a three-step method that models rationales as sets of sentences, enhancing explainability without requiring explicit rationale supervision. In this framework, \"Hop\" involves selecting relevant sentences, \"Union\" aggregates these sentences into a coherent rationale set, and \"Generate\" produces the final answer based on the aggregated rationale. The rationales modeled are the sets of sentences that collectively support the answer, providing transparency in the reasoning process by explicitly outlining the evidence considered. Multimodal-CoT and Multi-Chain Reasoning (MCR) [100] further advance reasoning by respectively separating rationale generation from answer inference for science question answering, and by prompting LLMs to examine multiple parallel chains of thought before synthesizing final solutions. These approaches address complex reasoning types that require integrating diverse information sources and evaluating multiple reasoning pathways. The rationale generated includes intermediate reasoning steps that elucidate the thought process leading to the answer. Prompting is generated by designing specific instructions that guide the model to consider various perspectives and reasoning chains, thereby enhancing the robustness and accuracy of the final output. Although RAG improves factual correctness and can help the model explore a broader concept space by tapping into external repositories, it still does not imbue the model with a genuine, internal problem-solving strategy. Self-Reflection MethodsRecent advancements underscore the value of LLMs engaging in reflective reasoning before generating a final answer. Reflective reasoning involves the model's introspection and evaluation of its own thought processes to enhance decision-making and output quality. Implicit Retrieval-Augmented Generation (RAG) [78, 101, 102] instructs LLMs to first retrieve key chunks of context, specifying the number of sections and words in each section, then use these snippets to answer queries. The selection of the number of snippets and their lengths is typically determined through empirical tuning, balancing the need for comprehensive context with the constraints of the model's input capacity. This method has achieved near state-of-the-art results in both general and biomedical contextual question-answering tasks. Metacognitive Prompting (MP) [72] draws on the concept of metacognition, comprising five phases: 1. _Interpreting the Input_: The model analyzes the input text to grasp its context and meaning, ensuring a clear understanding of the task at hand. This is implemented by prompting the model to restate or summarize the input, confirming comprehension. 2. _Forming an Initial Judgment_: Based on the interpreted input, the model generates a preliminary response or hypothesis, reflecting its immediate understanding. This involves producing an initial answer without external validation. 3. _Critically Assessing that Judgment_: The model evaluates its preliminary response, identifying potential errors or uncertainties. This is achieved by prompting the model to question its initial answer, consider alternative interpretations, and assess the confidence level of its response. 4. _Presenting a Final Decision with Reasoning:_ After critical assessment, the model formulates a refined answer, providing a rationale that outlines the reasoning process. This step ensures transparency and allows users to understand the basis of the model's conclusion. 5. _Gauging Confidence in the Entire Process_: The model reflects on the overall process, assigning a confidence score to its final answer, indicating the reliability of the response. This is implemented by having the model express its certainty level, guiding users in decision-making. MP consistently outperforms Chain-of-Thought (CoT) and Plan-and-Solve methods across paraphrasing, natural language inference, and relation extraction tasks. Self-Critique Methods/Evaluation- and Verification-Focused MethodsTo enhance reliability and reduce factual inaccuracies in automated reasoning, self-critique methods have emerged as critical tools. These methods address challenges in producing consistent, accurate outputs by systematically verifying and refining initial responses. Chain-of-Verification (CoVe) [103] uses a four-step process: (1) generating an initial response, (2) formulating verification questions to identify potential errors or inconsistencies, (3) answering these questions to produce supporting evidence or rationale, and (4) revising the original response based on validated findings. CoVe has demonstrated over 10% performance improvements compared to basic prompting and Chain-of-Thought (CoT) methods in both context-free and contextual question-answering tasks. Verify-and-Edit (VE) [104] enhances uncertain CoT outputs by integrating external knowledge from reliable sources such as encyclopedias, knowledge graphs, or domain-specific repositories. Self-consistency identifies weak points in reasoning by generating multiple reasoning paths for the same problem and comparing their outputs for discrepancies or logical contradictions, revealing areas of low confidence or errors. The response is then revised by incorporating validated evidence, ensuring factual accuracy and logical coherence. Cross-referencing further verifies the revised response by re-checking it against retrieved knowledge to confirm it resolves inconsistencies while maintaining alignment across all reasoning steps, avoiding the introduction of new errors or contradictions. VE evaluates the reliability of the final output by analyzing agreement across revised reasoning paths and ensuring alignment with external knowledge. This approach has achieved up to 10% gains in multi-hop reasoning tasks and 2% improvements in truthfulness evaluations over CoT and self-consistency techniques. In summary, self-critique methods, i.e., CoVe and VE, concentrate on verifying and refining initial outputs to reduce inaccuracies, while self-reflection techniques, e.g., Implicit RAG and MP, emphasize reflective reasoning for deepening understanding and clarity before producing an answer. CoVe and VE diverge in methodology: CoVe generates verification queries for self-checking, whereas VE specifically pinpoints uncertain outputs and edits them using external knowledge."
    },
    {
      "title": "_Reinforcement Learning_",
      "text": "Reinforcement Learning (RL) provides a systematic framework for refining LLM behavior by guiding models toward desired objectives through iterative feedback and carefully designed reward signals from human feedback, automated metrics, or a pre-trained reward model [105]. There are six main components: agent, environment, state, action, reward, and policy [106]. To apply RL for fine-tuning LLMs, the first step maps the six components to the LLM framework: the LLM represents the policy, while the current textual sequence is the state, and based on this state, the LLM generates an action, the next token. This action updates the state, creating a new state that incorporates the newly added token. After generating a complete textual sequence, a reward is determined by assessing the quality of the LLM output. This reward can be used to train a pre-trained reward model or can be directly integrated into the alignment process to guide the behavior of the model. The RL methods adopted by these models can be divided into two main categories, model-based RL approaches and model-free approaches, which were discussed next. Model-based RL ApproachesThe methods in this category can be grouped into three categories, RLHF, RLAIF and exploration, which are discussed next. Reinforcement Learning from Human Feedback (RLHF): RL from Human Feedback (RLHF) re-train LLMs by incorporating a reward signal derived from human evaluations. RLHFs perform three fundamental stages: They initially perform supervised fine-tuning (SFT) using labeled datasets, followed by training a reward model (RM) based on human-evaluated outputs, and finally use this reward signal to inform the model's policy fine-tuning using the Proximal Policy Optimization (PPO) algorithm [107]. [108] created fine-tuned models like InstructGPT using human feedback to better adhere to user instructions. Similarly, [109] and [110] explored reward modeling and methods to address challenges such as length bias, ensuring outputs are concise and aligned with human expectations. Frameworks like trlX [111] and high-quality datasets introduced by [112] have scaled RLHF applications, improving the performance of LLMs in tasks such as summarization, translation, and dialogue generation. Summarization tasks, for example, leverage reinforcement learning (RL) through both extractive and abstractive methods; extractive summarization selects key sentences from the source, while abstractive summarization generates novel sentences to convey the essence of the content [113]. RL optimizes summarization by using rewards based on metrics like ROUGE to iteratively enhance the quality of outputs. Policy optimization, on the other hand, employs pairwise feedback, comparing response pairs to align LLM outputs with human preferences. Techniques such as Pairwise Proximal Policy Optimization simplify the process by directly operating on comparative rewards, avoiding complexities like value function estimation and normalization [114]. PPO algorithms iteratively adjust the weights of a model to maximize the expected reward [107]. Central to this process is the collection of human feedback, which is critical in training reward models. Studies, such as Skywork-Reward [115] and TULU-V2-mix [116], utilize human preferences by curating datasets of ranked examples, enabling models to align more effectively with human judgments. Additionally, [117] introduces tool-augmented reward modeling, integrating external resources like calculators and search engines to refine alignment. Recent generative reward models use synthetic preferences, which are artificially created by sampling and ranking model outputs using a base preference model, to reduce reliance on extensive human feedback. [118] examined efficient methods for collecting pairwise human preferences, optimizing reward model design within RLHF frameworks. Additionally, research on over-optimization risks underscores the importance of balanced training to prevent performance degradation [119]. [114] propose novel pairwise feedback pipelines that improve preference learning and policy optimization by comparing response pairs to better capture human preferences. RLHF's multi-step process remains resource-intensive and reliant on extensive human feedback [120]. Over-optimization risks may cause models to exploit weaknesses in the reward function rather than achieving genuine alignment with human preferences [119]. RL from AI Feedback (RLAIF) is a training method designed to replace human evaluators with AI systems, offering better scalability and consistency by mitigating the variability of human judgment [121]. In RLAIF, a Reward Model (RM) is trained using preference labels generated by an LLM. These labels are transformed into a probability distribution through a softmax function and optimized via cross-entropy loss, enabling the RM to guide the training of the target AI model [122]. Various approaches have been proposed to address the specific challenges of RLAIF. For example, UltraFeedback compiles a large-scale dataset of over one million GPT-4 feedback annotations on 250,000 user-assistant conversations to train reward models [112]. Similarly, Magpie employs a self-synthesis method, where an aligned LLM generates large-scale alignment data that fine-tunes reward models [123]. HelpSteer2 introduces a permissively licensed preference dataset to train reward models, demonstrating improved alignment with human preferences [124]. Another approach focuses on prompting LLMs to function as reward functions, directly guiding model training through reward scores, as seen in Exploring with LLMs (ELLM) Rewards [125]. Additional work, such as Reward Design with Language Models, emphasizes constructing reward mechanisms that align model outputs with desired outcomes by leveraging LLM capabilities [126]. Self-supervised feedback mechanisms have also been explored; for instance, the Eureka framework introduces a novel approach to reward optimization through self-generated feedback loops [127]. Self-rewarding systems, including Self-Refined LLMs [128] and Self-Rewarding Language Models (SRLM) [129], enable iterative refinement of model outputs based on their own evaluations. Despite its potential, RLAIF remains less widely adopted compared to RLHF. This discrepancy stems from challenges, such as difficulties in achieving alignment and the risk of propagating biases inherent in AI-generated feedback [112, 127]. These challenges can create feedback loops that amplify existing biases, constraining model diversity and limiting its ability to generalize effectively [129]. Moreover, the absence of human evaluators in RLAIF can result in a lack of nuance, leading to a narrower latent space influenced by the biases of the training AI [128]. Exploration techniques in RL involves seeking new information to improve future decisions, whereas exploitation capitalizes on current knowledge to maximize immediate rewards [130]. In these algorithms, each action decision can be made stochastic via epsilon-greedy [131] or entropy regularization [132] to ensure diverse coverage of the environment, but excessive exploration can be inefficient. Traditional approaches, such as epsilon-greedy [133] and Boltzmann exploration [134], introduce randomness without leveraging prior knowledge, slowing convergence. Recent methods, like, ExploRLM [135] presents a hierarchical reinforcement learning framework that combines the strengths of LLMs and affordance-based policies. In this approach, LLMs generate high-level plans to outline strategic goals, while affordance-based policies, which identify actionable possibilities within the environment, execute specific actions to achieve those goals. This method improves exploration efficiency by prioritizing high-value states and reducing reliance on frequent LLM invocations. Despite its effectiveness in structured environments, the approach faces challenges in adapting to dynamic and open-ended domains [136]. Soft RLLF [137] integrates natural language as logical feedback to balance exploration and exploitation, enabling improved performance in reasoning tasks such as negation understanding and logical consistency in high-stakes applications. This is achieved by encoding logical consistency checks and negation handling into the learning process, utilizing feedback loops to iteratively refine the agent's decision-making. However, its effectiveness diminishes when tackling problems requiring broader adaptability and creativity, as it is optimized for structured reasoning [137]. Another recent approach, LLM+Exp [138], employs a dual-LLM framework: one LLM analyzes action-reward trajectories to derive exploration strategies, while the other dynamically adjusts action probabilities to refine future decisions. Action-reward trajectories represent sequences of actions taken by an agent and the corresponding rewards, offering insights into the learning process. Action probabilities define the likelihood of selecting specific actions based on learned patterns and anticipated outcomes. While this adaptive approach excels in structured environments, it faces scalability issues and struggles to generalize effectively to unpredictable or unstructured tasks. Guided Pretraining RL [125] leverages LLMs to enhance exploration by providing contextual background knowledge. This knowledge helps prioritize relevant actions, improving sample efficiency--allowing the agent to learn more effectively from fewer interactions. The method involves generating structured trajectories, which represent meaningful sequences of actions related to the task, using LLMs. These trajectories are used to pretrain the agent, providing a solid foundation before fine-tuning its policies in the target environment. While this approach excels in providing structure and reducing exploration costs, it struggles with tasks that require broader adaptability and creative problem-solving. The reliance on predefined trajectories limits its ability to generalize to highly variable or unpredictable environments, where more flexible reasoning is needed. Model Free ApproachesThese methods can be grouped into three categories, DPO, IPO, and actor critical. Their discussion follows next. Direct Preference Optimization (DPO) addresses the limitations of RLHF/PPO, which necessitates meticulous oversight and significant computational resources due to the initial phase to train a reward model using a preference dataset, followed by training an RL policy with the pre-trained reward model serving as the environment. DPO offers a simpler alternative by directly optimizing LLM parameters using preference data, bypassing the need for a reward model [139]. DPO relies on a preference loss function trained on datasets of paired human preferences (e.g., \"Response A is better than Response B\"). Several extensions to DPO improve upon this baseline. For instance, DPOP [140] (also termed DPO-positive) introduces a margin-based term to prevent rewarding both preferred and disfavored outputs concurrently, thereby improving performance on tasks with small edit distances. Specifically, the margin-based term in DPOP introduces a penalty for assigning high probabilities to both preferred and disfavored outputs, ensuring that the model distinctly favors the preferred response to improve task performance. Iterative DPO [129] (also known as _online_ DPO) mitigates distribution shifts by continually updating the policy on newly generated responses, an advantage over vanilla DPO, which can overfit to a narrower distribution. Meanwhile, \\(\\beta\\)-DPO [141] adaptively tunes the regularization term based on the data quality, making it more robust to noisy preferences. Stepwise DPO (sDPO) [142] partitions the preference dataset to perform incremental updates, leveraging a stronger intermediate reference model at each phase. DPO methods are particularly advantageous for structured problem-solving, like in creative writing or complex reasoning because they can directly incorporate human preferences and avoid undesired behavior without heavily relying on large-scale reward modeling or complex RL training loops [139]. However, a recurring drawback is their sensitivity to distribution shifts, e.g., when the model starts generating out-of-domain responses, alignment performance can drop unless the reference model or preference data is iteratively updated [143]. Moreover, purely relying on pairwise or setwise human judgments can still introduce label noise or ambiguity, especially for creative or unstructured tasks [144]. Despite these limitations, DPO-based techniques are promising for balancing helpfulness and correctness in open-ended LLM outputs [145]. Identity Preference Optimization (IPO) [146] was introduced to address the overfitting inherent in RLHF and DPO. Unlike traditional methods that transform pairwise preferences into pointwise rewards using the Bradley-Terry (BT) model [147], IPO directly optimizes preferences without relying on nonlinear transformations, which are known to exacerbate overfitting. The objective function of Identity Preference Optimization (IPO), as defined in eq. (1), aims to directly optimize preference probabilities while mitigating overfitting issues inherent in methods like RLHF and DPO. The function maximizes the expected preference utility, represented by \\[\\mathbb{E}_{x}\\left[\\mathbb{E}_{y,y^{\\prime}}\\Psi(P_{\\theta}(y>y^{\\prime})) \\right],\\] where \\(\\Psi(P_{\\theta}(y>y^{\\prime}))\\) captures the model's ability to predict and optimize preference probabilities for pairs of outputs (\\(y\\) and \\(y^{\\prime}\\)). To prevent excessive deviation from a reference policy, the KL divergence term \\(D_{\\text{KL}}(\\pi||\\pi_{\\text{ref}})\\) imposes a regularization constraint, controlled by the coefficient \\(\\beta\\). By balancing preference optimization and regularization, this approach avoids transforming pairwise preferences into pointwise rewards, which can exacerbate overfitting, and directly aligns the model's behavior with human preferences while maintaining stability. \\[\\pi_{\\theta}^{*}=\\max_{\\pi}\\mathbb{E}_{x}\\left[\\mathbb{E}_{y,y^{\\prime}}\\Psi(P _{\\theta}(y>y^{\\prime}))-\\beta D_{\\text{KL}}(\\pi||\\pi_{\\text{ref}})\\right]. \\tag{1}\\] To address the overfitting caused by the nonlinear transformation \\(\\Psi(x)\\), IPO simplifies \\(\\Psi(x)\\) to a linear function, \\(\\Psi(x)=x\\), and formulates a robust loss function, as defined in eq. (2). This loss function, \\(L_{\\text{IPO}}\\), directly optimizes the policy \\(\\pi_{\\theta}\\) by aligning it with human preferences while mitigating overfitting. The expectation is taken over pairs of outputs \\((y_{w},y_{l})\\), where \\(y_{w}\\) represents the preferred (winning) output and \\(y_{l}\\) the less preferred (losing) output. The terms \\(-\\log\\frac{\\pi_{\\theta}(y_{w})}{\\pi_{\\text{ref}}(y_{w})}\\) and \\(-\\log\\frac{\\pi_{\\theta}(y_{l})}{\\pi_{\\text{ref}}(y_{l})}\\) measure how well the current policy \\(\\pi_{\\theta}\\) aligns with the reference policy \\(\\pi_{\\text{ref}}\\), accounting for both preferred and less preferred outputs. A regularization term, \\(\\frac{1}{2\\beta}\\), balances the trade-off between optimizing preferences and maintaining adherence to the reference policy, ensuring model stability and reducing the risk of overfitting. By incorporating a squared penalty term, \\(L_{\\text{IPO}}\\) captures and penalizes deviations from ideal preference alignment, whether positive or negative. The simplified approach avoids the complexity and instability of nonlinear transformations, providing a stable and effective framework for aligning policies with human preferences. This makes IPO a robust and efficient alternative to traditional preference-based learning methods that rely on pointwise rewards or complex transformations. \\[L_{\\text{IPO}}=-\\mathbb{E}_{(y_{w},y_{l})}\\left[\\log\\frac{\\pi_{\\theta}(y_{w})}{ \\pi_{\\text{ref}}(y_{w})}-\\log\\frac{\\pi_{\\theta}(y_{l})}{\\pi_{\\text{ref}}(y_{l} )}-\\frac{1}{2\\beta}\\right]^{2}. \\tag{2}\\] This approach proves particularly robust in scenarios with deterministic or near-deterministic feedback, where existing methods often struggle due to unstable gradients [148]. By leveraging a simpler optimization framework and incorporating strong regularization, IPO effectively mitigates over-fitting and outperforms DPO in experimental settings [149]. However, IPO faces challenges due to its reliance on static preference distributions, which limits adaptability to dynamic or diverse scenarios. Additionally, its sensitivity to noise and dependence on high-quality data reduce robustness in complex, evolving environments [150]. Actor-critic methods, such as Advantage Actor-Critic (A2C) and Deep Deterministic Policy Gradient (DDPG), have been effectively adapted to optimize prompts for LLMs. Frameworks like Prompt Actor-Critic Editing (PACE) [151] employ an iterative process where the _actor_ (the LLM) generates a response \\(a\\) based on a prompt \\(p\\) and input \\(X\\). This process is formalized as \\[a=f_{\\text{actor}}([p;X],M),\\] where \\(f_{\\text{actor}}\\) represents the decision-making mechanism of the actor, \\([p;X]\\) is the concatenated context consisting of the prompt \\(p\\) and the specific input \\(X\\), and \\(M\\) is the LLM being optimized. The actor function processes the concatenated context to produce the response \\(a\\), guided by the prompt \\(p\\) and the input \\(X\\). The critic, another LLM or evaluation mechanism, evaluates the relevance, coherence, and task-specific accuracy of the response against the objective \\(Y\\). The critique is calculated as follows [151]: \\[c=f_{\\text{critic}}([p;X;a;Y],M),\\] where \\(f_{\\text{critic}}\\) represents the evaluation function of the critic. The input \\([p;X;a;Y]\\) consists of the prompt \\(p\\), the input \\(X\\), the actor-generated response \\(a\\), and the objective \\(Y\\), which defines the desired or target output. The critic processes this concatenated input using the language model \\(M\\) to generate a critique \\(c\\). This critique assesses how well the response \\(a\\) aligns with the objective \\(Y\\), considering both the input \\(X\\) and prompt \\(p\\). [152] leverages KL-regularization to balance fidelity to the original prompt while allowing modifications that improve task-specific performance. By iterating on this actor-critic loop, PACE enhances prompt effectiveness and guides LLMs toward better alignment with task objectives. Additionally, actor-critic methods assume well-structured feedback loops, which might be unreasonable for problems with sparse or noisy signals. Recent work addresses these challenges. [153] explores open-ended learning in the context of unsupervised skill discovery, highlighting the need for more flexible reward functions in high-dimensional environments. HDFlow [154] combines fast and slow thinking modes to enhance complex reasoning. [155] introduces Direct Q-function Optimization (DQO), which formulates response generation as a Markov Decision Process (MDP), allowing each token generation to be treated as a state transition. Leveraging the soft actor-critic (SAC) framework, DQO directly parameterizes the Q-function within the language model, enabling it to learn effectively from offline data, including unbalanced or negative samples that helps improve multi-step reasoning. IV Similarities of LLMs and Traditional Automated Implementation Generation Methods and Related Research Needs A broad analogy can be identified between using Genetic Algorithms (GAs) and LLMs for implementation creation. 1. **Selection:** GA selection chooses the fittest individuals to pass their genes to the next generation. In fine-tuning or training data selection, LLMs prioritize coherence and relevance when generating textS, similar to selecting relevant context for responses. Like choosing the best seeds from a harvest, LLMs select the most relevant words or sentences to continue a conversation [156]. 2. **Crossover (Recombination)**: GA crossover combines the genomes of two parents to create a new individual. This is similar to blending knowledge from different domains during text generation. For example, merging insights from literature and science in a single response. Crossover is like an LLM writing poetry about quantum physics, e.g., combining Shakespearean elegance with scientific rigor [157]. 3. **Mutation:** GA mutation introduces random changes in a genome to explore new possibilities. Similar to the slight randomness added during sampling techniques like top-k or temperature settings, which allow LLMs to produce diverse responses. Mutation in GAs is like LLMs occasionally breaking patterns to say something unexpected or creative [158]. 4. **Inversion:** GA inversion reverses a segment of the genome to explore new configurations. This parallels rephrasing or reordering sentences during text generation while preserving the original meaning. Like flipping a playlist order for a new vibe, LLMs rephrase \"The car is fast\" into \"A fast car it is\" [159]. 5. **Elitism:** GA ensures the best solutions carry over unchanged to the next generation. Similar to checkpointing the best-performing weights during training or favoring high-confidence outputs in decoding strategies. Like archiving the best answers during an essay edit, LLMs retain their most confident responses for the final output [160]. 6. **Replacement:** GA decides how much of the old population to keep versus the new one. Similar to parameter updates during fine-tuning, where new knowledge replaces older information incrementally. Replacement is like LLMs balancing old facts while integrating new updates, ensuring a model doesn't \"forget\" but adapts to current knowledge [161]. 7. **Fitness Evaluation:** GA scores individuals based on quality to determine their survival. Similar to evaluating model outputs using metrics like BLEU, ROUGE, or user feedback in RLHF. Fitness evaluation is like an LLM receiving human feedback to improve its responses based on relevance, coherence, or creativity [162]. 8. **Exploration vs. Exploitation:** GA balances trying new possibilities (exploration) and refining known solutions(exploitation). Balancing randomness and coherence during response generation. Parameters like temperature encourage exploration, while context relevance drives exploitation. Just as genetic algorithms search for novel solutions, LLMs strike a balance between playful creativity and logical reasoning in ambiguous prompts [163]. The next part discusses using Cognitive Architectures for implementation creation, possibly using the features of LLMs. Similar to [32], this report considers that devising an implementation for a problem specification utilizes the five strategies shown in Figure 1. The problem solving process is a mixture of the five strategies. Each strategy starts from a _kernel_, which is the invariant set of features used in the process. The problem solving process creates a solution cluster corresponding to the kernel features, e.g., each implementation in the cluster includes the features. Implementations are created through implementation elaboration by exploring a sequence of detailing alternatives. For example, the principle of the bubble sorting algorithm can be described as repeatedly comparing the adjacent values of an array and swapping them if they are in the wrong order until no more value swapping are needed. The kernel features include three features: (i) the values of an array, (ii) the swapping of adjacent values if they are in the wrong order, and (iii) the repetition of the process until no more swapping are needed. The corresponding cluster includes all implementations obtained by elaborating the three kernel features. The five strategies are as follows [32]: * Strategy 1 describes the elaboration process in which each kernel is elaborated without changing the kernel. A set of detailing alternatives can be used for each elaboration step to produce an implementation envelope. The envelopes are incrementally elaborated until the final implementation is created. * Strategy 2 represents the process, which in addition to the elaboration steps of Strategy 1 also uses elaboration results corresponding to a different implementation cluster. Figure 1 shows the using of features from Implementation cluster 1 (red arrow in the figure) to build the implementations of Implementation cluster 2. Hence, the subsequent solution include elaboration of all kernel features and the features adopted from another cluster. * Strategy 3 uses a kernel that combines kernel features from two different implementation clusters. The blue arrows in Figure 1 illustrates the combination. * Strategy 4 presents an elaboration process in which the selected detailing alternatives are excluded from the elaboration steps used for building other implementation clusters. It represents the excluded niche in Figure 1 (green arrow). * Strategy 5 creates a kernel bottom-up by identifying and generalizing the features of individual implementations. The individual implementations were produced through less-structured methods, like, for example, through experimental trial-and-error. While the five strategies provide templates for the implementation elaboration process, automated implementation generation requires the following additional activities: 1. _Divide and conquer_: The activity partitions a problem into sub-problems and then provides ways to integrate the implementation for the sub-problems. Task decomposition methods in LLM prompting [73, 74] can produce certain decompositions, especially in situations for which the sb-problems are less coupled. However, design problems are often strongly coupled, so that even though there are specialized modules to implement a certain function, their operation and performance are tightly related. Decomposition requires not only a static problem partitioning based on the items in the prompts (i.e. words) but also the interpretation of a sub-problem within the context set-up by the interpretations of other sub-problems. LLM fine tuning through RL is likely infeasible due to the huge space of possible decompositions possible in real-life. A mechanism is also needed to track the analyzed decompositions, so that the information can be used to improve future decompositions. This capability is absent in current methods. 2. _Kernel creation_: The method creates kernels either by assembling the features likely to address the problem requirements and then elaborating them top-down or in a bottom-up process as detailed in Strategy 5. Separate kernels can be created for different sub-problems followed by integrating them into a single kernel and its elaboration or separately elaborating each kernel and integrating their implementations. Ideas on LLM self-reflection and focus on the main information [71] can help identify the features to be included in a kernel. However, finding kernels, e.g., the invariant features present in all implementations pertaining to a cluster, remains mostly a manual process. Methods similar to RLHF [107] can help retrieving similar features, but Fig. 1: Five strategies for automated implementation creation their scalability is likely low. Moreover, combining features from different kernels to generate a new kernel (Strategy 3) has not been studied by current LLM methods. The combination of features needs a way to predict the expected performance at a high level (possibly a qualitative evaluation), which can be offered to some degree by LLM, similar to the use of LLM to solve ambiguities [82, 83]. However, it is likely that the current methods are insufficient for this purpose. 3. _Elaboration_: Executing the five strategies requires devising additional methods for detailing alternatives, predicting the effectiveness of each alternative in the context of a partial implementation, assigning a priority (preference) to each alternative, and incorporating the alternative into the partial implementation. A possible approach is to use schema for elaboration, similar to RAG methods for LLMs [80, 81]. Schema matching can benefit from LLMs to clarify certain ambiguities, such as in [82, 83]. However, schema are static structures, useful in analogical reasoning, even though problem solving often requires performing new sequences of decisions beyond a static schema. 4. _Implementation assessment_: LLMs can be used for two kinds of performance assessments. Qualitative assessment, including comparing implementations, such as pairs of circuit designs, can be obtained by prompting traditional LLMs. CoT prompting can be used to obtain performance assessment at a finer granularity. RLHF can fine tune assessment by adding human feedback about the quality of the implementations [112]. Moreover, self-critique methods could be used to improve the correctness and completeness of the LLM responses, like self-consistency and cross-referencing methods in VE [104]. A second approach uses datasets of characterized implementations to train an LLM, similar to exploration techniques in RL [133, 134]. Then, the generalization capacity of LLMs are used to quantitatively predict the performance parameters of a new implementation. Nevertheless, the two approaches do not scale beyond the samples used in training an LLM, including situations in which a new implementation uses a nonlinear combination of the features of different implementation. There is no mechanism similar to setting-up precise physical models of an implementation, so that the models can be solved to produce quantitative performance assessment, like in traditional automated implementation creation methods. 5. _Memory and learning_: Similar to using long-term memory for knowledge retrieval in RAG, memory systems are needed to for learning to store associations, like kernel features, their most relevant implementations fragments, and their performance values or between high-level features and their detailed elaborations, the causal relationships of main features and performance attributes, and elaboration sequences that produced high-quality implementations. Similar to schema-based retrieval, memory cueing must solve semantic ambiguities. 6. _Adaptive process_: It includes the sequence of automated activities performed to create an implementation. It requires devising new means to predict the expected outcomes of the available activities, selecting and adapting an activity to the current context, understanding the degree to which the sequence advances towards creating an implementation, and learning new knowledge available during the process. Also, when addressing collaboration between humans and LLMs to tackle unexpected challenges, such as handling zero-day attacks, the process necessitates reasoning, understanding of prior instructions, and intuitive decision-making within the context of new parameters and constraints. To automate this process, exploring reasoning techniques, including deductive reasoning, inductive reasoning, analogical reasoning, common sense reasoning, tree-of-thoughts, multiple chains of thought, causal reasoning, heuristic reasoning, and symbolic reasoning, is required. Among these, the primary human thought process often involves mapping the current problem to a previously encountered one or identifying similarities with analogous problems, like in analogical reasoning. Consequently, an effective approach to problem modeling could involve neuro-symbolic representations that allow LLMs to dynamically learn and adapt in real-time. Techniques such as grokking [164], which enable models to discover relationships and patterns through iterative refinement, and masked LLMs are promising methods to achieve this goal. These approaches empower the model to derive connections on the fly, effectively merging learned representations with reasoning capabilities."
    },
    {
      "title": "V Conclusions",
      "text": "Recent advances in Large Language Models (LLMs) offer the opportunity to extend automated implementation generation techniques beyond the current methods that require algorithmic specifications as input and can use only statically domain knowledge. LLMs can process multi-modal descriptions, including ideas communicated in natural language and using images and with certain degrees of specification completeness, unknowns, and ambiguity. LLMs learn a broad range of associations and for diverse contexts. These new capabilities might offer intriguing paths beyond traditional implementation generation, such as support problem framing and exploration of possible solution approaches, improved implementation assessment across abstraction levels by comprehensive comparison to similar, externally available implementations, collective feedback and preferences, and enhanced elaboration by incorporating continuously updated domain knowledge. These features are critical in solving open-ended problem, currently hard to address with existing methods. Summarizing the state-of-the-art on LLMs and their related improvements is a first step towards devising novel LLM-based methods for implementation generation. This report offers a comprehensive overview of existing LLM techniques and studied the degree to which they can model the activities needed for implementation generation for open-ended problem solving. The overview presents LLM enhancements, like prompting, Reinforcement Learning (RL) and Retrieval-Augmented Generation (RAG). Then the report discusses the possibility of using LLMs to realize problem solving activities that are not available in traditional automated implementation generation methods. New research requirements are also presented, e.g., support for problem framing, creating an implementation approach, effective elaboration control, robust qualitative and quantitative assessment across abstraction levels, knowledge memorizing during learning, and managing the problem solving process."
    },
    {
      "title": "References",
      "text": "* [1]A. Doboli and R. Vemuri (2003) Behavioral modeling for high-level synthesis of analog and mixed-signal systems from vhdl-ams. IEEE Transactions on CADICS22 (11). Cited by: SSI. * [2]A. Doboli and A. Umbarkar (2014) The role of precedents in increasing creativity during iterative design of electronic embedded systems. Design Studies35 (3), pp. 298-326. Cited by: SSI. * [3]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [4]A. Doboli and R. Vemuri (2003) Behavioral modeling for high-level synthesis of analog and mixed-signal systems from vhdl-ams. IEEE Transactions on CADICS22 (11). Cited by: SSI. * [5]A. Doboli, A. Umbarkar, S. Doboli, and J. Betz (2015) Modeling semantic knowledge structures for creative problem solving: studies on expressing concepts, categories, associations, goals and context. Knowledge-based Systems78, pp. 34-50. Cited by: SSI. * [6]A. Doboli and R. Vemuri (2003) Behavioral modeling for high-level synthesis of analog and mixed-signal systems from vhdl-ams. IEEE Transactions on CADICS22 (11). Cited by: SSI. * [7]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [8]A. Doboli and A. Umbarkar (2014) The role of precedents in increasing creativity during iterative design of electronic embedded systems. Design Studies35 (3), pp. 298-326. Cited by: SSI. * [9]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [10]A. Doboli and R. Vemuri (2003) Exploration-based high-level synthesis of linear analog systems operating at low/medium frequencies. IEEE Transactions on CADICS22 (22). Cited by: SSI. * [11]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [12]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [13]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [14]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [15]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [16]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [17]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [18]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [19]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [20]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [21]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [22]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [23]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [24]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [25]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [26]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [27]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [28]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [29]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [30]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [31]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [32]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [33]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [34]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [35]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2006) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [36]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [37]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [38]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [39]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2010) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [40]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [41]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. 238-271. Cited by: SSI. * [42]A. Doboli and R. Vemuri (2003) Behavioral modeling for high-level synthesis of analog and mixed-signal systems from vhdl-ams. IEEE Transactions on CADICS22 (11). Cited by: SSI. * [43]A. Doboli, N. Dhanwada, A. Nunez-Aldana, and R. Vemuri (2004) A library-based approach to analog synthesis from vhdl-ams specifications. ACM Transactions on Design Automation9 (2), pp. * [40] M. Post and D. Vilar, \"Fast lexically constrained decoding with dynamic beam allocation for neural machine translation,\" _arXiv preprint arXiv:1804.06609_, 2018. * [41] N. Houlsby, A. Giurigu, S. Jastrzebski, B. Morrone, Q. De Larousilhe, A. Gesmundo, M. Attariyan, and S. Gelly, \"Parameter-efficient transfer learning for nlp,\" in _International conference on machine learning_. PMLR, 2019, pp. 2790-2799. * [42] R. Geithos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann, \"Shortcut learning in deep neural networks,\" _Nature Machine Intelligence_, vol. 2, no. 11, pp. 665-673, 2020. * [43] Y. Hu, Q. Chen, J. Du, X. Peng, V. K. Keloth, X. Zuo, Y. Zhou, Z. Li, X. Jiang, Z. Lu _et al._, \"Improving large language models for clinical named entity recognition via prompt engineering,\" _Journal of the American Medical Informatics Association_, p. ocad259, 2024. * [44] T. Boyle, \"Medical transcriptions,\" 2018, accessed: 2024-12-26. [Online]. Available: [https://www.kaggle.com/datasets/tboyle10/medicatranscriptions](https://www.kaggle.com/datasets/tboyle10/medicatranscriptions) * [45] \"Vaccine adverse event reporting system (vaers),\" [https://users.hhs.gov/data/datasets.html](https://users.hhs.gov/data/datasets.html), Centers for Disease Control and Prevention (CDC) and U.S. Food and Drug Administration (FDA), 2024, accessed: 2024-12-26. * [46] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou _et al._, \"Chain-of-thought prompting elicits reasoning in large language models,\" _Advances in neural information processing systems_, vol. 35, pp. 24 824-24 837, 2022. * [47] Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot, \"Complexity-based prompting for multi-step reasoning,\" in _The Eleventh International Conference on Learning Representations_, 2022. * [48] Y. Zhou, X. Geng, T. Shen, C. Tao, G. Long, J.-G. Lou, and J. Shen, \"Thread of thought unraveling chaotic contexts,\" _arXiv preprint arXiv:2311.08734_, 2023. * [49] X. Li, R. Zhao, Y. K. Chia, B. Ding, S. Joty, S. Poria, and L. Bing, \"Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources,\" _arXiv preprint arXiv:2305.13296_, 2023. * [50] C. Li, J. Liang, A. Zeng, X. Chen, K. Hausman, D. Sadigh, S. Levine, L. Fei-Fei, F. Xia, and B. Ichter, \"Chain of code: Reasoning with a language model-augmented code emulator,\" _arXiv preprint arXiv:2312.04474_, 2023. * [51] X. Zhao, M. Li, W. Lu, C. Weber, J. H. Lee, K. Chu, and S. Wermter, \"Enhancing zero-shot chain-of-thought reasoning in large language models through logic,\" _arXiv preprint arXiv:2309.13339_, 2023. * [52] S. Bao, T. Li, and B. Cao, \"Chain-of-event prompting for multi-document summarization by large language models,\" _International Journal of Web Information Systems_, no. ahead-of-print, 2024. * [53] Z. Wang, H. Zhang, C.-L. Li, J. M. Eisenschlos, V. Perot, Z. Wang, L. Miculicich, Y. Fujij, J. Shang, C.-Y. Lee _et al._, \"Chain-of-table: Evolving tables in the reasoning chain for table understanding,\" _arXiv preprint arXiv:2401.04398_, 2024. * [54] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, \"Self-consistency improves chain of thought reasoning in language models,\" _arXiv preprint arXiv:2203.11171_, 2022. * [55] Y. K. Chia, G. Chen, L. A. Tuan, S. Poria, and L. Bing, \"Contrastive chain-of-thought prompting,\" _arXiv preprint arXiv:2311.09277_, 2023. * [56] X. Liu, T. Pang, and C. Fan, \"Federated prompting and chain-of-thought reasoning for improving lms answering,\" in _International Conference on Knowledge Science, Engineering and Management_. Springer, 2023, pp. 3-11. * [57] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, \"Tree of thoughts: Deliberate problem solving with large language models,\" _Advances in Neural Information Processing Systems_, vol. 36, 2024. * [58] J. Jung, L. Qin, S. Welleck, F. Brahman, C. Bhagavatula, R. L. Bras, and Y. Choi, \"Maietic prompting: Logically consistent reasoning with recursive explanations,\" _arXiv preprint arXiv:2205.11822_, 2022. * [59] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim, \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models,\" _arXiv preprint arXiv:2305.04091_, 2023. * [60] W. Chen, X. Ma, X. Wang, and W. W. Cohen, \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks,\" _arXiv preprint arXiv:2211.12588_, 2022. * [61] H. Hu, H. Lu, H. Zhang, Y.-Z. Song, W. Lam, and Y. Zhang, \"Chain-of-symbol prompting elicits planning in large language models,\" _arXiv preprint arXiv:2305.10276_, 2023. * [62] J. Li, G. Li, Y. Li, and Z. Jin, \"Structured chain-of-thought prompting for code generation,\" _ACM Transactions on Software Engineering and Methodology_, 2023. * [63] H. Fei, B. Li, Q. Liu, L. Bing, F. Li, and T.-S. Chua, \"Reasoning implicit sentiment with chain-of-thought prompting,\" _arXiv preprint arXiv:2305.11255_, 2023. * [64] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal _et al._, \"Towards expert-level medical question answering with large language models,\" _arXiv preprint arXiv:2305.09617_, 2023. * [65] Z. Zhang, A. Zhang, M. Li, and A. Smola, \"Automatic chain of thought prompting in large language models,\" _arXiv preprint arXiv:2210.03493_, 2022. * [66] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, \"React: Synergizing reasoning and acting in language models,\" _arXiv preprint arXiv:2210.03629_, 2022. * [67] S. Diao, P. Wang, Y. Lin, R. Pan, X. Liu, and T. Zhang, \"Active prompting with chain-of-thought for large language models,\" _arXiv preprint arXiv:2302.12246_, 2023. * [68] S. Imani, L. Du, and H. Shrivastava, \"Mathpromter: Mathematical reasoning using large language models,\" _arXiv preprint arXiv:2303.05398_, 2023. * [69] M. Yasunaga, X. Chen, Y. Li, P. Pasupat, J. Leskovec, P. Liang, E. H. Chi, and D. Zhou, \"Large language models as analogical reasoners,\" _arXiv preprint arXiv:2310.01714_, 2023. * [70] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen, \"Synthetic prompting: Generating chain-of-thought demonstrations for large language models,\" in _International Conference on Machine Learning_. PMLR, 2023, pp. 30 706-30 775. * [71] J. Weston and S. Sukhbaatar, \"System 2 attention (is something you might need too),\" _arXiv preprint arXiv:2311.11829_, 2023. * [72] Y. Wang and Y. Zhao, \"Metacognitive prompting improves understanding in large language models,\" _arXiv preprint arXiv:2308.05342_, 2023. * [73] D. Zhou, N. Scharli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le _et al._, \"Least-to-most prompting enables complex reasoning in large language models,\" _arXiv preprint arXiv:2205.10625_, 2022. * [74] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson, P. Clark, and A. Sabharwal, \"Decomposed prompting: A modular approach for solving complex tasks,\" _arXiv preprint arXiv:2210.02406_, 2022. * [75] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, \"Pal: Program-aided language models,\" in _International Conference on Machine Learning_. PMLR, 2023, pp. 10 764-10 799. * [76] Z. Cheng, T. Xie, P. Shi, C. Li, R. Nadkarni, Y. Hu, C. Xiong, D. Radey, M. Ostendorf, L. Zettlemeyer _et al._, \"Binding language models in symbolic languages,\" _arXiv preprint arXiv:2210.02875_, 2022. * [77] Y. Ye, B. Hui, M. Yang, B. Li, F. Huang, and Y. Li, \"Large language models are versatile decompopers: Decompose evidence and questions for table-based reasoning,\" _arXiv preprint arXiv:2201.13808_, 2023. * [78] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W.-t. Yih, T. Rocktaschel _et al._, \"Retrieval-augmented generation for knowledge-intensive nlp tasks,\" _Advances in Neural Information Processing Systems_, vol. 33, pp. 9459-9474, 2020. * [79] P. Bechard and O. M. Ayala, \"Reducing hallucination in structured outputs via retrieval-augmented generation,\" _arXiv preprint arXiv:2404.08189_, 2024. * [80] P. Dixit and T. Oates, \"Sbi-rag: Enhancing math word problem solving for students through schema-based instruction and retrieval-augmented generation,\" _arXiv preprint arXiv:2410.13293_, 2024. * [81] N. Matsumoto, J. Moran, H. Choi, M. E. Hernandez, M. Venkatesan, P. Wang, and J. H. Moore, \"Kragen: a knowledge graph-enhanced raz framework for biomedical problem solving using large language models,\" _Bioinformatics_, vol. 40, no. 6, 2024. * [82] X. Liu, R. Wang, Y. Song, and L. Kong, \"Gram: Generative retrieval augmented matching of data schemas in the context of data security,\" in _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, 2024, pp. 5476-5486. * [83] S * [84] Z. Yao, W. Qi, L. Pan, S. Cao, L. Hu, W. Liu, L. Hou, and J. Li, \"Sekar: Self-aware knowledge retrieval for adaptive retrieval augmented generation,\" _arXiv preprint arXiv:2406.19215_, 2024. * [85] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, \"Self-rag: Self-reflective retrieval augmented generation,\" in _NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_, 2023. * [86] X. Li, W. Xu, R. Zhao, F. Jiao, S. Joty, and L. Bing, \"Can we further elicit reasoning in llms? critic-guided planning with retrieval-augmentation for solving challenging tasks,\" _arXiv preprint arXiv:2410.01428_, 2024. * [87] R. Xu, H. Liu, S. Nag, Z. Dai, Y. Xie, X. Tang, C. Luo, Y. Li, J. C. Ho, C. Yang _et al._, \"Simrag: Self-improving retrieval-augmented generation for adapting large language models to specialized domains,\" _arXiv preprint arXiv:2410.17952_, 2024. * [88] M. Hu, L. Zong, H. Wang, J. Zhou, J. Li, Y. Gao, K.-F. Wong, Y. Li, and I. King, \"SeRTS: Self-rewarding tree search for biomedical retrieval-augmented generation,\" in _Findings of the Association for Computational Linguistics: EMNLP 2024_, Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, Eds. Miami, Florida, USA: Association for Computational Linguistics, Nov. 2024, pp. 1321-1335. [Online]. Available: [https://aclanthology.org/2024.findings-emnlp.71](https://aclanthology.org/2024.findings-emnlp.71) * [89] Z. Wang, Z. Wang, L. Le, H. S. Zheng, S. Mishra, V. Perot, Y. Zhang, A. Mattapalli, A. Taly, J. Shang _et al._, \"Speculative rag: Enhancing retrieval augmented generation through drafting,\" _arXiv preprint arXiv:2407.08223_, 2024. * [90] B. J. Gutierrez, Y. Shu, Y. Gu, M. Yasunaga, and Y. Su, \"Hipporag: Neurophysiological inspired long-term memory for large language models,\" _arXiv preprint arXiv:2405.14831_, 2024. * [91] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, and F. Wei, \"Augmenting language models with long-term memory,\" _Advances in Neural Information Processing Systems_, vol. 36, 2024. * [92] A. Aadhithya _et al._, \"Enhancing long-term memory using hierarchical aggregate tree for retrieval augmented generation,\" _arXiv e-prints_, pp. arXiv-2406, 2024. * [93] H. Qian, P. Zhang, Z. Liu, K. Mao, and Z. Dou, \"Memorag: Moving towards next-end rag via memory-inspired knowledge discovery,\" _arXiv preprint arXiv:2409.05591_, 2024. * [94] Y. Bai, Y. Miao, L. Chen, D. Wang, D. Li, Y. Ren, H. Xie, C. Yang, and X. Cai, \"Pistis-gra: Enhancing retrieval-augmented generation with human feedback,\" _arXiv preprint arXiv:2407.00072_, 2024. * [95] C. Gan, D. Yang, B. Hu, H. Zhang, S. Li, Z. Liu, Y. Shen, L. Ju, Z. Zhang, J. Gu _et al._, \"Similarity is not all you need: Endowing retrieval augmented generation with multi layered thoughts,\" _arXiv preprint arXiv:2405.19893_, 2024. * [96] J. Jiang, J. Chen, J. Li, R. Ren, S. Wang, W. X. Zhao, Y. Song, and T. Zhang, \"Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement,\" _arXiv preprint arXiv:2412.12881_, 2024. * [97] Y. Tang and Y. Yang, \"Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries,\" _arXiv preprint arXiv:2401.15391_, 2024. * [98] B. Liu, C. Lyu, Z. Min, Z. Wang, J. Su, and L. Wang, \"Retrieval-augmented multi-modal chain-of-thoughs reasoning for large language models,\" _arXiv preprint arXiv:2312.01714_, 2023. * [99] W. Zhao, J. T. Chiu, C. Cardie, and A. M. Rush, \"Hop, union, generate: Explainable multi-hop reasoning without rationale supervision,\" _arXiv preprint arXiv:2305.14237_, 2023. * [100] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola, \"Multimodal chain-of-thought reasoning in language models,\" _arXiv preprint arXiv:2302.00923_, 2023. * [101] S. Vatsal, A. Singh, and S. Tafreshi, \"Can gpt improve the state of prior authorization via guideline based automated question answering?\" in _AI for Health Equity and Fairness: Leveraging AI to Address Social Determinants of Health_. Springer, 2024, pp. 147-158. * [102] S. Vatsal and A. Singh, \"Can gpt redefine medical understanding? evaluating gpt on biomedical machine reading comprehension,\" _arXiv preprint arXiv:2405.18682_, 2024. * [103] S. Dhuliawala, M. Komelli, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and J. Weston, \"Chain-of-verification reduces hallucination in large language models,\" _arXiv preprint arXiv:2309.11495_, 2023. * [104] R. Zhao, X. Li, S. Joty, C. Qin, and L. Bing, \"Verify-and-edit: A knowledge-enhanced chain-of-thought framework,\" _arXiv preprint arXiv:2305.03268_, 2023. * [105] Y. Zhai, H. Bai, Z. Lin, J. Pan, S. Tong, Y. Zhou, A. Suhr, S. Xie, Y. LeCun, Y. Ma _et al._, \"Fine-tuning large vision-language models as decision-making agents via reinforcement learning,\" _arXiv preprint arXiv:2405.10292_, 2024. * [106] R. S. Sutton and A. G. Barto, _Reinforcement learning: An introduction_. MIT press, 2018. * [107] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \"Proximal policy optimization algorithms,\" _arXiv preprint arXiv:1707.06347_, 2017. * [108] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray _et al._, \"Training language models to follow instructions with human feedback,\" _Advances in neural information processing systems_, vol. 35, pp. 27 730-27 744, 2022. * [109] W. Shen, R. Zheng, W. Zhan, J. Zhao, S. Dou, T. Gui, Q. Zhang, and X. Huang, \"Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback,\" _arXiv preprint arXiv:2310.05199_, 2023. * [110] B. Wang, R. Zheng, L. Chen, Y. Liu, S. Dou, C. Huang, W. Shen, S. Jin, E. Zhou, C. Shi _et al._, \"Secrets of rhlr in large language models part in: Reward modeling,\" _arXiv preprint arXiv:2401.06080_, 2024. * [111] A. Havrilla, M. Zhuravinskiy, D. Phung, A. Tiwari, J. Tow, S. Biderman, Q. Anthony, and L. Castricato, \"trlx: A framework for large scale reinforcement learning from human feedback,\" in _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, 2023, pp. 8578-8595. * [112] G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun, \"Ultradedback: Boosting language models with high-quality feedback,\" 2023. * [113] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano, \"Learning to summarize with human feedback,\" _Advances in Neural Information Processing Systems_, vol. 33, pp. 3008-3021, 2020. * [114] R. Munos, M. Valko, D. Calandriello, M. G. Azar, M. Rowland, Z. D. Guo, Y. Tang, M. Geist, T. Mesnard, A. Michi _et al._, \"Nash learning from human feedback,\" _arXiv preprint arXiv:2312.00806_, 2023. * [115] C. Y. Liu, L. Zeng, J. Liu, R. Yan, J. He, C. Wang, S. Yan, Y. Liu, and Y. Zhou, \"Skywork-reward: Bag of tricks for reward modeling in llms,\" _arXiv preprint arXiv:2410.18451_, 2024. * [116] H. Ivison, Y. Wang, V. Pyrakin, N. Lambert, M. Peters, P. Dasigi, J. Jang, D. Wadden, N. A. Smith, I. Beltagy _et al._, \"Camels in a changing climate: Enhancing lm adaptation with tulu 2,\" _arXiv preprint arXiv:2311.10702_, 2023. * [117] L. Li, Y. Chai, S. Wang, Y. Sun, H. Tian, N. Zhang, and H. Wu, \"Tool-augmented reward modeling,\" _arXiv preprint arXiv:2310.01045_, 2023. * [118] A. Scheid, E. Boursier, A. Durmus, M. I. Jordan, P. Menard, E. Moulines, and M. Valko, \"Optimal design for reward modeling in rhlr,\" _arXiv preprint arXiv:2410.17055_, 2024. * [119] L. Gao, J. Schulman, and J. Hilton, \"Scaling laws for reward model overoptimization,\" in _International Conference on Machine Learning_. PMLR, 2023, pp. 10 835-10 866. * [120] T. Kaufmann, P. Weng, V. Bengos, and E. Hullermeier, \"A survey of reinforcement learning from human feedback,\" _arXiv preprint arXiv:2312.14925_, 2023. * [121] H. Lee, S. Phatale, H. Mansoor, K. R. Lu, T. Mesnard, J. Ferret, C. Bishop, E. Hall, V. Carburne, and A. Rastogi, \"Half: Scaling reinforcement learning from human feedback with ai feedback,\" 2023. * [122] A. Li, Q. Xiao, P. Cao, J. Tang, Y. Yuan, Z. Zhao, X. Chen, L. Zhang, X. Li, K. Yang _et al._, \"Htrial: Improvements in helpfulness and harmlessness in open-domain reinforcement learning from ai feedback,\" _arXiv preprint arXiv:2403.08309_, 2024. * [123] Z. Xu, F. Jiang, L. Niu, Y. Deng, R. Poovendran, Y. Choi, and B. Y. Lin, \"Magpie: Alignment data synthesis from scratch by prompting aligned lms with nothing,\" _arXiv preprint arXiv:2406.08464_, 2024. * [124] Z. Wang, A. Bukharin, O. Delalleau, D. Egert, G. Shen, J. Zeng, O. Kuchaiev, and Y. Dong, \"Helpsteer2-preference: Complementing ratings with preferences,\" _arXiv preprint arXiv:2410.01257_, 2024. * [125] Y. * [127] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y. Zhu, L. Fan, and A. Anandkumar, \"Eureka: Human-level reward design via coding large language models,\" _arXiv preprint arXiv:2310.12931_, 2023. * [128] J. Song, Z. Zhou, J. Liu, C. Fang, Z. Shu, and L. Ma, \"Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics,\" _arXiv preprint arXiv:2309.06687_, 2023. * [129] W. Yuan, R. Y. Pang, K. Cho, S. Sukhbaatar, J. Xu, and J. Weston, \"Self-rewarding language models,\" _arXiv preprint arXiv:2401.10020_, 2024. * [130] H. Wang, T. Zariphopoulou, and X. Zhou, \"Exploration versus exploitation in reinforcement learning: A stochastic control approach,\" _arXiv preprint arXiv:1812.01552_, 2018. * [131] C. Dam, Y. Mansour, M. Mohri, A. Sekhari, and K. Sridharan, \"Guarantees for epsilon-greedy reinforcement learning with function approximation,\" in _International conference on machine learning_. PMLR, 2022, pp. 4666-4689. * [132] V. Mnih, \"Asynchronous methods for deep reinforcement learning,\" _arXiv preprint arXiv:1602.01783_, 2016. * [133] M. Tokic, \"Adaptive \\(\\varepsilon\\)-greedy exploration in reinforcement learning based on value differences,\" in _Annual conference on artificial intelligence_. Springer, 2010, pp. 203-210. * [134] N. Cesa-Bianchi, C. Gentile, G. Lugosi, and G. Neu, \"Boltzmann exploration done right,\" _Advances in neural information processing systems_, vol. 30, 2017. * [135] R. Ma, J. Luijkx, Z. Ajanovic, and J. Kober, \"ExploIfm: Guiding exploration in reinforcement learning with large language models,\" _arXiv preprint arXiv:2403.09583_, 2024. * [136] Q. Zhao, H. Fu, C. Sun, and G. Konidaris, \"Epo: Hierarchical llm agents with environment preference optimization,\" _arXiv preprint arXiv:2408.16090_, 2024. * [137] H.-T. Nguyen and K. Satoh, \"Balancing exploration and exploitation in llm using soft lrr for enhanced negation understanding,\" _arXiv preprint arXiv:2403.01185_, 2024. * [138] F. Yang, P. Zhao, Z. Wang, L. Wang, J. Zhang, M. Garg, Q. Lin, S. Rajmohan, and D. Zhang, \"Empower large language model to perform better on industrial domain-specific question answering,\" _arXiv preprint arXiv:2305.11541_, 2023. * [139] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, \"Direct preference optimization: Your language model is secretly a reward model,\" _Advances in Neural Information Processing Systems_, vol. 36, 2024. * [140] A. Pal, D. Karkhanis, S. Dooley, M. Roberts, S. Naidu, and C. White, \"Nanap: Fixing failure modes of preference optimisation with dppositive,\" _arXiv preprint arXiv:2402.13228_, 2024. * [141] J. Wu, Y. Xie, Z. Yang, J. Wu, J. Gao, B. Ding, X. Wang, and X. He, _arXiv preprint arXiv:2407.08639_, 2024. * [142] D. Kim, Y. Kim, W. Song, H. Kim, Y. Kim, S. Kim, and C. Park, \"sdpo: Don't use your data all at once,\" _arXiv preprint arXiv:2403.19270_, 2024. * [143] Z. Yang, F. Wan, L. Zhong, T. Shi, and X. Quan, \"Weighted-reward preference optimization for implicit model fusion,\" _arXiv preprint arXiv:2412.03187_, 2024. * [144] S. R. Chowdhury, A. Kini, and N. Natarajan, \"Provably robust dp: Aligning language models with noisy feedback,\" _arXiv preprint arXiv:2403.00409_, 2024. * [145] W. Xiao, Z. Wang, L. Gan, S. Zhao, W. He, L. A. Tuan, L. Chen, H. Jiang, Z. Zhao, and F. Wu, \"A comprehensive survey of datasets, theories, variants, and applications in direct preference optimization,\" _arXiv preprint arXiv:2404.15595_, 2024. * [146] M. G. Azar, Z. D. Guo, B. Piot, R. Munos, M. Rowland, M. Valko, and D. Calandriello, \"A general theoretical paradigm to understand learning from human preferences,\" in _International Conference on Artificial Intelligence and Statistics_. PMLR, 2024, pp. 4447-4455. * [147] H. Sun, Y. Shen, and J.-F. Ton, \"Rethinking bradley-terry models in preference-based reward modeling: Foundations, theory, and alternatives,\" _arXiv preprint arXiv:2411.04991_, 2024. * [148] Y. Wu, Z. Sun, H. Yuan, K. Ji, Y. Yang, and Q. Gu, \"Self-play preference optimization for language model alignment,\" _arXiv preprint arXiv:2405.00675_, 2024. * [149] C. Wang, Z. Zhao, C. Zhu, K. A. Sankararaman, M. Valko, X. Cao, Z. Chen, M. Khabsa, Y. Chen, H. Ma _et al._, \"Preference optimization with multi-sample comparisons,\" _arXiv preprint arXiv:2410.12138_, 2024. * [150] A. Fisch, J. Eisenstein, V. Zayats, A. Agarwal, A. Beirami, C. Nagpal, P. Shaw, and J. Berant, \"Robust preference optimization through reward model distillation,\" _arXiv preprint arXiv:2405.19316_, 2024. * [151] Y. Dong, K. Luo, X. Jiang, Z. Jin, and G. Li, \"Pace: Improving prompt with actor-critic editing for large language model,\" _arXiv preprint arXiv:2308.10088_, 2023. * [152] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving, \"Fine-tuning language models from human preferences,\" _arXiv preprint arXiv:1909.08593_, 2019. * [153] R. Meier and A. Mujika, \"Open-ended reinforcement learning with neural reward functions,\" _Advances in Neural Information Processing Systems_, vol. 35, pp. 2465-2479, 2022. * [154] W. Yao, H. Mi, and D. Yu, \"Hdflow: Enhancing llm complex problem-solving with hybrid thinking and dynamic workflows,\" _arXiv preprint arXiv:2409.17433_, 2024. * [155] G. Liu, K. Ji, R. Zheng, Z. Wu, C. Dun, Q. Gu, and L. Yan, \"Enhancing multi-step reasoning abilities of language models through direct q-function optimization,\" _arXiv preprint arXiv:2410.09302_, 2024. * [156] P. Aryan, \"Llms as debate partners: Utilizing genetic algorithms and adversarial search for adaptive arguments,\" _arXiv preprint arXiv:2412.06229_, 2024. * [157] X. Wu, S.-h. Wu, J. Wu, L. Feng, and K. C. Tan, \"Evolutionary computation in the era of large language model: Survey and roadmap,\" _arXiv preprint arXiv:2401.10034_, 2024. * [158] H. Yin, A. V. Kononova, T. Back, and N. van Stein, \"Controlling the mutation in large language models for the efficient evolution of algorithms,\" _arXiv preprint arXiv:2412.03250_, 2024. * [159] Q. Guo, R. Wang, J. Guo, B. Li, K. Song, X. Tan, G. Liu, J. Bian, and Y. Yang, \"Connecting large language models with evolutionary algorithms yeelds powerful prompt optimizers,\" _arXiv preprint arXiv:2309.08532_, 2023. * [160] S. Brahmachary, S. M. Joshi, A. Panda, K. Koneripalli, A. K. Sagorta, H. Patel, A. Sharma, A. D. Jagtap, and K. Kalyanaraman, \"Large language model-based evolutionary optimizer: Reasoning with elitism,\" _arXiv preprint arXiv:2403.02054_, 2024. * [161] W. Chao, J. Zhao, L. Jiao, L. Li, F. Liu, and S. Yang, \"A match made in consistency heaven: when large language models meet evolutionary algorithms,\" _arXiv preprint arXiv:2401.10510_, 2024. * [162] H. Hao, X. Zhang, and A. Zhou, \"Large language models as surrogate models in evolutionary algorithms: A preliminary study,\" _arXiv preprint arXiv:2406.10675_, 2024. * [163] X. Huang, W. Liu, X. Chen, X. Wang, D. Lian, Y. Wang, R. Tang, and E. Chen, \"Wese: Weak exploration to strong exploitation for llm agents,\" _arXiv preprint arXiv:2404.07456_, 2024. * [164] Z. Liu, O. Kitouni, N. S. Nolte, E. Michaud, M. Tegmark, and M. Williams, \"Towards understanding grokking: An effective theory of representation learning,\" _Advances in Neural Information Processing Systems_, vol. 35, pp. 34 651-34 663, 2022. This figure \"fig1.png\" is available in \"png\" format from: [http://arxiv.org/ps/2501.00562v2](http://arxiv.org/ps/2501.00562v2)"
    }
  ]
}