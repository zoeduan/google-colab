{
  "title": "An Open-Source Data Contamination Report for Large Language Models",
  "authors": [
    "Yucheng Li",
    "Frank Guerin",
    "Chenghua Lin"
  ],
  "abstract": "\n Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models. It allows models to \"cheat\" via memorisation instead of displaying true capabilities. Therefore, contamination analysis has become an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness. This paper presents an extensive data contamination report for over 15 popular large language models across six popular multiple-choice QA benchmarks. We also introduce an open-source pipeline that enables the community to perform contamination analysis on customised data and models. Our experiments reveal varying contamination levels ranging from 1% to 45% across benchmarks, with the contamination degree increasing rapidly over time. Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics: while significant accuracy boosts of up to 14% and 7% are observed on contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is noted on contaminated MMLU. We also find larger models seem able to gain more advantages than smaller models on contaminated test sets. \n",
  "references": [
    {
      "id": null,
      "title": "An Open-Source Data Contamination Report for Large Language Models",
      "authors": [
        "Yucheng Li",
        "Frank Guerin",
        "Chenghua Lin"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Program synthesis with large language models",
      "authors": [
        "Jacob Austin",
        "Augustus Odena",
        "Maxwell Nye",
        "Maarten Bosma",
        "Henryk Michalewski",
        "David Dohan",
        "Ellen Jiang",
        "Carrie Cai",
        "Michael Terry",
        "Quoc Le"
      ],
      "year": "2021",
      "venue": "Program synthesis with large language models",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang",
        "Binyuan Hui",
        "Luo Ji",
        "Mei Li",
        "Junyang Lin",
        "Runji Lin",
        "Dayiheng Liu",
        "Gao Liu",
        "Chengqiang Lu",
        "Keming Lu",
        "Jianxin Ma",
        "Rui Men",
        "Xingzhang Ren",
        "Xuancheng Ren",
        "Chuanqi Tan",
        "Sinan Tan",
        "Jianhong Tu",
        "Peng Wang",
        "Shijie Wang",
        "Wei Wang",
        "Shengguang Wu",
        "Benfeng Xu",
        "Jin Xu",
        "An Yang",
        "Hao Yang",
        "Jian Yang",
        "Shusheng Yang",
        "Yang Yao",
        "Bowen Yu",
        "Hongyi Yuan",
        "Zheng Yuan",
        "Jianwei Zhang",
        "Xingxuan Zhang",
        "Yichang Zhang",
        "Zhenru Zhang",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "authors": [
        "Satanjeev Banerjee",
        "Alon Lavie"
      ],
      "year": "2005",
      "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Piqa: Reasoning about physical commonsense in natural language",
      "authors": [
        "Yonatan Bisk",
        "Rowan Zellers",
        "Le Ronan",
        "Jianfeng Bras",
        "Yejin Gao",
        "Choi"
      ],
      "year": "2019",
      "venue": "Piqa: Reasoning about physical commonsense in natural language",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Quantifying memorization across neural language models",
      "authors": [
        "Nicholas Carlini",
        "Daphne Ippolito",
        "Matthew Jagielski",
        "Katherine Lee",
        "Florian Tramer",
        "Chiyuan Zhang"
      ],
      "year": "2022",
      "venue": "Quantifying memorization across neural language models",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "",
      "authors": [
        "Mark Chen",
        "Jerry Tworek",
        "Heewoo Jun",
        "Qiming Yuan",
        "Henrique Ponde De Oliveira Pinto",
        "Jared Kaplan",
        "Harri Edwards",
        "Yuri Burda",
        "Nicholas Joseph",
        "Greg Brockman",
        "Alex Ray",
        "Raul Puri",
        "Gretchen Krueger",
        "Michael Petrov",
        "Heidy Khlaaf",
        "Girish Sastry",
        "Pamela Mishkin",
        "Brooke Chan",
        "Scott Gray",
        "Nick Ryder",
        "Mikhail Pavlov",
        "Alethea Power",
        "Lukasz Kaiser",
        "Mohammad Bavarian",
        "Clemens Winter",
        "Philippe Tillet",
        "Felipe Petroski Such",
        "Dave Cummings",
        "Matthias Plappert",
        "Fotios Chantzis",
        "Elizabeth Barnes",
        "Ariel Herbert-Voss",
        "William Hebgen Guss",
        "Alex Nichol",
        "Alex Paino",
        "Nikolas Tezak",
        "Jie Tang",
        "Igor Babuschkin",
        "Suchir Balaji",
        "Shantanu Jain",
        "William Saunders",
        "Christopher Hesse",
        "Andrew N Carr",
        "Jan Leike",
        "Josh Achiam",
        "Vedant Misra",
        "Evan Morikawa",
        "Alec Radford",
        "Matthew Knight",
        "Miles Brundage",
        "Mira Murati",
        "Katie Mayer",
        "Peter Welinder",
        "Bob Mcgrew",
        "Dario Amodei",
        "Sam Mccandlish"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Quac: Question answering in context",
      "authors": [
        "Eunsol Choi",
        "He He",
        "Mohit Iyyer",
        "Mark Yatskar",
        "Wentau Yih",
        "Yejin Choi",
        "Percy Liang",
        "Luke Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Quac: Question answering in context",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann",
        "Parker Schuh",
        "Kensen Shi",
        "Sasha Tsvyashchenko",
        "Joshua Maynez",
        "Abhishek Rao",
        "Parker Barnes",
        "Yi Tay",
        "Noam Shazeer",
        "Emily Vinodkumar Prabhakaran",
        "Nan Reif",
        "Ben Du",
        "Reiner Hutchinson",
        "James Pope",
        "Jacob Bradbury",
        "Michael Austin",
        "Guy Isard",
        "Pengcheng Gur-Ari",
        "Toju Yin",
        "Anselm Duke",
        "Sanjay Levskaya",
        "Sunipa Ghemawat",
        "Henryk Dev",
        "Xavier Michalewski",
        "Vedant Garcia",
        "Kevin Misra",
        "Liam Robinson",
        "Denny Fedus",
        "Daphne Zhou",
        "David Ippolito",
        "Hyeontaek Luan",
        "Barret Lim",
        "Alexander Zoph",
        "Ryan Spiridonov",
        "David Sepassi",
        "Shivani Dohan",
        "Mark Agrawal",
        "Omernick ; Oleksandr",
        "Katherine Polozov",
        "Zongwei Lee",
        "Xuezhi Zhou",
        "Brennan Wang",
        "Mark Saeta",
        "Orhan Diaz",
        "Michele Firat",
        "Jason Catasta",
        "Kathy Wei",
        "Douglas Meier-Hellstern",
        "Eck"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Boolq: Exploring the surprising difficulty of natural yes/no questions",
      "authors": [
        "Christopher Clark",
        "Kenton Lee",
        "Ming-Wei Chang",
        "Tom Kwiatkowski",
        "Michael Collins",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Boolq: Exploring the surprising difficulty of natural yes/no questions",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
      "authors": [
        "Peter Clark",
        "Isaac Cowhey",
        "Oren Etzioni",
        "Tushar Khot",
        "Ashish Sabharwal",
        "Carissa Schoenick",
        "Oyvind Tafjord"
      ],
      "year": "2018",
      "venue": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "",
      "authors": [
        "Karl Cobbe",
        "Vineet Kosaraju",
        "Mohammad Bavarian",
        "Mark Chen",
        "Heewoo Jun",
        "Lukasz Kaiser",
        "Matthias Plappert",
        "Jerry Tworek",
        "Jacob Hilton",
        "Reiichiro Nakano",
        "Christopher Hesse",
        "John Schulman"
      ],
      "year": "2021",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Documenting large webtext corpora: A case study on the colossal clean crawled corpus",
      "authors": [
        "Jesse Dodge",
        "Maarten Sap",
        "Ana Marasović",
        "William Agnew",
        "Gabriel Ilharco",
        "Dirk Groeneveld"
      ],
      "year": "2021",
      "venue": "Documenting large webtext corpora: A case study on the colossal clean crawled corpus",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding",
      "authors": [
        "Dan Hendrycks",
        "Collin Burns",
        "Steven Basart",
        "Andy Zou",
        "Mantas Mazeika"
      ],
      "year": "",
      "venue": "Proceedings of the International Conference on Learning Representations (ICLR)",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Dawn Song, and Jacob Steinhardt. 2021b. Measuring massive multitask language understanding",
      "authors": [
        "Dan Hendrycks",
        "Collin Burns",
        "Steven Basart",
        "Andy Zou",
        "Mantas Mazeika"
      ],
      "year": "",
      "venue": "Dawn Song, and Jacob Steinhardt. 2021b. Measuring massive multitask language understanding",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Maosong Sun, and Junxian He. 2023. Ceval: A multi-level multi-discipline chinese evaluation suite for foundation models",
      "authors": [
        "Yuzhen Huang",
        "Yuzhuo Bai",
        "Zhihao Zhu",
        "Junlei Zhang",
        "Jinghan Zhang",
        "Tangjun Su",
        "Junteng Liu",
        "Chuancheng Lv",
        "Yikai Zhang",
        "Jiayi Lei",
        "Yao Fu"
      ],
      "year": "",
      "venue": "Maosong Sun, and Junxian He. 2023. Ceval: A multi-level multi-discipline chinese evaluation suite for foundation models",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks",
      "authors": [
        "Alon Jacovi",
        "Avi Caciularu",
        "Omer Goldman",
        "Yoav Goldberg"
      ],
      "year": "2023",
      "venue": "Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "",
      "authors": [
        "Albert Q Jiang",
        "Alexandre Sablayrolles",
        "Arthur Mensch",
        "Chris Bamford",
        "Devendra Singh Chaplot",
        "Diego De Las Casas",
        "Florian Bressand",
        "Gianna Lengyel",
        "Guillaume Lample",
        "Lucile Saulnier",
        "Renard Lélio",
        "Marie-Anne Lavaud",
        "Pierre Lachaux",
        "Teven Stock",
        "Thibaut Le Scao",
        "Thomas Lavril",
        "Timothée Wang",
        "Lacroix"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "authors": [
        "Mandar Joshi",
        "Eunsol Choi",
        "Daniel S Weld",
        "Luke Zettlemoyer"
      ],
      "year": "2017",
      "venue": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Natural questions: a benchmark for question answering research",
      "authors": [
        "Tom Kwiatkowski",
        "Jennimaria Palomaki",
        "Olivia Redfield",
        "Michael Collins",
        "Ankur Parikh",
        "Chris Alberti",
        "Danielle Epstein",
        "Illia Polosukhin",
        "Jacob Devlin",
        "Kenton Lee"
      ],
      "year": "2019",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
      "authors": [
        "Todor Mihaylov",
        "Peter Clark",
        "Tushar Khot",
        "Ashish Sabharwal"
      ],
      "year": "2018",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Gpt-4 technical report. OpenCompass. 2023. Opencompass: A universal evaluation platform for foundation models",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report. OpenCompass. 2023. Opencompass: A universal evaluation platform for foundation models",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Know what you don't know: Unanswerable questions for squad",
      "authors": [
        "Pranav Rajpurkar",
        "Robin Jia",
        "Percy Liang"
      ],
      "year": "2018",
      "venue": "Know what you don't know: Unanswerable questions for squad",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "NLP evaluation in trouble: On the need to measure LLM data contamination for each benchmark",
      "authors": [
        "Oscar Sainz",
        "Jon Campos",
        "Iker García-Ferrero",
        "Julen Etxaniz"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Winogrande: An adversarial winograd schema challenge at scale. Communications of the",
      "authors": [
        "Keisuke Sakaguchi",
        "Le Ronan",
        "Chandra Bras",
        "Yejin Bhagavatula",
        "Choi"
      ],
      "year": "2021",
      "venue": "Winogrande: An adversarial winograd schema challenge at scale. Communications of the",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Socialiqa: Commonsense reasoning about social interactions",
      "authors": [
        "Maarten Sap",
        "Hannah Rashkin",
        "Derek Chen",
        "Ronan Lebras",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Socialiqa: Commonsense reasoning about social interactions",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
      "authors": [
        "Aarohi Srivastava"
      ],
      "year": "2023",
      "venue": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
      "authors": [
        "Alon Talmor",
        "Jonathan Herzig",
        "Nicholas Lourie",
        "Jonathan Berant"
      ],
      "year": "2018",
      "venue": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "year": "",
      "venue": "Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava"
      ],
      "year": "",
      "venue": "Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Y Vincent",
        "Kelvin Zhao",
        "Adams Wei Guu",
        "Brian Yu",
        "Nan Lester",
        "Andrew M Du",
        "Quoc V Dai",
        "Le"
      ],
      "year": "2021",
      "venue": "Finetuned language models are zero-shot learners",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "",
      "authors": [
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Ce Bian",
        "Chenxu Chao Yin",
        "Da Lv",
        "Dian Pan",
        "Dong Wang",
        "Fan Yan",
        "Fei Yang",
        "Feng Deng",
        "Feng Wang",
        "Guangwei Liu",
        "Guosheng Ai",
        "Haizhou Dong",
        "Hang Zhao",
        "Haoze Xu",
        "Hongda Sun",
        "Hui Zhang",
        "Jiaming Liu",
        "Jian Ji",
        "Juntao Xie",
        "Kun Dai",
        "Lei Fang",
        "Liang Su",
        "Lifeng Song",
        "Liyun Liu",
        "Luyao Ru",
        "Mang Ma",
        "Mickel Wang",
        "Mingan Liu",
        "Lin",
        "Peidong Nie",
        "Ruiyang Guo",
        "Tao Sun",
        "Tianpeng Zhang",
        "Tianyu Li",
        "Wei Li",
        "Weipeng Cheng",
        "Xiangrong Chen",
        "Xiaochuan Zeng",
        "Xiaoxi Wang",
        "Xin Chen",
        "Xin Men",
        "Xuehai Yu",
        "Yanjun Pan",
        "Yiding Shen",
        "Yiyu Wang",
        "Youxin Li",
        "Yuchen Jiang",
        "Yupeng Gao",
        "Zhang"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "A series of large language models trained from scratch by developers at 01-ai",
      "authors": [
        "Yi"
      ],
      "year": "2023",
      "venue": "A series of large language models trained from scratch by developers at 01-ai",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Hellaswag: Can a machine really finish your sentence? arXiv preprint",
      "authors": [
        "Rowan Zellers",
        "Ari Holtzman",
        "Yonatan Bisk",
        "Ali Farhadi",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Hellaswag: Can a machine really finish your sentence? arXiv preprint",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Agieval: A human-centric benchmark for evaluating foundation models",
      "authors": [
        "Wanjun Zhong",
        "Ruixiang Cui",
        "Yiduo Guo",
        "Yaobo Liang",
        "Shuai Lu",
        "Yanlin Wang",
        "Amin Saied",
        "Weizhu Chen",
        "Nan Duan"
      ],
      "year": "2023",
      "venue": "Agieval: A human-centric benchmark for evaluating foundation models",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Table 4: Llama models' performance comparison. Here Dirty i. denotes input-only contamination and Dirty ii. demotes input-and-label contamination. All denotes the performance on the entire test set",
      "authors": [],
      "year": "",
      "venue": "Model MMLU MMLU-Humanities MMLU-STEM MMLU-Social-Science MMLU-Other Clean Dirty i Dirty ii Clean Dirty i Dirty ii Clean Dirty i Dirty ii Clean Dirty i Dirty ii Clean Dirty i Dirty ii Llama 7B 34",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Table 5: Llama series models' performance across different categories of MMLU. Figure 7: An example of input-and-label (a) and input-only (b) contamination from MMLU",
      "authors": [],
      "year": "",
      "venue": "Table 5: Llama series models' performance across different categories of MMLU. Figure 7: An example of input-and-label (a) and input-only (b) contamination from MMLU",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "An Open-Source Data Contamination Report For Large Language Models",
      "text": "Yucheng Li1, Frank Guerin1, Chenghua Lin2 1 University of Surrey, UK 2 University of Manchester, UK {yucheng.li, f.guerin}@surrey.ac.uk chenhua.lin@manchester.ac.uk"
    },
    {
      "title": "Abstract",
      "text": "Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models. It allows models to \"cheat\" via memorisation instead of displaying true capabilities. Therefore, contamination analysis has become an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness. This paper presents an extensive data contamination report for over 15 popular large language models across six popular multiple-choice QA benchmarks. We also introduce an open-source pipeline that enables the community to perform contamination analysis on customised data and models. Our experiments reveal varying contamination levels ranging from 1% to 45% across benchmarks, with the contamination degree increasing rapidly over time. Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics: while significant accuracy boosts of up to 14% and 7% are observed on contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is noted on contaminated MMLU. We also find larger models seem able to gain more advantages than smaller models on contaminated test sets."
    },
    {
      "title": "1 Introduction",
      "text": "Recent years have seen remarkable progress in language models pre-trained on massive text corpora scraped from the web. However, many widely used evaluation benchmarks are also constructed from similar web sources, leading to a concerning issue of _data contamination_ where examples from test sets are unintentionally included in training data. Contamination enables models to \"cheat\" via memorisation of test data rather than displaying true generalisation Marie (2023), which creates an illusion of progress, distorts model comparisons, and undermines the utility of benchmarks Jacovi et al. (2023); Sainz et al. (2023). Contamination analysis therefore became a crucial part of reliable LLM evaluation to validate the results. However, existing contamination analysis is often conducted internally by LLM developers and often lacks transparency and completeness. For instance, OpenAI's contamination study for GPT-4 OpenAI, 2023) only covered the pre-training data and omitted later fine-tuning stages. Llama-2 Touvron et al. (2023) only reported contamination statistics for 2 of the 20+ benchmarks used in their evaluation. In addition, their implementation details of contamination identification remains unclear. Overall, existing internal contamination studies tend to lack sufficient transparency, with minimal sharing of comprehensive contamination measurements across all evaluation benchmarks, as well as training data details and code to reproduce the results. This prevents the wider research community from fully auditing the credibility of reported metrics and model capabilities. This paper presents an open contamination analysis for over 15 popular large language models on six common multiple-choice benchmarks, aiming to provide more comprehensive measurements and insights compared to limited existing studies. The analysis includes a range of foundation models such as LLaMA Touvron et al. (2023), Llama-2, Yi Yi (2023), Mistral Jiang et al. (2023), Baichuan Yang et al. (2023), and Qwen Bai et al. (2023) across multiple model sizes (7B, 13B, 30B, 34B, 65B, 70B parameters) as well as instruct-tuned models built on these foundations like Llama-2 Chat and Mistral-Instruct. Six widely used multi-choice benchmarks are assessed: Winogrande Sakaguchi et al. (2021), AI2_ARC Clark et al. (2018), CommonsenseQA Talmor et al. (2018), HellaSwag Zellers et al. (2019), MMLU Hendrycks et al. (2021), and C-Eval Huang et al. (2023). Our methodology proceeds in four steps:first, we verify whether test examples appear in Common Crawl1, a popular large corpus often used in language model pre-training. If a test example is found verbatim in Common Crawl, it was very likely included in the pre-training phrase of language models, making it a \"contaminated\" sample. Based on the presence of test samples, we then categorise benchmarks into the clean set and contaminated set. Finally, we compare model performance on these subsets to assess the impact of data contamination on evaluation results. At the end of the paper, we compare our analysis to Llama-2's original contamination results and discuss the effectiveness of existing contamination mitigation methods. Footnote 1: [https://commoncrawl.org/](https://commoncrawl.org/) Our analysis reveals the following key findings: 1) we detect varying levels of data contamination across benchmarks, with 1% to 45.8% of examples showing verbatim overlap with Common Crawl; 2) by comparing the contamination degree between Common Crawl Dec 2020 to Oct 2023, we find data contamination grows rapidly through time; 3) data contamination does not necessarily lead to increased model performance: we found significant accuracy boosts of 14% and 7% on C-Eval and Hellaswag, but very little increase on MMLU; 4) we also find a tendency that larger models seems to obtain more advantages than smaller models from data contamination, perhaps due to the more powerful memorisation capacities of larger models; 5) finally, we show our results align well with Llama's original contamination reports, demonstrating the effectiveness of our method."
    },
    {
      "title": "2 Data Contamination",
      "text": "**What is data contamination?** Data contamination refers to the phenomenon that examples from the test set are also found in the training data. This might lead to the evaluation failing to accurately reflect models' capabilities, as models can cheat by memorising instead of learning to generalise. There are two primary types of data contamination (Dodge et al., 2021): _input contamination_ refers to only the input appearing in the pretraining corpus, and _input-and-label contamination_ is when both inputs and their labels are present. The latter is generally more problematic, as models can directly memorise input-output pairs. But the first can still cause issues as models may gain an advantage even if only the input is learned (see SS6 for details), especially for assessing few-shot and zero-shot learning capabilities. **How common is data contamination?** Data contamination appears to be quite widespread across commonly used NLP benchmark datasets based on findings from recent studies. Dodge et al. (2021) revealed exact match contamination rates ranging from under 2% to over 50% on various GLUE benchmarks when compared to the C4 pretraining data. The GPT-3 study (Brown et al., 2020) found over 90% of examples in Quac, SQuADv2, and DROP were flagged as contaminated. FLAN (Wei et al., 2021) evaluations identified 7 out of 26 datasets exhibiting a serious contamination ratio of 50% and over. Llama-2 (Touvron et al., 2023) reported over 16% of MMLU examples are contaminated and about 11% are seriously contaminated (more than 80% token leakage). GPT-4 (OpenAI, 2023) use academic exams instead of NLP benchmarks for model evaluation. While 4 out of 34 exams are found have zero contamination (e.g., Leet-code and Bar Exam), 9 out of 34 showed over 20% of instances are marked as dirty examples. In summary, we found data contamination is becoming an increasingly prevalent issue for LLMs, which must be carefully measured and accounted for in order to accurately assess model performance. **How to identify data contamination?** Dodge et al. (2021) takes a straightforward approach to detect exact matches between test set examples and the pretraining data after normalising for capitalisation and punctuation. The _exact match here_ means the entire input of an evaluation text is found in the training data. The GPT-3 paper (Brown et al., 2020) uses n-gram overlap to identify contamination, treating any examples with 13-gram co-occurrence in both test sets and training data as dirty examples. PaLM (Chowdhery et al., 2022) considers a sample to be contaminated if 70% of its 8-grams can be found at least once in the training data. Llama-2 matches on verbalised and tokenized input to allow a token-level approach to identify contamination. It also involves a \"skipgram budget\" to allow slight variants in overlapping. Overall, existing approaches usually use substring matching between evaluation examples and training data to identify data contamination. However, if we have no access to the training data, which is often the case for most recent closed models, it is extremely difficult to reveal contamination by observing models themselves. In this paper, we utilise a pipeline consisting of a search engine and Common Crawl index for detecting potentially contaminated test samples, avoiding the need for full training data. This enables the community and third parties to conduct contamination analysis. **To what extent does data contamination affect model evaluation?** While contaminated data can potentially inflate scores, models do not necessarily perform worse on clean subsets or better on dirty subsets across all datasets. The degree of impact likely depends on factors like the dataset characteristics, model scale, and nature of the pre-training data. For instance, GPT-3 (Brown et al., 2020) showed a small 1-2% performance drop on clean subsets for PIQA and ReCoRD, comparing to a significant 6% drop on clean set of SQuAD as 94% of its test examples were contaminated. The LLaMA model (Touvron et al., 2023) did not show significant gaps between clean and dirty subset performance. On HellaSwag, LLaMA's 70B model showed a 15.3 point gap between clean (63.5) and dirty (78.8) subsets. Detecting and accounting for data contamination remains an active area of research, as there is no consensus yet on best methodologies and acceptable contamination levels."
    },
    {
      "title": "3 Benchmarks For Language Models",
      "text": "Clean and robust benchmarks are the key to guide further progress of various models in NLP. Popular benchmarks used to evaluate large language models include: **Comprehensive**: MMLU, Big Bench (Srivastava and et al., 2023), AGI Eval (Zhong et al., 2023), C-Eval **Commonsense reasoning**: PIQA (Bisk et al., 2019), SIQA (Sap et al., 2019), HellaSwag, WinoGrande, ARC, OpenBookQA (Mihaylov et al., 2018), CommonsenseQA **World knowledge**: NaturalQuestions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) **Reading comprehension**: SQuAD (Rajpurkar et al., 2018), QuAC (Choi et al., 2018), BoolQ (Clark et al., 2019) **Math**: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021) **Code**: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) As many of their construction rely heavily on online materials, they are highly prone to data contamination as their source spread on the internet. In this paper, we analyse six representative multi-choice QA benchmarks: MMLU, C-Eval, Winogrande, CommonsenseQA, ARC, and Hellaswag. These benchmarks have been selected due to their varied sources and potential susceptibility to data contamination. MMLU, ARC, and C-Eval, which are academic test-based benchmarks, were compiled from online.docx/.pdf files using techniques like OCR, typically assumed to be less affected by data contamination as such files are often not indexed by online crawlers. However, C-Eval stands out as it is a non-English (Chinese) benchmark, offering an opportunity to assess the impact of non-English benchmarks on language models. Winogrande, uniquely human-authored from scratch, allows examination of whether manually created benchmarks are less prone to data contamination. CommonsenseQA and Hellaswag, both internet-sourced, differ in their source popularity; while CommonsenseQA is built upon the less influential ConceptNet, Hellaswag is sourced from the more popular WikiHow. This selection of benchmarks provides a comprehensive overview of how different sourcing and construction methods might influence the presence and extent of data contamination in language model evaluations."
    },
    {
      "title": "4 Our Approach",
      "text": "The central goal of data contamination analysis is to categorise test samples as either clean or contaminated and then evaluate models separately on the clean and contaminated samples to assess the impact of contamination on the performance metrics. In this section, we describe our methodology to identify contaminated test samples. The basic idea of detecting contaminated examples in our method is to check whether test examples appear verbatim in Common Crawl. We base on Common Crawl because it is completely open-sourced and often comprises the majority of pre-training data for large language models, e.g., Common Crawl weights over 80% in GPT-3 and LLaMA training data (Brown et al., 2020; Touvron et al., 2023). We tailor our search window based on each model's training data collection period. For example, LLaMA models use Common Crawl dumps from 2017 to 2020, which makes our contamination search window 2017-2020. For models with unknown data ranges, we use a period from 2017 to the release date as the search window. For example, Mistral 7B was released in Oct 2023 without details on their training data range. So we use a search window of Jan 2017 to Oct 2023. Since Common Crawl produces 20TB of data monthly, exhaustively searching the full contents is infeasible. Therefore, we try to find overlap between benchmarks and training data in two steps: First, 1) we use the Bing Search API to check if verbatim test examples appear online, which likely indicates inclusion in Common Crawl. Second, 2) we specifically verify if pages containing verbatim test examples were indexed in Common Crawl, by only searching the URLs rather than full contents. This reduces the disk usage to a manageable 2.5TB for detecting training data overlap. The _freshness_ parameter in the Bing API is configured to align the search window with the expected training data range of each model. To construct the search queries, we verbalise examples accordingly and make sure the question and the correct answer are involved in the queries. For example: **Question**: The flaw in Anderson's ACT theory was that some considered it. **Choices**: A: 'Only applicable to a motor system', B: 'Untestable and thus, of uncertain scientific value', C: 'Lacking in definition for its elements', D: 'Overly complex in explaining the operation of cognition', **Answer**: B _Verbalised Query_: The flaw in Anderson's ACT theory was that some considered it untestable and thus, of uncertain scientific value. We verbalise this multi-choice question to a query by filling the correct answer to the blank. We do not include other options in the query, because, as discussed in section 2, the presence of other options does not matter. The question and answer are the key for identifying data contamination. If there is no blank in the question, we simply append the answer after the question to form the query. To identify overlapping between test samples and training data, existing methods often rely on exact string matches. For example, Brown et al. (2020) use N-gram overlapping ranging from 8-grams to maximum 13-grams for all evaluation tasks. GPT-4's criterion for contamination is substring matching with at least 50 characters (OpenAI, 2023). However, according on our manual analysis, we find the approach of exact string matches often lead to false negative in our pipeline. Touvron et al. (2023) propose a more fine-grained method that assesses contamination in token-level and involves a small \"skipgram budget\" to accommodate slight variations of sequences. However, their exact implementation details remain unclear. We instead simply compute METEOR Banerjee and Lavie (2005) score between matched pages and the queries to quantify the extent of overlap. We consider examples with a METEOR recall score over 0.75 as contaminated cases. This method tolerates minor inserted phrases and word form variations, which greatly mitigates false negative issue that strict string matching would miss. To avoid po Figure 1: The categorisation of contaminated test samples. tential false positives, we configure our method with two key settings: 1) an order penalty (gamma of 0.8) for METEOR ensures matches respect sequence; 2) matching is constrained to a window up to 2x the query length, preventing partial or out-of-context matches. We compare our approach against to Llama-2's in section 7.2. According to section 2, here we distinguish two types of data contamination: 1) _input contamination_ where only question is presented in the matched pages but not answer; 2) _input-and-label contamination_ where both question and answer occur in the matched pages. In the upcoming sections, these two types of data contamination are compared and analysed separately."
    },
    {
      "title": "5 Contamination Statistics For Multi-Choice Benchmarks",
      "text": "Our analysis reveals varying levels of data contamination across six multi-choice QA benchmarks, as shown in Table 1. According to the table, we have the following key findings. First, Academic test-based benchmarks like MMLU and C-Eval, despite being collected through methods like OCR, exhibit the highest levels of contamination (29.1% and 45.8%, respectively). This high rate is attributed to the widespread distribution and communication of academic test examples, making them more prone to sharing and discussion. In contrast, benchmarks manually created from scratch like Winogrande demonstrate minimal contamination (1.1%), as they avoided using internet resources in their benchmark construction. Third, we find significant differences among internet-sourced benchmarks. For example, CommonsenseQA has low contamination (1.6%) but HellaSwag has a rather significant higher overlap (12.4%). This variation might stem from different popularity of the sources: ConceptNet, as the source of CommonsenseQA is less popular than WikiHow, the source of HellaSwag. Finally, we find most contamination belongs to _input-and-label_ contamination, indicating models can often find the answer alongside with the question for contaminated test samples. We also illustrate how data contamination increase over time, as shown in Figure 2. In the figure, benchmarks such as CommonsenseQA and Winogrande maintain very low rates of contaminated data, with increases of just 0.3% and 0.2% over the past three years. However, benchmarks collected from academic tests like ARC, MMLU, and C-Eval have experienced substantial increase in contamination, with up to 21% of examples flaged as contaminated during the same period. This shows how test content in academic benchmarks can easily propagate across the internet, which can be a serious issue for academic test based language model benchmarks. We also observe a moderate 8.3% increase for Hellaswag, further demonstrating the increasing risk of data contamination for internet sourced benchmarks. In Figure 3, we illustrate where these Hellaswag contaminated test samples come from. We discover that data contamination manifests in a centralised fashion, which means contaminated test samples are not evenly distributed across domains. Instead, \\begin{table} \\begin{tabular}{l l l r r r r} \\hline \\hline **Dataset** & **Split** & **\\#Total** & **\\#Online** & \\multicolumn{2}{c}{**\\#All Dirty (in**} & \\multicolumn{2}{c}{**\\#Input-only**} & \\multicolumn{1}{c}{**\\#Input-and-label**} \\\\ & & & & **CommonCrawt)** & **Contamination** & **Contamination** \\\\ \\hline ARC\\_c & Test & 1172 & 372 & 336 (28.7\\%) & 53 (4.5\\%) & 283 (24.1\\%) \\\\ CommonsenseQA & Dev & 1221 & 44 & 20 (1.6\\%) & 3 (0.2\\%) & 17 (1.4\\%) \\\\ Winogrande & Dev & 1267 & 54 & 14 (1.1\\%) & 0 (0.0\\%) & 14 (1.1\\%) \\\\ C-Eval & Dev & 1346 & 618 & 616 (45.8\\%) & 69 (5.1\\%) & 547 (40.6\\%) \\\\ Hellaswag & Dev & 10042 & 1690 & 1247 (12.4\\%) & 46 (0.4\\%) & 1201 (12.0\\%) \\\\ MMLU & Test & 13987 & 4285 & 4077 (29.1\\%) & 678 (4.8\\%) & 3399 (24.3\\%) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Data contamination statistics for multi-choice QA benchmarks. Search window: 2020.10-2023.10. Figure 2: Increase in Data Contamination from 2017-2020 to 2020-2023. CSQA stands for CommonsenseQA. they are significantly concentrated in specific domains and rare in others. This finding is meaningful as it reveals the possibility that blocking specific domains during training data collection might alleviate the issue of data contamination. You can find more domain analysis and contamination examples in Appendix A."
    },
    {
      "title": "6 Impact Of Contamination On Model Performance",
      "text": "To assess how data contamination impacts model evaluation, we assess over 20 popular large language models on contaminated and clean splits of each benchmark. Contaminated subsets contain examples identified in the previous section as having verbatim matches in the Common Crawl training dataset. Clean subsets on the contrary, contain examples have no matches with the training set. We employ the third party LLMs evaluation platform _OpenCompass_(OpenCompass, 2023) in our experiments to provide in-context demonstrations, prompts, and metrics computing. Following Touvron et al. (2023); OpenAI (2023), we evaluated Winogrande, ARC, CommonsenseQA, and HellaSwag in zero-shot setting by prompting only with the question and choices, but 5 examples are provided as context for MMLU and C-Eval. Accuracy was used as the metric. And we use perplexity to obtain the inference result, i.e., taking the choice with the lowest ppl as the predicted answer. The results are presented in Table 2. Note that the Dirty i. set contains both _input-only_ contamination and _input-and-label_ contamination, but the Dirty ii. set is solely for _input-and-label_ contamination. We place a \\(\\uparrow\\) for a result on dirty samples if they gain an advantage against the clean set, otherwise we add a \\(\\downarrow\\). For Llama-1,2 series models, we use the search window of 2017-2020 according to their reported training data collection period. For all other models we use an estimated search window of 2017.01-2023.10 as their exact training data collection period are unknown. **English Benchmarks.** Based on the table, we find data contamination does not uniformly improve model performance. Instead, the impact depends on both the specific benchmark and model scale. On Hellaswag and ARC benchmarks, most models achieve better metrics on contaminated subsets. However, on MMLU tasks we observe no consistent enhancement across models. We also find that larger language models appear more capable of exploiting data contamination to achieve better performance. For instance, LLaMA-2 70B displays increased metrics on most contaminated subsets. In contrast, the 13B LLaMA-2 only outperforms on contaminated ARC. In addition, LLaMA-2 70B realises larger gains on contaminated sets (6% and 11% boosts on Hellaswag and ARC) than the 7B variant (5% and 6%). This could due to the more powerful memorisation capacity in larger language models Carlini et al. (2022). Finally, most models achieve the highest scores on the input-and-label contaminated subset versus input-only or clean sets. This proves contamination of both inputs and labels can severely affect model evaluation results. Fine-tuned models like the Llama Chat variants exhibit generally lower overall metrics compared to their foundation counterparts, but they demonstrate comparable gains on contaminated splits of Hellaswag and ARC. Specifically, the fine-tuned chat models realise similar absolute performance increases on the dirty subsets of these benchmarks as their corresponding foundation versions. **Non-English Benchmark.** In Table 3, we present contamination analysis on the non-English benchmark C-Eval. Among the tested models, Llama and Mistral are considered pure English models, while Yi, Qwen, and Baichuan are pre-trained as multilingual language models. We find the pure English Figure 3: Domains analysis for data contamination in Hellaswag. models, Llama and Mistral, do not exhibit notable performance increases on C-Eval's contaminated subsets. However, the multilingual large language models all demonstrate significant performance advantages on dirty subsets. Yi 6B even achieves a 14% higher accuracy score on the input-and-label contaminated set, proving the potential for serious distortion of evaluation results. **What is the threshold of overlap for a test example to affect model prediction?** We illustrate how the METEOR score, which measures sentence similarity, correlates with model performance on test samples. The METEOR metric measures the similarity between two sentences. For instance, a test sample with a METEOR score of 0.8 indicates high equivalence between that test case and sentences in training data. In Figure 4, we group test samples by METEOR score and present the accuracy achieved on those groups by Llama-2 70B across four benchmarks. On ARC, Hellaswag, and C-Eval, a general upwards accuracy trend emerges as METEOR rises, indicating models attain higher metrics when more verbatim overlapping samples exist in the training data. In essence, substantial text duplication enables exploitation through memorisation, inflating model scores."
    },
    {
      "title": "7 Discussion",
      "text": ""
    },
    {
      "title": "Existing Methods To Mitigate Data Contamination",
      "text": "Several techniques have been proposed to mitigate data contamination issue in language model evaluation previously. Our findings provide some novel insights on the effectiveness of these approaches. **Blocklisting benchmark sources.** Blocking sources of benchmarks in training data collection is a common way to avoid data contamination. In our paper, we further demonstrate the feasibility of this method. As shown in Figure 3, the distribution of data contamination is very centralised, so blocking only a small set of domains can significantly \\begin{table} \\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline & \\multicolumn{3}{c}{MMLU} & \\multicolumn{3}{c}{Hellaswag} & \\multicolumn{3}{c}{ARC} & \\multicolumn{3}{c}{Average} \\\\ \\cline{2-13} & Clean & Dirty i. & Dirty ii. & Clean & Dirty i. & Dirty ii. & Clean & Dirty i. & Dirty ii. & Clean & Dirty i. & Dirty ii. \\\\ \\hline LLaMA 7B &.3518 &.3577 &.3488 \\(\\downarrow\\) &.6394 &.6696 \\(\\uparrow\\) &.6727 \\(\\uparrow\\) &.3627 &.3448 \\(\\downarrow\\) &.3167 \\(\\uparrow\\) &.4513 &.4574 \\(\\uparrow\\) &.4461 \\(\\downarrow\\) \\\\ LLaMA 13B &.4429 &.4309 \\(\\downarrow\\) &.4884 \\(\\uparrow\\) &.7073 &.7913 \\(\\uparrow\\) &.7909 \\(\\uparrow\\) &.3924 &.3563 \\(\\downarrow\\) &.3333 \\(\\downarrow\\) &.5142 &.5262 \\(\\uparrow\\) &.5375 \\(\\uparrow\\) \\\\ LLaMA 30B &.5381 &.5447 \\(\\uparrow\\) &.5930 \\(\\uparrow\\) &.7412 &.7913 \\(\\uparrow\\) &.7909 \\(\\uparrow\\) &.4249 &.4598 \\(\\uparrow\\) &.4667 \\(\\uparrow\\) &.5681 &.5986 \\(\\uparrow\\) &.6169 \\(\\uparrow\\) \\\\ LLaMA 65B &.6316 &.5447 \\(\\uparrow\\) &.6279 \\(\\downarrow\\) &.7613 &.8087 \\(\\uparrow\\) &.8091 \\(\\uparrow\\) &.4276 &.4713 \\(\\uparrow\\) &.4667 \\(\\uparrow\\) &.6068 &.6082 \\(\\uparrow\\) &.6346 \\(\\uparrow\\) \\\\ \\hline Llama-2 7B &.4180 &.4309 \\(\\uparrow\\) &.4535 \\(\\uparrow\\) &.6746 &.7217 \\(\\uparrow\\) &.7182 \\(\\uparrow\\) &.3803 &.4368 \\(\\uparrow\\) &.4167 \\(\\uparrow\\) &.4910 &.5298 \\(\\uparrow\\) &.5295 \\(\\uparrow\\) \\\\ Llama-2 13B &.5596 &.5285 \\(\\downarrow\\) &.5814 \\(\\uparrow\\) &.8254 &.8087 \\(\\downarrow\\) &.8000 \\(\\downarrow\\) &.4221 &.4368 \\(\\uparrow\\) &.4167 \\(\\uparrow\\) &.6024 &.5913 \\(\\downarrow\\) &.5994 \\(\\downarrow\\) \\\\ Llama-2 70B &.6763 &.6667 \\(\\uparrow\\) &.7093 \\(\\uparrow\\) &.7726 &.8348 \\(\\uparrow\\) &.8455 \\(\\uparrow\\) &.4555 &.5632 \\(\\uparrow\\) &.5667 \\(\\uparrow\\) &.6348 &.6882 \\(\\uparrow\\) &.7072 \\(\\uparrow\\) \\\\ \\hline Llama-2 Chat 7B &.4062 &.3851 \\(\\downarrow\\) &.4060 \\(\\uparrow\\) &.6760 \\(\\uparrow\\) &.7653 \\(\\uparrow\\) &.7361 \\(\\uparrow\\) &.3701 \\(\\uparrow\\) &.4474 \\(\\uparrow\\) &.5000 \\(\\uparrow\\) &.4841 \\(\\uparrow\\) &.5101 \\(\\uparrow\\) &.5564 \\(\\uparrow\\) \\\\ Llama-2 Chat 13B &.5417 &.5098 \\(\\uparrow\\) &.5279 \\(\\uparrow\\) &.73401 \\(\\downarrow\\) &.8051 \\(\\uparrow\\) &.8100 \\(\\uparrow\\) &.4334 &.5256 \\(\\uparrow\\) &.5769 \\(\\uparrow\\) &.5698 \\(\\uparrow\\) &.6226 \\(\\uparrow\\) &.6383 \\(\\uparrow\\) \\\\ Llama-2 Chat 70B &.6324 &.6159 \\(\\downarrow\\) &.6392 \\(\\uparrow\\) &.7576 \\(\\uparrow\\) &.8273 \\(\\uparrow\\) &.8341 \\(\\uparrow\\) &.4343 &.4737 \\(\\uparrow\\) &.4615 \\(\\uparrow\\) &.6081 &.6390 \\(\\uparrow\\) &.6450 \\(\\uparrow\\) \\\\ \\hline Mistral 7B &.6501 &.6291 \\(\\downarrow\\) &.6463 \\(\\downarrow\\) &.8533 \\(\\downarrow\\) &.8287 \\(\\downarrow\\) &.8326 \\(\\downarrow\\) &.4720 &.5263 \\(\\uparrow\\) &.5769 \\(\\uparrow\\) &.6585 &.6614 \\(\\uparrow\\) &.6853 \\(\\uparrow\\) \\\\ Mistral-FT 7B &.5576 &.5403 \\(\\downarrow\\) &.5588 \\(\\uparrow\\) &.7168 \\(\\uparrow\\) &.6691 \\(\\downarrow\\) &.6727 \\(\\uparrow\\) &.4426 &.5000 \\(\\uparrow\\) &.5577 \\(\\uparrow\\) &.5723 \\(\\uparrow\\) &.5698 \\(\\downarrow\\) &.5964 \\(\\uparrow\\) \\\\ Yi 6B &.6481 &.6386 \\(\\downarrow\\) &.6551 \\(\\uparrow\\) &.7628 \\(\\uparrow\\) &.7533 \\(\\downarrow\\) &.7617 \\(\\downarrow\\) &.4380 &.4868 \\(\\uparrow\\) &.5000 \\(\\uparrow\\) &.6163 &.6262 \\(\\uparrow\\) &.6389 \\(\\uparrow\\) \\\\ Qwen 7B &.5785 &.5665 \\(\\downarrow\\) &.5825 \\(\\uparrow\\) &.9153 \\(\\uparrow\\) &.9144 \\(\\downarrow\\) &.9186 \\(\\uparrow\\) &.4096 \\(\\uparrow\\) &.4423 \\(\\uparrow\\) &.6345 \\(\\uparrow\\) &.6471 \\(\\uparrow\\) &.6478 \\(\\uparrow\\) \\\\ Baichuan 27B &.5594 &.5320 \\(\\downarrow\\) &.5491 \\(\\downarrow\\) &.7494 \\(\\uparrow\\) &.7199 \\(\\downarrow\\) &.7240 \\(\\downarrow\\) &.3710 \\(\\downarrow\\) &.3158 \\(\\downarrow\\) &.3654 \\(\\downarrow\\) &.5599 &.5226 \\(\\downarrow\\) &.5462 \\(\\downarrow\\) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Model performance on the clean, not clean (denoted as Dirty i., including both _input-only_ and _input-and-label_ contaminated samples), and input-and-label contaminated (denoted as Dirty ii.) subsets. Figure 4: Accuracy of Llama-2 70B for test examples with different METEOR score. alleviate the issue of data contamination. However, we also find blocklisted links quickly expire but content spreads, making the blocklist ineffective over time. For instance, we test the contamination blocklist in the first release of MMLU2, and we found the given blocklist only avoids 1.5% of contaminated cases we detected in SS5. If we adopt a more aggressive method that skips all domains in the blocklist, it still just avoids 21% of contaminated cases. This suggests content used in MMLU spreads rapidly, which emphasises the necessity to update the blocklists regularly. Footnote 2: [https://people.eecs.berkeley.edu/~hendrycks/data.tar](https://people.eecs.berkeley.edu/~hendrycks/data.tar) **Avoid using data that appears with its solution on the internet**(Jacovi et al., 2023). According to our results, avoiding the presence of answer can indeed prevent memorising exact answers. As shown in Table 4, we found models perform generally worse on input-only contamination compare to input-and-label contamination, or sometimes even worse than the clean set. This suggest that preventing input-and-label contamination is the key to mitigate data contamination issue. However, our analysis also identify several cases where _input-only_ contamination provides unfair advantages. For example, most models obtain higher accuracy on the _input-only_ contaminated subset of ARC. As a result, completely avoiding using online resources is the best practice in benchmark construction. Winogrande is a role model in avoiding contamination by using only human-authored content. **Protecting test data from automatic crawlers via encryption and forbidding further distribution**(Jacovi et al., 2023). Forbidding further distribution of benchmarks can indeed prevent data contamination to some extent. This was proven in our Figure 3, where some contaminated cases are from huggingface.co, a dataset sharing platform. However, forbidding further distribution of the test data also significantly limits the popularity of benchmarks. For example, benchmarks such as Hellaswag and C-Eval make their test sets nonpublic to avoid potential data contamination issues. However, this also makes popular third party model evaluation platforms turn to using their validation sets instead of the test sets, as the platform hosts can access the answers in the validation sets to conduct the assessment (OpenCompass, 2023). Actually, most researchers tend to evaluate their models on publicly available splits rather than restricted ones, even if the latter have lower contamination risk. Therefore, benchmarks should consider balancing robustness against ease of adoption by the community."
    },
    {
      "title": "Comparison To Llama'S Original Contamination Report",
      "text": "The data contamination analysis in the original Llama-2 paper is quite incomplete, presenting results for only Hellaswag and MMLU benchmarks. However, we can still compare our contamination analysis results to theirs for these two datasets. As explained in SS6, we use a search window of 2017-2020 for Llama-1,2 series model. In Hellaswag, we detected a similar percentage of contamination (8.3%) to what was reported in Llama-2 (848 out of 10042 examples, 8.4%). For MMLU, we identified a slightly lower ratio of data contamination, with 8.7% flagged as contaminated versus 11% marked as contaminated in the original Llama-2 paper. Touvron et al. (2023) showed Llama-2 70B's performance gain on HellaSwag from _dirty_ to _not dirty3_ data was.0742, while our method demonstrated advantages of.0726 for _input-and-label_ contamination and.0622 for all contamination types. On MMLU, Touvron et al. (2023) only observed a performance boost from contamination on MMLU-Humanities, where Llama-2 70B had a gain (_dirty_ to _not dirty_) of.0980 on MMLU-Humanities. In contrast, our method showed a slightly lower increase of.0845 on MMLU-Humanities (shown in Table 5). Overall, our results align well with Llama-2's original contamination reports, demonstrating the effectiveness of our methodology. Footnote 3: This _not dirty_ set here is not the same as our clean set. See their paper for more details."
    },
    {
      "title": "8 Conclusion",
      "text": "This paper conducted an extensive data contamination analysis for popular large language models on six multi-choice QA benchmarks. We identified varying levels of test set contamination, ranging from 1% to 47% across benchmarks. We also find data contamination does not necessarily lead to increased metrics: data contamination in ARC and Hellaswag generally allow models to achieve significant higher accuracy, but contamination in MMLU has little impact on model's performance. Our findings offer a transparent perspective on data contamination, emphasising its significance as an urgent issue within the evaluation community."
    },
    {
      "title": "9 Limitation",
      "text": "Our pipeline leverages search engines followed by querying Common Crawl index. This avoids the access of full training dataset locally, which is often not open-sourced for modern large language models. However, relying on search APIs incurs notable costs - around $15 per 1000 queries with Bing. We spent about $900 in total calling Bing search API. Additionally, search engines restrict query lengths, which prevents analysis of benchmarks with long input passages like reading comprehension. Moreover, large language model developers may also use customised data collected from crowd sourcing or non-public databases. In this case, our search engine plus Common Crawl pipeline may be unable to identify data contamination from these hidden data sources. Future attempts may directly query the complete Common Crawl corpus hosted on AWS S3 services. This would enable scanning of lengthy input examples but at a higher financial cost. Alternatively, developing perplexity-based approaches to detect contaminated examples without requiring full passage matching could prove fruitful."
    },
    {
      "title": "References",
      "text": "* Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. _arXiv preprint arXiv:2108.07732_. * Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Bingyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Meng, Xingzhang Ren, Xuan Cheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. * Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In _Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization_, pages 65-72. * Bisk et al. (2019) Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. Piga: Reasoning about physical commonsense in natural language. * Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901. * Carlini et al. (2022) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. Quantifying memorization across neural language models. _arXiv preprint arXiv:2202.07646_. * Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khaal, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantantu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. * Choi et al. (2018) Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wentau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. Quac: Question answering in context. _arXiv preprint arXiv:1808.07036_. * Chowdhery et al. (2021) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodokumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pilali, Marie Pellat, Aitor Lewkowycz, Eric Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. * Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprisingdifficulty of natural yes/no questions. _arXiv preprint arXiv:1905.10044_. * Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. _ArXiv_, abs/1803.05457. * Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. * Dodge et al. (2021) Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. _arXiv preprint arXiv:2104.08758_. * Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. _Proceedings of the International Conference on Learning Representations (ICLR)_. * Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b. Measuring massive multitask language understanding. * Huang et al. (2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. _arXiv preprint arXiv:2305.08322_. * Jacovi et al. (2023) Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. 2023. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. * Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. 2023. Mistral 7b. * Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. * Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:453-466. * Marie (2023) Benjamin Marie. 2023. The decontaminated evaluation of gpt-4. Accessed: 2023-07-28. * Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In _Conference on Empirical Methods in Natural Language Processing_. * OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. * OpenCompass (2023) OpenCompass. 2023. Opencompass: A universal evaluation platform for foundation models. [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass). * Rajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. _arXiv preprint arXiv:1806.03822_. * Sainz et al. (2023) Oscar Sainz, Jon Campos, Iker Garcia-Ferrero, Julien Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. NLP evaluation in trouble: On the need to measure LLM data contamination for each benchmark. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 10776-10787, Singapore. Association for Computational Linguistics. * Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106. * Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. Socialiqa: Commonsense reasoning about social interactions. _arXiv preprint arXiv:1904.09728_. * Srivastava and et al. (2023) Aarohi Srivastava and et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. * Talmor et al. (2018) Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowledge. _arXiv preprint arXiv:1811.00937_. * Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_. * Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_. * Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_. [MISSING_PAGE_FAIL:11] Figure 5: Domains analysis for data contamination in ARC. Figure 6: Domains analysis for data contamination in MMLU. \\begin{table} \\begin{tabular}{l c c c c c c c c c c c c c} \\hline \\hline & \\multicolumn{6}{c}{MMLU} & \\multicolumn{6}{c}{Hellaswag} & \\multicolumn{6}{c}{ARC} \\\\ \\cline{2-13} & Clean & Dirty i & Dirty ii & All & Clean & Dirty i & Dirty ii & All & Clean & Dirty i & Dirty ii & All \\\\ \\hline LLaMA 7b &.3427 &.3367 &.3223 &.3356 &.7634 &.5769 &.7089 &.7560 &.3627 &.4167 &.3077 &.3614 \\\\ LLaMA 13b &.4652 &.3615 &.4686 &.4594 &.8347 &.7308 &.7979 &.8298 &.3930 &.3750 &.3269 &.3897 \\\\ LLaMA 30b &.5690 &.4563 &.5717 &.5624 &.8557 &.6923 &.8371 &.8527 &.4261 &.4167 &.4615 &.4275 \\\\ LLaMA 65b &.6364 &.4854 &.6525 &.6317 &.8805 &.8077 &.8612 &.8778 &.4288 &.4583 &.4615 &.4309 \\\\ Llama-2 7b &.4310 &.3426 &.4580 &.4340 &.7896 &.6538 &.7436 &.7834 &.3802 &.4583 &.4423 &.3845 \\\\ Llama-2 13b &.5647 &.4621 &.5435 &.5509 &.7236 &.6538 &.7783 &.7298 &.4233 &.4583 &.4038 &.4232 \\\\ Llama-2 70b &.6884 &.5671 &.7159 &.6894 &.8848 &.7692 &.8703 &.8825 &.4564 &.5417 &.5769 &.4635 \\\\ \\hline Llama-2 7b Chat &.4062 &.2813 &.4060 &.3978 &.6760 &.6923 &.7632 &.6865 &.3701 &.3333 &.5000 &.3751 \\\\ Llama-2 13b Chat &.5417 &.4198 &.5279 &.5291 &.7341 &.6923 &.8100 &.7430 &.4334 &.5000 &.5769 &.4412 \\\\ Llama-2 70b Chat &.6324 &.5000 &.6392 &.6259 &.7576 &.6538 &.8341 &.7663 &.4343 &.5000 &.4615 &.4369 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Llama models’ performance comparison. Here Dirty i. denotes input-only contamination and Dirty ii. denotes input-and-label contamination. All denotes the performance on the entire test set. \\begin{table} \\begin{tabular}{l|c c c|c c c|c c c|c c c|c c} \\hline \\hline Model & \\multicolumn{6}{c|}{MMLU} & \\multicolumn{6}{c|}{MMLU-Humanities} & \\multicolumn{6}{c|}{MMLU-STEM} & \\multicolumn{6}{c|}{MMLU-Social-Science} & \\multicolumn{6}{c}{MMLU-Other} \\\\ \\hline & Clean & Dirty i & Dirty ii & Clean & Dirty i & Dirty ii & Clean & Dirty i & Dirty ii & Clean & Dirty i & Dirty ii & Clean & Dirty i & Dirty ii \\\\ \\hline Llama 7B & 34.27 & 33.67 & 32.23 & 33.69 & 25.76 & 34.22 & 30.79 & 33.04 & 30.67 & 37.40 & 38.10 & 31.59 & 35.64 & 35.23 & 33.60 \\\\ Llama 13B & 46.52 & 36.15 & 46.86 & 43.79 & 43.94 & 53.38 & 37.78 & 27.73 & 37.37 & 55.55 & 49.52 & 51.33 & 50.31 & 41.48 & 49.53 \\\\ Llama 30B & 56.90 & 45.63 & 57.17 & 55.02 & 59.09 & 64.36 & 46.10 & 36.28 & 47.84 & 65.91 & 58.10 & 63.18 & 61.84 & 51.14 & 57.09 \\\\ Llama 65B & 63.64 & 48.54 & 65.25 & 63.71 & 56.06 & 74.73 & 52.58 & 41.59 & 54.09 & 72.08 & 59.05 & 74.17 & 67.30 & 52.84 & 62.21 \\\\ Llama 2 7B & 43.10 & 34.26 & 45.80 & 41.90 & 45.45 & 55.57 & 34.38 & 26.55 & 36.82 & 49.74 & 45.71 & 49.20 & 47.30 & 38.07 & 46.29 \\\\ Llama 2 13B & 56.47 & 46.21 & 54.35 & 55.73 & 59.09 & 60.91 & 44.27 & 37.17 & 44.17 & 64.22 & 60.95 & 61.69 & 62.64 & 50.00 & 54.39 \\\\ Llama 2 70B & 68.84 & 56.71 & 71.59 & 65.78 & 74.24 & 79.28 & 57.18 & 45.43 & 61.52 & 81.12 & 67.62 & 80.15 & 73.13 & 65.34 & 68.96 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: Llama series models’ performance across different categories of MMLU. Figure 7: An example of _input-and-label_ (a) and _input-only_ (b) contamination from MMLU."
    }
  ]
}