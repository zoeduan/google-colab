{
  "title": "The (ab)use of Open Source Code to Train Large Language Models",
  "authors": [
    "Ali Al-Kaswan",
    "Maliheh Izadi"
  ],
  "abstract": "\n In recent years, Large Language Models (LLMs) have gained significant popularity due to their ability to generate human-like text and their potential applications in various fields, such as Software Engineering. LLMs for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization. We argue why the use of copyleft code to train LLMs is a legal and ethical dilemma. Finally, we provide four actionable recommendations to address this issue. \n",
  "references": [
    {
      "id": null,
      "title": "The (ab)use of Open Source Code to Train Large Language Models",
      "authors": [
        "Ali Al-Kaswan",
        "Maliheh Izadi"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Extending source code pre-trained language models to summarise decompiled binaries",
      "authors": [
        "A Al-Kaswan",
        "T Ahmed",
        "M Izadi",
        "A A Sawant",
        "P Devanbu",
        "A Van Deursen"
      ],
      "year": "",
      "venue": "Proceedings of the 30th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Codefill: Multitoken code completion by jointly learning from structure and naming sequences",
      "authors": [
        "M Izadi",
        "R Gismondi",
        "G Gousios"
      ],
      "year": "2022",
      "venue": "Proceedings of the 44th International Conference on Software Engineering (ICSE)",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Extracting training data from large language models",
      "authors": [
        "N Carlini",
        "F Tramer",
        "E Wallace",
        "M Jagielski",
        "A Herbert-Voss",
        "K Lee",
        "A Roberts",
        "T Brown",
        "D Song",
        "U Erlingsson"
      ],
      "year": "2021",
      "venue": "30th USENIX Security Symposium (USENIX Security 21)",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Asleep at the keyboard? assessing the security of github copilot's code contributions",
      "authors": [
        "H Pearce",
        "B Ahmad",
        "B Tan",
        "B Dolan-Gavitt",
        "R Karri"
      ],
      "year": "2022",
      "venue": "IEEE Symposium on Security and Privacy (SP)",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Coprotector: Protect open-source code against unauthorized training usage with data poisoning",
      "authors": [
        "Z Sun",
        "X Du",
        "F Song",
        "M Ni",
        "L Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM Web Conference 2022",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "The (Ab)Use Of Open Source Code To Train Large Language Models",
      "text": "Ali Al-Kaswan _Delft University of Technology_ Delft, The Netherlands a.al-kaswan@tudelft.nl Maliheh Izadi _Delft University of Technology_ Delft, The Netherlands m.izadi@tudelft.nl"
    },
    {
      "title": "Abstract",
      "text": "In recent years, Large Language Models (LLMs) have gained significant popularity due to their ability to generate human-like text and their potential applications in various fields, such as Software Engineering. LLMs for Code are commonly trained on large unsonitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization. We argue why the use of copyright code to train LLMs is a legal and ethical dilemma. Finally, we provide four actionable recommendations to address this issue."
    },
    {
      "title": "I Language Models For Code",
      "text": "Large Language Models (LLMs) have gained significant attention in the field of Natural Language Processing (NLP) in recent years due to their ability to perform a wide range of NLP tasks with impressive accuracy. These models, trained on massive amounts of data, improve in accuracy as they grow from millions to billions of parameters. LLMs for code are trained on massive amounts of data and can learn the structure and syntax of programming languages, making them well-suited for tasks such as code summarization, generation, and completion [1, 2]. LLMs are even making their way into commercial products like GitHub's Copilot, Replits's GhostWriter and Tabnine. Meanwhile, some have identified that LLMs can memorize large swaths of training data [3]. Memorization enables the extraction of the data using Data Extraction Attacks. Some attacks have even been able to extract addresses and other personal information from public models [3]. Memorization also impacts LLMs for code, with all its associated consequences. We will discuss these consequences in three categories: security, privacy and licensing."
    },
    {
      "title": "Ii Security Implications",
      "text": "Text memorization has strong security implications. Firstly, massively mined code datasets are not sanitized or manually curated, the datasets could therefore contain many biases,1 and instances of badly written or buggy and insecure code. A recent study found that around 40% of GitHub Copilot's code generations for MITRE's top 25 Common Weakness Enumerations, a list of the most dangerous software weaknesses were found to be vulnerable [4]. If these models become more prevalent and trusted, they can introduce more vulnerable code into software. Footnote 1: Does GPT-2 Know Your Phone Number?: [http://archive.is/LxsyA](http://archive.is/LxsyA)"
    },
    {
      "title": "Iii Privacy Implications",
      "text": "Memorization enables adversaries to access training data, and everything contained within, simply by accessing the model. This has major privacy implications since code can contain private information. Think of credentials, API keys, directory structures, logged info, or in-code discussions by developers. Code can also contain personal information like emails or contact information. If personal data is published on the Internet, the data could be retracted and deleted from the source. But once it is mined and used to train an LLM, the information is forever embedded in a compact representation, which is queryable at scale. With query access to these models, an adversary can potentially extract this data [3] and threaten Internet users' privacy. There are many reasons why one could publicly share private information; (1) simply by accident, or (2) a malicious actor could share this information in a doxing campaign [3]. Even if the data is published willingly, the owner has a certain use and audience in mind and might not wish to share this information with the entire world. This is referred to as the re-purposed data problem.1 Footnote 2: Matrix Transpose: [http://archive.is/YU5BI](http://archive.is/YU5BI)"
    },
    {
      "title": "Iv Licensing",
      "text": "Publicly available source code is also subject to licences, some of which heavily regulate the use of the material. Initially, developers raised concerns about licensed code on social media. GitHub Copilot could be prompted to produce verbatim copies of copyrighted code, without providing the required attribution or licence terms.3 Similarly, Copilot was producing copyrighted code while attributing the wrong author and providing the wrong license.4 Later, a lawsuit was filed against GitHub, Microsoft and OpenAI, claiming that Copilot is violating the licence of open-source code.5 Footnote 3: Fast Inverse Square: [http://archive.is/HNiyg](http://archive.is/HNiyg) Footnote 5: GitHub Complaint (p.26): [http://archive.is/3PFAs](http://archive.is/3PFAs) Broadly, open-source code is licensed under two types of licences. **Permissive licenses**, allow users to use, modify, and distribute the software for any purpose, without requiring that the user share their work. **Non-permissive licenses**, also known as \"copyleft\" licenses, require that users freely share their own software under the same licence if they distributethe software or any _derivatives_ of it. Creating closed or commercial software based on non-permissively licensed code is unethical and possibly even illegal [5]. But this does raise the following question: **Does training LLMs on copyleft code infringe on their license?** Firstly, we must determine how many LLMs for code are trained on copyleft code. Looking at some of the most popular code models, we can observe that the vast majority are trained on open-source code. CodeBERT and CodeT5 are trained on CodeSearchNet, which contains copyleft code. We also found that CodeBERT, CodeGen, and CodeClippy make use of The Pile, a collection of \\(22\\) datasets, one of which is a GitHub dataset containing copyleft data. We found that only InCoder makes an effort to prevent training on copyleft code. InCoder does however make use of a dataset of StackOverflow answers, which are licensed under varying CC-BY-SA licences, all of which require attribution.5 Footnote 5: StackOverflow license: [http://archive.is/obaoy](http://archive.is/obaoy) Despite the public attention, it is not completely clear whether Codex, the model behind Copilot, is trained on non-permissive code. Many imply that it does,6 citing the copying of copyleft code and the fact that the system has encountered a copy of the GPL licence many times during training. The training data for Codex is not publicly available, and neither OpenAI nor Github have provided any clarification.7 Footnote 6: Comment on Copilot and OSS: [http://archive.is/6gEOU](http://archive.is/6gEOU) LLMs for code can be seen as derivatives of their training data. So unless the model is published under the same licence as the training data and includes the copyright notice, this would be a clear violation. Moreover, many licences are not inter-compatible, i.e., the inclusion of code licensed under them automatically warrants an infringement as the combined licence agreements contain irreconcilable conditions. Some opponents,8 including OpenAI, argue that the use of public code is an instance of transformative fair use, which is a defence that allows the use of copyrighted works in new and unexpected ways and exists in many jurisdictions including the US.8 Yet, it is still unclear whether the fair use defence applies to ML-systems,4 as it has not yet been tested in court. Furthermore, the fair use argument is sometimes based on the assumption that models do not memorize and emit training data, which is false. Even if the fair use argument protects the use of the data, the verbatim outputting might not be protected. Footnote 8: GitHub Copilot is not infringing your copyright: [http://archive.is/PYlm5](http://archive.is/PYlm5) A moral argument can also be made on this issue. Training LLMs on copyleft code goes against the will of some open-source developers, who share their code for the betterment of society and who believe in the principle of free and open software so profoundly, they're willing to add a full legal clause to their work to perpetuate this ideal. The use of their work, without attribution, especially by commercial parties, is not what they had in mind. Finally, some researchers have also proposed a different approach to this issue by letting the authors of open-source code take matters into their own hands. Using data poisoning techniques, the authors can reduce the performance and embed watermarks into the models [5]."
    },
    {
      "title": "V Discussion And Recommendations",
      "text": "To conclude we recommend the following: * The ML community should carefully consider the licence of their training material, from both a legal and an ethical point of view. The authors of published LLMs should be transparent about the licences of their training material. * More research should be conducted on the nature and proportionality of text memorization in LLMs for code and LLMs in general. Other topics include memorized text extraction and prevention. * Lawmakers should clarify whether the use of copyleft code (and copyrighted materials in general) and text to train LLMs constitutes fair use and under which conditions this clause applies. * Finally, the software engineering community should clarify their stance on this issue. Developers could make informed decisions and clearly denote if their source code can be used to train AI models. LLMs for code are likely to stay and bring new tools that change the way software is engineered. So the community needs to answer important questions on this matter. For instance, should open-source code be allowed for training these models? If so, should the developers be credited and compensated, and under which license should the models be released? Alternatively, do we need to revise current code licenses to clarify the community's stance?9 Footnote 9: Additional Reading Material: Link to our GitHub Repository"
    },
    {
      "title": "References",
      "text": "* [1] A. Al-Kaswan, T. Ahmed, M. Izadi, A. A. Sawant, P. Devanbu, and A. van Deursen, \"Extending source code pre-trained language models to summarise decompiled binaries,\" in _Proceedings of the 30th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)_, 2023. * [2] M. Izadi, R. Gismondi, and G. Gousios, \"Codefill: Multitoken code completion by jointly learning from structure and naming sequences,\" in _Proceedings of the 44th International Conference on Software Engineering (ICSE)_. ACM, 2022, p. 401-412. * [3] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson _et al._, \"Extracting training data from large language models,\" in _30th USENIX Security Symposium (USENIX Security 21)_, 2021, pp. 2633-2650. * [4] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri, \"Asleep at the keyboard? assessing the security of github copilot's code contributions,\" in _IEEE Symposium on Security and Privacy (SP)_, 2022, pp. 754-768. * [5] Z. Sun, X. Du, F. Song, M. Ni, and L. Li, \"Coprotector: Protect open-source code against unauthorized training usage with data poisoning,\" in _Proceedings of the ACM Web Conference 2022_, 2022, pp. 652-660."
    }
  ]
}