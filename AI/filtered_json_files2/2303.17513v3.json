{
  "title": "Improving the Diproche CNL through Autoformalization via Large Language Models",
  "authors": [
    "Merlin Carl",
    "Euf Flensburg",
    "Germany"
  ],
  "abstract": "\n The Diproche system is an automated proof checker for texts written in a controlled fragment of German, designed for didactical applications in classes introducing students to proofs for the first time. The first version of the system used a controlled natural language for which a Prolog formalization routine was written. In this paper, we explore the possibility of prompting large language models for autoformalization in the context of Diproche, with encouraging first results. \n",
  "references": [
    {
      "id": null,
      "title": "Improving the Diproche CNL through Autoformalization via Large Language Models",
      "authors": [
        "Merlin Carl",
        "Euf Flensburg",
        "Germany"
      ],
      "year": "",
      "venue": "",
      "doi": "10.4204/EPTCS.400.4"
    },
    {
      "id": "b0",
      "title": "ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics",
      "authors": [
        "Zhangir Azerbayev",
        "Bartosz Piotrowski",
        "Hailey Schoelkopf",
        "Edward W Ayers",
        "R Dragomir",
        "Jeremy Radev",
        "Avigad"
      ],
      "year": "2023",
      "venue": "ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics",
      "doi": "10.48550/arXiv.2302.12433"
    },
    {
      "id": "b1",
      "title": "THF0 -The Core of the TPTP Language for Higher-Order Logic",
      "authors": [
        "Christoph Benzmüller",
        "Florian Rabe",
        "Geoff Sutcliffe"
      ],
      "year": "2008",
      "venue": "THF0 -The Core of the TPTP Language for Higher-Order Logic",
      "doi": "10.1007/978-3-540-71070-7_41"
    },
    {
      "id": "b2",
      "title": "Number Theory and Axiomatic Geometry in the Diproche System",
      "authors": [
        "Merlin Carl"
      ],
      "year": "2020",
      "venue": "Electronic Proceedings in Theoretical Computer Science",
      "doi": "10.4204/EPTCS.328.4"
    },
    {
      "id": "b3",
      "title": "Diproche -ein automatisierter Tutor für den Einstieg ins Beweisen",
      "authors": [],
      "year": "2020",
      "venue": "Digitale Kompetenzen und Curriculare Konsequenzen",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Natural Language Proof Checking in Introduction to Proof Classes -First Experiences with Diproche",
      "authors": [
        "Merlin Carl",
        "Hinrich Lorenzen",
        "Michael Schmitz"
      ],
      "year": "2022",
      "venue": "Electronic Proceedings in Theoretical Computer Science",
      "doi": "10.4204/EPTCS.354.5"
    },
    {
      "id": "b5",
      "title": "How is ChatGPT's behavior changing over time?",
      "authors": [
        "Lingjiao Chen",
        "Matei Zaharia",
        "& James",
        "Y Zou"
      ],
      "year": "2023",
      "venue": "How is ChatGPT's behavior changing over time?",
      "doi": "10.48550/arXiv.2307.09009"
    },
    {
      "id": "b6",
      "title": "Proof-checking mathematical texts in controlled natural language",
      "authors": [
        "Marcos Cramer"
      ],
      "year": "2013",
      "venue": "Proof-checking mathematical texts in controlled natural language",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "The Naproche Project Controlled Natural Language Proof Checking of Mathematical Texts",
      "authors": [
        "Marcos Cramer",
        "Bernhard Fisseni",
        "Peter Koepke",
        "Daniel Kühlwein",
        "Bernhard Schröder",
        "Jip Veldman"
      ],
      "year": "2009",
      "venue": "The Naproche Project Controlled Natural Language Proof Checking of Mathematical Texts",
      "doi": "10.1007/978-3-642-14418-9_11"
    },
    {
      "id": "b8",
      "title": "The language of mathematics",
      "authors": [
        "Mohan Ganesalingam"
      ],
      "year": "2013",
      "venue": "The language of mathematics",
      "doi": "10.1007/978-3-642-37012-0"
    },
    {
      "id": "b9",
      "title": "ProofML -eine Annotationssprache für natürliche Beweise",
      "authors": [
        "Peter Koepke",
        "Bernhard Schröder"
      ],
      "year": "2003",
      "venue": "LDV Forum",
      "doi": "10.21248/jlcl.18.2003.48"
    },
    {
      "id": "b10",
      "title": "Exploration of Neural Machine Translation in Autoformalization of Mathematics in Mizar",
      "authors": [
        "Wang Qingxiang",
        "Chad Brown",
        "Cezary Kaliszyk",
        "Josef Urban"
      ],
      "year": "2019",
      "venue": "CPP 2020: Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs",
      "doi": "10.1145/3372885.3373827"
    },
    {
      "id": "b11",
      "title": "Autoformalization with Large Language Models",
      "authors": [
        "Yuhuai Wu",
        "Albert Jiang",
        "Wenda Li",
        "Markus Rabe",
        "Charles Staats",
        "Mateja Jamnik",
        "Christian Szegedy"
      ],
      "year": "2022",
      "venue": "36th Conference on Neural Information Processing Systems (NeurIPS)",
      "doi": "10.48550/arXiv.2205.12615"
    }
  ],
  "sections": [
    {
      "title": "Improving The Diproche Cnl Through Autoformalization Via Large Language Models",
      "text": "Merlin Carl EUF Flensburg, Germany Institut fur Mathematik Europa-Universitat Flensburg Flensburg, Germany merlin.carl@uni-flensburg.de"
    },
    {
      "title": "Abstract",
      "text": "The Diproche system is an automated proof checker for texts written in a controlled fragment of German, designed for didactical applications in classes introducing students to proofs for the first time. The first version of the system used a controlled natural language for which a Prolog formalization routine was written. In this paper, we explore the possibility of prompting large language models for autoformalization in the context of Diproche, with encouraging first results."
    },
    {
      "title": "1 Introduction",
      "text": "The Diproche (\"Didactical Proof Checking\")1 system is an automated proof checker for proofs in a controlled natural language (CNL) specifically adapted to elementary proving exercises in \"introduction to proof\" classes in first-year university education. Users can enter a proof in a controlled fragment of German and receive immediate feedback on logical correctness, type correctness, fulfillment of proof goals etc.; see [4], [5] for an introduction to the system and [6] for a report on the experiences with the first version of the system. Footnote 1: Both the system’s architecture and its name are inspired by the Naproche system, of which it is a kind of didactical “offspring”, see [https://naproche-net.github.io/0](https://naproche-net.github.io/0). In spite of the impressive progress recently made with machine translation and the possibility of using this for the task of autoformalization (see, e.g., [12]), the Diproche CNL was implemented using classical techniques from computational linguistics, more specifically via a definite clause grammar written in Prolog. This was used to convert the natural language input into an internal list format, from which the proof obligations at each proof step are generated, to be passed on to an automated theorem provers (ATPs) in which those steps that should be accepted in the context of a specific exercise are hard-coded. The reason for this choice was to make the system as transparent as possible to the user: There should be no surprises concerning how the program interprets certain formulations, or which formulations it accepts. However, in practice, it quickly became apparent that this goal is in conflict with the other, similarly important, goal of providing a convenient CNL sufficiently rich for expressing proof texts in a way that resembles natural mathematical texts well enough to spare users the burden of learning a formalism on top of the difficulties they face when learning how to prove. Since the intended users are mathematical beginners with little to no experience with formal deduction, formal languages or proof calculi, it turned out that transparency is lost rather quickly. This leaves little reason to not take advantage of the possibilities of natural language processing based on machine learning techniques, such as pretrained language models. An obvious reason in favor of this approach is that developing, refining and changing such aprompt-based CNL is much easier and quicker than (re-)writing a formal grammar. In particular, making the model work in other natural languages is as easy as translating the natural language sentences in the prompt, no matter how (grammatically) different the new language is from the current one.2 Footnote 2: Not even this may be necessary: Prompted entirely with German sentences, our model was able to correctly process several (simple) sentences written in English, French and Chinese. In this paper, we begin to explore the possibilities of using the language model DaVinci-3 developed by OpenAI3 for various tasks in didactical systems that aim at teaching how to prove, in particular transforming natural language input into a formal representation that can be further processed by formal proof checkers. For this purpose, we built a prototype in which a Python-based preprocessing routine using DaVinci-3 was combined with the logical components of Diproche written in Prolog. We also consider a very recent version of GPT-4, which shows an even stronger autoformalization performance. Footnote 3: See [https://openai.com/product](https://openai.com/product). Footnote 4: Some qualifications apply here; see below. At first, the experiences reported in Avigad et al. [2] with using large language models for autoformalization appear to be discouraging for this plan: Only about 11 percent of the natural language inputs were formalized correctly ([2], p. 3). Much better results were reported in [13], where more than 25 percent of the natural language inputs (which were problems for math competitions) were translated correctly into Isabelle (ibid., p. 1). Still, for reliably checking even a simple natural language argument consisting of typically more than 10 sentences with sufficient reliability to be of didactical use to beginner's students, anything considerably below a hundred percent is not good enough. However, one needs to keep in mind that the approach in the works just mentioned is explicitly not to use a CNL, but to formalize sentences from everyday mathematical discourse. In contrast, our aim is by far more modest: Retaining the restriction to a small fragment of natural mathematical language, we want to use language models to (i) simplify the process of designing and converting to such languages and (ii) allow for more freedom in the specific choice of formulations compared with a CNL given by a formal syntax. The classical approach to translating between a CNL and a formal representation would be to implement a formal grammar for it, which is a quite cumbersome task. In contrast, our experience shows that, with certain qualifications, the Diproche CNL could be learned and improved with merely 71 lines of example formalization by text-davinci-003. Even more impressingly, if one merely requires translation to a more standard first-order format that is easily convertible to the internal representation format used by Diproche, the same effect could be achieved for GPT-4-Turbo with a prompt merely containing the relevant to notation, but no examples.4 Footnote 4: An experimental system has been implemented using text-davinci-003. The integration of GPT-4-Turbo is currently being developed. The new system architecture, adapted from the one given in [4], is as follows, where Python components are marked with \"Py\" and Prolog components are marked with \"Pr\", while \"LLM\" denotes the large language model used for the autoformalization:5 Footnote 5: Cf., e.g., [9], p. 171; for a detailed treatments of the specifics of the language of mathematics, see Ganesalingam [10]."
    },
    {
      "title": "2 Autoformalization And Proof-Checking",
      "text": "A naive approach to using autoformalization in automated proof-checking is the following: Each natural language sentence corresponds to some formula in first-order logic. By translating each sentence separately, one obtains a formalization of the whole text, which can then be given to an automated theorem prover for verification. This view, however, ignores a great deal of well-known features of natural language mathematics:6Figure 1: Flowchart of Diproche with integrated LLM 1. Sentences have different functions, such as goal announcements, deductions, assumptions, annotations. For a (logical) check of the text, these need to be identified, along with the content. 2. Sentences may have content that is not immediately expressible in first order logic. Consider \"Since \\(a=b\\), it follows that \\(a^{2}=b^{2}\\)\". The claim here is apparently that, at this point of the argument, it is established that \\(a=b\\) and that, from this, \\(a^{2}=b^{2}\\) can be deduced. It would certainly be wrong to formalize this as \\((a=b)\\rightarrow(a^{2}=b^{2})\\), since then, only the implication would be established, while it is its conclusion that the sentence is claiming. However, omitting the \"Since \\(a=b\\)\" would considerably distort the sentence's meaning: We do not want a sentence like \"Since grass is green, it follows that \\(a^{2}=b^{2}\\)\" to be marked as correct. Another example would be a sentence containing multiple conclusions, such as \"Now we have \\(A\\), so we get \\(A\\lor B\\), and thus also \\(C\\rightarrow(A\\lor B)\\)\". Thus, the formal representation of a sentence will need to use means beyond mere first-order logic. 3. The whole text has a structure. It may contain subproofs, variables and assumptions are introduced for certain parts of the argument and gone in others.7 This overall structure needs to be taken into account when formally representing proofs. Footnote 7: See, e.g., Cramer [8], p. 255. 4. Elliptic sentences that gain their meaning from context: \"We show that there are infinitely many primes. Suppose otherwise.\"; \"Thus, there is a line passing through \\(P\\) and \\(Q\\). Call it \\(l\\).\"; \"Hence, \\(n\\) has at least one prime divisor. Pick one, and call it \\(p\\).\"; \"So we have \\(x\\in A\\). Consequently, it is even.\"8 Footnote 8: Cf, e.g., [11], p. 7-8. Thus, a naive \"sentence by sentence\"-formalization is not enough as a basis for automated proof-checking. Along with a formalization of content, one needs to identify the function of the sentence, which is a task for automated text classification, one needs a formalism capable of representing figures such as justifications that do not appear in first-order logic (and the autodormalization routine needs to translate to this formalism), and one needs some kind of structural markers to identify the scope of declarations and assumptions, and, when formalizing a sentence, it must be possible to take into account earlier sentences as context."
    },
    {
      "title": "3 Prompting And Training Large Language Models",
      "text": "The pre-trained language models offered by OpenAI can be adapted to a specific task in two different ways: Prompting and fine-tuning. While prompting can simply mean writing a description of the task at hand (such as \"Write a birthday card for the person named in the input\"), in the case of autodormalization, it is best done by offering a carefully chosen series of examples.9 Prompting leaves the model internally unchanged; intuitively, one could regard it as writing a (long) question. Fine-tuning, in contrast, means actually modifying (training) the language model with an appropriate data set. This option is currently only available for language models considerably weaker than text-davinci-003; in order to achieve satisfying result, a considerably larger amount of examples is required.10 Since such data is currently unavailable for Diproche, and also somewhat cumbersome to generate, we will only consider prompting in this paper, which seems to offer a quick and easy way to achieve an impressive level of autoformalization sufficient for basic didactical applications.11 Footnote 11: A word of warning is in order here: A prompt length currently cannot go beyond 4000 tokens, which is quite limited."
    },
    {
      "title": "The Diproche Cnl And The Internal List Format",
      "text": "The Diproche CNL is explained in some detail in [4]. Here, we recall some basic features. The Diproche CNL is a fragment of natural mathematical German, comprising typical ways to express assumptions (\"Suppose that \\(x\\) is even\"), claims (\"Hence, \\(x+1\\) is odd\"), variable declarations (\"Let \\(k\\) be an integer\"), goal announcements (\"We will show that \\(x\\) is a square\") and annotations (\"Proof:\", \"qed\", \"Case 1:\" etc.); further sub-types include declarations combined with assumptions, such as \"Let \\(k\\) be an integer such that \\(n=2k\\)\" (which are existentially loaded and are in need of verification) and justified claims (\"Since \\(x=3(a+b)\\), \\(x\\) is a multiple of 3\"). The sentences12 written in this CNL are converted into an internal list format whose crucial ingredients are a list of the of variables occuring in the sentence, its type (assumption, declaration, claim, annotation,...) and its actual content (which can be empty, as in the case of annotations). Thus, the sentence \"Therefore, \\(x\\) is even\" would be translated as [[x],beh,[even,[x]]]. Footnote 12: In the relevant sense here, “sentence” includes annotations. When processing formulations such as \"Suppose not\" or resolving anaphors, prior sentences need to be taken into account as the context in which a certain sentences is to be translated. A typical line of our prompt looks like this:13 Footnote 13: Translated into English for the convenience of the reader; the actual prompt lines are German. context:{We show that the intersection of \\(A\\) and \\(B\\) is a subset of the union of \\(A\\) and \\(B\\).} Suppose not. # [[A,B],ang,[not,[[A,cap,B],subseteq,[A,cup,B]]]]SS Here, the part \"context:{}\" contains the relevant context, the next part (\"Suppose not\") is the sentence to be translated, # serves to separate the natural language sentence from its formalization, then we have the formalization in the internal list format and finally SS as the stop symbol, which prevents the language model from generating further text, such as more examples of natural language sentences and their formalizations in the same spirit. The examples also included ungrammatical and formally invalid sentences, for which the translation was \"invalid\" (\"ungultig\"). If the formalization routine leads to this result, the process is stopped and the sentence in question is reported to the user as not processable."
    },
    {
      "title": "First Experiences",
      "text": "The first experiences with prompting DaVinci-3 for autoformalization in Diproche were encouraging: After only a few examples, the model had \"grasped\" the extraction of variables along with the classification and even offered (usually sensible) completions in the \"spirit\" of the given examples although no specific example for the case at hand was given; for example, after learning that \"\\(x\\) is even\" was to be formalized as [even,[x]], it drew the obvious analogy for \"\\(x\\) is odd\" or \"\\(x\\) is prime\". It correctly dealt with formulations that it had not seen in the examples - for example, after having seen that \\(x\\in X\\) was to be turned into [\\(x\\),in,\\(X\\)], it correctly processed sentences like \"\\(x\\) is an element of \\(X\\)\" or \"\\(X\\) contains \\(x\\)\";14 similarly, after being given one example of how to formalize an assumption, it correctly identified a variety of ways to formulate assumptions - including some that, such as \"Gesetzt, es ware der Fall,class\" (\"Let it be the case that\"), were intentionally chosen to be somewhat uncommon. It even had a considerable success rate when, after a series of German prompts, it was given a sentence in English, French, Italian or Chinese; a system once developed in this way can thus be easily made available in other languages as well. Even without prior examples, the model exhibited the ability introduce variable names not given in the text and pick them in a sensible way; thus, for example, \"Every natural number has a prime divisor\" was formalized as [all,[n,in,nat],[exists,p,[[prime,p],and,[divides,[p,n]]]]]; in particular, in no instance was a variable name used for different variables. In most cases, the model was able to resolve anaphors, taking advantage not merely of grammatical categories, but even of content: For example, for the input \"Hence \\(x\\) is an element of \\(A\\). Thus, it is even. Consequently, it cannot be empty.\", the first \"it\" was formalized as \\(x\\), while the second was formalized as \\(A\\). Turning formulas into the internal list format was also \"learned\" reliably along the way. Likewise, the model could correctly deal with elliptic formulations such as \"We will show that \\(A=B\\). Suppose not.\" of \"Hence, there exists \\(k\\) such that \\(n=2k\\). Pick one.\". We also observed that a rather common input format for automated theorem provers, namely THF15 was already \"known\" to DaVinci-3; when merely asked to \"formalize\" various statements without given specific examples, it generated THF formulas.16 This, however, is not of immediate relevance for our purposes, since Diproche uses its own internal format. Footnote 15: See, e.g., [https://www.tptp.org/Seminars/THF/Contents.html](https://www.tptp.org/Seminars/THF/Contents.html) or [3]. Footnote 16: A similar “surprise” is reported in [13], p. 1. Still, there were issues, most of which, however, could be resolved to our satisfaction: 1. Along with the requested formalization, the model occasionally generated extra example pairs of natural language and formalization of its own. This could be prevented by introducing SS as a stop symbol and putting this after each formalization. 2. Even so, the model sometimes added to the natural language input rather than merely formalizing the given expression. This was resolved by separating the natural language expression from the formalization by an # and making # a part of the input for each request. 3. When the input consisted of several sentences, the model frequently misrepresented later sentences, perhaps according to \"expectations\" of what these should have been rather than what was actually there. Only giving it one sentence per time was not an option, since this would have ruled out using the abilities of the model to refer to context, e.g., in the resolution of anaphors, or in processing such constructions as \"We will show that \\(p\\) is prime. Suppose not.\" (see above). This was solved by explicitly labeling the preceding sentences as \"context\". The model was prompted with examples of sentences that could not be formalized without further context, in which case it should return the error message \"missing context\". In order to minimize such unwanted interference, the autoformalization routine works with the minimal amount of context that is required for the sentence at hand: Thus, given a list of sentences \\([S_{1},S_{2},...,S_{n}]\\), it will, in the \\(i\\)-step, attempt to formalize \\(S_{i}\\) without invoking context (i.e., using the empty context). If this yields to a \"missing context\" error, \\(S_{i}\\) is tried again, this time with context \\(\\{S_{i-1}\\}\\). If the error persists, the earlier sentences are added one by one; when all earlier sentences up to \\(S_{1}\\) were added without success, the formalization attempt is stopped unsuccessfully. 4. Even when the correct formalization was given in several examples, the model occasionally choose to express it differently, i.e., express \\(A\\cup B\\) as [A,union,B], rather than [A,cup,B]. Since these cases were of a limited and surveyable number, this problem could be dealt with by a postprocessing routine. 5. There were also difficulties to distinguish between implications (\"If \\(x\\) is even, then \\(x+1\\) is odd\") and justifications (\"Now \\(x+1\\) is odd, because \\(x\\) is even\"): rewriting the examples and adding the tag \"justification\" improved the performance, but attained nothing near perfect accuracy. We therefore decided to drop phrases containing justifications from the CNL in the first version. Moreover, it soon became apparent that converting all types of formulas occuring in any of the sub-areas currently available in Diproche - in particular, propositional logic, Boolean set theory, axiomatic geometry and elementary number theory - was too much to ask from a model prompted with only 4000 tokens. We thus decided to further specify the task by writing separate prompts for each of these areas; when requested, the system would then use the model for the area to which the current proof text belongs. For the first experiments reported here, we concentrated on writing a prompt for Boolean set theory, an area for which several example Diproche texts are available and which also forms an important part of the beginner's lectures taught in Flensburg. Besides text-davinci-003, we also had the opportunity to test an \"assistant\" based on a recent version of GPT-4. An assistant is a chatbot whose behaviour can be controlled by a prompt describing its intended functioning.17 In order to keep the prompt length limited, we did not insist on a direct translation into the internal Diproche format, but were instead content with a standard first-order format that is easily automatically translatable into the required format. It turned out that, with a prompt explaining in detail the desired output format, the performance of the assistant was satisfying (see below) even without presenting a single example. However, it should be noted that the model's responses still depend on the previous course of the dialogue, so that answers to earlier requests play a role similar to the examples given to text-davinci-003. Thus, while the model was able to autoformalize the given statements with a rather high success rate, this might have been different had the statements been given in a different order. Thus, adding a set of examples representative of the task at hand seems advisable also for this option. Footnote 17: See, e.g., [https://platform.openai.com/docs/assistants/how-it-works/agents](https://platform.openai.com/docs/assistants/how-it-works/agents)."
    },
    {
      "title": "4 Performance On Typical Diproche Texts",
      "text": "To see whether the prompted language model would perform, we tested it against three solutions for set-theoretical exercises written in the Diproche CNL, and also modified versions of these solutions that contained mistakes. Not counting these variants, and only considering sentences that were actually passed on to the language model18, these texts contained 33 sentences, all of which were processed correctly. Since these texts were typical texts that users of the system would be expected to write, this confirms that the prompted language model could serve as a replacement for the Diproche CNL in a practical setting. Footnote 18: Some standard annotations are processed separately and not given to the language model. In order to evaluate the performance of the GPT-4-based assistant, we used 50 example sentences from the area of Boolean set theory. The model was then asked to identify the type of the sentence (declaration, assumption, claim, declaration with additional assumption) and to provide a formalization. The results of this experiment can be found in a table in the appendix. In order to make it easier for the reader to evaluate the quality of the formalizations, we worked with English sentences, although the Diproche CNL is a fragment of German. As experiments suggest, the performance on German translations of the given sentences did not substantially deviate. We also asked several mathematicians to write up clearly structured solutions using short and simple sentences to several sample exercises, but without mentioning any particular syntax. Our goal is to evaluate how much of these solution texts will be processed correctly by the model in order to quantify the \"naturalness\" of the \"learned CNL\". The results of this will appear in future work. These results show that, at least in the field of Boolean set theory, the \"learned CNL\", covers most of what the hard-coded Diproche-CNL for this purpose had to offer (we recall that \"justified claims\" were excluded in this investigation). On the other hand, it offers a much greater degree of freedom of expression; in particular, natural language variants of simple formal expressions, such as \"\\(x\\) is an element of the intersection of \\(A\\) and \\(B\\)\" are usually processed correctly, which would be somewhat cumbersome to obtain with a formal syntax.19 Moreover, it turned out to be rather tolerant with respect to minor misspellings or typos, in contrast to the Diproche CNL, the orthographical strictness of which had apparently been a source of frustration for several students. This is a clear advantage of using large language models over using hand-crafted formalization routines. Footnote 19: We remark, though, that this did not always work reliably: For example, we observed one case where the model confused “the intersection of the complements of \\(A\\) and \\(B\\)” with “the complement of the intersection of \\(A\\) and \\(B\\)”. To get a clearer picture of the prospects of this approach, it would certainly be desirable to have a systematic statistical evaluation of the system. This, however, would in fact of limited value due to the following reason: Since the models used above are continually modified, the performance measured at one point of time can be vastly different from the performance a few months later, even when evaluated using the exactly same requests.20 We are planning such an evaluation after the system has been changed to work with local LLMs that can be kept stable over time (see also the next section). Moreover, since the preparation of the first version of this paper (in spring 2023) and the present version (in December 2024), new LLMs have become available that show a strongly improved performance for autoformalization tasks relevant to our purposes. Footnote 20: See, e.g., [7], which indicates that the performance of GPT-4 and GPT-3.5 considerably _dropped_ from March to June 2023 in several areas. To give the reader at least some impression of the possibilities, we discuss here briefly the 50 above-mentioned example sentences processed with an OpenAI API-assistant based on the (currently experimental) model GPT-4-Turbo.21 The precise prompt and the table of results can be found in the appendix. Note that the prompt does not contain any examples. Of these 50 examples, 49 were processed correctly, leading to a success rate of 98 percent. The resolution of anaphorical expressions worked well in most cases, see, e.g., (13), (19), (23), (24). The exception was example (30) \"From this, we get that, if \\(A\\) is not empty, then \\(B\\) is\", where the anaphorical expression \"then \\(B\\) is\" was wrongly interpreted as referring to \"not empty\", while it would usually be understood as \"empty\". Phrases such as \"exactly one\" were correctly interpreted (46). The results also show that a variety of formulations was correctly processed, which considerably goes beyond the Diproche CNL, including in particular term description in natural language (13), (39), (41). Still, there are several points to be noted: First of all, the input format in this case did not take into account context. Due to this, sentences such as \"Let \\(x\\) be an element of \\(X\\)\" (sentence 11) become ambiguous in their role, being either variable declarations (if \\(x\\) appears for the first time in this sentence) or plain assumptions (if \\(x\\) was introduced earlier on). Similarly, sentence 16 (\"Let \\(A\\) be a subset of \\(B\\)\") was interpreted as a variable declaration of \\(A\\) (but not of \\(B\\)). While this is indeed a plausible reading, there are of course contexts where it would be wrong. In order to fix this, some kind of context needs to be provided, at least in the form of declared variables available at a certain point in the text. While sentence 6 was correctly formalized, the use of an existential quantifier for formalizing the phrase \"non-empty intersection\", rather than merely writing \\(G\\cap H\\neq\\emptyset\\), would in many contexts lead to difficulties in the further processing. In sentence (36), \"whenever\" could be read as an implicit universal quantifier, in which case the given formalization would be missing the quantifiers. However, since the formalization would be processed correctly in the context of Diproche, we regarded this as correct."
    },
    {
      "title": "5 Conclusion And Further Work",
      "text": "The work reported in this paper indicates that prompting large language models such as DaVinci-3 is apparently a good \"quick and dirty\" way for developing and improving controlled natural languages for didactical systems such as Diproche which aim at automated proof verification at the beginner level; in particular, such languages tend to be more flexible and error-tolerant than those obtainable by classical methods with reasonable effort. Clearly, this merely scratches the surface of the possibilities that large language models open for such systems, and which will be the subject of further work. Moreover, the purpose of the present paper is merely to present the general concept and argue for its prospects; an accurate evaluation of the didactical advantages of using LLMs over formal grammars will be done once a system version suitable for the actual employment in teaching has been obtained. On the one hand, the quality and reliability of autoformalization, which is currently mainly hindered by the bound on prompt length, could most likely be considerably improved by \"specialization\", i.e., writing prompts for various sub-tasks, classifying the input accordingly and then handing them to the appropriate sub-module. On the other hand, there is a number of other tasks relevant for such systems, which could conceivably be treated with large language models, including the following: 1. Immediate feedback on the proof text. Our experiments indicate that GPT is currently quite poor and unstable in evaluating the logical coherence of a proof text. However, for other kinds of feedback, such as stylistic remarks, it may be more useful. 2. Generating hints. While GPT is currently not able to reliably solve basic proving exercises, the texts it generates quite often tend to have the right overall structure. This could be used to provide hints for users how to approach a certain problem. 3. Mistake diagnosis. The current Diproche version includes an \"Anti-ATP\", a logical (and algebraic) mal-rule library that codifies typical false deduction steps (such as deducing \\(\\neg B\\) from \\(\\neg A\\) and \\(A\\to B\\)) and reports them to the user; the hope is that this can help becoming aware of such mistakes and eventually to avoid them. Using large language models for mistake diagnosis at least for algebraic manipulations may lead to a more \"semantical\" approach to this task, in contrast to the current one based on formal patterns.22 Footnote 22: For example, both \\((x+y)^{2}=x^{2}+y^{2}\\) and \\((x+1)^{3}=x^{3}+1\\) are currently identified as an instance of a distributive use of exponentiation, but \\((x+y+z)^{2}=x^{2}+y^{2}+z^{2}\\) is not; in contrast, DaVinci-3 was able to recognize the third example as an instance of the same mistake, even though only examples with two summands were given to it in the prompt. Concerning the practical use in (large) teaching situations, however, the following should be considered: * Since DaVinci-3 and similar large language models cannot be run locally, each checking requires a considerable amount of web traffic between the user's device and the servers on which the model is hosted. Compared with the good old-fashioned parsing approach, this takes noticably more time, in particular depending on the internet connection. In our experiments, the time from demanding feedback to receiving feedback typically went up from hardly noticable to at least 30 seconds. * The servers to which the formalization requests are sent are of limited capacity and, if too many requests are sent in a certain amount of time, will refuse them. This already happened to the author frequently during development. With a considerable amount of students frequently using the system, it can be expected to happen regularly. * which is the whole purpose of the system - this can quickly get expensive: Four such exercises with an average of five intermediate checkings would lead to a weekly price of 400$. Footnote 23: 2eper 1000 tokens, see [https://openai.com/pricing](https://openai.com/pricing) (accessed 12.03.2023). * Moreover, this approach has all the disadvantages of relying on external services: The underlying model may be changed, resulting in unexpected behaviour, or discontinued altogether; prices can go up; a certain amount of user data is sent to external servers, which may potentially lead to privacy issues etc. * In particular, as [7] indicate, the continued modifications to GPT have led to a considerable _decrease_ in certain mathematical abilities in three months. Similarly, we observed that certain requests that consistently worked fine at some point of time did so no longer some months later. * The \"learned CNL\", although quite accurate most of the time, is still somewhat unreliable and occasionally shows surprising behaviour. Thus, for example, the sentence \"Thus, \\(x\\) is an element of \\(A\\) or \\(x\\) is an element of \\(A\\).\" was reported by text-davinci-003 as \"invalid\", while it worked perfectly well when replacing one of the \\(A\\)'s by a different letter. This is likely due to the fact that such constructions are extremely uncommon in natural language. Moreover, there were instances where claims in the form conditional constructions were mis-identified as assumptions. In one case, a sentence was processed wrongly after changing the variable names. These difficulties can certainly be overcome by providing more examples, but it should still be kept in mind that, compared with hard-coded parsers, a certain amount of reliability is lost. * A potential problem that may arise in use is that a \"less controlled CNL\" may make it more difficult for users to develop a feeling for what will be understood and what not. Whether this is an actual problem will have to be evaluated empirically. To sum up, experiences indicate that large language models can be fruitfully and easily applied in developing CNLs for proof checkers for didactical applications. However, for practical applications, the aim should be to train models that can be run locally. This would at least solve the first five of the issues mentioned above. To this end, we have experimented with several large language models available on Hugging Face.24 A general experience so far is that even the largest of the general LLMs perform poorly on autoformalization tasks and, perhaps surprisingly, large models specifically trained with mathematical content, such as WizardLM-70B25 fare not much better. However, models trained for automated for code generation, such as WizardCoder-Python-34B26 (apparently the largest local LLM for code generation currently available) show an autoformalization performance comparable to that reported above for text-davinci-003. We plan to systematically develop and evaluate this approach in the near future. Footnote 24: [https://huggingface.co/](https://huggingface.co/) Footnote 25: [https://huggingface.co/WizardLM/WizardLM-70B-V1.0](https://huggingface.co/WizardLM/WizardLM-70B-V1.0) Footnote 26: [https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0](https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0) The ideal would be to create a language model specifically trained on large amounts of data for the task of autoformalization. Work towards such models is done, e.g., in [12]. However, until such pretrained models are available, the approach discussed in this paper seems to offer a workable alternative for certain applications. Acknowledgements We thank our three anonymous referees for several comments that helped in improving the presentation of the paper, along with constructive criticism concerning its content."
    },
    {
      "title": "References",
      "text": "* [1] * [2] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir R. Radev & Jeremy Avigad (2023): _ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics_. ArXiv, doi:10.48550/arXiv.2302.12433. arXiv:arXiv:2302.12433v1. * The Core of the TPTP Language for Higher-Order Logic_. In A. Armando, P. Baumgartner & G. Dowek, editors: Automated Reasoning. IJCAR 2008, 5195, pp. 491-506, doi:10.1007/978-3-540-71070-7_41. * [4] Merlin Carl (2020): _Number Theory and Axiomatic Geometry in the Diproche System_. Electronic Proceedings in Theoretical Computer Science 328, pp. 56-78, doi:10.4204/EPTCS.328.4. * ein automatisierter Tutor fur den Einstieg ins Beweisen_. In: Digitale Kompetenzen und Curriculare Konsequenzen, pp. 43-56. * First Experiences with Diproche_. Electronic Proceedings in Theoretical Computer Science 354, pp. 59-70, doi:10.4204/EPTCS.354.5. * [7] Lingjiao Chen, Matei Zaharia & James Y. Zou (2023): _How is ChatGPT's behavior changing over time?_ArXiv abs/2307.09009, doi:10.48550/arXiv.2307.09009. Available at [https://api.semanticscholar.org/CorpusID:259951081](https://api.semanticscholar.org/CorpusID:259951081). * [8] Marcos Cramer (2013): _Proof-checking mathematical texts in controlled natural language_. Ph.D. thesis, Rheinische Friedrich-Wilhelms-Universitat Bonn. * [9] Marcos Cramer, Bernhard Fisseni, Peter Koepke, Daniel Kuhlwein, Bernhard Schroder & Jip Veldman (2009): _The Naproche Project Controlled Natural Language Proof Checking of Mathematical Texts_. In: Controlled Natural Language. CNL 2009, 5972, pp. 170-186, doi:10.1007/978-3-642-14418-9_11. * [10] Mohan Ganesalingam (2013): _The language of mathematics_. Springer Berlin Heidelberg, doi:10.1007/978-3-642-37012-0. * eine Annotationssprache fur naturliche Beweise_. LDV Forum 18, pp. 428-441, doi:10.21248/jlcl.18.2003.48. Available at [https://api.semanticscholar.org/CorpusID:30895733](https://api.semanticscholar.org/CorpusID:30895733). * [12] Wang Qingxiang, Chad Brown, Cezary Kaliszyk & Josef Urban (2019): _Exploration of Neural Machine Translation in Autoformalization of Mathematics in Mizar_. In: CPP 2020: Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, pp. 85-98, doi:10.1145/3372885.3373827. * [13] Yuhuai Wu, Albert Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik & Christian Szegedy (2022): _Autoformalization with Large Language Models_. In: 36th Conference on Neural Information Processing Systems (NeurIPS), doi:10.48550/arXiv.2205.12615. Appendix We give here the precise prompt for the assistant used in the autoformalization experiments with GPT-4-Turbo, along with the table listing the results. We used the following prompt (typesetting adapted for the sake of the reader): Given a sentence, translate it into the format [type,subtype,formalization], where \"type\" is one of the following: \"claim\", \"assumption\", \"variable declaration\", \"goal declaration\". \"variable declaration\" has the subtypes \"plain\" and \"with additional assumption\", for statements in which an assumption is made about the introduced variables beyond the type declaration. \"formalization\" works as follows: For a plain claim, it is just a formalization of the statement in first-order logic. For an assumption, it is a formalization in first-order logic of the assumed statement. For a plain variable declaration, it is a list of pairs [var,type], where var is the variable to be declared and type is the type assigned to it. For a variable declaration with additional assumption, it is a pair [declarations,assumption] consisting of the list of declarations as for a plain declaration and a formalization of the assumption. A mere existence claim is not a declaration; thus, \"there is an integer x such that...\" is a claim, not a declaration. Declarations are indicated by words such as \"Let\", \"pick\", \"Choose\" etc. If the given sentence contains free variables - that are not quantified over in the sentence - they should remain free in the formalization. Return only the triple [type,subtype,formalization], nothing else. Use \\(\\emptyset\\) for the empty set, \\(c(X)\\) for the complement of \\(X\\), \\(\\cup\\) for the union of sets, \\(\\cap\\) for the intersection of sets, \\(=\\) for equality, \\(\\subseteq\\) for the subset relation, \\(\\in\\) for the element relation. The results obtained with this prompt for 50 example sentences can be found in the below table. \\begin{tabular}{|c|l|l|c|c|} \\hline Nr & Input & Output type & Output formalization & +/- \\\\ \\hline \\hline 1 & Let \\(A\\) be a set. & decl/plain & [[\\(A\\),set]] & + \\\\ \\hline 2 & Let \\(A\\) be a non-empty set. & decl/assmpt & [[\\(A\\), set]], \\(A\\neq\\emptyset\\)] & + \\\\ \\hline 3 & Let \\(A,B,C\\), be sets. & decl/plain & [[\\(A\\), set], [\\(B\\), set], [\\(C\\), set]] & + \\\\ \\hline 4 & Let \\(A\\), \\(B\\), \\(C\\) be sets such that \\(A\\cup B=c(C)\\). & decl/assmpt & [[\\(A\\), set], [\\(B\\), set], [\\(C\\), set]], \\(A\\cup B=c(C)\\)]] & + \\\\ \\hline 5 & Let \\(Q\\), \\(X\\) and \\(U\\) as well as \\(P\\) be non-empty sets. & decl/assmpt & [[[\\(Q\\), set], [\\(X\\), set], [\\(U\\), set], [\\(P\\), set]], \\((Q\\neq\\emptyset)\\wedge(X\\neq\\emptyset)\\) & + \\\\ & & & \\(\\wedge(U\\neq\\emptyset)\\wedge(P\\neq\\emptyset)\\)] & \\\\ \\hline 6 & Let \\(G\\), \\(H\\) be sets with non-empty intersection & decl/assmpt & [[[\\(G\\), set], [\\(H\\),set]], \\(\\exists x(x\\in G\\wedge x\\in H)\\)] & + \\\\ \\hline 7 & Let \\(P\\), \\(Q\\) be disjoint sets. & decl/assmpt & [[[\\(P\\), set], [\\(Q\\), set]], \\(P\\cap Q=\\emptyset\\)] & + \\\\ \\hline \\end{tabular} [MISSING_PAGE_FAIL:13] \\begin{tabular}{|c|l|l|l|} \\hline 33 & For all sets \\(X\\), \\(Y\\), \\(Z\\) it holds that: & claim & \\(\\forall X\\forall Y\\forall Z((Y\\subseteq Z)\\rightarrow(X\\cap Y\\subseteq X\\cap Z))\\) & + \\\\ & If it holds that \\(Y\\) is a subset of \\(Z\\), & & \\\\ & then it also holds that the intersection of \\(X\\) and \\(Y\\) is a subset of & & \\\\ & the intersection of X and Z. & & \\\\ \\hline 34 & We will now show that \\(A\\) is in fact empty. & goal & \\(A=\\emptyset\\) & + \\\\ \\hline 35 & We need to show that \\(x\\) belongs to \\(K\\). & goal & \\(x\\in K\\) & + \\\\ \\hline 36 & We need to demonstrate that, whenever \\(A\\cap B=A\\cup B\\), then \\(A=B\\). & goal & \\((A\\cap B=A\\cup B)\\rightarrow(A=B)\\) & + \\\\ \\hline 37 & Our goal is to see that \\(x\\) and \\(y\\) belong to \\(Q\\) & goal & \\((x\\in Q)\\wedge(y\\in Q)\\) & + \\\\ \\hline 38 & To see this, first suppose that \\(A\\) is empty while \\(B\\) is not. & assmpt & \\((A=\\emptyset)\\wedge(B\\neq\\emptyset)\\) & + \\\\ \\hline 39 & Now it remains to establish the pairwise disjointness of \\(U\\), \\(V\\) and \\(W\\). & goal & \\((U\\cap V=\\emptyset)\\wedge(U\\cap W=\\emptyset)\\) & + \\\\ \\hline 40 & If \\(x\\in A\\), then \\(x\\in(A\\cup B)\\) & claim & \\(\\forall x(x\\in A\\to x\\in(A\\cup B))\\) & + \\\\ \\hline 41 & As we will presently show, the complement of \\(A\\cup B\\) equals the intersection of the complements of \\(A\\) and \\(B\\). & goal & \\(c(A\\cup B)=c(A)\\cap c(B)\\) & + \\\\ \\hline 42 & It is thus excluded that \\(A\\) is a subset of \\(B\\). & & \\\\ \\hline 43 & Therefore, we get that, if \\(A\\) is empty, then \\(B\\) is not. & claim & \\((A=\\emptyset)\\rightarrow(B\\neq\\emptyset)\\) & + \\\\ \\hline 44 & If \\(X\\) and \\(Y\\) are non-empty and disjoint then both are subsets of \\(U\\). & claim & \\((X\\neq\\emptyset\\wedge Y\\neq\\emptyset\\wedge X\\cap Y=\\emptyset)\\rightarrow(S \\subseteq U\\wedge Y\\subseteq U)\\) & + \\\\ \\hline 45 & It now follows that at least one of \\(A\\) and \\(B\\) must be empty. & claim & \\((A=\\emptyset)\\vee(B=\\emptyset)\\) & + \\\\ \\hline 46 & From this, we get that exactly one of \\(A\\) and \\(B\\) is empty. & claim & \\(((A=\\emptyset)\\wedge(B\\neq\\emptyset))\\) & + \\\\ \\hline 47 & Let \\(A\\), \\(B\\), \\(C\\) be sets and additionally pick \\(D\\) to be a non-empty set. & decl/assmpt & [[\\(A\\),set], [\\(B\\),set], [\\(C\\), set]], \\(D\\neq\\emptyset\\) & + \\\\ \\hline 48 & Consider sets \\(A\\), \\(B\\), \\(C\\) satisfying \\(A\\cap B=c(C)\\) & decl/assmpt & [[[\\(A\\),set], [\\(B\\), set], [\\(C\\),set]], \\(A\\cap B=c(C)\\)] & + \\\\ \\hline \\end{tabular} \\begin{tabular}{|c|l|l|l|l|} \\hline 49 & If one of \\(A\\), \\(B\\) is equal to \\(C\\), then & claim & \\(\\begin{array}{l}((A=C)\\vee(B=C))\\\\ \\rightarrow(C\\subseteq(A\\cup B))\\end{array}\\) & + \\\\ \\hline 50 & Thus, the empty set is disjoint & claim & \\(\\forall X(\\emptyset\\cap X=\\emptyset)\\) & + \\\\ from every set. & & & \\\\ \\hline \\end{tabular}"
    }
  ]
}