{
  "title": "Selection Bias Induced Spurious Correlations in Large Language Models",
  "authors": [
    "Emily Mcmilin"
  ],
  "abstract": "\n In this work we show how large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias. To demonstrate the effect, we developed a masked gender task that can be applied to BERT-family models to reveal spurious correlations between predicted gender pronouns and a variety of seemingly gender-neutral variables like date and location, on pre-trained (unmodified) BERT and RoBERTa large models. Finally, we provide an online demo, inviting readers to experiment further. \n",
  "references": [
    {
      "id": null,
      "title": "Selection Bias Induced Spurious Correlations in Large Language Models",
      "authors": [
        "Emily Mcmilin"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Controlling selection bias in causal inference",
      "authors": [
        "Elias Bareinboim",
        "Judea Pearl"
      ],
      "year": "2012",
      "venue": "Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Recovering causal effects from selection bias",
      "authors": [
        "Elias Bareinboim",
        "Jin Tian"
      ],
      "year": "2015",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Recovering from selection bias in causal and statistical inference",
      "authors": [
        "Elias Bareinboim",
        "Jin Tian",
        "Judea Pearl"
      ],
      "year": "2014",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Underspecification presents chal-lenges for credibility in modern machine learning",
      "authors": [
        "Katherine A Alexander D'amour",
        "Dan Heller",
        "Ben Moldovan",
        "Babak Adlam",
        "Alex Alipanahi",
        "Christina Beutel",
        "Jonathan Chen",
        "Jacob Deaton",
        "Matthew D Eisenstein",
        "Farhad Hoffman",
        "Neil Hormozdiari",
        "Shaobo Houlsby",
        "Ghassen Hou",
        "Alan Jerfel",
        "Mario Karthikesalingam",
        "Yi-An Lucic",
        "Cory Y Ma",
        "Diana Mclean",
        "Akinori Mincu",
        "Andrea Mitani",
        "Zachary Montanari",
        "Vivek Nado",
        "Christopher Natarajan",
        "Thomas F Nielson",
        "Rajiv Osborne",
        "Kim Raman",
        "Rory Ramasamy",
        "Jessica Sayres",
        "Martin Schrouff",
        "Shannon Seneviratne",
        "Harini Sequeira",
        "Victor Suresh",
        "Max Veitch",
        "Xuezhi Vladymyrov",
        "Kellie Wang",
        "Steve Webster",
        "Taedong Yadlowsky",
        "Xiaohua Yun",
        "D Zhai",
        "Sculley"
      ],
      "year": "2020",
      "venue": "Underspecification presents chal-lenges for credibility in modern machine learning",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Openwebtext corpus",
      "authors": [
        "Aaron Gokaslan",
        "Vanya Cohen"
      ],
      "year": "2019",
      "venue": "Openwebtext corpus",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Sample selection bias as a specification error",
      "authors": [
        "James J Heckman"
      ],
      "year": "1979",
      "venue": "Econometrica",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Administrative records mask racially biased policing",
      "authors": [
        "Dean Knox",
        "Will Lower",
        "Jonathan Mummolo"
      ],
      "year": "2020",
      "venue": "American Political Science Review",
      "doi": "10.1017/S0003055420000039"
    },
    {
      "id": "b8",
      "title": "Generating text from structured data with application to the biography domain",
      "authors": [
        "Rémi Lebret",
        "David Grangier",
        "Michael Auli"
      ],
      "year": "2016",
      "venue": "Generating text from structured data with application to the biography domain",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized BERT pretraining approach",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "",
      "authors": [],
      "year": "2009",
      "venue": "",
      "doi": "10.1017/CBO9780511803161"
    },
    {
      "id": "b11",
      "title": "Gender bias in coreference resolution",
      "authors": [
        "Rachel Rudinger",
        "Jason Naradowsky",
        "Brian Leonard",
        "Benjamin Van Durme"
      ],
      "year": "2018",
      "venue": "Gender bias in coreference resolution",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "TL;DR: Mining Reddit to learn automatic summarization",
      "authors": [
        "Michael Völske",
        "Martin Potthast",
        "Shahbaz Syed",
        "Benno Stein"
      ],
      "year": "2017",
      "venue": "Proceedings of the Workshop on New Frontiers in Summarization",
      "doi": "10.18653/v1/W17-4508"
    },
    {
      "id": "b13",
      "title": "Measuring and reducing gendered correlations in pre-trained models",
      "authors": [
        "Kellie Webster",
        "Xuezhi Wang",
        "Ian Tenney",
        "Alex Beutel",
        "Emily Pitler",
        "Ellie Pavlick",
        "Jilin Chen",
        "Ed Chi",
        "Slav Petrov"
      ],
      "year": "2020",
      "venue": "Measuring and reducing gendered correlations in pre-trained models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Selection Variables",
      "text": "Recall we desire to demonstrate a learned statistical association between gender, \\(G\\), and gender-neutral variables, \\(W\\), that is driven by _access_ (now referred as \\(Z\\)) to the dataset sampling process. For our datasets, suitable instantiations for \\(W\\) and \\(Z\\) as related to \\(G\\) are as follows. For Wiki-Bio: _access_ to resources has generally become less gender inequitable over time as _date_ (\\(W_{0}\\)) increases, but not evenly in every _place_ (\\(W_{1}\\)). Generally only those with _access_ to resources will achieve the level of notoriety necessary for an entry in Wikipedia. For Reddit-TLDR: despite many _subreddit_ channels (\\(W\\)) having a focus on gender-neutral topics of interest, the style of moderation and community within a given subreddit may reduce gender-equal _access_ to participation in that _subreddit_."
    },
    {
      "title": "Causal Dags",
      "text": "The above written descriptions of variables and their causal relationships can be compactly represented in a causal directed acyclic graph (DAG), where nodes are variables and arrows are the direction of causation, as can be seen in Figure 1(a) for our assumed data generating process for the Wiki-Bio and Reddit TLDR datasets. From Figure 1(a), we see that \\(W\\) and \\(G\\) can cause one's _access_, \\(Z\\), to dataset selection, as discussed above. At the bottom we see our dataset's features: \\(X\\) for _text_, and labels: \\(Y\\) for _pronouns_. We argue that despite the complex causal interactions between words in a sentence, the _text_ are more likely to cause the _pronouns_, rather than vice versa.2 Footnote 2: For example, if the subject is a famous doctor and the object is her wealthy father, these context words will determine which person is being referred to, and thus which gendered-pronoun to use. Further we assume that \\(Z\\) and \\(W\\) have an effect on one's life and thus the _text_ written about them or by them. And clearly \\(G\\) does cause the gender _pronouns_, \\(Y\\). However, because the goal of the masked gender task is to mask out explicitly gendered words, we'd argue that \\(G\\) is not a direct cause of the _text_, \\(X\\). Finally, note \\(Z\\) is grayed out, as it is not explicitly recorded in the dataset."
    },
    {
      "title": "3 Selection Bias",
      "text": "To explain how selection bias can cause two unconditionally independent variables to become dependent, we revisit the causal DAG representing the data generating processes in Figure 1(a). Causal DAGs are associated with a set of structural equations that together compose a structural causal model (Pearl, 2009). For the _access_ variable, the structural equation is \\(Z:=f_{z}(W,G,U_{z})\\), where \\(U_{z}\\) is the exogenous noise of the \\(Z\\) variable, and again \\(W\\) are the variables _date_ and _place_ for Wiki-Bio, and _subreddit_ for Reddit TLDR, and \\(G\\) is gender. Although \\(W\\) and \\(G\\) are independent in Figure 1(a), for all but trivial special cases, they will become statistically associated in the equation \\(f_{z}\\) for \\(Z\\)."
    },
    {
      "title": "Conditioning On A Collider",
      "text": "Thus any model that conditions on the variable \\(Z\\), will introduce this spurious association between \\(W\\) and \\(G\\), known as collider bias (Pearl, 2009). Excluding \\(Z\\) from the predictive model seems trivial, especially because \\(Z\\) is not directly observed in the dataset. However, recall that \\(Z\\) is a cause of the selection process, depicted in the selection diagram, Figure 1(b), where the selection node, \\(S\\), can only take values \\(S\\)=1 for a sample selected into the dataset and \\(S\\)=0 otherwise (Bareinboim and Pearl, 2012). Recall from Section 1 that during dataset formation, we implicitly condition on \\(S\\)=1, as only selected samples appear in the dataset. Conditioning on a descendant of \\(Z\\), induces the collider bias relationship, as if we had conditioned on \\(Z\\) directly. We are thus inducing the latent structural equation for \\(f_{z}\\) into the dataset, from which the statistical association between \\(W\\) and \\(G\\) can be learned by our models. Figure 1: Causal DAG representing the assumed data generating process for the Wiki-Bio and Reddit TLDR datasets."
    },
    {
      "title": "Selection Bias Recoverability",
      "text": "Figure 1(b) is a modified version of our original causal DAG, which satisfies the requirements of the masked gender task, specifically we have obscured \\(G\\) and replaced it with double headed arrows to represent an unobserved common cause of both \\(Z\\) and \\(Y\\). Although the act of obscuring gender for gender pronoun prediction may seem contrived, we argue that LLMs are often in similar circumstances, for example whenever a prompt, dialog, translation or classification task has not been provided gender features, yet predictions about gender are required. Structural causal models very similar to that in Figure 1(b) have been described in practice in (Knox et al., 2020), and proven in (Bareinboim et al., 2014), to be not'recoverable'. Formally, because we are unable to d-separate the selection mechanism from the label: \\((Y\\not\\!\\!\\perp S|X)\\), the conditional distribution of \\(P(Y|X)\\) cannot be determined without further assumptions or additional data about target populations (Bareinboim and Tian, 2015)."
    },
    {
      "title": "4 Masked Gender Task",
      "text": "This lack of recoverability for \\(P(Y|X)\\) is consistent with our goals of underspecification for the masked gender task."
    },
    {
      "title": "Inference Test Texts",
      "text": "Revisiting Figure 1(b), to maintain underspecification we now require gender-neutral text values for the input _text_, \\(X\\), and the _date_, _place_, and _subreddit_ variables, \\(W\\). In addition to gender-neutral, for \\(X\\) we desire extremely simplistic texts, to avoid inducing unrelated spurious correlations. We were unable to find such a benchmark dataset, so we used the heuristic described in Table 1 to generate over 700 input texts for each of the three \\(W\\) categories."
    },
    {
      "title": "\\(W\\) X-Axis Values",
      "text": "Remaining undefined in the heuristic for input texts in Table 1, is the text values to use for \\(W\\). These \\(W\\) values will also serve as our x-axis in the coming plots. We require values that are gender-neutral in the real world, yet are hypothesized, due to the selection bias process, to be a spectrum of gender-inequitable values in the dataset. For \\(W\\) as _date_, it's easy to just use time itself, as over time women have become more likely to be recorded into historical documents reflected in Wikipedia, so we pick years ranging from 1801 - 2011. For \\(W\\) as _place_, we use the bottom and top 10 World Economic Forum Global Gender Gap ranked countries (see B.1). And for \\(W\\) as _subreddit_, we use _subreddit_ name ordered by subreddits channels that have an increasingly larger percentage of self-reported female commenters, with a minimum size of 400,000 commenters overall (see B.2). The original premise for the \\(W\\) variable was to use categories that are gender-neutral in the real world, but not necessarily so in the dataset. To achieve this, we considered filtering the _subreddit_ list to only topics deemed gender-neutral, however this subjective process invited too much cherry picking on our behalf. Thus, for the _subreddit_ (and _place_) values, we copied the referenced lists verbatim from their sources. To help disambiguate the role of non-gender-neutral subreddit topic names, from the role of _access_ based selection bias, in contributing to a correlation between \\(W\\) and \\(G\\), we tested on one pre-trained model likely exposed to the selection bias effect during pre-training, and one that was likely not exposed. Specifically, RoBERTa was trained with the same data sources as BERT, plus additional data sources including OpenWebText3(Gokaslan and Cohen, 2019). Thus, we'd expect RoBERTa to exhibit a stronger _subreddit_ to _gender_ correlation, than that of BERT, due to the role of subreddit _access_ selection bias during the pre-training of RoBERTa. Footnote 3: Although OpenWebText does not explicitly include Reddit data, because it is composed of scraped web content from URLs shared on Reddit that received at least 3 upvotes (Liu et al., 2019), we conjecture subreddit topic names would appear in the context of Reddit in this dataset."
    },
    {
      "title": "Pre-Trained Bert-Like Models",
      "text": "For testing the masked gender task on pre-trained models, we selected BERT large and RoBERT large, as explained in Section 4.2, using the default weights hosted for the models on Hugging Face. We are able to test the pre-trained LLMs without any modification to the models, as the masked gender task is simply a special case of the masked language modeling (MLM) task, with which all these models were pre-trained. Rather than random masking, the masked gender task masks only explicitly gendered words (listed in Table 3). During LLM pre-training, the MLM prediction is a softmax over the entire tokenizer's vocabulary. The masked gender task sums the gendered portion (as listed in Table 3) of that probability mass from the top five predicted words."
    },
    {
      "title": "Finetuned Models",
      "text": "We also finetune BERT-like models using a similar masked gender task. The difference being that for our finetuning task, the prediction outcome is binary (as opposed to the entire tokenizer's vocabulary), largely for run-time expediency. We elected to finetune the models with data sources similar to those in their pre-training, so we selected BERTfor the Wiki-Bio dataset and RoBERTa for the Reddit TLDR dataset. For the Wiki-Bio dataset, we finetune three BERT base models: 1) with birth _date_ metadata, 2) birth _place_ metadata, and 3) with no extra metadata, prepended to each training sample. In the case of the Reddit TLDR dataset we finetune two RoBERTa base models: 1) with _subreddit_ metadata and 2) with no extra metadata, prepended to each training sample. As these models are finetuned with a single dataset (Wiki-Bio or Reddit TLDR) for the single task of gendered-word prediction, we'd expect them to serve as an upper limit for the magnitude of spurious correlations a model could exhibit for the masked gender task. In particular, the models conditioned with the textual metadata values for \\(W\\) at train time are expected to learn the strongest relationship between \\(W\\) and \\(G\\)."
    },
    {
      "title": "5 Results",
      "text": "In this section we share the results of the masked gender task, tested on pre-trained BERT and RoBERTa large, as well as our finetuned models which can serve as a rough upper limit for the magnitude of expected spurious correlations."
    },
    {
      "title": "Wiki-Bio Date Results",
      "text": "Figure 2(a) shows the results for \\(W\\) as _date_ vs gender pronoun predictions. The spurious correlations shown in these plots are consistent with our hypothesized outcome of the selection bias effect, since all of the models were trained on Wikipedia biographical data. Specifically, as _date_ increases, women's _access_ to resources increases. And thus their representation in Wikipedia increases, inducing the spurious correlation between _date_ and _gender_. Due to the nature this collider bias, as female representation goes up, male representation tends to go down4. Footnote 4: However, in pre-trained models there exist many selection pressures, one of which appears to cause BERT and RoBERTa to be more likely to predict gender pronouns (as opposed to non-gendered pronouns) for both genders after 2000. Table 2 shows the slope and Pearson's \\(r\\) correlation coefficient (following (Rudinger et al., 2018)) for all the plots in this section. These reported coefficients are limited in many ways, including the improbable assumption that the learned relationship between \\(W\\) and _gender_ is linear. Further limiting is that we calculate these coefficients against the index of the x-axis, rather than the _date_ value on the x-axis. We do this here for consistency with the coming _place_ and _subreddit_ plots, for which the most convenient quantitative value we can assign to an ordered list of countries or subreddits is their index in the list. Thus, these coefficients only serve to compare one model's response to another's for a given \\(W\\) category. The most noteworthy comparison here is that the correlation coefficients of the pre-trained models are comparable to those of the finetuned models for female pronouns, all above a Pearson's \\(r\\) value of 0.75 (perhaps in part as a trade-off for the male pronoun coefficients)."
    },
    {
      "title": "Wiki-Bio Place Results",
      "text": "The results for \\(W\\) as _place_ in Figure 2(b) are similar to those discussed above, although the correlation coefficients appear slightly weaker. As mentioned, reliable comparisons of these coefficients across \\(W\\) variables are limited. We nonetheless conjecture that the slightly weaker correlations for _place_ are perhaps in part due to the subjective nature by which the countries are ordered along the x-axis. Despite this limitation, we still see comparable slope and correlation coefficients between the finetuned and pre-trained models for the spurious correlation between _place_ and _gender_ pronouns."
    },
    {
      "title": "Subreddit Results",
      "text": "A challenge in interpreting the results for \\(W\\) as _subreddit_ in Figure 2(c) is that our claim of \\(W\\) as gender-neutral in the'real-word' is dubious for subreddit names, as compared to dates and country names. As discussed in Section 4.2, we elected against filtering the x-axis to only subreddit names that were more gender-neutral, as this was a subjective process that invited cherry picking. \\begin{table} \\begin{tabular}{l c c} \\hline \\hline \\(W\\) Category & Python f-string & Example text \\\\ \\hline \\hline Date \\& Place & ‘f”[MASK] (verb) (life\\_stage) in [w].”’’ & ‘[MASK] was a teenager, in 1953.’ \\\\ & ‘f”In (w), [MASK] (verb) (life\\_stage).”’ & ‘In Mali, [MASK] will be an adult.’ \\\\ \\hline Subreddit & ‘f”[MASK] (verb) (life\\_stage). (w).”’’ & ‘[MASK] is a kid. gifs.’ \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Heuristic for creating gender-neutral input texts for the masked gender task for each \\(W\\) variable category, and example rendered text. For verb we used past, present and future tenses of the verb _to be_: [”was”,“is”,”will be”], and for life\\_stage we used proper and colloquial terms for a range of life stages: [“a child”, “a kid”, “an adolescent”, “a teenager”, “an adult”, “all grown up”]. We didn’t include any life\\_stage past adulthood, as there are not equal gender ratios of elderly men to women, in many locations. Finally for the text versions of w, we used a spectrum of values defined in Section 4.2. To help disambiguate the role of selection bias vs the role of'real-word' 'gendered terms for \\(W\\), we tested one pre-trained model that was likely exposed to the selection bias effect during pre-training, and one that was likely not exposed. Specifically, RoBERTa should learn a more gendered representation for the otherwise gender-neutral subreddit channel names since the selection bias was likely present during its pre-training, whereas BERT should not. The bold-faced correlation coefficients in Table 2 do appear to confirm this hypothesis, with BERT's slope and \\(r\\) coefficients roughly \\(\\frac{1}{5}\\) to \\(\\frac{1}{10}\\) that of RoBERTa, but the authors note more robust testing is desired to strengthen this argument."
    },
    {
      "title": "6 Demonstration And Open-Source Code",
      "text": "The authors would greatly appreciate community feedback that supports or challenges our results. To enable greater access, we have developed a demo of the masked gender task, where users can choose their own input text, as well as the \\(W\\) variable, x-axis values, and the plotted degree of fit, to test spurious correlation to gender in almost any BERT-like model hosted on Hugging Face at [https://huggingface.co/spaces/emilylearning/spurious_correlation_evaluation](https://huggingface.co/spaces/emilylearning/spurious_correlation_evaluation). We additionally we will make all code available at [https://github.com/2dot7lmily/spurious_correlations_ICML_2022](https://github.com/2dot7lmily/spurious_correlations_ICML_2022)."
    },
    {
      "title": "7 Conclusion",
      "text": "In this paper we have introduced and applied the masked gender task to reveal spurious correlations between gender pronouns and real-world gender-neutral entities like dates and countries, on BERT and RoBERTa large pre-trained models. We showed similar spurious correlations between gender pronouns and subreddit channel names, however as we lacked an objectively gender-neutral list of subreddit names, it is more difficult to disambiguate the role of selection bias vs that of non-gender-neutral topic names. The measured correlations between date, country and subreddit-topic vs the probability of a man or woman existing (as a child, adolescent or adult), may be predictive for Wikipedia entries or Subreddit commenters, as \\(P(Y|X,W,S\\)=1), but will not necessarily generalize to the probabilities of men and women existing in real-world inference domains of \\(P(Y|X,W)\\). Our results indicate that sentences previously considered as gender-neutral baselines for testing gender bias in LLMs (e.g. input text such as 'a woman is walking.' (D'Amour et al., 2020)), are also vulnerable to spurious correlations. We explained the role of dataset selection bias in inducing the spurious association between otherwise unconditionally independent entities, such as gender and time, and suggested broad applicability beyond the particular relationships investigated here. As mentioned in Section 3.2, further assumptions or data can help to mitigate the effects of selection bias, which we hope to apply to LLMs in the future."
    },
    {
      "title": "Acknowledgements",
      "text": "Thank you to the SCIS reviewers for their helpful comments, to Rosanne Liu and Jason Yosinski for their encouragement, to Hugging Face for their open source services, and to Judea Pearl, Elias Bareinboim, Brady Neal and Paul Hunermund for their fantastic online causal inference resources. Figure 2: Averaged softmax percentages for predicted gender pronouns vs a range of \\(W\\) values as described in Section 4.2, for gender-neutral input texts described in Table 1. Shaded regions show the 95% confidence interval for a 1st degree linear fit between \\(W\\) and \\(G\\). For all plots we expect a correlation between the x-axis and the predictions, except for Pre-trained BERT large’s predictions vs subreddit."
    },
    {
      "title": "References",
      "text": "* Bareinboim and Pearl (2015) Elias Bareinboim and Judea Pearl. Controlling selection bias in causal inference. In Neil D. Lawrence and Mark Girolami, editors, _Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics_, volume 22 of _Proceedings of Machine Learning Research_, pages 100-108, La Palma, Canary Islands, 21-23 Apr 2015. PMLR. URL [https://proceedings.mlr.press/v22/bareinboim12.html](https://proceedings.mlr.press/v22/bareinboim12.html). * Bareinboim and Tian (2015) Elias Bareinboim and Jin Tian. Recovering causal effects from selection bias. _Proceedings of the AAAI Conference on Artificial Intelligence_, 29(1), Mar. 2015. doi: 10.1609/aaai.v29i1.9679. URL [https://ojs.aaai.org/index.php/AAAI/article/view/9679](https://ojs.aaai.org/index.php/AAAI/article/view/9679). * Bareinboim et al. (2014) Elias Bareinboim, Jin Tian, and Judea Pearl. Recovering from selection bias in causal and statistical inference. _Proceedings of the AAAI Conference on Artificial Intelligence_, 28(1), Jun. 2014. URL [https://ojs.aaai.org/index.php/AAAI/article/view/9074](https://ojs.aaai.org/index.php/AAAI/article/view/9074). * D'Amour et al. (2014) Alexander D'Amour, Katherine A. Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yi-An Ma, Cory Y. McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. Underspecification presents challenges for credibility in modern machine learning. _CoRR_, abs/2011.03395, 2020. URL [https://arxiv.org/abs/2011.03395](https://arxiv.org/abs/2011.03395). * Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. _CoRR_, abs/1810.04805, 2018. URL [http://arxiv.org/abs/1810.04805](http://arxiv.org/abs/1810.04805). * Gokaslan and Cohen (2019) Aaron Gokaslan and Vanya Cohen. Openwebtext corpus, 2019. URL [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus). * Heckman (1979) James J. Heckman. Sample selection bias as a specification error. _Econometrica_, 47(1):153-161, 1979. ISSN 00129682, 14680262. URL [http://www.jstor.org/stable/1912352](http://www.jstor.org/stable/1912352). * Knox et al. (2020) Dean Knox, Will Lower, and Jonathan Mummolo. Administrative records mask racially biased policing. _American Political Science Review_, 114(3):619-637, 2020. doi: 10.1017/S0003055420000039. * Lebret et al. (2016) Remi Lebret, David Grangier, and Michael Auli. Generating text from structured data with application to the biography domain. _CoRR_, abs/1603.07771, 2016. URL [http://arxiv.org/abs/1603.07771](http://arxiv.org/abs/1603.07771). * Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. _CoRR_, abs/1907.11692, 2019. URL [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692). \\begin{table} \\begin{tabular}{c c c c c c c} \\hline \\hline \\(W\\) Category & Model Type & Model Training Details & \\multicolumn{2}{c}{Female} & \\multicolumn{2}{c}{Male} \\\\ & & & Slope & Pearson’s r & Slope & Pearson’s r \\\\ \\hline \\hline \\multirow{3}{*}{Date} & finetuned & WikBio no Metadata & 0.558 & 0.929 & -0.558 & -0.929 \\\\ & & WikBio w Birthdate & 0.616 & 0.936 & -0.616 & -0.936 \\\\ & pre-trained & BERT large & 0.235 & 0.826 & -0.016 & -0.116 \\\\ & & RoBERTa large & 0.149 & 0.759 & -0.055 & -0.290 \\\\ \\hline \\multirow{3}{*}{Place} & finetuned & WikBio no Metadata & 0.817 & 0.763 & -0.817 & -0.763 \\\\ & & WikBio w Birthplace & 0.591 & 0.752 & -0.591 & -0.752 \\\\ & pre-trained & BERT large & 0.381 & 0.476 & -0.589 & -0.701 \\\\ & & RoBERTa large & 0.277 & 0.724 & -0.247 & -0.525 \\\\ \\hline \\multirow{3}{*}{Subreddit} & finetuned & Reddit TLDR no Metadata & 0.243 & 0.452 & -0.243 & -0.452 \\\\ & & Reddit TLDR w Subreddit & 0.345 & 0.494 & -0.345 & -0.494 \\\\ \\cline{1-1} & & BERT large & **0.006** & **0.049** & **0.016** & **0.071** \\\\ \\cline{1-1} & & RoBERTa large & 0.071 & 0.335 & -0.074 & -0.300 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Slopes and Pearson’s \\(r\\) correlation coefficient for the plots in Figure 2 of spurious correlations between _gender_ and several otherwise gender-neutral categories for the \\(W\\) variable: _date_, _place_ and _subreddit_ channel topics. Values in bold font are for the only experiment in which a model was tested against \\(W\\) variables for which the model under test has no selection bias pressures. * Pearl (2009) Judea Pearl. _Causality_. Cambridge University Press, Cambridge, UK, 2 edition, 2009. ISBN 978-0-521-89560-6. doi: 10.1017/CBO9780511803161. * Rudinger et al. (2018) Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. _CoRR_, abs/1804.09301, 2018. URL [http://arxiv.org/abs/1804.09301](http://arxiv.org/abs/1804.09301). * Volske et al. (2017) Michael Volske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining Reddit to learn automatic summarization. In _Proceedings of the Workshop on New Frontiers in Summarization_, pages 59-63, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4508. URL [https://www.aclweb.org/anthology/W17-4508](https://www.aclweb.org/anthology/W17-4508). * Webster et al. (2020) Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and Slav Petrov. Measuring and reducing gendered correlations in pre-trained models, 2020. URL [https://arxiv.org/abs/2010.06032](https://arxiv.org/abs/2010.06032)."
    },
    {
      "title": "Appendix A Explicitly Gendered Words",
      "text": "See Table 3 for list explicitly gendered words that were masked for prediction during both finetuning and at inference time for the masked gender task."
    },
    {
      "title": "Appendix B \\(W\\) Variable X-Axis Values",
      "text": ""
    },
    {
      "title": "Place Values",
      "text": "Ordered list of bottom 10 and top 10 World Economic Forum Global Gender Gap ranked countries used for the x-axis in Figure 2(b), that were taken directly without modification from [https://www3.weforum.org/docs/WEF_GGGR_2021.pdf](https://www3.weforum.org/docs/WEF_GGGR_2021.pdf): \"Afghanistan\", \"Yemen\", \"Iraq\", \"Pakistan\", \"Syria\", \"Democratic Republic of Congo\", \"Iran\", \"Mali\", \"Chad\", \"Saudi Arabia\", \"Switzerland\", \"Ireland\", \"Lithuania\", \"Rwanda\", \"Namibia\", \"Sweden\", \"New Zealand\", \"Norway\", \"Finland\", \"Iceland\""
    },
    {
      "title": "Subreddit Values",
      "text": "Ordered list of subreddits used for the x-axis in Figure 2(c), that were taken directly without modification from [http://bburky.com/subredditgenderratios/](http://bburky.com/subredditgenderratios/) with minimum subreddit size: 400,000. \"GlobalOffensive\", \"pcmasterrace\", \"nfl\", \"sports\", \"The_Donald\", \"leagueoflegends\", \"Overwatch\", \"gonewild\", \"Futurology\", \"space\", \"technology\", \"gaming\", \"Jokes\", \"dataisbeautiful\", \"woahdude\", \"askscience\", \"wow\", \"anime\", \"BlackPeopleTwitter\", \"polictics\", \"pokemon\", \"woordhews\", \"reddit.com\", \"interestingafsuck\", \"videos\", \"nottheonion\", \"television\", \"science\", \"atheism\", \"movies\", \"gifs\", \"Music\", \"trees\", \"EarthPorn\", \"GetMotivated\", \"pokemongo\", \"news\", \"Fitness\", \"Showstroughs\", \"OldSchoolCoul\", \"explainlikeimfive\", \"todayilearned\", \"gameofthrones\", \"AdviceAnimals\", \"DIY\", \"WTF\", \"IAmA\", \"cringepics\", \"tifu\", \"mildjustreesting\", \"funny\", \"pics\", \"LifeProTips\", \"creepy\", \"personalfinance\", \"food\", \"AskReddit\", \"books\", \"aww\", \"sex\", \"relationships\" \\begin{table} \\begin{tabular}{c c} \\hline \\hline male-variant & FEMALE-variant \\\\ \\hline HE & SHE \\\\ HIM & HER \\\\ HIS & HER \\\\ HHMSELF & HERSELF \\\\ MALE & FEMALE \\\\ MAN & WOMAN \\\\ MEN & WOMEN \\\\ HUSBAND & WIFE \\\\ Father & MOTHER \\\\ boyfriend & gIRLIFI \\\\ BROTHER & SISTER \\\\ ACTOR & ACTRESS \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: List of explicitly gendered words that are masked out for prediction as part of the masked gender task. These words were largely selected for convenience, as each is a single token in both the BERT and RoBERTa tokenizer vocabs, for ease of downstream token to word alignment. During finetuning, it is expected that this list will not fully mask gender in every sample, reducing the underspecification of the learning task and the potential learning of gender-neutral spurious associations to gender. At inference time, it is critical that all gendered words are masked, and because the inference input texts are constructed by a heuristic, this is trivial to achieve."
    }
  ]
}