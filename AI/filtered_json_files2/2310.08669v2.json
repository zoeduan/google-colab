{
  "title": "Multimodal Large Language Model for Visual Navigation",
  "authors": [
    "Yao-Hung Hubert Tsai",
    "Vansh Dhar",
    "Hugues Thomas",
    "Jialu Li",
    "Bowen Zhang",
    "Jian Zhang",
    "† Apple"
  ],
  "abstract": "\n Recent efforts to enable visual navigation using large language models have mainly focused on developing complex prompt systems. These systems incorporate instructions, observations, and history into massive text prompts, which are then combined with pre-trained large language models to facilitate visual navigation. In contrast, our approach aims to fine-tune large language models for visual navigation without extensive prompt engineering. Our design involves a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input. For output, our design provides a probability distribution of possible actions that the agent can take during navigation. We train our model using human demonstrations and collision signals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results demonstrate that our method outperforms state-of-theart behavior cloning methods and effectively reduces collision rates. \n",
  "references": [
    {
      "id": null,
      "title": "Multimodal Large Language Model for Visual Navigation",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Vansh Dhar",
        "Hugues Thomas",
        "Jialu Li",
        "Bowen Zhang",
        "Jian Zhang",
        "† Apple"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Chatgpt as an educational tool: Opportunities, challenges, and recommendations for communication, business writing, and composition courses",
      "authors": [
        "Mohammad Awad Alafnan",
        "Samira Dishari",
        "Marina Jovic",
        "Koba Lomidze"
      ],
      "year": "2023",
      "venue": "Journal of Artificial Intelligence and Technology",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Visual navigation for mobile robots: A survey",
      "authors": [
        "Francisco Bonin-Font",
        "Alberto Ortiz",
        "Gabriel Oliver"
      ],
      "year": "2008",
      "venue": "Journal of intelligent and robotic systems",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
      "authors": [
        "Anthony Brohan",
        "Noah Brown",
        "Justice Carbajal",
        "Yevgen Chebotar",
        "Xi Chen",
        "Krzysztof Choromanski",
        "Tianli Ding",
        "Danny Driess",
        "Avinava Dubey",
        "Chelsea Finn"
      ],
      "year": "2023",
      "venue": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Learning to explore using active neural slam",
      "authors": [
        "Devendra Singh Chaplot",
        "Dhiraj Gandhi",
        "Saurabh Gupta",
        "Abhinav Gupta",
        "Ruslan Salakhutdinov"
      ],
      "year": "2020",
      "venue": "Learning to explore using active neural slam",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Object goal navigation using goal-oriented semantic exploration",
      "authors": [
        "Devendra Singh Chaplot",
        "Dhiraj Prakashchand Gandhi",
        "Abhinav Gupta",
        "Russ R Salakhutdinov"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Semantic curiosity for active visual learning",
      "authors": [
        "Devendra Singh Chaplot",
        "Helen Jiang",
        "Saurabh Gupta",
        "Abhinav Gupta"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Neural topological slam for visual navigation",
      "authors": [
        "Devendra Singh Chaplot",
        "Ruslan Salakhutdinov",
        "Abhinav Gupta",
        "Saurabh Gupta"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "The adoption of self-driving delivery robots in last mile logistics",
      "authors": [
        "Cheng Chen",
        "Emrah Demir",
        "Yuan Huang",
        "Rongzu Qiu"
      ],
      "year": "2021",
      "venue": "Transportation research part E: logistics and transportation review",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "Junyoung Chung",
        "Caglar Gulcehre",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Instructblip: Towards generalpurpose vision-language models with instruction tuning",
      "authors": [
        "Wenliang Dai",
        "Junnan Li",
        "Dongxu Li",
        "Anthony Meng",
        "Huat Tiong",
        "Junqi Zhao",
        "Weisheng Wang",
        "Boyang Li",
        "Pascale Fung",
        "Steven Hoi"
      ],
      "year": "2006",
      "venue": "Instructblip: Towards generalpurpose vision-language models with instruction tuning",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "An image is worth 16x16 words: Trans-formers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Trans-formers for image recognition at scale",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Palme: An embodied multimodal language model",
      "authors": [
        "Danny Driess",
        "Fei Xia",
        "S M Mehdi",
        "Corey Sajjadi",
        "Aakanksha Lynch",
        "Brian Chowdhery",
        "Ayzaan Ichter",
        "Jonathan Wahid",
        "Quan Tompson",
        "Tianhe Vuong",
        "Yu"
      ],
      "year": "2023",
      "venue": "Palme: An embodied multimodal language model",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Lsdslam: Large-scale direct monocular slam",
      "authors": [
        "Jakob Engel",
        "Thomas Schöps",
        "Daniel Cremers"
      ],
      "year": "2014",
      "venue": "European conference on computer vision",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Visual language maps for robot navigation",
      "authors": [
        "Chenguang Huang",
        "Oier Mees",
        "Andy Zeng",
        "Wolfram Burgard"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Robotics and Automation (ICRA)",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Instruct2act: Mapping multimodality instructions to robotic actions with large language model",
      "authors": [
        "Siyuan Huang",
        "Zhengkai Jiang",
        "Hao Dong",
        "Yu Qiao",
        "Peng Gao",
        "Hongsheng Li"
      ],
      "year": "2023",
      "venue": "Instruct2act: Mapping multimodality instructions to robotic actions with large language model",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Reinforcement learning with unsupervised auxiliary tasks",
      "authors": [
        "Max Jaderberg",
        "Volodymyr Mnih",
        "Wojciech",
        "Marian Czarnecki",
        "Tom Schaul",
        "Joel Z Leibo",
        "David Silver",
        "Koray Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "Reinforcement learning with unsupervised auxiliary tasks",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Reinforcement learning: A survey",
      "authors": [
        "Leslie Pack",
        "Kaelbling Michael L Littman",
        "Andrew W Moore"
      ],
      "year": "1996",
      "venue": "Journal of artificial intelligence research",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Rtab-map as an opensource lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation",
      "authors": [
        "Mathieu Labbé",
        "Franc",
        "Michaud"
      ],
      "year": "2019",
      "venue": "Journal of field robotics",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "authors": [
        "Patrick Lewis",
        "Ethan Perez",
        "Aleksandra Piktus",
        "Fabio Petroni",
        "Vladimir Karpukhin",
        "Naman Goyal",
        "Heinrich Küttler",
        "Mike Lewis",
        "Wen-Tau Yih",
        "Tim Rocktäschel"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2006",
      "venue": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "Visual instruction tuning",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Introducing llama: A foundational, 65-billionparameter large language model",
      "authors": [
        "Meta Ai"
      ],
      "year": "2023",
      "venue": "Meta AI",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Navigation",
      "authors": [
        "Daniel R Montello"
      ],
      "year": "2005",
      "venue": "Navigation",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Orb-slam2: An opensource slam system for monocular, stereo, and rgb-d cameras",
      "authors": [
        "Raul Mur",
        "-Artal",
        "Juan D Tardós"
      ],
      "year": "2017",
      "venue": "IEEE transactions on robotics",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Introduction to Simulation and SLAM II",
      "authors": [
        "B Alan",
        "Pritsker"
      ],
      "year": "1984",
      "venue": "Introduction to Simulation and SLAM II",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai",
      "authors": [
        "Aaron Santhosh K Ramakrishnan",
        "Erik Gokaslan",
        "Oleksandr Wijmans",
        "Alex Maksymets",
        "John Clegg",
        "Eric Turner",
        "Wojciech Undersander",
        "Andrew Galuba",
        "Westbury",
        "Angel X Chang"
      ],
      "year": "2021",
      "venue": "Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Pirlnav: Pretraining with imitation and rl finetuning for objectnav",
      "authors": [
        "Ram Ramrakhya",
        "Dhruv Batra",
        "Erik Wijmans",
        "Abhishek Das"
      ],
      "year": "2007",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Charmie: A collaborative healthcare and home service and assistant robot for elderly care",
      "authors": [
        "Tiago Ribeiro",
        "Fernando Gonc ¸alves",
        "S Inês",
        "Gil Garcia",
        "António F Lopes",
        "Ribeiro"
      ],
      "year": "2021",
      "venue": "Applied Sciences",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Habitat: A Platform for Embodied AI Research",
      "authors": [
        "Manolis Savva",
        "Abhishek Kadian",
        "Oleksandr Maksymets",
        "Yili Zhao",
        "Erik Wijmans",
        "Bhavana Jain",
        "Julian Straub",
        "Jia Liu",
        "Vladlen Koltun",
        "Jitendra Malik",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Lmnav: Robotic navigation with large pre-trained models of language, vision, and action",
      "authors": [
        "Dhruv Shah",
        "Błażej Osiński",
        "Sergey Levine"
      ],
      "year": "2023",
      "venue": "Conference on Robot Learning",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Reinforcement learning: An introduction",
      "authors": [
        "S Richard",
        "Andrew G Sutton",
        "Barto"
      ],
      "year": "2018",
      "venue": "Reinforcement learning: An introduction",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Habitat 2.0: Training home assistants to rearrange their habitat",
      "authors": [
        "Andrew Szot",
        "Alex Clegg",
        "Eric Undersander",
        "Erik Wijmans",
        "Yili Zhao",
        "John Turner",
        "Noah Maestre",
        "Mustafa Mukadam",
        "Devendra Chaplot",
        "Oleksandr Maksymets",
        "Aaron Gokaslan",
        "Vladimir Vondrus",
        "Sameer Dharur",
        "Franziska Meier",
        "Wojciech Galuba",
        "Angel Chang",
        "Zsolt Kira",
        "Vladlen Koltun",
        "Jitendra Malik",
        "Manolis Savva",
        "Dhruv Batra"
      ],
      "year": "2005",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Chatgpt is fun, but not an author",
      "authors": [
        "Thorp Holden"
      ],
      "year": "2023",
      "venue": "Chatgpt is fun, but not an author",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Chatgpt for robotics: Design principles and model abilities",
      "authors": [
        "Sai Vemprala",
        "Rogerio Bonatti",
        "Arthur Bucker",
        "Ashish Kapoor"
      ],
      "year": "2023",
      "venue": "Microsoft Auton. Syst. Robot. Res",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames",
      "authors": [
        "Erik Wijmans",
        "Abhishek Kadian",
        "Ari Morcos",
        "Stefan Lee",
        "Irfan Essa",
        "Devi Parikh",
        "Manolis Savva",
        "Dhruv Batra"
      ],
      "year": "2019",
      "venue": "Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
      "authors": [
        "Chenfei Wu",
        "Shengming Yin",
        "Weizhen Qi",
        "Xiaodong Wang",
        "Zecheng Tang",
        "Nan Duan"
      ],
      "year": "2023",
      "venue": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark",
      "authors": [
        "Haoran Wu",
        "Wenxuan Wang",
        "Yuxuan Wan",
        "Wenxiang Jiao",
        "Michael Lyu"
      ],
      "year": "2023",
      "venue": "Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Habitat-matterport 3d semantics dataset",
      "authors": [
        "Karmesh Yadav",
        "Ram Ramrakhya",
        "Santhosh Kumar Ramakrishnan",
        "Theo Gervet",
        "John Turner",
        "Aaron Gokaslan",
        "Noah Maestre",
        "Xuan Angel",
        "Dhruv Chang",
        "Manolis Batra",
        "Savva"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "authors": [
        "Zhengyuan Yang",
        "Linjie Li",
        "Kevin Lin",
        "Jianfeng Wang",
        "Chung-Ching Lin",
        "Zicheng Liu",
        "Lijuan Wang"
      ],
      "year": "2023",
      "venue": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "L3mvn: Leveraging large language models for visual target navigation",
      "authors": [
        "Bangguo Yu",
        "Hamidreza Kasaei",
        "Ming Cao"
      ],
      "year": "2023",
      "venue": "L3mvn: Leveraging large language models for visual target navigation",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "A survey of autonomous driving: Common practices and emerging technologies",
      "authors": [
        "Ekim Yurtsever",
        "Jacob Lambert",
        "Alexander Carballo",
        "Kazuya Takeda"
      ],
      "year": "2020",
      "venue": "IEEE access",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Explicit reasoning in vision-and-language navigation with large language models",
      "authors": [
        "Gengze Zhou",
        "Yicong Hong",
        "Qi Wu",
        "Navgpt"
      ],
      "year": "2007",
      "venue": "Explicit reasoning in vision-and-language navigation with large language models",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2007",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Multimodal Large Language Model For Visual Navigation",
      "text": "Yao-Hung Hubert Tsai\\({}^{\\dagger}\\), Vansh Dhar\\({}^{\\dagger}\\), Hugues Thomas\\({}^{\\dagger}\\), Jialu Li\\({}^{*}\\), Bowen Zhang\\({}^{\\dagger}\\), Jian Zhang\\({}^{\\dagger}\\) \\({}^{\\dagger}\\)Apple, \\({}^{\\ddagger}\\)University of North Carolina, Chapel Hill {yaohung_tsai,v_dhar,thomas23,bowen_zhang4,jianz}@apple.com,jialuli@cs.unc.edu Work done during internship at Apple."
    },
    {
      "title": "Abstract",
      "text": "Recent efforts to enable visual navigation using large language models have mainly focused on developing complex prompt systems. These systems incorporate instructions, observations, and history into massive text prompts, which are then combined with pre-trained large language models to facilitate visual navigation. In contrast, our approach aims to fine-tune large language models for visual navigation without extensive prompt engineering. Our design involves a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input. For output, our design provides a probability distribution of possible actions that the agent can take during navigation. We train our model using human demonstrations and collision signals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results demonstrate that our method outperforms state-of-the-art behavior cloning methods and effectively reduces collision rates."
    },
    {
      "title": "1 Introduction",
      "text": "Visual navigation is a crucial feature for mobile agents, allowing them to process visual inputs and generate corresponding actions [2]. This technology finds applications in various fields, including elder care [30], autonomous driving [44], and logistics delivery [8]. However, solving visual navigation is a complex task that requires a comprehensive understanding of different environments and the implementation of safety measures to protect both the agent and the surrounding objects [25]. In recent years, the emergence of large language models (LLMs) has transformed artificial intelligence and business [11]. These models have found applications in document drafting [1], storytelling [35], grammar checking [40], and more. Researchers have also explored the use of LLMs for visual navigation, focusing on developing complex prompt systems [16, 32, 43, 45]. These systems incorporate instructions, observations, and history into text prompts, which are then combined with pre-trained LLMs to facilitate visual navigation. However, a limitation of this approach is that pre-trained LLMs are typically trained only with text data and may not be best suited for tasks that require an understanding of other modalities [39], such as visual observations, GPS information, and compass data. To address this limitation, recent work has focused on fine-tuning LLMs using additional image-text pairs [10, 22, 23, 46]. This approach enables LLMs to answer questions about images [10, 22, 46] or generate stories that interleave text and images [23]. Building upon this, we propose to fine-tune LLMs specifically for visual navigation using observation-action pairs. During inference, LLMs directly process observations and generate low-level guidelines for the agent to follow, eliminating the need for extensive prompt system design. Our approach involves utilizing a simple text prompt, current observations (including visual inputs, GPS, and compass values), and a history collector model that gathers information from previous observations. These inputs are transformed into prompt tokens, current observation tokens, and history tokens. The large language model then processes these tokens and outputs a probability distribution of possible actions for the agent during navigation. For training, we use human demonstrations on the Habitat-Matterport 3D Dataset (HM3D) [28] to form the probability Figure 1: Our approach leverages a finetune multimodal large language model to solve object goal navigation. of actions based on 1) human-demonstrated actions, 2) action probability distributions from state-of-the-art behavior cloning methods, and 3) collision signals. In our experiments, we compare our approach with state-of-the-art behavior cloning methods and observe significant improvements in object goal navigation. We also find that having the large language model output a probability distribution over actions leads to better performance compared to directly outputting the action itself. Additionally, by considering collision signals during training, we observe a decrease in the number of collisions during visual navigation."
    },
    {
      "title": "2 Related Work",
      "text": "This paper covers a wide range of literature. Below, we will discuss them in various topics. **Visual Navigation.** There are three important components in visual navigation: map building, localization, and path planning [2]. Map building involves the agent creating a map of the environment, localization involves the agent determining its position on the map, and path planning involves the agent deciding its actions based on the current context. In some scenarios where a pre-built map already exists, approaches like RTAB-Map [20] perform localization and path planning. However, in most real-world scenarios, maps are not provided, and SLAM systems [27] offer a solution by simultaneously building maps and performing localization. While classic approaches like orb-SLAM [26] or LSD-SLAM [14] perform well, there is a growing trend of incorporating differentiable models, such as deep neural networks, into SLAM systems [4, 7]. Furthermore, recent work has demonstrated that explicit map building and path planning are not necessary, and directly training reactive policies using recurrent neural networks like GRU [9] can achieve excellent performance [6, 29]. Our method is similar to these approaches, but we use LLMs to train a reactive policy. During inference, our method outputs a probability distribution for the actions, and we select the action with the highest probability. **Large Language Models for Visual Navigation.** There have been several studies on visual navigation using LLMs. LM-Nav [32] utilizes LLMs to extract landmarks from free-form navigation instructions. These landmarks are then passed to a vision-and-language model for grounding and a visual navigation model for navigation planning. L3MVN [43] proposes a method that calculates the entropy of objects in each frontier using a semantic segmentation model. This entropy is represented as query strings, and LLMs are used to determine a more relevant frontier. NavGPT [45] and another recent approach [37] interact with different visual foundation models to handle multi-modal inputs. They also incorporate a history buffer and an LLM summarizer to handle the history, and aggregate information from various sources through a prompt manager. However, these approaches heavily rely on prompt engineering for LLMs and do not fine-tune the LLMs. In contrast, our method does not require extensive prompt engineering and directly fine-tunes LLMs for visual navigation policy. **Multimodal Large Language Models.** Performing visual navigation using LLMs requires LLMs to understand modalities beyond text. In this context, we discuss approaches that involve fine-tuning LLMs with image-text pairs to enhance their visual capabilities. MiniGPT4 [46] proposes fine-tuning the pre-trained Llama [36] model using curated image-text pairs. It utilizes the visual encoder and q-former from BLIP2 [22], adds a trainable linear layer to transform visual features into visual tokens, inserts the visual tokens and text tokens from the text prompt into Llama [36], and conducts the training. InstructBlip [10] extends the idea of MiniGPT4 by training LLMs with high-quality image-text pairs. InstructBlip collects 26 publicly available datasets covering various tasks and capabilities and converts them into an instruction tuning format for fine-tuning LLMs. Similar to MiniGPT4 and InstructBlip, our method involves creating pairs between agent observations and actions, which we use to fine-tune LLMs. We consider observations from the visual image, compass values, and GPS information. Text is used to represent actions, such as \"go forward\" or \"turn right\". **Large Language Models for Robotics.** In this discussion, we explore the use of large language models (LLMs) for general robotics control. Palm-e [13] proposes inputting tokens from various modalities (such as images, neural 3D representations, or states), along with text tokens, into LLMs. The model then generates high-level robotics instructions for tasks such as mobile manipulation, task and motion planning, and tabletop manipulation. In contrast, Instruct2Act [17] generates Python programs that form a complete perception, planning, and action loop for robotic tasks. Moving further, RT-2 [3] generates low-level actions for robots, enabling closed-loop control. While this paper does not solve general-purpose robotics tasks, it focuses on visual navigation, which requires exploration in unseen environments, unlike the tasks studied in these works. It is worth noting that our method aligns with the approach of RT-2, as we generate low-level actions (in the form of a probability distribution) for the robot to execute."
    },
    {
      "title": "3 Proposed Method",
      "text": "Our method involves fine-tuning Large Language Models (LLMs) using pairs of observations and actions from a visual navigation agent. Our proposed architecture does the following: firstly, we have an observation encoding model that converts observations into observation tokens; secondly, we have a history collector model that gathers past observations as history and transforms this information into history tokens. Lastly, we have a pre-trained large language model that takes in text tokens from text prompts, observation tokens, and history tokens. It then outputs a probability distribution over the set of actions. During training, we utilize a human demonstration dataset. We construct the probability distribution using labels from the dataset, action outputs from a state-of-the-art behavior cloning method, and collision signals."
    },
    {
      "title": "Dataset",
      "text": "Our task is object goal navigation, with object categories including \"chair\", \"bed\", \"plant\", \"toilet\", \"tv_monitor\", and \"sofa\". In simple terms, the agent is required to navigate in an environment based on a given object goal. If the agent successfully reaches any objects within a distance of 1 meter that belong to the specified object category, it is considered a success. For fine-tuning, we use the human demonstration dataset curated by a recent work [29] using environments from the Habitat-Matterport 3D Research Dataset (HM3D) [28] within the Habitat-sim simulator [31]. The dataset contains \\(77\\)k human demonstrations from \\(80\\) training scenes. In the Habitat-sim simulator [31, 34], we consider the following observations: RGB visual image, compass values, and GPS values. The available actions are: \"stop\", \"go forward by 25 centimeters\", \"turn right by 30 degrees\", \"turn left by 30 degrees\", \"look up by 30 degrees\", and \"look down by 30 degrees\". The simulator also provides collision information. It is important to note that when a collision occurs, the agent remains at the same location in the simulator."
    },
    {
      "title": "Architecture",
      "text": "We present the architecture in Figure 2. The architecture consists of five core modules: the history collector model, the observation encoding model, the projection layers, the text prompt, and the pre-trained large language model. The history collector model is responsible for encoding historical features from the current observation and past history. The observation encoding model encodes observation features. The projection layer transforms history tokens and observation tokens from history and observation features, respectively. The text prompt is used to provide hints to the large language models (LLMs) for visual navigation. The prompt is transformed into text tokens using a text tokenizer. The pre-trained large language model takes text tokens, history tokens, and observation tokens as input and generates a probability distribution over a set of actions as text output. Figure 2: Architecture for fine-tuning large language models for visual navigation. The history collector model is responsible for encoding history features from the current observation and past history. The observation encoding model encodes observation features. The projection layer transforms history tokens and observation tokens from history and observation features, respectively. The text prompt is used to provide hints to the large language models (LLMs) for visual navigation. The pre-trained large language model takes text tokens, history tokens, and observation tokens as input, and generates a probability distribution over a set of actions as text output."
    },
    {
      "title": "3.2.1 History Collector Model",
      "text": "The history collector model generates history features that carry meaningful information from the beginning of a training episode to the current time step. After evaluating different model combinations, we use ResNet-50 [15] to encode visual features. Linear layers are used to encode GPS, compass values, previous action, and object category into corresponding features. These features are then transformed into history features using GRUs [9]. We pretrain the history collector model using the behavior cloning method described in the paper [29] on the human demonstration dataset. It is important to note that the history collector model can be seen as a standalone model that generates actions from observations. We will provide further details on its performance in the experimental section. During the fine-tuning of the large language model with visual navigation, the weights of this history collector model remain fixed."
    },
    {
      "title": "3.2.2 Observation Encoding Model",
      "text": "We utilize the pre-trained ViT [12] and the Q-former [22] from the BLIP-2 model [22] as our observation encoding model for encoding visual images into our observation features. It is important to note that we have not observed any benefits in including information from GPS, compass values, or previous actions in our observation features. We believe this is because GPS, compass values, and previous actions are only meaningful when considered as a sequence. However, since our observation encoding model is designed to process only the current observation, we have chosen to include only the current visual image. We keep the weights of the observation encoding model fixed when fine-tuning the large language model for visual navigation."
    },
    {
      "title": "3.2.3 Projection Layers",
      "text": "During the fine-tuning process, only the projection layers are trained. We use the Q-former [22] followed by a linear layer to project history features into 32 history tokens. Additionally, we use a linear layer to project observation features into 32 observation tokens."
    },
    {
      "title": "3.2.4 Text Prompt",
      "text": "We provide a list of text prompts which are paraphrased with each other (using ChatGPT for paraphrasing). An example of the text prompt is: _\"Imagine you are a robot, and you are navigating to find \\(\\langle\\) Goal \\(\\rangle\\) \\(\\langle\\) GoalHere \\(\\rangle\\) \\(\\langle\\) Goal \\(\\rangle\\). With current observation \\(\\langle\\) Img \\(\\rangle\\) \\(\\langle\\) ImageHere \\(\\rangle\\) \\(\\langle\\) / Img \\(\\rangle\\), history tokens \\(\\langle\\) History \\(\\rangle\\) \\(\\langle\\) History Here \\(\\rangle\\) \\(\\langle\\) / History \\(\\rangle\\), and suggested actions probabilities \\(\\langle\\) ActionProb \\(\\rangle\\) \\(\\langle\\) ActionProbHere \\(\\rangle\\) \\(\\langle\\) ActionProb \\(\\rangle\\), please plan out your following action.\"_ In this text prompt, \\(\\langle\\) GoalHere \\(\\rangle\\) represents the object category. \\(\\langle\\) ImageHere \\(\\rangle\\) represents the observation tokens. \\(\\langle\\) HistoryHere \\(\\rangle\\) represents the history tokens. \\(\\langle\\) ActionProbHere \\(\\rangle\\) represents the action probability output from the history collector model (See Section 3.2.1). We present it via text, and an example for it is _\"Stop with probability 0.03, move forward with probability 0.44, turn left with probability 0.28, turn right with probability 0.21, look up with probability 0.03, and look down with probability 0.01\"_"
    },
    {
      "title": "3.2.5 Pre-Trained Large Language Model",
      "text": "We consider the pre-trained LLama-13B model [24] as a large language model that has its weights fixed during the fine-tuning process."
    },
    {
      "title": "Fine-Tuning Paradigm",
      "text": "We perform \\(80\\)k iterations for fine-tuning, with each iteration considering a batch size of \\(6\\) observation-action pairs. Each episode in the human demonstration contains around \\(50\\) to \\(100\\) time steps. Therefore, the fine-tuning is conducted over \\(4.8\\)k to \\(9.6\\)k episodes out of the total of \\(77\\)k episodes in our dataset. For the output, we perform the following steps to construct the probability over the set of actions. Firstly, we use the state-of-the-art (SOTA) behavior cloning method from the paper [29] to compute the output probability over the actions, starting from the beginning of an episode until the current observation. We denote this probability as \\(P_{\\mathrm{SOTA}}\\). Secondly, we construct a one-hot probability vector from the ground truth human action for the current observation. We denote this probability as \\(P_{\\mathrm{gt}}\\). Thirdly, we merge these two probabilities using hyperparameters 0.8 and 0.2 (we select the combination that yields the best result). We also zero out the actions that cause collisions. Finally, we renormalize the probability. The equation can be formulated as: \\[P:=\\mathrm{Collision\\ check\\ and\\ Renorm}(0.8P_{\\mathrm{SOTA}}+0.2P_{\\mathrm{ gt}}).\\] An example of the text output for \\(P\\) is _\"Stop with probability 0.03, move forward with probability 0.55, turn left with probability 0.38, turn right with probability 0.00, look up with probability 0.03, and look down with probability 0.01\"_ In this example, the action \"turn right\" has a probability of \\(0.00\\) because we set it to zero due to a detected collision."
    },
    {
      "title": "4 Experiments",
      "text": "We evaluate our method and compare it with baseline approaches on the Habitat-Matterport 3D Research Dataset (HM3D) [34]. We use the validation split from the HM3D-Semantics dataset [41], which consists of \\(20\\) validation scenes. Following the evaluation pipeline of the work [29], we report metrics on \\(2\\)k episodes. Our task is object goal navigation, where our agent starts at a random point within an indoor environment, and explores the environment until it reaches an object of a given object category (within \\(1\\) meter distance). The exploration is limited to \\(500\\) actions. **Metrics.** We report two metrics: success rate (Success) and soft success rate weighted by path length (SoftSPL). The Success measures the agent's ability to locate the target object goal within the allocated limit of permissible actions. Let \\(d_{\\mathrm{init}}\\) and \\(d_{\\mathrm{T}}\\) denote the geodesic distances to the target upon episode start and termination. The SoftSPL for an episode is defined as: \\(\\mathrm{SoftSPL}=\\left(1-\\frac{d_{\\mathrm{T}}}{d_{\\mathrm{init}}}\\right)\\cdot \\left(\\frac{s}{\\max(s,p)}\\right)\\), where \\(s\\) and \\(p\\) are the lengths of the shortest path and the path taken by the agent. **Baselines.** We compare four groups of baselines to evaluate our method. In the first group, we compare our method with non-behavior cloning methods. Specifically, we select two representative baselines: reinforcement learning (RL) [41] and Goal-Oriented Semantic Exploration (SemExp) [5]. The RL baseline is trained using the DDPPO [38] method without human demonstrations. On the other hand, SemExp constructs a top-down semantic map by combining the first-person semantic segmentation predictions with depth. It determines an exploration objective by considering the semantic map and the target object using a trained exploration policy. Furthermore, SemExp devises low-level actions to achieve this objective. For the second group, we compare our method with state-of-the-art behavior cloning methods. We consider two baselines from the paper [29]: IL and RL_Ft. IL [29] stands for imitation learning, which is learned purely based on behavior cloning. RL_Ft [29] performs fine-tuning with reinforcement learning on top of the IL method. Note that, in Section 3.2.1, we pre-train our history collector model using the behavior cloning method. Hence, another way to understand our history collector is as the IL [29] method. For the third group, we compare our method with three variants. The first variant involves using the pre-trained multimodal large language model (referred to as \\(\\mathrm{LLM}_{\\mathrm{no~{}ft}}\\)) without fine-tuning to directly output the action for the agent based on the text prompt. We adopt MiniGPT4 [46] for this variant. The second variant replaces the history collector model with \\(15\\) consecutive observations (including images, GPS, compass values, and previous actions) to fine-tune LLMs, denoted as \\(\\mathrm{LLM}_{\\mathrm{consecutive~{}obs}}\\). In the third variant, instead of providing a probability output over actions, the model directly predicts the action itself, denoted as \\(\\mathrm{LLM}_{\\mathrm{direct~{}action}}\\). For the fourth group, we aim to compare the impact of the collision check in our fine-tuning stage. In Section 3.3, our method sets the probability to zero for the action that leads to a collision during training. In this case, we introduce a variant for the baseline that does not include a collision check, denoted as \\(\\mathrm{LLM}_{\\mathrm{no~{}collision~{}check}}\\). As a summary, we compare our method with RL, SemExp, IL, RL_Ft, \\(\\mathrm{LLM}_{\\mathrm{no~{}ft}}\\), \\(\\mathrm{LLM}_{\\mathrm{consecutive~{}obs}}\\), \\(\\mathrm{LLM}_{\\mathrm{direct~{}action}}\\), and \\(\\mathrm{LLM}_{\\mathrm{no~{}collision~{}check}}\\) approaches. IL and RL_Ft are SOTA behavior cloning approaches, and the latter four are variants of our method."
    },
    {
      "title": "Comparisons With Non Behavior Cloning Methods",
      "text": "Here, we compare methods with and without human demonstrations. Specifically, in Table 1, we compare RL and SemExp with IL, RL_Ft, and Ours. We observe that the methods trained without human demonstrations perform worse than the methods trained with human demonstrations. It is undeniable that human demonstrations provide us with exceptionally valuable information and can elevate the performance of models to the next level."
    },
    {
      "title": "Comparisons With Sota Behavior Cloning Methods",
      "text": "In this section, we compare our method with IL [29] and FL_Ft [29] approaches. Both our method and the baselines utilize behavior cloning. The difference is that IL [29] and RL_Ft [29] approaches are trained with non-large language models, while ours is fine-tuned using large language models. We present the results in Table 1. First, we observe that RL_Ft outperforms IL in terms of performance. It is important to note that IL is a pure imi \\begin{table} \\begin{tabular}{c|c c} \\hline \\hline Methods & Success (\\(\\uparrow\\)) & Soft SPL (\\(\\uparrow\\)) \\\\ \\hline \\hline \\multicolumn{3}{c}{_Non Behavior Cloning and No Large Language Models_} \\\\ \\hline \\hline RL [41] & 0.3936 & - \\\\ SemExp [5] & 0.5560 & - \\\\ \\hline \\hline \\multicolumn{3}{c}{_Behavior Cloning without Large Language Models_} \\\\ \\hline \\hline IL [29] & 0.5980 & 0.3051 \\\\ RL_Ft [29] & 0.6615 & 0.3604 \\\\ \\hline \\hline \\multicolumn{3}{c}{_Behavior Cloning with Large Language Models_} \\\\ \\hline \\hline Ours & **0.6790** & **0.3723** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Quantitative results for the comparisons among non-behavior-cloning methods, state-of-the-art behavior cloning methods, and our approach. tation learning approach, while RL_Ft is fine-tuned on top of IL using reinforcement learning. Therefore, we can conclude that reinforcement learning fine-tuning is beneficial. Second, we discover that our approach surpasses both IL and RL_Ft, demonstrating the potential of LLMs to enhance visual navigation."
    },
    {
      "title": "Comparisons With Llms Variants",
      "text": "We present results that compare different variants of our approach using Language Models (LLMs). The results are shown in Table 2. The discussions in this section revolve around answering the following questions: \"_Does fine-tuning matter?_\", \"_Does the history collector help?_\", and \"_Direct action output or probability output?_\". **Does fine-tuning matter?_** To address this question, we compare the results of \\(\\mathrm{LLM}_{\\mathrm{no~{}ft}}\\) with other approaches that involve fine-tuning of LLMs. We find that \\(\\mathrm{LLM}_{\\mathrm{no~{}ft}}\\) performs poorly, with a success rate of \\(0\\%\\). However, any method of fine-tuning can significantly improve visual navigation performance. It is important to note that \\(\\mathrm{LLM}_{\\mathrm{no~{}ft}}\\) directly relies on a pre-trained large language model for action prediction, without using a history collector or performing fine-tuning. This approach is similar to zero-shot visual question answering experiments conducted in recent multimodal large language model research [3, 10, 13, 17, 22, 46]. These studies reported success in those experiments. Therefore, the fact that zero-shot visual navigation in unfamiliar environments produces poor results indicates that visual navigation is a much more challenging problem compared to zero-shot visual question answering. Hence, fine-tuning is needed. **Does the history collector help?** To answer this question, we compare \\(\\mathrm{LLM}_{\\mathrm{consecutive~{}obs}}\\) and \\(\\mathrm{LLM}_{\\mathrm{direct~{}action}}\\). The difference between these two approaches is that \\(\\mathrm{LLM}_{\\mathrm{consecutive~{}obs}}\\) considers input from 15 consecutive observations, while \\(\\mathrm{LLM}_{\\mathrm{direct~{}action}}\\) uses a history collector model to summarize all the information of the observations from the start of the episode until the current observation. We can clearly see a significant performance improvement from \\(\\mathrm{LLM}_{\\mathrm{consecutive~{}obs}}\\) to \\(\\mathrm{LLM}_{\\mathrm{direct~{}action}}\\), suggesting the benefits of using the history collector model. **Direct action output or probability output?** To answer this question, we compare \\(\\mathrm{LLM}_{\\mathrm{direct~{}action}}\\) and our approach. The main difference between these two approaches is that \\(\\mathrm{LLM}_{\\mathrm{direct~{}action}}\\) directly produces an action as output, whereas our approach outputs a probability distribution over all possible actions. Our results show that our approach significantly outperforms \\(\\mathrm{LLM}_{\\mathrm{direct~{}action}}\\), which suggests that, for visual navigation, generating probabilities as a form of uncertainty modeling is crucial. We also provide qualitative results comparing these two approaches in Figure 3. The results show that our method has better path planning, with less turning and improved navigation in narrow aisles, compared to \\(\\mathrm{LLM}_{\\mathrm{direct~{}action}}\\)."
    },
    {
      "title": "Comparisons On Collision Check",
      "text": "Collision avoidance is crucial for visual navigation. In this section, we compare our method with a variant that does not include collision check (\\(\\mathrm{LLM}_{\\mathrm{no~{}collision~{}check}}\\)). In Section 3.3, we specify that during training, we zero out the action that leads to a collision. \\(\\mathrm{LLM}_{\\mathrm{no~{}collision~{}check}}\\) simply removes this zeroing-out step. The results are reported in Table 3. Based on the numbers, it is evident that the collision check has a positive impact on all the metrics. It improves \\begin{table} \\begin{tabular}{c|c c} \\hline \\hline Methods & Success (\\(\\uparrow\\)) & Soft SPL (\\(\\uparrow\\)) \\\\ \\hline \\hline \\multicolumn{3}{c}{_without Large Language Models Fine-tuning_} \\\\ \\hline \\hline \\(\\mathrm{LLM}_{\\mathrm{no~{}ft}}\\) & 0.0000 & 0.0506 \\\\ \\hline \\hline \\multicolumn{3}{c}{_with Large Language Models Fine-tuning_} \\\\ \\hline \\hline \\(\\mathrm{LLM}_{\\mathrm{consecutive~{}obs}}\\) & 0.0910 & 0.0977 \\\\ \\(\\mathrm{LLM}_{\\mathrm{direct~{}action}}\\) & 0.4610 & 0.2616 \\\\ Ours & **0.6790** & **0.3723** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Quantitative results for the comparisons among variants of our approach. Figure 4: Qualitative results for comparing LLMs fine-tuned with visual navigation between with and without collision check. We show the results on the same scene, same initial location, and the same target object goal. Figure 3: Qualitative results for comparing LLMs fine-tuned with visual navigation between direct action output and probability output. We show the results on the same scene, same initial location, and the same target object goal. the success rate, enhances the SoftSPL, and reduces the collision count. These results indicate the significance of collision avoidance in visual navigation. Moving forward, our future work involves exploring and developing a more effective collision avoidance mechanism during the fine-tuning of LLMs. We also provide qualitative results comparing these two approaches in Figure 4. The results indicate that the method can achieve fewer collisions when a collision check is performed."
    },
    {
      "title": "5 Discussion And Future Work",
      "text": "This work tackles the challenge of multimodal large language modeling with partial observation. In particular, the multimodal large language model is only able to access a limited portion of the overall environment and needs to navigate and explore the entire environment in order to complete tasks. In other words, our work involves addressing the setup of using multimodal large language models for long-horizon tasks. This approach is different from existing work on multimodal large language models, which usually assume full or nearly full observation. For example, previous work focuses on tasks like visual question answering given a specific image [46] or region grounding on a given image [42]. We argue that our work is the first to adopt multimodal large language models with partial observations, using visual navigation as a prime example. Other examples include long video generation, Atari game playing, and search and rescue operations. Due to the limited context length in large language models, it is not possible to directly feed all the information into the models. Therefore, a memory module is necessary to interact with the large language models. In the following, we present several potential solutions for adopting multimodal large language models with partial observations, using visual navigation as an example to illustrate these solutions. **Text-based RAG with Text Prompts.** One example of a memory module is extensive text-based prompt engineering system [45]. The large language models retrieve relevant contents from the extensive prompt engineering, incorporate them into the input, and generate a response based on the input. This process is known as augmented retrieval generation (RAG) [21]. While RAG has proven to be powerful for pure-text tasks, its performance with multimodal context has not been thoroughly studied. Representing all the multimodal context information directly in text may seem like an obvious solution, but prior work on visual navigation has shown that this approach is suboptimal [45]. **Multimodal RAG with Multimodal Prompts.** In this approach, we create explicit memory with multimodal context. We retrieve relevant multimodal content from the memory and use it as a prompt for the large language models. For visual navigation, we can explore the idea of creating an SLAM (Simultaneous Localization And Mapping) system that generates maps in real-time. These maps can then be used as the relevant multimodal content. However, in order for the large language models to understand how to interpret maps, a fine-tuning process is necessary. **Multimodal Implicit Memory Module with Implicit Features Prompts.** In this approach, we do not focus on forming an explicit memory. Instead, we utilize neural networks as an implicit memory module to condense all past information into a fixed-dimensional feature. This implicit feature is then fed into the large language models. The advantage of this approach is its simplicity, as there is no need to select the most appropriate multimodal content for the language models. However, the effectiveness of this approach heavily relies on the design and training of the implicit memory module (neural networks). Our paper follows this approach, using GRUs as the implicit memory module and training them with the same dataset as the large language models. Lastly, similar to the multimodal RAG with multimodal prompts, fine-tuning of the large language models is necessary to understand the implicit feature as a prompt."
    },
    {
      "title": "More On Data",
      "text": "In this paper, we propose using human demonstrations as the training data for multimodal large language models with partial observations. Human demonstrations have the advantage of being high quality and low noise. However, collecting human demonstrations can be expensive, so it is important to consider other data sources as well. In the context of visual navigation, prior work [29] also explores using _shortest path_ and _frontier exploration_ as data sources. These data are easier to collect since they can be automatically gathered without human intervention. However, the quality of these data is not guaranteed, resulting in models trained with these data performing less favorably compared to models trained with human demonstrations. Taking inspiration from Tesla's data collection efforts, we argue the best approach is to curate human demonstrations with extensive data augmentations from simulation."
    },
    {
      "title": "More On Training",
      "text": "The concept of learning with partial observations or learning with long horizon tasks is often discussed in the rein \\begin{table} \\begin{tabular}{c|c c} \\hline \\hline Methods & \\(\\mathrm{LLM_{no~{}collision~{}check}}\\) & Ours \\\\ \\hline Success (\\(\\uparrow\\)) & 0.6510 & **0.6790** \\\\ Soft SPL (\\(\\uparrow\\)) & 0.3641 & **0.3723** \\\\ Collision Count (\\(\\downarrow\\)) & 39.7755 & **27.7615** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Quantitative results for the comparisons between our approach with and without collision check. forcement learning (RL) literature [33]. Therefore, in addition to the behavior cloning algorithm, RL algorithms can be a potential alternative for training or fine-tuning large language models. However, there is currently no evidence to suggest that RL algorithms can effectively work with large language models. The main challenge for RL in training large language models is the sparse nature of the supervision signals. We argue that training large language models requires strong, dense, and semantically meaningful supervision signals. We demonstrate an example of dense and semantically meaningful supervision signals in our paper, where the output for the large language models is designed to be a probability distribution over all possible actions. To enable RL training with large language models, we need to convert sparse supervision signals into dense and semantically meaningful signals. However, this problem remains unsolved in the RL community [19]. One potential workaround is to consider unsupervised auxiliary tasks [18], such as predicting the next action or predicting the next input. In summary, we believe that RL can be a potential method for training and fine-tuning multimodal large language models with partial observations. However, there is still a long way to go, and significant efforts are required to address the challenges."
    },
    {
      "title": "What'S The Next Step?",
      "text": "So far, we have discussed several solutions, data, and training methods for adopting multimodal large language models with partial observations. For our next step, we plan to investigate the following: 1) multimodal RAG with multimodal prompts, 2) data augmentations on human demonstration data using simulators, and 3) exploring other datasets or tasks."
    },
    {
      "title": "6 Conclusion",
      "text": "In this paper, we explore the fine-tuning of Large Language Models (LLMs) for visual navigation. Unlike previous work, which focuses on complex prompt engineering for visual navigation using LLMs, our approach is simple. We use a basic text prompt, a history collector model that incorporates tokens from past observations, an observation encoding model that embeds observation tokens, and a pre-trained large language model. During training, we employ two tricks based on human demonstrations. First, instead of directly outputting the action for the agent, we output the probability distribution over all possible actions. Second, we construct this probability distribution using a state-of-the-art behavior cloning method, the action demonstrated by a human, while avoiding actions that cause collisions. We believe that our work highlights the advantages of fine-tuning LLMs for visual navigation. Our experimental results support this claim, as our approach outperforms state-of-the-art methods."
    },
    {
      "title": "References",
      "text": "* [1] Mohammad Awad AlAfnan, Samira Dishari, Marina Jovic, and Koba Lomidze. Chatgpt as an educational tool: Opportunities, challenges, and recommendations for communication, business writing, and composition courses. _Journal of Artificial Intelligence and Technology_, 3(2):60-68, 2023. * [2] Francisco Bonin-Font, Alberto Ortiz, and Gabriel Oliver. Visual navigation for mobile robots: A survey. _Journal of intelligent and robotic systems_, 53:263-296, 2008. * [3] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. _arXiv preprint arXiv:2307.15818_, 2023. * [4] Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, and Ruslan Salakhutdinov. Learning to explore using active neural slam. _arXiv preprint arXiv:2004.05155_, 2020. * [5] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ R Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. _Advances in Neural Information Processing Systems_, 33:4247-4258, 2020. * [6] Devendra Singh Chaplot, Helen Jiang, Saurabh Gupta, and Abhinav Gupta. Semantic curiosity for active visual learning. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VI 16_, pages 309-326. Springer, 2020. * [7] Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, and Saurabh Gupta. Neural topological slam for visual navigation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12875-12884, 2020. * [8] Cheng Chen, Emrah Demir, Yuan Huang, and Rongzu Qiu. The adoption of self-driving delivery robots in last mile logistics. _Transportation research part E: logistics and transportation review_, 146:102214, 2021. * [9] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. _arXiv preprint arXiv:1412.3555_, 2014. * [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. * [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018. * [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020. * [13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023. * [14] Jakob Engel, Thomas Schops, and Daniel Cremers. LSM-slam: Large-scale direct monocular slam. In _European conference on computer vision_, pages 834-849. Springer, 2014. * [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016. * [16] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 10608-10615. IEEE, 2023. * [17] Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li. Instruct2act: Mapping multi-modality instructions to robotic actions with large language model. _arXiv preprint arXiv:2305.11176_, 2023. * [18] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. _arXiv preprint arXiv:1611.05397_, 2016. * [19] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. _Journal of artificial intelligence research_, 4:237-285, 1996. * [20] Mathieu Labbe and Francois Michaud. Rtab-map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation. _Journal of field robotics_, 36(2):416-446, 2019. * [21] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459-9474, 2020. * [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023. * [23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023. * [24] AI Meta. Introducing llama: A foundational, 65-billion-parameter large language model. _Meta AI. https://ai. facebook. com/blog/large-language-model-llama-meta-ai_, 2023. * [25] Daniel R Montello. _Navigation_. Cambridge University Press, 2005. * [26] Raul Mur-Artal and Juan D Tardos. Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. _IEEE transactions on robotics_, 33(5):1255-1262, 2017. * [27] A Alan B Pritsker. _Introduction to Simulation and SLAM II_. Halsted Press, 1984. * [28] Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. _arXiv preprint arXiv:2109.08238_, 2021. * [29] Rammakhya, Dhruv Batra, Erik Wijmans, and Abhishek Das. Prlnav: Pretraining with imitation and rl finetuning for objectnav. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17896-17906, 2023. * [30] Tiago Ribeiro, Fernando Goncalves, Ines S Garcia, Gil Lopes, and Antonio F Ribeiro. Charmie: A collaborative healthcare and home service and assistant robot for elderly care. _Applied Sciences_, 11(16):7248, 2021. * [31] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI Research. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2019. * [32] Dhruv Shah, Blazej Osinski, Sergey Levine, et al. Lmnav: Robotic navigation with large pre-trained models of language, vision, and action. In _Conference on Robot Learning_, pages 492-504. PMLR, 2023. * [33] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018. * [34] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021. * [35] H Holden Thorp. Chatgpt is fun, but not an author, 2023. * [36] Hugo Touvron, Thibaut Lavril, Gautier Izcard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023. * [37] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt for robotics: Design principles and model abilities. _Microsoft Auton. Syst. Robot. Res_, 2:20, 2023. * [38] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. _arXiv preprint arXiv:1911.00357_, 2019. * [39] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. _arXiv preprint arXiv:2303.04671_, 2023. * [40] Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, and Michael Lyu. Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark. _arXiv preprint arXiv:2303.13648_, 2023. * [41] Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Theo Gervet, John Turner, Aaron Gokaslan, Noah Maestre, Angel Xuan Chang, Dhruv Batra, Manolis Savva, et al. Habitat-matterport 3d semantics dataset. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4927-4936, 2023. * [42] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of Imms: Preliminary explorations with gpt-4v (ision). _arXiv preprint arXiv:2309.17421_, 2023. * [43] Bangguo Yu, Hamidreza Kasaei, and Ming Cao. L3mvn: Leveraging large language models for visual target navigation. _arXiv preprint arXiv:2304.05501_, 2023. * [44] Ekim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda. A survey of autonomous driving: Common practices and emerging technologies. _IEEE access_, 8:58443-58469, 2020. * [45] Genze Zhou, Yicong Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language navigation with large language models. _arXiv preprint arXiv:2305.16986_, 2023. * [46] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023."
    }
  ]
}