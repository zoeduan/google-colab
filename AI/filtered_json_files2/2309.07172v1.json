{
  "title": "Exploring Large Language Models for Ontology Alignment",
  "authors": [
    "Yuan He",
    "Jiaoyan Chen",
    "Hang Dong",
    "Ian Horrocks"
  ],
  "abstract": "\n This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot 1 performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design. 2    \n",
  "references": [
    {
      "id": null,
      "title": "Exploring Large Language Models for Ontology Alignment",
      "authors": [
        "Yuan He",
        "Jiaoyan Chen",
        "Hang Dong",
        "Ian Horrocks"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "BERTMap: A BERT-based ontology alignment system",
      "authors": [
        "Y He",
        "J Chen",
        "D Antonyrajah",
        "I Horrocks"
      ],
      "year": "2022",
      "venue": "BERTMap: A BERT-based ontology alignment system",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Truveta mapper: A zero-shot ontology alignment framework",
      "authors": [
        "M Amir",
        "M Baruah",
        "M Eslamialishah",
        "S Ehsani",
        "A Bahramali",
        "S Naddaf-Sh",
        "S Zarandioon"
      ],
      "year": "2023",
      "venue": "Truveta mapper: A zero-shot ontology alignment framework",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "H W Chung",
        "L Hou",
        "S Longpre",
        "B Zoph",
        "Y Tay",
        "W Fedus",
        "E Li",
        "X Wang",
        "M Dehghani",
        "S Brahma"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Machine learning-friendly biomedical datasets for equivalence and subsumption ontology matching",
      "authors": [
        "Y He",
        "J Chen",
        "H Dong",
        "E Jiménez-Ruiz",
        "A Hadian",
        "I Horrocks"
      ],
      "year": "2022",
      "venue": "Machine learning-friendly biomedical datasets for equivalence and subsumption ontology matching",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Deeponto: A python package for ontology engineering with deep learning",
      "authors": [
        "Y He",
        "J Chen",
        "H Dong",
        "I Horrocks",
        "C Allocca",
        "T Kim",
        "B Sapkota"
      ],
      "year": "2023",
      "venue": "Deeponto: A python package for ontology engineering with deep learning",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Exploring Large Language Models For Ontology Alignment",
      "text": "Yuan He The term \"zero-shot\" in the context of LLMs usually refers to using the pre-trained LLMs without fine-tuning. Jiaoyan Chen Hang Dong The term \"zero-shot\" in the context of LLMs usually refers to using the pre-trained LLMs without fine-tuning. Ian Horrocks The term \"zero-shot\" in the context of LLMs usually refers to using the pre-trained LLMs without fine-tuning."
    },
    {
      "title": "Abstract",
      "text": "This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot1 performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design.2 O 2023 Copyright © 2023 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). O 2023 O 2023 Copyright © 2023 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)."
    },
    {
      "title": "1 Introduction",
      "text": "Ontology alignment, also known as ontology matching (OM), is to identify semantic correspondences between ontologies. It plays a crucial role in knowledge representation, knowledge engineering and the Semantic Web, particularly in facilitating semantic interoperability across heterogeneous sources. This study focuses on _equivalence matching for named concepts_. Previous research has effectively utilised pre-trained language models like BERT and T5 for OM [1, 2], but recent advancements in large language models (LLMs) such as ChatGPT [3] and Flan-T5 [4] necessitate further exploration. These LLMs, characterised by larger parameter sizes and task-specific fine-tuning, are typically guided by task-oriented prompts in a zero-shot setting or a small set of examples in a few-shot setting when applying to downstream tasks. This work explores the feasibility of employing LLMs for zero-shot OM. Given the significant computational demands of LLMs, it is crucial to conduct experiments with smaller yet representative datasets before full deployment. To this end, we extract two challenging subsets from the NCIT-DOID and the SNOMED-FMA (Body) equivalence matching datasets, both part of Bio-ML1[5] - a track of the Ontology Alignment Evaluation Initiative (OAEI) that is compatible with machine learning-based OM systems. Notably, the extracted subsets exclude \"easy\" mappings, i.e., concept pairs that can be aligned through string matching. We mainly evaluate the open-source LLM, Flan-T5-XXL, the largest version of Flan-T5 containing 11B parameters [4]. We assess its performance factoring in the use of concept labels, score thresholding, and structural contexts. For baselines, we adopt the previous top-performing OM system BERTMap and its lighter version, BERTMapLt. Preliminary tests are also conducted on GPT-3.5-turbo; however, due to its high cost, only initial results are reported. Our findings suggest that LLM-based OM systems hold the potential to outperform existing ones, but require efforts in prompt design and exploration of optimal presentation methods for ontology contexts."
    },
    {
      "title": "2 Methodology",
      "text": "Task DefinitionThe task of OM can be defined as follows. Given the source and target ontologies, denoted as \\(\\mathcal{O}_{src}\\) and \\(\\mathcal{O}_{tgt}\\), and their respective sets of named concepts \\(\\mathcal{C}_{src}\\) and \\(\\mathcal{C}_{tgt}\\), the objective is to generate a set of mappings in the form of \\((c\\in\\mathcal{C}_{src},c^{\\prime}\\in\\mathcal{C}_{tgt},s_{c\\equiv c^{\\prime}})\\), where \\(c\\) and \\(c^{\\prime}\\) are concepts from \\(\\mathcal{C}_{src}\\) and \\(\\mathcal{C}_{tgt}\\), respectively, and \\(s_{c\\equiv c^{\\prime}}\\in[0,1]\\) is a score that reflects the likelihood of the equivalence \\(c\\equiv c^{\\prime}\\). From this definition, we can see that a paramount component of an OM system is its mapping scoring function \\(s:\\mathcal{C}_{src}\\times\\mathcal{C}_{tgt}\\rightarrow[0,1]\\). In the following, we formulate a sub-task for LLMs regarding this objective. Concept IdentificationThis is essentially a binary classification task that determines if two concepts, given their names (multiple labels per concept possible) and/or additional structural contexts, are identical or not. As LLMs typically work in a chat-like manner, we need to provide a task prompt that incorporates the available information of two input concepts, and gather classification results from the responses of LLMs. To avoid excessive prompt engineering, we present the task description (as in previous sentences) and the available input information (such as concept labels and structural contexts) to ChatGPT based on GPT-42, and ask it to generate a task prompt for an LLM like itself. The resulting template is as follows: Footnote 2: ChatGPT (GPT-4 version): [https://chat.openai.com/?model=gpt-4](https://chat.openai.com/?model=gpt-4) Given the lists of names _and hierarchical relationships_ associated with two concepts, your task is to determine whether these concepts are identical or not. Consider the following: Source Concept Names: -list of concept names> _Parent Concepts of the Source Concept: -list of concept names>_ _Child Concepts of the Source Concept: -list of concept names>_ ... (same for the target concept) Analyze the names _and the hierarchical information_ provided for each concept and provide a conclusion on whether these two concepts are identical or different (\"Yes\" or \"No\") based on their associated names _and hierarchical relationships_. where the _italic_ part is generated in the second round when we inform ChatGPT parent/child contexts can be considered. Since the prompt indicates a yes/no question, we anticipate the generation of \"Yes\" or \"No\" tokens in the LLM responses. For simplicity, we use the generation probability of the \"Yes\" token as the classification score. Note that this score is proportional to the final mapping score but is not normalised. For ranking-based evaluation, given a source concept, we also consider candidate target concepts with the \"No\" answer as well as their \"No\" scores, placing them after the candidate target concepts with the \"Yes\" answer in an ascending order - a larger \"No\" score implies a lower rank."
    },
    {
      "title": "3 Evaluation",
      "text": "Dataset ConstructionEvaluating LLMs with the current OM datasets of normal or large scales can be time and resource intensive. To yield insightful results prior to full implementation, we leverage two challenging subsets extracted from the NCIT-DOID and the SNOMED-FMA (Body) equivalence matching datasets of the OAEI Bio-ML track. We opt for Bio-ML as its ground truth mappings are curated by humans and derived from dependable sources, Mondo and UMLS. We choose NCIT-DOID and SNOMED-FMA (Body) from five available options because their ontologies are richer in hierarchical contexts. For each original dataset, we first randomly select \\(50\\)_matched_ concept pairs from ground truth mappings, but excluding pairs that can be aligned with direct string matching (i.e., having at least one shared label) to restrict the efficacy of conventional lexical matching. Next, with a fixed source ontology concept, we further select 99 unmatched target ontology concepts, thus forming a total of 100 candidate mappings (inclusive of the ground truth mapping). This selection is guided by the sub-word inverted index-based idf scores as in He et al. [1], which are capable of producing target ontology concepts lexically akin to the fixed source concept. We finally randomly choose 50 source concepts that _do not have a matched target concept_ according to the ground truth mappings, and create 100 candidate mappings for each. Therefore, each subset comprises 50 source ontology concepts with a match and 50 without. Each concept is associated with 100 candidate mappings, culminating in a total extraction of 10,000, i.e., (50+50)*100, concept pairs. Evaluation MetricsFrom all the 10,000 concept pairs in a given subset, the OM system is expected to predict the true mappings, which can be compared against the 50 available ground truth mappings using Precision, Recall, and F-score defined as: \\[P=\\frac{|\\mathcal{M}_{pred}\\cap\\mathcal{M}_{ref}|}{|\\mathcal{M}_{pred}|},\\ \\ R=\\frac{|\\mathcal{M}_{pred}\\cap\\mathcal{M}_{ref}|}{|\\mathcal{M}_{ref}|},\\ \\ F_{1}=\\frac{2PR}{P+R}\\] where \\(\\mathcal{M}_{pred}\\) refers to the set of concept pairs (among the 10,000 pairs) that are predicted as true mappings by the system, and \\(\\mathcal{M}_{ref}\\) refers to the \\(50\\) ground truth (reference) mappings. Given that each source concept is associated with 100 candidate mappings, we can calculate ranking-based metrics based on their scores. Specifically, we calculate Hits@1 for the 50 matched source concepts, counting a hit when the top-scored candidate mapping is a ground truth mapping. The MRR score is also computed for these matched source concepts, summing the inverses of the ground truth mappings' relative ranks among candidate mappings. These two scores are formulated as: \\[Hits@K=\\sum_{(c,c^{\\prime})\\in\\mathcal{M}_{ref}}\\mathbb{I}_{rank_{c^{\\prime}} \\leq K}/|\\mathcal{M}_{ref}|,\\ \\ MRR=\\sum_{(c,c^{\\prime})\\in\\mathcal{M}_{ref}}rank_{c^{\\prime}}^{-1}/| \\mathcal{M}_{ref}|\\] For the 50 unmatched source concepts, we compute the Rejection Rate (RR), considering a successful rejection when **all** the candidate mappings are predicted as false mappings by the system. The unmatched source concepts are assigned a \"null\" match, denoted as \\(c_{null}\\). This results in a set of \"unreferenced\" mappings, represented as \\(\\mathcal{M}_{unref}\\). We can then define RR as: \\[RR=\\sum_{(c,c_{null})\\in\\mathcal{M}_{unref}}\\prod_{d\\in\\mathcal{T}_{c}}(1- \\mathbb{I}_{c\\equiv d})/|\\mathcal{M}_{unref}|\\] where \\(\\mathcal{T}_{c}\\) is the set of target candidate classes for a source concept \\(c\\), and \\(\\mathbb{I}_{c\\equiv d}\\) is a binary indicator that outputs 1 if the system predicts a match between \\(c\\) and \\(d\\), and 0 otherwise. It is worth noting that the product term becomes 1 only when all target candidate concepts are predicted as false matches, i.e., \\(\\forall d\\in\\mathcal{T}_{c}\\mathbb{I}_{c\\equiv d}=0\\). **Model Settings** We examine Flan-T5-XXL under various settings: _(i)_ the vanilla setting, where a mapping is deemed true if it is associated with a \"Yes\" answer; _(ii)_ the threshold3 setting, which filters out the \"Yes\" mappings with scores below a certain threshold; _(iii)_ the parent/child setting, where sampled parent and child concept names are included as additional contexts; and _(iv)_ parent/child+threshold setting, incorporating both structural contexts and thresholding. We also conduct experiments for GPT-3.5-turbo, the most capable variant in the GPT-3.5 series, using the same prompt. However, only setting _(i)_ is reported due to a high cost of this model. For the baseline models, we consider BERTMap and BERTMapLt [1, 6], where the former uses a fine-tuned BERT model for classification and the latter uses the normalised edit similarity. Note that both BERTMap and BERTMapLt inherently adopt setting _(ii)_. Footnote 3: The thresholds are empirically set to \\(0.650\\), \\(0.999\\), and \\(0.900\\) for Flan-T5-XXL, BERTMap, and BERTMapLt in a pioneer experiment concerning small fragments. **Results** As shown in Table 1-2, we observe that the Flan-T5-XXL (+threshold) obtains the \\begin{table} \\begin{tabular}{l c c c c c c} \\hline \\hline **System** & **Precision** & **Recall** & **F-score** & **Hits@1** & **MRR** & **RR** \\\\ \\hline Flan-T5-XXL & 0.643 & 0.720 & 0.679 & 0.860 & 0.927 & 0.860 \\\\ + threshold & **0.861** & 0.620 & **0.721** & 0.860 & 0.927 & **0.940** \\\\ + parent/child & 0.597 & **0.740** & 0.661 & 0.880 & 0.926 & 0.760 \\\\ + threshold \\& parent/child & 0.750 & 0.480 & 0.585 & 0.880 & 0.926 & 0.920 \\\\ \\hline GPT-3.5-turbo & 0.217 & 0.560 & 0.313 & - & - & - \\\\ \\hline BERTMap & 0.750 & 0.540 & 0.628 & **0.900** & **0.940** & 0.920 \\\\ BERTMapLt & 0.196 & 0.180 & 0.187 & 0.460 & 0.516 & 0.920 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Results on the challenging subset of the NCIT-DOID equivalence matching dataset of Bio-ML. \\begin{table} \\begin{tabular}{l c c c c c c} \\hline \\hline **System** & **Precision** & **Recall** & **F-score** & **Hits@1** & **MRR** & **RR** \\\\ \\hline Flan-T5-XXL & 0.257 & 0.360 & 0.300 & 0.500 & 0.655 & 0.640 \\\\ + threshold & 0.452 & 0.280 & 0.346 & 0.500 & 0.655 & 0.820 \\\\ + parent/child & 0.387 & 0.240 & 0.296 & **0.540** & 0.667 & 0.900 \\\\ + threshold \\& parent/child & 0.429 & 0.120 & 0.188 & **0.540** & 0.667 & 0.940 \\\\ \\hline GPT-3.5-turbo & 0.075 & 0.540 & 0.132 & - & - & - \\\\ \\hline BERTMap & 0.485 & **0.640** & **0.552** & **0.540** & **0.723** & 0.920 \\\\ BERTMapLt & **0.516** & 0.320 & 0.395 & 0.340 & 0.543 & **0.960** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Results on the challenging subset of the SNOMED-FMA (Body) equivalence matching dataset of Bio-ML. best F-score among its settings. While it outpaces BERTMap by 0.093 in F-score on the NCIT-DOID subset but lags behind BERTMap and BERTMapLt by 0.206 and 0.049, respectively, on the SNOMED-FMA (Body) subset. Regarding MRR, BERTMap leads on both subsets. Among Flan-T5-XXL settings, using a threshold enhances precision but reduces recall. Incorporating parent/child contexts does not enhance matching results - this underscores the need for a more in-depth examination of strategies for leveraging ontology contexts. GPT-3.5-turbo4 does not perform well with the given prompt. One possible reason is the model's tendency to furnish extended explanations for its responses, making it challenging to extract straightforward yes/no answers. Besides, no ranking scores are presented for GPT-3.5-turbo because it does not support extracting generation probabilities. The suboptimal performance of BERTMapLt is as expected because we exclude concept pairs that can be string-matched from the extracted datasets while BERTMapLt relies on the edit similarity score. Footnote 4: The experimental trials for text-davinci-003 and GPT-4 also showed suboptimal results."
    },
    {
      "title": "4 Conclusion And Future Work",
      "text": "This study presents an exploration of LLMs for OM in a zero-shot setting. Results on two challenging subsets of OM datasets suggest that using LLMs can be a promising direction for OM but various problems need to be addressed including, but not limited to, the design of prompts and overall framework5, and the incorporation of ontology contexts. Future studies include refining prompt-based approaches, investigating efficient few-shot tuning, and exploring structure-informed LLMs. The lessons gleaned from these OM studies can also offer insights into other ontology engineering tasks such as ontology completion and embedding, and pave the way for a broader study on the integration of LLMs with structured data. Footnote 5: This work focuses on the mapping scoring, but the searching (or candidate selection) part of OM is also crucial, especially considering that LLMs are highly computationally expensive."
    },
    {
      "title": "References",
      "text": "* [1] Y. He, J. Chen, D. Antonyrajah, I. Horrocks, BERTMap: A BERT-based ontology alignment system, in: AAAI, 2022. * [2] M. Amir, M. Baruah, M. Eslamialishah, S. Ehsani, A. Bahramali, S. Naddaf-Sh, S. Zarandioon, Truveta mapper: A zero-shot ontology alignment framework, arXiv (2023). * [3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language models to follow instructions with human feedback, in: NeurIPS, 2022. * [4] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned language models, arXiv (2022). * [5] Y. He, J. Chen, H. Dong, E. Jimenez-Ruiz, A. Hadian, I. Horrocks, Machine learning-friendly biomedical datasets for equivalence and subsumption ontology matching, in: ISWC, 2022. * [6] Y. He, J. Chen, H. Dong, I. Horrocks, C. Allocca, T. Kim, B. Sapkota, Deeponto: A python package for ontology engineering with deep learning, arXiv preprint arXiv:2307.03067 (2023)."
    }
  ]
}