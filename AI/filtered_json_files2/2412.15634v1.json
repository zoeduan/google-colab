{
  "title": "Darkit: A User-Friendly Software Toolkit for Spiking Large Language Model",
  "authors": [
    "Xin Du",
    "Shifan Ye",
    "Qian Zheng",
    "Yangfan Hu",
    "Rui Yan",
    "Shunyu Qi",
    "Shuyang Chen",
    "Huajin Tang",
    "Gang Pan",
    "Shuiguang Deng"
  ],
  "abstract": "\n Large language models (LLMs) have been widely applied in various practical applications, typically comprising billions of parameters, with inference processes requiring substantial energy and computational resources Brown [2020], Touvron et al. [2023]. In contrast, the human brain, employing bio-plausible spiking mechanisms, can accomplish the same tasks while significantly reducing energy consumption, even with a similar number of parameters Xing et al. [2024a]. Based on this, several pioneering researchers have proposed and implemented various large language models that leverage spiking neural networks Xing et al. [2024b], Zhu et al. [2023]. They have demonstrated the feasibility of these models, validated their performance, and open-sourced their frameworks and partial source code. To accelerate the adoption of brain-inspired large language models and facilitate secondary development for researchers, we are releasing a software toolkit named DarwinKit (Darkit). The toolkit is designed specifically for learners, researchers, and developers working on spiking large models, offering a suite of highly user-friendly features that greatly simplify the learning, deployment, and development processes. \n",
  "references": [
    {
      "id": null,
      "title": "Darkit: A User-Friendly Software Toolkit for Spiking Large Language Model",
      "authors": [
        "Xin Du",
        "Shifan Ye",
        "Qian Zheng",
        "Yangfan Hu",
        "Rui Yan",
        "Shunyu Qi",
        "Shuyang Chen",
        "Huajin Tang",
        "Gang Pan",
        "Shuiguang Deng"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom B Brown"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Spikellm: Scaling up spiking neural network to large language models via saliency-based spiking",
      "authors": [
        "Xingrun Xing",
        "Boyan Gao",
        "Zheng Zhang",
        "David A Clifton",
        "Shitao Xiao",
        "Li Du",
        "Guoqi Li",
        "Jiajun Zhang"
      ],
      "year": "2024",
      "venue": "Spikellm: Scaling up spiking neural network to large language models via saliency-based spiking",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Towards general spike-driven language modeling via elastic bi-spiking mechanisms",
      "authors": [
        "Xingrun Xing",
        "Zheng Zhang",
        "Ziyi Ni",
        "Shitao Xiao",
        "Yiming Ju",
        "Siqi Fan",
        "Yequan Wang",
        "Jiajun Zhang",
        "Guoqi Li",
        "Spikelm"
      ],
      "year": "2024",
      "venue": "Towards general spike-driven language modeling via elastic bi-spiking mechanisms",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Generative pre-trained language model with spiking neural networks",
      "authors": [
        "Rui-Jie Zhu",
        "Qihang Zhao",
        "Guoqi Li",
        "Jason K Eshraghian",
        "Spikegpt"
      ],
      "year": "2023",
      "venue": "Generative pre-trained language model with spiking neural networks",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "A survey on gpt-3",
      "authors": [
        "Mingyu Zong",
        "Bhaskar Krishnamachari"
      ],
      "year": "2022",
      "venue": "A survey on gpt-3",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "What does bert learn about the structure of language?",
      "authors": [
        "Ganesh Jawahar",
        "Benoît Sagot",
        "Djamé Seddah"
      ],
      "year": "2019",
      "venue": "ACL 2019-57th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Svelte. js: The most loved framework today",
      "authors": [
        "Saumyamani Bhardwaz",
        "Rohan Godha"
      ],
      "year": "2023",
      "venue": "2023 2nd International Conference for Innovation in Technology (INOCON)",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "",
      "authors": [
        "Bill Lubanovic",
        "Fastapi"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Darkit: A User-Friendly Software Toolkit For Spiking Large Language Model",
      "text": "Xin Du, Shifan Ye, Qian Zheng, Yangfan Hu, Rui Yan, Shunyu Qi, Shuyang Chen, Huajin Tang, Gang Pan and Shuiguang Deng Corresponding author.School of Software Technology, Zhejiang University, Zhejiang, China.College of Computer Science and Technology, Zhejiang University, Zhejiang, China.The State Key Lab of Brain-Machine Intelligence, Zhejiang University, Zhejiang, China."
    },
    {
      "title": "2 Integrated Preprocessed Datasets:",
      "text": "Darkit includes preprocessed datasets such as Wikitext, Wikipedia, Ultrachat, and Fineweb, which are readily available via code or a graphical interface on the web (Figure 2). The tool will continue to maintain and expand its dataset collection while providing interfaces for users to contribute new datasets for integration."
    },
    {
      "title": "3 Integrated Preprocessed Tokenizers:",
      "text": "Popular tokenizers, including GPT-2 (small, medium, and large variants), BERT-based-cased, and BERT-base-Chinese, are pre-integrated in Darkit (as shown in Figure 3). Similar to datasets, tokenizers will be continuously updated and allow users to add new tokenizers through the provided interfaces."
    },
    {
      "title": "4 Gui-Based Command Generator For Tuning And Testing:",
      "text": "Darkit includes a user-friendly GUI tool to automate the generation of tuning and testing commands (Figure 4). For instance, users can select basic settings (e.g., model name, dataset, and parameters) from dropdown menus. The tool Figure 4: GUI-based tool to automate the generation of tuning and testing commands. Figure 3: The display of integrated preprocessed tokenizers. Figure 2: The display of integrated preprocessed datasets. [MISSING_PAGE_FAIL:3] **7. Code Editing and Re-Integration:** Darkit provides interfaces for users to edit extracted source code via the web frontend or backend. The modified code can be re-injected into the model for convenient architectural and computational flow adjustments (Figure 6(a)). The tool validates user edits, offers suggestions for corrections, and supports saving, training, and executing the modified models directly (Figure 6(b)). **8. Flowchart-Based Model Design:** Darkit allows users to design spiking large language model computational graphs through a visual flowchart interface or terminal commands. The combination of graphical and coding interfaces facilitates the creation and implementation of spiking neural network models (Figure 8). **9. Comprehensive Logging and Visualization Tools:** To enhance transparency and traceability, Darkit includes robust logging and visualization capabilities. These tools automatically organize, trace, and analyze issues encountered during training and inference. Additionally, the saved logs can be used to generate comparative experiment charts (Figure 9). **10. Unified Interface for Third-Party Extensions:** Darkit supports seamless integration of third-party contributions. New models, datasets, and modules can be incorporated through a unified plugin system, enabling easy expansion and collaboration (Figure 10). Darkit encapsulates and integrates mainstream large language model architectures (such as GPT Zong and Krishnamachari (2022), BERT Jawahar et al. (2019), and Llama Touvron et al. (2023)) through standardized application programming interfaces (APIs), providing a flexible, open, and efficient framework that offers comprehensive support for the learning, research, and development of spiking large language models. Darkit provides an accessible web-based version, which can be accessed via the website [http://121.40.226.59:8080/](http://121.40.226.59:8080/). The platform's frontend is developed using the Svelte framework Bhardwaz and Godha (2023), while the backend is built on the FastAPI stack Lubanovic (2023), utilizing asynchronous non-blocking I/O to ensure high performance and stability. Users can also download the source code for local installation from [https://github.com/zju-bmi-lab/DarwinKit](https://github.com/zju-bmi-lab/DarwinKit). Figure 8: Illustration of flowchart-based model design process. Figure 9: Illustration of logging and visualization tools."
    },
    {
      "title": "References",
      "text": "* Brown (2020) Tom B Brown. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020. * Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Praijwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023. * Xing et al. (2024) Xingrun Xing, Boyan Gao, Zheng Zhang, David A Clifton, Shitao Xiao, Li Du, Guoqi Li, and Jiajun Zhang. Spikellm: Scaling up spiking neural network to large language models via saliency-based spiking. _arXiv preprint arXiv:2407.04752_, 2024a. * Xing et al. (2024b) Xingrun Xing, Zheng Zhang, Ziyi Ni, Shitao Xiao, Yiming Ju, Siqi Fan, Yequan Wang, Jiajun Zhang, and Guoqi Li. Spikelm: Towards general spike-driven language modeling via elastic bi-spiking mechanisms. _arXiv preprint arXiv:2406.03287_, 2024b. * Zhu et al. (2023) Rui-Jie Zhu, Qihang Zhao, Guoqi Li, and Jason K Eshraghian. Spikegpt: Generative pre-trained language model with spiking neural networks. _arXiv preprint arXiv:2302.13939_, 2023. * Zong and Krishnamachari (2022) Mingyu Zong and Bhaskar Krishnamachari. A survey on gpt-3. _arXiv preprint arXiv:2212.00857_, 2022. * Jawahar et al. (2019) Ganesh Jawahar, Benoit Sagot, and Djame Seddah. What does bert learn about the structure of language? In _ACL 2019-57th Annual Meeting of the Association for Computational Linguistics_, 2019. * Bhardwaz and Godha (2023) Saumyamani Bhardwaz and Rohan Godha. Svelte. js: The most loved framework today. In _2023 2nd International Conference for Innovation in Technology (INOCON)_, pages 1-7. IEEE, 2023. * Lubanovic (2023) Bill Lubanovic. _FastAPI_. \" O'Reilly Media, Inc.\", 2023. Figure 10: Workflow for uploading and running custom models and parameters."
    }
  ]
}