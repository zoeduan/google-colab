{
  "title": "Solving and Generating NPR Sunday Puzzles with Large Language Models",
  "authors": [
    "Jingmiao Zhao",
    "Carolyn Jane Anderson"
  ],
  "abstract": "\n We explore the ability of large language models to solve and generate puzzles from the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15 years of on-air puzzles. We evaluate four large language models using PUZ-ZLEQA, in both multiple choice and free response formats, and explore two prompt engineering techniques to improve free response performance: chain-of-thought reasoning and prompt summarization. We find that state-of-the-art large language models can solve many PUZZLEQA puzzles: the best model, GPT-3.5, achieves 50.2% loose accuracy. However, in our few-shot puzzle generation experiment, we find no evidence that models can generate puzzles: GPT-3.5 generates puzzles with answers that do not conform to the generated rules. Puzzle generation remains a challenging task for future work. \n",
  "references": [
    {
      "id": null,
      "title": "Solving and Generating NPR Sunday Puzzles with Large Language Models",
      "authors": [
        "Jingmiao Zhao",
        "Carolyn Jane Anderson"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "We hope that the PUZZLEQA dataset will aid future work in this area. References [Brown et al. 2020]",
      "authors": [
        "T B Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D M Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mc-Candlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "IBM Journal of Research and Development",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Quizbowl: The case for incremental question answering",
      "authors": [
        "Rodriguez"
      ],
      "year": "2021",
      "venue": "Quizbowl: The case for incremental question answering",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models",
      "authors": [
        "Potts Rozner",
        "J Mahowald ; Rozner",
        "C Potts",
        "K Mahowald",
        "A Sobieszek",
        "T Price"
      ],
      "year": "2021",
      "venue": "Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "LLaMA: Open and efficient foundation language models",
      "authors": [
        "Touvron"
      ],
      "year": "2023",
      "venue": "A 6 Billion Parameter Autoregressive Language Model",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Wei"
      ],
      "year": "2023",
      "venue": "Chain-of-thought prompting elicits reasoning in large language models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Solving And Generating Npr Sunday Puzzles With Large Language Models",
      "text": "Jingmiao Zhao and Carolyn Jane Anderson Computer Science Department Wellesley College Wellesley, MA 02482 USA carolyn.anderson@wellesley.edu"
    },
    {
      "title": "Abstract",
      "text": "We explore the ability of large language models to solve and generate puzzles from the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15 years of on-air puzzles. We evaluate four large language models using PUZZLEQA, in both multiple choice and free response formats, and explore two prompt engineering techniques to improve free response performance: chain-of-thought reasoning and prompt summarization. We find that state-of-the-art large language models can solve many PUZZLEQA puzzles: the best model, GPT-3.5, achieves 50.2% loose accuracy. However, in our few-shot puzzle generation experiment, we find no evidence that models can generate puzzles: GPT-3.5 generates puzzles with answers that do not conform to the generated rules. Puzzle generation remains a challenging task for future work."
    },
    {
      "title": "Introduction",
      "text": "Puzzles and games have long been used to benchmark progress in AI. We continue this tradition by exploring the ability of large language models (LLMs) to solve word puzzles from the NPR Sunday Puzzle on-air game show. Recent advances have led to new techniques for using general-purpose text generation models to solve a variety of tasks. In few-shot learning, a model is prompted with a handful of examples and asked to generate a solution. In prompt engineering, the input to the model is manipulated in order to improve the model's performance on a task. These techniques have led to surprisingly good performance by LLMs on novel tasks, without any further training of the model. In this paper, we explore whether few-shot learning and prompt engineering can allow LLMs to solve questions from the NPR Sunday Puzzle game show, which combines information retrieval, wordplay, and pattern recognition. We introduce PUZZLEQA, consisting of puzzle descriptions, questions, and answers for 558 puzzles, and use it to benchmark four state-of-the-art LLMs. We explore prompt engineering techniques, but find that they have little impact on performance. We also explore whether models can generate new puzzles and find that this remains a challenging task. Although the best model, GPT-3.5, is capable of solving 50.2% of the puzzles, it cannot generate playable games."
    },
    {
      "title": "Benchmarking Ai Through Games",
      "text": "Our work continues the tradition of evaluating AI progress through puzzles and games [16, 15, 17, 18, 19]. Contemporary LLMs have demonstrated strong performance on a wide variety of language tasks, including question-answering. However, the extent of their ability to generalize patterns and to solve wordplay puzzles is under-explored. The NPR Sunday Puzzle game show represents a particularly interesting genre of puzzle to explore because it synthesizes a variety of skills: information retrieval; rhyming, anagram-solving, and other wordplay; and pattern recognition. Figure 1 shows an example of a puzzle, which involves knowledge of country names and wordplay. Despite the complexity of some NPR Sunday Puzzle games, compared to other question-answering games used to benchmark LLMs, such as Jeopardy! and Quiz Bowl, they are targeted towards a broader audience and require less specialized knowledge."
    },
    {
      "title": "Dataset",
      "text": "We present PUZZLEQA, a dataset of 558 NPR Sunday Puzzle games from 2007-2021. During this period, a group of fans ran a mailing list, NPR Puzzle Synopsis, that distributed questions and answers for each week's puzzle.1 We obtained the puzzle explanations from the NPR website,2 and extracted the answers from the mailing list, using GPT-J to aid in preprocessing the data. We also classified the puzz Figure 1: NPR Sunday Puzzle from March 12, 2023zles into 11 different categories. The dataset, preprocessing tools, and analysis scripts will be released publicly.3 Footnote 3: [https://github.com/Wellesley-EASEL-lab/PuzzleQA](https://github.com/Wellesley-EASEL-lab/PuzzleQA)"
    },
    {
      "title": "Model Selection",
      "text": "We explored two publicly available LLMs, GPT-J [23] and LLaMA [22], and two proprietary OpenAI LLMs: GPT-3 Davinci and GPT-3.5 [2]. The amount of randomness in each of these model's generations can be manipulated via the temperature hyperparameter, where a high temperature means more randomness. After exploring temperature settings of 0.75, 0.5, 0.25, and 0.1, we found that temperature = 0.1 was optimal."
    },
    {
      "title": "Multiple Choice Experiments",
      "text": "As an easier benchmark, we constructed a multiple-choice version of the PUZZLEQA dataset. For each problem, we randomly selected three answers to other questions from the same puzzle to present alongside the correct answer. Answer only baselineIn multiple choice tasks, there can be biases towards or against certain question options, even in the absence of the question. To obtain an accurate baseline, we measure how often the model selects the correct answer when it is not given the question. An unbiased set of answer options would result in at-chance performance (25%). We refer to this task as the answer-only baseline. We find that the model selects the correct answer 21% of the time when it is not given the question, suggesting that there is no significant bias towards the correct answer from the answer options alone. ResultsFigure 2 shows the performance of each model on the multiple choice task. The smallest model, GPT-J, does not perform better than chance on this simplified task. As a result, we exclude it from the rest of our experiments. The other publicly available model, LLaMA, performs well above chance, showing that it is able to correctly identify responses for many problems. GPT-3 and GPT-3.5 both perform well on this task, solving 78% percent of problems."
    },
    {
      "title": "Free Response Experiments",
      "text": "We perform two sets of free response experiments. To explore various prompt engineering techniques, we first create a subset of our data balanced by question type. We then compare the best prompting technique against a baseline on the full dataset. In all experiments, a few-shot paradigm is used: the model is given two examples of solved questions from the same puzzle (following the same game rules) and asked to solve a third."
    },
    {
      "title": "Evaluation Metrics",
      "text": "Free response question-answering is difficult to evaluate, since a correct answer may be phrased in various ways. We use two conservative metrics for evaluating performance. _Exact Matching_: the response is correct if it exactly matches the gold solution. _Loose Matching_: the response is correct if it is contained or contains the gold solution, after removing non-alphabetical characters and lowercasing both strings."
    },
    {
      "title": "Exploring Prompt Engineering Techniques",
      "text": "We subsample our dataset in order to evaluate the impact of various prompt engineering techniques. 10 questions from each of our 11 categories were randomly sampled for the subset, for a total of 110 items. We explore two prompt engineering techniques: summarization and chain-of-thought reasoning. SummarizationOne potential challenge for the model in solving the PUZZLEQA puzzles is that the games are described informally. We hypothesize that the lack of consistency in puzzle wording might hinder the model. We experiment with using GPT-3.5 to summarize the puzzle description to a more consistent format (Figure 3). Figure 2: Results on full PUZZLEQA dataset, by model, prompting technique, and format. Chain-of-thought ReasoningPrompting models to explain their reasoning before generating an answer has been shown to improve model performance on other tasks [22]. This is known as _chain-of-thought prompting_. One limitation of this approach is that humans must write explanations to provide as examples to the model. We automate the process by using the model to generate explanations for rule-question-answer triplets. We then use the generated explanations as input to the chain-of-thought prompting experiment. Figure 4 shows an example prompt used to gather model explanations. GPT-3's generated explanation was _The word \"blouse\" can have two letters dropped to spell the color \"blue\"_. This explanation was then added to the example to use in few-shot prompting."
    },
    {
      "title": "Prompt Engineering Results",
      "text": "In our small-scale experiment, we found that both summarization and chain-of-thought prompting improved performance. Figure 5 shows GPT-3 results for each technique."
    },
    {
      "title": "Free Response Results",
      "text": "We select the best-performing prompt engineering technique to compare against a baseline prompt on the full PUZZEQA dataset. Our small-scale experiments suggested that both summarization and chain-of-thought prompting improve performance. We compare this model to a baseline few-shot learning model. Figure 2 shows the free response results for the full PUZZEQA dataset. Although chain-of-thought reasoning and summarization improved model performance in our small-scale experiment, this did not replicate for the entire dataset. The baseline GPT-3.5 model performs best, solving 50.2% of the puzzles. We note that performance is very sensitive to prompt wording: when we rephrase the chain-of-thought prompt to ask for the \"answer and reasoning\" rather than the \"reasoning and answer,\" performance drops substantially. DiscussionOverall, the best model, GPT-3.5, performs fairly well on the PUZZLEQA dataset. We observe a large gap in performance between the LLaMA model, which is available for academic research, and the proprietary OpenAI models, illustrating the need for better public LLMs. Surprisingly, we find that the prompt engineering techniques we explored did not improve model performance. Although chain-of-thought prompting did not improve performance, we feel that it still has some benefits. The chain-of-thought-prompted model: when prompted this way, GPT-3.5 produces \"N/A\" 87 times, compared to only 3 refusals to provide an answer in the baseline version. In some cases, the puzzle is faulty due to webscraping errors. When we manually examined the explanations generated by the model, most are consistent with the answer. Thus, chain-of-thought prompting may decrease overconfidence in models while providing a window into the model's decisions."
    },
    {
      "title": "Game Generation",
      "text": "Although our exploration of popular prompt engineering techniques was not fruitful, we nonetheless found that state-of-the-art LLMs are capable of solving many of the NPR puzzles. In this section, we explore whether LLMs are also capable of generating puzzles for humans to play. We explore puzzle generation with the LLM that achieved the highest performance on the free response task, GPT-3.5."
    },
    {
      "title": "Prompt Design",
      "text": "We construct a few-shot puzzle generation dataset using our balanced 110 question subset. In each prompt, we provide the model with five examples of puzzles, presented as explanation-question-answer triplets (Figure 6), for a total of 22 prompts. We then evaluate each generated puzzle by asking the model to generate an answer to the question. Figure 4: Chain-of-thought prompt to elicit explanations Figure 5: GPT-3 results for prompt engineering experiment"
    },
    {
      "title": "Evaluation Metrics",
      "text": "We use two metrics to evaluate the generated games: _Consistency_: can the model solve its own puzzle? We provide the generated explanation and question to GPT-3.5 and generate an answer. If the answers match, the puzzle is consistent. _Conformity_: of the questions that are consistent, how many have answers that conform to the rules in the explanation? We assess this manually."
    },
    {
      "title": "Results",
      "text": "Of the 22 games generated by GPT-3.5, it answers 17 questions consistently. However, just 6 of the questions conform with the explanation provided. In addition, the conforming games are trivial to solve (Table 1). Thus, though LLMs succeed in playing the NPR Sunday Puzzle, we find no evidence that they can generate new puzzles for human players."
    },
    {
      "title": "Limitations And Future Work",
      "text": "Our experiments with PUZZLEQA show that current LLMs are capable of solving, but not creating, NPR Sunday Puzzle questions. However, our results come with a number of caveats. First, since the training data for GPT-3, GPT-3.5, and LLaMA is not publicly available, we cannot measure whether models have been trained on problems within our dataset. To investigate potential training/test overlap, we manually constructed a test set of questions from 2023, which is more recent than the models' training data cutoff dates (GPT-3: 2019; GPT-3.5: 2021; LLaMA: 2022). We find that model performance on this small (n=116) subset is on par with the full dataset (Figure 7). In general, although performance varies by year, there is no clear trend.4 Footnote 4: We note that puzzle types and topics may vary over time; an in-depth analysis of the puzzle content is one area for future work. Our methodology could also be refined in a number of ways. Our webscraping techniques failed to capture some questions, which could be added to our dataset. Our loose accuracy metric is a conservative measure of model capability, since it may fail to identify some valid answers. Finally, future work could incorporate a rating of question difficulty by identifying from the game transcript whether the human player succeeded or failed in answering the question."
    },
    {
      "title": "Conclusion",
      "text": "Using data from the NPR Sunday Puzzle game show, we explore the ability of contemporary large language models to solve and generate word puzzles. We show that PUZZLEQA is a challenging benchmark for LLMs: although GPT-3.5 solves 50.2% of the problems in the free response task, information about its training data is not public, and the best publicly available model achieves only 33%. The fact that the prompt engineering techniques we explored failed to improve performance is puzzling, given promising results from chain-of-thought prompting reported for similar tasks [14]. However, we argue that chain-of-thought reasoning is still helpful for explainability. Our game generation results show that being able to generate NPR Sunday Puzzle-style games is beyond the capabilities of current LLMs, even if they are capable of solving \\begin{table} \\begin{tabular}{|p{113.8pt}|c|c|c|} \\hline Explanation & Question & Response & Gold \\\\ \\hline I’ll give you a word that can be split into two smaller words. The first word is the name of a U.S. state, and the second word is a type of animal. For example, given “MontanaLion,” the answer would be “Montana, Lion.” & Delaware, Hawk & Delaware, Hawk \\\\ \\hline Every answer is a well-known movie title with one or more letters from the title replaced with a number. For example, if the clue is “Th3 Matrix,” the answer would be “The Matrix.” & F1ght Club & Fight Club & Fight Club \\\\ \\hline \\end{tabular} \\end{table} Table 1: Game that satisfies both consistency and conformity, but are trivial Figure 6: Game generation prompt Figure 7: Exact accuracy by puzzle air datethem. Future work could explore fine-tuning a model on our dataset rather than using few-shot learning. We hope that the PUZZLEQA dataset will aid future work in this area."
    },
    {
      "title": "Author Contributions",
      "text": "All authors contributed to the writing of this paper."
    },
    {
      "title": "Acknowledgements",
      "text": "We would like to thank our anonymous ICCC reviewers for their helpful feedback. We also extend our thanks to Eni Mustafaraj and Brian Tjaden for their comments on this work at various stages in its development, and to Arjun Guha for the initial idea of exploring the NPR Sunday Puzzle game."
    },
    {
      "title": "References",
      "text": "* [Brown et al.2020] Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language models are few-shot learners. * [Ferrucci2012] Ferrucci, D. A. 2012. Introduction to \"This is Watson\". _IBM Journal of Research and Development_ 56(3.4):1:1-1:15. * [Rodriguez et al.2021] Rodriguez, P.; Feng, S.; Iyyer, M.; He, H.; and Boyd-Graber, J. 2021. Quizbowl: The case for incremental question answering. * [Rozner, Potts, and Mahowald2021] Rozner, J.; Potts, C.; and Mahowald, K. 2021. Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP. arXiv:2104.08620. * [Sobieszek and Price2022] Sobieszek, A., and Price, T. 2022. Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models. _Minds and Machines_ 32(2):341-364. * [Touvron et al.2023] Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Roziere, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023. LLaMA: Open and efficient foundation language models. * [Wang and Komatsuzaki2021] Wang, B., and Komatsuzaki, A. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. [https://github.com/kingofolz/mesh-transformer-jax](https://github.com/kingofolz/mesh-transformer-jax). * [Wei et al.2023] Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023. Chain-of-thought prompting elicits reasoning in large language models."
    }
  ]
}