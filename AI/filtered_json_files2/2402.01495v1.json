{
  "title": "A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation",
  "authors": [
    "Phillip Schneider",
    "Manuel Klettner",
    "Elena Simperl",
    "Florian Matthes"
  ],
  "abstract": "\n Generating natural language text from graphstructured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models' performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient finetuning techniques, particularly for smaller models that exhibit lower zero-shot performance. \n",
  "references": [
    {
      "id": null,
      "title": "A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation",
      "authors": [
        "Phillip Schneider",
        "Manuel Klettner",
        "Elena Simperl",
        "Florian Matthes"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Analysing mixed initiatives and search strategies during conversational search",
      "authors": [
        "Mohammad Aliannejadi",
        "Leif Azzopardi",
        "Hamed Zamani",
        "Evangelos Kanoulas",
        "Paul Thomas",
        "Nick Craswell"
      ],
      "year": "2021",
      "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management, CIKM '21",
      "doi": "10.1145/3459637.3482231"
    },
    {
      "id": "b1",
      "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "authors": [
        "Satanjeev Banerjee",
        "Alon Lavie"
      ],
      "year": "2005",
      "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Alec Radford, Ilya Sutskever, and Dario Amodei",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell",
        "Sandhini Agarwal",
        "Ariel Herbert-Voss",
        "Gretchen Krueger",
        "Tom Henighan",
        "Rewon Child",
        "Aditya Ramesh",
        "Daniel Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Chris Hesse",
        "Mark Chen",
        "Eric Sigler",
        "Mateusz Litwin",
        "Scott Gray",
        "Benjamin Chess",
        "Jack Clark",
        "Christopher Berner",
        "Sam Mccandlish"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+ 2020)",
      "authors": [
        "Thiago Castro Ferreira",
        "Claire Gardent",
        "Nikolai Ilinykh",
        "Chris Van Der Lee",
        "Simon Mille",
        "Diego Moussallem",
        "Anastasia Shimorina"
      ],
      "year": "2020",
      "venue": "Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Neural data-to-text generation: A comparison between pipeline and end-to-end architectures",
      "authors": [
        "Thiago Castro Ferreira",
        "Chris Van Der Lee",
        "Emiel Van Miltenburg",
        "Emiel Krahmer"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1052"
    },
    {
      "id": "b5",
      "title": "Few-shot NLG with pre-trained language model",
      "authors": [
        "Zhiyu Chen",
        "Harini Eavani",
        "Wenhu Chen",
        "Yinyin Liu",
        "William Yang",
        "Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.18"
    },
    {
      "id": "b6",
      "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph E Gonzalez",
        "Ion Stoica",
        "Eric P Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Control prefixes for parameter-efficient text generation",
      "authors": [
        "Jordan Clive",
        "Kris Cao",
        "Marek Rei"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
      "doi": "10.18653/v1/2022.gem-1.31"
    },
    {
      "id": "b8",
      "title": "The WebNLG challenge: Generating text from DBPedia data",
      "authors": [
        "Emilie Colin",
        "Claire Gardent",
        "M' Yassine",
        "Shashi Rabet",
        "Laura Narayan",
        "Perez-Beltrachini"
      ],
      "year": "2016",
      "venue": "Proceedings of the 9th International Natural Language Generation conference",
      "doi": "10.18653/v1/W16-6626"
    },
    {
      "id": "b9",
      "title": "Is GPT-3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text",
      "authors": [
        "Yao Dou",
        "Maxwell Forbes",
        "Rik Koncel-Kedziorski",
        "Noah A Smith",
        "Yejin Choi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.501"
    },
    {
      "id": "b10",
      "title": "Pive: Prompting with iterative verification improving graph-based generative capability of llms",
      "authors": [
        "Jiuzhou Han",
        "Nigel Collier",
        "Wray Buntine",
        "Ehsan Shareghi"
      ],
      "year": "2023",
      "venue": "Pive: Prompting with iterative verification improving graph-based generative capability of llms",
      "doi": "10.48550/arXiv.2305.12392"
    },
    {
      "id": "b11",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Survey of hallucination in natural language generation",
      "authors": [
        "Ziwei Ji",
        "Nayeon Lee",
        "Rita Frieske",
        "Tiezheng Yu",
        "Dan Su",
        "Yan Xu",
        "Etsuko Ishii",
        "Ye",
        "Jin Bang",
        "Andrea Madotto",
        "Pascale Fung"
      ],
      "year": "2023",
      "venue": "ACM Comput. Surv",
      "doi": "10.1145/3571730"
    },
    {
      "id": "b13",
      "title": "Neural pipeline for zero-shot data-to-text generation",
      "authors": [
        "Zdeněk Kasner",
        "Ondrej Dusek"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.271"
    },
    {
      "id": "b14",
      "title": "Mind the labels: Describing relations in knowledge graphs with pretrained models",
      "authors": [
        "Zdeněk Kasner",
        "Ioannis Konstas",
        "Ondrej Dusek"
      ],
      "year": "2023",
      "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Pretrained language models for text generation: A survey",
      "authors": [
        "Junyi Li",
        "Tianyi Tang",
        "Wayne Xin Zhao",
        "Ji Rong",
        "Wen"
      ],
      "year": "2021",
      "venue": "International Joint Conference on Artificial Intelligence",
      "doi": "10.24963/ijcai.2021/612"
    },
    {
      "id": "b16",
      "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu",
        "Zhengbao Jiang",
        "Hiroaki Hayashi",
        "Graham Neubig"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys",
      "doi": "10.1145/3560815"
    },
    {
      "id": "b17",
      "title": "Awakening latent grounding from pretrained language models for semantic parsing",
      "authors": [
        "Qian Liu",
        "Dejian Yang",
        "Jiahui Zhang",
        "Jiaqi Guo",
        "Bin Zhou",
        "Jian-Guang Lou"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
      "doi": "10.18653/v1/2021.findings-acl.100"
    },
    {
      "id": "b18",
      "title": "Step-by-step: Separating planning from realization in neural data-to-text generation",
      "authors": [
        "Amit Moryossef",
        "Yoav Goldberg",
        "Ido Dagan"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1236"
    },
    {
      "id": "b19",
      "title": "Chatgpt: Optimizing language models for dialogue",
      "authors": [
        "Openai"
      ],
      "year": "2022",
      "venue": "Chatgpt: Optimizing language models for dialogue",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "Kishore Papineni",
        "Salim Roukos",
        "Todd Ward",
        "Wei-Jing Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.3115/1073083.1073135"
    },
    {
      "id": "b21",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "Language models are unsupervised multitask learners",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "A theoretical framework for conversational search",
      "authors": [
        "Filip Radlinski",
        "Nick Craswell"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval, CHIIR '17",
      "doi": "10.1145/3020165.3020183"
    },
    {
      "id": "b23",
      "title": "An unsupervised joint system for text generation from knowledge graphs and semantic parsing",
      "authors": [
        "Martin Schmitt",
        "Sahand Sharifzadeh",
        "Hinrich Volker Tresp",
        "Schütze"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.577"
    },
    {
      "id": "b24",
      "title": "Evaluating large language models in semantic parsing for conversational question answering over knowledge graphs",
      "authors": [
        "Phillip Schneider",
        "Manuel Klettner",
        "Kristiina Jokinen",
        "Elena Simperl",
        "Florian Matthes"
      ],
      "year": "2024",
      "venue": "Proceedings of the 16th International Conference on Agents and Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "A decade of knowledge graphs in natural language processing: A survey",
      "authors": [
        "Phillip Schneider",
        "Tim Schopf",
        "Juraj Vladika",
        "Mikhail Galkin",
        "Elena Simperl",
        "Florian Matthes"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "A study of translation edit rate with targeted human annotation",
      "authors": [
        "Matthew Snover",
        "Bonnie Dorr",
        "Rich Schwartz",
        "Linnea Micciulla",
        "John Makhoul"
      ],
      "year": "2006",
      "venue": "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Plan-then-generate: Controlled data-to-text generation via planning",
      "authors": [
        "Yixuan Su",
        "David Vandyke",
        "Sihui Wang",
        "Yimai Fang",
        "Nigel Collier"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": "10.18653/v1/2021.findings-emnlp.76"
    },
    {
      "id": "b28",
      "title": "Lamda: Language models for dialog applications",
      "authors": [
        "Romal Thoppilan",
        "Daniel De Freitas",
        "Jamie Hall",
        "Noam Shazeer",
        "Apoorv Kulshreshtha",
        "Heng-Tze",
        "Alicia Cheng",
        "Taylor Jin",
        "Leslie Bos",
        "Yu Baker",
        "Du"
      ],
      "year": "2022",
      "venue": "Lamda: Language models for dialog applications",
      "doi": "10.48550/arXiv.2201.08239"
    },
    {
      "id": "b29",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "doi": "10.48550/arXiv.2302.13971"
    },
    {
      "id": "b30",
      "title": "Stage-wise fine-tuning for graph-to-text generation",
      "authors": [
        "Qingyun Wang",
        "Semih Yavuz",
        "Victoria Xi",
        "Heng Lin",
        "Nazneen Ji",
        "Rajani"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop",
      "doi": "10.18653/v1/2021.acl-srw.2"
    },
    {
      "id": "b31",
      "title": "Bertscore: Evaluating text generation with bert",
      "authors": [
        "Tianyi Zhang",
        "Varsha Kishore",
        "Felix Wu",
        "Kilian Q Weinberger",
        "Yoav Artzi"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "A Comparative Analysis Of Conversational Large Language Models In Knowledge-Based Text Generation",
      "text": "Phillip Schneider\\({}^{1}\\), Manuel Klettner\\({}^{1}\\), Elena Simperl\\({}^{2}\\), and Florian Matthes\\({}^{1}\\) \\({}^{1}\\)Technical University of Munich, Department of Computer Science, Germany \\({}^{2}\\)King's College London, Department of Informatics, United Kingdom {phillip.schneider, manuel.klettner, matthes}@tum.de elena.simperl@kcl.ac.uk"
    },
    {
      "title": "Abstract",
      "text": "Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models' performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance."
    },
    {
      "title": "1 Introduction",
      "text": "Accessing structured information through natural language interfaces has garnered significant research interest in natural language processing (NLP) Aliannejadi et al. (2021); Radlinski and Craswell (2017). For instance, the emerging information retrieval paradigm of conversational search frames information-seeking processes within multi-turn dialogue interactions. Conversational search facilitates exploring and progressively narrowing the search scope to relevant knowledge items within an information space. These search-oriented conversational interfaces are often connected to structured data sources like knowledge graphs. However, a key challenge lies in mediating between natural language, in which users express their queries, and machine-readable knowledge representations. The task of data-to-text generation focuses on this issue, taking structured data as input to produce coherent, human-readable text, which has been extensively studied with approaches ranging from rule-based to supervised neural network-based techniques. Over the last years, the field of NLP has witnessed a shift in methodologies with the advent of pre-trained large language models (LLMs). Unlike traditional supervised learning approaches that rely on annotated datasets, LLMs are trained in a self-supervised manner, predicting tokens within vast amounts of unlabeled data. Combined with scaling up the model size and training corpora, this approach has demonstrated remarkable emergent capabilities of LLMs and their prowess in multi-task learning Radford et al. (2019); Brown et al. (2020). An advantage of LLMs lies in prompt-based (in-context) learning. Through carefully defined prompts, these foundation models can perform multiple tasks like question-answering, semantic parsing, or text summarization Liu et al. (2023). More recently, there has been a growing interest in optimizing LLMs for conversational interactions by pre-training on dialogue corpora, instruction fine-tuning, and reinforcement learning from human feedback Thoppilan et al. (2022); OpenAI (2022). Although LLMs offer tremendous potential for conversational interaction, owing to their ability to produce responses for arbitrary input, they have known limitations, such as the risk of hallucinating or omitting important information and a lack of transparency regarding the origins of information sources from which they derive their outputs Dou et al. (2022); Ji et al. (2023). In order to mitigate these limitations, it becomes imperative to ground their generated outputs in verifiable factual data from knowledge graphs. However, there has been insufficient systematic investigation into their proficiency in verbalizing graph-structured data input. To assess LLMs in knowledge-based text generation, we compare four models of different sizes and training objectives, with a primary focus on models optimized for conversational interaction. Based on the popular WebNLG benchmark dataset, we evaluate the models' performance in generating natural language text from semantic triples. Through multiple experiments, we analyze different configurations of models and prompting techniques, discussing insights about their individual capabilities and limitations. Our contributions include: (1) adapting the WebNLG benchmark to evaluate closed- and open-source LLMs, (2) providing a thorough error analysis and insights on model performance with automatic reference-based metrics as well as human evaluation, and (3) creating a new fine-tuning dataset with 26,422 conversations with triple-to-text verbalizations in chat completion format. To ensure reproducibility, we publish our source code and datasets in a GitHub repository.1 Footnote 1: GitHub: [https://github.com/sebischair/LLM-KG-DZT](https://github.com/sebischair/LLM-KG-DZT)"
    },
    {
      "title": "2 Related Work",
      "text": "Existing works from the NLP literature have explored knowledge-based text generation, with significant advancements driven by new deep learning architectures and fine-tuning language models on downstream tasks (Li et al., 2021; Schneider et al., 2022). For triple-to-text generation, many evaluations use the established WebNLG benchmark (Colin et al., 2016). Several studies have focused on comparing neural pipeline versus end-to-end approaches, assessing supervised versus unsupervised training regimes, and developing frameworks for making text generation more controllable through neuro-symbolic methods (Castro Ferreira et al., 2019; Schmitt et al., 2020; Moryossef et al., 2019; Su et al., 2021). Concerning pre-trained language models, Chen et al. (2020) were among the first to propose the task of few-shot natural language generation. With just 200 table-to-text training examples, their approach achieves strong performance and good generalization. By collecting a novel dataset and experimenting with few-shot fine-tuning, Kasner et al. (2023) demonstrate that pre-trained language models trained with a diverse set of labels exhibit robustness in verbalizing knowledge graph relations, being capable of generalizing to novel domains. Another study from Liu et al. (2021) highlights the ability of pre-trained language models (PLMs) to uncover hidden mappings between linguistic tokens and real-world concepts. Conducting experiments on four datasets, the authors show the effectiveness of their awakening latent grounding approach for generating structured queries from text. Similar to our work, Han et al. (2023) assess capabilities of LLMs but for text-to-graph generation with the GPT-3.5-Turbo model. They develop a prompting framework with iterative verification, improving the generation quality. In contrast, our objective is to achieve a comprehensive understanding of conversational LLMs for triple verbalization rather than solely concentrating on individual use cases or models. To the best of our knowledge, we are the first to conduct a comparative analysis of conversational LLMs and prompt configurations on the task of triple-to-text generation. The empirical approach employed in this study is related to our previous work on evaluating LLMs for semantic parsing for conversational question answering over knowledge graphs (Schneider et al., 2024)."
    },
    {
      "title": "3 Experiments",
      "text": "Experimental SetupWe conduct our experiments on the _WebNLG+ 2020_ dataset, a DBpedia-based triple-to-text benchmark with a total of 1,779 test examples (Castro Ferreira et al., 2020). As evaluation metrics, we calculate the lexical similarity between model outputs and human annotations using _BLEU_(Papineni et al., 2002), _METEOR_(Banerjee and Lavie, 2005), and _TER_(Snover et al., 2006). Since these metrics mainly focus on lexical overlaps, we also use the _BERTScore_ metric, which captures semantic similarity (Zhang et al., 2020). As a commercial state-of-the-art LLM, we include _GPT-3.5-Turbo_(_ChatGPT_) (OpenAI, 2022) in our comparison. It is optimized for conversations and has demonstrated remarkable zero-shot performance on various NLP tasks. Consequently, it is often used as a benchmark for comparing LLMs. We ran our experiments with the model released in June 2023 (GPT-3.5-Turbo-0613). Further, we opted to test _LLaMA_, a collection of open-source LLMs from Meta (Touvron et al., 2023), achieving competitive performance on various benchmarks. We include three model variations with 7B parameters of the first LLaMA version. In addition to the non-conversational base model (_LLaMA-7B_), we included a fine-tuned model (_LLaMA-FT-7B_) which we trained on WebNLG examples in a con versational format. To have a sufficiently large fine-tuning corpus, we created a new dataset encompassing 26,422 conversations from all 13,211 WebNLG training examples. We ensured that each triple-to-text example appeared, on average, five times in different contexts. The conversations have different lengths and contain verbalizations from various triple categories. The training was done through _low-rank adaptation (LoRA)_, a method that fine-tunes only a subset of the model's parameters, referred to as low-rank matrices, rather than updating the entire parameter space, improving the fine-tuning efficiency (Hu et al., 2022). During training time, the model takes in a full conversation in chat completion format, characterized by a series of turns attributed to the user or assistant role (i.e., the model learns from a sequence of sequence-to-sequence examples). We employed five training epochs, a per-device training batch size of eight, and used a half-precision floating-point format (FP16). Another fine-tuned LLaMA model we compared is _Vicuna_. It was trained on a corpus of around 70K user-shared ChatGPT conversations crawled from the ShareGPT website. Preliminary evaluations from Chiang et al. (2023) demonstrate that Vicuna exhibits a higher level of detail and structure in its responses than LLaMA, highlighting the advantage of fine-tuning on dialogue data. The LLaMA and Vicuna models are prompted in the chat completion structure of the FastChat2 platform, replicating OpenAI's chat completion API endpoint with a structured list of system, user, and assistant messages. We set the token limit to 128 and the temperature parameter to 0, maximizing deterministic generation by favoring high-probability words. The zero-shot prompt contains only the following system message with a triple verbalization instruction: _\"SYS-TEM: Generate a concise text for the given set of triples. Ensure that the generated output only includes the provided information from the triples.\"_. The few-shot prompt expands the instruction with three in-context examples provided as user and assistant messages in the format: _\"USER: Input triples: [['object': 'Mike_Mullarkey','property': 'coach','subject': 'Tennessee_Titans']] \"ASSISTANT: Output text: Mike Mularkey is the coach of the Tennessee Titans.\"_ Table 3 in Appendix A displays each prompt in full length. Footnote 2: FastChat: [https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat) Results of Performance MetricsTable 1 summarizes the calculated metrics. The Copy-Baseline denotes copying the triples as output without processing. It is included as a metric reference point to establish a lower bound (Kasner and Dusek, 2022). We distinguish between scores for raw and post-processed (+ PP) outputs. Post-processing involved the removal of \"Output text\" or \"Output\" since they are not intended parts of the desired text prediction but were present in the few-shot prompt. Additionally, repeated instructions or in-context examples from the prompt were removed when they appeared in the generated output. Examining the scores, LLaMA-FT-7B demonstrates superior performance compared to the other models. Even without few-shot examples, it effectively learned from fine-tuning to handle the triple verbalization task, gaining only a minor performance increase through few-shot prompting. The second-ranking model, GPT-3.5-Turbo, shows similar scores, which is remarkable because it was not explicitly trained for triple-to-text generation. Notably, Vicuna achieves a performance level almost on par with the much bigger GPT-3.5-Turbo model when it was provided with in-context examples and the output was post-processed. In the zero-shot setting, Vicuna could not match the scores of GPT \\begin{table} \\begin{tabular}{l c c c c c c c c} \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{4}{c}{**Zero-Shot Prompt**} & \\multicolumn{4}{c}{**Few-Shot Prompt**} \\\\ \\cline{2-9} & BLEU & METEOR & TER & BERTScore & BLEU & METEOR & TER & BERTScore \\\\ \\hline LLaMA-7B & 0.06 & 0.21 & 1.03 & 0.84 & 0.11 & 0.26 & 1.03 & 0.85 \\\\ LLaMA-7B + PP & 0.15 & 0.25 & 0.76 & 0.89 & 0.38 & 0.36 & 0.53 & 0.94 \\\\ Vicuna-7B & 0.27 & 0.35 & 0.68 & 0.92 & 0.39 & 0.38 & 0.64 & 0.93 \\\\ Vicuna-7B + PP & 0.27 & 0.35 & 0.68 & 0.92 & 0.43 & 0.39 & 0.51 & 0.95 \\\\ LLaMA-FT-7B & 0.47 & 0.40 & 0.55 & 0.94 & 0.47 & 0.40 & 0.55 & 0.94 \\\\ LLaMA-FT-7B + PP & **0.52** & **0.41** & **0.42** & **0.96** & **0.53** & **0.41** & **0.42** & **0.96** \\\\ GPT-3.5-Turbo & 0.41 & **0.41** & 0.56 & 0.95 & 0.39 & 0.40 & 0.65 & 0.94 \\\\ GPT-3.5-Turbo + PP & 0.41 & **0.41** & 0.56 & 0.95 & 0.44 & **0.41** & 0.50 & 0.95 \\\\ \\hline Copy-Baseline & 0.02 & 0.02 & 0.95 & 0.79 & 0.02 & 0.02 & 0.95 & 0.79 \\\\ \\hline \\end{tabular} \\end{table} Table 1: Zero-shot and few-shot performance metrics on WebNLG test set evaluated by BLEU, METEOR, TER, and BERTScore-F1 (+ PP denotes post-processed model output). Bold values indicate the best value per metric. 3.5-Turbo but outperformed LLaMA-7B. Although LLaMA is the worst-performing model, it claims the most significant improvements through few-shot prompting and post-processing, with scores not too far from Vicuna. The metrics collectively suggest that all tested LLMs can generate reasonable output text from knowledge graph triples. Besides, we observe that while all models show improvements with few-shot prompting or post-processing, models trained on conversations like Vicuna require less post-processing and exhibit better zero-shot proficiency, resulting in comparatively smaller performance gains from post-processed outputs or in-context examples. Analysis and DiscussionOur experiments reveal that LLMs, especially those fine-tuned on conversations, are capable of triple-to-text generation without explicit training. However, as expected, the fine-tuned LLaMA-FT-7B model achieved the best overall performance. The WebNLG triple verbalization task involves different subtasks, such as segmentation of the input data, lexicalization of the DBpedia properties, information aggregation, and surface realization of grammatically correct text (Colin et al., 2016). All of these subtasks are handled by LLMs in an end-to-end manner. In direct comparison to state-of-the-art models evaluated on WebNLG like _Control Prefixes_ (BLEU: 0.62, METEOR: 0.45, TER: 0.35) from Clive et al. (2022) or _T5-Large+Wiki+Position_ (BLEU: 0.61, METEOR: 0.44, TER: 0.36, BERTScore: 0.96) from Wang et al. (2021), the LLMs' lexical similarity metrics are worse. Yet, when looking at semantic similarity, the BERTScore metric of the LLaMA-FT-7B model is identical at 0.96. We hypothesize that the lower lexical similarity is partly caused by the concise writing style of the WebNLG human ground truth verbalizations, aggregating as much information as possible in succinct sentences. While many WebNLG annotations are as short as possible (e.g., _\"The 98.0 minute film Super Capers starring Danielle Harris was written by the director Ray Griggs.\"_), the more verbose output of LLMs like GPT-3.5-Turbo consists of multiple sentences (e.g., _\"Danielle Harris stars in the movie Super Capers. The writer of the movie is Ray Griggs. The movie has a runtime of 98.0 minutes.\"_). This concise writing style can be better learned and replicated by LLaMA-FT and other fine-tuned models. We also observed that the LLMs had a tendency to occasionally use passive voice, initiating sentences with the object because the input triples were ordered as (_object, property, subject_), whereas the human annotators started with the subject using an active voice structure. This might be another factor of lower lexical similarity metrics, although the semantic content was the same. With a larger number of input triples, models struggle more to transform structured information into cohesive text. Figure 1 illustrates the decreasing model performance when confronted with multiple triples. While all four LLMs follow the same trend, the performance loss seems to be a tapering decrease. Besides, we analyzed model performance differences across the 16 triple categories and found a similar pattern that the worst-performing categories, such as _Food_, _SportsTeam_, or _ComicsCharacter_ also had the highest average triple count per example. Since aggregating information into short sentences is also desired in conversational user interactions, we compared the sentence count of generated predictions for each model regarding the number of input triples. As can be discerned from Figure 2 in Appendix A, the fine-tuned LLaMA-FT model produces sentences in direct proportion to the number of input triples in alignment with the human annotations. Vicuna and GPT-3.5-Turbo, which have been explicitly trained on conversation data, exhibit similar generation behavior. While LLaMA-FT produces the fewest sentences, Vicuna seems to be a bit less verbose than GPT-3.5-Turbo. In contrast, text outputs from LLaMA contain, on average, the largest number of sentences and show a much higher variance. This suggests that fine-tuning LLMs on instructions from dialogue corpora improves adherence to concise triple verbalization. After conducting the automatic evaluation, we Figure 1: Comparison of BLEU score by number of triples for few-shot models with post-processing. manually examined the model predictions to gauge their reliability and grouped the most common issues into five types as presented in Table 4 in Appendix A. For example, the LLMs sometimes misinterpreted the prompt, failed to lexicalize triples correctly, or produced inaccurate information. Most of these issues occurred in zero-shot predictions from LLaMA or Vicuna, whereas GPT-3.5-Turbo produced the most reliable outputs. To obtain more profound insights into the model-specific occurrence rates of the issue types, two researchers jointly evaluated a sample of 75 zero- and 75 few-shot predictions for the lowest averaged BLEU and METEOR scores across all models. The obtained results are summarized in Table 2. Looking at the relative frequencies, it can be seen that the LLaMA base model has the highest incidence of issues from all types, followed by Vicuna and then LLaMA-FT with better reliability, and GPT-3.5-Turbo as the most dependable model. As to be expected from instruction-tuned and fine-tuned models, LLaMA-FT, Vicuna, and GPT-3.5-Turbo demonstrate a much greater ability to generate zero-shot output that aligns with the given prompt. Conversely, LLaMA tended to misinterpret the prompt, failing to produce the desired output format in nearly two-thirds of the evaluated instances (0.65). Interestingly, off-prompt issues could be effectively resolved in all models by including few-shot examples in the prompt. While few-shot prompting reduced off-prompt generations and caused the LLMs to produce actual sentences based on the graph triples, this led to a relative increase of inaccurate generations, such as hallucinated information, twisted numbers, or omitted facts from the triples. Occasionally, the relationships within these triples were also compromised. The rate of inaccurate zero-shot output in LLaMA (0.60) and Vicuna (0.41) was three to four times higher in comparison to GPT-3.5-Turbo (0.13). Another issue type where the usefulness of few-shot examples became evident is unlexicalized triples, meaning the translation of entities and relations into their intact word form. This was observed across all models except LLaMA-FT, with LLaMA and Vicuna particularly affected. Providing in-context examples with lexicalized triples could completely resolve unlexicalized triples for all models. Problems with redundancy, which involves the unnecessary repetition of information, are mostly associated with LLaMA. This was due to some instances where LLaMA became stuck in a loop, repeatedly generating the same sequence until the maximum token limit was reached. In contrast, this issue type appears to be less of a problem for the other models. Lastly, there are rare cases in which the LLM generated output in a language other than the prompt language English. This happened, for example, when most of the input triples contained words in Spanish. Only Vicuna faced translation issues in our benchmark test, specifically in zero-shot scenarios. This behavior may be attributed to its diverse fine-tuning dataset that contains text translation instructions."
    },
    {
      "title": "4 Conclusion",
      "text": "We compared the abilities of LLMs in knowledge-based text generation. Our results indicate that even smaller 7B-LLMs exhibit reasonable performance in verbalizing triples, conveying intended meanings and facts in a coherent manner, although they might not always be factually accurate or perfectly replicate the writing style of human annotations. We also discussed model-specific differences and common generation issues that can be mitigated through few-shot prompting or fine-tuning. In future work, we plan to investigate how our findings generalize to more complex graph data structures. \\begin{table} \\begin{tabular}{l c c c c} \\hline \\hline **Issue Type** & **LLaMA-7B** & **Vicuna-7B** & **LLaMA-FT-7B** & **GPT-3.5-Turbo** \\\\ & \\multicolumn{3}{c}{relative frequency: zero-shot / few-shot} \\\\ \\hline Inaccurate & 0.60* / 0.61 & 0.41* / 0.48 & 0.19 / 0.17 & 0.13 / 0.11 \\\\ Mistranslated & - / - & 0.01* / - & - / - & - / - \\\\ Off-prompt & 0.65 / - & 0.27 / - & - / - & - / - \\\\ Redundant & 0.23* / 0.07 & 0.02* / - & - / 0.01 & 0.01 / 0.01 \\\\ Unlexicalized & 0.69* / - & 0.27* / - & - / - & 0.07 / - \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Relative frequency of issue types for zero-shot and few-shot prompts in evaluated sample of 150 predictions with lowest averaged BLEU and METEOR scores. For values marked with “*”, the relative frequency only considers generations being on-prompt."
    },
    {
      "title": "5 Limitations",
      "text": "Our comparative analysis has certain limitations. We focus solely on text generation based on knowledge graph triples, and we acknowledge that verbalizing entire subgraphs or producing graph queries are other important tasks worth exploring. Nonetheless, by studying semantic triples, we can still derive valuable insights about the performance of LLMs for processing more complex graph data structures. In that regard, it is recommended to expand the comparison with human evaluations that go beyond automatically calculated metrics and to assess more models, particularly those trained on source code or documents with structured data. Further, the employed test dataset is limited to English triples. Since pre-training corpora of LLMs primarily consist of English text data, they likely work better where entities and relations correspond to meaningful English words or morphemes. Consequently, it is to be expected that LLMs exhibit worse performance on multilingual benchmarks with more morphologically rich languages, such as Russian, which is also part of the WebNLG dataset."
    },
    {
      "title": "6 Ethical Considerations",
      "text": "Our experiments were conducted on the publicly available WebNLG dataset, ensuring that no demographic or identifying information about individuals was processed or disclosed. Because our focus was not on addressing well-documented issues like privacy or biases associated with LLMs, we acknowledge potential risks and concerns in line with similar studies dealing with LLMs. The experiments with LLaMA, LLaMA-FT, and Vicuna were executed on a single NVIDIA V100 GPU and required relatively low computational resources, with around one GPU hour of inference time per model."
    },
    {
      "title": "Acknowledgements",
      "text": "This work has been supported by the German Federal Ministry of Education and Research (BMBF) Software Campus grant 01IS17049."
    },
    {
      "title": "References",
      "text": "* M. Aliannejadi, L. Azzopardi, H. Zamani, E. Kanoulas, P. Thomas, and N. Craswell (2021)Analysing mixed initiatives and search strategies during conversational search. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, CIKM '21, New York, NY, USA, pp. 16-26. External Links: ISBN 978-1-460-3873-3 Cited by: SS1. * S. Banerjee and A. Lavie (2005)METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Ann Arbor, Michigan, pp. 65-72. External Links: ISBN 978-1-460-3873-3 Cited by: SS1. * T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020)Language models are few-shot learners. In Advances in Neural Information Processing Systems, Vol. 33, pp. 1877-1901. External Links: Link, Document Cited by: SS1. * T. C. Ferreira, C. Gardent, N. Ilinykh, C. van der Lee, S. Mille, D. Moussallem, and A. Shimorina (2020)The 2020 bilingual bi-directional WebNLG+ shared task: overview and evaluation results (WebNLG+ 2020). In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), Dublin, Ireland, pp. 55-76. External Links: Link, Document Cited by: SS1. * T. C. Ferreira, C. van der Lee, E. van Miltenburg, and E. Krahmer (2019)Neural data-to-text generation: a comparison between pipeline and end-to-end architectures. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China, pp. 552-562. External Links: Link, Document Cited by: SS1. * Z. Chen, H. Eavani, W. Chen, Y. Liu, and W. Y. Wang (2020)Few-shot NLG with pre-trained language model. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online, pp. 183-190. External Links: Link, Document Cited by: SS1. * W. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing (2023)Vicuna: an open-source chatbot impressing gpt-4 with 90%* chatgpt quality. LMSYS Org Blog. Cited by: SS1. * J. Clive, K. Cao, and M. Rei (2022)Control prefixes for parameter-efficient text generation. In Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM),pages 363-382, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. * Colin et al. (2016) Emilie Colin, Claire Gardent, Yassine M'rabet, Shashi Narayan, and Laura Perez-Beltrachini. 2016. The WebNLG challenge: Generating text from DBPedia data. In _Proceedings of the 9th International Natural Language Generation conference_, pages 163-167, Edinburgh, UK. Association for Computational Linguistics. * Dou et al. (2022) Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, and Yejin Choi. 2022. Is GPT-3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 7250-7274, Dublin, Ireland. Association for Computational Linguistics. * Han et al. (2023) Jiuzhou Han, Nigel Collier, Wray Buntine, and Ehsan Shareghi. 2023. Pive: Prompting with iterative verification improving graph-based generative capability of llms. _arXiv:2305.12392_. * Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_. * Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. _ACM Comput. Surv._, 55(12). * Kasner and Dusek (2022) Zdenek Kasner and Ondrej Dusek. 2022. Neural pipeline for zero-shot data-to-text generation. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3914-3932, Dublin, Ireland. Association for Computational Linguistics. * Kasner et al. (2023) Zdenek Kasner, Ioannis Konstas, and Ondrej Dusek. 2023. Mind the labels: Describing relations in knowledge graphs with pretrained models. In _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics_, pages 2398-2415, Dubrovnik, Croatia. Association for Computational Linguistics. * Li et al. (2021) Junyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji rong Wen. 2021. Pretrained language models for text generation: A survey. In _International Joint Conference on Artificial Intelligence_. * Liu et al. (2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35. * Liu et al. (2021) Qian Liu, Dejian Yang, Jiahui Zhang, Jiaqi Guo, Bin Zhou, and Jian-Guang Lou. 2021. Awakening latent grounding from pretrained language models for semantic parsing. In _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 1174-1189, Online. Association for Computational Linguistics. * Moryossef et al. (2019) Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019. Step-by-step: Separating planning from realization in neural data-to-text generation. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2267-2277, Minneapolis, Minnesota. Association for Computational Linguistics. * OpenAI (2022) OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. _OpenAI_. * Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. * Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. _OpenAI_. * Radlinski and Craswell (2017) Filip Radlinski and Nick Craswell. 2017. A theoretical framework for conversational search. In _Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval_, CHIIR '17, page 117-126, New York, NY, USA. Association for Computing Machinery. * Schmitt et al. (2020) Martin Schmitt, Sahand Sharifzadeh, Volker Tresp, and Hinrich Schutze. 2020. An unsupervised joint system for text generation from knowledge graphs and semantic parsing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 7117-7130, Online. Association for Computational Linguistics. * Schneider et al. (2024) Phillip Schneider, Manuel Klettner, Kristiina Jokinen, Elena Simperl, and Florian Matthes. 2024. Evaluating large language models in semantic parsing for conversational question answering over knowledge graphs. In _Proceedings of the 16th International Conference on Agents and Artificial Intelligence_, Rome, Italy. SCITEPRESS. * Schneider et al. (2022) Phillip Schneider, Tim Schopf, Juraj Vladika, Mikhail Galkin, Elena Simperl, and Florian Matthes. 2022. A decade of knowledge graphs in natural language processing: A survey. In _Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 601-614, Online only. Association for Computational Linguistics. * Snover et al. (2006) Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. InProceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers_, pages 223-231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas. * Su et al. (2021) Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, and Nigel Collier. 2021. Plan-then-generate: Controlled data-to-text generation via planning. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 895-909, Punta Cana, Dominican Republic. Association for Computational Linguistics. * Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. _arXiv:2201.08239_. * Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. _arXiv:2302.13971_. * Wang et al. (2021) Qingyun Wang, Semih Yavuz, Xi Victoria Lin, Heng Ji, and Nazneen Rajani. 2021. Stage-wise fine-tuning for graph-to-text generation. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop_, pages 16-22, Online. Association for Computational Linguistics. * Zhang et al. (2020) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In _International Conference on Learning Representations_. Appendix The Appendix provides further insights into the results of our research, including the model prompts in full length (Table 3), an overview of common issue types identified in the predictions (Table 4), and a comparative distribution chart of generated sentences by number of triples for each model (Figure 2). \\begin{table} \\begin{tabular}{l l} \\hline **Prompt Type** & **Prompt Content** \\\\ \\hline Zero-shot & SYSTEM: Generate a concise text for the given set of triples. Ensure that the generated output only includes the provided information from the triples. \\\\ \\hline Few-shot & SYSTEM: Generate a concise text for the given set of triples. Ensure that the generated output only includes the provided information from the triples. \\\\ \\hline USER: Input triples: [{’object’: ’Mike_Mularkey’,’property’: ’coach’,’subject’: ’Tennessee_Titans’}] \\\\ ASSISTANT: Output text: Mike Mularkey is the coach of the Tennessee Ti-tans. \\\\ USER: Input triples: [{’object’: ’Albert_E_Austin’, ’property’: ’successor’, ’subject’: ’Alfred_N_Phillips’}, {’object’: ’Connecticut’, ’property’: ’birthPlace’, ’subject’: ’Alfred_N_Phillips’}, {’object’: ’United_States_House_of_Representatives’, ’property’: ’office’, ’subject’: ’Alfred_N_Phillips’}] \\\\ ASSISTANT: Output text: Albert E. Austin succeeded Alfred N. Phillips who was born in Connecticut and worked at the United States House of Representatives. \\\\ USER: Input triples: [{’object’: ’College_of_William_&_Mary’, ’property’: ’owner’, ’subject’: ’Alan_B_Miller_Hall’}, {’object’: ’2009-06-01’, ’property’: ’completionDate’, ’subject’: ’Alan_B_Miller_Hall’}, {’object’: ’101 Ukrop Way’, ’property’: ’address’, ’subject’: ’Alan_B_Miller_Hall’}, {’object’: ’Williamsburg_Virginia’, ’property’: ’location’, ’subject’: ’Alan_B_Miller_Hall’}, {’object’: ’Robert_A_M_Stern’, ’property’: ’architect’, ’subject’: ’Alan_B_Miller_Hall’}] \\\\ ASSISTANT: Output text: The Alan B Miller Hall’s location is 101 Ukrop Way, Williamsburg, Virginia. It was designed by Robert A.M. Stern and was completed on 1 June 2009. Its owner is the College of William and Mary. \\\\ \\hline \\end{tabular} \\end{table} Table 3: Overview of applied zero-shot and few-shot prompts. \\begin{table} \\begin{tabular}{l l l} \\hline **Issue Type** & **Definition** & **Example** \\\\ \\hline Inaccurate & Prediction is incomplete or factually incorrect. & GT: Binignit is a dessert that comes from the Philippines. \\\\ \\hline Mistranslated & Triples were verbalized in a language other than English. & GT: The Mexican based city of Ciudad Ayala in Morelos stands at 1147 metres above sea level and is in the PDT time zone. \\\\ & & PRED: Ciudad Ayala es parte de Morelos, que es un estado en Mexico. Ciudad Ayala tiene una elevación de 1147 metros sobre el nivel del mar [...] \\\\ \\hline Off-prompt & Prediction is unrelated to the prompt and contradicts desired output format. & GT: GMA Network is a public company. \\\\ & & PRED: The input triples are: [{’object’: ’Public_company’, ’property’: ’type’, ’subject’: ’GMA_Network_(company)’}]. \\\\ \\hline Redundant & Unnecessary repetition of triple information. & GT: The Pontiac Rageous, a car with a coupe body style, assembled in Detroit Michigan, was first and last produced in 1997. \\\\ & & PRED: 1997 Pontiac Rageous was produced in Detroit, Michigan, in 1997. It was a coupe. It was assembled in 1997. It was produced in 1997 [...] \\\\ \\hline Unlexicalized & Entities or relations are not lexicalized. & GT: The Fellowship of the Ring was followed by The Two Towers. \\\\ & & PRED: The\\_Fellowship\\_of\\_the\\_Ring was followed by The\\_Two\\_Towers. \\\\ \\hline \\end{tabular} \\end{table} Table 4: Overview of five identified issue types with examples from generated model predictions (PRED) and human ground truth annotations (GT). Figure 2: Distribution of model generated sentences by number of triples for few-shot models with post-processing. The size of the dots reflects the occurrence frequency. The ground truth annotations are denoted as “Human”."
    }
  ]
}