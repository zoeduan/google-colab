{
  "title": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models",
  "authors": [
    "Tao Fan",
    "Yan Kang",
    "Guoqiang Ma",
    "Weijing Chen",
    "Wenbin Wei",
    "Lixin Fan",
    "Qiang Yang"
  ],
  "abstract": "\n Large Language Models (LLMs), such as Chat-GPT, LLaMA, GLM, and PaLM, have exhibited remarkable performances across various tasks in recent years. However, LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises. To address these challenges, we propose FATE-LLM, an industrial-grade federated learning framework for large language models. FATE-LLM (1) facilitates federated learning for large language models (coined FedLLM); (2) promotes efficient training of FedLLM using parameter-efficient finetuning methods; (3) protects the intellectual property of LLMs; (4) preserves data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at  https://github.com/FederatedAI/FATE-LLM  to facilitate the research of FedLLM and enable a broad range of industrial applications. \n",
  "references": [
    {
      "id": null,
      "title": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models",
      "authors": [
        "Tao Fan",
        "Yan Kang",
        "Guoqiang Ma",
        "Weijing Chen",
        "Wenbin Wei",
        "Lixin Fan",
        "Qiang Yang"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Language models are few-shot learners",
      "authors": [
        "Brown"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Autofednlp: An efficient fednlp framework",
      "authors": [
        "Cai"
      ],
      "year": "2022",
      "venue": "Autofednlp: An efficient fednlp framework",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "When homomorphic encryption marries secret sharing: Secure large-scale sparse logistic regression and applications in risk control",
      "authors": [
        "Chen"
      ],
      "year": "2012",
      "venue": "Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Privacy-preserving federated adversarial domain adaptation over feature groups for interpretability",
      "authors": [
        "Hu"
      ],
      "year": "2021",
      "venue": "Low-rank adaptation of large language models",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Rouge: A package for automatic evaluation of summaries",
      "authors": [
        "Lin",
        "Chin-Yew Lin"
      ],
      "year": "2004",
      "venue": "Text summarization branches out",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "P-tuning v2: Prompt tuning can be comparable to finetuning universally across scales and tasks",
      "authors": [
        "Liu"
      ],
      "year": "2021",
      "venue": "P-tuning v2: Prompt tuning can be comparable to finetuning universally across scales and tasks",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Fate: An industrial grade platform for collaborative learning with data protection",
      "authors": [
        "Liu"
      ],
      "year": "2021",
      "venue": "J. Mach. Learn. Res",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data",
      "authors": [
        "Liu"
      ],
      "year": "2017",
      "venue": "Artificial intelligence and statistics",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "",
      "authors": [
        "Openai"
      ],
      "year": "2022",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Public-key cryptosystems based on composite degree residuosity classes",
      "authors": [
        "Pascal Paillier",
        "; Paillier",
        "Papineni"
      ],
      "year": "1999",
      "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Long and diverse text generation with planning-based hierarchical variational model",
      "authors": [
        "Adi Shamir",
        "; Shamir",
        "Shao"
      ],
      "year": "1979",
      "venue": "Communications of the ACM",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "Touvron"
      ],
      "year": "2022",
      "venue": "Will we run out of data? an analysis of the limits of scaling datasets in machine learning",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "",
      "authors": [
        "Wang"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Yongfeng Huang, and Xing Xie. Communication-efficient federated learning via knowledge distillation",
      "authors": [
        "Manzil Xu",
        "; Zaheer",
        "Wu"
      ],
      "year": "2022",
      "venue": "Workshop on Challenges in Deployable Generative AI at International Conference on Machine Learning (ICML)",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Offsite-tuning: Transfer learning without full model",
      "authors": [
        "Xiao"
      ],
      "year": "2019",
      "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond",
      "authors": [
        "Yang"
      ],
      "year": "2023",
      "venue": "Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "When federated learning meets pre-trained language models' parameter-efficient tuning methods",
      "authors": [
        "Zhang"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Fate-Llm: A Industrial Grade Federated Learning Framework For Large Language Models",
      "text": "Tao Fan\\({}^{1,2}\\), Yan Kang\\({}^{2}\\), Guoqiang Ma\\({}^{2}\\), Weijing Chen\\({}^{2}\\), Wenbin Wei\\({}^{2}\\), Lixin Fan\\({}^{2}\\), Qiang Yang\\({}^{1,2}\\) \\({}^{1}\\) Hong Kong University of Science and Technology, China \\({}^{2}\\) WeBank, China tfanac@cse.ust.hk, yangkang@webank.com, zotrseeewma@webank.com, weijingchen@webank.com, sagewei@webank.com, lixinfan@webank.com, qyang@cse.ust.hk Corresponding Author"
    },
    {
      "title": "Abstract",
      "text": "Large Language Models (LLMs), such as Chat-GPT, LLaMA, GLM, and PaLM, have exhibited remarkable performances across various tasks in recent years. However, LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises. To address these challenges, we propose FATE-LLM, an industrial-grade federated learning framework for large language models. FATE-LLM (1) facilitates federated learning for large language models (coined FedLLM); (2) promotes efficient training of FedLLM using parameter-efficient fine-tuning methods; (3) protects the intellectual property of LLMs; (4) preserves data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at [https://github.com/FederatedAI/FATE-LLM](https://github.com/FederatedAI/FATE-LLM) to facilitate the research of FedLLM and enable a broad range of industrial applications."
    },
    {
      "title": "1 Introduction",
      "text": "In recent few years, the advent of large language models (LLMs) [15, 16] has been reshaping the field of artificial intelligence. In particular, the most advanced LLMs, such as ChatGPT [1], GPT-4 [1], and PaLM [11] that boast billions of parameters have gained considerable attention due to their remarkable performance in a variety of natural language generation tasks. Many open-sourced LLMs with high performance have been released, and the public's enthusiasm for research and application of LLMs has been stimulated. However, grounding LLMs in real-world applications faces many challenges. The two main challenges are (i) training LLMs consumes vast computing resources, which prevents LLMs from being adopted by small and medium-sized companies with limited computing resources; (ii) training LLMs requires a large amount of public data, which may run out soon [20]. Federated learning (FL) [13][15], a privacy-preserving collaborative machine learning paradigm, is a promising approach to deal with these two challenges. For one thing, FL enables many companies with different computing resources to collaboratively train powerful machine learning models such that the computational burden of training large models can be alleviated. For another, massive high-quality data are scattered among companies that are typically isolated from each other, and FL can exploit these data silos in a privacy-preserving way. In this work, we propose FATE-LLM, built upon FATE (Federated AI Technology Enabler) [14], to facilitate federated learning for large language models. More specifically, FATE-LLM (1) enables federated learning for both homogeneous and heterogeneous large language models (FedLLM); (2) promotes efficient training of FedLLM through parameter-efficient fine-tuning methods, such as LoRA [12] and P-Tuning-v2 [14]; (3) protects the intellectual property of LLMs using federated intellectual property protection approach [10]; (4) protects data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at [https://github.com/FederatedAI/FATE-LLM](https://github.com/FederatedAI/FATE-LLM) to promote the research of FedLLM and enable a broad range of industrial applications. Figure 1: **Large Language Models are federated on FATE.**Related Work In this section, we briefly review related work regarding large language models and federated learning."
    },
    {
      "title": "Large Language Models",
      "text": "The advancements in large language models(LLMs) have led to significant advances in a variety of NLP tasks. A great example of LLMs application is ChatGPT[14]. ChatGPT is fine-tuned from the generative pretrained transformer GPT-3.5, which was trained on a blend of text and code. ChatGPT applies reinforcement learning from human feedback (RLHF), which has become a promising way to align LLMs with a human's intent. LLMs are generally divided into two categories: encoder-decoder or encoder-only large language models and decoder-only large language models [11]. Bert [15] is the representative of encoder-only large language models. GPTs [12] is the representative of decoder-only large language models. At the early stage of LLMs development, decoder-only LLMs were not as popular as encoder-only and encoder-decoder LLMs. However, after 2021, with the introduction of GPT-3 [1], decoder-only LLMs experienced a significant boom. At the same time, after the initial explosion brought about by BERT [15], encoder-only LLMs gradually began to fade away. Recently, many decoder-only LLMs have been released, such as LLaMA [21], OPT [22], PaLM [20], and BLOOM [23]. These LLMs demonstrated reasonable few-/zero-shot performance via prompting and in-context learning."
    },
    {
      "title": "Federated Learning",
      "text": "Federated learning (FL) [16][22] is a distributed machine learning paradigm that enables clients (devices or organizations) to train a machine learning model collaboratively without exposing clients' data. Unlike traditional centralized machine learning techniques, data are fixed locally rather than being gathered in a central server, which exists many of the systemic privacy risks and costs [17]. Hence, FL is a promising approach to deal with this data isolation challenge. To enhance data privacy, federated learning uses a variety of secure computing protocols. The most popular protocols are Homomorphic Encryption (HE) [14], Multi-Party Computation(MPC) [2][15], and Differential Privacy (DP) [16]. In recent years, the literature has presented various algorithms in the FL setting. [1] proposed vertical logistic regression (VLR) using homomorphic encryption (HE) to protect data privacy. [2] further enhanced the privacy-preserving capability of VLR by employing a hybrid strategy combining HE and secret sharing (SS). [2] proposed the SecureBoost, a VFL version of XGBoost, that leverages HE to protect the parameters exchanged among parties. [18] applied a semi-supervised learning method to estimate missing features and labels for further training. [16] proposed Secure Aggregation to enhance data protection."
    },
    {
      "title": "3 Fate-Llm System Design",
      "text": "We introduce the FATE-LLM system, including its components, architecture, and roadmap."
    },
    {
      "title": "Overview Of Fate-Llm System",
      "text": "FATE-LLM1 was open-sourced as a submodule of FATE, and it contains three components: Communication-Efficient Hub, FedLLM Model Hub, and FedLLM Privacy Hub. Figure 2 overviews the FATE-LLM system. Footnote 1: FATE-LLM was open-sourced in April 2023 in the FATE Community and is running on the infrastructure of FATE. **The Communication-Efficient Hub** integrates a variety of communication-efficient methods into FedLLM to reduce the communication cost for training LLMs, including parameter-efficiency fine-tuning (PEFT) [22] methods (e.g., Adapter Tuning [13] and Prompt Tuning [20], Knowledge Distillation(KD) [21], and Model Quantization [22]. More specifically, [22] proposed PETuning methods that can reduce the communication overhead by \\(1\\sim 2\\) orders of magnitude under the FL setting compared with full fine-tuning. They also found that PETuning methods can bring down local model adaptation costs for clients in FL systems. These results imply that FL clients (e.g., devices) with limited storage capacity can benefit from PETuning methods since these methods enable sharing an LLM across different tasks and maintaining a few parameters for each task, reducing the storage requirement. **The FedLLM Model Hub** integrates a variety of mainstream LLMs, including BERT [12], GPTs [12], ChatGLM-6B [15], LLaMA [21], BLOOM [23], and Baiduan [20]. These LLMs have different architectures and sizes and can be applied in different scenarios. **The FedLLM Trainer Hub** offers a variety of training methods for different federated LLMs learning scenarios, including FedHomoLLM, FedHeteroLLM, FedCoLLM, and FedOST. In FL, clients may have sufficient computing resources to train LLMs of the same size. However, in many heterogeneous scenarios, clients are likely to have quite different computing or data resources so that they can afford to train LLMs of quite different sizes. FATE-LLM offers Federated Homogeneous LLMs (FedHomoLLM) and Federated Figure 2: **Components of the FATE-LLM system.** Heterogeneous LLMs (FedHeteroLLM) to support both scenarios. FedHomoLLM leverages PEFT techniques to train clients' LLMs with the same architecture and size (illustrated in Figure 3(a)). FedHeteroLLM leverages knowledge distillation (KD) [2] and PEFT techniques to deal with the FL scenario where FL clients own LLMs of different sizes (illustrated in Figure 3(b)). Specifically, each client in FedHeteroLLM leverages KD to learn a mentee model from its local pre-trained LLM. Then, all clients send adaptor or prompt parameters to the server for secure aggregation. Next, the server dispatches the aggregated model to all clients for the next round of training. Initializing clients with an LLM distilled from a larger one hosted by the server enables federated LLMs to obtain a better global model more efficiently than starting clients' models from random initialization [23]. On the other hand, the domain knowledge captured by clients' local LLMs allows the server's larger LLM to continue to evolve. FATE offers the FedCoLLM (Federated Co-tuning LLM) framework to co-evolve the LLMs of the server and clients. Figure 3(c) illustrates the FedCoLLM. Specifically, in FedCoLLM, each client having a LLMa-7B model conducts federated learning applying PEFT techniques. On the server side, the server distills the knowledge between its LLMa-65B model and the aggregated LLMa-7B mode to co-evolve models on both sides. [18] proposed Offsite-Tuning, a privacy-preserving and efficient transfer learning framework that can adapt an LLM to downstream tasks without access to the LLM's full weights. More specifically, in Offsite-Tuning, the server sends two adaptors and an emulator of its LLM to a client, which in turn finetunes adaptors with the help of the frozen emulator using its domain-specific data. Next, the client sends adaptors back to the server, which then plugs them into its LLM to form an adapted LLM for the client. The Offsite-Tuning has the potential to protect the client's data privacy and the server's model property. FATE-LLM offers the FedOST (Federated OffSite-Tuning) that extends the Offsite-Tuning framework to the federated learning setting (see Figure 3(d)). In FedOST, multiple clients collaboratively train two global adaptors that adapt the LLM to all clients. FedOST brings two additional benefits than Offsite-Tuning: (1) FedOST enhances data privacy by adopting secure aggregation, and (2) it adapts an LLM to clients that did not even participate in the FL because of the generalization of the FL global model. Figure 3: FATE-LLM Trainers. FATE-LLM offers four trainers for four different federated LLM learning scenarios. [MISSING_PAGE_FAIL:4]"
    },
    {
      "title": "Communication Cost",
      "text": "We investigate the communication cost for FedLLM using LoRA and P-Tuning-v2 in terms of the size of parameters to be fine-tuned. Table 3 reports the results, and it shows that FedLLM using LoRA consumes 0.058% communication cost of FedLLM fine-tuning all parameters, while FedLLM using P-Tuning-v2 accounts for 0.475% communication cost of FedLLM fine-tuning all parameters."
    },
    {
      "title": "Communication Cost",
      "text": "We investigate the communication cost for FedLLM using LoRA and P-Tuning-v2 in terms of the size of parameters to be fine-tuned. Table 3 reports the results, and it shows that FedLLM using LoRA consumes 0.058% communication cost of FedLLM fine-tuning all parameters, while FedLLM using P-Tuning-v2 accounts for 0.475% communication cost of FedLLM fine-tuning all parameters."
    },
    {
      "title": "5 Conclusions And Future Work",
      "text": "We proposed FATE-LLM, an industrial-grade federated learning framework for large language models(FedLLM). As an open-sourced software, FATE-LLM encourages collaboration among the research and industry communities and expects to receive increasing feedback on its use. In the future, we may consider research directions: (1) reconcile LLMs of different model architectures during FL fine-tuning; (2) fine-tune private LLMs of one party using private data of another party without compromising the data privacy and model ownership; (3) protect the privacy of user prompts efficiently in the inference stage; (4) apply FedLLM to vertical federated learning [11]."
    },
    {
      "title": "References",
      "text": "* [1]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1. * [2]D. Cai, Y. Wu, S. Wang, F. Xiaozhu Lin, and M. Xu (2022) Autofednlp: an efficient fednlp framework. arXiv preprint arXiv:2205.10162. Cited by: SS1. * [3]C. Chen, J. Zhou, L. Wang, X. Wu, W. Fang, J. Tan, L. Wang, A. X. Liu, H. Wang, and C. Hong (2021) When homomorphic encryption marries secret sharing: secure large-scale sparse logistic regression and applications in risk control. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 2652-2662. Cited by: SS1. * [4]K. Cheng, T. Fan, Y. Jin, Y. Liu, T. Chen, D. Papadopoulos, and Q. Yang (2021) Secureboost: a lossless federated learning framework. IEEE Intelligent Systems36 (6), pp. 87-98. Cited by: SS1. * [5]A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. Won Chung, C. \\begin{table} \\begin{tabular}{c|c|c|c|c} \\hline Metrics & LoRA Federated & LoRA Centralized & LoRA Client-1 & LoRA Client-2 \\\\ \\hline \\hline Rouge-1 & 32.331 & 32.384 & 31.824 & 31.764 \\\\ \\hline Rouge-2 & 7.740 & 8.150 & 7.849 & 7.765 \\\\ \\hline Rouge-\\(l\\) & 25.600 & 25.830 & 25.408 & 25.404 \\\\ \\hline BLEU-4 & 8.344 & 8.730 & 8.340 & 8.366 \\\\ \\hline \\end{tabular} \\end{table} Table 1: FedLLM fine-tuning ChatGLM-6B using LoRA. \\begin{table} \\begin{tabular}{c|c|c|c|c} \\hline Metrics & P-Tuning-v2 Federated & P-Tuning-v2 Centralized & P-Tuning-v2 Client-1 & P-Tuning-v2 Client-2 \\\\ \\hline \\hline Rouge-1 & 32.227 & 32.184 & 31.362 & 31.18 \\\\ \\hline Rouge-2 & 7.644 & 8.048 & 7.472 & 7.478 \\\\ \\hline Rouge-\\(l\\) & 25.853 & 26.010 & 25.454 & 25.227 \\\\ \\hline BLEU-4 & 8.490 & 8.851 & 8.329 & 8.221 \\\\ \\hline \\end{tabular} \\end{table} Table 2: FedLLM fine-tuning ChatGLM-6B using P-Tuning-v2. \\begin{table} \\begin{tabular}{c|c|c} \\hline Methods & Model Size (MB) & Param Percent (\\%) \\\\ \\hline \\hline LoRA & 3.6 & 0.058 \\\\ \\hline P-Tuning-v2 & 29.3 & 0.475 \\\\ \\hline Fine-tune All & 6173 & 100 \\\\ \\hline \\end{tabular} \\end{table} Table 3: Comparison of communication cost for FedLLM fine-tuning all parameters of ChatGLM-6B, fine-tuning ChatGLM-6B using LoRA and P-Tuning-v2. Model Size denotes the size of parameters to be fine-tuned. Param Percent denotes the ratio of parameters to be fine-tuned to all parameters. Figure 6: **RoadMap of FATE-LLM**. Figure 7: Multiple clients leverage LoRA or P-Tuning-v2 to fine-time their local ChatGLM-6B models through federated learning. Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022. * [Damgard _et al._2012] Ivan Damgard, Valerio Pastro, Nigel Smart, and Sarah Zakarias. Multiparty computation from somewhat homomorphic encryption. In _Annual Cryptology Conference_, pages 643-662. Springer, 2012. * [Devlin _et al._2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018. * [Du _et al._2022] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GIm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335, 2022. * [Dwork _et al._2014] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014. * [Hardy _et al._2017] Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume Smith, and Brian Thorne. Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption. _arXiv preprint arXiv:1711.10677_, 2017. * [Hu _et al._2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021. * [Kairouz _et al._2021] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021. * [Kang _et al._2022] Yan Kang, Yuanqin He, Jiahuan Luo, Tao Fan, Yang Liu, and Qiang Yang. Privacy-preserving federated adversarial domain adaptation over feature groups for interpretability. _IEEE Transactions on Big Data_, 2022. * [Li _et al._2022] Bowen Li, Lixin Fan, Hanlin Gu, Jie Li, and Qiang Yang. Fedipr: Ownership verification for federated deep neural network models. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022. * [Lin2004] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81, 2004. * [Liu _et al._2021a] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. _arXiv preprint arXiv:2110.07602_, 2021. * [Liu _et al._2021b] Yang Liu, Tao Fan, Tianjian Chen, Qian Xu, and Qiang Yang. Fate: An industrial grade platform for collaborative learning with data protection. _J. Mach. Learn. Res._, 22(226):1-6, 2021. * [Liu _et al._2022] Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, and Qiang Yang. Vertical federated learning. _arXiv preprint arXiv:2211.12814_, 2022. * [McMahan _et al._2017] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017. * [OpenAI2022] OpenAI. Chatgpt. 2022. * [OpenAI2023] OpenAI. Gpt-4. 2023. * [Paillier1999] Pascal Paillier. Public-key cryptosystems based on composite degree residuosity classes. In _International conference on the theory and applications of cryptographic techniques_, pages 223-238. Springer, 1999. * [Papineni _et al._2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318, 2002. * [Radford _et al._2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. * [Scao _et al._2022] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022. * [Shamir1979] Adi Shamir. How to share a secret. _Communications of the ACM_, 22(11):612-613, 1979. * [Shao _et al._2019] Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. Long and diverse text generation with planning-based hierarchical variational model. _arXiv preprint arXiv:1908.06605_, 2019. * [Shen _et al._2020] Tao Shen, Jie Zhang, Xinkang Jia, Fengda Zhang, Gang Huang, Pan Zhou, Kun Kuang, Fei Wu, and Chao Wu. Federated mutual learning. _arXiv preprint arXiv:2006.16765_, 2020. * [Touvron _et al._2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023. * [Villalobos _et al._2022] Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. _arXiv preprint arXiv:2211.04325_, 2022. * [Wang _et al._2023] Boxin Wang, Yibo Jacky Zhang, Yuan Cao, Bo Li, H Brendan McMahan, Sewoong Oh, ZhengXu, and Manzil Zaheer. Can public large language models help private cross-device federated learning? _Workshop on Challenges in Deployable Generative AI at International Conference on Machine Learning (ICML)_, 2023. * [Wu _et al._2022] Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie. Communication-efficient federated learning via knowledge distillation. _Nature communications_, 13(1):2032, 2022. * [Xiao _et al._2023] Guangxuan Xiao, Ji Lin, and Song Han. Offsite-tuning: Transfer learning without full model. _arXiv preprint arXiv:2302.04870_, 2023. * [Yang _et al._2019] Qiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen, and Han Yu. Federated learning. _Synthesis Lectures on Artificial Intelligence and Machine Learning_, 13(3):1-207, 2019. * [Yang _et al._2023a] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, et al. Baichuan 2: Open large-scale language models. _arXiv preprint arXiv:2309.10305_, 2023. * [Yang _et al._2023b] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond. _arXiv preprint arXiv:2304.13712_, 2023. * [Zhang _et al._2018] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In _Proceedings of the European conference on computer vision (ECCV)_, pages 365-382, 2018. * [Zhang _et al._2022a] Susan Zhang, Stephen Roller, Nam Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022. * [Zhang _et al._2022b] Zhuo Zhang, Yuanhang Yang, Yong Dai, Lizhen Qu, and Zenglin Xu. When federated learning meets pre-trained language models' parameter-efficient tuning methods. _arXiv preprint arXiv:2212.10025_, 2022. * [Zhao _et al._2022] Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and Gongshen Liu. Reduce communication costs and preserve privacy: Prompt tuning method in federated learning. _arXiv preprint arXiv:2208.12268_, 2022. * [Zhou _et al._2023] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. _arXiv preprint arXiv:2302.09419_, 2023."
    }
  ]
}