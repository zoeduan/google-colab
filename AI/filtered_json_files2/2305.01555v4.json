{
  "title": "How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?",
  "authors": [
    "Xin Xu",
    "Yuqi Zhu",
    "Xiaohan Wang",
    "Ningyu Zhang",
    "Aakanksha Chowdhery",
    "Sharan Narang",
    "Jacob Devlin",
    "Maarten Bosma",
    "Gaurav Mishra",
    "Adam Roberts",
    "Hyung Paul Barham",
    "Won Chung",
    "Charles Sutton",
    "Sebastian Gehrmann",
    "Parker Schuh",
    "Kensen Shi",
    "Sasha Tsvyashchenko",
    "Joshua Maynez",
    "Abhishek Rao",
    "Parker Barnes",
    "Yi Tay",
    "Noam Shazeer",
    "Vin- Odkumar Prabhakaran",
    "Emily Reif",
    "Nan Du",
    "Ben Hutchinson",
    "Reiner Pope",
    "James Bradbury",
    "Jacob Austin",
    "Michael Isard",
    "Guy Gur-Ari",
    "Pengcheng Yin",
    "Toju Duke",
    "Anselm Levskaya",
    "Sanjay Ghemawat",
    "Sunipa Dev",
    "Henryk Michalewski",
    "Xavier Garcia",
    "Vedant Misra",
    "Kevin Robinson",
    "Liam Fedus",
    "Denny Zhou",
    "Daphne Ippolito",
    "David Luan",
    "Hyeontaek Lim",
    "Barret Zoph",
    "Alexander Spiridonov",
    "Ryan Sepassi",
    "David Dohan",
    "Shivani Agrawal",
    "Mark Omernick",
    "Andrew M Dai",
    "Thanumalayan Sankaranarayana Pillai",
    "Marie Pellat",
    "Aitor Lewkowycz",
    "Erica Mor- Eira",
    "Rewon Child",
    "Oleksandr Polozov",
    "Katherine Lee",
    "Zongwei Zhou",
    "Xuezhi Wang",
    "Brennan Saeta",
    "Mark Diaz",
    "Orhan Firat",
    "Michele Catasta",
    "Jason Wei",
    "Kathy Meier-Hellstern",
    "Douglas Eck",
    "Jeff Dean"
  ],
  "abstract": "\n Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction 1 . \n",
  "references": [
    {
      "id": null,
      "title": "How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?",
      "authors": [
        "Xin Xu",
        "Yuqi Zhu",
        "Xiaohan Wang",
        "Ningyu Zhang",
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann",
        "Parker Schuh",
        "Kensen Shi",
        "Sasha Tsvyashchenko",
        "Joshua Maynez",
        "Abhishek Rao",
        "Parker Barnes",
        "Yi Tay",
        "Noam Shazeer",
        "Vin- Odkumar Prabhakaran",
        "Emily Reif",
        "Nan Du",
        "Ben Hutchinson",
        "Reiner Pope",
        "James Bradbury",
        "Jacob Austin",
        "Michael Isard",
        "Guy Gur-Ari",
        "Pengcheng Yin",
        "Toju Duke",
        "Anselm Levskaya",
        "Sanjay Ghemawat",
        "Sunipa Dev",
        "Henryk Michalewski",
        "Xavier Garcia",
        "Vedant Misra",
        "Kevin Robinson",
        "Liam Fedus",
        "Denny Zhou",
        "Daphne Ippolito",
        "David Luan",
        "Hyeontaek Lim",
        "Barret Zoph",
        "Alexander Spiridonov",
        "Ryan Sepassi",
        "David Dohan",
        "Shivani Agrawal",
        "Mark Omernick",
        "Andrew M Dai",
        "Thanumalayan Sankaranarayana Pillai",
        "Marie Pellat",
        "Aitor Lewkowycz",
        "Erica Mor- Eira",
        "Rewon Child",
        "Oleksandr Polozov",
        "Katherine Lee",
        "Zongwei Zhou",
        "Xuezhi Wang",
        "Brennan Saeta",
        "Mark Diaz",
        "Orhan Firat",
        "Michele Catasta",
        "Jason Wei",
        "Kathy Meier-Hellstern",
        "Douglas Eck",
        "Jeff Dean"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Tacred revisited: A thorough evaluation of the tacred relation extraction task",
      "authors": [
        "Christoph Alt",
        "Aleksandra Gabryszak",
        "Leonhard Hennig"
      ],
      "year": "2020",
      "venue": "Proceedings of ACL",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "SciB-ERT: A pretrained language model for scientific text",
      "authors": [
        "Iz Beltagy",
        "Kyle Lo",
        "Arman Cohan"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1371"
    },
    {
      "id": "b2",
      "title": "Codekgc: Code language model for generative knowledge graph construction",
      "authors": [
        "Zhen Bi",
        "Jing Chen",
        "Yinuo Jiang",
        "Feiyu Xiong",
        "Wei Guo",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "year": "2023",
      "venue": "Codekgc: Code language model for generative knowledge graph construction",
      "doi": "10.48550/arXiv.2304.09048"
    },
    {
      "id": "b3",
      "title": "Towards realistic few-shot relation extraction",
      "authors": [
        "Sam Brody",
        "Sichao Wu",
        "Adrian Benton"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2021.emnlp-main.433"
    },
    {
      "id": "b4",
      "title": "Alec Radford, Ilya Sutskever, and Dario Amodei",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell",
        "Sandhini Agarwal",
        "Ariel Herbert-Voss",
        "Gretchen Krueger",
        "Tom Henighan",
        "Rewon Child",
        "Aditya Ramesh",
        "Daniel Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Chris Hesse",
        "Mark Chen",
        "Eric Sigler",
        "Mateusz Litwin",
        "Scott Gray",
        "Benjamin Chess",
        "Jack Clark",
        "Christopher Berner",
        "Sam Mccandlish"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Knowprompt: Knowledgeaware prompt-tuning with synergistic optimization for relation extraction",
      "authors": [
        "Xiang Chen",
        "Ningyu Zhang",
        "Xin Xie",
        "Shumin Deng",
        "Yunzhi Yao",
        "Chuanqi Tan",
        "Fei Huang",
        "Luo Si",
        "Huajun Chen",
        "; Dong",
        "Lei Li",
        "Damai Dai",
        "Ce Zheng",
        "Zhiyong Wu",
        "Baobao Chang",
        "Xu Sun",
        "Jingjing Xu",
        "Lei Li",
        "Zhifang Sui"
      ],
      "year": "2022",
      "venue": "A survey for in-context learning",
      "doi": "10.48550/arXiv.2301.00234"
    },
    {
      "id": "b6",
      "title": "Exploring task difficulty for few-shot relation extraction",
      "authors": [
        "Jiale Han",
        "Bo Cheng",
        "Wei Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.emnlp-main.204"
    },
    {
      "id": "b7",
      "title": "Generative prompt tuning for relation classification",
      "authors": [
        "Jiale Han",
        "Shuai Zhao",
        "Bo Cheng",
        "Shengkun Ma",
        "Wei Lu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Opennre: An open and extensible toolkit for neural relation extraction",
      "authors": [
        "Xu Han",
        "Tianyu Gao",
        "Yuan Yao",
        "Deming Ye",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",
      "doi": "10.18653/v1/D19-3029"
    },
    {
      "id": "b9",
      "title": "2021b. PTR: prompt tuning with rules for text classification",
      "authors": [
        "Xu Han",
        "Weilin Zhao",
        "Ning Ding",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "year": "",
      "venue": "2021b. PTR: prompt tuning with rules for text classification",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
      "authors": [
        "Xu Han",
        "Hao Zhu",
        "Pengfei Yu",
        "Ziyun Wang",
        "Yuan Yao",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1514"
    },
    {
      "id": "b11",
      "title": "Spanbert: Improving pre-training by representing and predicting spans",
      "authors": [
        "Mandar Joshi",
        "Danqi Chen",
        "Yinhan Liu",
        "Daniel S Weld",
        "Luke Zettlemoyer",
        "Omer Levy"
      ],
      "year": "2020",
      "venue": "Trans. Assoc. Comput. Linguistics",
      "doi": "10.1162/tacl_a_00300"
    },
    {
      "id": "b12",
      "title": "Co-training improves prompt-based learning for large language models",
      "authors": [
        "Hunter Lang",
        "Monica N Agrawal",
        "Yoon Kim",
        "David A Sontag"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized BERT pretraining approach",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
      "authors": [
        "Yao Lu",
        "Max Bartolo",
        "Alastair Moore",
        "Sebastian Riedel",
        "Pontus Stenetorp"
      ],
      "year": "2021",
      "venue": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Multi-task identification of entities, relations, and coreferencefor scientific knowledge graph construction",
      "authors": [
        "Yi Luan",
        "Luheng He",
        "Mari Ostendorf",
        "Hannaneh Hajishirzi"
      ],
      "year": "2018",
      "venue": "Proc. Conf. Empirical Methods Natural Language Process",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Relation classification with entity type restriction",
      "authors": [
        "Shengfei Lyu",
        "Huanhuan Chen"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.findings-acl.34"
    },
    {
      "id": "b17",
      "title": "Large language model is not a good few-shot information extractor, but a good reranker for hard samples! CoRR",
      "authors": [
        "Yubo Ma",
        "Yixin Cao",
        "Yongching Hong",
        "Aixin Sun"
      ],
      "year": "2023",
      "venue": "Large language model is not a good few-shot information extractor, but a good reranker for hard samples! CoRR",
      "doi": "10.48550/arXiv.2303.08559"
    },
    {
      "id": "b18",
      "title": "Chatgpt: Optimizing language models for dialogue",
      "authors": [
        "Openai"
      ],
      "year": "2022",
      "venue": "Chatgpt: Optimizing language models for dialogue",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "OpenAI. 2023a. Gpt-4 technical report",
      "authors": [],
      "year": "",
      "venue": "OpenAI. 2023a. Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "OpenAI. 2023b. Text-davinci-003",
      "authors": [],
      "year": "",
      "venue": "OpenAI. 2023b. Text-davinci-003",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll L Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul F Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback",
      "doi": "10.48550/arXiv.2203.02155"
    },
    {
      "id": "b22",
      "title": "Cícero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages",
      "authors": [
        "Giovanni Paolini",
        "Ben Athiwaratkun",
        "Jason Krone",
        "Jie Ma",
        "Alessandro Achille",
        "Rishita Anubhai"
      ],
      "year": "2021",
      "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Matching the blanks: Distributional similarity for relation learning",
      "authors": [
        "Baldini Livio",
        "Nicholas Soares",
        "Jeffrey Fitzgerald",
        "Tom Ling",
        "Kwiatkowski"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
      "doi": "10.18653/v1/p19-1279"
    },
    {
      "id": "b25",
      "title": "Learning from noisy labels with deep neural networks: A survey",
      "authors": [
        "Hwanjun Song",
        "Minseok Kim",
        "Dongmin Park",
        "Jae-Gil Lee"
      ],
      "year": "2020",
      "venue": "Learning from noisy labels with deep neural networks: A survey",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Re-tacred: Addressing shortcomings of the TACRED dataset",
      "authors": [
        "George Stoica"
      ],
      "year": "2021",
      "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service",
      "authors": [
        "Tianxiang Sun",
        "Yunfan Shao",
        "Hong Qian"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Galactica: A large language model for science",
      "authors": [
        "Ross Taylor",
        "Marcin Kardas",
        "Guillem Cucurull",
        "Thomas Scialom",
        "Anthony Hartshorn",
        "Elvis Saravia",
        "Andrew Poulton",
        "Viktor Kerkez",
        "Robert Stojnic"
      ],
      "year": "2022",
      "venue": "Galactica: A large language model for science",
      "doi": "10.48550/arXiv.2211.09085"
    },
    {
      "id": "b29",
      "title": "Want to reduce labeling cost? GPT-3 can help",
      "authors": [
        "Shuohang Wang",
        "Yang Liu",
        "Yichong Xu",
        "Chenguang Zhu",
        "Michael Zeng"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic",
      "doi": "10.18653/v1/2021.findings-emnlp.354"
    },
    {
      "id": "b30",
      "title": "Gdpnet: Refining latent multi-view graph for relation extraction",
      "authors": [
        "Fuzhao Xue",
        "Aixin Sun",
        "Hao Zhang",
        "Eng Siong",
        "Chng"
      ],
      "year": "2021",
      "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "LUKE: deep contextualized entity representations with entity-aware self-attention",
      "authors": [
        "Ikuya Yamada",
        "Akari Asai",
        "Hiroyuki Shindo",
        "Hideaki Takeda",
        "Yuji Matsumoto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2020.emnlp-main.523"
    },
    {
      "id": "b32",
      "title": "Entity conceptenhanced few-shot relation extraction",
      "authors": [
        "Shan Yang",
        "Yongfei Zhang",
        "Guanglin Niu",
        "Qinghua Zhao",
        "Shiliang Pu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-short.124"
    },
    {
      "id": "b33",
      "title": "Generative knowledge graph construction: A review",
      "authors": [
        "Hongbin Ye",
        "Ningyu Zhang",
        "Hui Chen",
        "Huajun Chen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Gpt3mix: Leveraging large-scale language models for text augmentation",
      "authors": [
        "Min Kang",
        "Dongju Yoo",
        "Jaewook Park",
        "Sang-Woo Kang",
        "Woo-Myoung Lee",
        "Park"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.findings-emnlp.192"
    },
    {
      "id": "b35",
      "title": "GLM-130B: an open bilingual pre-trained model",
      "authors": [
        "Aohan Zeng",
        "Xiao Liu",
        "Zhengxiao Du",
        "Zihan Wang",
        "Hanyu Lai",
        "Ming Ding",
        "Zhuoyi Yang",
        "Yifan Xu",
        "Wendi Zheng",
        "Xiao Xia",
        "Weng Lam Tam",
        "Zixuan Ma",
        "Yufei Xue",
        "Jidong Zhai",
        "Wenguang Chen",
        "Peng Zhang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "GLM-130B: an open bilingual pre-trained model",
      "doi": "10.48550/arXiv.2210.02414"
    },
    {
      "id": "b36",
      "title": "OPT: open pre-trained transformer language models",
      "authors": [
        "Susan Zhang",
        "Stephen Roller",
        "Naman Goyal",
        "Mikel Artetxe",
        "Moya Chen",
        "Shuohui Chen",
        "Christopher Dewan",
        "Mona T Diab",
        "Xian Li",
        "Xi Victoria Lin",
        "Todor Mihaylov",
        "Myle Ott",
        "Sam Shleifer",
        "Kurt Shuster",
        "Daniel Simig",
        "Punit Singh Koura",
        "Anjali Sridhar",
        "Tianlu Wang",
        "Luke Zettlemoyer"
      ],
      "year": "2022",
      "venue": "OPT: open pre-trained transformer language models",
      "doi": "10.48550/arXiv.2205.01068"
    },
    {
      "id": "b37",
      "title": "Position-aware attention and supervised data improve slot filling",
      "authors": [
        "Yuhao Zhang",
        "Victor Zhong",
        "Danqi Chen",
        "Gabor Angeli",
        "Christopher D Manning"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "An improved baseline for sentence-level relation extraction",
      "authors": [
        "Wenxuan Zhou",
        "Muhao Chen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, AACL/IJCNLP 2022",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "How To Unleash The Power Of Large Language Models For",
      "text": "Few-shot Relation Extraction? Xin Xu, Yuqi Zhu, Xiaohan Wang, Ningyu Zhang Zhejiang University & AZFT Joint Lab for Knowledge Engine {xxucs, wangxh07, zhangningyu}@zju.edu.cn Corresponding author."
    },
    {
      "title": "Abstract",
      "text": "Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction1. Footnote 1: Code is available in [https://github.com/zjunlp/DeepKE/tree/main/example/llm](https://github.com/zjunlp/DeepKE/tree/main/example/llm)."
    },
    {
      "title": "1 Introduction",
      "text": "Few-shot Relation Extraction (RE) appeals to many researchers in Natural Language Processing (NLP) due to the capability to extract textual information where only a few support examples are given [11, 12, 13, 14, 15]. Most previous works focus on fine-tuning [18, 19] or prompt-tuning [10, 11] with relatively small language models, e.g., RoBERTa [13]. Recently, with the scaling of model size and corpus size, large language models (LLMs) such as ChatGPT [3] and GPT-4 [3] have demonstrated powerful abilities by demonstrating only a few instances, a.k.a In-Context Learning [3]. Although LLMs have achieved remarkable results in many NLP tasks, their potential in few-shot relation extraction has not been fully explored yet. In this paper, we take GPT-3.5 [3] as an exemplary LLM to investigate how to maximize the utilization of LLMs for the few-shot relation extraction task with in-context learning and data generation. Different from text classification, the relation extraction task contains rich predefined schemas (e.g., entity and relation type constraints) and a relatively large and complex classification space with noisy data. We further design two simple-yet-effective strategies to unleash the power of large language models better: **task-related instructions** and **schema-constrained data generation**. We conduct exhaustive experiments on four well-known relation extraction datasets. Empirical results indicate that LLMs can potentially be advantageous to few-shot relation extraction and boost previous prompt learning performance."
    },
    {
      "title": "2 Background",
      "text": ""
    },
    {
      "title": "Few-Shot Relation Extraction",
      "text": "The relation extraction task aims to extract the relationship between head and tail entities within a plain context. Specifically, one instance for the relation extraction task consists of a context \\(\\mathbf{x}=\\{x_{1},x_{2},...,h,...,t,...,x_{|\\mathbf{x}|}\\}\\), head and tail entity mentions \\(\\mathbf{h}\\) and \\(\\mathbf{t}\\), entity types \\(\\mathbf{t}_{h}\\) and \\(\\mathbf{t}_{t}\\), and the relation \\(\\mathbf{y}\\in\\mathcal{Y}\\) between \\(\\mathbf{h}\\) and \\(\\mathbf{t}\\), where \\(\\mathcal{Y}\\) is the set of candidate relations. RE systems will predict \\(\\mathbf{y}\\) given \\(\\mathbf{x},\\mathbf{h},\\mathbf{t},\\mathbf{t}_{h}\\) and \\(\\mathbf{t}_{t}\\). For few-shot relation extraction, fine-tuning pre-trained language models (PLMs) is a direct solution [11, 12, 13, 14, 15]. To alleviate the gap between pre-training objectives and downstream applications, prompt tuning has recently been applied to relation extraction, especially for low-resource scenarios [10, 11, 12]. Most of those approachesutilize relatively small language models (RoBERTa Liu et al., 2019), GPT2 Radford et al. (2019)), demonstrating empirical success regarding few-shot relation extraction performance. To date, large language models have demonstrated powerful abilities by prompting a few instances without tuning Ding et al. (2022); however, the power of LLMs for few-shot relation extraction is little known."
    },
    {
      "title": "Large Language Models",
      "text": "Large language models, trained with exceedingly large corpora and often with a great number of parameters (\\(\\geq\\)10B), have achieved excellent performance in numerous downstream NLP tasks Taylor et al. (2022); Zhang et al. (2022); Zeng et al. (2022); Chowdhery et al. (2022); Ouyang et al. (2022). Compared to relatively small language models (SLMs), LLMs are usually not open-source and can not be fine-tuned, which is challenging for downstream task adaptation. Therefore, in-context learning Brown et al. (2020) is proposed to utilize prompts with a few demonstrations for few-shot learning. Previous studies Yoo et al. (2021); Wang et al. (2021) have investigated using LLMs for text classification and generation. In this work, we take the first step to study few-shot RE with large language models, which brings new challenges and insights."
    },
    {
      "title": "3 Llms For Few-Shot Relation Extraction",
      "text": "In this section, we introduce two strategies to utilize LLMs for relation extraction: 1) in-context learning (SS3.1); 2) data generation (SS3.2) with LLMs, as shown in Figure 1."
    },
    {
      "title": "In-Context Learning With Llms",
      "text": "The first strategy applies in-context learning (ICL) by providing LLMs with demonstrations in the prompt to elicit comprehension of the relation extraction task from LLMs. To this end, specific and compelling prompts for RE with demonstrations are manually constructed and designed to instruct LLMs to understand the relation extraction task and how to execute relation extraction. Considering aspects and characteristics of the relation extraction task, including task definition, candidate relation (label) words, entity types (schemas) and so on, we design prompts of different articulation and complexity to investigate how prompts help LLMs release the power of few-shot RE. First, text prompt only contains essential elements for RE, including relation categories, contexts, and corresponding head and tail entities. Inspired by the fantastic performance of InstructGPT Ouyang et al. (2022) and ChatGPT OpenAI (2022), we design the **task-related instruction** describing the relation extraction task and add it to the prompt, which is named **instruct prompt**. Meanwhile, according to previous few-shot RE works Zhou and Chen (2022), entity types (schemas) are helpful; therefore, we also explore the effectiveness of schemas in prompts."
    },
    {
      "title": "Data Generation With Llms",
      "text": "To complement the scarcity of labeled data, we introduce another strategy: data generation via LLMs. Specifically, we utilize specific prompts with descriptions of data forms to guide LLMs to generate more in-domain labeled data autonomously, which is subsequently employed to fine-tune a relatively small language model with existing few-shot labeled training data. We design the prompt to tell the essential components (\\(\\mathbf{x},\\mathbf{h},\\mathbf{t},\\mathbf{t}_{\\mathbf{h}},\\mathbf{t}_{\\mathbf{t}}\\) and \\(\\mathbf{y}\\)) of one RE training instance and show few-shot instances as demonstrations to teach LLMs to comprehend Figure 1: Strategies to unleash the power of LLMs for few-shot relation. HEAD TYPE and TAIL TYPE are schemas. HEAD ENTITY and TAIL ENTITY are entity mentions. RELation refers the verbalized relation label words. features of labeled RE data. Note that schemas, such as types of relations and entities, are significant structural information in RE data. Therefore, we propose **schema-constrained data generation** by adding entity types as schema guidance to the prompt (in Figure 1) to boost performance. Then, the prompt is utilized to guide LLMs to create augmented relation extraction data that are converted into the expected format for future usage."
    },
    {
      "title": "4 Experimental Setups",
      "text": ""
    },
    {
      "title": "Methods And Datasets",
      "text": "**GPT-3.5** is utilized via OpenAI API2 as the large language model in our experiments. We implement experiments on four relation extraction datasets, including TACRED Zhang et al. (2017), TACREV Alt et al. (2020), RE-TACRED Stoica et al. (2021) and SciERC Luan et al. (2018). Compared with the LLM, six baselines methods are conducted via relatively small models (details in Appendix A). Footnote 2: [https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)"
    },
    {
      "title": "Few-Shot Settings",
      "text": "\\(K\\) **instances per relation (\\(K\\)-shot) are sampled for training and validation.** For all baselines, we use randomly sampled 8-shot and 16-shot datasets for training and validation. As for in-context learning, because GPT-3.5 has the limitation of maximum request tokens (4097 tokens) and the series of TACRED datasets have more than 40 relations, **one-shot demonstrations** can only be used, and the one-shot performance is reported in Table 1. For the same reason, to generate more labeled data for each relation independently, only three demonstrations for the relation are added to the prompts. In-context learning is implemented on the four whole test sets. Different demonstrations are randomly sampled from the shuffled training set every time to avoid effects from permutations of demonstrations Lu et al. (2021). As for data generation, generated data from GPT-3.5 and original few-shot training data are combined to fine-tune two baselines, TYP Marker Zhou and Chen (2022) and KnowPrompt Chen et al. (2022). Using different shots of generated data will lead to different results. Therefore, we increasingly add generated \\(k\\)-shot (\\(k\\in\\{8,16,32,48\\}\\)) data to the original 8-shot and 16-shot training data respectively and report the best performance over \\(k\\) in Tabel 1. More details are shown in Appendix A.3."
    },
    {
      "title": "5 Results And Discussion",
      "text": ""
    },
    {
      "title": "Main Findings For Relation Extraction",
      "text": "**In-context learning on LLMs can achieve comparable performance for RE with tuning relatively small PLMs.** From Table 1, we notice that ICL with only one-shot demonstrations can obtain competitive performance with full parameter tuning-based prompt learning baselines. Using LLMs via ICL does not necessitate any parameter updates, which contains the potential value of mak \\begin{table} \\begin{tabular}{c l c c c c c c c} \\hline \\hline & & TACRED & TACREV & RE-TACRED & SciERC \\\\ \\cline{2-10} & Method & K=8 & K=16 & K=8 & K=16 & K=8 & K=16 & K=8 & K=16 \\\\ \\hline \\multirow{6}{*}{**Category**} & SpanBERT Joshi et al. (2020) & 8.4 & 17.5 & 5.2 & 5.7 & 14.2 & 29.3 & 29.0 & 38.7 \\\\ & LUKE Yamada et al. (2020) & 9.5 & 21.5 & 9.8 & 22.0 & 14.1 & 37.5 & 33.2 & 48.9 \\\\ & GDPNet Xue et al. (2021) & 11.8 & 22.5 & 8.3 & 20.8 & 18.8 & 48.0 & 33.5 & 42.3 \\\\ & TANL Paolini et al. (2021) & 18.1 & 27.6 & 18.6 & 28.8 & 26.7 & 50.4 & 32.4 & 38.7 \\\\ & TYP Marker Zhou and Chen (2022) & 26.5 & 29.9 & 26.7 & 29.5 & 44.8 & 54.1 & 50.4 & 59.0 \\\\ & KnowPrompt Chen et al. (2022) & 29.4 & 32.1 & 29.8 & 34.1 & 56.1 & 61.4 & 50.2 & 57.1 \\\\ \\hline \\multirow{6}{*}{**Category**} & In-context Learning\\(\\dagger\\) & 31.9 & 32.4 & 49.9 & 46.6 \\\\ & In-context Learning\\(\\dagger\\)(w/ Instruction) & 31.0 & 31.9 & 51.8 & 48.8 \\\\ \\cline{1-1} \\cline{2-10} & Data Generation (TYP Marker) & 35.8 & 36.6 & 36.7 & 36.5 & 58.4 & 60.6 & 63.2 & 64.3 \\\\ \\cline{1-1} & Data Generation (KnowPrompt) & 37.9 & 37.4 & 42.6 & 41.0 & 62.7 & 66.2 & 58.6 & 67.8 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Micro F1 (%) of few-shot performance. \\(\\dagger\\) refers to the performance with one-shot demonstrations. \\begin{table} \\begin{tabular}{c c c c} \\hline \\hline Prompts & TACRED & TACREV & RE-TACRED & SciERC \\\\ \\hline TEXT & 31.9 & 32.4 & 49.9 & 46.6 \\\\ TEXT + Schema & 36.9 & 37.7 & 54.3 & 45.9 \\\\ INSTRUCT & 31.0 & 31.9 & 51.8 & 48.8 \\\\ INSTRUCT + Schema & 38.3 & 36.7 & 58.5 & 50.2 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Micro F1 (%) of performance on different prompt: text prompt and instruct prompt. ing models scenario-adaptable, unlike supervised learning requiring parameter optimization. Data generation with LLMs can boost previous solutions to obtain new state-of-the-art few-shot RE results.We find that previous baselines can significantly improve with **10.7%** for 16-shot in SciERC and **6.6%** for 16-shot in RE-TACRED by simply using generated data from GPT-3.5 in Table 1. To be noted, data generation is a simple yet effective approach to elicit the power from the LLM to previous methods, and we demonstrate that using schema-constrained generation with LLMs can benefit all previous approaches with SLMs."
    },
    {
      "title": "Prompts In In-Context Learning With Llms",
      "text": "Instructions and schemas play an essential role in in-context learning for RE with LLMs.From Table 2, we notice that the model with instruct prompt obtains better performance than text prompt in most cases, indicating task-related information indeed helps to unlock more ability of LLMs for RE. Aberrant results are shown in TACRED and TACREV because incorrectly labeled demonstrations from the two datasets will violate the correct instruction fed into LLMs, which confuses LLMs and results on worse performance than ICL without the instruction. Moreover, adding schema information obtains much better performance, exhibiting the importance of pre-defined structural information for relation extraction. More demonstrations, counter-intuitively, may not lead to performance improvement for RE with LLMs.We find performance will not improve even drop and the gap between instruct prompt and text prompt becomes relatively smaller as the number of in-context demonstrations increases from Figure 2. We argue that there may be two reasons: 1) it is challenging to select representative demonstrations; 2) it is non-trivial for LLMs to understand structure prediction tasks with more large output (relation) space. More case studies for GPT-3.5 can be found in Appendix B.1."
    },
    {
      "title": "Utility Of Generated Data From Llms",
      "text": "Combining data generated from LLMs with original training data can yield better RE performance than from traditional data augmentation approaches.We compare data generation through the LLM with previous widely used data generation approaches, such as substituting words in training sets with WordNet's synonyms and contextual word embedding in Figure 3 (details in Appendix A.3). Data generation with LLMs can obtain better performance than all others, indicating guiding LLMs to generate data is an effective method to compensate for the lack of labeled data. Using more and more generated data from LLMs can only boost RE performance to a certain extent, not continuously better.From Figure 4, we observe that with more generated data, the result climbs up first and then declines, and is always higher than without generated data. We think low-quality generated data introduces much noise in the training course, according to the analysis on generated data in Appendix B.2, and LMs may have an anti-noise capacity Song et al. (2020)."
    },
    {
      "title": "6 Discussion And Conclusion",
      "text": "In this paper, we take the first step to investigate how to utilize the large language model for few-shot relation extraction. We observe that task-related information, including instructions or Figure 3: Performance of data generation with LLMs and different data augmentation methods. _Roberta_ and _SciBERT_ are used on RE-TACRED and SciERC, respectively, in the context embedding-based DA method. Figure 2: Micro F1 (%) of \\(k\\) in-context demonstrations in SciERC. schemas, helps to elicit the capability of LLMs and boost few-shot relation extraction performance. At this stage, using LLMs to generate data may be a simple yet effective solution to enhance the power of foundation models (relatively small PLMs) for practical applications. We hope this work can deliver the benefits of using LLMs for the NLP community. Note that LLMs can make predictions only based on contexts combined with a few training examples as demonstrations. We argue that it has the potential to design sophisticated human-readable prompts for scenario-adaptable (e.g., low-shot and any domains) relation extraction."
    },
    {
      "title": "Acknowledgment",
      "text": "We would like to express gratitude to the anonymous reviewers for their kind comments. This work was supported by the National Natural Science Foundation of China (No.62206246), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), and Yongjiang Talent Introduction Programme (2021A-156-G), CAAI-Huawei MindSporc Open Fund."
    },
    {
      "title": "Limitations",
      "text": "Despite our best efforts, there may still be some limitations remaining in this paper. LLMs:Due to the limited budgets, we can not afford all kinds of LLMs, so we only evaluate GPT-3.5 (_text-davinci-003_). We will try to investigate relation extraction with more LLMs, such as OPT (Zhang et al., 2022), GLM-130B (Zeng et al., 2022), or code language models (Bi et al., 2023) like Codex. Other Methods to utilize LLMs:There are several other techniques to leverage LLMs, such as black-box optimization (Sun et al., 2022) and feature-based learning (Lang et al., 2022); however, we find that most of those approaches cannot directly be applied to relation extraction due to the large label space and complex schema structures. We leave these for future work to leverage other methods with LLMs for relation extraction. Datasets:We only evaluate four relation extraction datasets and will try to investigate relation extraction performance with LLMs on more diverse datasets across different domains and languages."
    },
    {
      "title": "References",
      "text": "* Alt et al. (2020) Christoph Alt, Aleksandra Gabryszak, and Leonhard Hennig. 2020. Tacred revisited: A thorough evaluation of the tacred relation extraction task. In _Proceedings of ACL_. * Beltagy et al. (2019) Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3615-3620, Hong Kong, China. Association for Computational Linguistics. * Bi et al. (2023) Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen, and Ningyu Zhang. 2023. Codekgc: Code language model for generative knowledge graph construction. _CoRR_, abs/2304.09048. * Brody et al. (2021) Sam Brody, Sichao Wu, and Adrian Benton. 2021. Towards realistic few-shot relation extraction. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 5338-5345, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. * Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc. * Chen et al. (2022) Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022. Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. In _WWW '22: The ACM Web Figure 4: Micro F1 (%) of KnowPrompt with generated training data and original 8-shot data. Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022_, pages 2778-2788. ACM. * Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanunalamayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkovycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. _arXiv preprint_, abs/2204.02311. * Ding et al. (2022) Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing, Shafiq R. Joty, and Boyang Li. 2022. Is GPT-3 a good data annotator? _arXiv preprint_, abs/2212.10450. * Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey for in-context learning. _arXiv preprint_, abs/2301.00234. * Han et al. (2021a) Jiale Han, Bo Cheng, and Wei Lu. 2021a. Exploring task difficulty for few-shot relation extraction. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, pages 2605-2616. Association for Computational Linguistics. * Han et al. (2022) Jiale Han, Shuai Zhao, Bo Cheng, Shengkun Ma, and Wei Lu. 2022. Generative prompt tuning for relation classification. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 3170-3185, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. * System Demonstrations_, pages 169-174. Association for Computational Linguistics. * Han et al. (2021b) Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2021b. PTR: prompt tuning with rules for text classification. _arXiv preprint_, abs/2105.11259. * Han et al. (2018) Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 4803-4809, Brussels, Belgium. Association for Computational Linguistics. * Joshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predicting spans. _Trans. Assoc. Comput. Linguistics_, 8:64-77. * Lang et al. (2022) Hunter Lang, Monica N. Agrawal, Yoon Kim, and David A. Sontag. 2022. Co-training improves prompt-based learning for large language models. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 11985-12003. PMLR. * Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. _arXiv:1907.11692_, abs/1907.11692. * Lu et al. (2021) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. _arXiv preprint_, abs/2104.08786. * Lu et al. (2018) Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreferencefor scientific knowledge graph construction. In _Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)_. * Lyu and Chen (2021) Shengfei Lyu and Huanhuan Chen. 2021. Relation classification with entity type restriction. In _Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021_, volume ACL/IJCNLP 2021 of _Findings of ACL_, pages 390-395. Association for Computational Linguistics. * Ma et al. (2023) Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. 2023. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! _CoRR_, abs/2303.08559. * OpenAI (2022) OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/). * OpenAI (2023a) OpenAI. 2023a. Gpt-4 technical report. _arXiv preprint_, abs/2303.08774. * OpenAI (2023b) OpenAI. 2023b. Text-davinci-003. [https://platform.openai.com/docs/models/text-davinci-003](https://platform.openai.com/docs/models/text-davinci-003). Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. _arXiv preprint_, abs/2203.02155. * Paolini et al. (2021) Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net. * Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9. * Soares et al. (2019) Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for relation learning. In _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pages 2895-2905. Association for Computational Linguistics. * Song et al. (2020) Hwanjun Song, Minesok Kim, Dongmin Park, and Jaeg-Gil Lee. 2020. Learning from noisy labels with deep neural networks: A survey. _arXiv preprint_, abs/2007.08199. * Stoica et al. (2021) George Stoica, Emmanouil Antonios Platanios, and Barnabas Poczos. 2021. Re-tacred: Addressing shortcomings of the TACRED dataset. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pages 13843-13850. AAAI Press. * Sun et al. (2022) Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 20841-20855. PMLR. * Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. _arXiv preprint_, abs/2211.09085. * Wang et al. (2021) Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. 2021. Want to reduce labeling cost? GPT-3 can help. In _Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021_, pages 4195-4205. Association for Computational Linguistics. * Xue et al. (2021) Fuzhao Xue, Aixin Sun, Hao Zhang, and Eng Siong Chng. 2021. Gdpnet: Refining latent multi-view graph for relation extraction. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pages 14194-14202. AAAI Press. * Yamada et al. (2020) Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. 2020. LUKE: deep contextualized entity representations with entity-aware self-attention. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_, pages 6442-6454. Association for Computational Linguistics. * Yang et al. (2021) Shan Yang, Yongfei Zhang, Guanglin Niu, Qinghua Zhao, and Shiliang Pu. 2021. Entity concept-enhanced few-shot relation extraction. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual Event, August 1-6, 2021_, pages 987-991. Association for Computational Linguistics. * Ye et al. (2022) Hongbin Ye, Ningyu Zhang, Hui Chen, and Huajun Chen. 2022. Generative knowledge graph construction: A review. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 1-17, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. * Yoo et al. (2021) Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woo-Myoung Park. 2021. Gpt3mix: Leveraging large-scale language models for text augmentation. In _Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021_, pages 2225-2239. Association for Computational Linguistics. * Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. 2022. GLM-130B: an open bilingual pre-trained model. _CoRR_, abs/2210.02414. * Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihalyov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: open pre-trained transformer language models. _CoRR_, abs/2205.01068. Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. 2017. Position-aware attention and supervised data improve slot filling. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017)_, pages 35-45. * Volume 2: Short Papers, Online only, November 20-23, 2022_, pages 161-168. Association for Computational Linguistics."
    },
    {
      "title": "Appendix A Experimental Details",
      "text": ""
    },
    {
      "title": "Datasets",
      "text": "TACRED3 is a widely used RE dataset. It has 42 relation labels, including _no_relation_, meaning no relation is found. TACREV4 includes the same training set and relabeled development and test sets from TACRED. RE-TACRED5 is a re-annotated version of TACRED with 40 relations. SciERC6 has seven relation categories and is constructed in the scientific domain. All datasets are derived from their official webs without modification, including contents and train/test/dev splits. Footnote 3: [https://nlp.stanford.edu/projects/tacred/](https://nlp.stanford.edu/projects/tacred/) Footnote 4: [https://github.com/DFKI-NLP/tacrev](https://github.com/DFKI-NLP/tacrev) Footnote 5: [https://github.com/gstoica27/Re-TACRED](https://github.com/gstoica27/Re-TACRED) Footnote 6: [http://nlp.cs.washington.edu/sci1E/](http://nlp.cs.washington.edu/sci1E/)"
    },
    {
      "title": "Baselines",
      "text": "We compare LLMs with recent baseline methods using relatively small models. 1) Normal fine-tuning methods: **SpanBERT**(Joshi et al., 2020), a span-based PLM; **LUKE**(Yamada et al., 2020), pre-trained contextualized representations of words and entities based on the bidirectional transformer; **GDPNet**, a gaussian dynamic time warping pooling net able to select important words for relation prediction; **TYP Marker**(Zhou and Chen, 2022), fine-tuning with entity typed markers. 2) Generative method: **TANL**(Paolini et al., 2021), framing a structured prediction language task as a translation task between augmented natural languages. 3) Prompt-tuning methods: **KnowPrompt**, knowledge-aware continuous prompt-based tuning with synergistic optimization."
    },
    {
      "title": "Implementation Details",
      "text": "Generated data with existing training data is then evaluated on KnowPrompt. Data augmentation methods with Word-Net's synonyms and contextual word embedding are achieved by _nlpaug_7. The parameter _temperature_ in OpenAI API is set to 0 for precision in ICL and 1 for generating diverse RE data. One NVIDIA GeForce RTX 3090 GPU with 24GB memory is employed to run all experiments. We rerun the official code of baselines with their original settings except on the SciERC dataset. Due to the vertical domain of SciERC, _SciBERT_(Beltagy et al., 2019) is used in TYP Marker and KnowPrompt for fairness. And for another three datasets, _RoBERTa-large_ is utilized in _TYP Marker_ and _KnowPrompt_. Footnote 7: [https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug)"
    },
    {
      "title": "Appendix B Case Analysis",
      "text": ""
    },
    {
      "title": "Wrong Cases From Icl",
      "text": "From Table 4, we notice that some RE instances are challenging for LLMs, and there are several limitations with LLMs: 1) LLMs are not good at clearly distinguishing the order between head and tail entities. 2) The same mention of head and tail entities will confuse LLMs. 4) If the distance between head and tail entities in the context is long, it is difficult for LLMs to decide the relation correctly. 5) Semantically-similar relation label words and entity mentions will puzzle LLMs because their embeddings are similar. 6) LLMs cannot afford very long instances since there is a large label space for relation extraction. 7) LLMs may mostly fail to extract those ambitious or wrongly labeled relations; those are also challenging for humans. More high-quality demonstrations may help mitigate these issues. And we think it is necessary to develop step-by-step (Chat-style) approaches with LLMs to extract limited relations in one stage."
    },
    {
      "title": "Generated Data From Llms",
      "text": "There are some cases for generated data from GPT-3.5 in Table 5. Through human checks on 100 generated samples per dataset, about 78% generated data are corrected labeled and of a high quality (85% for TACRED, 82.5% for TACREV, 72% for RE-TACRED, 75% for SciERC). Meanwhile, we add generated data and original gold training data respectively to 8-shot datasets and fine-tune _KnowPrompt_, we evaluate the quality of generated data as shown in Table 3. We observe that labeled data generated by GPT-3.5 are mostly correct. As for TACRED and TACREV, generated data achieve more improvements than gold labeled data. Sincethere are many incorrect labeled data in TACRED and TACREV (Zhang et al., 2017; Alt et al., 2020), we think better performance results from GPT-3.5's help. However, we also find that Some generated data from GPT-3.5 are of less quality than gold data. As for RE-TACRED and SciERC, using more gold data perform better than generated data. Through human checks, some generated samples are too short and concatenated by some semantically irrelevant sentences. Meanwhile, big performance's difference on SciERC shows GPT-3.5 is not good at vertical domains such as science. \\begin{table} \\begin{tabular}{l c c c c c c c c} \\hline \\hline & \\multicolumn{2}{c}{**TACRED**} & \\multicolumn{2}{c}{**TACREV**} & \\multicolumn{2}{c}{**RE-TACRED**} & \\multicolumn{2}{c}{**SciERC**} \\\\ 8-shot Dataset & generated & gold & generated & gold & generated & gold & generated & gold \\\\ \\hline add 0-shot & 29.35 & 29.35 & 29.77 & 29.77 & 56.05 & 56.05 & 45.80 & 45.80 \\\\ add 8-shot & 31.63 & 30.73 & 34.30 & 33.16 & 59.85 & 60.92 & 48.30 & 57.08 \\\\ add 16-shot & 34.78 & 31.88 & 36.33 & 33.49 & 59.59 & 61.30 & **58.62** & 65.15 \\\\ add 32-shot & 36.45 & 33.35 & 38.19 & 33.98 & 60.06 & 64.65 & 57.70 & 72.11 \\\\ add 48-shot & **37.89** & 33.97 & 38.80 & 35.06 & **62.67** & 65.56 & 51.64 & 74.29 \\\\ add 64-shot & 36.67 & 34.36 & **42.61** & 35.57 & 61.07 & 67.28 & 54.52 & 75.36 \\\\ add 72-shot & 35.69 & **34.58** & 41.72 & **35.96** & 59.09 & **67.43** & 49.59 & **75.87** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Micro F1 (%) of _KnowPrompt_ after adding labeled data generated by GPT-3.5 or gold labeled data to 8-shot datasets. \\begin{table} \\begin{tabular}{c l l l} \\hline \\hline Dataset & Case & Gold Relation & In-context Learning \\\\ \\hline \\multirow{3}{*}{TACRED} & Context: And strangely enough, Cain’s short, three-year tenure at the NRA is evidently the only period in his decades-long career during which he ’s alleged to have been a sexual predator. & org:top\\_members/employees per:employee\\_of \\\\ & Head Type: ORGANIZATION. Head Entity: NRA. & \\\\ & Tail Type: PERSON. Tail Type: Cain & \\\\ \\hline \\multirow{3}{*}{TACRED} & Context: “I learn from students and I challenge them,” says Heloise, 58, who took over the family hints business when her mother, also named Heloise, died in 1977. & per:parents & per:alternate\\_names \\\\ & Head Type: PERSON. Head Entity: Heloise. & \\\\ & Tail Type: PERSON. Tail Entity: Heloise. & \\\\ \\hline \\multirow{3}{*}{TACREV} & Context: Anna Mate Pictou Aquash, a Mi ’ kmaq Indian from Canada, was brutally murdered in 1975. & per:country\\_of\\_birth & per:countries\\_of\\_residence \\\\ & Head Type: PERSON. Head Entity: Anna Mate Pictou Aquash. & & \\\\ & Tail Type: COUNTRY. Tail Entity: Canada. & & \\\\ \\hline \\multirow{3}{*}{TACREV} & Context: Messina Denaro has been trying to impose his power in Palermo, the Sicilian capital, and become the new head of the Sicilian Mafia, weakened by the arrest of Provenzano in April 2006. & no\\_relation & per:cities\\_of\\_residence \\\\ & Head Type: PERSON. Head Entity: his. & & \\\\ & Tail Type: CITY. Tail Entity: Palermo. & & \\\\ \\hline \\multirow{3}{*}{RE-TACRED} & Context: They say Vladimir Ladyzhenskiy died late Saturday during the contest in southern Finland, while his Finnish rival Timo Kaukonen was rushed to a hospital. & per:identity & per:date\\_of\\_death \\\\ & Head Type: PERSON. Head Entity: Vladimir Ladyzhenskiy. & & \\\\ & Tail Type: PERSON. Tail Entity: his. & & \\\\ \\hline \\multirow{3}{*}{RE-TACRED} & President of the Central American Parliament (Parlacen) & org:top\\_members/employees per:title & \\\\ & Jacinto Suarez said on Monday that the presidents of the Central American countries did not support Panama ’s request of withdrawal from the Parlacen. & & \\\\ & Head Type: ORGANIZATION. Head Entity: Central American Parliament. & & \\\\ & Tail Type: PERSON. Tail Entity: Jacinto Suarez. & & \\\\ \\hline \\multirow{3}{*}{SciERC} & Context: We evaluate across two corpora (conversational telephone speech and broadcast news speech) on both human transcriptions and speech recognition output. & CONJUNCTION & COMPARE \\\\ & Head Type: OtherScientificTerm. Head Entity: transcriptions. & & \\\\ & Tail Type: OtherScientific Term. Tail Entity: output. & & \\\\ \\hline \\multirow{3}{*}{Context: We validate this new method on nine standard person re-identification datasets including two large scale Market-1501 and CUHK03 datasets and show that we improve upon the current state-of-the-art methods on all of them. Head Type: Material. Head Entity: CUHK03 datasets. Tail Type: Material. Tail Entity: datasets.} & HYPONYM-OF & PART-OF \\\\ \\cline{1-1} \\cline{2-5} & & & \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Wrong cases predicted by GPT-3.5. The gold relation categories are listed in the third column and the results predicted by in-context learning are in the fourth column. \\begin{table} \\begin{tabular}{c l c} \\hline \\hline Dataset & Case & Corrective Data \\\\ \\hline \\multirow{3}{*}{TACRED} & Context: The American Cancer Society is headquartered in Atlanta and was & Tail Type: PERSON \\\\ & founded in 1913 by 15 trained laywomen. & \\\\ & Head Type: ORGANIZATION. Head Entity: American Cancer Society. & \\\\ & **Tail Type: ORGANIZATION**. Tail Entity: 15 trained laywomen. & \\\\ & Relation: org:founded\\_by. & \\\\ \\cline{2-3} & Context: Mary Brown, CEO of Brown Corp and renowned businesswoman, & Tail Type: TITTLE \\\\ & is a regular speaker at industry conferences and events. & \\\\ & Head Type: PERSON. Head Entity: Mary Brown. & \\\\ & **Tail Type: PERSON. Tail Entity: CEO.** & \\\\ & Relation: per:title. & \\\\ \\hline \\multirow{3}{*}{TACREV} & Context: Gustav Mahler was born in Kalischt, Bohemia on July 7th, 1860. & Tail Type: DATE \\\\ & Head Type: PERSON. Head Entity: Gustav Mahler. & Relation: per:date\\_of\\_birth \\\\ & **Tail Type: PERSON. Tail Entity: 1860.** & \\\\ & **Relation: per:country\\_of\\_birth.** & \\\\ \\cline{2-3} & Context: MTN Nigeria, a subsidiary of South African-based MTN Group, has & - \\\\ & begun to list its shares on the Nigerian Stock Exchange. & \\\\ & Head Type: ORGANIZATION. Head Entity: MTN Group. & \\\\ & Tail Type: ORGANIZATION. Tail Entity: MTN Nigeria. & \\\\ & Relation: org:subsidiaries. & \\\\ \\hline \\multirow{3}{*}{RE-TACRED} & Context: Pope John Paul II was a hugely popular Catholic leader who was & Tail Type: CITY \\\\ & based in the Vatican City for most of his papacy. & Reltaion: \\\\ & Head Type: PERSON. Head Entity: Pope John Paul II. & per:cities\\_of\\_residence \\\\ & **Tail Type: PERSON. Tail Entity: Vatican City.** & \\\\ & **Relation: per:countries\\_of\\_residence.** & \\\\ \\cline{2-3} & Context: French drug manufacturer Sanofi-Aventis dissolved its Chinese & - \\\\ & subsidiary Guangzhou Pharma following a bribery scandal. & \\\\ & Head Type: ORGANIZATION. Head Entity: Sanofi-Aventis. & \\\\ & Tail Type: ORGANIZATION. Tail Entity: Guangzhou Pharma. & \\\\ & Relation: org:dissolved. & \\\\ \\hline \\multirow{3}{*}{SciERC} & Context: The comparison between the two approaches indicates that the neural & - \\\\ & method produces far better results than the rule-based system. & \\\\ & Head Type: Method. Head Entity: neural method. & \\\\ & Tail Type: Method. Tail Entity: rule-based system. & \\\\ & Relation: COMPARE. & \\\\ \\cline{2-3} & Context: The combination of chromatography and mass spectrometry has & Relation: CONJUNCTION \\\\ & enabled scientists to achieve unparalleled levels of proteome analysis. & \\\\ & Head Type: Method. Head Entity: mass spectrometry. & \\\\ & Tail Type: Method. Tail Entity: chromatography. & \\\\ & **Relation: FEATURE-OF**. & \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: Generated data from LLMs. Errors are bold in the second column and corrected in the third column."
    }
  ]
}