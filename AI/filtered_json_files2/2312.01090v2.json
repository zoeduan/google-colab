{
  "title": "SELF GENERATED WARGAME AI: DOUBLE LAYER AGENT TASK PLANNING BASED ON LARGE LANGUAGE MODEL A PREPRINT",
  "authors": [
    "Yuxiang Sun",
    "Junjie Zhao",
    "Checheng Yu",
    "Wei Wang",
    "Xianzhong Zhou"
  ],
  "abstract": "\n The large language models represented by ChatGPT have a disruptive impact on the field of artificial intelligence. But it mainly focuses on natural language processing, speech recognition, machine learning and natural language understanding. This paper innovatively applies the large language model to the field of intelligent decision-making, places the large language model in the decisionmaking center, and constructs an agent architecture with the large language model as the core. Based on this, it further proposes a two-layer agent task planning, issues and executes decision commands through the interaction of natural language, and carries out simulation verification through the wargame simulation environment. Through the game confrontation simulation experiment, it is found that the intelligent decision-making ability of the large language model is significantly stronger than the commonly used reinforcement learning AI and rule AI, and the intelligence, understandability and generalization are all better. And through experiments, it was found that the intelligence of the large language model is closely related to prompt. This work also extends the large language model from previous human-computer interaction to the field of intelligent decision-making, which has important reference value and significance for the development of intelligent decision-making. \n",
  "references": [
    {
      "id": null,
      "title": "SELF GENERATED WARGAME AI: DOUBLE LAYER AGENT TASK PLANNING BASED ON LARGE LANGUAGE MODEL A PREPRINT",
      "authors": [
        "Yuxiang Sun",
        "Junjie Zhao",
        "Checheng Yu",
        "Wei Wang",
        "Xianzhong Zhou"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Chatgpt: five priorities for research",
      "authors": [
        "Eva Am Van Dis",
        "Johan Bollen",
        "Willem Zuidema",
        "Robert Van Rooij",
        "Claudi L Bockting"
      ],
      "year": "2023",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "What chatgpt and generative ai mean for science",
      "authors": [
        "Chris Stokel",
        "- Walker",
        "Richard Van Noorden"
      ],
      "year": "2023",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Use chat gpt to solve programming bugs",
      "authors": [
        "Shafiq Nigar",
        "Mohammed Y Surameery",
        "Shakor"
      ],
      "year": "2023",
      "venue": "International Journal of Information Technology & Computer Engineering (IJITC)",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Role of chat gpt in public health",
      "authors": [
        "Biswas Som",
        "S"
      ],
      "year": "2023",
      "venue": "Annals of biomedical engineering",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Potential use of chat gpt in global warming",
      "authors": [
        "S Som",
        "Biswas"
      ],
      "year": "2023",
      "venue": "Annals of biomedical engineering",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Knowledge traction and data-driven wargame ai design and key technologies",
      "authors": [
        "G Cheng",
        "X Chen",
        "M Yu",
        "Liu",
        "Shao"
      ],
      "year": "2021",
      "venue": "Syst. Eng. Electron. Technol",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Decision method for tactical level military chess entity operations",
      "authors": [
        "H Liu",
        "W Zhang",
        "K Hao",
        "Cheng",
        "Wang"
      ],
      "year": "2020",
      "venue": "Control and Decision",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Multi-agent decision-making method under the actor-critic framework and its application in wargames",
      "authors": [
        "Li Chen",
        "Huang Yanyan",
        "Zhang Yongliang",
        "Chen Tiande"
      ],
      "year": "2021",
      "venue": "Systems engineering and electronic technology",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Tactical maneuver strategy learning of land war based on convolutional neural network",
      "authors": [
        "H Xu",
        "D Zhang",
        "Zhao",
        "Ni"
      ],
      "year": "2022",
      "venue": "Journal of System Simulation",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Mastering complex control in moba games with deep reinforcement learning",
      "authors": [
        "Deheng Ye",
        "Zhao Liu",
        "Mingfei Sun",
        "Bei Shi",
        "Peilin Zhao",
        "Hao Wu",
        "Hongsheng Yu",
        "Shaojie Yang",
        "Xipeng Wu",
        "Qingwei Guo"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Towards playing full moba games with deep reinforcement learning",
      "authors": [
        "Guibin Chen",
        "Deheng Ye",
        "Wen Zhang",
        "Sheng Chen",
        "Bo Yuan",
        "Bo Liu",
        "Jia Chen",
        "Zhao Liu",
        "Fuhao Qiu",
        "Hongsheng Yu"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Human-level control through deep reinforcement learning",
      "authors": [
        "Volodymyr Mnih",
        "Koray Kavukcuoglu",
        "David Silver",
        "Andrei A Rusu",
        "Joel Veness",
        "Marc G Bellemare",
        "Alex Graves",
        "Martin Riedmiller",
        "Andreas K Fidjeland",
        "Georg Ostrovski"
      ],
      "year": "2015",
      "venue": "nature",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Mastering the game of go with deep neural networks and tree search",
      "authors": [
        "David Silver",
        "Aja Huang",
        "Chris J Maddison",
        "Arthur Guez",
        "Laurent Sifre",
        "George Van Den",
        "Julian Driessche",
        "Ioannis Schrittwieser",
        "Veda Antonoglou",
        "Marc Panneershelvam",
        "Lanctot"
      ],
      "year": "2016",
      "venue": "nature",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Grandmaster level in starcraft ii using multi-agent reinforcement learning",
      "authors": [
        "Oriol Vinyals",
        "Igor Babuschkin",
        "Wojciech M Czarnecki",
        "MichaÃ«l Mathieu",
        "Andrew Dudzik",
        "Junyoung Chung",
        "David H Choi",
        "Richard Powell",
        "Timo Ewalds",
        "Petko Georgiev"
      ],
      "year": "2019",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Overview of deep reinforcement learning algorithm and its application",
      "authors": [
        "C Y Liu",
        "C Y Mu",
        "Sun"
      ],
      "year": "2020",
      "venue": "Journal of Intelligent Science and Technology",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Review of intelligent games: Implications of game ai for combat inference",
      "authors": [
        "Sun Yuxiang",
        "Peng Yihui",
        "L I Bin",
        "Zhou Jiawei",
        "Zhang Xinlei",
        "Xianzhong Zhou"
      ],
      "year": "2022",
      "venue": "Chinese Journal of Intelligent Science and Technology",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Outracing champion gran turismo drivers with deep reinforcement learning",
      "authors": [
        "Samuel Peter R Wurman",
        "Kenta Barrett",
        "James Kawamoto",
        "Kaushik Macglashan",
        "Thomas J Subramanian",
        "Roberto Walsh",
        "Alisa Capobianco",
        "Franziska Devlic",
        "Florian Eckert",
        "Fuchs"
      ],
      "year": "2022",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Mastering atari, go, chess and shogi by planning with a learned model",
      "authors": [
        "Julian Schrittwieser",
        "Ioannis Antonoglou",
        "Thomas Hubert",
        "Karen Simonyan",
        "Laurent Sifre",
        "Simon Schmitt",
        "Arthur Guez",
        "Edward Lockhart",
        "Demis Hassabis",
        "Thore Graepel"
      ],
      "year": "2020",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "A general reinforcement learning algorithm that masters chess, shogi, and go through self-play",
      "authors": [
        "David Silver",
        "Thomas Hubert",
        "Julian Schrittwieser",
        "Ioannis Antonoglou",
        "Matthew Lai",
        "Arthur Guez",
        "Marc Lanctot",
        "Laurent Sifre",
        "Dharshan Kumaran",
        "Thore Graepel"
      ],
      "year": "2018",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Intelligent decision-making and human language communication based on deep reinforcement learning in a wargame environment",
      "authors": [
        "Yuxiang Sun",
        "Bo Yuan",
        "Qi Xiang",
        "Jiawei Zhou",
        "Jiahui Yu",
        "Di Dai",
        "Xianzhong Zhou"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Human-Machine Systems",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Research and implementation of intelligent decision based on a priori knowledge and dqn algorithms in wargame environment",
      "authors": [
        "Yuxiang Sun",
        "Bo Yuan",
        "Tao Zhang",
        "Bojian Tang",
        "Wanwen Zheng",
        "Xianzhong Zhou"
      ],
      "year": "2020",
      "venue": "Electronics",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Multi-attribute decision-making in wargames leveraging the entropy-weight method in conjunction with deep reinforcement learning",
      "authors": [
        "Yufan Xue",
        "Yuxiang Sun",
        "Jiawei Zhou",
        "Lisha Peng",
        "Xianzhong Zhou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Games",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Self Generated Wargame Ai: Double Layer Agent Task Planning Based On Large Language Model",
      "text": "Yuxiang Sun School of Management and Engineering Nanjing University Nanjing, 210023 sunyuxiang@nju.edu.cn Junjie Zhao School of Management and Engineering Nanjing University Nanjing, 210023 junjiezhao@smail.nju.edu.cn Checheng Yu School of Management and Engineering Nanjing University Nanjing, 210023 211870228@smail.nju.edu.cn &Wei Wang School of Intelligence Science and Technology Nanjing University Nanjing, 210023 2219002550@smail.nju.edu.cn &Xianzhong Zhou School of Management and Engineering Nanjing University Nanjing, 210023 zhouxz@nju.edu.cn This work was supported by the National Natural Science Foundation of China under Grant 61876079. (Corresponding author: Xianzhong Zhou)"
    },
    {
      "title": "1 Introduction",
      "text": "Since ChatGPT was officially launched on November 30, 2022, it has quickly become one of the most popular intelligent Chatbot Van Dis et al. (2023), Stokel-Walker and Van Noorden (2023). Since its inception, ChatGPT has been applied in multiple fields such as code correction Surameery and Shakor (2023), public health Som S (2023), and global warming Biswas (2023). In July 2023, OpenAI released the Code Interpreter plugin, further enhancing ChatGPT's data parsing capabilities and addressing the natural weaknesses of large language models in mathematics and language. These developments have provided new inspiration for improving the intelligence and generalization of AI in the field of intelligent wargame simulation, that is, using ChatGPT self generated AI to make intelligent decisions in war games. Although the development and application of rule AI and data-driven AI Cheng et al. (2021)is the starting point in the field of intelligent wargame, data-driven AI has gradually become a research hotspot in recent years, in which Reinforcement learning AI has made a series of breakthroughs. In terms of data-driven AI, Liu Man, Zhang Hongjun, and others have designed a wargame decision-making framework that balances rules and data Liu et al. (2020). In terms of Reinforcement learning AI, Li Chen's team from Nanjing University of Science and Technology designed a multi-agent decision-making method under the Actor Critical framework and achieved good intelligence Chen et al. (2021). Xu Jiale, Zhang Haidong, and others designed a CNN based strategy learning model to improve the accuracy of wargame situation prediction Xu et al. (2022). Tencent's AI Lab used Deep reinforcement learning to achieve game confrontation in the King's Glory game, and defeated professional players Ye et al. (2020), Chen et al. (2020). In a word, with the deepening of the combination of deep learning, Reinforcement learning and intelligent wargame, the intelligence of agents has been continuously improved Mnih et al. (2015), Silver et al. (2016), Vinyals et al. (2019), Liu et al. (2020). Although rule AI does not require a long period of training, due to its limitations in rules, the upper limit of intelligence level is difficult to break through the upper limit of rules; While data-driven AI and Reinforcement learning AI improve their intelligence and flexibility by processing large amounts of data through Reinforcement learning algorithms, their interpretability is poor, and it is difficult to achieve model migration under scenario and capture point changes SUN et al. (2022), Wurman et al. (2022), Schrittwieser et al. (2020), Silver et al. (2018). Therefore, improving the intelligence and generalization of AI in the field of intelligent wargame becomes the focus of further research. Moreover, the decision-making of adversarial games is complex and continuous. In order to make decisions more intelligent and generalized, this article focuses on introducing a self generated AI wargame architecture based on a large language model. Create a decision-making mechanism that involves multiple generative agents interacting, mimicking human behavior and generating interpretable, credible, and highly generalizable game adversarial intelligent decisions. The core work of this article is as follows: 1. The self generated AI wargame architecture is an intelligent agent architecture centered on a large language model. This architecture consists of multiple generative agents, each with its own large language model (this article uses ChatGPT as the driving tool). These intelligent agents can communicate and cooperate with each other through reflective and memory streams, and jointly make decisions. By talking to each other, they can share information, analyze the situation, and make inferences and decisions based on the conversation content. 2. Build a two-layer agent task planning model, targeting strategic agents and tactical agents to plan tasks for the game confrontation process. Strategic agents describe specific situations observed by all current agents. Planning refers to task allocation and execution based on all observed situational information. The tactical agent only focuses on the situation observed by a single agent chess piece and executes related tasks according to the strategic planning agent. However, tactical agents can also have their own judgments and feedback based on the prompts issued by strategic agents. 3. Taking wargame as the experimental platform, the experiment shows that the intelligent decision-making ability of the large language model is significantly stronger than reinforcement learning AI and rule AI, and the intelligence, understandability, and generalization are all better. And through experiments. Meanwhile, research has found that providing prior knowledge from experts in the field specifically to large language models can significantly improve their intelligence."
    },
    {
      "title": "2 Generative Wargame Ai Architecture",
      "text": "In the war chess environment, we have realized the confrontation between six red chemsmen and five blue chemsmen as Figure 1 shows. The red and blue chemsmen in different clusters have different semantic interaction information, which is generated through ChatGPT. To implement the decision-making mechanism mentioned above, we have developed an agent architecture consisting of three main components: a memory stream for storing and allocating buffers and generating batches; a reflection stream for using batches as prompts for the large language model to understand its role in the decision-making process; and a task planning stream for synthesizing higher-level reasoning from batches to enable the agent to integrate situational information and make better pre-battle plans. The agent architecture is designed to store, synthesize, and apply past battlefield experience to enable the large language model to generate trustworthy decisions. Figure 1: Specific interaction of self generated wargame AI in the environment Figure 2: The relationship between strategic and tactics agent"
    },
    {
      "title": "3 Generative Wargame Ai Model",
      "text": ""
    },
    {
      "title": "Wargame Agent Interaction",
      "text": "In the architecture described above, the wargame agents obtain situational information and interact with each other in natural language to maintain collaboration. Each agent describe their action in natural language, such as \"red agent 1 is preparing to capture the control point and moving along the road\", \"blue agent 2 is preparing to aim at the enemy target 1\". Then the sentence is translated into specific actions and directly affect the wargame environment. In the meanwhile, all the actions and movements will be displayed as a series of number symbols which appear above each avatar to provide abstract representation of actions. To achieve this, the architecture utilizes a language model to translate language into actions, while representing a concise symbol above each wargame to represent ChatGPT's suggestions for actions that this agent should take. For example, \"red agent 1 is preparing to capture the control point\" is displayed as \"!\" appearing above the wargame, while \"red agent 1 is preparing to aim at the enemy\" is displayed as \"\\(\\rightarrow\\)\". In this environment, agents communicate with each other in natural language which is fully understandable by humans. They obtain the situational information of other operators and environment from the semantics of sentences. Here is a sample of an agent communicating with another."
    },
    {
      "title": "Composing Model",
      "text": "The generative wargame AI aims to provide a novel decision-making framework for intelligent decision-making in wargame environment. Compared to the traditional rule-based AI, data-driven AI, or the reinforcement learning Figure 3: Double Layer Agent Task Planning Decision Framework Based on Large Language ModelAI, our architecture utilizes ChatGPT for intelligent decision-making and interaction with the wargame environment. Generative wargame AI takes the current environment and past experiences as inputs, and produces output in the form of generated actions. Generated actions can be described as these steps: the architecture provides the large language model with well-trained prompts, the language model chooses the numbers which encompass all the actions to take according to prompts, the agents take actions with the corresponding number. The innovation of the architecture lies in the combination of the large language model with retrieval of relevant information and the utilization of prompts to adjust the output of language model. Building upon the foundation of the architecture, we constructed a dual-layer agent system including the strategic agent and tactical agent. Strategic agent obtains all the information regarding the states of both its own sites and the observed opponents as input, then it combines this with the overall environment and input as prompts to generate a macro-level tactics intelligent task planning flow. And strategic agent assigns tasks to the tactics agent in the form of prompts, and tactics agent, based on its own states, provides modification suggestions and reasons for modifying. Then strategic agent keeps replanning according to these suggestions until all the tactics agents are not going to provide furthermore suggestions. Certainly, the strategic and tactics agents still face a lot of challenges even with the use of the state-of-the-art large language model like GPT-4 LLM(Large Language Model). Since extensive generation of events and memories generated by the two agents, the most critical challenge in this architecture lies in generating the most relevant memory fragments while retrieving and synthesizing relevant data from the memory stream. Therefore, this article attempts to reduce computational power and memory requirements, and uses GPT-4 LLM for strategic agent decision-making, facilitating overall strategic input and expert knowledge document input. For tactical agents, this article uses GPT-3.5 LLM for decision-making, as tactical agents can interact and provide feedback on results in turn, reducing computational power and memory requirements without affecting intelligence Memory StreamAs the central component of the architecture, the memory stream directly influences the efficiency and accuracy of decision-making. The entire memory stream is a list of memory objects, with each object consisting of a natural language description, a creation timestamp and a recent access timestamp. The fundamental element in memory list is observation, which contains all the situational information observed by agents. Due to the presence of the fog of the war, the battlefield environment doesn't allow for complete knowledge and awareness. The common information observed by an agent in a particular state is subject to certain limitations, which includes individual actions, actions taken by our own side's agent and the actions taken by opponent agents within our visible range. Examples: _Observation \\(1\\)_: agent observes its own side's agent approaching the control point and trying to control it. _Observation \\(2\\)_: agent observes its opposed agent approaching urban residential area and trying to shoot. We construct a retrieval function within the entire memory stream architecture and utilize it to extract observations from the historical experiences of the agents, providing a foundation for generating reasonable prompts and enable the language model to produce rational decision. The retrieval function can be selective, with the form of prioritizing the extraction of recently observations, important nodes set before and relevant memory to produce effective outcome. _Recency_ assigns a higher score to the observation added recently, in which case the agent prioritizes the memory information generated by the recent several steps. To account for the influence of time factors, we implement a time decay coefficient to calculate the score. _Importance_ categorizes the data within the memory stream into regular memories and core memories, and allocating higher scores to the core memories generated by agents. For example, a red agent moving towards the left and approaching the road can be categorized as a regular memory while a red agent approaching the control point and eliminating a blue agent can be classified as a core memory. In this architecture, we ask the language model to directly output the importance integer scores in a range from \\(1\\) to \\(10\\), in which case \\(1\\) means the purely common memory like moving on the road while \\(10\\) means the most important core memory like seizing the control point or shooting successfully. The specific implementation process can be described as follows: retrieving the corresponding memory from the memory stream to form a prompt, allowing the agent to generate importance scores accordingly and storing them back to the memory stream. Example: Memory: the red agent one is seizing the control point. _Importance_ score: \\(8\\)_Relevance_ assigns a higher score to those object that are relevant to the current situation because of the presence of relevance between different memory objects. For example, a red agent arrives at the road and approaching the control point at a higher speed, this memory has a strong correlation with the red agent's seizing the control point. In this paper we ask ChatGPT to generate the relevance score in a scale from 1 to 10 to describe the degree of relevance between memory objects. As mentioned above, the three components of our architecture have been quantified into scores for the agents. To calculate the final scores of agents we normalize _recency_, _importance_ and _relevance_ scores to a range of \\([0,1]\\) by scaling method. The final scores can be calculated using the following formula: \\(score_{final}=\\alpha_{recency}*score_{recency}+\\alpha_{importance}*score_{importance}+ \\alpha_{relevance}*score_{relevance}\\) We use this score to comprehensively determine the prompts that should be extracted, and instruct the agent to generate corresponding and reasonable action-plannings based on these prompts. Reflection StreamHowever, the observational performance of memory flow has limitations in the decision-making process in practical wargame environment. Reasoning based on raw observation is not efficient enough to allow a large language model to generate high-level decision results. It is necessary to infer and generate advanced reasoning semantics through the observation of information and the planning of actions. This paper defines this reasoning process as a higher-level memory flow, referred to as _reflection_. It is essentially a higher-level and more abstract thinking process. The reflection flow is generated together with the memory flow, but the generation of reflection is distinguished by the retrieval function in the preceding memory flow. When the score in the retrieval function exceeds a certain threshold, reflection is triggered. This reflective process involves a higher-level abstraction and understanding of previously observed information. It is essentially a combination of observed semantics and planned semantics generated through prompts, and it is generated periodically, providing the wargame agent with reasoning semantics. The first step of reflection is to raise questions and clarify the reflective process based on the previous experience flow of wargame agent. For example, the blue agent is approaching the road and accelerating towards the control point. The planning suggests that the red agent should reach hexagon \\(1403\\) and shoot the blue agent at that point. From this, reflective semantics are generated: The blue agent poses a significant threat and may create a disadvantageous situation for the red agent in this confrontation. The reflection process allows the agent to reflect not only on their current observation but on other reflection. As a result, memories generated by the agent can be divided into different levels under the reflection mechanism, allowing for more accurate decision-making at an abstract level. Figure 4: Experimental Simulation Environment Display Task Planning StreamStrategic agent, based on the current situation observed by all the agents of our side, describes it as a prompt following a specific format: \\(<\\textit{Summary}\\), \\(\\textit{Observations}\\), \\(\\textit{Planning}>\\). The Summary aims to convert the current situation from visual to semantic information Sun et al. (2022). _Observations_ describe specific circumstances observed by all agents, further enrich semantic information based on the summary. Planning involves task allocation and execution based on the observed situation. Example: _Summary_: our \\(10\\) agents are moving towards the control point and have identified \\(3\\) blue agents. _Observations_: blue agent \\(1\\) is nearing the control point. _Planning_: red agent \\(1-3\\) will prioritize engaging blue agent \\(1\\), while agents \\(4-10\\) will quickly move towards the control point."
    },
    {
      "title": "Chatgpt + Wargaming Business Process",
      "text": "The whole core process is to transform the situation image information in the wargame simulation into semantic information, which includes description information and situation information, and this information is sent to the wargame agent in the form of prompt, and then the agent feedback the corresponding planning semantics, which is goal oriented. The planning semantics are then transformed into action sequences (such as \\(1,2,3,4,..10\\), where numbers represent specific actions. Alternatively, they are transformed into corresponding actions such as attack, defense, evasion, acceleration, shooting, left movement, etc.), which affect the environment and generate new environments. These actions are then recycled back to the starting situation image and converted into semantics. On this basis, in order to reduce computational power and memory requirements, and improve operational efficiency, this article allows strategic agents to use GPT-4 LLM and tactical agents to use GPT-3.5 LLM. Compared to using GPT-4 or GPT-3.5 LLM entirely, this can comprehensively improve the intelligence of intelligent decision-making without requiring too much computing power and memory space. Firstly, input expert prior knowledge documents into the strategic agent for learning through GPT-4 LLM, and then provide appropriate prompt inputs to enable the strategic agent to make decisions through GPT-4 LLM and convert them into action outputs that affect the wargame environment. The strategic agent then sends corresponding instructions to each tactical agent for execution. The tactical agent provides feedback on whether the task is suitable for the current agent through GPT-3.5 LLM combined with appropriate prompts, and provides the recommended execution results to the strategic agent for adjustment. _Strategic agent_: Based on the task planning flow, the strategic agent synthesizes states 1 to 10 and provides a task planning sequence, which is the action that each wargame should take in the step allocation; _Tactical agent_: The tactical agent receives task planning and provides modification suggestions and reasons for the assigned tasks based on its own state; The strategic agent plans again based on the modification suggestions until all tactical agents no longer provide modification suggestions."
    },
    {
      "title": "4 Verification Of Simulation Experimental Environment",
      "text": ""
    },
    {
      "title": "Experimental Environment Display",
      "text": "This paper verifies the above established large language model through simulation experiments. The simulation platform is a wargame simulation platform, which can conduct game confrontation between red and blue sides. Both red and blue sides can use intelligent algorithms to make decisions and execute actions Sun et al. (2020). The basic adversarial rule is that the red and blue sides compete for the middle control point (red flag), and the party who first reaches the control point wins. Or if one party is completely destroyed by the other party, the other party wins."
    },
    {
      "title": "Advantages Of Large Language Model Over Reinforcement Learning Intelligent Decision-Making",
      "text": "In the previous experiments, we mainly made decisions through rule AI and reinforcement learning AI. For the first time, this work used the large language model to make decisions for agents, and it was verified on this platform. Interestingly, this work found that there is a large difference between large language models and Reinforcement learning. First, large language models or trained large language models can make decisions without waiting for the convergence of training, and can directly obtain high intelligence. Reinforcement learning algorithms often need a lot of training to gradually adapt to a new task. At the same time, compared with the reinforcement learning algorithm, the decision making using the large language model can directly achieve excellent intelligence in multiple different tasks, and does not need to re-train for different tasks, which is of high value for practical applications. [MISSING_PAGE_EMPTY:8] algorithm is closer to GWA. If expert experience documents are fed to GWA, the GWAE algorithm's victory rate will be significantly improved. Overall, the overall winning rate of the algorithm proposed in this paper is significantly better than the previous classical reinforcement learning algorithm. Also on the premise of fixed blue intelligence and fixed reasoning scenarios, the overall effect of intelligent decision-making using the large language model is also relatively small. Figure 6: Violin Plot for Mean Winrate of All Algorithms Over Episodes. The GWAE algorithm and GWA algorithm have a higher winning rate and are more stable. Figure 7: Scatter plot of the average winning rate of all Figure 8: Task Mean Scores Heatmap.The performance algorithms. The results of comparing the overall winning of GWAE, GWA, RNM-PPO, PPO, PK-DQN, and DQN rates of GWAE, GWA, RNM-PPO, PPO, PK-DQN, and algorithms in three typical tasks: kill, control, and survive. DQN algorithms The darker the color, the better the algorithm performs in this task. stable, the overall victory rate fluctuates relatively little. For different typical task scenarios, GWA and GWAE exhibits outstanding performance, especially in the task of scoring, the performance of GWA and GWAE is significantly better than that of classical reinforcement learning algorithms. These two algorithms have demonstrated the potential value of large language models in the field of intelligent decision-making by making appropriate decisions for different task planning scenarios without undergoing extensive training."
    },
    {
      "title": "5 Conclusion",
      "text": "This work innovatively applies the large language model to intelligent decision-making, and verifies the feasibility of the large language model for decision-making in the wargame platform. Compared with the intelligent decision-making of Reinforcement learning, this paper finds that the large language model has obvious advantages. Firstly, the large language model for decision-making has strong adaptability in practical game confrontations due to sufficient training in advance. There is no need to wait to restart training, and it has strong intelligence and generalization for different tasks. Secondly, the intelligence shown by the large language model is obviously stronger than the general Reinforcement learning algorithm, which proves the great potential of the large language model in decision-making. Finally, this article also found through experiments that there is a significant correlation between the intelligence of large language models and prompt. If there is a more suitable prompt, its displayed intelligence is significantly improved. Of course, the work of this article is still an initial exploration of the large language model, and there is still much work to be innovated in the future, such as the attempt of the large language model in different scenarios, and the use of the large language model in more complex game adversarial environments to further enhance the intelligence of the adversarial blue, in order to test the intelligence level of the large language model. This work also extends the large language model from previous human-computer interaction to the field of intelligent decision-making, which has important reference value and significance for the development of intelligent decision-making."
    },
    {
      "title": "References",
      "text": "* [1]E. A. Van Dis, J. Bollen, W. Zuidema, R. van Rooij, and C. L. Bockting (2023)Chatgpt: five priorities for research. Nature614 (7947), pp. 224-226. Cited by: SS1. * [2]C. Stokel-Walker and R. V. Noorden (2023)What chatgpt and generative ai mean for science. Nature614 (7947), pp. 214-216. Cited by: SS1. * [3]N. M. S. Surameery and M. Y. Shakor (2023)Use chat gpt to solve programming bugs. International Journal of Information Technology & Computer Engineering (IJITC) ISSN: 2455-5290, 3(01), pp. 17-22. Cited by: SS1. * [4]B. Som S. Role of chat gpt in public health. Annals of biomedical engineering51 (5), pp. 868-869. Cited by: SS1. * [5]S. S. Biswas (2023)Potential use of chat gpt in global warming. Annals of biomedical engineering51 (6), pp. 1126-1127. Cited by: SS1. * [6]K. Cheng, G. Chen, X. Yu, M. Liu, and T. Shao (2021)Knowledge traction and data-driven wargame ai design and key technologies. Syst. Eng. Electron. Technol43, pp. 2911-2917. Cited by: SS1. * [7]M. Liu, H. Zhang, W. Hao, K. Cheng, and J. Wang (2020)Decision method for tactical level military chess entity operations. Control and Decision35 (12), pp. 2977-2985. Cited by: SS1. * [8]L. Chen, H. Yanyan, Z. Yongliang, and C. Tiande (2021)Multi-agent decision-making method under the actor-critic framework and its application in wargames [j]. Systems engineering and electronic technology43 (03), pp. 755-762. Cited by: SS1. * [9]J. Xu, H. Zhang, D. Zhao, and W. Ni (2022) Tactical maneuver strategy learning of land war based on convolutional neural network. Journal of System Simulation34 (10), pp. 2181. Cited by: SS1. * [10]D. Ye, Z. Liu, M. Sun, B. Shi, P. Zhao, H. Wu, H. Yu, S. Yang, X. Wu, Q. Guo, et al. (2020) Mastering complex control in moba games with deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, pp. 6672-6679. Cited by: SS1. * [11]G. Chen, D. Ye, W. Zhang, S. Chen, B. Yuan, B. Liu, J. Chen, Z. Liu, F. Qiu, H. Yu, et al. (2020) Towards playing full moba games with deep reinforcement learning. Advances in Neural Information Processing Systems33, pp. 621-632. Cited by: SS1. * [12]V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. (2015)Human-level control through deep reinforcement learning. nature518 (7540), pp. 529-533. Cited by: SS1. * [13]D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. (2016) Mastering the game of go with deep neural networks and tree search. nature529 (7587), pp. 484-489. Cited by: SS1. * Vinyals et al. (2019) Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019. * Liu et al. (2020) CY Liu, CX Mu, and CY Sun. Overview of deep reinforcement learning algorithm and its application. _Journal of Intelligent Science and Technology_, 2(4):314-326, 2020. * SUN et al. (2022) Yuxiang SUN, Yihui PENG, Bin LI, Jiawei ZHOU, Xinlei ZHANG, and Xianzhong ZHOU. Review of intelligent games: Implications of game ai for combat inference. _Chinese Journal of Intelligent Science and Technology_, 4(2):157-173, 2022. * Wurman et al. (2022) Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al. Outracing champion gran turismo drivers with deep reinforcement learning. _Nature_, 602(7896):223-228, 2022. * Schrittwieser et al. (2020) Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020. * Silver et al. (2018) David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. _Science_, 362(6419):1140-1144, 2018. * Sun et al. (2022) Yuxiang Sun, Bo Yuan, Qi Xiang, Jiawei Zhou, Jiahui Yu, Di Dai, and Xianzhong Zhou. Intelligent decision-making and human language communication based on deep reinforcement learning in a wargame environment. _IEEE Transactions on Human-Machine Systems_, 53(1):201-214, 2022. * Sun et al. (2020) Yuxiang Sun, Bo Yuan, Tao Zhang, Bojian Tang, Wanwen Zheng, and Xianzhong Zhou. Research and implementation of intelligent decision based on a priori knowledge and dqn algorithms in wargame environment. _Electronics_, 9(10):1668, 2020. * Xue et al. (2023) Yufan Xue, Yuxiang Sun, Jiawei Zhou, Lisha Peng, and Xianzhong Zhou. Multi-attribute decision-making in wargames leveraging the entropy-weight method in conjunction with deep reinforcement learning. _IEEE Transactions on Games_, 2023."
    },
    {
      "title": "Appendix A Large Language Model Prompt",
      "text": "In this experiment, it was found that the intelligence of the large language model is closely related to prompt. Two different prompts were used for guidance in this experience, and the results returned after prompt input are shown in the figure. The figure 9 shows the response results of the strategic agent feedback. The figure 10 shows the response results of tactical agent feedback."
    },
    {
      "title": "Appendix B Prior Knowledge Document",
      "text": "This article feeds professional domain knowledge documents into the proposed GWA algorithm, forming the GWAE algorithm. Document link: [https://github.com/sunyuxiang926/wargame/blob/b0118cd0](https://github.com/sunyuxiang926/wargame/blob/b0118cd0) 1fa46f2790a89895336df81519d91707/files/theses/explanat ion_of_domain_expert_knowledge.pdf"
    },
    {
      "title": "Strategic Agent:",
      "text": "Red tank 1 Move to: 16, 14 Reason: Tank 1 is moved from the closest corner to the target, while avoiding the direct attack of blue tank 2. Red tank 2 Move to: 17, 17 Reason: Tank 2 is moved as close to the target as possible while avoiding the possible fire range of blue tanks 2 and 10. Red tank 3 Move to: 18, 19 Reason: Tank 3 is moved towards the target position, trying to stay out of the range of blue tanks 2 and 10. Red tank 4 Move to: 15, 17 Reason: Tank 4 is moved to a location that can fend off the firepower of blue tanks 2 and 10 and quickly reach the target when needed. Red tank 5 Move to: 16, 19 Reason: Tank 5 is trying to get close to the target, while maintaining a safe distance to avoid the firepower of blue tanks 2 and 10. Red tank 6 Move to: 17, 19 Reason: Tank 6 is heading towards the target, keeping a distance as much as possible to avoid the firepower of blue tanks 2 and 10. Red tank 7 Move to: 16, 17 Reason: Tank 7 is maintained in a good position towards the target, keeping itself out of the direct range of blue tanks 2 and 10. Red tank 8 Move to: 19, 18 Reason: Tank 8 is moved towards the target, trying to stay out of the range of blue tanks 2 and 10. Red tank 9 Move to: 20, 17 Reason: Tank 9 is moving forward to the target, avoiding the fire range of blue tanks 2 and 10. Red tank 10 Move to: 20, 16 Reason: Tank 10 is trying to get as close to the target as possible, keeping a safe distance to avoid the firepower of blue tanks 2 and 10. These moves aim to position each tank towards the target point and to keep each tank at as varied locations as possible to avoid the concentrated firepower from the blue team. This strategy aims to get the tanks to the target point as quickly as possible and to avoid enemy firepower where possible."
    },
    {
      "title": "Tactical Agent:",
      "text": "As Red tank 1, the command \"Red tank 1 Hide\" given by the command center seems somewhat unreasonable from my perspective. \"Hide\" implies that 1 should avoid blue tank 2's attacks, which may lead to distancing myself from the target control point. As an attacking force, my priority should be to approach the target and engage in combat at the right moment, rather than evading the battle. My ultimate goal is to capture control point [12, 24], and currently, blue tank 2 (located at [16, 40]) is closer to the control point. If 1 choose to hide, it will further increase the distance between me and the target, which is not conducive to reaching the objective quickly. In the last step, blue tank 5, 7, and 8 launched indirect attacks at [17, 7], and blue tank 10 also moved near [17, 41]. Given that blue tank 5, 7, and 8 launched indirect attacks, 1 have reason to believe that the area around [17, 7] might not be the position of blue tank 2. Therefore, avoiding direct confrontation with blue tank 2 is not the most urgent task at the moment. In conclusion, from my perspective, 1 believe that \"Red tank 1 Hide\" is not the most reasonable command. Instead, 1 should choose to advance towards the control point [12, 24], adopt a suitable strategy to avoid direct engagement with blue tank 2, and consider how to coordinate with the movements of other friendly tanks to achieve victory. Figure 10: Tactical Agent Prompt Feedback Display Figure 9: Strategic Agent Prompt Feedback Display"
    }
  ]
}