{
  "title": "RETHINKING MEMORY AND COMMUNICATION COSTS FOR EFFICIENT LARGE LANGUAGE MODEL TRAINING",
  "authors": [
    "Chan Wu",
    "Hanxiao Zhang",
    "Lin Ju",
    "Jinjing Huang",
    "Youshao Xiao",
    "Zhaoxin Huan",
    "Siyuan Li",
    "Fanzhuang Meng",
    "Lei Liang",
    "Xiaolu Zhang",
    "Jun Zhou"
  ],
  "abstract": "\n Recently, various distributed strategies for large language model training have been proposed. However, these methods provided limited solutions for the trade-off between memory consumption and communication cost. In this paper, we rethink the impact of memory consumption and communication costs on the training speed of large language models, and propose a memory-communication balanced strategy set Partial Redundancy Optimizer (PaRO). PaRO provides comprehensive options which reduces the amount and frequency of inter-group communication with minor memory redundancy by fine-grained sharding strategy, thereby improving the training efficiency in various training scenarios. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring) communication topology to enhance communication efficiency between nodes or across switches in large language model training. Our experiments demonstrate that PaRO significantly improves training throughput by 1.19×-2.50× compared to the SOTA method and achieves a near-linear scalability. The HO-Ring algorithm improves communication efficiency by 36.5% compared to the traditional Ring algorithm. \n",
  "references": [
    {
      "id": null,
      "title": "RETHINKING MEMORY AND COMMUNICATION COSTS FOR EFFICIENT LARGE LANGUAGE MODEL TRAINING",
      "authors": [
        "Chan Wu",
        "Hanxiao Zhang",
        "Lin Ju",
        "Jinjing Huang",
        "Youshao Xiao",
        "Zhaoxin Huan",
        "Siyuan Li",
        "Fanzhuang Meng",
        "Lei Liang",
        "Xiaolu Zhang",
        "Jun Zhou"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "A three-dimensional approach to parallel matrix multiplication",
      "authors": [
        "R C Agarwal",
        "S M Balle",
        "F G Gustavson",
        "M Joshi",
        "P Palkar"
      ],
      "year": "1995",
      "venue": "IBM Journal of Research and Development",
      "doi": "10.1147/rd.395.0575"
    },
    {
      "id": "b1",
      "title": "Bringing hpc techniques to deep learning",
      "authors": [
        "G Andrew"
      ],
      "year": "2017",
      "venue": "Bringing hpc techniques to deep learning",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Maximizing parallelism in distributed training for huge neural networks",
      "authors": [
        "Z Bian",
        "Q Xu",
        "B Wang",
        "Y You"
      ],
      "year": "2021",
      "venue": "Maximizing parallelism in distributed training for huge neural networks",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J D Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": "2019",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "authors": [
        "R Girshick",
        "J Donahue",
        "T Darrell",
        "J Malik"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Low-rank adaptation of large language models. International Conference on Learning Representations",
      "authors": [
        "E J Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen",
        "Lora"
      ],
      "year": "2022",
      "venue": "Low-rank adaptation of large language models. International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes",
      "authors": [
        "X Jia",
        "S Song",
        "W He",
        "Y Wang",
        "H Rong",
        "F Zhou",
        "L Xie",
        "Z Guo",
        "Y Yang",
        "L Yu"
      ],
      "year": "2018",
      "venue": "Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "A method for stochastic optimization",
      "authors": [
        "D P Kingma",
        "J Ba",
        "Adam"
      ],
      "year": "2017",
      "venue": "A method for stochastic optimization",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems",
      "authors": [
        "V A Korthikanti",
        "J Casper",
        "S Lym",
        "L Mcafee",
        "M Andersch",
        "M Shoeybi",
        "B Catanzaro"
      ],
      "year": "2023",
      "venue": "Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "1bit lamb: Communication efficient large-scale large-batch training with lamb's convergence speed",
      "authors": [
        "C Li",
        "A A Awan",
        "H Tang",
        "S Rajbhandari",
        "Y He"
      ],
      "year": "2021",
      "venue": "1bit lamb: Communication efficient large-scale large-batch training with lamb's convergence speed",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Colossal-ai: A unified deep learning system for large-scale parallel training",
      "authors": [
        "S Li",
        "H Liu",
        "Z Bian",
        "J Fang",
        "H Huang",
        "Y Liu",
        "B Wang",
        "Y You"
      ],
      "year": "2023",
      "venue": "Proceedings of the 52nd International Conference on Parallel Processing",
      "doi": "10.1145/3605573.3605613"
    },
    {
      "id": "b12",
      "title": "Scaling down to scale up: A guide to parameter-efficient fine-tuning",
      "authors": [
        "V Lialin",
        "V Deshpande",
        "A Rumshisky"
      ],
      "year": "2023",
      "venue": "Scaling down to scale up: A guide to parameter-efficient fine-tuning",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
      "authors": [
        "X Liu",
        "K Ji",
        "Y Fu",
        "W Tam",
        "Z Du",
        "Z Yang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-short.8"
    },
    {
      "id": "b14",
      "title": "Massively distributed sgd: Imagenet/resnet-50 training in a flash",
      "authors": [
        "H Mikami",
        "H Suganuma",
        "P U-Chupala",
        "Y Tanaka",
        "Y Kageyama"
      ],
      "year": "2018",
      "venue": "Massively distributed sgd: Imagenet/resnet-50 training in a flash",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Improving all-reduce collective operations for imbalanced process arrival patterns",
      "authors": [
        "J Proficz"
      ],
      "year": "2018",
      "venue": "The Journal of Supercomputing",
      "doi": "10.1007/s11227-018-2356-z"
    },
    {
      "id": "b17",
      "title": "Memory optimizations toward training trillion parameter models",
      "authors": [
        "S Rajbhandari",
        "J Rasley",
        "O Ruwase",
        "Y He",
        "Zero"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '20",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Two-tree algorithms for full bandwidth broadcast, reduction and scan",
      "authors": [
        "P Sanders",
        "J Speck",
        "J L Träff"
      ],
      "year": "2009",
      "venue": "Parallel Computing",
      "doi": "10.1016/j.parco.2009.09.001"
    },
    {
      "id": "b19",
      "title": "Horovod: fast and easy distributed deep learning in tensorflow",
      "authors": [
        "A Sergeev",
        "M D Balso"
      ],
      "year": "2018",
      "venue": "Horovod: fast and easy distributed deep learning in tensorflow",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Megatron-lm: Training multibillion parameter language models using model parallelism",
      "authors": [
        "M Shoeybi",
        "M Patwary",
        "R Puri",
        "P Legresley",
        "J Casper",
        "B Catanzaro"
      ],
      "year": "2020",
      "venue": "Megatron-lm: Training multibillion parameter language models using model parallelism",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Communication-optimal parallel 2.5d matrix multiplication and lu factorization algorithms",
      "authors": [
        "E Solomonik",
        "J Demmel"
      ],
      "year": "2011",
      "venue": "Euro-Par 2011 Parallel Processing",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "5-dimensional distributed model training",
      "authors": [
        "B Wang",
        "Q Xu",
        "Z Bian",
        "Y You"
      ],
      "year": "2021",
      "venue": "5-dimensional distributed model training",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Blink: Fast and generic collectives for distributed ml",
      "authors": [
        "G Wang",
        "S Venkataraman",
        "A Phanishayee",
        "N Devanur",
        "J Thelin",
        "I Stoica"
      ],
      "year": "2020",
      "venue": "Proceedings of Machine Learning and Systems",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Extremely efficient collective communication for giant model training",
      "authors": [
        "G Wang",
        "H Qin",
        "S A Jacobs",
        "C Holmes",
        "S Rajbhandari",
        "O Ruwase",
        "F Yan",
        "L Yang",
        "Y He",
        "Zero++"
      ],
      "year": "2023",
      "venue": "Extremely efficient collective communication for giant model training",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Distributed meta learning in gpu clusters for large-scale recommender systems",
      "authors": [
        "Y Xiao",
        "S Zhao",
        "Z Zhou",
        "Z Huan",
        "L Ju",
        "X Zhang",
        "L Wang",
        "J Zhou",
        "G-Meta"
      ],
      "year": "2023",
      "venue": "Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM '23",
      "doi": "10.1145/3583780.3615208"
    },
    {
      "id": "b27",
      "title": "An efficient 2d method for training superlarge deep learning models",
      "authors": [
        "Q Xu",
        "Y You"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)",
      "doi": "10.1109/IPDPS54959.2023.00031"
    },
    {
      "id": "b28",
      "title": "Large batch optimization for deep learning: Training bert in 76 minutes",
      "authors": [
        "Y You",
        "J Li",
        "S Reddi",
        "J Hseu",
        "S Kumar",
        "S Bhojanapalli",
        "X Song",
        "J Demmel",
        "K Keutzer",
        "C.-J Hsieh"
      ],
      "year": "2020",
      "venue": "Large batch optimization for deep learning: Training bert in 76 minutes",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Mics: Near-linear scaling for training gigantic model on public cloud",
      "authors": [
        "Z Zhang",
        "S Zheng",
        "Y Wang",
        "J Chiu",
        "G Karypis",
        "T Chilimbi",
        "M Li",
        "Jin",
        "X"
      ],
      "year": "2022",
      "venue": "Proc. VLDB Endow",
      "doi": "10.14778/3561261.3561265"
    },
    {
      "id": "b30",
      "title": "Pytorch fsdp: Experiences on scaling fully sharded data parallel",
      "authors": [
        "Y Zhao",
        "A Gu",
        "R Varma",
        "L Luo",
        "C.-C Huang",
        "M Xu",
        "L Wright",
        "H Shojanazeri",
        "M Ott",
        "S Shleifer",
        "A Desmaison",
        "C Balioglu",
        "P Damania",
        "B Nguyen",
        "G Chauhan",
        "Y Hao",
        "A Mathews",
        "S Li"
      ],
      "year": "2023",
      "venue": "Proc. VLDB Endow",
      "doi": "10.14778/3611540.3611569"
    }
  ],
  "sections": [
    {
      "title": "Rethinking Memory And Communication Costs For Efficient Large Language Model Training",
      "text": "Chan Wu Hanxiao Zhang Lin Ju Jinjing Huang Youshao Xiao Zhaoxin Huan Siyuan Li Fanzhuang Meng Lei Liang Xiaolu Zhang Jun Zhou"
    },
    {
      "title": "Abstract",
      "text": "Recently, various distributed strategies for large language model training have been proposed. However, these methods provided limited solutions for the trade-off between memory consumption and communication cost. In this paper, we rethink the impact of memory consumption and communication costs on the training speed of large language models, and propose a memory-communication balanced strategy set Partial Redundancy Optimizer (PaRO). PaRO provides comprehensive options which reduces the amount and frequency of inter-group communication with minor memory redundancy by fine-grained sharding strategy, thereby improving the training efficiency in various training scenarios. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring) communication topology to enhance communication efficiency between nodes or across switches in large language model training. Our experiments demonstrate that PaRO significantly improves training throughput by 1.19\\(\\times\\)-2.50\\(\\times\\) compared to the SOTA method and achieves a near-linear scalability. The HO-Ring algorithm improves communication efficiency by 36.5% compared to the traditional Ring algorithm."
    },
    {
      "title": "1 Introduction",
      "text": "With the development of machine learning technology, the overall performance of deep learning algorithms in fields such as face recognition, recommender system, and natural language processing has significantly improved Girshick et al. (2014); Xiao et al. (2023); Brown et al. (2020). Recent research shows that large model training is beneficial to improve model quality. Over the past few years, model size has increased from 110 million parameters for BERT Devlin et al. (2019) to 175 billion parameters for GPT-3 Brown et al. (2020). However, training such large language model (LLM) is not an easy task, as it requires a significant amount of computing resources and presents challenges in terms of system complexity. As the size of the model and the amount of training data increase, the computing power of a single GPU cannot meet the training needs of large-scale networks. In LLM training, to effectively utilize the computing power and memory of hundreds of GPU devices, a variety of distributed parallel training technologies have been proposed, such as data parallelism (DP), tensor parallelism (TP) and pipeline parallelism (PP) Li et al. (2023). In DP, an entire dataset is evenly partitioned into mutually exclusive subsets before training, and each worker works on a separate subset of them. TP divides the calculation and memory load of a single layer onto multiple GPUs by modifying the calculation method within the layer. PP puts different layers on different GPUs, and then divides the computing and memory loads onto multiple GPUs. However, TP and PP require modification of the model implement, which is inefficient for developers. In contrast, data parallelism has become the most mainstream distributed parallel method due to its simplicity. In data parallelism, the replicated model on each GPU processes a portion of the input batch, resulting in a large amount of communication data when fusing gradients. Andrew Andrew (2017) applied a ring topology on all-reduce to balance the communication load. By defining the communication topology, the communication pressure is evenly distributed to each GPU. However, since a complete model is copied on each GPU, significant memory redundancy occurs, especially when training large models Proficz (2018). To this end, Rajbhandari et al. Rajbhandari et al. (2020) proposed the Zero Redundancy Optimization (ZeRO) strategy set, which splits the model state (i.e. optimizer state, gradient and parameters) based on data parallelism and reconstructs them through the collective communication. It reduces memory consumption in LLM training and improves training efficiency by applying larger batch sizes. Since ZeRO retains the simplicity, ease of use, and versatility of DP, it has been widely used in LLM training. ZeRO needs to be adapted to specific training frameworks and hardware equipment to fully exploit its advantages. In high-performance clusters such as NVIDIA DGX-2 or DGX-A100 Wang et al. (2020), NVLink/NVSwitch with a bandwidth of up to 4.8TGbps is configured within the node, while the bandwidth of InfiniBand or Ethernet betweennodes is only 200\\(\\sim\\)800Gbps. The mismatch of bandwidth within and between nodes limits the training efficiency of ZeRO. To speed up model training, ZeRO requires more GPU resources, which will result in greater collective communication volume. To reduce collective communication volume, MiCS (Zhang et al., 2022) proposes a cluster grouping strategy in which all model states are partitioned within each group and replicated across different groups. However, this partitioning strategy incurs significant memory costs, particularly in scenarios with a large number of groups. In this paper, we systematically combine cluster grouping with different partitioning of different model states to trade off the memory and communication costs. Based on the memory consumption and synchronization frequency of the optimizer state, gradients and parameters, we design several optimization solutions to reduce overall communication costs and frequency with minimal memory redundancy. Additionally, we optimize the communication topology of ring all-gather and reduce-scatter operations by performing intra- and inter-node communication simultaneously. This strategy reduces inter-node communication volume and improves inter-node bandwidth utilization. We plan to release the code, pending approval from the company. The main contributions of the paper are summarized as follows: * We systematically analyzed the impact of memory consumption and communication costs on the training speed of LLMs, and proposed an overall guideline for balancing memory and communication. * We proposed the Partial Redundancy Optimizer (PaRO) strategy set, which provides more refined options for the trade-off between memory consumption and communication costs in different training scenarios. PaRO significantly increased training throughput by 1.19\\(\\times\\)-2.50\\(\\times\\) comparing with ZeRO, and can also improve the efficiency of complex ML systems. * We proposed a Hierarchical Overlapping Ring (HO-Ring) communication topology for inter-node or cross-switch collective communication operations for LLM training or other scenarios. Compared with the traditional Ring, the communication efficiency of HO-Ring was increased by 36.5%."
    },
    {
      "title": "2 Background And Related Works",
      "text": ""
    },
    {
      "title": "Data And Tensor Parallelism",
      "text": "According to different parallel objects, distributed parallel training technology can be divided into data parallelism and tensor parallelism (Korthikanti et al., 2023). Data parallelism divides the input data equally into several shards and assigns them to different GPUs. Each GPU owns the complete replica of model parameters. After forward and backward computation, each GPU obtains the corresponding parameter gradients. These gradients are then aggregated and transmitted back to each GPU through the all-reduce operation. Finally, the model parameters are updated based on the gradient and optimizer state (Sergeev and Balso, 2018). Data parallelism simplifies model training and deployment, but requires each GPU to maintain a complete replica of the model state. It may not meet the memory requirements of LLMs, especially when using the Adam optimizer (Kingma and Ba, 2017). Additionally, the communication cost during gradient transmission increases almost linearly with the number of GPUs, making the network bandwidth a bottleneck for training efficiency. Tensor parallelism shards tensors onto multiple GPU devices by modifying the model structure, and implements model parallelism through distributed matrix multiplication. Based on the characteristics of the Transformer architecture, Megatron-LM (Shoeybi et al., 2020) divides the layers in the row or column dimension to achieve 1D tensor parallelism. Since the output of each layer in 1D tensor parallelism is incomplete, an all-gather operation is required to aggregate the complete input before passing it to the next layer. In this process, the collective communication of 1D tensor parallelism generates a large amount of communication cost. Low bandwidth between nodes will affect the efficiency of 1D tensor parallel training. Additionally, 1D tensor parallelism incurs redundant memory consumption due to repeated inputs to each layer and repeated outputs after all-reduce. To address these issues, more advanced tensor parallelism methods, such as 2D (Xu and You, 2023), 2.5D (Wang et al., 2021), and 3D (Bian et al., 2021) tensor parallelism, have been introduced in LLM training. These methods shard the initial inputs using distributed matrix multiplication (Solomonik and Demmel, 2011; Agarwal et al., 1995), which eliminates communication in the middle layer and only requires one all-gather communication in the last layer."
    },
    {
      "title": "Model Finetuning",
      "text": "Powerful BERT(Devlin et al., 2019) and GPT3(Brown et al., 2020) models are both pre-trained on a large amount of general domain data. A widely-used approach, fine-tuning, freezes part of the pre-trained parameters and finetunes the remaining layers on task-specific data provides a significant performance and efficiency gain in different domains (Girshick et al., 2014; Brown et al., 2020). Different finetuning approaches vary on the ratio of trainable parameters of existing pre-trained models, including full parameter finetuning, and partial parameter finetuning(Lialin et al., 2023). The full parameter finetuning is as expensive as the pre-training since all model states must be stored. In the partial parameter finetuing, only the parameters are required to fully stored for computation, while the gradients and optimizer states are limited to trainable parameters. However,the enormity of pre-trained model, such as GPT-3, makes it challenging to perform traditional partial fine-tuning, so Parameter-efficient fine-tuning (PEFT), such as LoRAHu et al. (2022), P-tuningLiu et al. (2022), is introduced to resolve this problem by only training a very small set of parameters, which might be a subset of the existing model parameters or a set of newly added parameters."
    },
    {
      "title": "Zero Optimizer",
      "text": "The training process of deep learning models mainly consists of three stages: forward computation, backward computation, and model update. During the training process, GPUs need to store both model state and residual memory. ZeRO Rajbhandari et al. (2020) primarily reduces the memory consumption of model states, which mainly include model parameters, gradients from backward computation, and optimizer states for parameter updates. ZeRO gradually optimizes redundant memory in three stages: ZeRO-1, ZeRO-2 and ZeRO-3. ZeRO-1 globally shards the optimizer state across all GPU devices. During the training process, each GPU performs forward and backward computation independently to obtain the gradient, which are then synchronized among all GPUs using the all-reduce operation. Since each GPU retains a shard of the optimizer state, only the corresponding model parameters can be updated. After that, the updated model parameter shards are retrieved from other GPUs using the all-gather operation to ensure that all GPUs have the latest model parameters. Compared to ZeRO-1, ZeRO-2 further shards the optimizer state. During the training process, each GPU stores a complete set of model parameters and independently performs forward and backward computation to obtain a gradient. Afterwards, each GPU updates the gradient shards through the reduce-scatter operation and discards the other gradient shards. The subsequent processes remain the same as in ZeRO-1. In ZeRO-3, model parameters, gradients, and optimizer state are all sharded. Before performing forward and backward computation, each GPU performs an all-gather operation to collect model parameter shards from other GPUs and construct the complete model parameters. After gradient calculation, each GPU immediately discards the unmaintained model parameter shards. Then, each GPU updates the corresponding shard of model parameters using the maintained shard of optimizer parameters and gradients. Since each GPU only maintains one model parameter shard, there is no need to perform all-reduce operations."
    },
    {
      "title": "Communication Cost",
      "text": "For models with billions to trillions of parameters, ZeRO-3 transfers a significant amount of data ranging from tens to hundreds of gigabytes during forward computation, backward computation, and model updates. As the cluster size grows, each GPU needs to communicate multiple times, which amplifies the latency of collective communication operations. Therefore, an efficient communication topology is crucial to reduce communication costs. For communication primitives, the traditional ring all-reduce fails to consider the differences in intra- and inter-node network bandwidth, thereby being unable to fully utilize the bandwidth of clusters. The hierarchical ring (H-Ring) all-reduce Jia et al. (2018) groups GPUs based on their respective nodes and improves the efficiency through the communication topology of intra-group reduce, inter-group all-reduce, and intra-group broadcast. However, in inter-group all-reduce, only one GPU of each node participates in communication, resulting in low inter-group bandwidth utilization. To address this issue, Mikami et al. Mikami et al. (2018) proposed the 2D-Torus all-reduce scheme, where the communication topology is modified into intra-group reduce-scatter, inter-group all-reduce and intra-group all-gather. While the total communication volume of 2D-Torus all-reduce is the same as H-Ring all-reduce, 2D-Torus is more efficient due to the simultaneous communication of all GPUs in inter-group all-reduce. The community further optimizes the communication cost based on the inherent characteristic of LLM. To reduce the inter-node communication costs, MiCS Zhang et al. (2022) introduces the group sharding strategy by dividing the GPU cluster into subgroups, where the model state is partitioned within the subgroups and replicated across the subgroups. By configuring suitable subgroup sizes, MiCS can leverage the high intra-node bandwidth and a hierarchical communication strategy to reduce the communication volume between nodes. Similarly, the ZeRO++ Wang et al. (2023) system performs a secondary sharding of parameters while keeping other model states sharded across all GPUs to reduce inter-node communication volume. In addition, ZeRO++ compresses model parameters and gradients through quantization to reduce inter-node communication volume and latency. Additionally, the PyTorch's official Fully Sharded Data Parallel Zhao et al. (2023) provides a hybrid sharding (FSDP-hs) strategy, which leverages data center locality to accelerate training and reduce inter-node communication."
    },
    {
      "title": "3 Paro Design",
      "text": ""
    },
    {
      "title": "Analysis And Insights In Llm Training",
      "text": "This subsection examines the memory and communication costs of LLM training using the group sharding strategy. We consider training tasks with different model sizes and three levels of trainable model parameters: full, partial, and PEFT. We refer finetuning as partial training tasks for simplicity and introduce the following notations to aid in the explanation: * \\(N\\): Number of GPUs in the cluster. \\(M\\): Number of GPUs in the group or node. \\(g\\): Number of groups or nodes, \\(g=N/M\\). \\(s\\): Step of gradient accumulations. \\(K\\): Optimizer parameters. \\(\\Psi\\): Number of model parameters. \\(\\Psi^{\\prime}\\): Number of trainable parameters. \\(P\\): Parameter. \\(G\\): Gradient. \\(OS\\): Optimizer state."
    },
    {
      "title": "3.1.1 Analysis Of Communication Cost",
      "text": "As mentioned in the subsection 2.4, there exists a substantial performance gap in the bandwidth and latency between intra- and inter-node networks, which bottlenecks the training efficiency. Grouping GPU with a little memory redundancy can reduce communication participants and communication costs. Additionally, the subgroup could be grouped within the intra- and inter-node networks to fully leverage the high-throughput intra-node network. It could significantly improve communication efficiency. Therefore, we define three sharding states: _no sharding_, _intra-group sharding_ and _global sharding_, based on the sharding scope for three components of model states. The order of sharding granularity, from coarse-grained to fine-grained, is as follows: _no sharding \\(>\\) intra-group sharding \\(>\\) global sharding_. More specifically, _intra-group sharding_ means that model states are sharded within the group, while each group holds the complete replica. _No sharding_ means that each GPU holds a replica of model states, while each GPU holds a part of model states in _global sharding_. In the context of gradient accumulation where one mini-batch step contains several micro-batch steps, we analyze the communication cost of model states with different sharding states. * **Parameter sharding:** Parameters are utilized in both forward and backward computations during each iteration of micro-batch. In the both _global sharding_ and _intra-group sharding_ states, an all-gather operation is necessary to obtain all parameters of the current layer before usage. While only intra-group all-gather is required when sharding model parameters within a group. It reduces the frequency of high time-cost inter-group communication with little redundant memory across the inter-group. In _no sharding_ state, no communication operation is required since each device holds replicated parameters. * **Gradient sharding:** Gradients are computed during the backward computation and used in the model update stage. Likewise, in both _global sharding_ and _intra-group sharding_ states, the aggregated gradient of the corresponding local shard is obtained through collective communication. When sharding gradients within a group, only intra-group reduce-scatter is required in mini-batch step. After a number of gradient accumulations in a mini-batch step, an intra-group or global reduce-scatter operation is performed depending on the sharding scope of the optimizer state. In _no sharding_ state, no communication operation is required before parameter update stage. * **Optimizer state sharding:** The optimizer state is utilized during the model updating stage. However, the communication operations before or after the model updating stage become more complicated since they depend on the consistency of sharding scope between gradients and model parameters. If the sharding scope of optimizer states differs from that of gradients or parameters, the communication operations will vary before and after the model updating stage. For instance, it requires to perform an inter-group reduce-scatter before model updating and an inter-group all-gather after model updating, when the OS is global sharding and others are intra-group sharding. Therefore, _the order of communication costs_ is as follows: _no sharding \\(<\\) intra-group sharding \\(<\\) global sharding_. Additionally, we highlight that the communication bottleneck vary with trainable parameters. * When \\(\\Psi^{\\prime}\\leq\\Psi\\) in the full or partial parameters training, the bottleneck lies in the inter-node communication bandwidth. * When \\(\\Psi^{\\prime}\\ll\\Psi\\), e.g. PEFT, the bottleneck lies in the communication frequency. This is because a large amount of fragmented communications reduces overall bandwidth utilization."
    },
    {
      "title": "3.1.2 Analysis Of Memory Cost",
      "text": "In contrast to ZeRO optimizer, we account for the memory consumption of the model states with an extra _intra-group sharding_ state. Obviously, the order of memory consumption is: _global sharding \\(<\\) intra-group sharding \\(<\\) no sharding_, which is inverse to the order of communication cost. Memory savings come atthe cost of increased communication. In the mainstream mixed precision training using Adam optimizer(Kingma and Ba, 2017), the memory consumption of the parameters, gradients and optimizer states are respectively \\(2\\Psi\\), \\(2\\Psi^{\\prime}\\), and \\(12\\Psi^{\\prime}\\). In PEFT tasks, the sizes of G and OS are relatively small compared to P of mega pre-trained models. We summarize: * When \\(\\Psi^{\\prime}\\leq\\Psi\\), optimizer states consume the most memory. * When \\(\\Psi^{\\prime}\\ll\\Psi\\), parameters consumes the most memory, followed by optimizer states and model parameters."
    },
    {
      "title": "3.1.3 Trade-Off Between Memory And Communication",
      "text": "The above three levels of sharding granularity on P, G and OS brings up 27 combinations of model sharding strategies. Generally, utilizing a more fine-grained sharding level can save memory for larger batch input needs, and thus increases throughput per GPU. Therefore, there exists a trade-off between memory savings and communication costs when selecting the appropriate model sharding strategy. In all scenarios, any strategy with sharding priority \\(S_{OS}>S_{P}\\) or \\(S_{OS}>S_{G}\\) is inferior to the corresponding strategy with \\(S_{OS}=S_{P}\\) or \\(S_{OS}=S_{G}\\). This is because the former not only consumes more memory but also fails to achieve any savings in communication overhead compared to the latter. Based on this key insight, it infers to **Principle 1** that \\(S_{P}\\geq S_{OS}\\) and \\(S_{G}\\geq S_{OS}\\) in terms of the order of sharding granularity for all levels of trainable parameters. In other words, a more fine-grained shard strategy should be employed for OS compared to P and G. According to this principle, we can eliminate 13 out of the 27 possible combinations mentioned earlier. Secondly, when \\(\\Psi^{\\prime}\\geq\\frac{\\Psi}{6}\\), the memory consumption of \\(P\\) is greater than or equal to that of \\(G\\). In the full parameter training when \\(\\Psi^{\\prime}=\\Psi\\), both \\(P\\) and \\(G\\) own the same memory consumption, however, the communication frequency of \\(P\\) is as twice as \\(G\\). This is because \\(P\\) is utilized in both the forward pass and backward pass, while \\(G\\) is only used in the backward propagation. Furthermore, in the partial parameter training when \\(\\Psi^{\\prime}>\\frac{\\Psi}{6}\\), the memory consumption of gradients reduce to \\(\\Psi^{\\prime}\\) while the \\(P\\) is still \\(\\Psi\\) since all of the parameters have to be utilized in the training. Therefore, we infer that \\(S_{P}\\geq S_{G}\\). Combined with Principle 1, we achieve **Principle 2** that \\(S_{P}\\geq S_{G}\\geq S_{OS}\\) when \\(\\Psi^{\\prime}\\geq\\frac{\\Psi}{6}\\). Thirdly, in PEFT training tasks when \\(\\Psi^{\\prime}\\ll\\Psi\\), the memory consumption of \\(P\\) is still \\(\\Psi\\) to be used in the forward computation while \\(G\\) and \\(OS\\) are quite small. In this case, sharding \\(G\\) would result in a negligible amount of memory savings but would lead to increased communication overhead. This infers to **Principle 3** that \\(G\\) should not be sharded in PEFT training."
    },
    {
      "title": "Partial Redundancy Optimizer",
      "text": "Although ZeRO and MiCS advanced the development of LLM training, they only provide limited solutions. Based on Principle 1, we can filter out 14 meaningful combinations from the previously mentioned 27 combinations. These 14 combinations form our proposed PaRO strategy set, which is presented in Table 1. Among these solutions, DDP, ZeRO and MiCS can be regard as special cases within the PaRO. Based on Principles 2 and 3, we can deduce that certain strategies are meaningless under the conditions of \\(\\Psi^{\\prime}\\geq\\frac{\\Psi}{6}\\) and PEFT. Additionally, based on specific scenarios, more meaningless strategies can be eliminated.For example, in the case of \\(\\Psi^{\\prime}=\\frac{\\Psi}{6}\\), PaRO-INI is always inferior to PaRO-IIG, due to both the memory and communication costs of the former are greater than those of the latter. Another example is that, in the case of PEFT, PaRO-ING is always worse than PaRO-INI. This is because, for PEFT training, the memory consumption is almost identical for both strategies, but the former has higher communication overhead than the latter. Furthermore, we argue that the comprehensive PaRO strategy set provides more flexibility to complicated machine learning system, such as distributed RLHF system (Ouyang et al., 2022). In the following paragraphs, we provide a detailed explanation of three PaRO solutions as running examples in full parameter training: PaRO-IGG, PaRO-IIG, and PaRO-NIG. The implementation of other PaRO strategies can be easily derived from these three solutions. Figure 1 illustrates the schematic of PaRO-IGG. To simplify the diagram, we only use four GPUs and divide them into two groups. To reduce inter-group communication \\begin{table} \\begin{tabular}{l c c c c} \\hline \\hline P/G/OS & \\(\\Psi^{\\prime}=\\Psi\\) & \\(\\Psi^{\\prime}\\geq\\frac{\\Psi}{6}\\) & \\(\\Psi^{\\prime}<\\frac{\\Psi}{6}\\) & PEFT \\\\ \\hline NNN(DDP) & ✓ & ✓ & ✓ & ✓ \\\\ NNI & ✓ & ✓ & ✓ & ✓ \\\\ NNG(ZeRO-1) & ✓ & ✓ & ✓ & ✗ \\\\ NII & ✓ & ✓ & ✓ & ✗ \\\\ NIG & ✓ & ✓ & ✓ & ✗ \\\\ NGG (ZeRO-2) & ✓ & ✓ & ✓ & ✗ \\\\ INI & ✗ & ✗ & ✗ & ✓ \\\\ ING & ✗ & ✓ & ✓ & ✗ \\\\ III(MiCS) & ✗ & ✗ & ✓ & ✗ \\\\ IIG & ✓ & ✓ & ✗ & ✗ \\\\ IGG & ✓ & ✓ & ✓ & ✗ \\\\ GNG & ✗ & ✓ & ✓ & ✓ \\\\ GIG & ✗ & ✓ & ✓ & ✗ \\\\ GGG (ZeRO-3) & ✓ & ✓ & ✓ & ✗ \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Optional sharding strategies for varying numbers of trainable parameters. P/G/OS represents the combination of sharding strategies for Parameter/Gradient/Optimizer, N: no sharding, I: intra-group sharding, G: global sharding. \\(\\Psi^{\\prime}=\\Psi\\), \\(\\Psi^{\\prime}\\geq\\frac{\\Psi}{6}\\), \\(\\Psi^{\\prime}<\\frac{\\Psi}{6}\\) and PEFT means the different ratios of trainable parameters to model parameters. ✓ is recommended while ✗ is the opposite. frequency and volume, model parameters are intra-group sharded, while gradients and optimizer states are globally sharded. Therefore, a complete replica of the model parameters is stored within each group. During the training process, a mini-batch is divided into multiple micro-batches to reduce the memory consumption for storing activation outputs. In the Forward stage, each GPU obtains a complete replica of model parameters through the intra-group all-gather operation. These model parameters are used to perform the forward computation of the current layer on the input micro-batch, and are later released to reduce GPU memory consumption. After completing the Forward stage of the current layer, the system proceeds to the Forward stage of the next layer until the final layer of the network. In the Backward phase, the model parameters are collected again through the intra-group all-gather and released after the backward computation of this layer. After the backward computation, each GPU obtains a complete replica of the gradients and releases the redundant model parameters. Each GPU aggregates gradients from other GPUs through HO-Ring reduce-scatter operations for global gradient synchronization. In addition, each GPU maintains a gradient shard that accumulates gradients generated by each micro-batch. Similarly, after completing the Backward phase of the current layer, the system will execute the Backward phase of the previous layer until the first layer of the network. Once the gradients of the last micro-batch are accumulated, each GPU utilizes the gradient shard to update the optimizer state maintained by itself and generate low-precision model parameters. Finally, model parameter shards are obtained from other groups through an inter-group all-gather operation. Figure 2 illustrates the schematic of PaRO-IIG. Different from PaRO-IGG, in PaRO-IIG, the model parameters and gradients are intra-group sharded, while the optimizer states are globally sharded. Therefore, full model parameters and gradients are preserved within each group. In the Forward and pre-Backward stages, the computation processes of PaRO-IIG and PaRO-IGG are consistent. After the backward computation, each GPU aggregates gradients from other GPUs through intra-group reduce-scatter operations for local gradient synchronization. These gradients are temporarily stored on each GPU through gradient accumulation. Once the gradients of the last micro-batch are accumulated, each GPU performs an inter-group reduce-scatter operation to achieve global gradient synchronization. The subsequent Update operations are the same as PaRO-IGG. Figure 3 illustrates the schematic of PaRO-NIG. In PaRO-NIG, the parameters of the model are not sharded, the gradients are intra-group sharded, and the optimizer states are global sharded. Different from the above two solutions, each GPU retains complete model parameters in PaRO-NIG. Therefore, in the Forward and Backward stages, each GPU can directly perform the forward and backward computation without collecting and releasing model parameters. The subsequent four-step computation process of PaRO-NIG is consistent with that of PaRO-IIG. Finally, each GPU collects Figure 1: Schematic of PaRO-IGG in a grouped cluster with four GPUs. The parameters (P) of the model are sharded within the group, while gradients (G) and optimizer states (OS) are sharded globally. Labeled rectangular blocks represent shards of model parameters, gradients, and optimizer states. The solid and dashed rectangular blocks represent fixed and temporary shards respectively. Circular nodes represent operations in different stages (Forward, Backward, and Update). After feeding a micro-batch, the model will sequentially execute the Forward stage of each layer, followed by the reverse execution of the Backward stage of each layer. The Update stage will only be executed after completing the Backward stage of the last micro-batch. Figure 2: Schematic of PaRO-IIG in a grouped cluster with four GPUs. updated gradients by HO-Ring all-gather operations."
    },
    {
      "title": "Paro With Gradient Accumulation",
      "text": "In PaRO, we introduce the gradient accumulation strategy (Li et al., 2021; You et al., 2020) to obtain large batches of inputs. Furthermore, we narrow the scope of gradient synchronization to reduce communication volume and frequency. Specifically, we perform intra-group sharding and inter-group replication of gradients. The gradients of each micro-batch are synchronized through the intra-group reduce-scatter. After accumulating the gradients from all micro-batches, global gradient synchronization can be achieved by performing an inter-group reduce-scatter operation only once. Compared with the global reduce-scatter, the single-GPU communication volume reduced by the grouped two-step reduce-scatter is calculated as follows: \\[\\begin{split}\\Delta_{C}&=s*\\frac{\\Psi}{N}*(N-1)-\\\\ &\\quad\\left(s*\\frac{\\Psi}{M}*(M-1)+\\frac{\\Psi}{N}*(g-1)\\right)\\\\ &=\\frac{\\Psi*(s-1)*(g-1)}{N}\\end{split} \\tag{1}\\] where, the first item is the communication volume of global reduce-scatter, and the second item is the total communication volume of intra- and inter-group reduce-scatters. It can be observed that as the number of groups \\(g\\) and the accumulation steps \\(s\\) increase, the communication volume on a single GPU decreases further. In the absence of cluster grouping (i.e. \\(g=1\\)) or gradient accumulation (i.e. \\(s=1\\)), there is no reduction in single-GPU communication volume. Therefore, the combination of gradient accumulation and cluster grouping is of practical significance to reduce communication."
    },
    {
      "title": "Ho-Ring For All-Gather And Reduce-Scatter",
      "text": "Since the model state is sharded in a GPU cluster, it is necessary to aggregate or scatter these shards for global synchronization, such as the all-gather for parameters in PaRO-NIG and the reduce-scatter for gradients in PaRO-IGG. In the traditional Ring, each GPU sequentially transfers its shard of data to the next GPU. The transmission efficiency of cross-node communication may be a bottleneck affecting model training. The H-Ring groups GPUs based on their respective nodes. The global all-gather/reduce-scatter is divided into two steps: intra- and inter-group all-gather/reduce-scatter, to improve inter-group bandwidth utilization and avoid partial GPU waiting. However, during inter-group communication, the intra-group bandwidth is idle, resulting in a waste of resources. Therefore, we proposed a HO-Ring communication topology for all-gather/reduce-scatter. Figure 4 shows the communication topology of HO-Ring. Like H-Ring, the GPUs in HO-Ring are also grouped based on their respective nodes. Each GPU transmits its own shards simultaneously through the intra- and inter-group communication rings, as shown in the first two steps in Figure 4. Different from the H-Ring, HO-Ring can simultaneously utilize communication resources within and between groups to improve transmission efficiency. After the inter-group communication ring is completed, an intra-group communication ring is executed to gather the remaining shards within the group, as shown in the third step in Figure 4."
    },
    {
      "title": "Memory And Communication Analysis",
      "text": "This section analyzes the advantages of the above three solutions in terms of memory consumption and communication by comparing other solutions. Table 2 shows the single-GPU memory consumption of parameter P, gradient G and optimizer state OS in different solutions. As can be seen, the single-GPU memory consumption of ZeRO-1, ZeRO-2, and ZeRO-3 is not af Figure 4: Communication topology of HO-Ring. The \\(N\\) (\\(N=9\\)) GPUs (G0-GS) are divided equally into \\(g\\) (\\(g=3\\)) groups. Red and black arrows represent intra- and inter-group communication respectively. Orange and green blocks represent data obtained through intra- and inter-group communication, respectively. Figure 3: Schematic of PaRO-NIG in a grouped cluster with four GPUs. fected by the number of groups, as they only perform global sharding operations. MiCS shards the entire model state within the group and introduces inter-group redundancy. As a result, the memory of MiCS linearly increases with the number of groups. Based on ZeRO-3, ZeRO++ additionally retains the intra-group sharding of model parameters, while PaRO-IGG only retains the intra-group sharding of model parameters. Therefore, the memory of PaRO-IGG and ZeRO++ slowly increases with the number of groups. PaRO-IIG shards model parameters and gradients within groups, further increasing memory redundancy. Based on ZeRO-2, PaRO-NIG shards gradient groups, and its memory also increases slowly as the number of groups increases. Table 3 shows the total communication volume of different solutions in the Forward, Backward and Update stages with a mini-batch input. As can be seen from Table 3, each GPU in ZeRO-1 performs gradient accumulation locally, and only performs a global synchronization after gradient accumulation. MiCS performs intra-node communication in the Forward and Backward stages, and only performs a partial gradient all-reduce operation for parameter update. For ZeRO++, due to the secondary intra-node sharding of the collected model parameters in the Forward stage, the parameters can be collected using an intra-node all-gather operation in the Backward stage. Since MiCS, PaRO-IGG and PaRO-IIG shard the model parameters within the group, the all-gather operation in forward and backward computation is intra-group communication. Compared with ZeRO-3, these solutions increase the size of a single transmission (\\(\\frac{\\Psi}{M}\\) vs. \\(\\frac{\\Psi}{N}\\)) and reduce the number of communications (\\(s*(M-1)\\) vs. \\(s*(N-1)\\)), which can improve the bandwidth utilization within the group. Compared with ZeRO-2, PaRO-NIG splits the global reduce-scatter of the gradient into two steps: intra- and inter-group reduce-scatters. Figure 5 shows the total intra- and inter-group communication volume of different solutions under the condition of \\(\\Psi=7B,N=64,s=8,g=8\\), as well as the memory occupation of a single GPU. ZeRO-1, ZeRO-2, and ZeRO-3 progressively shard the model state, resulting in a near-linear reduction in single-GPU memory and a near-linear increase in intra- and inter-group communication volume. Compared with ZeRO-3, the single-GPU memory of ZeRO++, PaRO-IGG and PaRO-IIG increases slightly, while that of MiCS increases significantly; the intra-group communication volume of ZeRO++, PaRO-IGG and PaRO-IIG is the same as ZeRO-3, only slightly increased in MiCS; the inter-group communication volume of MiCS, ZeRO++, PaRO-IGG and PaRO-IIG is smaller than ZeRO-3, with MiCS and PaRO-IIG being the lowest. Compared with ZeRO-2, the intra-group communication volume and single-GPU memory of PaRO-NIG increase slightly, while the inter-group communication volume decreases significantly. In summary, PaRO offer a better balance between memory consumption and communication costs for LLM training."
    },
    {
      "title": "Use Paro In Complex Ml Systems",
      "text": "PaRO can also be applied in complex ML systems. For instance, in the PPO step of RLHF, it is sometimes necessary to deploy partial states of multiple models, such as Actor, Critical, and Reward models, on a single GPU. Each model has different memory and communication requirements. By applying different PaRO strategies to different models, it is possible to better balance the cost of each model, thereby improving the end-to-end PPO speed."
    },
    {
      "title": "4 Experiments And Analysis",
      "text": "In this section, we perform end-to-end training to evaluate the throughput and scalability of the proposed PaRO. Afterwards, we evaluate the transmission efficiency of the HO-Ring communication topology. Finally, we demonstrate the consistent convergence of PaRO and ZeRO, which validates the correctness of our system. \\begin{table} \\begin{tabular}{l l l l} \\hline \\hline Model states & P & G & OS \\\\ \\hline ZeRO-1 & \\(2\\Psi\\) & \\(2\\Psi\\) & \\(\\frac{K\\Psi}{N}\\) \\\\ ZeRO-2 & \\(2\\Psi\\) & \\(\\frac{2\\Psi}{N}\\) & \\(\\frac{K\\Psi}{N}\\) \\\\ ZeRO-3 & \\(\\frac{2\\Psi}{N}\\) & \\(\\frac{2\\Psi}{N}\\) & \\(\\frac{K\\Psi}{N}\\) \\\\ MiCS & \\(\\frac{2\\Psi}{M}\\) & \\(\\frac{2\\Psi}{M}\\) & \\(\\frac{K\\Psi}{M}\\) \\\\ ZeRO++ & \\(\\frac{2\\Psi}{N}+\\frac{2\\Psi}{M}\\) & \\(\\frac{2\\Psi}{N}\\) & \\(\\frac{K\\Psi}{N}\\) \\\\ PaRO-IGG & \\(\\frac{2\\Psi}{M}\\) & \\(\\frac{2\\Psi}{N}\\) & \\(\\frac{K\\Psi}{N}\\) \\\\ PaRO-IIG & \\(\\frac{2\\Psi}{M}\\) & \\(\\frac{2\\Psi}{M}\\) & \\(\\frac{K\\Psi}{N}\\) \\\\ PaRO-NIG & \\(2\\Psi\\) & \\(\\frac{2\\Psi}{M}\\) & \\(\\frac{K\\Psi}{N}\\) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Single-GPU memory consumption of parameter P, gradient G and optimizer state OS in different solutions. Figure 5: Under the conditions of \\(\\Psi=7B,N=64,s=8,g=8\\), the total communication volume (intra- and inter-group), and single GPU memory of model states."
    },
    {
      "title": "Experiment Environments",
      "text": "Our experimental cluster consists of up to 16 DGX nodes, with each node containing 8 Ampere A100 SXM3 80GB GPUs. The GPUs in each node are interconnected via NVLink/NVSwitch with a bidirectional bandwidth of up to 600GB/s. These nodes are connected through 8 InfiniBand adapters without NVIDIA SHARP, which can provide more than 100GB/s of inter-node bandwidth. The software environment includes CUDA-11.7, DeepSpeed-v0.10.0, PyTorch-v1.9.2, and NCCL-v2.14.3."
    },
    {
      "title": "Throughput Performance",
      "text": "We used ZeRO-2 and ZeRO-3 in Deepspeed as baselines to implement PaROs with different sharding strategies. To evaluate the performance of PaROs, we compared them with current state-of-the-art solutions, including: ZeRO, ZeRO-3, MiCS, ZeRO++ and FSDP-hs. ZeRO-1 was not considered due to its inability to run the smallest scale model in our experiments. We used two LLMs with different parameter sizes: LLaMA-7B and LLaMA-65B (Touvron et al., 2023), to evaluate the throughput and acceleration performance at varying GPU counts. For the LLaMA-65B model, we activated checkpointing to ensure successful training. The C4 corpus in RedPajama was used as the training data set. During training, we set the sequence length to 512, the batch size to 40 (divided into 4 micro-batches), the number of gradient accumulation steps to 10, and mixed precision. All throughput data reported was the average of 100 iterations. Figure 6 shows the throughput and peak memory of LLaMA-7B and LLaMA-65B in different solutions. In Figure 6(a), the throughput of PaRO-IGG is only better than that of ZeRO-3 and ZeRO++; the throughput of PaRO-IIG is almost the same as that of MiCS. Compared with the baseline ZeRO-3, the throughput of PaRO-IGG and PaRO-IIG is improved by 1.37x and 1.94x (with 32 GPUs), 1.31x and 2.50x (with 128 GPUs), respectively. Compared with the baseline ZeRO-2, the throughput of PaRO-NIG is improved by 1.70x (with 32 GPUs) and 2.25x (with 128 GPUs), respectively. For the small-scale LLaMA-7B, PaRO-NIG approaches show higher throughput in clusters with 32 and 128 GPUs. Figure 6(b) shows the maximum reserved memory during training for different solutions. The peak memory of ZeRO-3 is the smallest. Since both MiCS and FSDP-hs adopt an intra-group sharding strategy, their peak memory is only related to the number of GPUs in the group, but not to the number of GPUs in the cluster. The peak memory of PaRO-IGG, PaRO-IIG and PaRO-NIG increase slightly compared to baseline ZeRO-3 and ZeRO-2 respectively. The peak memory of all three PaROs is smaller than MiCS. Figure 6(c) presents the single GPU throughput of LLaMA-65B with different approaches. Since LLaMA-65B requires finer-grained sharding, only ZeRO-3, ZeRO++, PaRO-IGG, and PaRO-IIG can perform training, while other solutions suffer from out-of-memory (OOM) issues. Compared with the baseline ZeRO-3, the throughput of PaRO-IGG and PaRO-IIG is improved by 1.19x and 1.35x (with 32 GPUs), 1.36x and 1.77x (with 128 GPUs), respectively. For the large-scale LLaMA-65B, training efficiency is higher with the PaRO-IGG and PaRO-IIG compared to ZeRO-3. \\begin{table} \\begin{tabular}{l c c c c c} \\hline \\hline & Forward & \\multicolumn{2}{c}{Backward} & \\multicolumn{2}{c}{Update} \\\\ \\cline{2-6} Methods & A-G(P) & A-G(P) & R-S(G) & R-S(G)/**A-R(G)** & A-G(P) \\\\ \\hline ZeRO-1 & 0 & 0 & 0 & \\(\\begin{array}{cc}\\mathbf{2*g*g}\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}- \\mathbf{1})^{\\dagger}+\\\\ 2*(\\mathbf{N}-\\mathbf{g})*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1} )&(N-g)*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1})\\end{array}\\) & \\(\\begin{array}{cc}g*g*(\\mathbf{N}-\\mathbf{1})^{\\dagger}+\\\\ (N-g)*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1})\\end{array}\\) \\\\ ZeRO-2 & 0 & 0 & \\(\\begin{array}{cc}g*s*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}- \\mathbf{1})^{\\dagger}+\\\\ (N-g)*s*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1})\\end{array}\\) & \\(\\begin{array}{cc}0&g*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1})^{ \\dagger}+\\\\ (N-g)*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1})\\end{array}\\) \\\\ ZeRO-3 & \\(\\begin{array}{cc}g*s*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1})^{ \\dagger}+\\\\ (N-g)*s*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1})\\end{array}\\) & \\(\\begin{array}{cc}g*s*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1})^{ \\dagger}+\\\\ (N-g)*s*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1})\\end{array}\\) & \\(\\begin{array}{cc}g*s*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1})^{ \\dagger}+\\\\ (N-g)*s*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{N}-\\mathbf{1})\\end{array}\\) \\\\ MiCS & \\(\\begin{array}{cc}N*s*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{M}-\\mathbf{1})& N*s*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{M}-\\mathbf{1})&N*s*\\frac{\\mathbf{g}}{\\mathbf{N}}*( \\mathbf{M}-\\mathbf{1})&\\begin{array}{cc}\\mathbf{2*g*g}*\\frac{\\mathbf{g}}{ \\mathbf{N}}*(\\mathbf{g}-\\mathbf{1})^{\\dagger}+\\\\ 2*(\\mathbf{N}-\\mathbf{g})*\\frac{\\mathbf{g}}{\\mathbf{N}}*(\\mathbf{g"
    },
    {
      "title": "Near-Linear Scalability",
      "text": "To analyze the relationship between throughput and GPU resources, we collected the single-GPU throughput of PaRO and ZeRO under different GPU numbers, as shown in Figure 8. The experiments were conducted using LLaMA-7B. Overall, under the same GPU resource conditions, the single-GPU throughput of PaRO is higher than the baseline ZeRO-2 and ZeRO-3. The single-GPU throughput of different approaches gradually decreases as the number of GPUs increases. The throughput of PaRO-IIG decreases the least, and the throughput of ZeRO-2 decreases the most. Since NCCL adopts a multi-machine communication method based on Double Binary Tree (Sanders et al., 2009), the communication efficiency of 96 GPUs (not an integer power of 2) is lower than that of 64 and 128 GPUs. It can be seen that as the cluster size increases, PaRO-IIG can maintain near-linear scalability."
    },
    {
      "title": "Ho-Ring Communication Performance",
      "text": "In the section, we performed experiments using 16 DGX nodes, with a total communication volume set to 1GB. We measured the communication time of the all-gather operation with the traditional Ring (baseline), H-Ring and HO-Ring. The communication times of traditional Ring, H-Ring and HO-Ring are 288ms, 183ms and 162ms respectively. Compared with Ring and H-Ring, the communication time of HO-Ring is reduced by 36.5% and 11.5% respectively. Therefore, HO-Ring can significantly improve communication efficiency by improving the communication topology."
    },
    {
      "title": "Model Convergence",
      "text": "We used LLaMA-7B and C4 corpus in RedPajama to evaluate the convergence of PaRO. During training, we set the sequence length to 128, the batch size to 1024 (divided into 8 micro-batches) and the number of gradient accumulation steps to 8. The loss validation process does not aim to produce exactly the same loss as ZeRO but to ensure the convergence behaviours are the same. As shown in Figure 9, PaRO provides the same convergence as ZeRO."
    },
    {
      "title": "5 Conclusion",
      "text": "In this paper, we present PaRO, a system balances the memory occupation and communication costs across diverse training scenarios. PaRO provides comprehensive options which reduces the communication cost of inter-group communication with minor memory redundancy by fine-grained sharding strategy, thereby improving the training efficiency. Additionally, we propose a HO-Ring communication topology to enhance collective communication efficiency between nodes or across switches. We evaluate PaRO on different training workloads on large-scale clusters. PaRO outperforms ZeRO by up to \\(2.50\\times\\) and demonstrates near-linear scalability in various industrial-level training settings."
    },
    {
      "title": "References",
      "text": "* A. C. Agarwal, S. M. Balle, F. G. Gustavson, M. Joshi, and P. Palkar (1995)A three-dimensional approach to parallel matrix multiplication. IBM Journal of Research and Development39 (5), pp. 575-582. External Links: Document, Link Cited by: SS1. * G. Andrews (2017)Bringing hpc techniques to deep learning. External Links: Link Cited by: SS1. * Z. Bian, Q. Xu, B. Wang, and Y. You (2021)Maximizing parallelism in distributed training for huge neural networks. External Links: Link Cited by: SS1. * T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, I. Radford, and D. Sutskever (2020)Language models are few-shot learners. In Advances in Neural Information Processing Systems, H. Ranzato, M. Hadsell, M. Balcan, and H. Lin (Eds.), pp. 1877-1901. External Links: Link Cited by: SS1. * J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019)Bert: pre-training of deep bidirectional transformers for language understanding. External Links: Link Cited by: SS1. * R. Girshick, J. Donahue, T. Darrell, and J. Malik (2014)Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 580-587. Cited by: SS1. * E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen (2022)LoRA: low-rank adaptation of large language models. International Conference on Learning Representations. External Links: Link Cited by: SS1. * X. Jia, S. Song, W. He, Y. Wang, H. Hong, F. Zhou, L. Xie, Z. Guo, Y. Yang, L. Yu, et al. (2018)Highly scalable deep learning training system with mixed-precision: training imagenet in four minutes. arXiv preprint. External Links: Link Cited by: SS1. * D. P. Kingma and J. Ba (2017)Adam: a method for stochastic optimization. External Links: 1712.02146 Cited by: SS1. * V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersen, M. Shoeybi, and B. Catanzaro (2023)Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems5. Cited by: SS1. * C. Li, A. A. Awan, H. Tang, S. Rajbhandari, and Y. He (2021)1-bit lamb: communication efficient large-scale large-batch training with lamb's convergence speed. External Links: 2102.02146 Cited by: SS1. * S. Li, H. Liu, Z. Bian, J. Fang, H. Huang, Y. Liu, B. Wang, and Y. You (2023)Colossal-ai: a unified deep learning system for large-scale parallel training. In Proceedings of the 52nd International Conference on Parallel Processing, ICPP '23, New York, NY, USA, pp. 766-775. External Links: Link, Document Cited by: SS1. * V. Lialin, V. Deshpande, and A. Rumshisky (2023)Scaling down to scale up: a guide to parameter-efficient fine-tuning. arXiv preprint arXiv:2303.15647. Cited by: SS1. * X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, and J. Tang (2022)P-tuning: prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Dublin, Ireland, May 2022. External Links: Link Cited by: SS1. * H. Mikami, H. Suganuma, P. U-chupala, Y. Tanaka, and Y. Kageyama (2018)Massively distributed sgd: imagenet/resnet-50 training in a flash. arXiv preprint. External Links: 1803.02146 Cited by: SS1. * L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishra, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. (2022)Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems35, pp. 27730-27744. External Links: Link, 2202.02146 Cited by: SS1. * J. Proficz (2018)Improving all-reduce collective operations for imbalanced process arrival patterns. The Journal of Supercomputing74 (7), pp. 3071-3092. External Links: Link, Document Cited by: SS1. * S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He (2020)Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '20, New York, NY, USA, pp. 9781728199986. External Links: Link, Document Cited by: SS1. * P. Sanders, J. Speck, and J. L. Traff (2009)Two-tree algorithms for full bandwidth broadcast, reduction and scan. Parallel Computing35 (12), pp. 581-594. External Links: Link, Document Cited by: SS1. * M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro (2020)Megatron-lm: training multi-billion parameter language models using model parallelism. External Links: 2006.02146 Cited by: SS1. * M. P. Solomonik, E. and Demmel, J. Communication-optimal parallel 2.5d matrix multiplication and lu factorization algorithms. In Jeannot, E., Namyst, R., and Roman, J. (eds.), _Euro-Par 2011 Parallel Processing_, pp. 90-109, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg. ISBN 978-3-642-23397-5. * Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _arXiv preprint_, 2023. * Wang et al. (2021) Wang, B., Xu, Q., Bian, Z., and You, Y. 2.5-dimensional distributed model training. _arXiv e-prints_, pp. arXiv-2105, 2021. * Wang et al. (2020) Wang, G., Venkataraman, S., Phanishayee, A., Devanur, N., Thelin, J., and Stoica, I. Blink: Fast and generic collectives for distributed ml. _Proceedings of Machine Learning and Systems_, 2:172-186, 2020. * Wang et al. (2023) Wang, G., Qin, H., Jacobs, S. A., Holmes, C., Rajbhandari, S., Ruwase, O., Yan, F., Yang, L., and He, Y. Zero++: Extremely efficient collective communication for giant model training, 2023. * Xiao et al. (2023) Xiao, Y., Zhao, S., Zhou, Z., Huan, Z., Ju, L., Zhang, X., Wang, L., and Zhou, J. G-meta: Distributed meta learning in gpu clusters for large-scale recommender systems. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, CIKM '23, pp. 4365-4369, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701245. doi: 10.1145/3583780.3615208. * Xu & You (2023) Xu, Q. and You, Y. An efficient 2d method for training super-large deep learning models. In _2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)_, pp. 222-232, 2023. doi: 10.1109/IPDPS54959.2023.00031. * You et al. (2020) You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J. Large batch optimization for deep learning: Training bert in 76 minutes, 2020. * Zhang et al. (2022) Zhang, Z., Zheng, S., Wang, Y., Chiu, J., Karypis, G., Chilimbi, T., Li, M., and Jin, X. Mics: Near-linear scaling for training gigantic model on public cloud. _Proc. VLDB Endow._, 16(1):37-50, sep 2022. ISSN 2150-8097. doi: 10.14778/3561261.3561265. * Zhao et al. (2023) Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., Desmaison, A., Balioglu, C., Damania, P., Nguyen, B., Chauhan, G., Hao, Y., Mathews, A., and Li, S. Pytorch fsdp: Experiences on scaling fully sharded data parallel. _Proc. VLDB Endow._, 16(12):3848-3860, sep 2023. ISSN 2150-8097. doi: 10.14778/3611540.3611569."
    }
  ]
}