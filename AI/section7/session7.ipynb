{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IdBjSbjjMWkW","executionInfo":{"status":"ok","timestamp":1749664158638,"user_tz":240,"elapsed":21163,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"ec009a0a-ad90-4ef8-bec5-369f3650d6ec"},"id":"IdBjSbjjMWkW","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Constructing a Knowledge Graph from Research Publications\n","\n","1. Introduction to Knowledge Graph\n","\n","2. Why Are Knowledge Graphs Useful in Literature Study?\n","\n","3. How to build a knowledge graph\n","  - Set up a Neo4j instance with Neo4j and Cypher\n","  - Extract Metadata from research paper\n","  - Knowledge Graph Setup\n","  - Vector embedding\n","  - Ask questions\n","  - Visualize Knowledge Graph\n","  - Writing Cypher with an LLM"],"metadata":{"id":"E-o3EbLOVTG_"},"id":"E-o3EbLOVTG_"},{"cell_type":"markdown","source":["## 1. Introduction to Knowledge Graphs:\n","\n","Graphs are great at representing and storing heterogeneous and interconnected information in a structured manner, effortlessly capturing complex relationships and attributes across diverse data types. In contrast, vector databases often struggle with such structured information, as their strength lies in handling unstructured data through high-dimensional vectors. (https://medium.com/neo4j/enhancing-the-accuracy-of-rag-applications-with-knowledge-graphs-ad5e2ffab663)\n"],"metadata":{"id":"HG1nDP2BVWeG"},"id":"HG1nDP2BVWeG"},{"cell_type":"markdown","source":["<image src= 'https://media.licdn.com/dms/image/v2/C5612AQEt5xJTvQ0HDg/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1639491776767?e=1749686400&v=beta&t=O9kUhJuqtm1HEPdrsWHhIkKVlQ_-TKSgyLyzt2vCyvU'>\n","\n","\n","Image source: https://www.linkedin.com/pulse/getting-intelligent-answers-from-knowledge-graphs-peter-lawrence/"],"metadata":{"id":"o7PAjzAmVaJs"},"id":"o7PAjzAmVaJs"},{"cell_type":"markdown","source":["## 2. Why Are Knowledge Graphs Useful in Literature Study?\n","\n","Connected Papers:\n","A connected graph approach that shows similarity between publications, even when there's no direct citation, helping to infer topic similarity. **Highlights the most relevant and essential papers.**\n","\n","Bibliometric Networks:\n","Maps and constructs networks based on co-authorship or co-occurrence. **Uses distance-based visualization to indicate similarity between publications. Helps identify high-impact publications in a field.**\n","\n","Concept Structuring and Clustering:\n","Tools like SciKGraph automate the structuring of concepts/topics within a domain. **Uses semantic-based analysis and NLP-driven classification. High accuracy in organizing scientific content.**\n","\n","Moreover, when we use RAG for literature study, RAG using only vector search has limitations. It misses out on context, can't grasp relationships between data points, and often produces unreliable results. When your app needs to understand the connections between author, topic, and citations, basic RAG falls short. You can end up with hallucinations or irrelevant, unexplainable results. Combining knowledge graph and traditional RAG aporaches can better solve this issue."],"metadata":{"id":"JI_EJR_oVdBy"},"id":"JI_EJR_oVdBy"},{"cell_type":"markdown","source":["## 3. How to build a knowledge graph\n","\n","**Set up a Neo4j instance.**  \n","   This will serve as the platform for building and exploring our knowledge graph.\n","\n","**Process the data.**  \n","   We'll extract key metadataâ€”such as authors and keywords (topics)â€”from a sample of 50 research papers. Each paper will also be split into smaller chunks for more granular analysis.\n","\n","**Build the knowledge graph.**  \n","   We'll add nodes representing papers, chunks, authors, and topics, and define relationships between them.  \n","   Each chunk will be embedded using a vector embedding model. These embeddings will be stored in a vector store, enabling efficient semantic search and question answering.\n","\n","**Query with Cypher using an LLM.**  \n","   Weâ€™ll use a large language model (LLM) to help generate Cypher queries, so you donâ€™t need to write them manually. Simply ask your question in natural language, and the LLM will translate it into Cypher.\n"],"metadata":{"id":"QpeeYu20Vf0g"},"id":"QpeeYu20Vf0g"},{"cell_type":"markdown","source":["\n","### Set up a Neo4j instance with Neo4j and Cypher\n","\n","#### What is Neo4j\n","\n","**Neo4j** is a **graph database**. Unlike traditional relational databases that use tables, Neo4j stores data in **nodes** and **relationships**.\n","\n","This model is ideal for use cases such as:\n","- Social networks\n","- Recommendation engines\n","- Knowledge graphs\n","- Fraud detection\n","\n","| Concept        | Description                             | Example                          |\n","|----------------|-----------------------------------------|----------------------------------|\n","| **Node**       | An entity or object                     | A person, a product, a location  |\n","| **Relationship** | A connection between nodes            | 'FRIENDS_WITH', 'PURCHASED'      |\n","| **Label**      | A category or type assigned to nodes    | ':Person', ':Movie'             |\n","| **Property**   | Key-value pair on a node/relationship   | 'name: \"Alice\"', 'age: 30'       |\n","\n","\n","\n","#### What is Cypher?\n","\n","**Cypher** is Neo4jâ€™s query language. It's designed to work with **graph patterns** and is intuitive, much like SQL is for relational databases.\n","\n","\n"],"metadata":{"id":"u40mrl1dViht"},"id":"u40mrl1dViht"},{"cell_type":"markdown","source":["### Neo4j instance\n","In this workshop, we will provide a Neo4j instance via api_keys.txt.\n","\n","If you would like to use your own Neo4j instance, the easiest way is to start a on Neo4j Aura (https://neo4j.com/product/auradb/), which offers cloud instances of Neo4j database. Alternatively, you can also set up a local instance of the Neo4j database by downloading the Neo4j Desktop application (https://neo4j.com/download/) and creating a local database instance.\n","\n","\n","Please remember to keep the private key\n","\n","**Neo4j Desktop uses Graph Apps and other web content. Some of these are provided by the community and, like any other software you install, could potentially cause data integrity and security issues. If you're working with sensitive data we recommend that you perform an independent security audit before using it.**"],"metadata":{"id":"iMAMtynyVmLJ"},"id":"iMAMtynyVmLJ"},{"cell_type":"markdown","source":["Loading api keys from '/content/drive/MyDrive/Colab_Notebooks/AI/api_keys.txt'"],"metadata":{"id":"jn3M8QB5VtGm"},"id":"jn3M8QB5VtGm"},{"cell_type":"code","source":["import os\n","\n","api_keys_path = '/content/drive/MyDrive/Colab_Notebooks/AI/api_keys.txt'\n","\n","with open(api_keys_path) as f:\n","    for line in f:\n","        key, value = line.strip().split('=')\n","        os.environ[key] = value"],"metadata":{"id":"YjGNBjJQ3MTn","executionInfo":{"status":"ok","timestamp":1749664161570,"user_tz":240,"elapsed":2933,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"YjGNBjJQ3MTn","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["If you prefer to save the informaiton to Google Secrets instead of\n","using the api_keys.txt file, please uncomment the following code."],"metadata":{"id":"K9fDkQHd2S5n"},"id":"K9fDkQHd2S5n"},{"cell_type":"code","source":["# from google.colab import userdata\n","# import os\n","\n","# # Securely access the API key from Colab's Secrets\n","# try:\n","#     openai_api_key = userdata.get('OPENAI_API_KEY')\n","# except Exception as e:\n","#     print(\"Error: Please add your OpenAI API key to Colab Secrets\")\n","#     print(\"Steps: 1. Click the 'key' icon in the left panel\")\n","#     print(\"       2. Add a secret named OPENAI_API_KEY with your API key\")\n","#     raise e\n","\n","# os.environ['OPENAI_API_KEY'] = openai_api_key\n","# os.environ[\"NEO4J_URI\"] = userdata.get('NEO4J_URI')\n","# os.environ[\"NEO4J_USERNAME\"] = userdata.get('NEO4J_USERNAME')\n","# os.environ[\"NEO4J_PASSWORD\"] = userdata.get('NEO4J_PASSWORD')\n"],"metadata":{"id":"jYxEt_QrMUJ3","executionInfo":{"status":"ok","timestamp":1749664161602,"user_tz":240,"elapsed":31,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"jYxEt_QrMUJ3","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4RYm22BxOIwP"},"source":["### Install packages"],"id":"4RYm22BxOIwP"},{"cell_type":"code","source":["!pip install -q langchain langchain-community langchain-openai langchain-experimental neo4j tiktoken yfiles_jupyter_graphs PyPDF2 faiss-cpu sentence-transformers transformers python-dotenv requests torch pypdf PyPDF2 pdfplumber keybert"],"metadata":{"id":"A4X_BntYOfg1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749664284639,"user_tz":240,"elapsed":123027,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"a2f8edf2-88c7-4d62-c3e1-8f9c8f68805e"},"id":"A4X_BntYOfg1","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m312.3/312.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"B2x9CMFgOIwP","executionInfo":{"status":"ok","timestamp":1749664328506,"user_tz":240,"elapsed":43864,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"outputs":[],"source":["import os\n","import textwrap\n","import getpass\n","import logging\n","import time\n","\n","# Langchain and vector embeddings\n","from langchain_community.graphs import Neo4jGraph\n","from langchain_community.vectorstores import Neo4jVector\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.chains import RetrievalQAWithSourcesChain\n","from langchain_openai import ChatOpenAI\n","from langchain_openai import OpenAIEmbeddings\n","from langchain.prompts import PromptTemplate\n","\n","# Import KeyBERT for better phrase extraction\n","from keybert import KeyBERT\n","\n","import pdfplumber\n","import hashlib\n","from typing import Dict, List\n","import re\n","import shutil\n","import glob\n","import json\n","\n","from neo4j.exceptions import ServiceUnavailable, IncompleteCommit\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"id":"B2x9CMFgOIwP"},{"cell_type":"code","source":["os.environ[\"NEO4J_URI\"],"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lmJphohoiV1q","executionInfo":{"status":"ok","timestamp":1749664328514,"user_tz":240,"elapsed":5,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"26d06a60-66fc-4b1e-9eda-67d0360d4430"},"id":"lmJphohoiV1q","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('neo4j+s://f21edcc2.databases.neo4j.io',)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"l1_URP4VkrQg","executionInfo":{"status":"ok","timestamp":1749664328516,"user_tz":240,"elapsed":1,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"l1_URP4VkrQg","execution_count":6,"outputs":[]},{"cell_type":"code","source":["import os, socket, subprocess, pprint, re\n","\n","def check_env():\n","    uri = os.getenv(\"NEO4J_URI\")\n","    user = os.getenv(\"NEO4J_USERNAME\")\n","    pwd = os.getenv(\"NEO4J_PASSWORD\")\n","    print(\"ğŸ” Environment values\")\n","    print(\"  NEO4J_URI     :\", repr(uri))\n","    print(\"  NEO4J_USERNAME:\", repr(user))\n","    print(\"  NEO4J_PASSWORD:\", \"<set>\" if pwd else None)\n","\n","    if uri and uri.strip() != uri:\n","        print(\"âš ï¸  NEO4J_URI has leading/trailing whitespace!\")\n","    if uri and not re.match(r\"^neo4j\\+s://.+\\.databases\\.neo4j\\.io$\", uri.strip()):\n","        print(\"âš ï¸  URI does not look like a valid Aura URI (neo4j+s://<hash>.databases.neo4j.io).\")\n","\n","def dns_lookup(host):\n","    print(f\"\\nğŸ” DNS lookup for {host}\")\n","    try:\n","        info = socket.getaddrinfo(host, 7687)\n","        pprint.pprint(info)\n","        print(\"âœ… Host resolved.\")\n","        return True\n","    except socket.gaierror as e:\n","        print(\"âŒ DNS resolution failed:\", e)\n","        return False\n","\n","def port_check(host, port=7687, timeout=3):\n","    print(f\"\\nğŸ” TCP connect to {host}:{port}\")\n","    try:\n","        with socket.create_connection((host, port), timeout=timeout):\n","            print(\"âœ… TCP connection succeeded.\")\n","            return True\n","    except OSError as e:\n","        print(\"âŒ TCP connection failed:\", e)\n","        return False\n","\n","def shell_nslookup(host):\n","    print(\"\\nğŸ” shell nslookup\")\n","    try:\n","        out = subprocess.check_output([\"nslookup\", host], text=True)\n","        print(out)\n","    except FileNotFoundError:\n","        print(\"`nslookup` not available in this container.\")\n","    except subprocess.CalledProcessError as e:\n","        print(\"nslookup error:\", e.output)\n","\n","def run_all():\n","    check_env()\n","\n","    uri = os.getenv(\"NEO4J_URI\", \"\")\n","    host = uri.split(\"://\")[-1] if \"://\" in uri else uri\n","    host = host.strip()\n","\n","    if host:\n","        if dns_lookup(host):\n","            port_check(host, 7687)\n","        shell_nslookup(host)\n","    else:\n","        print(\"\\nâ›”ï¸ NEO4J_URI is empty; set it before running network tests.\")\n","\n","run_all()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyfnZZ6Mibl-","executionInfo":{"status":"ok","timestamp":1749664328758,"user_tz":240,"elapsed":242,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"5a2ca89d-7692-4ef9-bcc1-eff181f20ea8"},"id":"zyfnZZ6Mibl-","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” Environment values\n","  NEO4J_URI     : 'neo4j+s://f21edcc2.databases.neo4j.io'\n","  NEO4J_USERNAME: 'neo4j'\n","  NEO4J_PASSWORD: <set>\n","\n","ğŸ” DNS lookup for f21edcc2.databases.neo4j.io\n","[(<AddressFamily.AF_INET: 2>,\n","  <SocketKind.SOCK_STREAM: 1>,\n","  6,\n","  '',\n","  ('34.121.155.65', 7687)),\n"," (<AddressFamily.AF_INET: 2>,\n","  <SocketKind.SOCK_DGRAM: 2>,\n","  17,\n","  '',\n","  ('34.121.155.65', 7687)),\n"," (<AddressFamily.AF_INET: 2>,\n","  <SocketKind.SOCK_RAW: 3>,\n","  0,\n","  '',\n","  ('34.121.155.65', 7687))]\n","âœ… Host resolved.\n","\n","ğŸ” TCP connect to f21edcc2.databases.neo4j.io:7687\n","âœ… TCP connection succeeded.\n","\n","ğŸ” shell nslookup\n","Server:\t\t127.0.0.11\n","Address:\t127.0.0.11#53\n","\n","Non-authoritative answer:\n","Name:\tf21edcc2.databases.neo4j.io\n","Address: 34.121.155.65\n","\n","\n"]}]},{"cell_type":"markdown","source":["### Connect to Neo4j"],"metadata":{"id":"7ztdohexSttt"},"id":"7ztdohexSttt"},{"cell_type":"code","source":["kg = Neo4jGraph(\n","    url=os.environ[\"NEO4J_URI\"],\n","    username=os.environ[\"NEO4J_USERNAME\"],\n","    password=os.environ[\"NEO4J_PASSWORD\"]\n",")\n"],"metadata":{"id":"lL_m8Xw-RYLs","executionInfo":{"status":"ok","timestamp":1749664330997,"user_tz":240,"elapsed":2227,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"lL_m8Xw-RYLs","execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Function - Extracting Metadata from research paper\n","\n","- paper id\n","- file name\n","- title\n","- authors\n","- year\n","- venue\n","- keywords\n","- abstract\n","\n","\n"],"metadata":{"id":"etseE-W0Z-1E"},"id":"etseE-W0Z-1E"},{"cell_type":"code","source":["def extract_text_from_file(file_path):\n","    \"\"\"Extract text content from a file (PDF,TXT or JSON)\"\"\"\n","    # when the file is .pdf\n","    if file_path.lower().endswith('.pdf'):\n","        text = \"\"\n","        try:\n","            with pdfplumber.open(file_path) as pdf:\n","                for page in pdf.pages:\n","                    extracted = page.extract_text()\n","                    if extracted:\n","                        text += extracted + \"\\n\"\n","        except Exception as e:\n","            print(f\"Error extracting text from {file_path}: {e}\")\n","            return \"\"\n","        return text\n","\n","    # when the file is .txt\n","    elif file_path.lower().endswith('.txt'):\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                return file.read()\n","        except UnicodeDecodeError:\n","            try:\n","                with open(file_path, 'r', encoding='latin-1') as file:\n","                    return file.read()\n","            except Exception as e:\n","                print(f\"Error reading {file_path}: {e}\")\n","                return \"\"\n","    # when the file is .fson\n","    elif file_path.lower().endswith('.json'):\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                json_data = json.load(file)\n","                # For JSON files, we'll return both the raw text (for chunking) and the structured data\n","                # The structured data will be used directly in extract_paper_metadata\n","                text_content = json.dumps(json_data, indent=2)  # Convert to string for text chunking\n","                return {\"text\": text_content, \"json_data\": json_data}\n","        except Exception as e:\n","            print(f\"Error reading JSON {file_path}: {e}\")\n","            return \"\"\n","\n","    return \"\"\n","\n","def generate_paper_id(file_path, content_sample=None):\n","    \"\"\"Generate a unique paper ID based on file path and optional content sample\"\"\"\n","    base_name = os.path.basename(file_path)\n","\n","    # Create a hash from the filename and optionally first 100 chars of content\n","    if content_sample:\n","        hash_input = f\"{base_name}_{content_sample[:100]}\"\n","    else:\n","        hash_input = base_name\n","\n","    # Create a short hash as paper_id\n","    paper_id = hashlib.md5(hash_input.encode()).hexdigest()[:12]\n","    return paper_id\n","\n","def extract_paper_metadata(content, file_path):\n","    \"\"\"Extract metadata from paper content using pattern matching or direct JSON extraction\"\"\"\n","    # Check if we're dealing with JSON content (which would be a dict with 'text' and 'json_data')\n","    if isinstance(content, dict) and 'json_data' in content:\n","        # We have structured JSON data\n","        json_data = content['json_data']\n","        # Get text content for generating ID\n","        text_content = content['text']\n","\n","        # Create a basic metadata dict with file info\n","        base_name = os.path.basename(file_path)\n","        paper_id = generate_paper_id(file_path, text_content[:500])\n","\n","        # Initialize metadata with defaults\n","        metadata = {\n","            'paper_id': paper_id,\n","            'filename': base_name,\n","            'title': '',\n","            'authors': [],\n","            'year': '',\n","            'venue': '',\n","            'keywords': [],\n","            'abstract': ''\n","        }\n","\n","        # Map JSON fields to metadata fields - adjust these based on JSON structure\n","        # These are common field names in academic paper JSON metadata\n","        json_field_mapping = {\n","            'title': ['title', 'paper_title', 'articleTitle', 'name'],\n","            'authors': ['authors', 'author', 'creator', 'creators', 'contributors'],\n","            'year': ['year', 'date', 'publicationDate', 'publication_date', 'publishedAt'],\n","            'venue': ['venue', 'journal', 'conference', 'publication', 'publisher'],\n","            'keywords': ['keywords', 'tags', 'subjects', 'categories', 'topics'],\n","            'abstract': ['abstract', 'summary', 'description']\n","        }\n","\n","        # Extract metadata from JSON based on field mapping\n","        for meta_field, json_fields in json_field_mapping.items():\n","            for field in json_fields:\n","                if field in json_data:\n","                    value = json_data[field]\n","                    if value:\n","                        metadata[meta_field] = value\n","                        break\n","\n","        # Handle special cases and formatting\n","\n","        # Authors - ensure it's a list of strings\n","        if metadata['authors'] and not isinstance(metadata['authors'], list):\n","            # If authors is a string, try to split it\n","            if isinstance(metadata['authors'], str):\n","                metadata['authors'] = [a.strip() for a in re.split(r'[;,]|and', metadata['authors']) if a.strip()]\n","\n","        # Extract year from date string if year is missing but we have a date\n","        if not metadata['year'] and any(field in json_data for field in ['date', 'publicationDate', 'publication_date']):\n","            for date_field in ['date', 'publicationDate', 'publication_date']:\n","                if date_field in json_data and json_data[date_field]:\n","                    # Try to extract year from date string using regex\n","                    date_str = str(json_data[date_field])\n","                    year_match = re.search(r'(?:19|20)\\d{2}', date_str)\n","                    if year_match:\n","                        metadata['year'] = year_match.group(0)\n","                        break\n","\n","        # Keywords - ensure it's a list of strings\n","        if metadata['keywords'] and not isinstance(metadata['keywords'], list):\n","            if isinstance(metadata['keywords'], str):\n","                metadata['keywords'] = [k.strip().lower() for k in re.split(r'[;,]', metadata['keywords']) if k.strip()]\n","\n","        # If we have no keywords but have an abstract, extract keywords from abstract\n","        if not metadata['keywords'] and metadata['abstract']:\n","            try:\n","                # Initialize KeyBERT model\n","                kw_model = KeyBERT()\n","\n","                # Extract keywords and keyphrases\n","                # Use ngram_range to extract multi-word phrases (1,3 means 1-3 word phrases)\n","                # Use top_n to limit the number of keywords/phrases returned\n","                keywords = kw_model.extract_keywords(\n","                    metadata['abstract'],\n","                    keyphrase_ngram_range=(1, 3),\n","                    stop_words='english',\n","                    use_mmr=True,      # Use Maximal Marginal Relevance for diversity\n","                    diversity=0.7,     # Higher diversity means more diverse phrases\n","                    top_n=10           # Get top 10 keyphrases\n","                )\n","\n","                # Keywords will be a list of tuples (phrase, score)\n","                # Extract just the phrases\n","                metadata['keywords'] = [keyword[0] for keyword in keywords]\n","\n","            except ImportError:\n","                # Fallback to simpler extraction if KeyBERT is not available\n","                try:\n","                    import nltk\n","                    nltk.download('stopwords', quiet=True)\n","                    nltk.download('punkt', quiet=True)\n","                    from nltk.corpus import stopwords\n","                    from nltk.tokenize import word_tokenize\n","                    from nltk.util import ngrams\n","\n","                    stop_words = set(stopwords.words('english'))\n","                    words = word_tokenize(metadata['abstract'].lower())\n","                    filtered_words = [w for w in words if w.isalpha() and w not in stop_words and len(w) > 3]\n","\n","                    # Extract single words first\n","                    word_counts = Counter(filtered_words)\n","\n","                    # Extract bigrams and trigrams\n","                    bigrams_list = list(ngrams(filtered_words, 2))\n","                    trigrams_list = list(ngrams(filtered_words, 3))\n","\n","                    # Convert tuples to strings for counting\n","                    bigrams_strings = [' '.join(bigram) for bigram in bigrams_list]\n","                    trigrams_strings = [' '.join(trigram) for trigram in trigrams_list]\n","\n","                    # Count n-grams\n","                    bigram_counts = Counter(bigrams_strings)\n","                    trigram_counts = Counter(trigrams_strings)\n","\n","                    # Combine single words, bigrams and trigrams, prioritizing longer phrases\n","                    # This gives more weight to meaningful multi-word phrases\n","                    combined_keywords = []\n","\n","                    # Add top trigrams first\n","                    combined_keywords.extend([term for term, count in trigram_counts.most_common(3)])\n","\n","                    # Add top bigrams next\n","                    combined_keywords.extend([term for term, count in bigram_counts.most_common(3)])\n","\n","                    # Fill the rest with top single words\n","                    remaining_slots = 10 - len(combined_keywords)\n","                    combined_keywords.extend([term for term, count in word_counts.most_common(remaining_slots)])\n","\n","                    metadata['keywords'] = combined_keywords[:10]  # Limit to top 10\n","                except:\n","                    # Super simple fallback\n","                    words = re.findall(r'\\b[a-z]{4,}\\b', metadata['abstract'].lower())\n","                    word_counts = {}\n","                    for word in words:\n","                        if word not in ['with', 'that', 'this', 'from', 'have', 'were']:\n","                            word_counts[word] = word_counts.get(word, 0) + 1\n","                    metadata['keywords'] = [w for w, c in sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:5]]\n","\n","    else:\n","        # Regular text content processing for PDF/TXT files\n","        # Create a basic metadata dict with file info\n","        base_name = os.path.basename(file_path)\n","        paper_id = generate_paper_id(file_path, content[:500] if isinstance(content, str) else \"\")\n","\n","        metadata = {\n","            'paper_id': paper_id,\n","            'filename': base_name,\n","            'title': '',\n","            'authors': [],\n","            'year': '',\n","            'venue': '',\n","            'keywords': [],\n","            'abstract': ''\n","        }\n","\n","        # Skip if content is empty or not a string\n","        if not content or not isinstance(content, str):\n","            metadata['authors'] = 'Unknown'\n","            metadata['keywords'] = 'research'\n","            return metadata\n","\n","        # Extract title - usually at the beginning and often the largest text\n","        # Look for the first substantial line that's not a header\n","        title_candidates = re.findall(r'(?:\\n|^)([A-Z][^.\\n]{10,150})\\n', content[:2000])\n","        if title_candidates:\n","            metadata['title'] = title_candidates[0].strip()\n","        else:\n","            # Simpler approach - just take the first line with substantial text\n","            first_lines = content[:1000].split('\\n')\n","            for line in first_lines:\n","                line = line.strip()\n","                if len(line) > 15 and not line.lower().startswith(('http', 'www', 'proceedings')):\n","                    metadata['title'] = line\n","                    break\n","\n","        # Rest of the existing PDF/TXT extraction code...\n","        abstract_patterns = [\n","            # (?i) - Makes the pattern case-insensitive, so it will match \"Abstract\", \"ABSTRACT\", \"abstract\", etc.\n","            # abstract - Looks for the literal text \"abstract\"\n","            # [\\s\\.\\:]+ - Matches one or more whitespace characters, periods, or colons that might follow the word \"abstract\"\n","            # ([^\\n]+ - Starts capturing the abstract content, matching any characters except newlines\n","            # (?:\\n[^\\n]+){0,10}?) - A non-capturing group that allows the pattern to match up to 10 additional lines of text (each line started with a newline character)\n","            # (?:\\n\\n|\\n[A-Z][a-z]) - Stops capturing when it encounters either: A double newline (indicating a paragraph break) or A newline followed by a capitalized word (likely the start of a new section)\n","            r'(?i)abstract[\\s\\.\\:]+([^\\n]+(?:\\n[^\\n]+){0,10}?)(?:\\n\\n|\\n[A-Z][a-z])',\n","            r'(?i)abstract[\\s\\.\\:]+([^\\n]+(?:\\n[^\\n]+){0,10}?)(?:\\n\\s*keywords|\\n\\s*index terms)',\n","            r'(?i)abstract[\\s\\.\\:]+([^\\n]+(?:\\n[^\\n]+){0,10}?)(?:\\n\\s*introduction|\\n\\s*1\\.)'\n","        ]\n","\n","        for pattern in abstract_patterns:\n","            abstract_match = re.search(pattern, content[:5000], re.DOTALL)\n","            if abstract_match:\n","                metadata['abstract'] = abstract_match.group(1).strip()\n","                break\n","\n","        author_patterns = [\n","            r'(?i)(?:authors?|by)[\\s\\.\\:]+([^\\n]+)(?:\\n)',\n","            r'(?:^|\\n)([A-Z][^,\\n]+(?:,[^,\\n]+){1,6})(?:\\n)'\n","        ]\n","\n","        for pattern in author_patterns:\n","            authors_match = re.search(pattern, content[:2000])\n","            if authors_match:\n","                authors_text = authors_match.group(1).strip()\n","                # Clean and split authors\n","                authors = re.split(r'(?:,\\s*|;|and\\s+)', authors_text)\n","                metadata['authors'] = [a.strip() for a in authors if a.strip()]\n","                break\n","\n","        # Extract year - look for a 4-digit year in the first few pages\n","        year_match = re.search(r'(?:19|20)\\d{2}', content[:3000])\n","        if year_match:\n","            metadata['year'] = year_match.group(0)\n","\n","        # Extract keywords\n","        # (?i) - Case insensitive matching\n","        # (?:keywords|index terms) - Either \"keywords\" or \"index terms\" (non-capturing group)\n","        # [\\s\\.\\:]+ - Followed by whitespace, periods, or colons\n","        # ([^\\n]+ - Capturing group starts with any characters except newline\n","        # (?:\\n[^\\n]+){0,3}?) - Optionally followed by up to 3 lines of text\n","        # (?:\\n\\n|\\n[A-Z][a-z]) - Ending when either a double newline is found or a newline followed by a capitalized word (likely a new section)\n","        keyword_patterns = [\n","            r'(?i)(?:keywords|index terms)[\\s\\.\\:]+([^\\n]+(?:\\n[^\\n]+){0,3}?)(?:\\n\\n|\\n[A-Z][a-z])',\n","            r'(?i)(?:keywords|index terms)[\\s\\.\\:]+([^\\n]+(?:\\n[^\\n]+){0,3}?)(?:\\n\\s*introduction|\\n\\s*1\\.)'\n","        ]\n","\n","        for pattern in keyword_patterns:\n","            keywords_match = re.search(pattern, content[:5000], re.DOTALL)\n","            if keywords_match:\n","                keywords_text = keywords_match.group(1).strip()\n","                # Clean and split keywords\n","                keywords = re.split(r'(?:,\\s*|;|\\n)', keywords_text)\n","                metadata['keywords'] = [k.strip().lower() for k in keywords if k.strip()]\n","                break\n","\n","        # If we failed to extract meaningful keywords, try to extract from the abstract\n","        if not metadata['keywords'] and metadata['abstract']:\n","            import nltk\n","            try:\n","                nltk.download('stopwords', quiet=True)\n","                nltk.download('punkt', quiet=True)\n","                from nltk.corpus import stopwords\n","                from nltk.tokenize import word_tokenize\n","\n","                stop_words = set(stopwords.words('english'))\n","                words = word_tokenize(metadata['abstract'].lower())\n","                filtered_words = [w for w in words if w.isalpha() and w not in stop_words and len(w) > 3]\n","\n","                # Get word frequency\n","                word_counts = Counter(filtered_words)\n","\n","                # Extract top 5 words as keywords\n","                metadata['keywords'] = [word for word, count in word_counts.most_common(5)]\n","            except:\n","                # If NLTK fails, use a simple approach\n","                words = re.findall(r'\\b[a-z]{4,}\\b', metadata['abstract'].lower())\n","                word_counts = {}\n","                for word in words:\n","                    if word not in ['with', 'that', 'this', 'from', 'have', 'were']:\n","                        word_counts[word] = word_counts.get(word, 0) + 1\n","                metadata['keywords'] = [w for w, c in sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:5]]\n","\n","    # Convert lists to strings for Neo4j compatibility\n","    if metadata['authors']:\n","        if isinstance(metadata['authors'], list):\n","            metadata['authors'] = '; '.join(metadata['authors'])\n","    else:\n","        metadata['authors'] = 'Unknown'\n","\n","    if metadata['keywords']:\n","        if isinstance(metadata['keywords'], list):\n","            metadata['keywords'] = '; '.join(metadata['keywords'])\n","    else:\n","        metadata['keywords'] = 'research'\n","\n","    return metadata\n","\n"],"metadata":{"id":"BkLc1IeBZ6yL","executionInfo":{"status":"ok","timestamp":1749664331053,"user_tz":240,"elapsed":54,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"BkLc1IeBZ6yL","execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Function - Knowledge Graph Setup\n","\n","1. Setup knowledge graph schema\n","2. Create paper nod\n","3. Create chunk nodes\n","4. Create author nodes\n","5. Create keywork nodes\n"],"metadata":{"id":"5zBzzUqZao4p"},"id":"5zBzzUqZao4p"},{"cell_type":"code","source":["def setup_kg_schema():\n","    \"\"\"Create constraints and indexes in Neo4j\"\"\"\n","    # Create constraints to avoid duplicates\n","    kg.query(\"\"\"\n","    CREATE CONSTRAINT unique_paper IF NOT EXISTS\n","        FOR (p:Paper) REQUIRE p.paperID IS UNIQUE\n","    \"\"\")\n","\n","    kg.query(\"\"\"\n","    CREATE CONSTRAINT unique_chunk IF NOT EXISTS\n","        FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n","    \"\"\")\n","\n","    kg.query(\"\"\"\n","    CREATE CONSTRAINT unique_author IF NOT EXISTS\n","        FOR (a:Author) REQUIRE a.name IS UNIQUE\n","    \"\"\")\n","\n","    kg.query(\"\"\"\n","    CREATE CONSTRAINT unique_keyword IF NOT EXISTS\n","        FOR (k:Keyword) REQUIRE k.term IS UNIQUE\n","    \"\"\")\n","\n","    # Create vector index for semantic search\n","    kg.query(\"\"\"\n","    CREATE VECTOR INDEX research_paper_chunks IF NOT EXISTS\n","      FOR (c:Chunk) ON (c.textEmbedding)\n","      OPTIONS { indexConfig: {\n","        `vector.dimensions`: 1536,\n","        `vector.similarity_function`: 'cosine'\n","      }}\n","    \"\"\")\n","\n","    print(\"Knowledge graph schema setup complete!\")\n","\n","def create_paper_node(metadata):\n","    \"\"\"Create a Paper node in the graph\"\"\"\n","    cypher = \"\"\"\n","    MERGE (p:Paper {paperID: $paperParam.paper_id})\n","      ON CREATE\n","        SET p.title = $paperParam.title,\n","            p.year = $paperParam.year,\n","            p.venue = $paperParam.venue,\n","            p.abstract = $paperParam.abstract,\n","            p.filename = $paperParam.filename\n","    \"\"\"\n","    kg.query(cypher, params={'paperParam': metadata})\n","\n","def create_chunk_nodes(chunks_with_metadata):\n","    \"\"\"Create Chunk nodes and connect to Paper\"\"\"\n","    for chunk in chunks_with_metadata:\n","        # Create chunk node\n","        cypher = \"\"\"\n","        MERGE (c:Chunk {chunkId: $chunkParam.chunkId})\n","          ON CREATE\n","            SET c.text = $chunkParam.text,\n","                c.paperTitle = $chunkParam.paperTitle,\n","                c.sectionSeqId = $chunkParam.sectionSeqId,\n","                c.paperID = $chunkParam.paperID\n","        \"\"\"\n","        kg.query(cypher, params={'chunkParam': chunk})\n","\n","        # Connect chunk to paper\n","        cypher = \"\"\"\n","        MATCH (c:Chunk {chunkId: $chunkParam.chunkId}),\n","              (p:Paper {paperID: $chunkParam.paperID})\n","        MERGE (c)-[:PART_OF]->(p)\n","        \"\"\"\n","        kg.query(cypher, params={'chunkParam': chunk})\n","\n","def create_author_nodes(metadata):\n","    \"\"\"Create Author nodes and connect to Paper\"\"\"\n","    if isinstance(metadata['authors'], str):\n","        authors = [a.strip() for a in re.split(r'[;,]', metadata['authors']) if a.strip()]\n","    else:\n","        authors = metadata['authors']\n","\n","    for author in authors:\n","        if not author or author.lower() == 'unknown':\n","            continue\n","\n","        # Create author node\n","        cypher = \"\"\"\n","        MERGE (a:Author {name: $authorName})\n","        \"\"\"\n","        kg.query(cypher, params={'authorName': author})\n","\n","        # Connect author to paper\n","        cypher = \"\"\"\n","        MATCH (a:Author {name: $authorName}),\n","              (p:Paper {paperID: $paperID})\n","        MERGE (a)-[:AUTHORED]->(p)\n","        \"\"\"\n","        kg.query(cypher, params={'authorName': author, 'paperID': metadata['paper_id']})\n","\n","def create_keyword_nodes(metadata):\n","    \"\"\"Create Keyword nodes and connect to Paper\"\"\"\n","    if isinstance(metadata['keywords'], str):\n","        keywords = [k.strip().lower() for k in re.split(r'[;,]', metadata['keywords']) if k.strip()]\n","    else:\n","        keywords = metadata['keywords']\n","\n","    for keyword in keywords:\n","        if not keyword:\n","            continue\n","\n","        # Create keyword node\n","        cypher = \"\"\"\n","        MERGE (k:Keyword {term: $keywordTerm})\n","        \"\"\"\n","        kg.query(cypher, params={'keywordTerm': keyword})\n","\n","        # Connect keyword to paper\n","        cypher = \"\"\"\n","        MATCH (k:Keyword {term: $keywordTerm}),\n","              (p:Paper {paperID: $paperID})\n","        MERGE (p)-[:HAS_TOPIC]->(k)\n","        \"\"\"\n","        kg.query(cypher, params={'keywordTerm': keyword, 'paperID': metadata['paper_id']})\n","\n","# def create_chunk_sequence(paper_id):\n","#     \"\"\"Link chunks in sequence within the same paper\"\"\"\n","#     cypher = \"\"\"\n","#     MATCH (chunks:Chunk)\n","#     WHERE chunks.paperID = $paperID\n","#     WITH chunks ORDER BY chunks.sectionSeqId ASC\n","#     WITH collect(chunks) as paper_chunks\n","#     CALL apoc.nodes.link(paper_chunks, \"NEXT\", {avoidDuplicates: true})\n","#     RETURN size(paper_chunks)\n","#     \"\"\"\n","#     result = kg.query(cypher, params={'paperID': paper_id})\n","#     chunk_count = result[0] if result else 0\n","#     return chunk_count"],"metadata":{"id":"tqqO5gyPal9J","executionInfo":{"status":"ok","timestamp":1749664331111,"user_tz":240,"elapsed":56,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"tqqO5gyPal9J","execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Preprocess Papers"],"metadata":{"id":"1--JZ2Hla1HR"},"id":"1--JZ2Hla1HR"},{"cell_type":"markdown","source":["When the title of the paper is 'Not Known', we exclude such papers"],"metadata":{"id":"5pJ4prlGWF3F"},"id":"5pJ4prlGWF3F"},{"cell_type":"code","source":["# PAPERS_DIR = '/content/drive/MyDrive/Colab_Notebooks/AI/arxiv_markdowns2'\n","PAPERS_DIR = '/content/drive/MyDrive/Colab_Notebooks/AI/arxiv_json'\n","OUTPUT_DIR = '/content/drive/MyDrive/Colab_Notebooks/AI/filtered_json_files'\n","\n","# Clear the output directory first\n","if os.path.exists(OUTPUT_DIR):\n","    # Remove all files in the output directory\n","    for file in glob.glob(os.path.join(OUTPUT_DIR, \"*\")):\n","        os.remove(file)\n","else:\n","    # Create output directory if it doesn't exist\n","    os.makedirs(OUTPUT_DIR)\n","\n","json_files = glob.glob(os.path.join(PAPERS_DIR, \"*.json\"))\n","print(f\"Found {len(json_files)} JSON files in {PAPERS_DIR}\")\n","\n","valid_files_count = 0\n","invalid_files_count = 0\n","\n","# Process each JSON file\n","for file_path in json_files:\n","    try:\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","\n","        if 'title' in data and data['title'] != \"Not Found\":\n","            dest_path = os.path.join(OUTPUT_DIR, os.path.basename(file_path))\n","            shutil.copy2(file_path, dest_path)\n","            valid_files_count += 1\n","        else:\n","            invalid_files_count += 1\n","\n","    except json.JSONDecodeError:\n","        print(f\"Error: Could not decode JSON in file {file_path}\")\n","    except Exception as e:\n","        print(f\"Error processing file {file_path}: {str(e)}\")\n","\n","print(f\"Valid files copied: {valid_files_count}\")\n","print(f\"Invalid files excluded: {invalid_files_count}\")\n","\n","# Check what's in the output directory\n","paper_files = glob.glob(os.path.join(OUTPUT_DIR, \"*.json\"))\n","print(f\"Found {len(paper_files)} JSON files in {OUTPUT_DIR}\")\n","\n","for file in paper_files[:5]:\n","    print(f\" - {os.path.basename(file)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_aVfvTSiUUFK","executionInfo":{"status":"ok","timestamp":1749664343841,"user_tz":240,"elapsed":12731,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"7729cb17-d59c-4654-d001-88510a949795"},"id":"_aVfvTSiUUFK","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 197 JSON files in /content/drive/MyDrive/Colab_Notebooks/AI/arxiv_json\n","Valid files copied: 197\n","Invalid files excluded: 0\n","Found 197 JSON files in /content/drive/MyDrive/Colab_Notebooks/AI/filtered_json_files\n"," - 2211.02069v2.json\n"," - 2211.04715v1.json\n"," - 1911.09661v1.json\n"," - 2102.02503v1.json\n"," - 2210.10723v2.json\n"]}]},{"cell_type":"markdown","source":["### Function - Process paper and build knowledge graph"],"metadata":{"id":"jzpuwJCtWg3c"},"id":"jzpuwJCtWg3c"},{"cell_type":"code","source":["def process_papers(paper_files, batch_size=5):\n","    \"\"\"Process all papers and build the knowledge graph\"\"\"\n","    # Setup schema\n","    setup_kg_schema()\n","\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=2000,\n","        chunk_overlap=200,\n","        length_function=len,\n","        is_separator_regex=False,\n","    )\n","\n","    # Process papers in batches\n","    for batch_start in range(0, len(paper_files), batch_size):\n","        batch_end = min(batch_start + batch_size, len(paper_files))\n","        batch = paper_files[batch_start:batch_end]\n","\n","        print(f\"Processing batch {batch_start//batch_size + 1} ({batch_start+1}-{batch_end} of {len(paper_files)} papers)\")\n","\n","        # Process each paper in the batch\n","        for i, file_path in enumerate(batch):\n","            try:\n","                print(f\"  [{batch_start+i+1}/{len(paper_files)}] Processing: {os.path.basename(file_path)}\")\n","\n","                # Extract text content\n","                content = extract_text_from_file(file_path)\n","                if not content:\n","                    print(f\"    Skipping - no content extracted\")\n","                    continue\n","\n","                # Extract metadata from content\n","                metadata = extract_paper_metadata(content, file_path)\n","                print(f\"    Title: {metadata['title'][:50] + '...' if len(metadata['title']) > 50 else metadata['title']}\")\n","                print(f\"    Authors: {metadata['authors'][:50] + '...' if len(metadata['authors']) > 50 else metadata['authors']}\")\n","\n","                # Get text for chunking (handle both string and dict with 'text' key)\n","                text_for_chunking = content['text'] if isinstance(content, dict) and 'text' in content else content\n","\n","                # Split into chunks\n","                chunks = text_splitter.split_text(text_for_chunking)\n","                print(f\"    Split into {len(chunks)} chunks\")\n","\n","                # Create paper node\n","                create_paper_node(metadata)\n","\n","                # Create chunks with metadata\n","                chunks_with_metadata = []\n","                for j, chunk in enumerate(chunks):\n","                    chunks_with_metadata.append({\n","                        'text': chunk,\n","                        'paperTitle': metadata['title'],\n","                        'sectionSeqId': j,\n","                        'paperID': metadata['paper_id'],\n","                        'chunkId': f\"{metadata['paper_id']}_chunk_{j:04d}\"\n","                    })\n","\n","                # Create nodes and relationships\n","                create_chunk_nodes(chunks_with_metadata)\n","                create_author_nodes(metadata)\n","                create_keyword_nodes(metadata)\n","                # sequence_count = create_chunk_sequence(metadata['paper_id'])\n","                # print(f\"    Created sequence of {sequence_count} chunks\")\n","\n","            except Exception as e:\n","                print(f\"    Error processing paper {file_path}: {e}\")\n","\n","        print(f\"Batch {batch_start//batch_size + 1} completed.\")\n","\n","    print(\"Paper processing complete!\")\n"],"metadata":{"id":"GnSyThnlal11","executionInfo":{"status":"ok","timestamp":1749664343868,"user_tz":240,"elapsed":7,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"GnSyThnlal11","execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### Function - Vector embedding"],"metadata":{"id":"XrHmRo_Ca-Le"},"id":"XrHmRo_Ca-Le"},{"cell_type":"code","source":["def generate_embeddings(batch_size=20):\n","    \"\"\"Generate and store embeddings for chunks in batches\"\"\"\n","    embedding_model = OpenAIEmbeddings(api_key=os.environ[\"OPENAI_API_KEY\"])\n","\n","    # Get chunks without embeddings\n","    cypher = \"\"\"\n","    MATCH (chunk:Chunk)\n","    WHERE chunk.textEmbedding IS NULL\n","    RETURN chunk.chunkId AS id, chunk.text AS text\n","    LIMIT $batchSize\n","    \"\"\"\n","    chunks = kg.query(cypher, params={'batchSize': batch_size})\n","\n","    if not chunks:\n","        print(\"No chunks need embeddings!\")\n","        return 0\n","\n","    print(f\"Generating embeddings for {len(chunks)} chunks...\")\n","    processed = 0\n","\n","    for chunk in chunks:\n","        try:\n","            # Generate embedding\n","            embedding = embedding_model.embed_query(chunk['text'])\n","\n","            # Update the chunk with embedding\n","            update_query = \"\"\"\n","            MATCH (c:Chunk {chunkId: $id})\n","            SET c.textEmbedding = $embedding\n","            \"\"\"\n","            kg.query(update_query, params={'id': chunk['id'], 'embedding': embedding})\n","            processed += 1\n","\n","            # Show progress\n","            if processed % 5 == 0:\n","                print(f\"  Processed {processed}/{len(chunks)} chunks\")\n","\n","        except Exception as e:\n","            print(f\"  Error generating embedding for chunk {chunk['id']}: {str(e)[:100]}...\")\n","\n","    # Check if there are more to process\n","    remaining = kg.query(\"\"\"\n","    MATCH (chunk:Chunk)\n","    WHERE chunk.textEmbedding IS NULL\n","    RETURN count(chunk) as count\n","    \"\"\")[0]['count']\n","\n","    print(f\"Processed batch of {processed} chunks. {remaining} chunks still need embeddings.\")\n","    return remaining"],"metadata":{"id":"AI_-dYxzalzf","executionInfo":{"status":"ok","timestamp":1749664343881,"user_tz":240,"elapsed":12,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"AI_-dYxzalzf","execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### Function - QA Interface"],"metadata":{"id":"VB5BpOkPbEoi"},"id":"VB5BpOkPbEoi"},{"cell_type":"code","source":["def setup_qa_system():\n","    \"\"\"Set up a QA system for semantic search\"\"\"\n","    # Define the retrieval query with context\n","    retrieval_query = \"\"\"\n","    MATCH (node)-[:PART_OF]->(p:Paper)\n","    OPTIONAL MATCH (p)<-[:AUTHORED]-(author:Author)\n","    OPTIONAL MATCH (p)-[:HAS_TOPIC]->(keyword:Keyword)\n","    WITH node, score, p,\n","         collect(distinct author.name) as authors,\n","         collect(distinct keyword.term) as keywords\n","    RETURN \"Paper: \" + p.title +\n","           CASE WHEN p.year <> 'Unknown' THEN \" (\" + p.year + \")\" ELSE \"\" END +\n","           \"\\nAuthors: \" + CASE WHEN size(authors) > 0 THEN apoc.text.join(authors, \", \") ELSE \"Unknown\" END +\n","           \"\\nKeywords: \" + CASE WHEN size(keywords) > 0 THEN apoc.text.join(keywords, \", \") ELSE \"None\" END +\n","           CASE WHEN p.abstract <> '' THEN \"\\nAbstract: \" + left(p.abstract, 200) + \"...\" ELSE \"\" END +\n","           \"\\n\\n\" + node.text AS text,\n","        score,\n","        { source: p.filename, title: p.title, paperID: p.paperID} AS metadata\n","    \"\"\"\n","\n","    # Create vector store\n","    vector_store = Neo4jVector.from_existing_index(\n","        embedding=OpenAIEmbeddings(api_key=os.environ[\"OPENAI_API_KEY\"]),\n","        url=os.environ[\"NEO4J_URI\"],\n","        username=os.environ[\"NEO4J_USERNAME\"],\n","        password=os.environ[\"NEO4J_PASSWORD\"],\n","        index_name=\"research_paper_chunks\",\n","        text_node_property=\"text\",\n","        retrieval_query=retrieval_query,\n","    )\n","\n","    # Create retriever and QA chain\n","    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n","\n","    qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n","        ChatOpenAI(temperature=0, api_key=os.environ[\"OPENAI_API_KEY\"]),\n","        chain_type=\"stuff\",\n","        retriever=retriever\n","    )\n","\n","    return qa_chain"],"metadata":{"id":"6rJGEdmUalu3","executionInfo":{"status":"ok","timestamp":1749664343898,"user_tz":240,"elapsed":16,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"6rJGEdmUalu3","execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### Function - Analysis\n","1. Find clusters of papers based on shared keywords\n","2. Find papers that share topics\n","3. Get statistics about the knowledge graph"],"metadata":{"id":"k5qziz37bJbm"},"id":"k5qziz37bJbm"},{"cell_type":"code","source":["def analyze_research_clusters():\n","    \"\"\"Find clusters of papers based on shared keywords\"\"\"\n","    cypher = \"\"\"\n","    MATCH (p:Paper)-[:HAS_TOPIC]->(k:Keyword)\n","    WITH k, collect(p) AS papers\n","    WHERE size(papers) > 1\n","    RETURN k.term AS topic,\n","           [p IN papers | p.title] AS paperTitles,\n","           size(papers) AS clusterSize\n","    ORDER BY clusterSize DESC\n","    LIMIT 10\n","    \"\"\"\n","    clusters = kg.query(cypher)\n","\n","    print(\"\\nResearch clusters by topic:\")\n","    for cluster in clusters:\n","        print(f\"\\nTopic: {cluster['topic']} ({cluster['clusterSize']} papers)\")\n","        for i, title in enumerate(cluster['paperTitles']):\n","            print(f\"  {i+1}. {title}\")\n","\n","    return clusters\n","\n","def find_paper_connections():\n","    \"\"\"Find papers that share topics\"\"\"\n","    cypher = \"\"\"\n","    MATCH (p1:Paper)-[:HAS_TOPIC]->(k:Keyword)<-[:HAS_TOPIC]-(p2:Paper)\n","    WHERE p1.paperID < p2.paperID  // To avoid duplicate pairs\n","    WITH p1, p2, collect(k.term) as shared_topics\n","    RETURN p1.title AS paper1,\n","           p2.title AS paper2,\n","           shared_topics,\n","           size(shared_topics) AS connection_strength\n","    ORDER BY connection_strength DESC\n","    LIMIT 15\n","    \"\"\"\n","    connections = kg.query(cypher)\n","\n","    print(\"\\nPaper connections through shared topics:\")\n","    for conn in connections:\n","        print(f\"\\n{conn['paper1']} <--> {conn['paper2']}\")\n","        print(f\"  Shared topics ({conn['connection_strength']}): {', '.join(conn['shared_topics'])}\")\n","\n","    return connections\n","\n","def get_kg_statistics():\n","    \"\"\"Get statistics about the knowledge graph\"\"\"\n","    stats = {}\n","\n","    # Count papers\n","    stats['paper_count'] = kg.query(\"\"\"\n","    MATCH (p:Paper)\n","    RETURN count(p) as count\n","    \"\"\")[0]['count']\n","\n","    # Count chunks\n","    stats['chunk_count'] = kg.query(\"\"\"\n","    MATCH (c:Chunk)\n","    RETURN count(c) as count\n","    \"\"\")[0]['count']\n","\n","    # Count authors\n","    stats['author_count'] = kg.query(\"\"\"\n","    MATCH (a:Author)\n","    RETURN count(a) as count\n","    \"\"\")[0]['count']\n","\n","    # Count keywords/topics\n","    stats['keyword_count'] = kg.query(\"\"\"\n","    MATCH (k:Keyword)\n","    RETURN count(k) as count\n","    \"\"\")[0]['count']\n","\n","    # Count relationships\n","    stats['relationship_count'] = kg.query(\"\"\"\n","    MATCH ()-[r]->()\n","    RETURN count(r) as count\n","    \"\"\")[0]['count']\n","\n","    print(\"\\nKnowledge Graph Statistics:\")\n","    print(f\"Papers: {stats['paper_count']}\")\n","    print(f\"Chunks: {stats['chunk_count']}\")\n","    print(f\"Authors: {stats['author_count']}\")\n","    print(f\"Keywords/Topics: {stats['keyword_count']}\")\n","    print(f\"Relationships: {stats['relationship_count']}\")\n","\n","    return stats"],"metadata":{"id":"H44pq0M7all-","executionInfo":{"status":"ok","timestamp":1749664343929,"user_tz":240,"elapsed":22,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"H44pq0M7all-","execution_count":15,"outputs":[]},{"cell_type":"code","source":["def ask_research_question(qa_chain, question):\n","    \"\"\"Ask a question about the research papers\"\"\"\n","    print(f\"\\nQuestion: {question}\\n\")\n","    try:\n","        answer = qa_chain({\"question\": question}, return_only_outputs=True)\n","        print(f\"Answer: {answer['answer']}\\n\")\n","        print(f\"Sources: {answer['sources']}\")\n","        return answer\n","    except Exception as e:\n","        print(f\"Error processing question: {e}\")\n","        return None\n"],"metadata":{"id":"DVekt4ppbQnh","executionInfo":{"status":"ok","timestamp":1749664343955,"user_tz":240,"elapsed":27,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"DVekt4ppbQnh","execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Ask questions\n","\n","**Below cell takes several hours when running for the frist time, so we will use the resue_existing flag**.\n","\n","- When running for the first time, we set the resue_existing as False, so it will build the knowledge graph from scratch.\n","- Later, we can set the reuse_existing as True. so that we can reuse the existing knowledge graph."],"metadata":{"id":"FTx-2xD2wbxN"},"id":"FTx-2xD2wbxN"},{"cell_type":"code","source":["# Suppress the CropBox warnings\n","logging.getLogger('pdfminer').setLevel(logging.ERROR)\n","\n","# We set a flag reuse_existing: True to reuse existing data, False to build from scratch\n","reuse_existing = True\n","\n","def execute_with_retry(query_func, max_retries=3, retry_delay=5):\n","    \"\"\"Execute a database operation with retry logic\"\"\"\n","    retries = 0\n","    while retries < max_retries:\n","        try:\n","            return query_func()\n","        except (ServiceUnavailable, IncompleteCommit, OSError) as e:\n","            retries += 1\n","            if retries == max_retries:\n","                raise\n","            print(f\"Connection error: {e}. Retrying in {retry_delay} seconds... (Attempt {retries}/{max_retries})\")\n","            time.sleep(retry_delay)\n","\n","# Check if we already have data in the graph\n","try:\n","    existing_papers = execute_with_retry(\n","        lambda: kg.query(\"\"\"\n","            MATCH (p:Paper)\n","            RETURN count(p) as count\n","        \"\"\")[0]['count']\n","    )\n","    print(f\"Found {existing_papers} existing papers in the knowledge graph.\")\n","\n","    if reuse_existing and existing_papers > 0:\n","        print(\"Reusing existing knowledge graph data.\")\n","    else:\n","        # If not reusing or no existing data, build from scratch\n","        if existing_papers > 0:\n","            print(\"Clearing existing data from the knowledge graph...\")\n","            # Break down the delete operation into smaller batches to avoid timeout\n","            def clear_graph():\n","                # First delete relationships in batches\n","                while True:\n","                    result = kg.query(\"\"\"\n","                        MATCH ()-[r]->()\n","                        WITH r LIMIT 5000\n","                        DELETE r\n","                        RETURN count(r) as deleted\n","                    \"\"\")\n","                    deleted = result[0]['deleted']\n","                    print(f\"Deleted {deleted} relationships...\")\n","                    if deleted == 0:\n","                        break\n","\n","                # Then delete nodes in batches\n","                while True:\n","                    result = kg.query(\"\"\"\n","                        MATCH (n)\n","                        WITH n LIMIT 5000\n","                        DELETE n\n","                        RETURN count(n) as deleted\n","                    \"\"\")\n","                    deleted = result[0]['deleted']\n","                    print(f\"Deleted {deleted} nodes...\")\n","                    if deleted == 0:\n","                        break\n","                return True\n","\n","            execute_with_retry(clear_graph)\n","            print(\"Knowledge graph cleared.\")\n","\n","        print(\"Building new knowledge graph...\")\n","        execute_with_retry(lambda: process_papers(paper_files, batch_size=5))\n","\n","    print(\"Generating embeddings...\")\n","    remaining = 1\n","    while remaining > 0:\n","        remaining = execute_with_retry(lambda: generate_embeddings(batch_size=20))\n","    print(\"Embedding generation complete.\")\n","\n","    # Display knowledge graph statistics\n","    stats = execute_with_retry(get_kg_statistics)\n","    print(f\"Knowledge graph statistics: {stats}\")\n","\n","    print(\"Setting up QA system...\")\n","    qa_chain = execute_with_retry(setup_qa_system)\n","\n","    print(\"\\nAnalyzing research clusters...\")\n","    execute_with_retry(analyze_research_clusters)\n","\n","    print(\"\\nFinding paper connections...\")\n","    execute_with_retry(find_paper_connections)\n","\n","    print(\"\\nReady to answer questions about the research papers!\")\n","\n","except Exception as e:\n","    print(f\"Error: {e}\")\n","    print(\"Tip: If you're experiencing connection issues, try the following:\")\n","    print(\"1. Check your Neo4j database connection settings\")\n","    print(\"2. Make sure your Neo4j instance has enough memory\")\n","    print(\"3. Break large operations into smaller batches\")\n","    print(\"4. Check if your cloud provider (if using one) is experiencing issues\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CdvF-rgowbf","executionInfo":{"status":"ok","timestamp":1749664350036,"user_tz":240,"elapsed":6095,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"6b4df719-9768-4987-bb36-3771ec5cebac"},"id":"0CdvF-rgowbf","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 197 existing papers in the knowledge graph.\n","Reusing existing knowledge graph data.\n","Generating embeddings...\n","No chunks need embeddings!\n","Embedding generation complete.\n","\n","Knowledge Graph Statistics:\n","Papers: 197\n","Chunks: 9731\n","Authors: 1951\n","Keywords/Topics: 1841\n","Relationships: 14005\n","Knowledge graph statistics: {'paper_count': 197, 'chunk_count': 9731, 'author_count': 1951, 'keyword_count': 1841, 'relationship_count': 14005}\n","Setting up QA system...\n","\n","Analyzing research clusters...\n","\n","Research clusters by topic:\n","\n","Topic: large language models (44 papers)\n","  1. Enhancing Advanced Visual Reasoning Ability of Large Language Models\n","  2. The structure of the token space for large language models\n","  3. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n","  4. Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)\n","  5. Selection Bias Induced Spurious Correlations in Large Language Models\n","  6. ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design\n","  7. Algorithmic Ghost in the Research Shell: Large Language Models and Academic Knowledge Creation in Management Research\n","  8. Large Language Models for Business Process Management: Opportunities and Challenges â‹†\n","  9. M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models\n","  10. Large Language Models\n","  11. TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety\n","  12. Leveraging Large Language Models (LLMs) for Process Mining (\n","  13. Addressing Compiler Errors: Stack Overflow or Large Language Models?\n","  14. Does Correction Remain A Problem For Large Language Models?\n","  15. ETHICAL CONSIDERATIONS AND POLICY IMPLICATIONS FOR LARGE LANGUAGE MODELS: GUIDING RESPONSIBLE DEVELOPMENT AND DEPLOYMENT\n","  16. Performance of Large Language Models in a Computer Science Degree Program\n","  17. LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models\n","  18. Response: Emergent analogical reasoning in large language models\n","  19. Why do universal adversarial attacks work on large language models?: Geometry might be the answer\n","  20. Do Generative Large Language Models need billions of parameters?\n","  21. MaaSDB: Spatial Databases in the Era of Large Language Models (Vision Paper)\n","  22. Knowledge Sanitization of Large Language Models\n","  23. Watermarking LLMs with Weight Quantization\n","  24. An Open-Source Data Contamination Report for Large Language Models\n","  25. A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4\n","  26. Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles\n","  27. METACOGNITION-ENHANCED FEW-SHOT PROMPTING WITH POSITIVE REINFORCEMENT\n","  28. An Assessment on Comprehending Mental Health through Large Language Models\n","  29. An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets\n","  30. Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval\n","  31. Measuring Social Norms of Large Language Models\n","  32. AI for Biomedicine in the Era of Large Language Models\n","  33. From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples\n","  34. Position Engineering: Boosting Large Language Models through Positional Information Manipulation\n","  35. KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering\n","  36. Large Language Models Are Biased Because They Are Large Language Models\n","  37. A Comparative Analysis of Distributed Training Strategies for GPT-2\n","  38. Ranking LLMs by compression\n","  39. DNAHLM -DNA sequence and Human Language mixed large language Model\n","  40. Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback\n","  41. Evaluation of Large Language Models via Coupled Token Generation\n","  42. Bactrainus: Optimizing Large Language Models for Multihop Complex Question Answering Tasks\n","  43. VeCoGen: Automating Generation of Formally Verified C Code with Large Language Models\n","  44. Still \"Talking About Large Language Models\": Some Clarifications\n","\n","Topic: large (10 papers)\n","  1. Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)\n","  2. Very Large Language Model as a Unified Methodology of Text Mining\n","  3. Leveraging Large Language Model and Story-Based Gamification in Intelligent Tutoring System to Scaffold Introductory Programming Courses: A Design-Based Research Study\n","  4. Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics\n","  5. Getting More out of Large Language Models for Proofs\n","  6. Garbage in, garbage out: Zero-shot detection of crime using Large Language Models\n","  7. ETHICAL CONSIDERATIONS AND POLICY IMPLICATIONS FOR LARGE LANGUAGE MODELS: GUIDING RESPONSIBLE DEVELOPMENT AND DEPLOYMENT\n","  8. Probing the \"Creativity\" of Large Language Models: Can Models Produce Divergent Semantic Association?\n","  9. Evaluation Ethics of LLMs in Legal Domain\n","  10. Can Large Language Models Predict Antimicrobial Resistance Gene?\n","\n","Topic: large language (9 papers)\n","  1. Turning large language models into cognitive models\n","  2. Revisiting Automated Topic Model Evaluation with Large Language Models\n","  3. Trustworthiness of Children Stories Generated by Large Language Models\n","  4. Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering\n","  5. The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models\n","  6. How to Protect Copyright Data in Optimization of Large Language Models?\n","  7. Knowledge Engineering using Large Language Models\n","  8. RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS\n","  9. Darkit: A User-Friendly Software Toolkit for Spiking Large Language Model\n","\n","Topic: large language model (8 papers)\n","  1. Very Large Language Model as a Unified Methodology of Text Mining\n","  2. Fauno: The Italian Large Language Model that will leave you senza parole!\n","  3. Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model\n","  4. RETHINKING MEMORY AND COMMUNICATION COSTS FOR EFFICIENT LARGE LANGUAGE MODEL TRAINING\n","  5. Assessing Translation capabilities of Large Language Models involving English and Indian Languages\n","  6. Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis\n","  7. LLaMA-Reg: Using LLaMA 2 for Unsupervised Medical Image Registration\n","  8. Large Language Models for Judicial Entity Extraction: A Comparative Study\n","\n","Topic: language models llms (5 papers)\n","  1. The (ab)use of Open Source Code to Train Large Language Models\n","  2. Industrial Engineering with Large Language Models: A case study of ChatGPT's performance on Oil & Gas problems\n","  3. Visualization in the Era of Artificial Intelligence: Experiments for Creating Structural Visualizations by Prompting Large Language Models\n","  4. Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations\n","  5. A Case-Based Persistent Memory for a Large Language Model â‹†\n","\n","Topic: including (4 papers)\n","  1. How well do Large Language Models perform in Arithmetic tasks?\n","  2. Evaluating Large Language Models on Controlled Generation Tasks\n","  3. Large Language Model-Enabled Multi-Agent Manufacturing Systems\n","  4. Control Flow-Augmented Decompiler based on Large Language Model\n","\n","Topic: state (4 papers)\n","  1. Sentence Simplification via Large Language Models\n","  2. How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?\n","  3. Agent Instructs Large Language Models to be General Zero-Shot Reasoners\n","  4. Exploring Large Language Models for Hate Speech Detection in Rioplatense Spanish\n","\n","Topic: language (4 papers)\n","  1. FOODGPT: A LARGE LANGUAGE MODEL IN FOOD TESTING DOMAIN WITH INCREMENTAL PRE-TRAINING AND KNOWLEDGE GRAPH PROMPT\n","  2. Towards Automatic Support of Software Model Evolution with Large Language Models\n","  3. Assessing \"Implicit\" Retrieval Robustness of Large Language Models\n","  4. A Survey on LLM-based News Recommender Systems\n","\n","Topic: gpt (3 papers)\n","  1. Ontology engineering with Large Language Models\n","  2. Why do universal adversarial attacks work on large language models?: Geometry might be the answer\n","  3. Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models\n","\n","Topic: language models (3 papers)\n","  1. Emergent Analogical Reasoning in Large Language Models\n","  2. Getting More out of Large Language Models for Proofs\n","  3. scInterpreter: Training Large Language Models to Interpret scRNA-seq Data for Cell Type Annotation\n","\n","Finding paper connections...\n","\n","Paper connections through shared topics:\n","\n","Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract) <--> ETHICAL CONSIDERATIONS AND POLICY IMPLICATIONS FOR LARGE LANGUAGE MODELS: GUIDING RESPONSIBLE DEVELOPMENT AND DEPLOYMENT\n","  Shared topics (2): large language models, large\n","\n","Knowledge Engineering using Large Language Models <--> How to Protect Copyright Data in Optimization of Large Language Models?\n","  Shared topics (2): large language, llms\n","\n","Revisiting Automated Topic Model Evaluation with Large Language Models <--> Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering\n","  Shared topics (1): large language\n","\n","Revisiting Automated Topic Model Evaluation with Large Language Models <--> How to Protect Copyright Data in Optimization of Large Language Models?\n","  Shared topics (1): large language\n","\n","Revisiting Automated Topic Model Evaluation with Large Language Models <--> Knowledge Engineering using Large Language Models\n","  Shared topics (1): large language\n","\n","Revisiting Automated Topic Model Evaluation with Large Language Models <--> RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS\n","  Shared topics (1): large language\n","\n","ChatCoder: Chat-based Refine Requirement Improves LLMs' Code Generation <--> Bootstrapping Cognitive Agents with a Large Language Model\n","  Shared topics (1): good\n","\n","Revisiting Automated Topic Model Evaluation with Large Language Models <--> Darkit: A User-Friendly Software Toolkit for Spiking Large Language Model\n","  Shared topics (1): large language\n","\n","Helping Language Models Learn More: Multi-dimensional Task Prompt for Few-shot Tuning* <--> A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models\n","  Shared topics (1): best\n","\n","Using Large Language Models to Enhance Programming Error Messages <--> Large Language Models as Zero-Shot Conversational Recommenders\n","  Shared topics (1): generation\n","\n","Can Large Language Models Predict Antimicrobial Resistance Gene? <--> Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)\n","  Shared topics (1): large\n","\n","Can Large Language Models Predict Antimicrobial Resistance Gene? <--> Very Large Language Model as a Unified Methodology of Text Mining\n","  Shared topics (1): large\n","\n","Can Large Language Models Predict Antimicrobial Resistance Gene? <--> Leveraging Large Language Model and Story-Based Gamification in Intelligent Tutoring System to Scaffold Introductory Programming Courses: A Design-Based Research Study\n","  Shared topics (1): large\n","\n","Can Large Language Models Predict Antimicrobial Resistance Gene? <--> Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics\n","  Shared topics (1): large\n","\n","Revisiting Automated Topic Model Evaluation with Large Language Models <--> The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models\n","  Shared topics (1): large language\n","\n","Ready to answer questions about the research papers!\n"]}]},{"cell_type":"code","source":["ask_research_question(qa_chain, \"What methodologies are commonly used in these papers?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfWdoqANNJLw","executionInfo":{"status":"ok","timestamp":1749664353652,"user_tz":240,"elapsed":3611,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"246df51c-0b8b-4212-92ee-18e178e67466"},"id":"pfWdoqANNJLw","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Question: What methodologies are commonly used in these papers?\n","\n","Answer: The methodologies commonly used in these papers include natural language processing techniques, retrieval-augmented generation, vision-language modeling, alignment of radiology reports, and large language models for news recommender systems.\n","\n","\n","Sources: 2411.01195v1.json, 2408.12141v1.json, 2310.12321v1.json, 2502.09797v2.json, 2411.18583v1.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'answer': 'The methodologies commonly used in these papers include natural language processing techniques, retrieval-augmented generation, vision-language modeling, alignment of radiology reports, and large language models for news recommender systems.\\n',\n"," 'sources': '2411.01195v1.json, 2408.12141v1.json, 2310.12321v1.json, 2502.09797v2.json, 2411.18583v1.json'}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["ask_research_question(qa_chain, \"Summarize the key findings across these papers.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHcqu3T-NWI4","executionInfo":{"status":"ok","timestamp":1749664356493,"user_tz":240,"elapsed":2835,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"3305c17a-c6a9-46a2-ad67-7616943035eb"},"id":"wHcqu3T-NWI4","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Question: Summarize the key findings across these papers.\n","\n","Answer: Key findings across these papers include the use of Natural Language Processing techniques and retrieval-augmented generation to automate literature reviews, the improvement of zero-shot reasoning abilities of large language models, the observation of biases in case judgment summaries generated by legal datasets and large language models, and the importance of transfer learning for finetuning large language models.\n","\n","\n","Sources: 2411.18583v1.json, 2310.03710v2.json, 2312.00554v1.json, 2411.01195v1.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'answer': 'Key findings across these papers include the use of Natural Language Processing techniques and retrieval-augmented generation to automate literature reviews, the improvement of zero-shot reasoning abilities of large language models, the observation of biases in case judgment summaries generated by legal datasets and large language models, and the importance of transfer learning for finetuning large language models.\\n',\n"," 'sources': '2411.18583v1.json, 2310.03710v2.json, 2312.00554v1.json, 2411.01195v1.json'}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["ask_research_question(qa_chain, \"What are the most influential authors in this field?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-1BFPsjNffG","executionInfo":{"status":"ok","timestamp":1749664358718,"user_tz":240,"elapsed":2227,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"76c3bda2-cd44-430c-bbe9-8649e66e1578"},"id":"u-1BFPsjNffG","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Question: What are the most influential authors in this field?\n","\n","Answer: The most influential authors in this field are Gerd Gigerenzer, Henry Brighton, E Gold, Mark, Alison Gopnik, Henry M Wellman, Dirk Groeneveld, Iz Beltagy, Pete Walsh, and others.\n","\n","\n","Sources: 2406.13138v2.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'answer': 'The most influential authors in this field are Gerd Gigerenzer, Henry Brighton, E Gold, Mark, Alison Gopnik, Henry M Wellman, Dirk Groeneveld, Iz Beltagy, Pete Walsh, and others.\\n',\n"," 'sources': '2406.13138v2.json'}"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["ask_research_question(qa_chain, \"What are the limitations mentioned in these papers?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYsYZ_PUNnmO","executionInfo":{"status":"ok","timestamp":1749664360385,"user_tz":240,"elapsed":1666,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"f0e49a4e-88dc-4c47-cef2-d4dcf55c9237"},"id":"KYsYZ_PUNnmO","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Question: What are the limitations mentioned in these papers?\n","\n","Answer: The limitations mentioned in these papers include the dynamic nature of code leading to changes over time, lack of transparency in training methodologies, and non-reproducible papers due to companies not being transparent about their training methods.\n","\n","\n","Sources: 2411.01195v1.json, 2403.15230v1.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'answer': 'The limitations mentioned in these papers include the dynamic nature of code leading to changes over time, lack of transparency in training methodologies, and non-reproducible papers due to companies not being transparent about their training methods.\\n',\n"," 'sources': '2411.01195v1.json, 2403.15230v1.json'}"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["ask_research_question(qa_chain, \"What future research directions are suggested?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9hHmIyRNq_n","executionInfo":{"status":"ok","timestamp":1749664361809,"user_tz":240,"elapsed":1421,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"c88fce43-f36b-4304-d84b-ec6414269468"},"id":"m9hHmIyRNq_n","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Question: What future research directions are suggested?\n","\n","Answer: Future research directions suggested include working on developing research directions beyond the scope of the current paper, exploring the impact of large language models on how work is carried out, and investigating the potential impacts of large language models on various fields such as business process management and robotics education.\n","\n","\n","Sources: 2304.04309v1.json, 2402.06116v1.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'answer': 'Future research directions suggested include working on developing research directions beyond the scope of the current paper, exploring the impact of large language models on how work is carried out, and investigating the potential impacts of large language models on various fields such as business process management and robotics education.\\n',\n"," 'sources': '2304.04309v1.json, 2402.06116v1.json'}"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["ask_research_question(qa_chain, \"What large language model are used in these papers\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6W3ADk9gNwMF","executionInfo":{"status":"ok","timestamp":1749664364634,"user_tz":240,"elapsed":2810,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"21ce31b6-d892-4dab-a86b-86b9c07d113e"},"id":"6W3ADk9gNwMF","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Question: What large language model are used in these papers\n","\n","Answer: The large language models used in these papers include GPT-2, OpenAI's GPT series, and other models mentioned in the references.\n","\n","\n","Sources: 2405.15628v1.json, 2503.01887v1.json, 2310.03710v2.json, 2307.05782v2.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'answer': \"The large language models used in these papers include GPT-2, OpenAI's GPT series, and other models mentioned in the references.\\n\",\n"," 'sources': '2405.15628v1.json, 2503.01887v1.json, 2310.03710v2.json, 2307.05782v2.json'}"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["# ğŸ’¡ Ask you own question!"],"metadata":{"id":"Uy2yhrc5JkvG"},"id":"Uy2yhrc5JkvG"},{"cell_type":"code","source":["# ask_research_question(qa_chain, \"Your question\")"],"metadata":{"id":"fX8WD1qoJr5_","executionInfo":{"status":"ok","timestamp":1749664364651,"user_tz":240,"elapsed":16,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"fX8WD1qoJr5_","execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["### Visualize the knowledge graph"],"metadata":{"id":"FIUJceHtOOQh"},"id":"FIUJceHtOOQh"},{"cell_type":"code","source":["# Visualize the knowledge graph\n","from neo4j import GraphDatabase\n","from yfiles_jupyter_graphs import GraphWidget\n","\n","from google.colab import output\n","output.enable_custom_widget_manager()"],"metadata":{"id":"VbdmY83VGu0U","executionInfo":{"status":"ok","timestamp":1749664364666,"user_tz":240,"elapsed":1,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"VbdmY83VGu0U","execution_count":25,"outputs":[]},{"cell_type":"code","source":["# # Below default_cypther looks for:\n","# # (n) - A node (any node) which we're calling \"n\"\n","# # -[r]-> - A relationship (any relationship) which we're calling \"r\", with the arrow indicating direction\n","# # (m) - Another node which we're calling \"m\"\n","# # RETURN n,r,m - Return all three elements: the starting node, the relationship, and the ending node\n","# # LIMIT 50 - Only return up to 50 results, we can also set the limit to 5000.\n","# default_cypher = \"MATCH (n)-[r]->(m) RETURN n,r,m LIMIT 50\"\n","\n","# def showGraph(cypher: str = default_cypher):\n","#     # create a neo4j session to run queries\n","#     driver = GraphDatabase.driver(\n","#         uri=os.environ[\"NEO4J_URI\"],\n","#         auth=(os.environ[\"NEO4J_USERNAME\"],\n","#               os.environ[\"NEO4J_PASSWORD\"]))\n","#     session = driver.session()\n","#     widget = GraphWidget(graph=session.run(cypher).graph())\n","#     widget.node_label_mapping = 'id'\n","#     display(widget)\n","#     return widget\n","\n","# showGraph()"],"metadata":{"id":"tVR0ClNxuIVT","executionInfo":{"status":"ok","timestamp":1749664364667,"user_tz":240,"elapsed":1,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"tVR0ClNxuIVT","execution_count":26,"outputs":[]},{"cell_type":"code","source":["\n","def showGraph(keyword=None, limit=500):\n","    \"\"\"\n","    Visualize the knowledge graph with keywords/topics as central nodes\n","    and connected papers as edges.\n","\n","    Parameters:\n","    - keyword: Optional string to filter for a specific keyword\n","    - limit: Maximum number of nodes to return\n","    \"\"\"\n","    # create a neo4j session to run queries\n","    driver = GraphDatabase.driver(\n","        uri=os.environ[\"NEO4J_URI\"],\n","        auth=(os.environ[\"NEO4J_USERNAME\"],\n","              os.environ[\"NEO4J_PASSWORD\"]))\n","    session = driver.session()\n","\n","    if keyword:\n","        # Query for a specific keyword and its connected papers\n","        cypher = f\"\"\"\n","        MATCH (k:Keyword {{term: $keyword}})<-[r1:HAS_TOPIC]-(p:Paper)\n","        OPTIONAL MATCH (p)-[r2:HAS_TOPIC]->(k2:Keyword)\n","        WHERE k <> k2\n","        RETURN k, r1, p, r2, k2\n","        LIMIT {limit}\n","        \"\"\"\n","        params = {\"keyword\": keyword}\n","    else:\n","        # Query that puts keywords at the center with connected papers\n","        cypher = f\"\"\"\n","        MATCH (k:Keyword)<-[r1:HAS_TOPIC]-(p:Paper)\n","        RETURN k, r1, p\n","        LIMIT {limit}\n","        \"\"\"\n","        params = {}\n","\n","    # Run the query and display the graph\n","    widget = GraphWidget(graph=session.run(cypher, params).graph())\n","\n","    # Configure the visualization\n","    widget.node_label_mapping = 'id'\n","\n","    # Make keyword nodes more prominent\n","    widget.node_style_mapping = lambda node: {\n","        'fill': '#4CAF50' if 'Keyword' in node.labels else '#2196F3',\n","        'shape': 'ellipse' if 'Keyword' in node.labels else 'rectangle',\n","        'size': 50 if 'Keyword' in node.labels else 35\n","    }\n","\n","    # Edge styles based on relationship type\n","    widget.edge_style_mapping = lambda edge: {\n","        'stroke': '#FF5722' if edge.type == 'HAS_TOPIC' else '#9C27B0',\n","        'stroke-width': 3 if edge.type == 'HAS_TOPIC' else 1\n","    }\n","\n","    display(widget)\n","    return widget\n"],"metadata":{"id":"BFbGk9Cz39mu","executionInfo":{"status":"ok","timestamp":1749664364732,"user_tz":240,"elapsed":34,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"BFbGk9Cz39mu","execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Show a specific keyword\n","showGraph(keyword=\"large language models\")\n","# showGraph(keyword=\"gpt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8a792b39cafc4e0d8d91eba67e3b8006","f50ae02f615142b094c8c9454dbd0e58","178e0d38f1fa45389394f4a7ec4c6dd7"]},"id":"LZR28_YBJePv","executionInfo":{"status":"ok","timestamp":1749664483175,"user_tz":240,"elapsed":1711,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"8ed88b37-9b1a-4541-ca18-718742c3b3b3"},"id":"LZR28_YBJePv","execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":["GraphWidget(layout=Layout(height='800px', width='100%'))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a792b39cafc4e0d8d91eba67e3b8006"}},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"}}}}},{"output_type":"display_data","data":{"text/plain":["GraphWidget(layout=Layout(height='800px', width='100%'))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a792b39cafc4e0d8d91eba67e3b8006"}},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"}}}}}]},{"cell_type":"markdown","source":["### Writing Cypher with an LLM"],"metadata":{"id":"TxJLl5Hhtm1G"},"id":"TxJLl5Hhtm1G"},{"cell_type":"markdown","source":["Print the schema of the knowledge graph"],"metadata":{"id":"IC2Wlg7_v3yo"},"id":"IC2Wlg7_v3yo"},{"cell_type":"code","source":["kg.refresh_schema()\n","print(textwrap.fill(kg.schema, 60))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cBZMBTfSv0cX","executionInfo":{"status":"ok","timestamp":1749664368540,"user_tz":240,"elapsed":1175,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"133d06bd-e5f1-4210-d2c0-6004bfaa4e2e"},"id":"cBZMBTfSv0cX","execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Node properties: Paper {paperID: STRING, title: STRING,\n","year: STRING, venue: STRING, abstract: STRING, filename:\n","STRING} Chunk {paperID: STRING, chunkId: STRING,\n","textEmbedding: LIST, text: STRING, paperTitle: STRING,\n","sectionSeqId: INTEGER} Author {name: STRING} Keyword {term:\n","STRING} Relationship properties:  The relationships:\n","(:Paper)-[:HAS_TOPIC]->(:Keyword)\n","(:Chunk)-[:PART_OF]->(:Paper)\n","(:Author)-[:AUTHORED]->(:Paper)\n"]}]},{"cell_type":"code","source":["CYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to\n","query a graph database.\n","Instructions:\n","Use only the provided relationship types and properties in the\n","schema. Do not use any other relationship types or properties that\n","are not provided.\n","Schema:\n","{schema}\n","Note: Do not include any explanations or apologies in your responses.\n","Do not respond to any questions that might ask anything else than\n","for you to construct a Cypher statement.\n","Do not include any text except the generated Cypher statement.\n","Examples: Here are a few examples of generated Cypher\n","statements for particular questions:\n","\n","# Find papers that share topics\n","MATCH (p1:Paper)-[:HAS_TOPIC]->(k:Keyword)<-[:HAS_TOPIC]-(p2:Paper)\n","    WHERE p1.paperID < p2.paperID  // To avoid duplicate pairs\n","    WITH p1, p2, collect(k.term) as shared_topics\n","    RETURN p1.title AS paper1,\n","           p2.title AS paper2,\n","           shared_topics,\n","           size(shared_topics) AS connection_strength\n","    ORDER BY connection_strength DESC\n","The question is:\n","{question}\"\"\""],"metadata":{"id":"_xJAq62ntnqR","executionInfo":{"status":"ok","timestamp":1749664368558,"user_tz":240,"elapsed":16,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"_xJAq62ntnqR","execution_count":30,"outputs":[]},{"cell_type":"code","source":["from langchain.chains import GraphCypherQAChain\n","from langchain.prompts.prompt import PromptTemplate\n","CYPHER_GENERATION_PROMPT = PromptTemplate(\n","    input_variables=[\"schema\", \"question\"],\n","    template=CYPHER_GENERATION_TEMPLATE\n",")"],"metadata":{"id":"zH84obvvtsAR","executionInfo":{"status":"ok","timestamp":1749664368568,"user_tz":240,"elapsed":8,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"zH84obvvtsAR","execution_count":31,"outputs":[]},{"cell_type":"code","source":["cypherChain = GraphCypherQAChain.from_llm(\n","    ChatOpenAI(temperature=0),\n","    graph=kg,\n","    verbose=True,\n","    cypher_prompt=CYPHER_GENERATION_PROMPT,\n","    allow_dangerous_requests=True\n",")"],"metadata":{"id":"6kogOhiMtutP","executionInfo":{"status":"ok","timestamp":1749664368571,"user_tz":240,"elapsed":2,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"6kogOhiMtutP","execution_count":32,"outputs":[]},{"cell_type":"code","source":["def prettyCypherChain(question: str) -> str:\n","    response = cypherChain.run(question)\n","    print(textwrap.fill(response, 60))"],"metadata":{"id":"9IWRkYSbt1uc","executionInfo":{"status":"ok","timestamp":1749664368585,"user_tz":240,"elapsed":13,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"9IWRkYSbt1uc","execution_count":33,"outputs":[]},{"cell_type":"code","source":["prettyCypherChain(\"Please give me the most popular topics of these paper?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HrxEODO903OL","executionInfo":{"status":"ok","timestamp":1749664374082,"user_tz":240,"elapsed":5496,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}},"outputId":"9ed8be07-afb7-4136-eb7a-c596e620e8f1"},"id":"HrxEODO903OL","execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n","Generated Cypher:\n","\u001b[32;1m\u001b[1;3mMATCH (p:Paper)-[:HAS_TOPIC]->(k:Keyword)\n","RETURN k.term AS topic, COUNT(p) AS popularity\n","ORDER BY popularity DESC\u001b[0m\n","Full Context:\n","\u001b[32;1m\u001b[1;3m[{'topic': 'large language models', 'popularity': 44}, {'topic': 'large', 'popularity': 10}, {'topic': 'large language', 'popularity': 9}, {'topic': 'large language model', 'popularity': 8}, {'topic': 'language models llms', 'popularity': 5}, {'topic': 'state', 'popularity': 4}, {'topic': 'including', 'popularity': 4}, {'topic': 'language', 'popularity': 4}, {'topic': 'language models', 'popularity': 3}, {'topic': 'gpt', 'popularity': 3}]\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","The most popular topics of these papers are large language\n","models, large, and large language.\n"]}]},{"cell_type":"markdown","source":["# ğŸ’¡ Ask you own question!\n","<!-- <img src = 'https://www.shutterstock.com/image-photo/wood-letters-try-word-acronym-260nw-693999751.jpg'> -->\n","\n","<!-- <img src = 'https://static.vecteezy.com/system/resources/previews/010/784/649/non_2x/yellow-abstract-lamp-and-question-mark-logo-creative-logo-idea-concept-logo-concept-vector.jpg'> -->"],"metadata":{"id":"AkWS9RLlJUT1"},"id":"AkWS9RLlJUT1"},{"cell_type":"code","source":["# prettyCypherChain(\"Your question\")"],"metadata":{"id":"ejtnxc7KJR08","executionInfo":{"status":"ok","timestamp":1749664374089,"user_tz":240,"elapsed":2,"user":{"displayName":"Zoe Duan","userId":"05297923180961040270"}}},"id":"ejtnxc7KJR08","execution_count":35,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"8a792b39cafc4e0d8d91eba67e3b8006":{"model_module":"yfiles-jupyter-graphs","model_name":"GraphModel","model_module_version":"^1.10.7","state":{"_context_pane_mapping":[{"id":"Neighborhood","title":"Neighborhood"},{"id":"Data","title":"Data"},{"id":"Search","title":"Search"},{"id":"About","title":"About"}],"_data_importer":"neo4j","_directed":true,"_dom_classes":[],"_edges":[{"id":6917531226664337000,"start":43,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":43,"end":105,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":43,"end":107,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":43,"end":108,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":43,"end":109,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":43,"end":110,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":43,"end":111,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":43,"end":112,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":43,"end":113,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":43,"end":114,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6919783026478023000,"start":181,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":181,"end":234,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":181,"end":235,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":181,"end":236,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":181,"end":237,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":181,"end":238,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":181,"end":239,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":181,"end":240,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":181,"end":241,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664337000,"start":181,"end":242,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6922034826291708000,"start":1136,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1136,"end":1262,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1136,"end":1263,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1136,"end":1264,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1136,"end":1265,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1136,"end":1266,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1136,"end":1267,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1136,"end":1268,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1136,"end":1269,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1136,"end":1270,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6924286626105393000,"start":1442,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630104000,"start":1442,"end":1455,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1442,"end":1456,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1442,"end":1457,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1442,"end":1458,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1442,"end":1459,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1442,"end":1460,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1442,"end":1461,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1442,"end":1462,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664338000,"start":1442,"end":1463,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6926538425919078000,"start":1537,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6919783026478023000,"start":1537,"end":286,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1537,"end":1567,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1537,"end":1568,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1537,"end":1569,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1537,"end":1570,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1537,"end":1571,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1537,"end":1572,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1537,"end":1573,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1537,"end":1574,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6928790225732764000,"start":1779,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1779,"end":1832,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1779,"end":1833,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1779,"end":1834,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1779,"end":1835,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1779,"end":1836,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1779,"end":1837,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1779,"end":1838,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1779,"end":1839,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1779,"end":1840,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6931042025546449000,"start":1886,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1886,"end":1939,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1886,"end":1940,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1886,"end":1941,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1886,"end":1942,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1886,"end":1943,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1886,"end":1944,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1886,"end":1945,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1886,"end":1946,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":1886,"end":1947,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6933293825360134000,"start":2077,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2077,"end":2122,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2077,"end":2123,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2077,"end":2124,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2077,"end":2125,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2077,"end":2126,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2077,"end":2127,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2077,"end":2128,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2077,"end":2129,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2077,"end":2130,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6935545625173819000,"start":2345,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2345,"end":2416,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2345,"end":2417,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2345,"end":2418,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2345,"end":2419,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2345,"end":2420,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2345,"end":2421,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2345,"end":2422,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2345,"end":2423,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664339000,"start":2345,"end":2424,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6937797424987505000,"start":3100,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3100,"end":3223,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3100,"end":3224,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3100,"end":3225,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3100,"end":3226,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3100,"end":3227,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3100,"end":3228,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3100,"end":3229,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3100,"end":3230,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3100,"end":3231,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6940049224801190000,"start":3308,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3308,"end":3356,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3308,"end":3357,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3308,"end":3358,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3308,"end":3359,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3308,"end":3360,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3308,"end":3361,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3308,"end":3362,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3308,"end":3363,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3308,"end":3364,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6942301024614875000,"start":3365,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3365,"end":3469,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3365,"end":3470,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3365,"end":3471,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3365,"end":3472,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3365,"end":3473,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3365,"end":3474,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3365,"end":3475,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3365,"end":3476,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664340000,"start":3365,"end":3477,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6944552824428560000,"start":3573,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3573,"end":3638,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3573,"end":3639,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3573,"end":3640,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3573,"end":3641,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3573,"end":3642,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3573,"end":3643,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3573,"end":3644,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3573,"end":3645,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3573,"end":3646,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6946804624242246000,"start":3769,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3769,"end":3810,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3769,"end":3811,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3769,"end":3812,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3769,"end":3813,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3769,"end":3814,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3769,"end":3815,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3769,"end":3816,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3769,"end":3817,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3769,"end":3818,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6949056424055931000,"start":3897,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6931042025546450000,"start":3897,"end":1460,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3897,"end":3920,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3897,"end":3921,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3897,"end":3922,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3897,"end":3923,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3897,"end":3924,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3897,"end":3925,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3897,"end":3926,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3897,"end":3927,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6951308223869616000,"start":3961,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3961,"end":4005,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3961,"end":4006,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3961,"end":4007,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3961,"end":4008,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3961,"end":4009,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3961,"end":4010,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3961,"end":4011,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3961,"end":4012,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":3961,"end":4013,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6953560023683301000,"start":4099,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4099,"end":4158,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4099,"end":4159,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4099,"end":4160,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4099,"end":4161,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4099,"end":4162,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4099,"end":4163,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4099,"end":4164,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4099,"end":4165,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4099,"end":4166,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6955811823496987000,"start":4354,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630107000,"start":4354,"end":4368,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4354,"end":4369,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4354,"end":4370,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4354,"end":4371,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4354,"end":4372,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4354,"end":4373,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4354,"end":4374,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4354,"end":4375,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4354,"end":4376,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6958063623310672000,"start":4377,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6919783026478027000,"start":4377,"end":3696,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4377,"end":4418,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4377,"end":4419,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4377,"end":4420,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4377,"end":4421,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4377,"end":4422,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4377,"end":4423,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4377,"end":4424,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664342000,"start":4377,"end":4425,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6960315423124357000,"start":4591,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4591,"end":4641,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4591,"end":4642,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4591,"end":4643,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4591,"end":4644,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4591,"end":4645,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4591,"end":4646,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4591,"end":4647,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4591,"end":4648,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4591,"end":4649,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1155175503443792600,"start":4794,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6919783026478027000,"start":4794,"end":4093,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4794,"end":4823,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4794,"end":4824,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4794,"end":4825,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4794,"end":4826,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4794,"end":4827,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4794,"end":4828,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4794,"end":4829,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4794,"end":4830,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1155175503443792600,"start":4918,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4918,"end":4968,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4918,"end":4969,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4918,"end":4970,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4918,"end":4971,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4918,"end":4972,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4918,"end":4973,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4918,"end":4974,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4918,"end":4975,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664343000,"start":4918,"end":4976,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630108200,"start":5654,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5654,"end":5703,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5654,"end":5704,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5654,"end":5705,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5654,"end":5706,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5654,"end":5707,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5654,"end":5708,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5654,"end":5709,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5654,"end":5710,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5654,"end":5711,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630108200,"start":5712,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5712,"end":5767,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5712,"end":5768,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5712,"end":5769,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5712,"end":5770,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5712,"end":5771,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5712,"end":5772,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5712,"end":5773,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5712,"end":5774,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":5712,"end":5775,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630108700,"start":6169,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":6169,"end":6504,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":6169,"end":6505,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":6169,"end":6506,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":6169,"end":6507,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":6169,"end":6508,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":6169,"end":6509,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":6169,"end":6510,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":6169,"end":6511,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664344000,"start":6169,"end":6512,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1155175503443794400,"start":6783,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6783,"end":6829,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6783,"end":6830,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6783,"end":6831,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6783,"end":6832,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6783,"end":6833,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6783,"end":6834,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6783,"end":6835,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6783,"end":6836,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6783,"end":6837,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1155175503443794700,"start":6972,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6972,"end":6999,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6972,"end":7000,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6972,"end":7001,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6972,"end":7002,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6972,"end":7003,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6972,"end":7004,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6972,"end":7005,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6972,"end":7006,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664345000,"start":6972,"end":7007,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630110500,"start":8000,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664346000,"start":8000,"end":8054,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664346000,"start":8000,"end":8055,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664346000,"start":8000,"end":8056,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664346000,"start":8000,"end":8057,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664346000,"start":8000,"end":8058,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664346000,"start":8000,"end":8059,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664346000,"start":8000,"end":8060,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664346000,"start":8000,"end":8061,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664346000,"start":8000,"end":8062,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630111500,"start":9078,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9078,"end":9150,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9078,"end":9151,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9078,"end":9152,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9078,"end":9153,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9078,"end":9154,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9078,"end":9155,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9078,"end":9156,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9078,"end":9157,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9078,"end":9158,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1155175503443797000,"start":9216,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9216,"end":9259,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9216,"end":9260,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9216,"end":9261,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9216,"end":9262,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9216,"end":9263,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9216,"end":9264,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9216,"end":9265,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9216,"end":9266,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9216,"end":9267,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630111700,"start":9268,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9268,"end":9389,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9268,"end":9390,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9268,"end":9391,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9268,"end":9392,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9268,"end":9393,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9268,"end":9394,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9268,"end":9395,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9268,"end":9396,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9268,"end":9397,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630112000,"start":9398,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6919783026478031000,"start":9398,"end":8247,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9398,"end":9471,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9398,"end":9472,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9398,"end":9473,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9398,"end":9474,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9398,"end":9475,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9398,"end":9476,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9398,"end":9477,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9398,"end":9478,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630112000,"start":9479,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9479,"end":9578,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9479,"end":9579,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9479,"end":9580,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9479,"end":9581,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9479,"end":9582,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9479,"end":9583,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9479,"end":9584,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9479,"end":9585,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664347000,"start":9479,"end":9586,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1155175503443797500,"start":9833,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6919783026478023000,"start":9833,"end":179,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9833,"end":9889,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9833,"end":9890,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9833,"end":9891,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9833,"end":9892,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9833,"end":9893,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9833,"end":9894,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9833,"end":9895,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9833,"end":9896,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630112500,"start":9897,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9897,"end":9947,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9897,"end":9948,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9897,"end":9949,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9897,"end":9950,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9897,"end":9951,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9897,"end":9952,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9897,"end":9953,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9897,"end":9954,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":9897,"end":9955,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630112800,"start":10330,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10330,"end":10410,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10330,"end":10411,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10330,"end":10412,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10330,"end":10413,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10330,"end":10414,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10330,"end":10415,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10330,"end":10416,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10330,"end":10417,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10330,"end":10418,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630113000,"start":10419,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10419,"end":10484,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10419,"end":10485,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10419,"end":10486,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10419,"end":10487,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10419,"end":10488,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10419,"end":10489,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10419,"end":10490,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10419,"end":10491,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664348000,"start":10419,"end":10492,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630113300,"start":10670,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6922034826291712000,"start":10670,"end":4298,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664349000,"start":10670,"end":10862,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664349000,"start":10670,"end":10863,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664349000,"start":10670,"end":10864,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664349000,"start":10670,"end":10865,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664349000,"start":10670,"end":10866,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664349000,"start":10670,"end":10867,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664349000,"start":10670,"end":10868,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664349000,"start":10670,"end":10869,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630114300,"start":11797,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11797,"end":11825,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11797,"end":11826,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11797,"end":11827,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11797,"end":11828,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11797,"end":11829,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11797,"end":11830,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11797,"end":11831,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11797,"end":11832,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11797,"end":11833,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1155175503443799600,"start":11897,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11897,"end":11945,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11897,"end":11946,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11897,"end":11947,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11897,"end":11948,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11897,"end":11949,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11897,"end":11950,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11897,"end":11951,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11897,"end":11952,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664350000,"start":11897,"end":11953,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630115600,"start":12973,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":12973,"end":13071,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":12973,"end":13072,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":12973,"end":13073,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":12973,"end":13074,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":12973,"end":13075,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":12973,"end":13076,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":12973,"end":13077,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":12973,"end":13078,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":12973,"end":13079,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630115600,"start":13080,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13080,"end":13144,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13080,"end":13145,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13080,"end":13146,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13080,"end":13147,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13080,"end":13148,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13080,"end":13149,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13080,"end":13150,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13080,"end":13151,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13080,"end":13152,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1155175503443800800,"start":13153,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13153,"end":13216,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13153,"end":13217,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13153,"end":13218,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13153,"end":13219,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13153,"end":13220,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13153,"end":13221,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13153,"end":13222,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13153,"end":13223,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13153,"end":13224,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1152923703630116400,"start":13700,"end":106,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1155175503443801600,"start":13700,"end":13711,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":1157427303257486800,"start":13700,"end":13712,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13700,"end":13713,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13700,"end":13714,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13700,"end":13715,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13700,"end":13716,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13700,"end":13717,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13700,"end":13718,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"},{"id":6917531226664351000,"start":13700,"end":13719,"properties":{"label":"HAS_TOPIC"},"color":"#F44336","thickness_factor":1,"directed":true,"styles":{},"label":"HAS_TOPIC"}],"_graph_layout":{},"_highlight":[],"_license":{},"_model_module":"yfiles-jupyter-graphs","_model_module_version":"^1.10.7","_model_name":"GraphModel","_neighborhood":{},"_nodes":[{"id":106,"properties":{"term":"large language models","label":"Keyword"},"color":"#2196F3","styles":{},"label":"106","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":43,"properties":{"venue":"","filename":"2409.13980v1.json","year":"","abstract":"\n Recent advancements in Vision-Language (VL) research have sparked new benchmarks for complex visual reasoning, challenging models' advanced reasoning ability. Traditional Vision-Language Models (VLMs) perform well in visual perception tasks while struggling with complex reasoning scenarios. Conversely, Large Language Models (LLMs) demonstrate robust text reasoning capabilities; however, they lack visual acuity. To bridge this gap, we propose Complex Visual Reasoning Large Language Models (CVR-LLM), capitalizing on VLMs' visual perception proficiency and LLMs' extensive reasoning capability. Unlike recent multimodal large language models (MLLMs) that require a projection layer, our approach transforms images into detailed, context-aware descriptions using an iterative self-refinement loop and leverages LLMs' text knowledge for accurate predictions without extra training. We also introduce a novel multimodal in-context learning (ICL) methodology to enhance LLMs' contextual understanding and reasoning. Additionally, we introduce Chain-of-Comparison (CoC), a step-by-step comparison technique enabling contrasting various aspects of predictions. Our CVR-LLM presents the first comprehensive study across a wide array of complex visual reasoning tasks and achieves SOTA performance among all. \n","title":"Enhancing Advanced Visual Reasoning Ability of Large Language Models","paperID":"6b8fa9eb2e0e","label":"Paper"},"color":"#4CAF50","styles":{},"label":"43","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":105,"properties":{"term":"visual reasoning tasks","label":"Keyword"},"color":"#2196F3","styles":{},"label":"105","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":107,"properties":{"term":"predictions extra training","label":"Keyword"},"color":"#2196F3","styles":{},"label":"107","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":108,"properties":{"term":"icl methodology enhance","label":"Keyword"},"color":"#2196F3","styles":{},"label":"108","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":109,"properties":{"term":"cvr llm capitalizing","label":"Keyword"},"color":"#2196F3","styles":{},"label":"109","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":110,"properties":{"term":"achieves sota performance","label":"Keyword"},"color":"#2196F3","styles":{},"label":"110","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":111,"properties":{"term":"capabilities lack","label":"Keyword"},"color":"#2196F3","styles":{},"label":"111","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":112,"properties":{"term":"sparked new","label":"Keyword"},"color":"#2196F3","styles":{},"label":"112","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":113,"properties":{"term":"array complex","label":"Keyword"},"color":"#2196F3","styles":{},"label":"113","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":114,"properties":{"term":"acuity bridge gap","label":"Keyword"},"color":"#2196F3","styles":{},"label":"114","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":181,"properties":{"venue":"","filename":"2410.08993v1.json","year":"","abstract":"\n Large language models encode the correlational structure present in natural language by fitting segments of utterances (tokens) into a high dimensional ambient latent space upon which the models then operate. We assert that in order to develop a foundational, first-principles understanding of the behavior and limitations of large language models, it is crucial to understand the topological and geometric structure of this token subspace. In this article, we present estimators for the dimension and Ricci scalar curvature of the token subspace, and apply it to three open source large language models of moderate size: GPT2, LLEMMA7B, and MISTRAL7B. In all three models, using these measurements, we find that the token subspace is not a manifold, but is instead a stratified manifold, where on each of the individual strata, the Ricci curvature is significantly negative. We additionally find that the dimension and curvature correlate with generative fluency of the models, which suggest that these findings have implications for model behavior. \n","title":"The structure of the token space for large language models","paperID":"86d7f3e2f550","label":"Paper"},"color":"#4CAF50","styles":{},"label":"181","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":234,"properties":{"term":"dimension ricci scalar","label":"Keyword"},"color":"#2196F3","styles":{},"label":"234","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":235,"properties":{"term":"ambient latent space","label":"Keyword"},"color":"#2196F3","styles":{},"label":"235","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":236,"properties":{"term":"correlational structure","label":"Keyword"},"color":"#2196F3","styles":{},"label":"236","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":237,"properties":{"term":"understand topological geometric","label":"Keyword"},"color":"#2196F3","styles":{},"label":"237","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":238,"properties":{"term":"using measurements token","label":"Keyword"},"color":"#2196F3","styles":{},"label":"238","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":239,"properties":{"term":"gpt2 llemma7b mistral7b","label":"Keyword"},"color":"#2196F3","styles":{},"label":"239","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":240,"properties":{"term":"suggest findings implications","label":"Keyword"},"color":"#2196F3","styles":{},"label":"240","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":241,"properties":{"term":"instead","label":"Keyword"},"color":"#2196F3","styles":{},"label":"241","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":242,"properties":{"term":"apply open source","label":"Keyword"},"color":"#2196F3","styles":{},"label":"242","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1136,"properties":{"venue":"","filename":"2201.11903v6.json","year":"","abstract":"\n We explore how generating a chain of thought-a series of intermediate reasoning steps-significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier. A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 -20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9. \n Chain-of-Thought Prompting Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? A: The answer is 27. \n Standard Prompting Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Model Input Model Output Model Output Model Input Figure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). \n","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","paperID":"82a9b32949ef","label":"Paper"},"color":"#4CAF50","styles":{},"label":"1136","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":1262,"properties":{"term":"thought reasoning processes","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1262","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1263,"properties":{"term":"prompting roger tennis","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1263","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1264,"properties":{"term":"improves ability large","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1264","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1265,"properties":{"term":"explore generating","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1265","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1266,"properties":{"term":"complex arithmetic commonsense","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1266","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1267,"properties":{"term":"gsm8k benchmark","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1267","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1268,"properties":{"term":"23 apples originally","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1268","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1269,"properties":{"term":"verifier cafeteria","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1269","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1270,"properties":{"term":"input figure chain","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1270","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1442,"properties":{"venue":"","filename":"2301.13820v1.json","year":"","abstract":"\n While large language models (LLMs) have demonstrated strong capability in structured prediction tasks such as semantic parsing, few amounts of research have explored the underlying mechanisms of their success. Our work studies different methods for explaining an LLM-based semantic parser and qualitatively discusses the explained model behaviors, hoping to inspire future research toward better understanding them. \n","title":"Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)","paperID":"1cb84e564dce","label":"Paper"},"color":"#4CAF50","styles":{},"label":"1442","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":1455,"properties":{"term":"semantic parser qualitatively","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1455","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1456,"properties":{"term":"capability structured prediction","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1456","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1457,"properties":{"term":"models llms demonstrated","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1457","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1458,"properties":{"term":"explained","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1458","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1459,"properties":{"term":"behaviors hoping inspire","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1459","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1460,"properties":{"term":"large","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1460","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1461,"properties":{"term":"explored underlying mechanisms","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1461","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1462,"properties":{"term":"amounts research explored","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1462","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1463,"properties":{"term":"work studies different","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1463","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1537,"properties":{"venue":"","filename":"2207.08982v1.json","year":"","abstract":"\n In this work we show how large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias. To demonstrate the effect, we developed a masked gender task that can be applied to BERT-family models to reveal spurious correlations between predicted gender pronouns and a variety of seemingly gender-neutral variables like date and location, on pre-trained (unmodified) BERT and RoBERTa large models. Finally, we provide an online demo, inviting readers to experiment further. \n","title":"Selection Bias Induced Spurious Correlations in Large Language Models","paperID":"a10c6c2f22f9","label":"Paper"},"color":"#4CAF50","styles":{},"label":"1537","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":286,"properties":{"term":"like","label":"Keyword"},"color":"#2196F3","styles":{},"label":"286","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1567,"properties":{"term":"predicted gender pronouns","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1567","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1568,"properties":{"term":"bert roberta large","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1568","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1569,"properties":{"term":"dataset selection","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1569","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1570,"properties":{"term":"dependencies unconditionally","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1570","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1571,"properties":{"term":"models llms","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1571","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1572,"properties":{"term":"spurious correlations","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1572","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1573,"properties":{"term":"readers experiment","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1573","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1574,"properties":{"term":"date location pre","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1574","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1779,"properties":{"venue":"","filename":"2303.02155v2.json","year":"","abstract":"\n Large language models (LLMs) have taken the scientific world by storm, changing the landscape of natural language processing and human-computer interaction. These powerful tools can answer complex questions and, surprisingly, perform challenging creative tasks (e.g., generate code and applications to solve problems, write stories, pieces of music, etc.). In this paper, we present a collaborative game design framework that combines interactive evolution and large language models to simulate the typical human design process. We use the former to exploit users' feedback for selecting the most promising ideas and large language models for a very complex creative task-the recombination and variation of ideas. In our framework, the process starts with a brief and a set of candidate designs, either generated using a language model or proposed by the users. Next, users collaborate on the design process by providing feedback to an interactive genetic algorithm that selects, recombines, and mutates the most promising designs. We evaluated our framework on three game design tasks with human designers who collaborated remotely. CCS Concepts: â€¢ Computing methodologies â†’ Genetic algorithms; â€¢ Theory of computation â†’ Interactive computation; â€¢ Human-centered computing â†’ Systems and tools for interaction design. \n","title":"ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design","paperID":"1760e89c0203","label":"Paper"},"color":"#4CAF50","styles":{},"label":"1779","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":1832,"properties":{"term":"game design tasks","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1832","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1833,"properties":{"term":"combines interactive evolution","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1833","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1834,"properties":{"term":"generate code","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1834","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1835,"properties":{"term":"proposed users users","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1835","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1836,"properties":{"term":"collaborated remotely ccs","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1836","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1837,"properties":{"term":"recombination","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1837","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1838,"properties":{"term":"surprisingly perform","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1838","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1839,"properties":{"term":"storm changing landscape","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1839","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1840,"properties":{"term":"framework process starts","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1840","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1886,"properties":{"venue":"","filename":"2303.07304v1.json","year":"","abstract":"\n The paper looks at the role of large language models in academic knowledge creation based on a scoping review (2018 to January 2023) of how researchers have previously used the language model GPT to assist in the performance of academic knowledge creation tasks beyond data analysis. These tasks include writing, editing, reviewing, dataset creation and curation, which have been difficult to perform using earlier ML tools. Based on a synthesis of these papers, this study identifies pathways for a future academic research landscape that incorporates wider usage of large language models based on the current modes of adoption in published articles as a CoWriter, Research Assistant and Respondent. The paper concludes with a research and practice agenda for management knowledge creation based on the wider adoption of Large Language models. The paper's focus is on understanding the nature of the current usage of GPT to perform academic tasks. As such, it does not describe the challenges and problems of large language models. It does also not speculate about the extent to which they present machine intelligence or consciousness. \n","title":"Algorithmic Ghost in the Research Shell: Large Language Models and Academic Knowledge Creation in Management Research","paperID":"7c1b7edfd36a","label":"Paper"},"color":"#4CAF50","styles":{},"label":"1886","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":1939,"properties":{"term":"academic knowledge creation","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1939","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1940,"properties":{"term":"include writing editing","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1940","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1941,"properties":{"term":"usage gpt perform","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1941","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1942,"properties":{"term":"using earlier ml","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1942","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1943,"properties":{"term":"tasks data","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1943","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1944,"properties":{"term":"scoping review","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1944","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1945,"properties":{"term":"january 2023","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1945","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1946,"properties":{"term":"based current","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1946","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":1947,"properties":{"term":"does","label":"Keyword"},"color":"#2196F3","styles":{},"label":"1947","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2077,"properties":{"venue":"","filename":"2304.04309v1.json","year":"","abstract":"\n Large language models are deep learning models with a large number of parameters. The models made noticeable progress on a large number of tasks, and as a consequence allowing them to serve as valuable and versatile tools for a diverse range of applications. Their capabilities also offer opportunities for business process management, however, these opportunities have not yet been systematically investigated. In this paper, we address this research problem by foregrounding various management tasks of the BPM lifecycle. We investigate six research directions highlighting problems that need to be addressed when using large language models, including usage guidelines for practitioners. \n","title":"Large Language Models for Business Process Management: Opportunities and Challenges â‹†","paperID":"b7a9d972752f","label":"Paper"},"color":"#4CAF50","styles":{},"label":"2077","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":2122,"properties":{"term":"management tasks bpm","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2122","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2123,"properties":{"term":"usage guidelines","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2123","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2124,"properties":{"term":"versatile tools","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2124","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2125,"properties":{"term":"offer opportunities business","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2125","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2126,"properties":{"term":"lifecycle investigate","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2126","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2127,"properties":{"term":"number parameters","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2127","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2128,"properties":{"term":"paper address","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2128","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2129,"properties":{"term":"problem foregrounding various","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2129","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2130,"properties":{"term":"consequence allowing","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2130","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2345,"properties":{"venue":"","filename":"2305.10263v2.json","year":"","abstract":"\n Large language models have recently made tremendous progress in a variety of aspects, e.g., cross-task generalization, instruction following. Comprehensively evaluating the capability of large language models in multiple tasks is of great importance. In this paper, we propose M3KE, a Massive Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to measure knowledge acquired by Chinese large language models by testing their multitask accuracy in zeroand few-shot settings. We have collected 20,477 questions from 71 tasks. Our selection covers all major levels of Chinese education system, ranging from the primary school to college, as well as a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art and religion. All questions are multiple-choice questions with four options, hence guaranteeing a standardized and unified assessment process. We've assessed a number of state-of-theart open-source Chinese large language models on the proposed benchmark. The size of these models varies from 335M to 130B parameters. Experiment results demonstrate that they perform significantly worse than GPT-3.5 that reaches an accuracy of âˆ¼ 48% on M3KE. The dataset is available at  https://github.  com/tjunlp-lab/M3KE . \n","title":"M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models","paperID":"57a0d00704f6","label":"Paper"},"color":"#4CAF50","styles":{},"label":"2345","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":2416,"properties":{"term":"chinese education ranging","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2416","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2417,"properties":{"term":"multitask accuracy","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2417","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2418,"properties":{"term":"assessment process","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2418","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2419,"properties":{"term":"335m 130b parameters","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2419","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2420,"properties":{"term":"worse gpt reaches","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2420","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2421,"properties":{"term":"variety aspects cross","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2421","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2422,"properties":{"term":"religion questions","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2422","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2423,"properties":{"term":"zeroand shot settings","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2423","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":2424,"properties":{"term":"github com","label":"Keyword"},"color":"#2196F3","styles":{},"label":"2424","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3100,"properties":{"venue":"","filename":"2307.05782v2.json","year":"","abstract":"\n Artificial intelligence is making spectacular progress, and one of the best examples is the development of large language models (LLMs) such as OpenAI's GPT series. In these lectures, written for readers with a background in mathematics or physics, we give a brief history and survey of the state of the art, and describe the underlying transformer architecture in detail. We then explore some current ideas on how LLMs work and how models trained to predict the next word in a text are able to perform other tasks displaying intelligence. \n","title":"Large Language Models","paperID":"ec06e3856133","label":"Paper"},"color":"#4CAF50","styles":{},"label":"3100","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":3223,"properties":{"term":"openai gpt series","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3223","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3224,"properties":{"term":"predict","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3224","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3225,"properties":{"term":"underlying transformer","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3225","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3226,"properties":{"term":"spectacular progress best","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3226","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3227,"properties":{"term":"written readers background","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3227","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3228,"properties":{"term":"artificial","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3228","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3229,"properties":{"term":"llms work","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3229","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3230,"properties":{"term":"physics brief","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3230","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3231,"properties":{"term":"tasks displaying","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3231","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3308,"properties":{"venue":"","filename":"2307.15311v1.json","year":"","abstract":"\n Large Language Models (LLMs) have shown remarkable effectiveness in various generaldomain natural language processing (NLP) tasks. However, their performance in transportation safety domain tasks has been suboptimal, primarily attributed to the requirement for specialized transportation safety expertise in generating accurate responses  [1] . To address this challenge, we introduce TrafficSafetyGPT, a novel LLaMA-based model, which has undergone supervised fine-tuning using TrafficSafety-2K dataset which has human labels from government produced guiding books and ChatGPT-generated instruction-output pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset are accessible at  https://github.com/ozheng1993/TrafficSafetyGPT   \n","title":"TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety","paperID":"7b5ef9c15862","label":"Paper"},"color":"#4CAF50","styles":{},"label":"3308","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":3356,"properties":{"term":"using trafficsafety 2k","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3356","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3357,"properties":{"term":"responses address challenge","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3357","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3358,"properties":{"term":"safety domain tasks","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3358","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3359,"properties":{"term":"chatgpt generated instruction","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3359","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3360,"properties":{"term":"train","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3360","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3361,"properties":{"term":"llama based","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3361","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3362,"properties":{"term":"labels government produced","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3362","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3363,"properties":{"term":"suboptimal primarily","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3363","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3364,"properties":{"term":"accessible https github","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3364","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3365,"properties":{"venue":"","filename":"2307.12701v1.json","year":"","abstract":"\n This technical report describes the intersection of process mining and large language models (LLMs), specifically focusing on the abstraction of traditional and object-centric process mining artifacts into textual format. We introduce and explore various prompting strategies: direct answering, where the large language model directly addresses user queries; multi-prompt answering, which allows the model to incrementally build on the knowledge obtained through a series of prompts; and the generation of database queries, facilitating the validation of hypotheses against the original event log. Our assessment considers two large language models, GPT-4 and Google's Bard, under various contextual scenarios across all prompting strategies. Results indicate that these models exhibit a robust understanding of key process mining abstractions, with notable proficiency in interpreting both declarative and procedural process models. In addition, we find that both models demonstrate strong performance in the object-centric setting, which could significantly propel the advancement of the object-centric process mining discipline. Additionally, these models display a noteworthy capacity to evaluate various concepts of fairness in process mining. This opens the door to more rapid and efficient assessments of the fairness of process mining event logs, which has significant implications for the field. \n","title":"Leveraging Large Language Models (LLMs) for Process Mining (","paperID":"a276890a1259","label":"Paper"},"color":"#4CAF50","styles":{},"label":"3365","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":3469,"properties":{"term":"process mining discipline","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3469","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3470,"properties":{"term":"prompt answering allows","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3470","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3471,"properties":{"term":"logs significant implications","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3471","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3472,"properties":{"term":"abstraction traditional object","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3472","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3473,"properties":{"term":"gpt google","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3473","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3474,"properties":{"term":"fairness","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3474","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3475,"properties":{"term":"artifacts textual format","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3475","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3476,"properties":{"term":"queries multi","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3476","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3477,"properties":{"term":"llms specifically focusing","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3477","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3573,"properties":{"venue":"","filename":"2307.10793v1.json","year":"","abstract":"\n Compiler error messages serve as an initial resource for programmers dealing with compilation errors. However, previous studies indicate that they often lack sufficient targeted information to resolve code issues. Consequently, programmers typically rely on their own research to fix errors. Historically, Stack Overflow has been the primary resource for such information, but recent advances in large language models offer alternatives. This study systematically examines 100 compiler error messages from three sources to determine the most effective approach for programmers encountering compiler errors. Factors considered include Stack Overflow search methods and the impact of model version and prompt phrasing when using large language models. The results reveal that GPT-4 outperforms Stack Overflow in explaining compiler error messages, the effectiveness of adding code snippets to Stack Overflow searches depends on the search method, and results for Stack Overflow differ significantly between Google and StackExchange API searches. Furthermore, GPT-4 surpasses GPT-3.5, with \"How to fix\" prompts yielding superior outcomes to \"What does this error mean\" prompts. These results offer valuable guidance for programmers seeking assistance with compiler error messages, underscoring the transformative potential of advanced large language models like GPT-4 in debugging and opening new avenues of exploration for researchers in AI-assisted programming. \n","title":"Addressing Compiler Errors: Stack Overflow or Large Language Models?","paperID":"4c7b505e236c","label":"Paper"},"color":"#4CAF50","styles":{},"label":"3573","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":3638,"properties":{"term":"programmers encountering compiler","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3638","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3639,"properties":{"term":"reveal gpt","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3639","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3640,"properties":{"term":"google stackexchange api","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3640","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3641,"properties":{"term":"prompt phrasing using","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3641","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3642,"properties":{"term":"search method results","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3642","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3643,"properties":{"term":"outperforms stack overflow","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3643","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3644,"properties":{"term":"outcomes does error","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3644","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3645,"properties":{"term":"overflow primary resource","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3645","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3646,"properties":{"term":"adding","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3646","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3769,"properties":{"venue":"","filename":"2308.01776v2.json","year":"","abstract":"\n As large language models, such as GPT, continue to advance the capabilities of natural language processing (NLP), the question arises: does the problem of correction still persist? This paper investigates the role of correction in the context of large language models by conducting two experiments. The first experiment focuses on correction as a standalone task, employing few-shot learning techniques with GPTlike models for error correction. The second experiment explores the notion of correction as a preparatory task for other NLP tasks, examining whether large language models can tolerate and perform adequately on texts containing certain levels of noise or errors. By addressing these experiments, we aim to shed light on the significance of correction in the era of large language models and its implications for various NLP applications. \n","title":"Does Correction Remain A Problem For Large Language Models?","paperID":"1de25182cc14","label":"Paper"},"color":"#4CAF50","styles":{},"label":"3769","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":3810,"properties":{"term":"notion correction preparatory","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3810","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3811,"properties":{"term":"shot learning techniques","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3811","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3812,"properties":{"term":"noise errors addressing","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3812","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3813,"properties":{"term":"gptlike","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3813","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3814,"properties":{"term":"correction second experiment","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3814","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3815,"properties":{"term":"standalone task employing","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3815","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3816,"properties":{"term":"persist paper","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3816","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3817,"properties":{"term":"question arises does","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3817","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3818,"properties":{"term":"tolerate perform adequately","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3818","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3897,"properties":{"venue":"","filename":"2308.02678v1.json","year":"","abstract":"\n This paper examines the ethical considerations and implications of large language models (LLMs) in generating content. It highlights the potential for both positive and negative uses of generative AI programs and explores the challenges in assigning responsibility for their outputs. The discussion emphasizes the need for proactive ethical frameworks and policy measures to guide the responsible development and deployment of LLMs. \n","title":"ETHICAL CONSIDERATIONS AND POLICY IMPLICATIONS FOR LARGE LANGUAGE MODELS: GUIDING RESPONSIBLE DEVELOPMENT AND DEPLOYMENT","paperID":"5f7fd3889bf8","label":"Paper"},"color":"#4CAF50","styles":{},"label":"3897","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":3920,"properties":{"term":"uses generative ai","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3920","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3921,"properties":{"term":"assigning responsibility outputs","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3921","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3922,"properties":{"term":"ethical frameworks policy","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3922","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3923,"properties":{"term":"programs explores","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3923","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3924,"properties":{"term":"development deployment llms","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3924","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3925,"properties":{"term":"content highlights","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3925","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3926,"properties":{"term":"need proactive","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3926","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3927,"properties":{"term":"potential positive negative","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3927","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":3961,"properties":{"venue":"","filename":"2308.02432v1.json","year":"","abstract":"\n Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and dominate the current discourse. Their transformative capabilities have led to a paradigm shift in how we interact with and utilize (text-based) information. Each day, new possibilities to leverage the capabilities of these models emerge. This paper presents findings on the performance of different large language models in a university of applied sciences' undergraduate computer science degree program. Our primary objective is to assess the effectiveness of these models within the curriculum by employing them as educational aids. By prompting the models with lecture material, exercise tasks, and past exams, we aim to evaluate their proficiency across different computer science domains. We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10 tested modules, BingAI achieved 68.4%, and LLaMa, in the 65 billion parameter variant, 20%. Despite these convincing results, even GPT-4.0 would not pass the degree program -due to limitations in mathematical calculations. \n","title":"Performance of Large Language Models in a Computer Science Degree Program","paperID":"743641ffda2a","label":"Paper"},"color":"#4CAF50","styles":{},"label":"3961","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":4005,"properties":{"term":"computer science domains","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4005","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4006,"properties":{"term":"gpt pass degree","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4006","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4007,"properties":{"term":"lecture material exercise","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4007","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4008,"properties":{"term":"tested modules bingai","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4008","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4009,"properties":{"term":"79 total score","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4009","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4010,"properties":{"term":"day new","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4010","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4011,"properties":{"term":"assess effectiveness","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4011","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4012,"properties":{"term":"parameter variant","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4012","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4013,"properties":{"term":"shift interact","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4013","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4099,"properties":{"venue":"","filename":"2308.10252v1.json","year":"","abstract":"\n With the burgeoning development in the realm of large language models (LLMs), the demand for efficient incremental training tailored to specific industries and domains continues to increase. Currently, the predominantly employed frameworks lack modular design, it often takes a lot of coding work to kickstart the training of LLM. To address this, we present \"LMTuner\", a highly usable, integrable, and scalable system for training LLMs expeditiously and with minimal user-input. LMTuner comprises three main modules -the Interaction, Training, and Inference Modules. We advocate that LMTuner's usability and integrality alleviate the complexities in training large language models. Remarkably, even a novice user could commence training large language models within five minutes. Furthermore, it integrates DeepSpeed frameworks and supports Efficient Fine-Tuning methodologies like Low Rank Adaptation (LoRA), Quantized LoRA (QLoRA), etc., enabling the training of language models scaling from 300M to a whopping 130B parameters using a single server. The LMTuner's homepage 1 and screencast video 2 are now publicly available. \n","title":"LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models","paperID":"c78cc9e800c1","label":"Paper"},"color":"#4CAF50","styles":{},"label":"4099","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":4158,"properties":{"term":"deepspeed frameworks supports","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4158","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4159,"properties":{"term":"lmtuner homepage screencast","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4159","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4160,"properties":{"term":"usability integrality alleviate","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4160","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4161,"properties":{"term":"lora qlora enabling","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4161","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4162,"properties":{"term":"like low rank","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4162","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4163,"properties":{"term":"work kickstart training","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4163","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4164,"properties":{"term":"demand efficient incremental","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4164","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4165,"properties":{"term":"address","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4165","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4166,"properties":{"term":"using single server","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4166","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4354,"properties":{"venue":"","filename":"2308.16118v2.json","year":"","abstract":"\n In their recent Nature Human Behaviour paper, \"Emergent analogical reasoning in large language models,\"  (Webb, Holyoak, and Lu, 2023)  the authors argue that \"GPT-3 exhibits a very general capacity to identify and generalize-in zero-shot fashion-relational patterns found within both formal problems and meaningful texts.\" This conclusion arises from their comparison of GPT-3 with human performance across four analogical reasoning domains, where they find comparable results. In this response, we argue that this approach is unsuitable for evaluating general, zero-shot reasoning in large language models (LLMs). Two primary reasons underlie our objection. First, the term \"zeroshot\" implies problem sets entirely novel to GPT-3. However, the chosen approach cannot conclusively eliminate the possibility of these problems residing in the LLM's training data, as acknowledged by the authors themselves in the review file 1 . Second, the assumption underlying this approach is that tests designed for humans can accurately measure LLM capabilities. This assumption is prevalent, but remains unverified. We also provide empirical results to support our claims, see appendix (Section 7.1). Our counterexamples show that GPT-3 fails to solve simplest variations of the original tasks, whereas human performance remains consistently high across all modified versions. \n","title":"Response: Emergent analogical reasoning in large language models","paperID":"303f193c6465","label":"Paper"},"color":"#4CAF50","styles":{},"label":"4354","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":4368,"properties":{"term":"emergent analogical reasoning","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4368","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4369,"properties":{"term":"gpt human performance","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4369","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4370,"properties":{"term":"identify generalize zero","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4370","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4371,"properties":{"term":"shot fashion","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4371","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4372,"properties":{"term":"llms primary reasons","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4372","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4373,"properties":{"term":"domains comparable results","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4373","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4374,"properties":{"term":"webb holyoak lu","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4374","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4375,"properties":{"term":"underlie objection term","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4375","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4376,"properties":{"term":"review file","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4376","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4377,"properties":{"venue":"","filename":"2309.00254v1.json","year":"","abstract":"\n Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective potentially explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic meaning captured by their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving universal attacks could help us gain deeper insight into the internal workings and failure modes of LLMs, thus enabling their mitigation. \n","title":"Why do universal adversarial attacks work on large language models?: Geometry might be the answer","paperID":"18322781ccef","label":"Paper"},"color":"#4CAF50","styles":{},"label":"4377","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":3696,"properties":{"term":"gpt","label":"Keyword"},"color":"#2196F3","styles":{},"label":"3696","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4418,"properties":{"term":"explaining universal adversarial","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4418","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4419,"properties":{"term":"attacks remains largely","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4419","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4420,"properties":{"term":"potentially dangerous input","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4420","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4421,"properties":{"term":"transformer based large","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4421","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4422,"properties":{"term":"white box model","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4422","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4423,"properties":{"term":"new geometric perspective","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4423","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4424,"properties":{"term":"modes llms enabling","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4424","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4425,"properties":{"term":"comprising","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4425","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4591,"properties":{"venue":"","filename":"2309.06589v1.json","year":"","abstract":"\n This paper presents novel systems and methodologies for the development of efficient large language models (LLMs). It explores the trade-offs between model size, performance, and computational resources, with the aim of maximizing the efficiency of these AI systems. The research explores novel methods that allow different parts of the model to share parameters, reducing the total number of unique parameters required. This approach ensures that the model remains compact without sacrificing its ability to learn and represent complex language structures. This study provides valuable insights and tools for creating more efficient and effective LLMs, contributing to a more sustainable and accessible future for AI language modeling. \n","title":"Do Generative Large Language Models need billions of parameters?","paperID":"6d2202851548","label":"Paper"},"color":"#4CAF50","styles":{},"label":"4591","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":4641,"properties":{"term":"ai systems research","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4641","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4642,"properties":{"term":"efficient effective llms","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4642","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4643,"properties":{"term":"share parameters","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4643","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4644,"properties":{"term":"tools creating","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4644","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4645,"properties":{"term":"sacrificing ability learn","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4645","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4646,"properties":{"term":"presents novel","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4646","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4647,"properties":{"term":"total number unique","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4647","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4648,"properties":{"term":"methods allow different","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4648","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4649,"properties":{"term":"remains","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4649","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4794,"properties":{"venue":"","filename":"2309.17072v1.json","year":"","abstract":"\n Large language models (LLMs) are advancing rapidly. Such models have demonstrated strong capabilities in learning from large-scale (unstructured) text data and answering user queries. Users do not need to be experts in structured query languages to interact with systems built upon such models. This provides great opportunities to reduce the barrier of information retrieval for the general public. By introducing LLMs into spatial data management, we envisage an LLM-based spatial database system to learn from both structured and unstructured spatial data. Such a system will offer seamless access to spatial knowledge for the users, thus benefiting individuals, business, and government policy makers alike. \n CCS CONCEPTS â€¢ Information systems â†’ Spatial-temporal systems; Database query processing. \n","title":"MaaSDB: Spatial Databases in the Era of Large Language Models (Vision Paper)","paperID":"933a18534d02","label":"Paper"},"color":"#4CAF50","styles":{},"label":"4794","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":4093,"properties":{"term":"ccs concepts information","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4093","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4823,"properties":{"term":"based spatial database","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4823","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4824,"properties":{"term":"unstructured text data","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4824","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4825,"properties":{"term":"answering user queries","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4825","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4826,"properties":{"term":"llms advancing rapidly","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4826","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4827,"properties":{"term":"business government","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4827","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4828,"properties":{"term":"offer seamless access","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4828","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4829,"properties":{"term":"introducing","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4829","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4830,"properties":{"term":"alike","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4830","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4918,"properties":{"venue":"","filename":"2309.11852v2.json","year":"","abstract":"\n We explore a knowledge sanitization approach to mitigate the privacy concerns associated with large language models (LLMs). LLMs trained on a large corpus of Web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns. Our technique efficiently fine-tunes these models using the Low-Rank Adaptation (LoRA) method, prompting them to generate harmless responses such as \"I don't know\" when queried about specific information. Experimental results in a closed-book question-answering task show that our straightforward method not only minimizes particular knowledge leakage but also preserves the overall performance of LLMs. These two advantages strengthen the defense against extraction attacks and reduces the emission of harmful content such as hallucinations. 1   \n","title":"Knowledge Sanitization of Large Language Models","paperID":"b915a7591a9f","label":"Paper"},"color":"#4CAF50","styles":{},"label":"4918","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":4968,"properties":{"term":"explore knowledge sanitization","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4968","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4969,"properties":{"term":"content hallucinations","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4969","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4970,"properties":{"term":"llms llms","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4970","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4971,"properties":{"term":"leakage preserves overall","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4971","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4972,"properties":{"term":"prompting generate","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4972","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4973,"properties":{"term":"adaptation lora method","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4973","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4974,"properties":{"term":"queried specific information","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4974","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4975,"properties":{"term":"attacks reduces emission","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4975","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":4976,"properties":{"term":"low","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4976","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5654,"properties":{"venue":"","filename":"2310.11237v1.json","year":"","abstract":"\n Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of opensource large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications. 1   \n","title":"Watermarking LLMs with Weight Quantization","paperID":"d24c8bbb15ed","label":"Paper"},"color":"#4CAF50","styles":{},"label":"5654","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":5703,"properties":{"term":"weights avoid malicious","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5703","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5704,"properties":{"term":"watermark open","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5704","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5705,"properties":{"term":"gpt neo llama","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5705","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5706,"properties":{"term":"remains hidden model","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5706","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5707,"properties":{"term":"defined triggers","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5707","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5708,"properties":{"term":"quantized int8","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5708","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5709,"properties":{"term":"licenses opensource","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5709","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5710,"properties":{"term":"speed important","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5710","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5711,"properties":{"term":"proposed method","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5711","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5712,"properties":{"venue":"","filename":"2310.17589v3.json","year":"","abstract":"\n Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models. It allows models to \"cheat\" via memorisation instead of displaying true capabilities. Therefore, contamination analysis has become an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness. This paper presents an extensive data contamination report for over 15 popular large language models across six popular multiple-choice QA benchmarks. We also introduce an open-source pipeline that enables the community to perform contamination analysis on customised data and models. Our experiments reveal varying contamination levels ranging from 1% to 45% across benchmarks, with the contamination degree increasing rapidly over time. Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics: while significant accuracy boosts of up to 14% and 7% are observed on contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is noted on contaminated MMLU. We also find larger models seem able to gain more advantages than smaller models on contaminated test sets. \n","title":"An Open-Source Data Contamination Report for Large Language Models","paperID":"c835ab43fc72","label":"Paper"},"color":"#4CAF50","styles":{},"label":"5712","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":5767,"properties":{"term":"eval hellaswag benchmarks","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5767","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5768,"properties":{"term":"contaminated test","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5768","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5769,"properties":{"term":"developers lacks transparency","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5769","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5770,"properties":{"term":"crucial reliable","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5770","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5771,"properties":{"term":"report 15 popular","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5771","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5772,"properties":{"term":"customised data","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5772","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5773,"properties":{"term":"usually","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5773","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5774,"properties":{"term":"memorisation instead displaying","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5774","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":5775,"properties":{"term":"gain advantages","label":"Keyword"},"color":"#2196F3","styles":{},"label":"5775","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6169,"properties":{"venue":"","filename":"2310.12321v1.json","year":"","abstract":"\n Large language models (LLMs) are a special class of pretrained language models obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI's GPT-3 model, and the popularity of LLMs is increasing exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GPT-3 family large language models. \n","title":"A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4","paperID":"15c4991cc51c","label":"Paper"},"color":"#4CAF50","styles":{},"label":"6169","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":6504,"properties":{"term":"gpt successor openai","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6504","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6505,"properties":{"term":"learning self","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6505","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6506,"properties":{"term":"volumes text data","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6506","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6507,"properties":{"term":"size pretraining","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6507","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6508,"properties":{"term":"llms special class","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6508","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6509,"properties":{"term":"transformers transfer","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6509","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6510,"properties":{"term":"tasks specific domains","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6510","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6511,"properties":{"term":"increasing exponentially","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6511","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6512,"properties":{"term":"paper serve good","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6512","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6783,"properties":{"venue":"","filename":"2311.14876v1.json","year":"","abstract":"\n With the recent advent of Large Language Models (LLMs), such as ChatGPT from OpenAI, BARD from Google, Llama2 from Meta, and Claude from Anthropic AI, gain widespread use, ensuring their security and robustness is critical. The widespread use of these language models heavily relies on their reliability and proper usage of this fascinating technology. It is crucial to thoroughly test these models to not only ensure its quality but also possible misuses of such models by potential adversaries for illegal activities such as hacking. This paper presents a novel study focusing on exploitation of such large language models against deceptive interactions. More specifically, the paper leverages widespread and borrows wellknown techniques in deception theory to investigate whether these models are susceptible to deceitful interactions. This research aims not only to highlight these risks but also to pave the way for robust countermeasures that enhance the security and integrity of language models in the face of sophisticated social engineering tactics. Through systematic experiments and analysis, we assess their performance in these critical security domains. Our results demonstrate a significant finding in that these large language models are susceptible to deception and social engineering attacks. \n","title":"Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles","paperID":"c83504dc650e","label":"Paper"},"color":"#4CAF50","styles":{},"label":"6783","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":6829,"properties":{"term":"deception social engineering","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6829","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6830,"properties":{"term":"robust countermeasures enhance","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6830","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6831,"properties":{"term":"llms chatgpt openai","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6831","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6832,"properties":{"term":"illegal","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6832","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6833,"properties":{"term":"leverages widespread borrows","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6833","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6834,"properties":{"term":"claude anthropic","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6834","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6835,"properties":{"term":"wellknown","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6835","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6836,"properties":{"term":"significant finding","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6836","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6837,"properties":{"term":"reliability proper usage","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6837","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":6972,"properties":{"venue":"","filename":"2312.08642v2.json","year":"","abstract":"\n Few-shot prompting elicits the remarkable abilities of large language models by equipping them with a few demonstration examples in the input. However, the traditional method of providing large language models with all demonstration input-output pairs at once may not effectively guide large language models to learn the specific input-output mapping relationship. In this paper, inspired by the regulatory and supportive role of metacognition in students' learning, we propose a novel metacognition-enhanced few-shot prompting, which guides large language models to reflect on their thought processes to comprehensively learn the given demonstration examples. Furthermore, considering that positive reinforcement can improve students' learning motivation, we introduce positive reinforcement into our metacognition-enhanced few-shot prompting to promote the few-shot learning of large language models by providing response-based positive feedback. The experimental results on two real-world datasets show that our metacognition-enhanced few-shot prompting with positive reinforcement surpasses traditional few-shot prompting in classification accuracy and macro F1. \n","title":"METACOGNITION-ENHANCED FEW-SHOT PROMPTING WITH POSITIVE REINFORCEMENT","paperID":"1377176bf47d","label":"Paper"},"color":"#4CAF50","styles":{},"label":"6972","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":6999,"properties":{"term":"reinforcement metacognition enhanced","label":"Keyword"},"color":"#2196F3","styles":{},"label":"6999","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":7000,"properties":{"term":"equipping demonstration","label":"Keyword"},"color":"#2196F3","styles":{},"label":"7000","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":7001,"properties":{"term":"traditional shot","label":"Keyword"},"color":"#2196F3","styles":{},"label":"7001","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":7002,"properties":{"term":"students","label":"Keyword"},"color":"#2196F3","styles":{},"label":"7002","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":7003,"properties":{"term":"accuracy macro","label":"Keyword"},"color":"#2196F3","styles":{},"label":"7003","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":7004,"properties":{"term":"f1","label":"Keyword"},"color":"#2196F3","styles":{},"label":"7004","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":7005,"properties":{"term":"considering positive","label":"Keyword"},"color":"#2196F3","styles":{},"label":"7005","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":7006,"properties":{"term":"paper inspired regulatory","label":"Keyword"},"color":"#2196F3","styles":{},"label":"7006","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":7007,"properties":{"term":"output pairs","label":"Keyword"},"color":"#2196F3","styles":{},"label":"7007","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":8000,"properties":{"venue":"","filename":"2401.04592v2.json","year":"","abstract":"\n Mental health challenges pose considerable global burdens on individuals and communities. Recent data indicates that more than 20% of adults may encounter at least one mental disorder in their lifetime. On the one hand, the advancements in large language models have facilitated diverse applications, yet a significant research gap persists in understanding and enhancing the potential of large language models within the domain of mental health. On the other hand, across various applications, an outstanding question involves the capacity of large language models to comprehend expressions of human mental health conditions in natural language. This study presents an initial evaluation of large language models in addressing this gap. Due to this, we compare the performance of Llama-2 and ChatGPT with classical Machine as well as Deep learning models. Our results on the DAIC-WOZ dataset show that transformer-based models, like BERT or XLNet, outperform the large language models. \n","title":"An Assessment on Comprehending Mental Health through Large Language Models","paperID":"4258ac2f6eb2","label":"Paper"},"color":"#4CAF50","styles":{},"label":"8000","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":8054,"properties":{"term":"llama chatgpt","label":"Keyword"},"color":"#2196F3","styles":{},"label":"8054","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":8055,"properties":{"term":"deep","label":"Keyword"},"color":"#2196F3","styles":{},"label":"8055","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":8056,"properties":{"term":"xlnet outperform","label":"Keyword"},"color":"#2196F3","styles":{},"label":"8056","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":8057,"properties":{"term":"dataset transformer","label":"Keyword"},"color":"#2196F3","styles":{},"label":"8057","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":8058,"properties":{"term":"mental health hand","label":"Keyword"},"color":"#2196F3","styles":{},"label":"8058","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":8059,"properties":{"term":"considerable global burdens","label":"Keyword"},"color":"#2196F3","styles":{},"label":"8059","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":8060,"properties":{"term":"indicates 20 adults","label":"Keyword"},"color":"#2196F3","styles":{},"label":"8060","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":8061,"properties":{"term":"applications significant","label":"Keyword"},"color":"#2196F3","styles":{},"label":"8061","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":8062,"properties":{"term":"gap persists","label":"Keyword"},"color":"#2196F3","styles":{},"label":"8062","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9078,"properties":{"venue":"","filename":"2403.15230v1.json","year":"","abstract":"\n Does the training of large language models potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of large language models. Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 large language models trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code. Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses. We analyzed a total of 514 million code files, discovering 38 million exact duplicates present in our strong copyleft dataset. Additionally, we examined 171 million file-leading comments, identifying 16 million with strong copyleft licenses and another 11 million comments that discouraged copying without explicitly mentioning a license. Based on the findings of our study, which highlights the pervasive issue of license inconsistencies in large language models trained on code, our recommendation for both researchers and the community is to prioritize the development and adoption of best practices for dataset creation and management. \n","title":"An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets","paperID":"379a771a056c","label":"Paper"},"color":"#4CAF50","styles":{},"label":"9078","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":9150,"properties":{"term":"code recommendation researchers","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9150","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9151,"properties":{"term":"issue license inconsistencies","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9151","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9152,"properties":{"term":"comments discouraged copying","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9152","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9153,"properties":{"term":"safely used training","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9153","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9154,"properties":{"term":"overlap dataset created","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9154","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9155,"properties":{"term":"accomplish compiled list","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9155","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9156,"properties":{"term":"examine publicly","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9156","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9157,"properties":{"term":"file level","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9157","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9158,"properties":{"term":"creation","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9158","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9216,"properties":{"venue":"","filename":"2403.18405v1.json","year":"","abstract":"\n Collecting relevant judgments for legal case retrieval is a challenging and time-consuming task. Accurately judging the relevance between two legal cases requires a considerable effort to read the lengthy text and a high level of domain expertise to extract Legal Facts and make juridical judgments. With the advent of advanced large language models, some recent studies have suggested that it is promising to use LLMs for relevance judgment. Nonetheless, the method of employing a general large language model for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored. To fill this research gap, we devise a novel few-shot workflow tailored to the relevant judgment of legal cases. The proposed workflow breaks down the annotation process into a series of stages, imitating the process employed by human annotators and enabling a flexible integration of expert reasoning to enhance the accuracy of relevance judgments. By comparing the relevance judgments of LLMs and human experts, we empirically show that we can obtain reliable relevance judgments with the proposed workflow. Furthermore, we demonstrate the capacity to augment existing legal case retrieval models through the synthesis of data generated by the large language model. \n CCS CONCEPTS â€¢ Information systems â†’ Specialized information retrieval. \n","title":"Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval","paperID":"120d37859077","label":"Paper"},"color":"#4CAF50","styles":{},"label":"9216","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":9259,"properties":{"term":"legal case retrieval","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9259","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9260,"properties":{"term":"accurately judging relevance","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9260","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9261,"properties":{"term":"workflow breaks annotation","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9261","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9262,"properties":{"term":"experts","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9262","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9263,"properties":{"term":"make juridical","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9263","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9264,"properties":{"term":"use llms","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9264","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9265,"properties":{"term":"enabling flexible integration","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9265","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9266,"properties":{"term":"gap devise novel","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9266","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9267,"properties":{"term":"process series","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9267","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9268,"properties":{"venue":"","filename":"2404.02491v4.json","year":"","abstract":"\n We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12, 383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms. This method further improves large language models to be on par with humans. Given the increasing adoption of large language models in real-world applications, our finding is particularly important and presents a unique direction for future improvements. \n","title":"Measuring Social Norms of Large Language Models","paperID":"7078f4c7968a","label":"Paper"},"color":"#4CAF50","styles":{},"label":"9268","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":9389,"properties":{"term":"set social norm","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9389","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9390,"properties":{"term":"propose multi agent","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9390","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9391,"properties":{"term":"datasets dataset requires","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9391","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9392,"properties":{"term":"chat able","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9392","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9393,"properties":{"term":"slightly human performance","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9393","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9394,"properties":{"term":"12 383 questions","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9394","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9395,"properties":{"term":"gpt3 turbo llama2","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9395","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9396,"properties":{"term":"opinions arguments","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9396","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9397,"properties":{"term":"elementary students prior","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9397","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9398,"properties":{"venue":"","filename":"2403.15673v1.json","year":"","abstract":"\n The capabilities of AI for biomedicine span a wide spectrum, from the atomic level, where it solves partial differential equations for quantum systems, to the molecular level, predicting chemical or protein structures, and further extending to societal predictions like infectious disease outbreaks. Recent advancements in large language models, exemplified by models like ChatGPT, have showcased significant prowess in natural language tasks, such as translating languages, constructing chatbots, and answering questions. When we consider biomedical data, we observe a resemblance to natural language in terms of sequences -biomedical literature and health records presented as text, biological sequences or sequencing data arranged in sequences, or sensor data like brain signals as time series. The question arises: Can we harness the potential of recent large language models to drive biomedical knowledge discoveries? In this tutorial, we will explore the application of large language models to three crucial categories of biomedical data: 1) textual data, 2) biological sequences, and 3) brain signals. Furthermore, we will delve into large language models' challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to multi-modal data representation. \n","title":"AI for Biomedicine in the Era of Large Language Models","paperID":"639de6b897b1","label":"Paper"},"color":"#4CAF50","styles":{},"label":"9398","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":8247,"properties":{"term":"multi modal","label":"Keyword"},"color":"#2196F3","styles":{},"label":"8247","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9471,"properties":{"term":"questions consider biomedical","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9471","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9472,"properties":{"term":"records presented text","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9472","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9473,"properties":{"term":"outbreaks recent advancements","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9473","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9474,"properties":{"term":"signals time series","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9474","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9475,"properties":{"term":"harness","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9475","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9476,"properties":{"term":"including ensuring trustworthiness","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9476","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9477,"properties":{"term":"spectrum atomic level","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9477","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9478,"properties":{"term":"tutorial explore application","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9478","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9479,"properties":{"venue":"","filename":"2404.07544v3.json","year":"","abstract":"\n We analyze how well pre-trained large language models (e.g., Llama2, GPT-4, Claude 3, etc) can do linear and non-linear regression when given in-context examples, without any additional training or gradient updates. Our findings reveal that several large language models (e.g., GPT-4, Claude 3) are able to perform regression tasks with a performance rivaling (or even outperforming) that of traditional supervised methods such as Random Forest, Bagging, or Gradient Boosting. For example, on the challenging Friedman #2 regression dataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM, Random Forest, KNN, or Gradient Boosting. We then investigate how well the performance of large language models scales with the number of in-context exemplars. We borrow from the notion of regret from online learning and empirically show that LLMs are capable of obtaining a sub-linear regret. 1 \n","title":"From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples","paperID":"eedbd5546e10","label":"Paper"},"color":"#4CAF50","styles":{},"label":"9479","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":9578,"properties":{"term":"bagging gradient boosting","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9578","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9579,"properties":{"term":"online learning","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9579","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9580,"properties":{"term":"linear regret","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9580","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9581,"properties":{"term":"llama2 gpt claude","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9581","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9582,"properties":{"term":"forest knn","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9582","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9583,"properties":{"term":"exemplars borrow notion","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9583","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9584,"properties":{"term":"updates findings reveal","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9584","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9585,"properties":{"term":"methods random","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9585","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9586,"properties":{"term":"scales number context","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9586","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9833,"properties":{"venue":"","filename":"2404.11216v2.json","year":"","abstract":"\n The performance of large language models (LLMs) is significantly influenced by the quality of the prompts provided. In response, researchers have developed enormous prompt engineering strategies aimed at modifying the prompt text to enhance task performance. In this paper, we introduce a novel technique termed position engineering, which offers a more efficient way to guide large language models. Unlike prompt engineering, which requires substantial effort to modify the text provided to LLMs, position engineering merely involves altering the positional information in the prompt without modifying the text itself. We have evaluated position engineering in two widely-used LLM scenarios: retrieval-augmented generation (RAG) and in-context learning (ICL). Our findings show that position engineering substantially improves upon the baseline in both cases. Position engineering thus represents a promising new strategy for exploiting the capabilities of large language models. \n","title":"Position Engineering: Boosting Large Language Models through Positional Information Manipulation","paperID":"94179531dda3","label":"Paper"},"color":"#4CAF50","styles":{},"label":"9833","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":179,"properties":{"term":"offers","label":"Keyword"},"color":"#2196F3","styles":{},"label":"179","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9889,"properties":{"term":"prompt text enhance","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9889","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9890,"properties":{"term":"scenarios retrieval augmented","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9890","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9891,"properties":{"term":"task performance","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9891","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9892,"properties":{"term":"llms position engineering","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9892","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9893,"properties":{"term":"rag context","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9893","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9894,"properties":{"term":"termed","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9894","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9895,"properties":{"term":"way guide","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9895","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9896,"properties":{"term":"researchers developed","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9896","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9897,"properties":{"venue":"","filename":"2404.15660v1.json","year":"","abstract":"\n Large language models (LLMs) suffer from the hallucination problem and face significant challenges when applied to knowledge-intensive tasks. A promising approach is to leverage evidence documents as extra supporting knowledge, which can be obtained through retrieval or generation. However, existing methods directly leverage the entire contents of the evidence document, which may introduce noise information and impair the performance of large language models. To tackle this problem, we propose a novel Knowledge Selection of Large Language Models (KS-LLM) method, aiming to identify valuable information from evidence documents. The KS-LLM approach utilizes triples to effectively select knowledge snippets from evidence documents that are beneficial to answering questions. Specifically, we first generate triples based on the input question, then select the evidence sentences most similar to triples from the evidence document, and finally combine the evidence sentences and triples to assist large language models in generating answers. Experimental comparisons on several question answering datasets, such as Triv-iaQA, WebQ, and NQ, demonstrate that the proposed method surpasses the baselines and achieves the best results. \n * Corresponding author Question: What star sign is Jamie Lee Curtis? Evidence Document: Jamie Lee Curtis is an American actress and author. She is the daughter of renowned actors Tony Curtis and Janet Leigh. She was born on November 22, 1958. Curtis gained fame for her roles in ... \n","title":"KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering","paperID":"71e1d2cf3eac","label":"Paper"},"color":"#4CAF50","styles":{},"label":"9897","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":9947,"properties":{"term":"answering questions specifically","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9947","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9948,"properties":{"term":"question select evidence","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9948","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9949,"properties":{"term":"generate triples based","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9949","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9950,"properties":{"term":"author question star","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9950","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9951,"properties":{"term":"triv iaqa","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9951","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9952,"properties":{"term":"document finally combine","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9952","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9953,"properties":{"term":"renowned actors tony","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9953","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9954,"properties":{"term":"leigh born november","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9954","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":9955,"properties":{"term":"llm method aiming","label":"Keyword"},"color":"#2196F3","styles":{},"label":"9955","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10330,"properties":{"venue":"","filename":"2406.13138v2.json","year":"","abstract":"\n This position paper's primary goal is to provoke thoughtful discussion about the relationship between bias and fundamental properties of large language models. I do this by seeking to convince the reader that harmful biases are an inevitable consequence arising from the design of any large language model as LLMs are currently formulated. To the extent that this is true, it suggests that the problem of harmful bias cannot be properly addressed without a serious reconsideration of AI driven by LLMs, going back to the foundational assumptions underlying their design. \n","title":"Large Language Models Are Biased Because They Are Large Language Models","paperID":"319d6bb3ccae","label":"Paper"},"color":"#4CAF50","styles":{},"label":"10330","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":10410,"properties":{"term":"harmful biases inevitable","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10410","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10411,"properties":{"term":"ai driven","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10411","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10412,"properties":{"term":"llms currently formulated","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10412","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10413,"properties":{"term":"arising design large","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10413","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10414,"properties":{"term":"formulated extent true","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10414","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10415,"properties":{"term":"thoughtful discussion relationship","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10415","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10416,"properties":{"term":"fundamental properties","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10416","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10417,"properties":{"term":"position paper primary","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10417","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10418,"properties":{"term":"problem","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10418","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10419,"properties":{"venue":"","filename":"2405.15628v1.json","year":"","abstract":"\n The rapid advancement in Large Language Models has been met with significant challenges in their training processes, primarily due to their considerable computational and memory demands. This research examines parallelization techniques developed to address these challenges, enabling the efficient and scalable training of Large Language Models. A comprehensive analysis of both data and model parallelism strategies, including Fully Sharded Data Parallelism and Distributed Data Parallel frameworks, is provided to assess methods that facilitate efficient model training. Furthermore, the architectural complexities and training methodologies of the Generative Pre-Trained Transformer-2 model are explored. The application of these strategies is further investigated, which is crucial in managing the substantial computational and memory demands of training sophisticated models. This analysis not only highlights the effectiveness of these parallel training strategies in enhancing training efficiency but also their role in enabling the scalable training of large language models. Drawing on recent research findings, through a comprehensive literature review this research underscores the critical role of parallelization techniques in addressing the computational challenges of training state-of-the-art Large Language Models, thereby contributing to the advancement of training more sophisticated and capable artificial intelligence systems. \n","title":"A Comparative Analysis of Distributed Training Strategies for GPT-2","paperID":"5121c50dae37","label":"Paper"},"color":"#4CAF50","styles":{},"label":"10419","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":10484,"properties":{"term":"data parallel","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10484","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10485,"properties":{"term":"pre trained transformer","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10485","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10486,"properties":{"term":"sharded","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10486","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10487,"properties":{"term":"memory demands research","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10487","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10488,"properties":{"term":"efficiency role enabling","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10488","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10489,"properties":{"term":"comprehensive literature review","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10489","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10490,"properties":{"term":"underscores critical","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10490","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10491,"properties":{"term":"drawing recent","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10491","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10492,"properties":{"term":"techniques developed address","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10492","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10670,"properties":{"venue":"","filename":"2406.14171v1.json","year":"","abstract":"\n We conceptualize the process of understanding as information compression, and propose a method for ranking large language models (LLMs) based on lossless data compression. We demonstrate the equivalence of compression length under arithmetic coding with cumulative negative log probabilities when using a large language model as a prior, that is, the pretraining phase of the model is essentially the process of learning the optimal coding length. At the same time, the evaluation metric compression ratio can be obtained without actual compression, which greatly saves overhead. In this paper, we use five large language models as priors for compression, then compare their performance on challenging natural language processing tasks, including sentence completion, question answering, and coreference resolution. Experimental results show that compression ratio and model performance are positively correlated, so it can be used as a general metric to evaluate large language models. \n","title":"Ranking LLMs by compression","paperID":"bc607571682b","label":"Paper"},"color":"#4CAF50","styles":{},"label":"10670","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":4298,"properties":{"term":"llms","label":"Keyword"},"color":"#2196F3","styles":{},"label":"4298","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10862,"properties":{"term":"results compression ratio","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10862","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10863,"properties":{"term":"understanding information","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10863","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10864,"properties":{"term":"negative log probabilities","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10864","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10865,"properties":{"term":"prior pretraining phase","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10865","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10866,"properties":{"term":"greatly saves","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10866","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10867,"properties":{"term":"general metric evaluate","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10867","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10868,"properties":{"term":"challenging natural","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10868","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":10869,"properties":{"term":"overhead paper use","label":"Keyword"},"color":"#2196F3","styles":{},"label":"10869","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11797,"properties":{"venue":"","filename":"2410.16917v2.json","year":"","abstract":"\n There are already many DNA large language models, but most of them still follow traditional uses, such as extracting sequence features for classification tasks. More innovative applications of large language models, such as prompt engineering, RAG, and zero-shot or few-shot prediction, remain challenging for DNA-based models. The key issue lies in the fact that DNA models and human natural language models are entirely separate; however, techniques like prompt engineering require the use of natural language, thereby significantly limiting the application of DNA large language models. This paper introduces a pre-trained model trained on the GPT-2 network, combining DNA sequences and English text, and uses a unified BPE tokenization method. We then convert classification and other downstream tasks into Alpaca format instruction data, and perform instruction fine-tuning on this pre-trained model to create a fine-tuned model capable of handling multiple tasks. The model has demonstrated its effectiveness in DNA related zero-shot prediction and multitask application. This research provides a highly promising direction for building a unified DNA sequence task framework. \n","title":"DNAHLM -DNA sequence and Human Language mixed large language Model","paperID":"2e11253293e7","label":"Paper"},"color":"#4CAF50","styles":{},"label":"11797","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":11825,"properties":{"term":"introduces pre trained","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11825","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11826,"properties":{"term":"format instruction data","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11826","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11827,"properties":{"term":"bpe tokenization","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11827","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11828,"properties":{"term":"convert classification downstream","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11828","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11829,"properties":{"term":"sequence","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11829","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11830,"properties":{"term":"demonstrated effectiveness dna","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11830","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11831,"properties":{"term":"zero shot shot","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11831","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11832,"properties":{"term":"handling multiple tasks","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11832","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11833,"properties":{"term":"gpt network combining","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11833","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11897,"properties":{"venue":"","filename":"2411.00897v1.json","year":"","abstract":"\n Although large language models perform well in understanding and responding to user intent, their performance in specialized domains such as Traditional Chinese Medicine (TCM) remains limited due to lack of expertise. In addition, high-quality data related to TCM is scarce and difficult to obtain, making large language models ineffective in handling TCM tasks. In this work, we propose a framework to improve the performance of large language models for TCM tasks using only a small amount of data. First, we use medical case data for supervised fine-tuning of the large model, making it initially capable of performing TCM tasks. Subsequently, we further optimize the model's performance using reinforcement learning from AI feedback (RLAIF) to align it with the preference data. The ablation study also demonstrated the performance gain is attributed to both supervised fine-tuning and the direct policy optimization. The experimental results show that the model trained with a small amount of data achieves a significant performance improvement on a representative TCM task. \n","title":"Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback","paperID":"7feec3243648","label":"Paper"},"color":"#4CAF50","styles":{},"label":"11897","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":11945,"properties":{"term":"learning ai feedback","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11945","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11946,"properties":{"term":"chinese medicine tcm","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11946","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11947,"properties":{"term":"user intent performance","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11947","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11948,"properties":{"term":"data ablation study","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11948","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11949,"properties":{"term":"specialized domains","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11949","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11950,"properties":{"term":"tasks work propose","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11950","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11951,"properties":{"term":"obtain making large","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11951","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11952,"properties":{"term":"align preference data","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11952","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":11953,"properties":{"term":"lack","label":"Keyword"},"color":"#2196F3","styles":{},"label":"11953","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":12973,"properties":{"venue":"","filename":"2502.01754v1.json","year":"","abstract":"\n State of the art large language models rely on randomization to respond to a prompt. As an immediate consequence, a model may respond differently to the same prompt if asked multiple times. In this work, we argue that the evaluation and ranking of large language models should control for the randomization underpinning their functioning. Our starting point is the development of a causal model for coupled autoregressive generation, which allows different large language models to sample responses with the same source of randomness. Building upon our causal model, we first show that, on evaluations based on benchmark datasets, coupled autoregressive generation leads to the same conclusions as vanilla autoregressive generation but using provably fewer samples. However, we further show that, on evaluations based on (human) pairwise comparisons, coupled and vanilla autoregressive generation can surprisingly lead to different rankings when comparing more than two models, even with an infinite amount of samples. This suggests that the apparent advantage of a model over others in existing evaluation protocols may not be genuine but rather confounded by the randomness inherent to the generation process. To illustrate and complement our theoretical results, we conduct experiments with several large language models from the Llama family. We find that, across multiple knowledge areas from the popular MMLU benchmark dataset, coupled autoregressive generation requires up to 40% fewer samples to reach the same conclusions as vanilla autoregressive generation. Further, using data from the LMSYS Chatbot Arena platform, we find that the win-rates derived from pairwise comparisons by a strong large language model to prompts differ under coupled and vanilla autoregressive generation. 1 Tokens are the units that make up sentences and paragraphs, e.g., (sub-)words, numbers, and special end-of-sequence tokens. \n","title":"Evaluation of Large Language Models via Coupled Token Generation","paperID":"d8cf4e8086f3","label":"Paper"},"color":"#4CAF50","styles":{},"label":"12973","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":13071,"properties":{"term":"randomness inherent generation","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13071","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13072,"properties":{"term":"coupled autoregressive","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13072","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13073,"properties":{"term":"differently prompt asked","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13073","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13074,"properties":{"term":"derived pairwise comparisons","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13074","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13075,"properties":{"term":"surprisingly lead","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13075","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13076,"properties":{"term":"llama family","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13076","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13077,"properties":{"term":"requires 40 fewer","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13077","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13078,"properties":{"term":"using data","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13078","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13079,"properties":{"term":"arena platform","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13079","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13080,"properties":{"venue":"","filename":"2501.06286v1.json","year":"","abstract":"\n In recent years, the use of large language models (LLMs) has significantly increased, and these models have demonstrated remarkable performance in a variety of general language tasks. However, the evaluation of their performance in domain-specific tasks, particularly those requiring deep natural language understanding, has received less attention. In this research, we evaluate the ability of large language models in performing domain-specific tasks, focusing on the multi-hop question answering (MHQA) problem using the HotpotQA dataset. This task, due to its requirement for reasoning and combining information from multiple textual sources, serves as a challenging benchmark for assessing the language comprehension capabilities of these models. To tackle this problem, we have designed a two-stage selector-reader architecture, where each stage utilizes an independent LLM. In addition, methods such as Chain of Thought (CoT) and question decomposition have been employed to investigate their impact on improving the model's performance. The results of the study show that the integration of large language models with these techniques can lead to up to a 4% improvement in F1 score for finding answers, providing evidence of the models' ability to handle domain-specific tasks and their understanding of complex language. \n","title":"Bactrainus: Optimizing Large Language Models for Multihop Complex Question Answering Tasks","paperID":"459cb7680349","label":"Paper"},"color":"#4CAF50","styles":{},"label":"13080","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":13144,"properties":{"term":"domain specific tasks","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13144","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13145,"properties":{"term":"stage selector reader","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13145","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13146,"properties":{"term":"cot question decomposition","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13146","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13147,"properties":{"term":"problem using hotpotqa","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13147","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13148,"properties":{"term":"sources serves","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13148","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13149,"properties":{"term":"utilizes independent llm","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13149","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13150,"properties":{"term":"hop","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13150","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13151,"properties":{"term":"focusing multi","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13151","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13152,"properties":{"term":"research evaluate ability","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13152","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13153,"properties":{"venue":"","filename":"2411.19275v3.json","year":"","abstract":"\n Large language models have demonstrated impressive capabilities in generating code, yet they often produce programs with flaws or deviations from intended behavior, limiting their suitability for safety-critical applications. To address this limitation, this paper introduces VECOGEN, a novel tool that combines large language models with formal verification to automate the generation of formally verified C programs. VECOGEN takes a formal specification in ANSI/ISO C Specification Language, a natural language specification, and a set of test cases to attempt to generate a verified program. This program-generation process consists of two steps. First, VECOGEN generates an initial set of candidate programs. Secondly, the tool iteratively improves on previously generated candidates. If a candidate program meets the formal specification, then we are sure the program is correct. We evaluate VECOGEN on 15 problems presented in Codeforces competitions. On these problems, VECOGEN solves 13 problems. This work shows the potential of combining large language models with formal verification to automate program generation. \n","title":"VeCoGen: Automating Generation of Formally Verified C Code with Large Language Models","paperID":"38ae57ada0c0","label":"Paper"},"color":"#4CAF50","styles":{},"label":"13153","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":13216,"properties":{"term":"automate program generation","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13216","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13217,"properties":{"term":"iso specification language","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13217","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13218,"properties":{"term":"safety critical","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13218","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13219,"properties":{"term":"previously generated candidates","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13219","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13220,"properties":{"term":"correct evaluate vecogen","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13220","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13221,"properties":{"term":"takes formal","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13221","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13222,"properties":{"term":"limitation paper","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13222","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13223,"properties":{"term":"13","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13223","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13224,"properties":{"term":"applications address","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13224","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13700,"properties":{"venue":"","filename":"2412.10291v1.json","year":"","abstract":"\n My paper Talking About Large Language Models has more than once been interpreted as advocating a reductionist stance towards large language models. But the paper was not intended that way, and I do not endorse such positions. This short note situates the paper in the context of a larger philosophical project that is concerned with the (mis)use of words rather than metaphysics, in the spirit of Wittgenstein's later writing. \n","title":"Still \"Talking About Large Language Models\": Some Clarifications","paperID":"c671acd070b7","label":"Paper"},"color":"#4CAF50","styles":{},"label":"13700","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":13711,"properties":{"term":"larger philosophical","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13711","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13712,"properties":{"term":"metaphysics spirit wittgenstein","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13712","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13713,"properties":{"term":"models paper intended","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13713","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13714,"properties":{"term":"advocating reductionist stance","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13714","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13715,"properties":{"term":"mis use words","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13715","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13716,"properties":{"term":"note situates paper","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13716","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13717,"properties":{"term":"project concerned","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13717","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13718,"properties":{"term":"positions short","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13718","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":13719,"properties":{"term":"way","label":"Keyword"},"color":"#2196F3","styles":{},"label":"13719","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]}],"_overview":{"enabled":null,"overview_set":false},"_selected_graph":[[{"id":106,"properties":{"term":"large language models","label":"Keyword"}}],[]],"_sidebar":{"enabled":false,"start_with":null},"_view_count":null,"_view_module":"yfiles-jupyter-graphs","_view_module_version":"^1.10.7","_view_name":"GraphView","layout":"IPY_MODEL_178e0d38f1fa45389394f4a7ec4c6dd7"}},"f50ae02f615142b094c8c9454dbd0e58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"800px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"178e0d38f1fa45389394f4a7ec4c6dd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"800px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}}}}},"nbformat":4,"nbformat_minor":5}