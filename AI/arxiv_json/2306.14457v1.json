{
  "title": "Fauno: The Italian Large Language Model that will leave you senza parole!",
  "authors": [
    "Andrea Bacciu",
    "Giovanni Trappolini",
    "Andrea Santilli",
    "Emanuele Rodolà",
    "Fabrizio Silvestri"
  ],
  "abstract": "\n This paper presents Fauno, the first and largest open-source Italian conversational Large Language Model (LLM). Our goal with Fauno is to democratize the study of LLMs in Italian, demonstrating that obtaining a fine-tuned conversational bot with a single GPU is possible. In addition, we release a collection of datasets for conversational AI in Italian. The datasets on which we fine-tuned Fauno include various topics such as general question answering, computer science, and medical questions. We release our code and datasets on  https://github.com/RSTLess-research/Fauno-Italian-LLM   \n",
  "references": [
    {
      "id": null,
      "title": "Fauno: The Italian Large Language Model that will leave you senza parole!",
      "authors": [
        "Andrea Bacciu",
        "Giovanni Trappolini",
        "Andrea Santilli",
        "Emanuele Rodolà",
        "Fabrizio Silvestri"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Optimizing language models for dialogue",
      "authors": [
        "Chatgpt Openai"
      ],
      "year": "2023",
      "venue": "Optimizing language models for dialogue",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "OpenAI, Gpt-4 technical report",
      "authors": [],
      "year": "2023",
      "venue": "OpenAI, Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Baize: An open-source chat model with parameterefficient tuning on self-chat data",
      "authors": [
        "C Xu",
        "D Guo",
        "N Duan",
        "J Mcauley"
      ],
      "year": "2023",
      "venue": "Baize: An open-source chat model with parameterefficient tuning on self-chat data",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar",
        "A Rodriguez",
        "A Joulin",
        "E Grave",
        "G Lample"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Do we name the languages we study? the# benderrule in lrec and acl articles",
      "authors": [
        "F Ducel",
        "K Fort",
        "G Lejeune",
        "Y Lepage"
      ],
      "year": "2022",
      "venue": "Do we name the languages we study? the# benderrule in lrec and acl articles",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Polosukhin, Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A N Gomez",
        "Ł Kaiser",
        "I"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P J Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Multimodal neural databases",
      "authors": [
        "G Trappolini",
        "A Santilli",
        "E Rodolà",
        "A Halevy",
        "F Silvestri"
      ],
      "year": "2023",
      "venue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Unifying cross-lingual semantic role labeling with heterogeneous linguistic resources",
      "authors": [
        "S Conia",
        "A Bacciu",
        "R Navigli"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Jukebox: A generative model for music",
      "authors": [
        "P Dhariwal",
        "H Jun",
        "C Payne",
        "J W Kim",
        "A Radford",
        "I Sutskever"
      ],
      "year": "2020",
      "venue": "Jukebox: A generative model for music",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Musiclm: Generating music from text",
      "authors": [
        "A Agostinelli",
        "T I Denk",
        "Z Borsos",
        "J Engel",
        "M Verzetti",
        "A Caillon",
        "Q Huang",
        "A Jansen",
        "A Roberts",
        "M Tagliasacchi"
      ],
      "year": "2023",
      "venue": "Musiclm: Generating music from text",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Cycledrums: automatic drum arrangement for bass lines using cyclegan",
      "authors": [
        "G Barnabò",
        "G Trappolini",
        "L Lastilla",
        "C Campagnano",
        "A Fan",
        "F Petroni",
        "F Silvestri"
      ],
      "year": "2023",
      "venue": "Discover Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Point transformer",
      "authors": [
        "H Zhao",
        "L Jiang",
        "J Jia",
        "P H Torr",
        "V Koltun"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Shape registration in the time of transformers",
      "authors": [
        "G Trappolini",
        "L Cosmo",
        "L Moschella",
        "R Marin",
        "S Melzi",
        "E Rodolà"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Towards precise completion of deformable shapes",
      "authors": [
        "O Halimi",
        "I Imanuel",
        "O Litany",
        "G Trappolini",
        "E Rodolà",
        "L Guibas",
        "R Kimmel"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Self-attentive sequential recommendation",
      "authors": [
        "W.-C Kang",
        "J Mcauley"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on data mining (ICDM)",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Integrating item relevance in training loss for sequential recommender systems",
      "authors": [
        "A Bacciu",
        "F Siciliano",
        "N Tonellotto",
        "F Silvestri"
      ],
      "year": "2023",
      "venue": "Integrating item relevance in training loss for sequential recommender systems",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J D Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "R Bommasani",
        "D A Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M S Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill"
      ],
      "year": "2021",
      "venue": "On the opportunities and risks of foundation models",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Training compute-optimal large language models",
      "authors": [
        "J Hoffmann",
        "S Borgeaud",
        "A Mensch",
        "E Buchatskaya",
        "T Cai",
        "E Rutherford",
        "D D L Casas",
        "L A Hendricks",
        "J Welbl",
        "A Clark"
      ],
      "year": "2022",
      "venue": "Training compute-optimal large language models",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Dialogpt: Large-scale generative pre-training for conversational response generation",
      "authors": [
        "Y Zhang",
        "S Sun",
        "M Galley",
        "Y.-C Chen",
        "C Brockett",
        "X Gao",
        "J Gao",
        "J Liu",
        "B Dolan"
      ],
      "year": "2019",
      "venue": "Dialogpt: Large-scale generative pre-training for conversational response generation",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Towards a human-like open-domain chatbot",
      "authors": [
        "D Adiwardana",
        "M.-T Luong",
        "D R So",
        "J Hall",
        "N Fiedel",
        "R Thoppilan",
        "Z Yang",
        "A Kulshreshtha",
        "G Nemade",
        "Y Lu"
      ],
      "year": "2020",
      "venue": "Towards a human-like open-domain chatbot",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Lamda: Language models for dialog applications",
      "authors": [
        "R Thoppilan",
        "D De Freitas",
        "J Hall",
        "N Shazeer",
        "A Kulshreshtha",
        "H.-T Cheng",
        "A Jin",
        "T Bos",
        "L Baker",
        "Y Du"
      ],
      "year": "2022",
      "venue": "Lamda: Language models for dialog applications",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Geppetto carves italian into a language model",
      "authors": [
        "L D Mattei",
        "M Cafagna",
        "F Dell'orletta",
        "M Nissim",
        "M Guerini"
      ],
      "year": "2020",
      "venue": "Proceedings of the Seventh Italian Conference on Computational Linguistics, CLiC-it 2020",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "The wacky wide web: a collection of very large linguistically processed web-crawled corpora",
      "authors": [
        "M Baroni",
        "S Bernardini",
        "A Ferraresi",
        "E Zanchetta"
      ],
      "year": "2009",
      "venue": "Language resources and evaluation",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Large-scale text-to-text pretraining for italian language understanding and generation",
      "authors": [
        "G Sarti",
        "M Nissim"
      ],
      "year": "2022",
      "venue": "It",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "A massively multilingual pre-trained text-to-text transformer",
      "authors": [
        "L Xue",
        "N Constant",
        "A Roberts",
        "M Kale",
        "R Al-Rfou",
        "A Siddhant",
        "A Barua",
        "C Raffel"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.41"
    },
    {
      "id": "b31",
      "title": "Bart-it: An efficient sequence-to-sequence model for italian text summarization",
      "authors": [
        "M La Quatra",
        "L Cagliero"
      ],
      "year": "2023",
      "venue": "Future Internet",
      "doi": "10.3390/fi15010015"
    },
    {
      "id": "b32",
      "title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.703"
    },
    {
      "id": "b33",
      "title": "Camoscio: An italian instruction-tuned llama",
      "authors": [
        "A Santilli"
      ],
      "year": "2023",
      "venue": "Camoscio: An italian instruction-tuned llama",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Stanford alpaca: An instruction-following llama model",
      "authors": [
        "R Taori",
        "I Gulrajani",
        "T Zhang",
        "Y Dubois",
        "X Li",
        "C Guestrin",
        "P Liang",
        "T B Hashimoto"
      ],
      "year": "2023",
      "venue": "Stanford alpaca: An instruction-following llama model",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Parameter-efficient transfer learning for NLP",
      "authors": [
        "N Houlsby",
        "A Giurgiu",
        "S Jastrzebski",
        "B Morrone",
        "Q De Laroussilhe",
        "A Gesmundo",
        "M Attariyan",
        "S Gelly"
      ],
      "year": "2019",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
      "authors": [
        "E B Zaken",
        "Y Goldberg",
        "S Ravfogel"
      ],
      "year": "2022",
      "venue": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "authors": [
        "X L Li",
        "P Liang"
      ],
      "year": "2021",
      "venue": "ACL-IJCNLP",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
      "authors": [
        "X Liu",
        "K Ji",
        "Y Fu",
        "W Tam",
        "Z Du",
        "Z Yang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-short.8"
    },
    {
      "id": "b39",
      "title": "The power of scale for parameter-efficient prompt tuning",
      "authors": [
        "B Lester",
        "R Al-Rfou",
        "N Constant"
      ],
      "year": "2021",
      "venue": "The power of scale for parameter-efficient prompt tuning",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Low-rank adaptation of large language models",
      "authors": [
        "E J Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen",
        "Lora"
      ],
      "year": "2022",
      "venue": "Low-rank adaptation of large language models",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "A survey on efficient training of transformers",
      "authors": [
        "B Zhuang",
        "J Liu",
        "Z Pan",
        "H He",
        "Y Weng",
        "C Shen"
      ],
      "year": "2023",
      "venue": "A survey on efficient training of transformers",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "A question-entailment approach to question answering",
      "authors": [
        "A Ben Abacha",
        "D Demner-Fushman"
      ],
      "year": "2019",
      "venue": "BMC bioinformatics",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Is chatgpt a good translator? yes with gpt-4 as the engine",
      "authors": [
        "W Jiao",
        "W Wang",
        "J Huang",
        "X Wang",
        "Z Tu"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good translator? yes with gpt-4 as the engine",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Fauno: The Italian Large Language Model That Will Leave You Senza Parole!",
      "text": "Andrea Bacciu1 Giovanni Trappolini1 Andrea Santilli2 Emanuele Rodola2 and Fabrizio Silvestri1 1Sapienza University - Department of Computer, Control and Management Engineering 2Sapienza University - Computer Science Department"
    },
    {
      "title": "Abstract",
      "text": "This paper presents Fauno, the first and largest open-source Italian conversational Large Language Model (LLM). Our goal with Fauno is to democatize the study of LLMs in Italian, demonstrating that obtaining a fine-tuned conversational bot with a single GPU is possible. In addition, we release a collection of datasets for conversational AI in Italian. The datasets on which we fine-tuned Fauno include various topics such as general question answering, computer science, and medical questions. We release our code and datasets on [https://github.com/RSTLess-research/Fauno-Italian-LLM](https://github.com/RSTLess-research/Fauno-Italian-LLM) L 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 40 International (CC BY 4.0)."
    },
    {
      "title": "1 Introduction",
      "text": "Recently, there has been significant progress in enhancing the capabilities of natural language processing (NLP) methods. Notably, ChatGPT [1] and GPT-4 [2] have emerged as highly proficient chat models. These models exhibit an impressive aptitude for comprehending and producing human-like responses across various domains. Consequently, they have gained popularity in customer support, virtual assistants, and social media moderation applications. However, these models are often only accessible through restricted APIs, which hampers new research and development. Additionally, the limited availability of chat models creates challenges for researchers and practitioners, impeding the growth of the NLP community. This problem is further aggravated by the scarcity of publicly accessible and high-quality chat datasets for analyzing and improving these models. Recently, Baize [3], whose depiction can be seen in Figure 3, has been introduced to tackle these challenges effectively, utilizing the capabilities of ChatGPT to generate a top-notch chat dataset comprising multiple conversational turns automatically. Their approach involves employing ChatGPT to engage in a simulated dialogue with itself, mimicking both user and AI responses. This generated dataset is a valuable resource for training and evaluating chat models in the context of multi-turn conversations. Additionally, by specifying an initial dataset, they extract samples from specific domains and fine-tune chat models to specialize in technology or healthcare. To fine-tune Large LanguageModels (LLM) with limited resources, they adopt an efficient tuning technique that maximizes the utilization of available computational capabilities. This strategy allows state-of-the-art language models to adapt and perform well in resource-constrained scenarios. Their primary objective is to enhance an open-source LLM, LLaMA [4], which they believe has the potential to serve as an accessible alternative to proprietary chat models. By fine-tuning LLaMA with their generated chat dataset, they introduce a new model called Baize (Bai ze), inspired by a legendary creature from Chinese folklore known for its linguistic prowess and vast knowledge. Baize reaches incredible performance; however, as with many other models in this category, its performance heavily depends on the language considered [5]. In particular, these models tend to prefer the English language since this is the language they have seen the most during their training. Their performance in other languages, such as Italian, is vastly inferior, as it can be easily checked empirically by merely using these tools. For this reason, inspired by Latin mythology, in this paper, we introduce Fauno, the first and the largest open-source Italian conversational LLM. To build Fauno, we translated several datasets originally used by Baize in Italian. We use them to fine-tune and train our model, reaching good generation scores, as highlighted in later sections of this paper. Our goal is twofold. First, we want to provide the Italian (and the international) research community with an LLM that is highly proficient in Italian. We hope this will enhance and enable new research on this particular language by surpassing the current state-of-the-art capabilities of Italian. Second, we want to provide practitioners with a valid conversational AI able to deal with the Italian language. We provide an example for this latter case to hint at this tool's potential. Italy has one of the largest populations of elderly people. In the last twenty years, the number of people older than eighty has doubled, according to the Italian statistical entity (ISTAT). Furthermore, roughly 1.2 million people are older than 65 and live alone. These people are known in the medical literature to suffer from loneliness, which aggravates their health issues. With tools like Fauno, however, one could offer a conversational AI as a telehealth service. While not being a substitute for a human, this could improve the quality of life of millions of people and reduce the weight of the national health system. This is just one example of applications, and we hope our model can spark many such projects and research. Fauno can be operational on accessible hardware, thus expanding accessibility to a broader range of practitioners and researchers."
    },
    {
      "title": "2 Related Work",
      "text": ""
    },
    {
      "title": "Language Models For Chat",
      "text": "Transformers [6], introduced initially as a machine translation system, have had an arguably unprecedented impact on the AI world. Transformers and deep learning models, in general, are now used not only for NLP [7, 8, 9, 10], but also other fields such as image processing [11], audio [12, 13, 14], 3D data [15, 16, 17], recommendation systems [18, 19]. Language Models (LMs), based on the transformer architecture, have gained significant attention in Natural Language Processing due to their ability to produce coherent human-like text. Radford et al. [20] showed that LMs could learn a variety of tasks without direct human supervision but using self-supervision on large, noisy web corpora. Several works have developed this idea, further scaling the data and model size [21, 22, 23]. However, all these models are not explicitly designed with a conversational objective or training curriculum. Zhang et al. [24] introduced DialoGPT, which is a fine-tuned version of GPT-2 on Reddit data; Adiwardana et al. [25] proposed Meena, a 2.6B model trained with conversational data from social media; [26] proposed LaMDA, a 137B model trained on public dialog data. Recently OpenAI released ChatGPT [1], a model optimized for conversational interaction trained with Reinforcement Learning with Human Feedback [27]. With GPT-4 [2], this approach was further improved with additional reasoning capabilities. Recently, Xu et al. [3] introduced Baize as a finetuned version of LLaMA [4] for conversation by training the model on dialogs generated by ChatGPT. Our approach stems from this work and extends it to the Italian language."
    },
    {
      "title": "Language Models For Italian",
      "text": "To the best of our knowledge, no LMs are trained explicitly with a conversational objective or training curriculum for the Italian language. The currently available LMs for Italian include GePpeTto[28], a GPT-2 model finetuned on Italian Wikipedia and the ItWac corpus [29]; IT5 [30], an Italian version of T5 trained on a cleaned version of the Italian mC4 corpus[31]; BART-IT [32], an Italian version of BART[33] trained on the same data mixture of IT5; Camoscio [34], an Italian version of Stanford Alpaca [35] trained on the Alpaca dataset translated to Italian."
    },
    {
      "title": "Parameter-Efficient Tuning",
      "text": "Typical fine-tuning involves training all parameters of an LLM, which can be inefficient as the number of parameters increases. Various techniques have been proposed to address this issue under the umbrella term of Parameter-Efficient Fine-Tuning (PEFT). These techniques include Adapters[36], which adds a trainable layer inside the Transformer while freezing the others; BitFit[37], which only tunes bias terms in the linear layers; Prompting approaches like Prefix Tuning [38], P-Tuning [39] or Prompt Tuning [40] which fine-tune the optimal prompt parameters for a task; or LoRA [41] which trains low-rank matrices while keeping all the remaining parameters frozen. This is a very active research area, and we refer the reader to Zhuang et al. [42] for an up-to-date overview. Recently the community leveraged these techniques to train low-cost versions of Stanford Alpaca [35] under the name of Alpaca-LoRA1. Footnote 1: [https://github.com/tloen/alpaca-lora](https://github.com/tloen/alpaca-lora)"
    },
    {
      "title": "3 Conversational Ai Datasets In Italian",
      "text": "In this section, we report the data we use to fine-tune Fauno and the data generation and translation process. We train Fauno on extensive Italian synthetic datasets, encompassing a wide range of fields such as medical data (MedQuad [43]), technical content from StackOverflow, Quora discussions, and Alpaca [35] data translated into Italian with 47.000, 57.000, 54.000 and, 22.000 dialogues, respectively. The synthetic generation of the data is described in Section 3.1. To produce conversational AI Datasets, we inherit Xu et al. 2023 [3]'s dataset generation methodology. This methodology consists of allowing ChatGPT to engage in self-chatting to produce a conversation between a user and a helpful AI assistant in a conversational format. This self-chatting process is initiated with a question (or seed) from real users. For example, [3] takes as seed questions from Quora2 and Stack Overflow3. The generation methodology is shown in Figure 3. Footnote 2: [https://huggingface.co/datasets/quora](https://huggingface.co/datasets/quora) Footnote 3: [https://huggingface.co/datasets/pacovaldez/stackoverflow-questions](https://huggingface.co/datasets/pacovaldez/stackoverflow-questions) Footnote 4: [https://huggingface.co/datasets/andreabac3/StackOverflow-Italian-Fauno-Baize](https://huggingface.co/datasets/andreabac3/StackOverflow-Italian-Fauno-Baize) Footnote 5: [https://huggingface.co/datasets/andreabac3/MedQuaAD-Italian-Fauno-Baize](https://huggingface.co/datasets/andreabac3/MedQuaAD-Italian-Fauno-Baize) Footnote 6: [https://huggingface.co/datasets/andreabac3/Quora-Italian-Fauno-Baize](https://huggingface.co/datasets/andreabac3/Quora-Italian-Fauno-Baize) Footnote 7: [https://huggingface.co/datasets/teelinsan/camoscio_cleaned](https://huggingface.co/datasets/teelinsan/camoscio_cleaned)"
    },
    {
      "title": "Translation",
      "text": "To translate Fauno's data, we use two different translation systems to maximize translation accuracy and quality. Jiao et al. (2023) [44] have demonstrated that Google Translate is state-of-the-art among commercial language and programming code. To address this limitation, we opted to utilize ChatGPT 3.5 to translate datasets containing programming code, as it is better suited for preserving the integrity of the code's syntax and structure. Hence, we translate Alpaca and StackOverflow with ChatGPT 3.5 and the remaining datasets with Google Translate. We release the translated dataset of StackOverflow4, MedQuad5, Quora6 and Alpaca7 in the Huggingface hub. The cost of translating the StackOverflow dataset using the ChatGPT-3.5 API is $80, while the Alpaca Dataset translation costs around $30. For other datasets, we utilize Google's free translation API8. Footnote 4: [https://pypi.org/project/googletrans/](https://pypi.org/project/googletrans/) Footnote 5: [https://huggingface.co/datasets/andreabac3/MedQuaAD-Italian-Fauno-Baize](https://huggingface.co/datasets/andreabac3/MedQuaAD-Italian-Fauno-Baize) Footnote 6: [https://huggingface.co/datasets/andreabac3/Quora-Italian-Fauno-Baize](https://huggingface.co/datasets/andreabac3/Quora-Italian-Fauno-Baize) Footnote 7: [https://huggingface.co/datasets/teelinsan/camoscio_cleaned](https://huggingface.co/datasets/teelinsan/camoscio_cleaned)"
    },
    {
      "title": "4 Train Setup",
      "text": "We use the same hyper-parameters of [3] with a single epoch and batch size of 128 examples. We perform our experiments on a workstation equipped with an Intel Core i9-10940X (14-core CPU running at 3.3GHz) and 256GB of RAM, and a single Nvidia RTX A6000 with 48GB of VRAM. The training time for the 7B and 13B models are 19 and 35 hours, respectively."
    },
    {
      "title": "5 Evaluation",
      "text": "As done by Xu et al. 2023 [3] and Camoscio [34], we rely on a qualitative analysis of Fauno's outputs. In Table 1, we compare ChatGPT 3.5, Camoscio, and Fauno."
    },
    {
      "title": "6 Conclusion",
      "text": "This work shows that it is possible to democratize the research for LLMs in multiple languages with limited resources. Furthermore, we believe that the release of the Italian datasets for conversational AI will help researchers to develop further Large Language Models. \\begin{table} \\begin{tabular}{l l} \\hline \\hline **Model** & **Answer** \\\\ \\hline **Question** & **Qual e il senso della vita?** \\\\ \\hline ChatGPT & La domanda sul senso della vita e stata oggetto di riflessione filosofica e teologica per secoli, \\\\ & e non esiste una risposta definitiva o concorde. Tuttavia, molti individui trovano significato \\\\ & e scopo nella vita attraverso diverse vie. Alcuni cercano il senso nella connessione umana e \\\\ & nelle relazioni significative, altri nella realizzazione personale e nello sviluppo delle proprie \\\\ & passioni. Altri ancora si rivolgono alla spiritualita o alla fede per trovare un senso piu \\\\ & profondo. In generale, il senso della vita puo essere un’esperienza altamente individuale e \\\\ & soggettiva, e ognuno potrebbe trovare una risposta diversa a questa domanda. \\\\ \\hline Camoscio & Il senso della vita e trovare la felicita e la soddisfazione nella vita. \\\\ \\hline Fauno & Il senso della vita e una domanda popolare e complessa che ha suscitato dibattito in molte \\\\ & culture e generazioni. Alla fine, la comprensione di qual e il senso della vita dipende dalla \\\\ & prospettiva e dalle convinzioni personali di ogni singolo individuo. Alcune persone possono \\\\ &vedere il senso della vita in termini di un significato unico e globale, mentre altri possono \\\\ & sforzarsi di risolvere il mistero della vita trovando autentico benessere e piacere nei loro \\\\ & giorni quotidiani. \\\\ \\hline **Question** & **Come posso copiare un file da un server tramite ssh?** \\\\ \\hline ChatGPT & Per copiare un file da un server tramite SSH, puoi utilizzare il comando \"scp\" (Secure Copy). \\\\ & Ecco come puoi farlo: Apri una finestra di terminale sul tuo computer locale. Assicurati di \\\\ & avere il client SSH installato sul tuo sistema. Se stai utilizzando un sistema operativo basato \\\\ & su Unix (ad esempio Linux o macOS), lo avrai probabilmente gia installato. Se stai utilizz- \\\\ & zando Windows, puoi utilizzare un client SSH come PuTTY o installare un’implementazione \\\\ & di SSH come OpenSSH. Utilizza il seguente comando \"scp\" per copiare il file dal server \\\\ & al tuo computer locale: scp username@indirizzo\\_server:percorso\\_del\\_file\\_locale per- \\\\ & corso\\_di\\_destinazon \\\\ \\hline Camoscio & Per copiare un file da un server tramite ssh, e necessario utilizzare l’opzione \"scp\" \\\\ & dell’utente ssh. Per esempio, per copiare un file dal server \"server.example.com\" \\\\ & all’utente \"utente.example.com\", l’utente dovrebbe utilizzare l’opzione \"scp \\\\ & server.example.com:utente.example.com\". \\\\ \\hline Fauno & Per copiare un file da un server tramite ssh, puoi utilizzare il seguente comando: \\\\ & scp username@ip\\_delto_server:percorso/della/sorgente/del/file /percorso/della/destinazione/del/file \\\\ & sostituisci i dettagli con il tuo nome utente, il nome dell'host, il percorso della sorgente e il \\\\ & percorso finale del file e clicca su \"Avanti\". Il file verra copiato dal server alla tua marchina. \\\\ & C\\(\\,\\)\\({}^{\\circ}\\) qualcos’altro di cui hai bisogno? \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Qualitative analysis of ChatGPT-3.5, Camoscio, and Fauno 7B"
    },
    {
      "title": "Ack",
      "text": "This work was partially supported by projects FAIR (PE0000013) and SERICS (PE00000014) under the MUR National Recovery and Resilience Plan funded by the European Union - NextGenerationEU and by ERC Starting Grant No. 802554 (SPECGEO) and PRIN 2020 project n.2020TA3K9N \"LEGO.AI\". Supported by the ERC Advanced Grant 788893 AMDROMA, EC H2020RIA project \"SoBigData++\" (871042), PNRR MUR project IR0000013-SoBigData.it."
    },
    {
      "title": "References",
      "text": "* [1] OpenAI, Chatgpt: Optimizing language models for dialogue, 2023. URL: [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/). * [2] OpenAI, Gpt-4 technical report, 2023. arXiv:2303.08774. * [3] C. Xu, D. Guo, N. Duan, J. McAuley, Baize: An open-source chat model with parameter-efficient tuning on self-chat data, arXiv preprint arXiv:2304.01196 (2023). * [4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, G. Lample, Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971 (2023). * [5] F. Ducel, K. Fort, G. Lejeune, Y. Lepage, Do we name the languages we study? the# benderrule in lrec and acl articles, in: Proceedings of the Thirteenth Language Resources and Evaluation Conference, 2022, pp. 564-573. * [6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information processing systems 30 (2017). * [7] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805 (2018). * [8] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, The Journal of Machine Learning Research 21 (2020) 5485-5551. * [9] G. Trappolini, A. Santilli, E. Rodola, A. Halevy, F. Silvestri, Multimodal neural databases, in: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2023. * [10] S. Conia, A. Bacciu, R. Navigli, Unifying cross-lingual semantic role labeling with heterogeneous linguistic resources, in: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 338-351. * [11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929 (2020). * [12] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, I. Sutskever, Jukebox: A generative model for music, arXiv preprint arXiv:2005.00341 (2020). * [13] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen,A. Roberts, M. Tagliasacchi, et al., Musiclm: Generating music from text, arXiv preprint arXiv:2301.11325 (2023). * Barnabo et al. [2023] G. Barnabo, G. Trappolini, L. Lastilla, C. Campagnano, A. Fan, F. Petroni, F. Silvestri, Cycledrums: automatic drum arrangement for bass lines using cyclegan, Discover Artificial Intelligence 3 (2023) 4. * Zhao et al. [2021] H. Zhao, L. Jiang, J. Jia, P. H. Torr, V. Koltun, Point transformer, in: Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 16259-16268. * Trappolini et al. [2021] G. Trappolini, L. Cosmo, L. Moschella, R. Marin, S. Melzi, E. Rodola, Shape registration in the time of transformers, Advances in Neural Information Processing Systems 34 (2021) 5731-5744. * Halimi et al. [2020] O. Halimi, I. Imanuel, O. Litany, G. Trappolini, E. Rodola, L. Guibas, R. Kimmel, Towards precise completion of deformable shapes, in: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIV 16, Springer, 2020, pp. 359-377. * Kang and McAuley [2018] W.-C. Kang, J. McAuley, Self-attentive sequential recommendation, in: 2018 IEEE international conference on data mining (ICDM), IEEE, 2018, pp. 197-206. * Bacciu et al. [2023] A. Bacciu, F. Siciliano, N. Tonellotto, F. Silvestri, Integrating item relevance in training loss for sequential recommender systems, arXiv preprint arXiv:2305.10824 (2023). * Radford et al. [2019] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., Language models are unsupervised multitask learners, OpenAI blog 1 (2019) 9. * Brown et al. [2020] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models are few-shot learners, Advances in neural information processing systems 33 (2020) 1877-1901. * Bommasani et al. [2021] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., On the opportunities and risks of foundation models, arXiv preprint arXiv:2108.07258 (2021). * Hoffmann et al. [2022] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al., Training compute-optimal large language models, arXiv preprint arXiv:2203.15556 (2022). * Zhang et al. [2019] Y. Zhang, S. Sun, M. Galley, Y.-C. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, B. Dolan, Dialogpt: Large-scale generative pre-training for conversational response generation, arXiv preprint arXiv:1911.00536 (2019). * Adiwardana et al. [2020] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan, Z. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, et al., Towards a human-like open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). * Thoppilan et al. [2022] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al., Lamda: Language models for dialog applications, arXiv preprint arXiv:2201.08239 (2022). * Ouyang et al. [2022] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language models to follow instructions with human feedback, Advances in Neural Information Processing Systems 35 (2022) 27730-27744. * Mattei et al. [2020] L. D. Mattei, M. Cafagna, F. Dell'Orletta, M. Nissim, M. Guerini, Geppetto carves italian into a language model, in: J. Monti, F. Dell'Orletta, F. Tamburini (Eds.), Proceedings of the Seventh Italian Conference on Computational Linguistics, CLiC-it 2020, Bologna, Italy,March 1-3, 2021, volume 2769 of _CEUR Workshop Proceedings_, CEUR-WS.org, 2020. URL: [https://ceur-ws.org/Vol-2769/paper_46.pdf](https://ceur-ws.org/Vol-2769/paper_46.pdf). * Baroni et al. [2009] M. Baroni, S. Bernardini, A. Ferraresi, E. Zanchetta, The wacky wide web: a collection of very large linguistically processed web-crawled corpora, Language resources and evaluation 43 (2009) 209-226. * Sarti and Nissim [2022] G. Sarti, M. Nissim, It5: Large-scale text-to-text pretraining for italian language understanding and generation, 2022. arXiv:2203.03759. * Xue et al. [2021] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, C. Raffel, mT5: A massively multilingual pre-trained text-to-text transformer, in: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, Online, 2021, pp. 483-498. URL: [https://aclanthology.org/2021.naacl-main.41](https://aclanthology.org/2021.naacl-main.41). doi:10.18653/v1/2021.naacl-main.41. * La Quatra and Cagliero [2023] M. La Quatra, L. Cagliero, Bart-it: An efficient sequence-to-sequence model for italian text summarization, Future Internet 15 (2023). URL: [https://www.mdpi.com/1999-5903/15/1/15](https://www.mdpi.com/1999-5903/15/1/15). doi:10.3390/fi15010015. * Lewis et al. [2020] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, L. Zettlemoyer, BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online, 2020, pp. 7871-7880. URL: [https://aclanthology.org/2020.acl-main.703](https://aclanthology.org/2020.acl-main.703). doi:10.18653/v1/2020.acl-main.703. * Santilli [2023] A. Santilli, Camoscio: An italian instruction-tuned llama, [https://github.com/teelinsan/camoscio](https://github.com/teelinsan/camoscio), 2023. * Taori et al. [2023] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, T. B. Hashimoto, Stanford alpaca: An instruction-following llama model, [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023. * Houlsby et al. [2019] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, S. Gelly, Parameter-efficient transfer learning for NLP, in: ICML, volume 97 of _Proceedings of Machine Learning Research_, PMLR, 2019, pp. 2790-2799. * Zaken et al. [2022] E. B. Zaken, Y. Goldberg, S. Ravfogel, Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models, in: ACL, Association for Computational Linguistics, 2022, pp. 1-9. * Li and Liang [2021] X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for generation, in: ACL-IJCNLP, Association for Computational Linguistics, 2021, pp. 4582-4597. * Liu et al. [2022] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, J. Tang, P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks, in: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Association for Computational Linguistics, Dublin, Ireland, 2022, pp. 61-68. URL: [https://aclanthology.org/2022.acl-short.8](https://aclanthology.org/2022.acl-short.8). doi:10.18653/v1/2022.acl-short.8. * Lester and Constantin [2021] B. Lester, R. Al-Rfou, N. Constant, The power of scale for parameter-efficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). * Hu et al. [2022] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, Lora: Low-rank adaptation of large language models, in: ICLR, OpenReview.net, 2022. * [42] B. Zhuang, J. Liu, Z. Pan, H. He, Y. Weng, C. Shen, A survey on efficient training of transformers, ArXiv abs/2302.01107 (2023). * [43] A. Ben Abacha, D. Demner-Fushman, A question-entailment approach to question answering, BMC bioinformatics 20 (2019) 1-23. * [44] W. Jiao, W. Wang, J. tse Huang, X. Wang, Z. Tu, Is chatgpt a good translator? yes with gpt-4 as the engine, 2023."
    }
  ]
}