{
  "title": "Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics",
  "authors": [
    "Conor Houghton",
    "Nina Kazanina",
    "Priyanka Sukumaran"
  ],
  "abstract": "\n Large language models are not detailed models of human linguistic processing. They are, however, extremely successful at their primary task: providing a model for language. For this reason and because there are no animal models for language, large language models are important in psycholinguistics: they are useful as a practical tool, as an illustrative comparative, and philosophically, as a basis for recasting the relationship between language and thought. \n",
  "references": [
    {
      "id": null,
      "title": "Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics",
      "authors": [
        "Conor Houghton",
        "Nina Kazanina",
        "Priyanka Sukumaran"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Using deep neural networks to learn syntactic agreement",
      "authors": [
        "P Bernardy",
        "S Lappin"
      ],
      "year": "2017",
      "venue": "Linguistic Issues in Language Technology",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Deep problems with neural network models of human vision",
      "authors": [
        "J S Bowers",
        "G Malhotra",
        "M DujmoviÄ‡",
        "M L Montero",
        "C Tsvetkov",
        "V Biscione",
        "G Puebla",
        "F Adolfi",
        "J E Hummel",
        "R F Heaton"
      ],
      "year": "2022",
      "venue": "Behavioral and Brain Sciences",
      "doi": "10.1017/S0140525X22002813"
    },
    {
      "id": "b2",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J D Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Cartesian linguistics: A chapter in the history of rationalist thought",
      "authors": [
        "N Chomsky"
      ],
      "year": "1966",
      "venue": "Cartesian linguistics: A chapter in the history of rationalist thought",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Frequency and predictability effects on event-related potentials during reading",
      "authors": [
        "M Dambacher",
        "R Kliegl",
        "M Hofmann",
        "A M Jacobs"
      ],
      "year": "2006",
      "venue": "Brain Research",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Automatic and attentional processes in the effects of sentence contexts on word recognition",
      "authors": [
        "I Fischler",
        "P A Bloom"
      ],
      "year": "1979",
      "venue": "Journal of Verbal Learning and Verbal Behavior",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "The ERP response to the amount of information conveyed by words in sentences",
      "authors": [
        "S L Frank",
        "L J Otten",
        "G Galli",
        "G Vigliocco"
      ],
      "year": "2015",
      "venue": "Brain and Language",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Colorless green recurrent networks dream hierarchically",
      "authors": [
        "K Gulordava",
        "P Bojanowski",
        "E Grave",
        "T Linzen",
        "M Baroni"
      ],
      "year": "2018",
      "venue": "Colorless green recurrent networks dream hierarchically",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Sentence frame contexts and lexical decisions: Sentence-acceptability and wordrelatedness effects",
      "authors": [
        "G M Kleiman"
      ],
      "year": "1980",
      "venue": "Memory & Cognition",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "The perils of natural behaviour tests for unnatural models: the case of number agreement. Poster presented at Learning Language in Humans and in Machines",
      "authors": [
        "A Kuncoro",
        "C Dyer",
        "J Hale",
        "P Blunsom"
      ],
      "year": "2018",
      "venue": "Fr",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "The importance of shape in early lexical learning",
      "authors": [
        "B Landau",
        "L B Smith",
        "S S Jones"
      ],
      "year": "1988",
      "venue": "Cognitive Development",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Distinct patterns of syntactic agreement errors in recurrent networks and humans",
      "authors": [
        "T Linzen",
        "B Leonard"
      ],
      "year": "2018",
      "venue": "Distinct patterns of syntactic agreement errors in recurrent networks and humans",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
      "authors": [
        "T Linzen",
        "E Dupoux",
        "Y Goldberg"
      ],
      "year": "2016",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Targeted syntactic evaluation of language models",
      "authors": [
        "R Marvin",
        "T Linzen"
      ],
      "year": "2018",
      "venue": "Targeted syntactic evaluation of language models",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Do LSTMs know about Principle C?",
      "authors": [
        "J Mitchell",
        "N Kazanina",
        "C Houghton",
        "J Bowers"
      ],
      "year": "2019",
      "venue": "2019 Conference on Cognitive Computational Neuroscience",
      "doi": "10.32470/CCN.2019.1241-0"
    },
    {
      "id": "b16",
      "title": "Effects of contextual constraint on eye movements in reading: A further examination",
      "authors": [
        "K Rayner",
        "A D Well"
      ],
      "year": "1996",
      "venue": "Psychonomic Bulletin & Review",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Do LSTMs see gender? Probing the ability of LSTMs to learn abstract syntactic rules",
      "authors": [
        "P Sukumaran",
        "C Houghton",
        "N Kazanina"
      ],
      "year": "2022",
      "venue": "Do LSTMs see gender? Probing the ability of LSTMs to learn abstract syntactic rules",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Computing machinery and intelligence",
      "authors": [
        "A M Turing"
      ],
      "year": "1950",
      "venue": "Mind",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Beyond The Limitations Of Any Imaginable Mechanism: Large Language Models And Psycholinguistics",
      "text": "Conor Houghton Department of Computer Science, University of Bristol, UK Nina Kazanina,\\({}^{\\ddagger}\\) and Priyanka Sukumaran\\({}^{\\ddagger}\\) Department of Computer Science, University of Bristol, UK School of Psychological Sciences, University of Bristol, UK International Laboratory of Social Neurobiology, Institute for Cognitive Neuroscience, National Research University Higher School of Economics, HSE University, Moscow, Russia."
    },
    {
      "title": "Abstract",
      "text": "Large language models are not detailed models of human linguistic processing. They are, however, extremely successful at their primary task: providing a model for language. For this reason and because there are no animal models for language, large language models are important in psycholinguistics: they are useful as a practical tool, as an illustrative comparative, and philosophically, as a basis for recasting the relationship between language and thought."
    },
    {
      "title": "1 This Is A Commentary On Bowers Et Al. (2022).",
      "text": "Neural-network models of language are optimized to solve practical problems such as machine translation. Currently, when these large language models (LLMs) are interpreted as models of human linguistic processing they have similar shortcomings to those that deep neural network have as models of human vision. Two examples can illustrate this. First, LLMs do not faithfully replicate human behaviour on language tasks (Marvin and Linzen, 2018; Kuncoro et al., 2018; Linzen and Leonard, 2018; Mitchell et al., 2019). For example, an LLM trained on a word-prediction task shows similar error rates to humans overall on long-range subject-verb number agreement but errs in different circumstances: unlike humans, it makes more mistakes when sentences have relative clauses (Linzen and Leonard, 2018), indicating differences in how grammatical structure is represented. Second, the LLMs with better performance on language tasks do not necessarily have more in common with human linguistic processing or more obvious similarities to the brain. For example, Transformers learn efficiently on vast corpora and avoid human-like memory constraints but are currently more successful as language models than recurrent neural networks such as the Long-Short-Term-Memory LLMs (Devlin et al., 2018; Brown et al., 2020), which employ sequential processing, as humans do, and can be more easily compared to the brain. Furthermore, the target article suggests that, more broadly, the brain and neural networks are unlikely to resemble each other because evolution differs in trajectory and outcome from the optimization used to train a neural network. Generally, there is an unanswered question about which aspects of learning in LLMs are to be compared to the evolution of our linguistic ability and which to language learning in infants but in either case, the comparison seems weak. LMMs are typically trained using a next-word prediction task; it is unlikely our linguistic ability evolved to optimize this and next-word prediction can only partly describe language learning: for example, infants generalize word meanings based on shape (Landau et al., 1988) while LLMs lack any broad conceptual encounter with the world language describes. In fact, it would be peculiar to suggest that LLMs are models of the neural dynamics that support linguistic processing in humans; we simply know too little about those dynamics. The challenge presented by language is different to that presented by vision: language lacks animal models and debate in psycholinguistics is occupied with broad issues of mechanisms and principles, whereas visual neuroscience often has more detailed concerns. We believe that LLMs have a valuable role in psycholinguistics and this does not depend on any precise mapping from machine to human. Here we describe three uses of LLMs: **(1)** the **practical**, as a tool in experimentation; **(2)** the **comparative**, as an alternate example of linguistic processing and **(3)** the **philosophical**, recasting the relationship between language and thought. **(1)**: An LLM models language and this is often of **practical** quantitative utility in experiment. One straight-forward example is the evaluation of _surprisal_: how well a word is predicted by what has preceded it. It has been established that reaction times, (Fischler and Bloom, 1979; Kleiman, 1980), gaze duration, (Rayner and Well, 1996), and EEG responses, (Dambacher et al., 2006; Frank et al., 2015), are modulated by surprisal, giving an insight into prediction in neural processing. In the past, surprisal was evaluated using \\(n\\)-grams, but \\(n\\)-grams become impossible to estimate as \\(n\\) grows and as such they cannot quantify long-range dependencies. LLMs are typically trained on a task akin to quantifying surprisal and are superior to \\(n\\)-grams in estimating word probabilities. Differences between LLM-derived estimates and neural perception of surprisal may quantify which linguistic structures, perhaps poorly represented in the statistical evidence, the brain privileges during processing. **(2)**: LLMs are also useful as a point of **comparison**. LLMs combine different computational strategies, mixing representations of word properties with a computational engine based on memory or attention. Despite the clear differences between LLMs and the brain, it is instructive to compare the performance of different LLMs on language tasks to our own language ability. For example, although LLMs are capable of long range number and gender agreement, (Linzen et al., 2016; Gulordava et al., 2018; Bernardy and Lappin, 2017; Sukumaran et al., 2022), they are not successful in implementing another long-range rule: Principle C, (Mitchell et al., 2019), a near-universal property of languages which depends in its most straight-forward description on hierarchical parsing. Thus, LLMs allow us to recognize those aspects of language which require special consideration while revealing others to be within easy reach of statistical learning. **(3)**: In the past, **philosophical** significance was granted to language as evidence of thought or personhood. Turing (1950), for example, proposes conversation as a proxy for thought and Chomsky (1966) describes Descartes as attributing the possession of mind to other humans because the human capacity for innovation and for the creative use of language, is 'beyond the limitations of any imaginable mechanism'. It is significant that machines are now capable of imitating the use of language. While machine-generated text still has attributes of awkwardness and repetition that make it recognizable on careful reading, it would seem foolhardy to predict these final quirks are unresolvable or are characteristic of the division between human and machine. Nonetheless, most of us appear to feel intuitively that LLMs enact an imitation rather than a recreation of our linguistic ability: LLMs seem empty things whose pantomime of language is not underpinned by thought, understanding or creativity. Indeed, even if an LLM were capable of imitating us perfectly, we would still distinguish between a loved one and their simulation. This is a challenge to our understanding of the relationship between language and thought: either we must claim that, despite recent progress, machine-generated language will remain unlike human language in vital respects, or we must defy our intuition and consider machines to be as capable of thought as we are, or we must codify our intuition to specify why a machine able to produce language should, nonetheless, be considered lacking in thought."
    },
    {
      "title": "References",
      "text": "* Bernardy and Lappin (2017) J.-P. Bernardy and S. Lappin. Using deep neural networks to learn syntactic agreement. _Linguistic Issues in Language Technology_, 2017. * Bowers et al. (2022) J. S. Bowers, G. Malhotra, M. Dujmovic, M. L. Montero, C. Tsvetkov, V. Biscione, G. Puebla, F. Adolfi, J. E. Hummel, R. F. Heaton, and et al. Deep problems with neural network models of human vision. _Behavioral and Brain Sciences_, page 1-74, 2022. doi.org/10.1017/S0140525X22002813. * Brown et al. (2020) T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in Neural Information Processing Systems_, 33:1877-1901, 2020. * Chomsky (1966) N. Chomsky. _Cartesian linguistics: A chapter in the history of rationalist thought_. Cambridge University Press, 1966. * Dambacher et al. (2006) M. Dambacher, R. Kliegl, M. Hofmann, and A. M. Jacobs. Frequency and predictability effects on event-related potentials during reading. _Brain Research_, 1084(1):89-103, 2006. * Devlin et al. (2018) J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. _arXiv:1810.04805_, 2018. * Fischler and Bloom (1979) I. Fischler and P. A. Bloom. Automatic and attentional processes in the effects of sentence contexts on word recognition. _Journal of Verbal Learning and Verbal Behavior_, 18(1):1-20, 1979. * Frank et al. (2015) S. L. Frank, L. J. Otten, G. Galli, and G. Vigliocco. The ERP response to the amount of information conveyed by words in sentences. _Brain and Language_, 140:1-11, 2015. * Gulordava et al. (2018) K. Gulordava, P. Bojanowski, E. Grave, T. Linzen, and M. Baroni. Colorless green recurrent networks dream hierarchically. _arXiv:1803.11138_, 2018. * Kleiman (1980) G. M. Kleiman. Sentence frame contexts and lexical decisions: Sentence-acceptability and word-relatedness effects. _Memory & Cognition_, 8(4):336-344, 1980. * Kuncoro et al. (2018) A. Kuncoro, C. Dyer, J. Hale, and P. Blunsom. The perils of natural behaviour tests for unnatural models: the case of number agreement. _Poster presented at Learning Language in Humans and in Machines, Paris, Fr., July_, 5(6), 2018. * Landau et al. (1988) B. Landau, L. B. Smith, and S. S. Jones. The importance of shape in early lexical learning. _Cognitive Development_, 3(3):299-321, 1988. * Linzen and Leonard (2018) T. Linzen and B. Leonard. Distinct patterns of syntactic agreement errors in recurrent networks and humans. _arXiv:1807.06882_, 2018. * Linzen et al. (2016) T. Linzen, E. Dupoux, and Y. Goldberg. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. _Transactions of the Association for Computational Linguistics_, 4:521-535, 2016. * Marvin and Linzen (2018) R. Marvin and T. Linzen. Targeted syntactic evaluation of language models. _arXiv:1808.09031_, 2018. * Mitchell et al. (2019) J. Mitchell, N. Kazanina, C. Houghton, and J. Bowers. Do LSTMs know about Principle C? In _2019 Conference on Cognitive Computational Neuroscience_, 2019. doi.org/10.32470/CCN.2019.1241-0. * Rayner and Well (1996) K. Rayner and A. D. Well. Effects of contextual constraint on eye movements in reading: A further examination. _Psychonomic Bulletin & Review_, 3(4):504-509, 1996. * Sukumaran et al. (2022) P. Sukumaran, C. Houghton, and N. Kazanina. Do LSTMs see gender? Probing the ability of LSTMs to learn abstract syntactic rules. _arXiv:2211.00153_, 2022. * Turing (1950) A. M. Turing. Computing machinery and intelligence. _Mind_, 49:433-460, 1950. * Toutanova et al. (2018)"
    }
  ]
}