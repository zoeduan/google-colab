{
  "title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners",
  "authors": [
    "Nicholas Crispino",
    "Kyle Montgomery",
    "Fankun Zeng",
    "Dawn Song",
    "Chenguang Wang",
    "R Anil",
    "Firat, O A M Dai",
    "K Mishra",
    "Tworek, J M Chen",
    "Oliveira Pinto",
    "Khlaaf, H G Sastry",
    "Chan, B Gray",
    "Fuhrimann, F F Altay",
    "Shinzato, L M H De Bykhovetz",
    "Pàmies, M M Takeuchi",
    "Nezhurina, M., Sänger, M., Samwald, M M A Castillo",
    "Mihaljcic, M M Liu",
    "Freidank, M M Kang",
    "Mollo, D. C D Yang",
    "Erdem, E E Chang",
    "Zheltonozhskii, E., Xia, F., Siar, F F Martínez-Plumed",
    "Winata, G. I De Melo",
    "G Kruszewski",
    "G Wang",
    "Jaimovitch - López",
    "G Betz",
    "Zhang, L L Dugan",
    "Tolkiehn, M., Giulianelli, M M J R Lewis",
    "Kale, M M Cain"
  ],
  "abstract": "\n We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. To enable this, our agent only needs to generate a single set of instructions for each task. These instructions turn out to be extremely effective for improving the reasoning process of different large language models across all task instances. We show this approach further unleashes the zeroshot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b, Llama-2-70b-chat, and GPT-3.5 Turbo. Compared to zero-shot chain of thought, our improvement in reasoning is striking. With our method, Llama-2-70b-chat outperforms zeroshot GPT-3.5 Turbo significantly. \n",
  "references": [
    {
      "id": null,
      "title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners",
      "authors": [
        "Nicholas Crispino",
        "Kyle Montgomery",
        "Fankun Zeng",
        "Dawn Song",
        "Chenguang Wang",
        "R Anil",
        "A M Dai",
        "K Mishra",
        "M Chen",
        "Oliveira Pinto",
        "G Sastry",
        "Gray",
        "F Altay",
        "M H De Bykhovetz",
        "M Takeuchi",
        "M A Castillo",
        "M Liu",
        "M Kang",
        "D Yang",
        "E Chang",
        "F Martínez-Plumed",
        "De Melo",
        "G Kruszewski",
        "G Wang",
        "Jaimovitch - López",
        "G Betz",
        "L Dugan",
        "M J R Lewis",
        "M Cain"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "RAFT: A Real-World Few-Shot Text Classification Benchmark",
      "authors": [
        "N Alex",
        "E Lifland",
        "L Tunstall",
        "A Thakur",
        "P Maham",
        "C Riedel",
        "E Hine",
        "C Ashurst",
        "P Sedille",
        "A Carlier",
        "M Noetel",
        "A Stuhlmüller"
      ],
      "year": "2021",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Language Models as Agent Models",
      "authors": [
        "J Andreas",
        "S Ding",
        "J Shang",
        "S Wang",
        "Y Sun",
        "H Tian",
        "H Wu",
        "H Wang",
        "Ernie-Doc"
      ],
      "year": "2021",
      "venue": "A Retrospective Long-Document Modeling Transformer",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "",
      "authors": [
        "Q Dong",
        "L Li",
        "D Dai",
        "C Zheng",
        "Z Wu",
        "B Chang",
        "X Sun",
        "J Xu",
        "L Li",
        "Sui"
      ],
      "year": "2023",
      "venue": "Z. A Survey on In-context Learning. arXiv",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "PaLM-E: An Embodied Multimodal Language Model",
      "authors": [
        "D Driess",
        "F Xia",
        "M S M Sajjadi",
        "C Lynch",
        "A Chowdhery",
        "B Ichter",
        "A Wahid",
        "J Tompson",
        "Q Vuong",
        "T Yu",
        "W Huang",
        "Y Chebotar",
        "P Sermanet",
        "D Duckworth",
        "S Levine",
        "V Vanhoucke",
        "K Hausman",
        "M Toussaint",
        "K Greff",
        "A Zeng",
        "I Mordatch",
        "P Florence"
      ],
      "year": "2023",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "",
      "authors": [
        "X Geng",
        "A Gudibande",
        "H Liu",
        "E Wallace",
        "P Abbeel",
        "S Levine",
        "D Song",
        "Koala"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
      "authors": [
        "M Geva",
        "D Khashabi",
        "E Segal",
        "T Khot",
        "D Roth",
        "J Berant"
      ],
      "year": "2021",
      "venue": "TACL",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "",
      "authors": [
        "D Hendrycks",
        "C Burns",
        "S Basart",
        "A Zou",
        "M Mazeika",
        "D Song",
        "Steinhardt"
      ],
      "year": "2021",
      "venue": "J. Measuring Massive Multitask Language Understanding. In ICLR",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "",
      "authors": [
        "J Hoffmann",
        "S Borgeaud",
        "A Mensch",
        "E Buchatskaya",
        "T Cai",
        "E Rutherford",
        "D De Las Casas",
        "L A Hendricks",
        "J Welbl",
        "A Clark",
        "T Hennigan",
        "E Noland",
        "K Millican",
        "G Van Den Driessche",
        "B Damoc",
        "A Guy",
        "S Osindero",
        "K Simonyan",
        "E Elsen",
        "J W Rae",
        "O Vinyals",
        "L Sifre"
      ],
      "year": "2022",
      "venue": "Training Compute-Optimal Large Language Models. arXiv",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Universal Language Model Finetuning for Text Classification",
      "authors": [
        "J Howard",
        "S Ruder"
      ],
      "year": "2018",
      "venue": "ACL",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
      "authors": [
        "C.-Y Hsieh",
        "C.-L Li",
        "C.-K Yeh",
        "H Nakhost",
        "Y Fujii",
        "A Ratner",
        "R Krishna",
        "C.-Y Lee",
        "T Pfister"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Low-Rank Adaptation of Large Language Models",
      "authors": [
        "E J Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen",
        "Lora"
      ],
      "year": "2022",
      "venue": "ICLR",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Grasping Flow in History for Conversational Machine Comprehension",
      "authors": [
        "H Huang",
        "E Choi",
        "W Yih",
        "Flowqa"
      ],
      "year": "2019",
      "venue": "ICLR",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
      "authors": [
        "B Lester",
        "R Al-Rfou",
        "N Constant"
      ],
      "year": "2021",
      "venue": "EMNLP",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
      "authors": [
        "X L Li",
        "P Liang"
      ],
      "year": "2021",
      "venue": "ACL",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Holistic Evaluation of Language Models",
      "authors": [
        "P Liang",
        "R Bommasani",
        "T Lee",
        "D Tsipras",
        "D Soylu",
        "M Yasunaga",
        "Y Zhang",
        "D Narayanan",
        "Y Wu",
        "A Kumar",
        "B Newman",
        "B Yuan",
        "B Yan",
        "C Zhang",
        "C Cosgrove",
        "C D Manning",
        "C Ré",
        "D Acosta-Navas",
        "D A Hudson",
        "E Zelikman",
        "E Durmus",
        "F Ladhak",
        "F Rong",
        "H Ren",
        "H Yao",
        "J Wang",
        "K Santhanam",
        "L Orr",
        "L Zheng",
        "M Yuksekgonul",
        "M Suzgun",
        "N Kim",
        "N Guha",
        "N Chatterji",
        "O Khattab",
        "P Henderson",
        "Q Huang",
        "R Chi",
        "S M Xie",
        "S Santurkar",
        "S Ganguli",
        "T Hashimoto",
        "T Icard",
        "T Zhang",
        "V Chaudhary",
        "W Wang",
        "X Li",
        "Y Mai",
        "Y Zhang",
        "Y Koreeda"
      ],
      "year": "2023",
      "venue": "TMLR",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "ROUGE: A Package for Automatic Evaluation of Summaries",
      "authors": [
        "C.-Y Lin"
      ],
      "year": "2004",
      "venue": "ACL",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Measuring How Models Mimic Human Falsehoods",
      "authors": [
        "S Lin",
        "J Hilton",
        "O Evans",
        "Truthfulqa"
      ],
      "year": "2022",
      "venue": "ACL",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
      "authors": [
        "W Ling",
        "D Yogatama",
        "C Dyer",
        "P Blunsom"
      ],
      "year": "2017",
      "venue": "ACL",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
      "authors": [
        "P Liu",
        "W Yuan",
        "J Fu",
        "Z Jiang",
        "H Hayashi",
        "G Neubig"
      ],
      "year": "2023",
      "venue": "ACM Comput. Surv",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov",
        "Roberta"
      ],
      "year": "2019",
      "venue": "A Robustly Optimized BERT Pretraining Approach",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Learning Word Vectors for Sentiment Analysis",
      "authors": [
        "A L Maas",
        "R E Daly",
        "P T Pham",
        "D Huang",
        "A Y Ng",
        "C Potts"
      ],
      "year": "2011",
      "venue": "ACL",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Simplified Data Wrangling with ir datasets",
      "authors": [
        "S Macavaney",
        "A Yates",
        "S Feldman",
        "D Downey",
        "A Cohan",
        "N Goharian"
      ],
      "year": "2021",
      "venue": "SIGIR",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
      "authors": [
        "T Mihaylov",
        "P Clark",
        "T Khot",
        "A Sabharwal"
      ],
      "year": "2018",
      "venue": "EMNLP",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
      "authors": [
        "S Narayan",
        "S B Cohen",
        "M Lapata"
      ],
      "year": "2018",
      "venue": "EMNLP",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Finetuned Language Models are Zero-Shot Learners",
      "authors": [
        "J Wei",
        "M Bosma",
        "V Y Zhao",
        "K Guu",
        "A W Yu",
        "B Lester",
        "N Du",
        "A M Dai",
        "Q V Le"
      ],
      "year": "",
      "venue": "Finetuned Language Models are Zero-Shot Learners",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "B Ichter",
        "F Xia",
        "E H Chi",
        "Q V Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "A Large Language Model for Finance. arXiv",
      "authors": [
        "S Wu",
        "O Irsoy",
        "S Lu",
        "V Dabravolski",
        "M Dredze",
        "S Gehrmann",
        "P Kambadur",
        "D Rosenberg",
        "G Mann",
        "Bloomberggpt"
      ],
      "year": "2023",
      "venue": "A Large Language Model for Finance. arXiv",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
      "authors": [
        "Z Xi",
        "W Chen",
        "X Guo",
        "W He",
        "Y Ding",
        "B Hong",
        "M Zhang",
        "J Wang",
        "S Jin",
        "E Zhou",
        "R Zheng",
        "X Fan",
        "X Wang",
        "L Xiong",
        "Y Zhou",
        "W Wang",
        "C Jiang",
        "Y Zou",
        "X Liu",
        "Z Yin",
        "S Dou",
        "R Weng",
        "W Cheng",
        "Q Zhang",
        "W Qin",
        "Y Zheng",
        "X Qiu",
        "X Huang",
        "Gui",
        "T"
      ],
      "year": "2023",
      "venue": "The Rise and Potential of Large Language Model Based Agents: A Survey",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Instructing Large Language Models to be Distinguished Experts. arXiv",
      "authors": [
        "B Xu",
        "A Yang",
        "J Lin",
        "Q Wang",
        "C Zhou",
        "Y Zhang",
        "Z Mao",
        "Expertprompting"
      ],
      "year": "2023",
      "venue": "Instructing Large Language Models to be Distinguished Experts. arXiv",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention",
      "authors": [
        "Y Xu",
        "C Zhu",
        "S Wang",
        "S Sun",
        "H Cheng",
        "X Liu",
        "J Gao",
        "P He",
        "M Zeng",
        "X Huang"
      ],
      "year": "2022",
      "venue": "IJCAI",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": [
        "S Yao",
        "D Yu",
        "J Zhao",
        "I Shafran",
        "T L Griffiths",
        "Y Cao",
        "K Narasimhan"
      ],
      "year": "2023",
      "venue": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": [
        "S Yao",
        "J Zhao",
        "D Yu",
        "N Du",
        "I Shafran",
        "K Narasimhan",
        "Y Cao"
      ],
      "year": "",
      "venue": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Measuring social norms of large language models",
      "authors": [
        "Y Yuan",
        "K Tang",
        "J Shen",
        "M Zhang",
        "C Wang"
      ],
      "year": "2024",
      "venue": "Measuring social norms of large language models",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Can a Machine Really Finish Your Sentence?",
      "authors": [
        "R Zellers",
        "A Holtzman",
        "Y Bisk",
        "A Farhadi",
        "Y Choi",
        "Hellaswag"
      ],
      "year": "2019",
      "venue": "ACL",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
      "authors": [
        "J Zhang",
        "Y Zhao",
        "M Saleh",
        "P J Liu"
      ],
      "year": "2020",
      "venue": "ICML",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "",
      "authors": [
        "S Zhang",
        "S Roller",
        "N Goyal",
        "M Artetxe",
        "M Chen",
        "S Chen",
        "C Dewan",
        "M Diab",
        "X Li",
        "X V Lin",
        "T Mihaylov",
        "M Ott",
        "S Shleifer",
        "K Shuster",
        "D Simig",
        "P S Koura",
        "A Sridhar",
        "T Wang",
        "L Zettlemoyer",
        "Opt"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Automatic Chain of Thought Prompting in Large Language Models",
      "authors": [
        "Z Zhang",
        "A Zhang",
        "M Li",
        "A Smola"
      ],
      "year": "",
      "venue": "Automatic Chain of Thought Prompting in Large Language Models",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Automatic Model Selection with Large Language Models for Reasoning. arXiv",
      "authors": [
        "X Zhao",
        "Y Xie",
        "K Kawaguchi",
        "J He",
        "Q Xie"
      ],
      "year": "2023",
      "venue": "Automatic Model Selection with Large Language Models for Reasoning. arXiv",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models. arXiv",
      "authors": [
        "C Zheng",
        "Z Liu",
        "E Xie",
        "Z Li",
        "Y Li"
      ],
      "year": "2023",
      "venue": "Progressive-Hint Prompting Improves Reasoning in Large Language Models. arXiv",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "A Human-Centric Benchmark for Evaluating Foundation Models. arXiv",
      "authors": [
        "W Zhong",
        "R Cui",
        "Y Guo",
        "Y Liang",
        "S Lu",
        "Y Wang",
        "A Saied",
        "W Chen",
        "N Duan"
      ],
      "year": "2023",
      "venue": "A Human-Centric Benchmark for Evaluating Foundation Models. arXiv",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
      "authors": [
        "A Zhou",
        "K Wang",
        "Z Lu",
        "W Shi",
        "S Luo",
        "Z Qin",
        "S Lu",
        "A Jia",
        "L Song",
        "M Zhan",
        "H Li"
      ],
      "year": "2023",
      "venue": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Agents: An Open-source Framework for Autonomous Language Agents",
      "authors": [
        "W Zhou",
        "Y E Jiang",
        "L Li",
        "J Wu",
        "T Wang",
        "S Qiu",
        "J Zhang",
        "J Chen",
        "R Wu",
        "S Wang",
        "S Zhu",
        "J Chen",
        "W Zhang",
        "N Zhang",
        "H Chen",
        "P Cui",
        "M Sachan"
      ],
      "year": "2023",
      "venue": "Agents: An Open-source Framework for Autonomous Language Agents",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Designing Stable and Transferable Sparse Expert Models. arXiv",
      "authors": [
        "B Zoph",
        "I Bello",
        "S Kumar",
        "N Du",
        "Y Huang",
        "J Dean",
        "N Shazeer",
        "W Fedus",
        "St-Moe"
      ],
      "year": "2022",
      "venue": "Designing Stable and Transferable Sparse Expert Models. arXiv",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "You will be provided instructions for completing a task followed by a task to complete",
      "authors": [],
      "year": "",
      "venue": "You will be provided instructions for completing a task followed by a task to complete",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Read the question carefully. The dataset contains multiple-choice questions related to college-level chemistry",
      "authors": [],
      "year": "",
      "venue": "Read the question carefully. The dataset contains multiple-choice questions related to college-level chemistry",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Understand the context of the question. The topics can range from atomic structure, chemical reactions, stoichiometry, thermodynamics, chemical bonding, to organic chemistry",
      "authors": [],
      "year": "",
      "venue": "Understand the context of the question. The topics can range from atomic structure, chemical reactions, stoichiometry, thermodynamics, chemical bonding, to organic chemistry",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Evaluate each choice. The dataset provides a list of choices for each question",
      "authors": [],
      "year": "",
      "venue": "Evaluate each choice. The dataset provides a list of choices for each question",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Identify the correct answer. The correct answer is indicated by a class label in the dataset",
      "authors": [],
      "year": "",
      "venue": "Identify the correct answer. The correct answer is indicated by a class label in the dataset",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Avoid the negative answer. The dataset also provides a negative answer",
      "authors": [],
      "year": "",
      "venue": "Avoid the negative answer. The dataset also provides a negative answer",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Use your knowledge of college-level chemistry to answer the question. You might need to recall facts, apply formulas, or use logical reasoning based on the principles of chemistry",
      "authors": [],
      "year": "",
      "venue": "Use your knowledge of college-level chemistry to answer the question. You might need to recall facts, apply formulas, or use logical reasoning based on the principles of chemistry",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "If the question is complex, break it down into smaller parts and address each part separately",
      "authors": [],
      "year": "",
      "venue": "If the question is complex, break it down into smaller parts and address each part separately",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Always provide an explanation for your answer. This helps to demonstrate your understanding of the topic",
      "authors": [],
      "year": "",
      "venue": "Always provide an explanation for your answer. This helps to demonstrate your understanding of the topic",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Review your answer before finalizing it. Ensure that it is accurate and complete",
      "authors": [],
      "year": "",
      "venue": "Review your answer before finalizing it. Ensure that it is accurate and complete",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Keep learning and updating your knowledge about college-level chemistry",
      "authors": [],
      "year": "",
      "venue": "The field is vast and the",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "2 Approach",
      "text": "We present zero-shot AgentInstruct in this section. To solve a task, zero-shot AgentInstruct employs an agent to create Figure 1: Summary of our approach and results. Top: Zero-shot AgentInstruct generalizes the zero-shot reasoning abilities of large language models to a wide set of language understanding tasks including generation, classification, and reasoning. Our agent runs only once per task, producing just one set of task-specific instructions that are used with all \\(n\\) instances of the task to instruct the reasoning process of large language models. These same instructions are also shared across all \\(m\\) models. As an example, results from instance 1 and model \\(m\\) are shown. Both the agent instructions and task-specific reasoning process are highlighted. Bottom: Performance of zero-shot AgentInstruct compared with standard zero-shot and zero-shot chain of thought (CoT). Zero-shot AgentInstruct improves the performance of three large language models substantially on the 29 datasets we evaluate. one set of instructions per dataset which allows an LLM to reason toward the final prediction. Intuitively, humans often rely on specific instructions to more effectively guide their thought process as they work towards a solution to a problem. For instance, to understand the sentiment in the movie reviews, instructions such as \"_1. Understand the Dataset:... Movie Reviews dataset... 2. Analyze the Passage: Pay attention to... the tone of the review..._\" help humans decompose the problem into task-specific reasoning steps and solve each to deliver the final answer (Figure 1). Zero-shot AgentInstruct follows this intuition. Agent InstructionsInstead of handwriting task-specific instructions, we build an agent to automate the process. Our agent only needs to generate instructions once per task instead of running the agent on all dataset instances. The intuition is that an agent is able to synthesize high-quality instructions with access to a wide range of existing task knowledge on the web. We design our agent based on ReAct (Yao et al., 2023) using LangChain's zero-shot ReAct implementation (Chase, 2022). This is motivated by the recent developments of language agents for task solving. Our agent highlights two features (Figure 2): (i) Instruction generation. Our agent follows ReAct which uses an LLM to propose a series of thoughts. The agent then receives observations and takes actions following the thoughts. Different from ReAct which aims to directly solve the task, our agent outputs step-by-step instructions on how to solve the task. The major advantage of this is that we only need to generate instructions once per dataset instead of running the agent on all dataset instances. We use GPT-4 (OpenAI, 2023) as the default agent. Once the action is finish, the corresponding output is our task-specific instructions. (ii) Action space. We constrain our action space to contain two types of actions that support the instruction generation: (a) ask_about_dataset[string], which returns information from the top relevant web pages containing information about the dataset. To do this, we construct and utilize a question answering API as a tool. This API is meant to answer questions about the document by interfacing with Chroma (Huber and Troynikov, 2022), a vector database storing the web pages, to provide an answer. (b) finish[instructions], which finishes the instruction generation with the task-specific instructions. As shown in Figure 2, to produce the instructions, our agent takes basic dataset information such as the name of the dataset (e.g., IMDB), a few input-only examples (examples without ground truth labels), and the set of output labels for the dataset (if applicable, else the type of dataset, e.g., generation) as its input. Using the task knowledge from the web, our agent forms observations (e.g., \"_Observation 1:... labeled as either positive or negative..._\") and thoughts (e.g., \"_Thought 2:... creating instructions..._\") which trigger the agent to perform actions, such as the finish action to output the task-specific instructions. For a full description of the AgentInstruct pipeline for instruction generation, see Appendix A.3. Chain of Thought ReasoningChain of thought (CoT) (Wei et al., 2022; Kojima et al., 2022) prompts LLMs to break down the task into intermediate reasoning steps that lead to the final answer. Unlike zero-shot CoT which uses a fixed prompt \"Let's think step by step\", we prepend our task-specific agent instructions (created only once per dataset) to the input of whichever models we are using for reasoning. These same instructions can be used for various reasoning LLMs on all dataset instances. The instructions prompt the LLMs to optimize their reasoning processes for the task. The LLMs will then follow our task-specific instructions to decompose the task into a chain of more specific intermediate steps to solve the task. As shown in Figure 1, the agent instructions \"... _Pay attention to... explicit or implicit expressions of sentiment towards the movie..._\" are the key to producing the critical reasoning path \"_... the movie is worth a view only for the performances of the three actors..._\", which leads to the correct prediction where standard zero-shot and zero-shot CoT fail. We follow zero-shot CoT, which consists of a reasoning extraction prompt to produce the intermediate reasoning steps, and an answer extraction prompt to collect the answers. For simplicity of implementation, we replace zero-shot CoT's fixed prompt with zero-shot AgentInstruct's task-specific instructions in the reasoning extraction prompt. Zero-shot AgentInstruct enjoys several unique properties: (i) Zero-shot AgentInstruct is a new way to improve the zero-shot reasoning of LLMs. Zero-shot AgentInstruct decouples the language agent and reasoning process of LLMs, Figure 2: An example of our agent producing task-specific instructions (highlighted) for a classification dataset IMDB. The agent only runs once to produce the instructions. Then, the instructions are used for all our models during reasoning. Agent instructions for all datasets are in Appendix E. which helps zero-shot AgentInstruct generalize to more tasks. Importantly, our language agent only needs to run once to produce the instructions for each task. And, the agent instructions provide more task-specific controls to the reasoning paths of LLMs, which benefits human alignment and improves the safety of LLMs. (ii) Our agent instructions are customized for different tasks and verified by existing task knowledge. For each task, different LLMs use the same set of instructions. We find the instructions transfer well among these reasoning LLMs. This is important in practice as the agent LLMs are often more powerful and costly than the reasoning LLMs, so our approach is a cost-effective alternative to using agents directly. Following the teacher-student knowledge distillation setup, our work is focused on using a larger LLM-based agent to instruct the smaller reasoning LLM. (iii) By providing task-specific instructions, chain of thought reasoning abilities are further generalized to more tasks beyond reasoning tasks. We show general language understanding tasks such as generation and classification also benefit from chain of thought reasoning. AgentInstruct is zero-shot so no input-output examples are required to solve the task. To better utilize the emerging reasoning capabilities of LLMs, our task-specific instructions align the reasoning process with a particular task better than general or fixed instructions."
    },
    {
      "title": "3 Experiment",
      "text": "We show that zero-shot AgentInstruct successfully improves the zero-shot reasoning abilities of LLMs, namely, Vicuna (Chiang et al., 2023), Llama-2-chat (Touvron et al., 2023), and GPT-3.5 Turbo (OpenAI, 2022), by a large margin on average. We evaluate zero-shot AgentInstruct on an exhaustive selection of 29 benchmarking datasets containing 53 subsets. As shown in Figure 3, each dataset is either a generation or classification task, and a portion of the datasets in each category are also reasoning tasks. The datasets consist of all HELM core scenarios from Liang et al. (2023), as well as the reasoning datasets from Kojima et al. (2022). The details of the experimental setup, including datasets and models, are described in Appendix A."
    },
    {
      "title": "Main Results",
      "text": "Results are shown in Figure 5. We compare zero-shot AgentInstruct to standard zero-shot and zero-shot CoT. We focus our analysis on three models: Vicuna-13b, Llama-2-70b-chat, and GPT-3.5 Turbo. We first compare zero-shot AgentInstruct to standard zero-shot prompting (Figure 1). The zero-shot prompt design follows Liang et al. (2023). On each model, zero-shot AgentInstruct wins on the majority of datasets, with no less than a 13.0% increase on average. Average performance versus the zero-shot setting is best on Llama-2-70b-chat with a 23.2% improvement. Figure 4(a) and Figure 4(b) show the results for generation and classification tasks respectively. On average, with zero-shot AgentInstruct, the three models beat the zero-shot setup by 23.1% for generation and 13.5% for classification. We hypothesize that generation datasets generally require more specific instructions than classification datasets, as the model does not know the best format for the generation output unless it has sufficient task information. This shows that our agent is able to instruct the reasoning process to improve the final outputs for different tasks, and zero-shot AgentInstruct is able to generalize LLMs' reasoning abilities across tasks. Note that we only run the agent 53 times, resulting in 53 agent generated instructions, as we evaluate on 53 subsets. With zero-shot AgentInstruct, we also observe a large margin of improvement for zero-shot performance across different models. Significantly, Llama-2-70b-chat beats the performance of zero-shot GPT-3.5 Turbo by 10.2% on average across all datasets. This indicates our agent instructions are the key to improving the reasoning performance of LLMs. The most immediate comparison to zero-shot AgentInstruct is zero-shot CoT, as zero-shot AgentInstruct uses task-specific instructions instead of a fixed manual instruction. On average, across all three models, zero-shot AgentInstruct beats zero-shot CoT by 6.5%, with the largest growth being Vicuna-13b at 9.5%. On both generation and classification datasets, across three models, zero-shot AgentInstruct wins by 5.0% and 7.8% on each category respectively. This suggests that zero-shot AgentInstruct is able to generalize the zero-shot reasoning abilities of LLMs to both generation and classification tasks, and optimize the performance of specific tasks. Figure 3: Datasets for generation (blue), classification (green), and reasoning (orange). Reasoning contains generation and classification tasks. In particular, we look into the performance of reasoning tasks (Figure 4(c)). Of our three models, the average difference between zero-shot AgentInstruct and the zero-shot setting on reasoning tasks is 31.3%, whereas the difference between zero-shot AgentInstruct and zero-shot CoT is 10.5%. This shows that our task-specific instructions are more helpful for LLMs to break down tasks into more specific intermediate reasoning steps compared to the task-agnostic instructions in zero-shot CoT, which leads to improved final predictions. Overall, zero-shot AgentInstruct wins on 9 of the 13 generation datasets, 11 of the 16 classification datasets, and 10 of the 12 reasoning datasets (Figure 4). See Appendix C for additional results and analysis, including results on individual datasets and subsets."
    },
    {
      "title": "Ablation Studies",
      "text": "We examine how different components of zero-shot AgentInstruct impact its zero-shot reasoning performance. Results are shown in Table 1 on AddSub (reasoning), IMDB (classification), and NarrativeQA (generation). We use Llama-2-70b-chat for the reasoning step. The four settings examine the importance of agent instructions in zero-shot AgentInstruct. Descriptions of each setting are as follows: (i) w/o Agent Instructions: We compare the zero-shot AgentInstruct methodology to zero-shot CoT. (ii) w/o Input Examples: We remove the input-only examples from agent's input. (iii) w/o Label Description: We remove the description of the labels from the agent's input. (iv) w/o GPT-4: We use GPT-3.5 Turbo, instead of GPT-4, as the agent to generate instructions. The results suggest that all components of zero-shot AgentInstruct are effective in providing high-quality instructions and eliciting high-quality reasoning steps. See Appendix D.1 for further descriptions of each setting. Importance of Synthesizing InstructionsAs zero-shot AgentInstruct uses additional dataset information to craft instructions, we test zero-shot CoT with that same information prepended to isolate the impact of synthesizing this information into high-quality instructions. We find that merely prepending this information does not consistently improve the performance over zero-shot CoT, whereas the synthesis of this information into high-quality instructions with zero-shot AgentInstruct does, suggesting the instructions are integral in guiding the reasoning steps (Table 2). Comparison to GPT-4 MethodsWe test the following methods using GPT-4: zero-shot, zero-shot CoT, ReAct (Yao et al., 2023b), and zero-shot AgentInstruct. Figure 6 shows the performance of each method on AddSub. Zero-shot AgentInstruct outperforms zero-shot GPT-4 by 8.6% and ties the performance of zero-shot CoT GPT-4 for approximately one-tenth of the cost. Zero-shot AgentInstruct is using GPT-4 for the instructions and GPT-3.5 Turbo for the CoT reasoning, following the standard knowledge distillation setting. This shows zero-shot AgentInstruct is a cost-effective solution for improving performance, as it far exceeds GPT-3.5 performance for a small amount more and reaches GPT-4 performance for much less. Though RecAct narrowly outperforms zero-shot AgentInstruct, it costs nearly 100 times more since the zero-shot AgentInstruct agent is run only once per dataset rather than per instance. Each run of our agent to generate instructions cost less than $1. This result implies that decoupling the instruction generation and reasoning steps further unleashes the zero-shot reasoning abilities of LLMs, and that zero-shot AgentInstruct is a cost-effective alternative to using agents directly. \\begin{table} \\begin{tabular}{l c c c} \\hline \\hline Method & AddSub & IMDB & NarrativeQA \\\\ \\hline Zero-Shot CoT & 73.2 & 89.0 & 62.3 \\\\ Zero-Shot CoT + & 71.6 & 90.5 & 58.2 \\\\ Data Information & **79.5** & **94.0** & **65.0** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Comparison on Llama-2-70b-chat when providing the dataset information to zero-shot CoT. Figure 4: Winning rate (%) between zero-shot, zero-shot CoT, and zero-shot AgentInstruct based on the average results over three models. Using GPT-4 in AgentInstructThough zero-shot AgentInstruct is primarily a cost-effective solution to distill knowledge from a powerful LLM agent to a smaller reasoner, we also consider the use of the same underlying model for both the instruction generation and reasoning steps. Specifically, we leverage GPT-4 for both the instruction generation and reasoning steps on IMDB and AddSub to see whether zero-shot AgentInstruct is as effective outside the knowledge distillation setting. On IMDB, zero-shot GPT-4, zero-shot CoT GPT-4, and zero-shot AgentInstruct GPT-4 (where both the instruction generation and CoT reasoning steps use GPT-4) score 87.4, 96.1, and 96.6 respectively. Similarly, on AddSub, the results are 79.5, 88.1, and 88.1 respectively. In both cases, we see zero-shot AgentInstruct is at least as good as zero-shot GPT-4 and zero-shot CoT GPT-4, meaning our method shows promise when the same model is used throughout the AgentInstruct pipeline."
    },
    {
      "title": "Manual Prompt Sensitivity",
      "text": "Zero-shot AgentInstruct has two manual prompts during the CoT reasoning step: (1) the reasoning extraction prompt, which asks for intermediate reasoning steps, and (2) the answer extraction prompt, which extracts the final answer from the intermediate reasoning steps. To test the sensitivity of each manual prompt, we vary a single prompt while keeping the default zero-shot AgentInstruct prompt for the other. Results are shown in Table 3 based on Llama-2-70b-chat on AddSub. Overall, zero-shot AgentInstruct's performance does not appear particularly sensitive to changes in the manual prompts, suggesting that the methodology behind zero-shot AgentInstruct is robust. Additional prompt sensitivity experiments are conducted in Appendix D.3. Figure 5: Results on Vicuna-13b, Llama-2-70b-chat, and GPT-3.5 Turbo across tasks. Top: generation. Middle: classification. Bottom: reasoning. Figure 6: Comparison on GPT-4 using zero-shot, zero-shot CoT, ReAct, and zero-shot AgentInstruct (using GPT-3.5 Turbo for reasoning) on AddSub."
    },
    {
      "title": "Model Scaling",
      "text": "As it is often the case that more parameters substantially improve the reasoning capabilities of LLMs (Wei et al., 2022), we test zero-shot AgentInstruct's performance on models of various sizes. Specifically, we test on three Llama-2-chat models with 7 billion, 13 billion, and 70 billion parameters. Figure 7 shows the average score across all 29 datasets for zero-shot, zero-shot CoT, and zero-shot AgentInstruct. These results confirm that the average performance of all three methods increases with model size. Each time model size increases, zero-shot CoT and zero-shot AgentInstruct show consistent improvement of around 6%, while zero-shot exhibits smaller gains near 2%. This is because reasoning steps are best produced with more powerful models. Still, at just 13b parameters, Llama-2-13b-chat with zero-shot AgentInstruct surpasses the performance of zero-shot GPT-3.5 Turbo by over 2%. Additionally, zero-shot AgentInstruct's superiority over zero-shot and zero-shot CoT appears independent of model size."
    },
    {
      "title": "Error Analysis",
      "text": "To investigate the causes of errors made by zero-shot AgentInstruct, we manually analyze 25 samples from AddSub, IMDB, and NewsQA respectively where zero-shot AgentInstruct results in incorrect predictions on Llama-2-70b-chat. We define incorrect predictions as those with a quasi-exact match or F1 score less than 1. The most common error across these datasets is incorrect reasoning (32.0%), i.e., not correctly reasoning through the problem when applying accurate agent instructions. For example, on AddSub, zero-shot AgentInstruct may choose the wrong operation due to a misleading verb. On IMDB, zero-shot AgentInstruct may misread the sentiment due to emphasizing words describing the movie, not the review. This is problematic, as we want to describe the sentiment of the review, not the sentiment of the movie. See Figure 8 for an example of incorrect reasoning on IMDB. The answer ambiguity is another main source of errors (22.7%). For example, in a review where the reviewer clearly enjoyed the movie even though the reviewer acknowledged it was a typical bad movie, our prediction is \"Positive\" while the ground truth is \"Negative\" on IMDB. For many errors, either the instructions are taken too literally or partially ignored. As models become better at reasoning, such errors should be minimized. More thorough error analysis and full examples for each error category are in Appendix D.4. \\begin{table} \\begin{tabular}{l l l} \\hline \\hline CoT & Prompt & Quasi-Exact \\\\ Reasoning & & Match (\\%) \\\\ \\hline Reasoning & **Follow the instructions to generate an explanation that reasons towards** & **79.5** \\\\ extraction & **the correct answer to the task above. End the explanation with the correct** & \\\\ & **answer.\\\\n\\n Explanation:** & \\\\ \\cline{2-3} & Let’s think step-by-step according to the instructions. End with the correct & 77.5 \\\\ & answer. Explanation: & \\\\ \\cline{2-3} & Let’s think step-by-step according to the instructions. First, & 75.2 \\\\ \\cline{2-3} & Use the instructions to guide you towards your answer.\\n Explanation: & 79.0 \\\\ \\cline{2-3} & Explanation: & 78.5 \\\\ \\hline Answer & **Therefore, the answer to the task is below. Give the answer in the shortest** & 79.5 \\\\ extraction & **form possible that will still be correct. \\nAnswer:** & **79.7** \\\\ \\cline{2-3} & Therefore, the answer is & 79.2 \\\\ \\cline{2-3} & Answer: & 77.7 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Prompt sensitivity analysis of chain of thought reasoning of zero-shot AgentInstruct with Llama-2-70b-chat on AddSub. The default prompts are in bold. Higher scores are better. Figure 7: Model scaling results of zero-shot, zero-shot CoT, and zero-shot AgentInstruct with Llama-2-chat on all datasets."
    },
    {
      "title": "Case Study",
      "text": "Next, we analyze the quality of the CoT reasoning steps when predictions are correct. On three datasets (AddSub, IMDB, and NewsQA), we randomly select 25 examples from each dataset with a perfect quasi-exact match or F1 score. We find that the reasoning capabilities are greatly enhanced by the combination of effective agent instructions and the task-specific reasoning process of LLMs. An example is in Figure 9."
    },
    {
      "title": "Comparison To Related Methods",
      "text": ""
    },
    {
      "title": "Few-Shot",
      "text": "We compare Llama-2-70b-chat zero-shot AgentInstruct results with few-shot results on AddSub, IMDB, and NarrativeQA in Figure 10. Specifically, we sample 5 training instances from each dataset to prepend to the test instances. Surprisingly, zero-shot AgentInstruct reaches competitiveness with few-shot prompting. Zero-shot AgentInstruct, without any few-shot examples, outperforms few-shot performance on AddSub and NarrativeQA, by over 4.3% and 23.7% respectively, and loses by 0.7% on IMDB. Ideally, all the information encoded within the few-shot examples is found by AgentInstruct and synthesized into agent instructions, with additional information not in the few-shot examples also being used. We hypothesize this information can be presented in a clearer manner within the agent instructions rather than through examples to better utilize the instruction-following and reasoning capabilities of LLMs. As shown, zero-shot AgentInstruct has the potential to reach or even beat few-shot performance."
    },
    {
      "title": "Self-Consistency",
      "text": "Finally, we compare zero-shot AgentInstruct results with self-consistency (Wang et al., 2023c) results of Llama-2-70b-chat on AddSub, IMDB, and NarrativeQA in Figure 11. We adapt self-consistency to the zero-shot setting as follows: we sample three responses using a temperature of 0.7, top-\\(k\\) sampling with \\(k=40\\), and a randomly generated seed for each request. After cleaning the outputs, we use a majority vote to determine the consensus answer, choosing at random to break ties. On AddSub, IMDB, and NarrativeQA, zero-shot AgentInstruct outperforms self-consistency by 5.8%, 7.5%, and 1.9% respectively. Moreover, zero-shot AgentInstruct is more computationally efficient than self-consistency as there is no need for sampling the reasoning paths multiple times. Figure 11: Comparison between zero-shot AgentInstruct and zero-shot self-consistency on Llama-2-70b-chat on AddSub, IMDB, and NarrativeQA. Figure 8: An incorrect reasoning example for Llama-2-70b-chat with zero-shot AgentInstruct on IMDB for error analysis. Here, the model mistook the love of the commentary as the love of the movie (highlighted). Figure 10: Comparison between zero-shot AgentInstruct and few-shot on Llama-2-70b-chat on AddSub, IMDB, and NarrativeQA. Figure 9: Case study example for Llama-2-70b-chat with zero-shot AgentInstruct on IMDB. Here, the answer is correct and the task-specific reasoning is helpful for finding the answer (highlighted)."
    },
    {
      "title": "4 Related Work",
      "text": "Large language models, such as GPT-4 (OpenAI, 2023), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), PaLM-2 (Anil et al., 2023), BLOOM (Scao et al., 2023), OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), Llama-2 (Touvron et al., 2023), and many others (Radford et al., 2019; Wang and Komatuszaki, 2021; Black et al., 2021; Smith et al., 2022; Hoffmann et al., 2022; Penedo et al., 2023) have shown remarkable performance on natural language processing (NLP) tasks. Following the pretraining phase, additional finetuning enables models (e.g., FLAN (Wei et al., 2022), FLAN-T5 (Chung et al., 2022), InstructGPT (Ouyang et al., 2022)) to better align with human instructions to complete tasks. Moreover, instruction-tuning has enabled LLMs (e.g., GPT-3.5 Turbo (OpenAI, 2022), Llama-2-chat (Touvron et al., 2023), Self-Instruct (Wang et al., 2023), Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), Koala (Geng et al., 2023)) to better engage with users through dialogue. Zero-shot AgentInstruct builds on instruction-following language models, enabling better zero-shot reasoning abilities through the use of agent instructions. Language agents (Yao et al., 2023; Shinn et al., 2023; Xu et al., 2023; Park et al., 2023; Zhou et al., 2023; Andreas, 2022; Wang et al., 2023; Xi et al., 2023; Sumers et al., 2023; Chan et al., 2023) recently emerged due to the task planning capabilities of LLMs. Given a task often demonstrated in natural language, these agents aim to complete the task directly. We base our agent on ReAct (Yao et al., 2023), which uses structured \"Thought, Action, Observation\" steps to guide a model to accomplish a goal. Unlike existing agents that aim to solve a task directly, our agent generates task-specific instructions on how to complete the given task, decoupling the agent planning and reasoning steps. Besides reaching competitive performance with using agents directly, our design turns out to be more cost-effective. Finetuning is an effective method for generating higher-quality responses from LLMs on downstream tasks (Liu et al., 2019; Howard and Ruder, 2018). As model scale increases, finetuning becomes less practical, which lightweight tuning methods (such as prefix tuning (Li and Liang, 2021), prompt learning (Lester et al., 2021; Liu et al., 2023), LoRA (Hu et al., 2022)) have tried to solve. Even with such methods, prompting techniques have gained attention as an even cheaper alternative, such as chain of thought and zero-shot chain of thought prompting (Wei et al., 2022; Kojima et al., 2022), the latter of which we focus on in our work. Few-shot learning, which involves providing a few examples demonstrating the task before prompting the models during inference, is often effective on a range of tasks (Brown et al., 2020; Dong et al., 2023). Chain of thought (CoT) prompting (Wei et al., 2022) involves generating a series of intermediate reasoning steps, which can dramatically increase the performance of LLMs on complex reasoning tasks. While this reasoning behavior is traditionally learned from few-shot demonstrations, Kojima et al. (2022) extends CoT prompting to the zero-shot setting, where no examples are used. More recently, new approaches such as self-consistency (Wang et al., 2023), plan-and-solve prompting (Wang et al., 2023), tree of thought (Yao et al., 2023), and graph of thought (Besta et al., 2023) have further improved the reasoning quality. Zero-shot AgentInstruct focuses on the zero-shot setup. It generalizes the reasoning abilities of LLMs to more tasks by utilizing task-specific instructions generated from our agent to better align the reasoning process with a particular task. NLP benchmarking datasets provide a standardized interface to evaluate LLMs on specific downstream tasks. Common benchmarks (e.g., HELM (Liang et al., 2023), MMLU (Hendrycks et al., 2021), and many others (Shen et al., 2024; Yuan et al., 2024; Zhong et al., 2023; Srivastava et al., 2023; Suzgun et al., 2023; Chen et al., 2021; Wang et al., 2019; Rajpurkar et al., 2016)) have become part of the standard evaluation of LLMs. We benchmark our method on 29 datasets, consisting of the core scenarios from HELM (Liang et al., 2023) and the reasoning datasets from Kojima et al. (2022), and include results for comparison purposes. For reasoning tasks, our method significantly outperforms existing zero-shot approaches (Brown et al., 2020; Kojima et al., 2022). Besides reasoning tasks, our method generalizes to general language understanding tasks including generation and classification."
    },
    {
      "title": "5 Conclusion",
      "text": "Our work proposes a new way of improving the zero-shot reasoning abilities of large language models on general language understanding tasks. We build an agent to instruct the reasoning process of LLMs. Our core design principle shares the same spirit as the standard knowledge distillation setup. Our agent automatically generates a set of task-specific instructions for a wide set of tasks. The same instructions guide different LLMs to reason better across many task instances to make high-quality predictions. Our method is zero-shot so requires no input-output examples. Our results confirm the efficacy of our approach, leading to substantial improvements across various NLP tasks spanning generation, classification, and reasoning. Significant average score enhancements are achieved over the standard zero-shot setting across 29 datasets for Vicuna-13b, Llama-2-70b-chat, and GPT-3.5 Turbo respectively. Our method wins on 20 of the 29 datasets used for evaluation. We believe zero-shot AgentInstruct's style of human-understandable reasoning, along with its use of an autonomous agent, can replace more traditional styles of zero or few-shot prompting as models become better reasoners. [MISSING_PAGE_FAIL:10] S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating Large Language Models Trained on Code. _arXiv_, 2021. * Chiang et al. (2023) Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. _LMSYS Org_, 2023. * Choi et al. (2018) Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi, Y., Liang, P., and Zettlemoyer, L. QuAC: Question Answering in Context. In _EMNLP_, 2018. * Chowdhery et al. (2021) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. PaLM: Scaling Language Modeling with Pathways. _arXiv_, 2022. * Chung et al. (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. Scaling Instruction-Finetuned Language Models. _arXiv_, 2022. * Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In _NAACL-HLT_, 2019. * Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems. _arXiv_, 2021. * Ding et al. (2021) Ding, S., Shang, J., Wang, S., Sun, Y., Tian, H., Wu, H., and Wang, H. ERNIE-Doc: A Retrospective Long-Document Modeling Transformer. In _ACL_, 2021. * Dong et al. (2023) Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., Li, L., and Sui, Z. A Survey on In-context Learning. _arXiv_, 2023. * Driess et al. (2023) Driess, D., Xia, F., Sajiadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. PaLM-E: An Embodied Multimodal Language Model. In _ICML_, 2023. * Geng et al. (2023) Geng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P., Levine, S., and Song, D. Koala: A Dialogue Model for Academic Research. _BAIR_, 2023. * Geva et al. (2021) Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and Berant, J. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. _TACL_, 2021. * Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring Massive Multitask Language Understanding. In _ICLR_, 2021. * Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training Compute-Optimal Large Language Models. _arXiv_, 2022. * Howard & Ruder (2018) Howard, J. and Ruder, S. Universal Language Model Finetuning for Text Classification. In _ACL_, 2018. * Hsieh et al. (2023) Hsieh, C.-Y., Li, C.-L., Yeh, C.-k., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., and Pfister, T. Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. In _Findings of the Association for Computational Linguistics: ACL 2023_, 2023. * Hu et al. (2022) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-Rank Adaptation of Large Language Models. In _ICLR_, 2022. * Huang et al. (2019) Huang, H., Choi, E., and Yih, W. FlowQA: Grasping Flow in History for Conversational Machine Comprehension. In _ICLR_, 2019. * Huang et al. (2022) Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large Language Models Can Self-Improve. _arXiv_, 2022. * Huang et al. (2020) Huang, K., Altosaar, J., and Ranganath, R. ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission. _arXiv_, 2020. * Huang et al. (2023) Huang, S., Jiang, Z., Dong, H., Qiao, Y., Gao, P., and Li, H. Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model. _arXiv_, 2023. * Huber & Troynikov (2022) Huber, J. and Troynikov, A. Chroma, 2022. * Jarvelin & Kekalainen (2002) Jarvelin, K. and Kekalainen, J. Cumulated Gain-Based Evaluation of IR Techniques. _ACM TOIS_, 2002. * Joshi et al. (2020) Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., and Levy, O. SpanBERT: Improving Pre-training by Representing and Predicting Spans. _TACL_, 2020. * Kocisky et al. (2018) Kocisky, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The NarrativeQA Reading Comprehension Challenge. _TACL_, 2018. * Koh et al. (2021) Koh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R. L., Gao, I., Lee, T., David, E., Stavness, I., Guo, W., Earnshaw, B., Haque, I. S., Beery, S. M., Leskovec, J., Kundaje, A., Pierson, E., Levine, S., Finn, C., and Liang, P. WILDS: A Benchmark of in-the-Wild Distribution Shifts. In _ICML_, 2021. * Kojima et al. (2022) Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large Language Models are Zero-Shot Reasoners. In _NeurIPS_, 2022. * Koncel-Kedziorski et al. (2015) Koncel-Kedziorski, R., Hajishirzi, H., Sabharwal, A., Etzioni, O., and Ang, S. D. Parsing Algebraic Word Problems into Equations. _TACL_, 2015. * Koncel-Kedziorski et al. (2016) Koncel-Kedziorski, R., Roy, S., Amini, A., Kushman, N., and Hajishirzi, H. MAWPS: A Math Word Problem Repository. In _HLT-NAACL_, 2016. * Kwiatkowski et al. (2019) Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. Natural Questions: A Benchmark for Question Answering Research. _TACL_, 2019. * Kiciman et al. (2023) Kiciman, E., Ness, R., Sharma, A., and Tan, C. Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. _arXiv_, 2023. * Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. The Power of Scale for Parameter-Efficient Prompt Tuning. In _EMNLP_, 2021. * Li & Liang (2021) Li, X. L. and Liang, P. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In _ACL_, 2021. * Liang et al. (2022) Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C. D., Re, C., Acosta-Navas, D., Hudson, D. A., Zelikman, E., Durmus, E., Ladhak, F., Rong, F., Ren, H., Yao, H., Wang, J., Santhanam, K., Orr, L., Zheng, L., Yuksekgonul, M., Suzgun, M., Kim, N., Guha, N., Chatterji, N., Khattab, O., Henderson, P., Huang, Q., Chi, R., Xie, S. M., Santurkar, S., Ganguli, S., Hashimoto, T., Icard, T., Zhang, T., Chaudhary, V., Wang, W., Li, X., Mai, Y., Zhang, Y., and Koreeda, Y. Holistic Evaluation of Language Models. In _TMLR_, 2023. * Lin (2004) Lin, C.-Y. ROUGE: A Package for Automatic Evaluation of Summaries. In _ACL_, 2004. * Lin et al. (2022) Lin, S., Hilton, J., and Evans, O. TruthfulQA: Measuring How Models Mimic Human Falsehoods. In _ACL_, 2022. * Ling et al. (2017) Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. In _ACL_, 2017. * Liu et al. (2023) Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. _ACM Comput. Surv._, 2023. * Liu et al. (2019) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. RoBERTa: A Robustly Optimized BERT Pretraining Approach. _arXiv_, 2019. * Maas et al. (2011) Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning Word Vectors for Sentiment Analysis. In _ACL_, 2011. * MacAvaney et al. (2021) MacAvaney, S., Yates, A., Feldman, S., Downey, D., Cohan, A., and Goharian, N. Simplified Data Wrangling with ir_datasets. In _SIGIR_, 2021. * Mihaylov et al. (2018) Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. In _EMNLP_, 2018. * Narayan et al. (2018) Narayan, S., Cohen, S. B., and Lapata, M. Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization. In _EMNLP_, 2018. * Nogueira et al. (2020) Nogueira, R., Jiang, Z., Pradeep, R., and Lin, J. Document Ranking with a Pretrained Sequence-to-Sequence Model. In _EMNLP_, 2020. * OpenAI (2022) OpenAI. Introducing ChatGPT. _OpenAI_, 2022. * OpenAI (2023) OpenAI. GPT-4 Technical Report. _arXiv_, 2023. * Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In _NeurIPS_, 2022. * Park et al. (2023) Park, J. S., O'Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative Agents: Interactive Simulacra of Human Behavior. In _UIST_, 2023. * Patel et al. (2021) Patel, A., Bhattacharya, S., and Goyal, N. Are NLP Models really able to Solve Simple Math Word Problems? In _NAACL-HLT_, 2021. * Penedo et al. (2023) Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. _arXiv_, 2023. * Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language Models are Unsupervised Multitask Learners. _OpenAI_, 2019. * Rajpurkar et al. (2016) Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In _EMNLP_, 2016. * Roy & Roth (2015) Roy, S. and Roth, D. Solving General Arithmetic Word Problems. In _EMNLP_, 2015. * Scao et al. (2016) Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., Ruwase, O., Bawden, R., Bekman, S., McMillan-Major, A., Beltagy, I., Nguyen, H., Saulnier, L., Tan, S., Suarez, P. O., Sanh, V., Laurencon, H., Jernite, Y., Launay, J., Mitchell, M., Raffel, C., Gokaslan, A., Simhi, A., Soroa, A., Aji, A. F., Alfassy, A., Rogers, A., Nitzav, A. K., Xu, C., Mou, C., Emezue, C., Klamm, C., Leong, C., van Strien, D., Adelani, D. I., Radev, D., Ponferrada, E. G., Levkovizni, E., Kim, E. B., Natan, E. B., Toni, F. D., Dupont, G., Kruszewski, G., Pistilli, G., Elsahar, H., Benyamina, H., Tran, H., Yu, I., Abdulmumin, I., Johnson, I., Gonzalez-Dios, I., de la Rosa, J., Chim, J., Dodge, J., Zhu, J., Chang, J., Frohberg, J., Tobing, J., Bhattacharjee, J., Almubarak, K., Chen, K., Lo, K., Werra, L. V., Weber, L., Phan, L., allal, L. B., Tanguy, L., Dey, M., Munoz, M. R., Masoud, M., Grandury, M., Sasko, M., Huang, M., Coavoux, M., Singh, M., Jiang, M. T.-J., Vu, M. C., Jauhar, M. A., Ghaleb, M., Subramani, N., Kassner, N., Khamis, N., Nguyen, O., Espejel, O., de Gibert, O., Villegas, P., Henderson, P., Colombo, P., Amuok, P., Choest, Q., Harlim, R., Bonmassani, R., Lopez, R. L., Ribeiro, R., Osei, S., Pyysalo, S., Nagel, S., Bose, S., Muhammad, S. H., Sharma, S., Longpre, S., Nikpoor, S., Silberberg, S., Pai, S., Zink, S., Torrent, T. T., Schick, T., Thrush, T., Danchev, V., Nikoulina, V., Laippala, V., Lepercq, V., Prabhu, V., Alyafeai, Z., Talar, Z., Raja, A., Heinzerling, B., Si, C., Tasar, D. E., Salesky, E., Mielke, S. J., Lee, W. Y., Sharma, A., Santilli, A., Chaffin, A., Stiegler, A., Datta, D., Szczechla, E., Chhablani, G., Wang, H., Pandey, H., Strobelt, H., Fries, J. A., Rozen, J., Gao, L., Sutawika, L., Bari, M. S., Al-shaibani, M. S., Manica, M., Nayak, N., Teehan, R., Albanie, S., Shen, S., Ben-David, S., Bach, S. H., Kim, T., Bers, T., Fevry, T., Neeraj, T., Thakker, U., Raunak, V., Tang, X., Yong, Z.-X., Sun, Z., Brody, S., Uri, Y., Tojarieh, H., Roberts, A., Chung, H. W., Tae, J., Phang, J., Press, O., Li, C., Narayanan, D., Bourfoune, H., Casper, J., Rasley, J., Ryabinin, M., Mishra, M., Zhang, M., Shoeybi, M., Peyrounette, M., Patry, N., Tazi, N., Sanseviero, O., von Platen, P., Cornette, P., Lavallee, P. F., Lacroix, R., Rajbhandari, S., Gandhi, S., Smith, S., Requena, S., Patil, S., Dettmers, T., Baruwa, A., Singh, A., Cheveleva, A., Ligozat, A.-L., Subramonian, A., Neveol, A., Lovering, C., Garrette, D., Tunuguntla, D., Reiter, E., Taktasheva, E., Voloshina, E., Bogdanov, E., Winata, G. I., Schoelkopf, H., Kalo, J.-C., Novikova, J., Forde, J. Z., Clive, J., Kasai, J., Kawamura, K., Hazan, L., Carpuat, M., Clinciu, M., Kim, N., Cheng, N., Serikov, O., Antverg, O., van der Wal, O., Zhang, R., Zhang, R., Gehrmann, S., Mirkin, S., Pais, S., Shavrina, T., Scialom, T., Yun, T., Limisiewicz, T., Rieser, V., Protasov, V., Mikhailov, V., Pruksachatkun, Y., Belinkov, Y., Bamberger, Z., Kasner, Z., Rueda, A., Pestana, A., Feizpour, A., Khan, A., Farnank, A., Santos, A., Hevia, A., Unldreaj, A., Aghagol, A., Abdollahi, A., Tammour, A., HajiHosseini, A., Behroozi, B., Ajibade, B., Saxena, B., Ferrandis, C. M., McDuff, D., Contractor, D., Lansky, D., David, D., Kiela, D., Nguyen, D. A., Tan, E., Baylor, E., Ozoani, E., Mirza, F., Ononiwu, F., Rezanejad, H., Jones, H., Bhattacharya, I., Solaiman, I., Sedenko, I., Nejadgholi, I., Passmore, J., Seltzer, J., Sanz, J. B., Dutra, L., Samagaio, M., Elbadri, M., Mieskes, M., Gerchick, M., Akinolu, M., McKenna, M., Qiu, M., Ghauri, M., Burynok, M., Abrar, N., Rajani, N., Elkott, N., Fahmy, N., Samuel, O., An, R., Kromann, R., Hao, R., Alizadeh, S., Shubber, S., Wang, S., Roy, S., Viguier, S., Le, T., Oyebade, T., Le, T., Yang, Y., Nguyen, Z., Kashyap, A. R., Palasciano, A., Callahan, A., Shukla, A., Miranda-Escalada, A., Singh, A., Beilharz, B., Wang, B., Brito, C., Zhou, C., Jain, C., Xu, C., Fourrier, C., Perinan,D. L., Molano, D., Yu, D., Manjavacas, E., Barth, F., Fuhrmann, F., Altay, G., Bayrak, G., Burns, G., Vrabec, H. U., Bello, I., Dash, I., Kang, J., Giorgi, J., Golde, J., Posada, J. D., Sivaraman, K. R., Bulchandani, L., Liu, L., Shinzato, L., de Bykhovez, M. H., Takeuchi, M., Pamies, M., Castillo, M. A., Nezhurina, M., Sanger, M., Samwald, M., Cullan, M., Weinberg, M., Wolf, M. D., Mihaljcic, M., Liu, M., Freidank, M., Kang, M., Seelam, N., Dahlberg, N., Broad, N. M., Mueller, N., Fung, P., Haller, P., Chandrasekhar, R., Eisenberg, R., Martin, R., Canalli, R., Su, R., Su, R., Cahyawiiaya, S., Garda, S., Deshmukh, S. S., Mishra, S., Kiblawi, S., Ott, S., Sangaroonsiri, S., Kumar, S., Schweter, S., Bharati, S., Laud, T., Gigant, T., Kainuma, T., Kusa, W., Labrak, Y., Bajaj, Y. S., Venkatraman, Y., Xu, Y., Xu, Y., Tan, Z., Xie, Z., Ye, Z., Bras, M., Belkada, Y., and Wolf, T. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. _arXiv_, 2023. * See et al. (2017) See, A., Liu, P. J., and Manning, C. D. Get To The Point: Summarization with Pointer-Generator Networks. In _ACL_, 2017. * ShareGPT (2023) ShareGPT. ShareGPT: Share your wildest ChatGPT conversations with one click. _ShareGPT_, 2023. * Shen et al. (2024) Shen, J., Yuan, Y., Mirzoyan, S., Zhang, M., and Wang, C. Measuring vision-language stem skills of neural models, 2024. * Shinn et al. (2022) Shinn, N., Cassano, F., Labash, B., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language Agents with Verbal Reinforcement Learning. _arXiv_, 2023. * Smith et al. (2022) Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., Zhang, E., Child, R., Aminabadi, R. Y., Bernauer, J., Song, X., Shoeybi, M., He, Y., Houston, M., Tiwary, S., and Catanzaro, B. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. _arXiv_, 2022. * Srivastava et al. (2021) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek, A. W., Safaya, A., Tazarv, A., Xiang, A., Parrish, A., Nie, A., Hussain, A., Askell, A., Dsouza, A., Slone, A., Rahane, A., Iyer, A. S., Andreassen, A., Maddto, A., Santilli, A., Stuhlmuller, A., Dai, A., La, A., Lampinen, A., Zou, A., Jiang, A., Chen, A., Vuong, A., Gupta, A., Gottardi, A., Norelli, A., Venkatesh, A., Gholamiadovodi, A., Tabasum, A., Menezes, A., Kirubarajan, A., Mullokandov, A., Sabharwal, A., Herrick, A., Efrat, A., Erdem, A., Karakas, A., Roberts, B. R., Loe, B. S., Zoph, B., Bojanowski, B., Ozyurt, B., Hedayatnia, B., Neyshabur, B., Inden, B., Stein, B., Ekmekci, B., Lin, B. Y., Howald, B., Orinion, B., Diao, C., Dour, C., Stinson, C., Argueta, C., Ramirez, C. F., Singh, C., Rathkopf, C., Meng, C., Baral, C., Wu, C., Callison-Burch, C., Waites, C., Voigt, C., Manning, C. D., Potts, C., Ramirez, C., Rivera, C. E., Siro, C., Raffel, C., Ashcraft, C., Garbacea, C., D., Garrette, D., Hendrycks, D., Kilman, D., Roth, D., Freeman, D., Khashabi, D., Levy, D., Gonzalez, D. M., Perszyk, D., Hernandez, D., Chen, D., Ippolito, D., Gilboa, D., Dohan, D., Drakard, D., Jurgens, D., Datta, D., Ganguli, D., Emelin, D., Kleyko, D., Yuret, D., Chen, D., Tam, D., Hupkes, D., Misra, D., Buzan, D., Mollo, D. C., Yang, D., Lee, D.-H., Schrader, D., Shutova, E., Cubuk, E. D., Segal, E., Hagerman, E., Barnes, E., Donoway, E., Pavlick, E., Rodola, E., Lam, E., Chu, E., Tang, E., Erdem, E., Chang, E., Chi, E. A., Dyer, E., Jerzak, E., Kim, E., Manyasi, E. E., Zheltonozhskii, E., Xia, F., Sfar, F., Martinez-Plumed, F., Happe, F., Chollet, F., Rong, F., Mishra, G., Winata, G. I., de Melo, G., Kruszewski, G., Parascandolo, G., Mariani, G., Wang, G., Jaimovitch-Lopez, G., Betz, G., Gur-Ari, G., Galijasevic, H., Kim, H., Rashkin, H., Hajishirzi, H., Mehta, H., Bogar, H., Shevlin, H., Schutze, H., Yakura, H., Zhang, H., Wong, H. M., Ng, I., Noble, I., Jumelet, J., Geissinger, J., Kernion, J., Hilton, J., Lee, J., Fisac, J. F., Simon, J. B., Koppel, J., Zheng, J., Zou, J., Kocon, J., Thompson, J., Wingfield, J., Kaplan, J., Radom, J., Sohl-Dickstein, J., Phang, J., Wei, J., Yosinski, J., Novikova, J., Bosscher, J., Marsh, J., Kim, J., Taal, J., Engel, J., Alabi, J., Xu, J., Song, J., Tang, J., Waweru, J., Burden, J., Miller, J., Balis, J. U., Batchelder, J., Berant, J., Frohberg, J., Rozen, J., Hernandez-Orallo, J., Boudeman, J., Guerr, J., Jones, J., Tenenbaum, J. B., Rule, J. S., Chua, J., Kanclerz, K., Livescu, K., Krauth, K., Gopalakrishnan, K., Ignatyeva, K., Markert, K., Dhole, K. D., Gimpel, K., Omondi, K., Mathewson, K., Chiafullo, K., Shkartu, K., Shridhar, K., McDonell, K., Richardson, K., Reynolds, L., Gao, L., Zhang, L., Dugan, L., Qin, L., Contreras-Ochando, L., Morency, L.-P., Moschella, L., Lam, L., Noble, L., Schmidt, L., He, L., Colon, L. O., Metz, L., Senel, L. K., Bosma, M., Sap, M., ter Hoeve, M., Farooqi, M., Faruqui, M., Mazeika, M., Baturan, M., Marelli, M., Maru, M., Quintana, M. J. R., Tolkiehn, M., Giulianelli, M., Lewis, M., Potthast, M., Leavitt, M. L., Hagen, M., Schubert, M., Baitenirova, M. O., Arnaud, M., McElrath, M., Yee, M. A., Cohen, M., Gu, M., Ivanitskiy, M., Starritt, M., Strube, M., Swedrowski, M., Bevilacqua, M., Yasunaga, M., Kale, M., Cain, M., Xu, M., Suzgun, M., Walker, M., Tiwari, M., Bansal, M., Aminnaseri, M., Geva, M., Gheini, M., T, M. V., Peng, N., Chi, N. A., Lee, N., Krakover, N. G.-A., Cameron, N., Roberts, N., Doiron, N., Martinez, N., Nangia, N., Deckers, N., Muennighoff, N., Keskar, N. S., Iyer, N. S., Constant, N., Fiedel, N., Wen, N., Zhang, O., Agha, O., Elbaghdadi, O., Levy,O., Evans, O., Casares, P. A. M., Doshi, P., Fung, P., Liang, P. P., Vicol, P., Alipoormolabashi, P., Liao, P., Liang, P., Chang, P., Eckersley, P., Hutt, P. M., Hwang, P., Mikkowski, P., Patil, P., Pezeshkpour, P., Oli, P., Mei, Q., Lyu, Q., Chen, Q., Banjade, R., Rudolph, R. E., Gabriel, R., Habacker, R., Risco, R., Milliere, R., Garg, R., Barnes, R., Saurous, R. A., Arakawa, R., Raymaekers, R., Frank, R., Sikand, R., Novak, R., Sittelew, R., LeBras, R., Liu, R., Jacobs, R., Zhang, R., Salakhutdinov, R., Chi, R., Lee, R., Stovall, R., Teehan, R., Yang, R., Singh, S., Mohammad, S. M., Anand, S., Dillavou, S., Shleifer, S., Wiseman, S., Gruetter, S., Bowman, S. R., Schoenholz, S. S., Han, S., Kwatra, S., Rous, S. A., Ghazarian, S., Ghosh, S., Casey, S., Bischoff, S., Gehrmann, S., Schuster, S., Sadeghi, S., Hamdan, S., Zhou, S., Srivastava, S., Shi, S., Singh, S., Asaadi, S., Gu, S. S., Pachchigar, S., Toshniwal, S., Upadhyay, S., Shyamolima, Debnath, Shakeri, S., Thormeyer, S., Melzi, S., Reddy, S., Makini, S. P., Lee, S.-H., Torene, S., Hatwar, S., Dehaene, S., Divic, S., Ermon, S., Biderman, S., Lin, S., Prasad, S., Piantadosi, S. T., Shieber, S. M., Misherghi, S., Kiritchenko, S., Mishra, S., Linzen, T., Schuster, T., Li, T., Yu, T., Ali, T., Hashimoto, T., Wu, T.-L., Desbordes, T., Rothschild, T., Phan, T., Wang, T., Nkinyili, T., Schick, T., Kornev, T., Tunduny, T., Gerstenberg, T., Chang, T., Neeraj, T., Khot, T., Shultz, T., Shaham, U., Misra, V., Demberg, V., Nyamai, V., Raunak, V., Ramasesh, V., Prabhu, V. U., Padmakumar, V., Srikumar, V., Fedus, W., Saunders, W., Zhang, W., Vossen, W., Ren, X., Tong, X., Zhao, X., Wu, X., Shen, X., Yaghoobzadeh, Y., Lakretz, Y., Song, Y., Bahri, Y., Choi, Y., Yang, Y., Hao, Y., Chen, Y., Belinkov, Y., Hou, Y., Hou, Y., Bai, Y., Seid, Z., Zhao, Z., Wang, Z., Wang, Z., Wang, Z., and Wu, Z. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. _TMLR_, 2023. * Sumers et al. (2023) Sumers, T. R., Yao, S., Narasimhan, K., and Griffiths, T. L. Cognitive Architectures for Language Agents. _arXiv_, 2023. * Suzgun et al. (2023) Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q., Chi, E., Zhou, D., and Wei, J. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. In _ACL_, 2023. * Talmor et al. (2019) Talmor, A., Herzig, J., Lourie, N., and Berant, J. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In _NAACL-HLT_, 2019. * Taori et al. (2023) Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,, C. G., Liang, P., and Hashimoto, T. B. Alpaca: A Strong, Replicable Instruction-Following Model. _arXiv_, 2023. * Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. LLAMA: Open and Efficient Foundation Language Models. _arXiv_, 2023a. * Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open Foundation and Fine-Tuned Chat Models. _arXiv_, 2023b. * Trischler et al. (2017) Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K. NewsQA: A Machine Comprehension Dataset. In _ACL_, 2017. * Wan et al. (2023) Wan, X., Sun, R., Dai, H., Arik, S. O., and Pfister, T. Better Zero-Shot Reasoning with Self-Adaptive Prompting. In _ACL_, 2023. * Wang et al. (2019) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In _ICLR_, 2019. * Wang and Komatsuzaki (2021) Wang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. _GitHub_, 2021. * Wang et al. (2023a) Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., Zhao, W. X., Wei, Z., and Wen, J.-R. A Survey on Large Language Model based Autonomous Agents. _arXiv_, 2023a. * Wang et al. (2023b) Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lim, E.-P. Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. In _ACL_, 2023b. * Wang et al. (2023c) Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In _ICLR_, 2023c. * Wang et al. (2023d) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In _ACL_, 2023d. Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned Language Models are Zero-Shot Learners. In _ICLR_, 2022a. * Wei et al. (2022b) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In _NeurIPS_, 2022b. * Wu et al. (2023) Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., and Mann, G. BloombergGPT: A Large Language Model for Finance. _arXiv_, 2023. * Xi et al. (2023) Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., Zheng, R., Fan, X., Wang, X., Xiong, L., Zhou, Y., Wang, W., Jiang, C., Zou, Y., Liu, X., Yin, Z., Dou, S., Weng, R., Cheng, W., Zhang, Q., Qin, W., Zheng, Y., Qiu, X., Huang, X., and Gui, T. The Rise and Potential of Large Language Model Based Agents: A Survey. _arXiv_, 2023. * Xu et al. (2023) Xu, B., Yang, A., Lin, J., Wang, Q., Zhou, C., Zhang, Y., and Mao, Z. ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. _arXiv_, 2023. * Xu et al. (2022) Xu, Y., Zhu, C., Wang, S., Sun, S., Cheng, H., Liu, X., Gao, J., He, P., Zeng, M., and Huang, X. Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention. In _IJCAI_, 2022. * Yao et al. (2023a) Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. _arXiv_, 2023a. * Yao et al. (2023b) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. ReAct: Synergizing Reasoning and Acting in Language Models. In _ICLR_, 2023b. * Yuan et al. (2024) Yuan, Y., Tang, K., Shen, J., Zhang, M., and Wang, C. Measuring social norms of large language models, 2024. * Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can a Machine Really Finish Your Sentence? In _ACL_, 2019. * Zhang et al. (2020) Zhang, J., Zhao, Y., Saleh, M., and Liu, P. J. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. In _ICML_, 2020. * Zhang et al. (2022a) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. OPT: Open Pre-trained Transformer Language Models. _arXiv_, 2022a. * Zhang et al. (2022b) Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic Chain of Thought Prompting in Large Language Models. In _ICLR_, 2022b. * Zhao et al. (2023) Zhao, X., Xie, Y., Kawaguchi, K., He, J., and Xie, Q. Automatic Model Selection with Large Language Models for Reasoning. _arXiv_, 2023. * Zheng et al. (2023) Zheng, C., Liu, Z., Xie, E., Li, Z., and Li, Y. Progressive-Hint Prompting Improves Reasoning in Large Language Models. _arXiv_, 2023. * Zhong et al. (2023) Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. _arXiv_, 2023. * Zhou et al. (2023a) Zhou, A., Wang, K., Lu, Z., Shi, W., Luo, S., Qin, Z., Lu, S., Jia, A., Song, L., Zhan, M., and Li, H. Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification. _arXiv_, 2023a. * Zhou et al. (2023b) Zhou, W., Jiang, Y. E., Li, L., Wu, J., Wang, T., Qiu, S., Zhang, J., Chen, J., Wu, R., Wang, S., Zhu, S., Chen, J., Zhang, W., Zhang, N., Chen, H., Cui, P., and Sachan, M. Agents: An Open-source Framework for Autonomous Language Agents. _arXiv_, 2023b. * Zoph et al. (2022) Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and Fedus, W. ST-MoE: Designing Stable and Transferable Sparse Expert Models. _arXiv_, 2022."
    },
    {
      "title": "Appendix A Experimental Setup",
      "text": ""
    },
    {
      "title": "Datasets",
      "text": "We evaluate zero-shot AgentInstruct on a wide selection of datasets consisting of all HELM core scenarios (Liang et al., 2023) and all the reasoning datasets used to benchmark zero-shot CoT (Kojima et al., 2022). The datasets are listed in Table 4, along with their categorizations. See Figure 12 for general details and A.4 for the collection and processing of each dataset in more detail."
    },
    {
      "title": "Models",
      "text": "We focus on the following large language models: * Vicuna-13b (Chiang et al., 2023): A finetuned version of LLaMA-13b (Touvron et al., 2023a) which was trained on an additional 70 thousand user-shared conversations from ShareGPT (ShareGPT, 2023). Version 1.1 weight deltas were used. * Llama-2-chat (Touvron et al., 2023b): A family of chat models based on Llama-2. Llama-2-chat was further trained using supervised finetuning, reward modeling, and reinforcement learning from human feedback, and comes in three sizes: 7b, 13b, and 70b. We focus on the 70b model, but share results from the 7b and 13b models in Figure 7 and Appendix C. * GPT-3.5 Turbo (OpenAI, 2022): A finetuned version of GPT-3.5 which was further trained using reinforcement learning from human feedback. Specifically, the 0301 version was used."
    },
    {
      "title": "Implementation Details",
      "text": ""
    },
    {
      "title": "A.3.1 Zero-Shot Agentinstruct Details",
      "text": "Agent InstructionsWe implement our agent based on LangChain's zero-shot ReAct implementation (Chase, 2022; Yao et al., 2023b). To initiate instruction generation, we build the agent input based on basic dataset information including the name of the dataset, possible outputs (labels if applicable, else type of dataset like generation), and a few input-only examples without ground truth labels. We use GPT-4 (OpenAI, 2023) inside our agent with default temperature of 0.3 and the snapshot gpt-4-0613 when generating instructions. Given the temperature, if rerun it is not guaranteed to generate the same outputs each time, nor is there any guarantee that the same steps of our pipeline are reached. In the case that instructions were not returned, we reran the pipeline until there were valid instructions. We also reran the pipeline on a select number of datasets (NaturalQA (openbook), GSM8K) where the name of the dataset was unhelpful. In this case, we use an alias for the name of the dataset. Using the GPT-3.5 tokenizer (cl100k_base), the average number of tokens in the instructions is approximately 265. Based on a rough estimate, the average number of tokens used when generating instructions is 7000 per dataset. Below we provide an end-to-end example for instruction generation with IMDB: 1. We start by collecting the dataset name, input-only example, and possible outputs. On IMDB, we collect the following information: dataset_name = \"IMDB* possible_outputs = \"Negative\", \"Positive\" inputonlyexamples = [ 1. Passage: Lemen (Gulliver (Ted Danson) is a doctor who goes missing at sea, leaving pregnant wife Mary (Mary Steenburger) behind... 2. Passage: To tell you the truth, I do not speak Tamil, and I did not understand the film... 3. Passage: The main reason I loved this movie is because IMR (formerly Immature) were in it... 4. Passage: During the 13 years of schooling I had from Kindergarten through high school, there was only one day that my class took a field trip... 5. Passage: I'm not sure whether i like this film or not. I think it is creepy and completely weird... ] [MISSING_PAGE_EMPTY:18] 6. Now, the agent will come up with intermediate Thought, Action, Observation steps using the \"Ask_about_dataset\" tool. On IMDB, the agent's intermediate steps, including the queries and results to the vector database, are ``` **Thought:** I need to create instructions for the IMDb dataset. The IMDb dataset is a large database of information related to films, television programs, video games, and streaming content online. It's likely that the dataset contains information such as movie titles, genres, directors, actors, user ratings, and possibly user reviews. The prompt template suggests that the task involves determining the sentiment of a passage, which could be a movie review. The possible outputs are \"Negative' and \"Positive\", indicating that the task is a binary sentiment analysis task. **Action:** Ask_about_dataset(Nhxt are the features and labels in the IMDb dataset?) **Observation:** The IMDb dataset consists of several subsets, each with different features:\\n\\n\\n, title:Ask_ts.vq:Features include titleId, ordering, title, region, language, types, attributes, isoriginalTitle.\\n\\n\\n\\n2. title.basics.tsvq:Features include tosust, titleType, primaryTitle, originalTitle, isAdult, startYear, endYear, runtimtimethouses, genres.\\n\\n\\n3, title.cret.vq:Features include tconst, directors, writers.\\n\\n\\n4, title.episode.tsvq:Features include tconst, parentTconst, seasonNumber, episodeNumber.\\n\\n5, title.principals.tsvq:Features include tconst, ordering, nconst, category, job, characters.\\n\\n6, title.ratings.tsvq:Features include tconst, averageRating, numVotes.\\n\\n7, name.basics.tsvq:2Features include nconst, primaryName, birthYear, deathYear, primaryProfession, knownForFiles.\\n\\nIn the IMDb Movie Reviews dataset, the features are the text of the reviews and the labels are binary, indicating whether the review is positive (score 2265 7 out of 10) or negative (score 2264 4 out of 10). **Thought:** The IMDb dataset is quite extensive and contains a lot of information about movies, including titles, genres, directors, writers, ratings, and more. However, for the task at hand, we are interested in the IMDb Movie Reviews dataset, which contains the text of the reviews and binary labels indicating whether the review is positive or negative. The sentiment of a review is determined based on the score given by the reviewer, with scores of 7 or higher out of 10 considered positive and scores of 4 or lower considered negative. This information will be useful in creating the instructions. **Action:** Ask_about_dataset(Can you provide more details about the IMDb Movie Reviews dataset? Specifically, how is the sentiment of the reviews determined and how are the reviews structured?) **Observation:** The IMDb Movie Reviews dataset is a binary sentiment analysis dataset that consists of 50,000 reviews from the Internet Movie Database (IMDB). These reviews are labeled as either positive or negative. The dataset contains an even number of positive and negative reviews. \\n\\n\\nThe sentiment of the reviews is determined based on the score given by the reviewer. Only highly polarizing reviews are considered. A review is labeled as negative if it has a score of 4 or less out of 10, and it is labeled as positive if it has a score of 7 or more out of 10. \\n\\nThe dataset does not include more than 30 reviews per movie. In addition to the labeled reviews, the dataset also contains additional unlabeled data. The structure of the reviews is not explicitly mentioned in the provided context. 7. After the agent decides it's done, it uses the finish action and returns the final instructions. On IMDB, the instructions generated are ``` 1. Understand the Dataset: The IMDb Movie Reviews dataset contains 50,000 reviews from the Internet Movie Database (IMDB). These reviews are labeled as either positive or negative, with an even number of each. The sentiment of the reviews is determined based on the score given by the reviewer. A review is labeled as negative if it has a score of 4 or less out of 10, and it is labeled as positive if it has a score of 7 or more out of 10. The dataset does not include more than 30 reviews per movie.\\n\\n2. Analyze the Passage: Read the passage carefully. This will be a review from the IMDb dataset. Pay attention to the language used, the one of the review, and any explicit or implicit expressions of sentiment towards the movie. \\n\\n3. Determine the Sentiment: Based on your analysis of the reviews, determine whether the sentiment is positive or negative. Remember that positive reviews generally contain praise for the movie, its plot, characters, cinematography, etc., and have a score of 7 or more out of 10. Negative reviews, on the other hand, generally contain criticism of the movie and have a score of 4 or less out of 10.\\n\\n4. Answer the Question: Based on your determination of the sentiment, select the appropriate output: 'Negative' or 'Positive'.\\n\\nRemember, the goal is to correctly identify the sentiment of the review based on the text provided. ``` \\begin{table} \\begin{tabular}{l c c c} \\hline \\hline Dataset & Task Category & Metric & Number of Instances \\\\ \\hline AddSub (Koncel-Kedziorski et al., 2016) & Generation, Reasoning & QEM & 395 \\\\ AQuA (Ling et al., 2017) & Classification, Reasoning & EM & 254 \\\\ BooIQ (Clark et al., 2019) & Classification & QEM & 1,000 \\\\ CivilComments (Koh et al., 2021; Borkan et al., 2019) & Classification & QEM & 6,688 \\\\ All & Classification & QEM & 765 \\\\ Black & Classification & QEM & 735 \\\\ Christian & Classification & QEM & 733 \\\\ Female & Classification & QEM & 730 \\\\ LGBTQ & Classification & QEM & 735 \\\\ Male & Classification & QEM & 748 \\\\ Muslim & Classification & QEM & 760 \\\\ Other Religions & Classification & QEM & 767 \\\\ White & Classification & QEM & 715 \\\\ CNN/Daily Mail (See et al., 2017) & Generation & ROUGE-2 & 466 \\\\ Coin Flip (Wei et al., 2022b) & Classification, Reasoning & QEM & 500 \\\\ CommonsenseQA (Talmor et al., 2019) & Classification, Reasoning & EM & 1,000 \\\\ Date Understanding (Srivastava et al., 2023) & Classification, Reasoning & QEM & 369 \\\\ GSM8K (Cobbe et al., 2021) & Generation, Reasoning & QEM & 1,000 \\\\ Hellaswag (Zellers et al., 2019) & Classification & EM & 1,000 \\\\ IMDB (Maas et al., 2011) & Classification & QEM & 1,000 \\\\ Last Letter Concatenation (Wei et al., 2022b) & Generation, Reasoning & QEM & 500 \\\\ MMLU (Hendrycks et al., 2021) & Classification & EM & 514 \\\\ Abstract Algebra & Classification & EM & 100 \\\\ College Chemistry & Classification & EM & 100 \\\\ Computer Security & Classification & EM & 114 \\\\ Econometrics & Classification & EM & 100 \\\\ US Foreign Policy & Classification & RM@10 & 1,000 \\\\ MS MARCO (Regular) & Classification & NDCG@10 & 43 \\\\ MultiArith (Roy \\& Roth, 2015) & Generation, Reasoning & QEM & 600 \\\\ NarrativeQA (Kociský et al., 2018) & Generation & F1 & 355 \\\\ NaturalQuestions (closed-book) (Kwiatkowski et al., 2019) & Generation & F1 & 1,000 \\\\ NaturalQuestions (open-book) & Generation & F1 & 1,000 \\\\ NewsQA (Trischler et al., 2017) & Generation & F1 & 1,000 \\\\ OpenBookQA (Mihaylov et al., 2018) & Classification & EM & 500 \\\\ QuAC (Choi et al., 2018) & Generation & F1 & 1,000 \\\\ RAFT (Alex et al., 2021) & Classification & QEM & 440 \\\\ ADE Corpus v2 & Classification & QEM & 40 \\\\ Banking 77 & Classification & QEM & 40 \\\\ Neurips Impact Statement & Classification & QEM & 40 \\\\ One Stop English & Classification & QEM & 40 \\\\ Overruling & Classification & QEM & 40 \\\\ Semiconductor Org Types & Classification & QEM & 40 \\\\ Systematic Review Inclusion & Classification & QEM & 40 \\\\ Tai Safety Research & Classification & QEM & 40 \\\\ Terms of Service & Classification & QEM & 40 \\\\ Tweet Eval Hate & Classification & QEM & 40 \\\\ Twitter Complaints & Classification & QEM & 40 \\\\ Shuffled Objects (Srivastava et al., 2023) & Classification, Reasoning & QEM & 2,750 \\\\ Five Objects & Classification, Reasoning & QEM & 1,000 \\\\ Seven Objects & Classification, Reasoning & QEM & 1,000 \\\\ Three Objects & Classification, Reasoning & QEM & 750 \\\\ SingleEq (Koncel-Kedziorski et al., 2016) & Generation, Reasoning & QEM & 508 \\\\ StrategyQA (Geva et al., 2021; Srivastava et al., 2023) & Classification, Reasoning & QEM & 458 \\\\ SVAMP (Patel et al., 2021) & Generation, Reasoning & QEM & 1,000 \\\\ TruthfulQA (Lin et al., 2022) & Classification & EM & 654 \\\\ XSUM (Narayan et al., 2018) & Generation & ROUGE-2 & 518 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Overview of each dataset used. [MISSING_PAGE_EMPTY:21] Initial experimentation with formatting revealed that Llama-2-chat was very sensitive to this designation, and would often fail to generate thoughtful reasoning steps if omitted during runs with zero-shot AgentInstruct or standard zero-shot CoT. We made no such User/Assistant distinction before the answer extraction prompts. For Vicuna and GPT-3.5 Turbo runs, we made no User/Assistant distinctions either, effectively treating all model input as part of the User role. Generally, we use each dataset's default generation parameters outlined in HELM. All inference was done using a temperature of 0.0, except on datasets that involved summarization, where a temperature of 0.3 was used. For the reasoning extraction prompt, we request a maximum of 512 new tokens, and for the answer extraction prompt, the number of tokens requested is specific to each dataset."
    },
    {
      "title": "A.3.2 Evaluation Details",
      "text": "Each dataset uses one of the following accuracy metrics: * Exact Match (EM): A binary indicator that awards correctness if and only if the output exactly matches one or more of the correct references. We primarily use this metric for classification, in particular for multi-choice classification tasks. * Quasi-Exact Match (QEM): A more lenient version of Exact Match which awards correctness if the output text matches the correct reference after both have been normalized (i.e., after removing extra white space, articles, punctuation, and capitalization) (Choi et al., 2018). This metric is primarily used for classification tasks, as well as some generation tasks. * Quasi-Prefix Exact Match (QPEM): A more lenient version of Quasi-Exact Match that awards correctness if the normalized output begins with the normalized reference. Hence, \"Yes, Elmendorf\" would be marked correct if the reference was \"Yes\". The metric is useful on several classification tasks where the model has a tendency to generate additional tokens beyond that of the correct answer. * F1: Instead of checking for an exact match, this metric will award partial correctness for any string overlap (Rajpurkar et al., 2016). The metric is used for a number of question answering tasks with open-ended generation. * ROUGE-2: This metric scores correctness based on bigram overlap (Lin, 2004). We use ROUGE-2 as the accuracy measure for generation tasks involving summarization. * RR@10: This metric is used for MS MARCO (Regular) and it uses the reciprocal rank of the first relevant document. * NDCG@10: This metric is used for MS MARCO (TREC) to assess the normalized discounted cumulative gain using the set of graded rankings (Jarvelin & Kekalainen, 2002). Prior to evaluation, we often perform some post-processing to clean the generated text. On mathematical generation datasets, we parse out the first number in the output and use that for our final answer. Similarly, on classification (multi-choice) datasets, we grab the first letter in the output. Appendix A.4 gives a compact description of each dataset, as well as the accuracy metric used, and any initial or post-processing done. These choices followed HELM or other relevant papers. In general, we report evaluation results using the default accuracy metric outlined in HELM. For datasets with various subsets, we report the macro-average accuracy score across all subsets. Prompts, parsing techniques, and instructions were tested using 50 samples on most datasets to ensure the explanations and parsing were valid. Note that we are including these 50 samples in the full (up to) 1,000 sample evaluation, so there is a possibility of bias, especially regarding some of the Figure 15: Answer extraction prompt for the CoT reasoning of zero-shot AgentInstruct on a classification (multi-choice) dataset. Classification (multi-choice) datasets that also belong to the reasoning category use the same prompt. smaller datasets. Note that all experiments done in this paper are the result of a single run on our pipeline. We only generate instructions once and run inference once as well due to the cost of running our models on all datasets with a high amount of samples."
    },
    {
      "title": "Additional Information About Each Dataset",
      "text": ""
    },
    {
      "title": "A.4.1 Addsub",
      "text": "DescriptionAddSub (Koncel-Kedziorski et al., 2016) is a generation dataset containing addition and subtraction math word problems. We also classify AddSub as a reasoning dataset because of its inclusion in Kojima et al. (2022). The dataset contains 395 test instances. Implementation DetailsAddSub is a generation dataset. Specifically, we prompt for an answer that is at most 10 tokens. During few-shot runs, we use the 5 in-context learning examples extracted from the math word problem instances presented in the chain of thought paper (Wei et al., 2022). Responses undergo a post-processing step where the first number in the output is extracted for evaluation. The evaluation metric used is quasi-exact match. ExamplesAn example is shown in Figure 19. Comparison MethodsThe few-shot results on the GPT-4 model achieved the state-of-the-art (Zhao et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.2 Aqua",
      "text": "DescriptionAQuA (Ling et al., 2017) is a classification (multi-choice) dataset containing various math word problems. Because it is in Kojima et al. (2022), we consider it a reasoning dataset. The dataset contains 254 test instances. Implementation DetailsWe prompt for an answer that is at most 5 tokens. Responses undergo a post-processing step where the first letter in the output is extracted for evaluation. The evaluation metric used is exact match. ExamplesAn example is shown in Figure 20. Comparison MethodsThe few-shot results on the GPT-4 model achieved the state-of-the-art (Zheng et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.3 Boiq",
      "text": "DescriptionBoolQ (Clark et al., 2019) is a classification benchmark where yes/no questions are asked about a given passage. We sample 1,000 test instances following HELM. Implementation DetailsBoolQ is a core scenario in HELM (Liang et al., 2023), and we use the implementation parameters outlined in HELM. Specifically, we prompt for an answer that is at most 5 tokens. Quasi-prefix exact match is used for evaluation. ExamplesAn example is shown in Figure 21. Comparison MethodsThe result of finetuned ST-MoE-32B achieved the state-of-the-art (Zoph et al., 2022). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.4 Civilcomments",
      "text": "DescriptionCivil Comments (Koh et al., 2021; Borkan et al., 2019) is a classification dataset to identify toxicity in online comments. The dataset is split by demographic (All, Black, Christian, Female, LGBTQ, Male, Muslim, Other religigions,White). We sample 1,000 test instances from each subset following HELM. Implementation DetailsCivilComments is a core scenario in HELM, and we use the implementation parameters outlined in HELM. We prompt for an answer that is at most 5 tokens. Following HELM, we report the macro-average accuracy of each subset below with respect to the toxic/non-toxic classes, as well as the average accuracy across all subsets. We found it necessary to map yes/no outputs to true/false during zero-shot runs. Quasi-prefix exact match is used for evaluation. ExamplesExamples are shown in Figures 22, 23, 24, 25, 26, 27, 28, 29, 30. Comparison MethodsThe few-shot results on the GPT-3.5 Turbo model achieved the state-of-the-art (Liang et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.5. Cnn/Daily Mail",
      "text": "DescriptionCNN/Daily Mail (See et al., 2017) is a generation dataset where the goal is to summarize a selection of news articles. The dataset contains 466 test instances. Implementation DetailsCNN/Daily Mail is a core scenario in HELM. We generate a summary of no more than 128 tokens and use a temperature of 0.3. The ROUGE-2 metric is used for evaluation. ExamplesAn example is shown in Figure 31. Comparison MethodsThe result of the PEGASUS\\({}_{\\text{LARGE}}\\) (HugeNews) model achieved the state-of-the-art (Zhang et al., 2020). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.6. Coin Flip",
      "text": "DescriptionCoin Flip (Wei et al., 2022b) is a classification task where the objective is to determine the state of a coin after a number of successive flips. We consider Coin Flip a reasoning task because it is included in Kojima et al. (2022). The dataset contains 500 test instances. Implementation DetailsWe recreated the Coin Flip dataset following the details in Kojima et al. (2022). We generate a response of no more than 4 tokens and use quasi-exact match for evaluation. ExamplesAn example is shown in Figure 32. Comparison MethodsThe result of the GPT-3.5 (text-davinci-002) model achieved the state-of-the-art (Zhang et al., 2022b). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.7. Commonsenseqa",
      "text": "DescriptionCommonsenseQA (Talmor et al., 2019) is a classification (multi-choice) dataset focusing on common sense. Because it is in Kojima et al. (2022), we consider it a reasoning dataset. We sample 1,000 instances from the test set mirroring the existing implementation in HELM. Implementation DetailsThough CommonsenseQA was already implemented into HELM, we added a separate implementation to better align with Kojima et al. (2022). We use the joint multiple-choice adapter in HELM rather than the separate multiple-choice adapter. We generate a response of no more than 5 tokens, and grab the first letter from the output and then use exact match for evaluation. ExamplesAn example is shown in Figure 33. Comparison MethodsThe result of the DeBERTaV3-large model achieved the state-of-the-art (Xu et al., 2022). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.8 Date Understanding",
      "text": "DescriptionDate Understanding (Srivastava et al., 2023; Suzgun et al., 2023) is a classification (multi-choice) dataset from the BIG-bench benchmark (Srivastava et al., 2023), which aims to assess pretrained language models' capacity to comprehend dates through inquiries about specific days' dates. The dataset contains 369 test instances. Implementation DetailsWe use the Date Understanding instances from Suzgun et al. (2023). We generate up to 60 tokens, though in practice, the generation ends much quicker when it hits a stop token: either \"\\(\\backslash\\)n\" or \"\\(\\rangle\\)\". As with other classification multi-choice, we extract the first letter from the response. Quasi-exact match is used as the evaluation metric. ExamplesAn example is shown in Figure 34. Comparison MethodsThe few-shot CoT results on the PaLM 2 model achieved the state-of-the-art (Anil et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.9 Gsm8K",
      "text": "DescriptionGSM8K (Cobbe et al., 2021) is a generation dataset focused on mathematical reasoning. Because it is in Kojima et al. (2022), we consider it a reasoning dataset. We sample 1,000 test instances following HELM. Implementation DetailsGSM8K is included in HELM as a target scenario. We leverage this implementation, which generates up to 400 new tokens. Responses undergo a post-processing step where the first number in the output is extracted for evaluation. The evaluation metric used is quasi-exact match. ExamplesAn example is shown in Figure 35. Comparison MethodsThe result of the GPT-4 code interpreter achieved the state-of-the-art (Zhou et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.10 Hellaswag",
      "text": "DescriptionHellaSwag (Zellers et al., 2019) is a classification (multi-choice) dataset comprised of 70,000 multiple-choice questions about grounded situations, originating from ActivityNet and wikiHow domains, with human-verified adversarial incorrect answers and one correct answer aimed at studying grounded commonsense inference. We sample 1,000 test instances following HELM. Implementation DetailsWe use the implementation of HellaSwag in HELM, but use the joint adapter to mimic a standard multiple-choice question, rather than the separate adapter which scores each reference separately using the sentence probability normalized by the sentence length. We generate only a single token. During zero-shot and zero-shot CoT runs, we include the following instructions from HELM: \"The following are multiple choice questions (with answers) about common sense.\" The exact match metric is used for evaluation. Like with other classification (multi-choice) datasets, we parse out the first letter from the response for evaluation. ExamplesAn example is shown in Figure 36. Comparison MethodsThe few-shot results on the GPT-4 model achieved the state-of-the-art (OpenAI, 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.11. Imdb",
      "text": "DescriptionIMDB (Maas et al., 2011) is a classification dataset comprising 50,000 movie reviews, with 25,000 for training and 25,000 for testing, focused on binary sentiment classification. We sample 1,000 test instances following HELM. Implementation DetailsIMDB is a core scenario in HELM, and we follow their implementation. We generate up to 5 new tokens. During few-shot runs, we sampled 5 in-context examples from the IMDB training set. Quasi-exact match is used for evaluation. ExamplesAn example is shown in Figure 37. Comparison MethodsThe result of the ERNIE-DOC-Large model achieved the state-of-the-art (Ding et al., 2021). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.12. Last Letter Concatenation",
      "text": "DescriptionLast Letter Concatenation (Wei et al., 2022b) is a generation task where the objective is to concatenate the last letters of a series of two words. Because it is in Kojima et al. (2022), we consider it a reasoning dataset. We generate full names by randomly concatenating names from the top one-thousand first and last names from name census data ([https://namecensus.com/](https://namecensus.com/)), as referenced in Wei et al. (2022b). The dataset contains 500 test instances. Implementation DetailsWe recreated the Last Letter Concatenation dataset following the details in Kojima et al. (2022); Wei et al. (2022b). Unlike Kojima et al. (2022) which does concatenation on 4 words, we only do so on 2. We generate a response of no more than 4 tokens. We noticed that quasi-exact match unfairly penalized zero-shot CoT runs, so for all runs we extract the answer directly from the generated text, grabbing only the first 2 letters after stripping away white space and punctuation. ExamplesAn example is shown in Figure 38. Comparison MethodsThe few-shot CoT results on the PaLM-540B model achieved the state-of-the-art (Wei et al., 2022b). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.13. Mmlu",
      "text": "DescriptionMMLU (Hendrycks et al., 2021) is a classification (multi-choice) benchmark comprised of 57 diverse subjects, ranging from elementary to professional levels, encompassing STEM, humanities, social sciences, and specialized fields. Following HELM, we focus on 5 subjects (Abstract Algebra, College Chemistry, Computer Security, Econometrics, and US Foreign Policy) totaling 514 test instances. Implementation DetailsMMLU is a core HELM scenario, and we use HELM's implementation of the dataset. We prompt for a single token. HELM's implementation includes the following instructions, where \"{subject}\" is replaced with the subject of the dataset: \"The following are multiple choice questions (with answers) about {subject}.\" We prepend these instructions to instances during zero-shot and zero-shot CoT runs. We parse out the first letter from the response, and the exact match metric is used for evaluation. ExamplesExamples are shown in Figures 39, 40, 41, 42, 43. Comparison MethodsThe few-shot results on the Palmyra X (43B) model achieved the state-of-the-art (Liang et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.14. Ms Marco (Regular)",
      "text": "DescriptionMS MARCO (Bajaj et al., 2016; Nogueira et al., 2020; MacAvaney et al., 2021) is a classification dataset containing real Bing search queries and nearly 9 million web passages, annotated for relevance. It consists of two tracks: the Regular track uses binary RR@k metric, while the TREC track employs graded annotations with NDCG as the main metric, featuring more extensively annotated queries and passages with relevance grades. We sample 1,000 test instances for the Regular track following HELM. Implementation DetailsWe use the default HELM implementation for MS MARCO (Regular) and evaluate using RR@10. ExamplesAn example is shown in Figures 44. Comparison MethodsThe few-shot results on the Cohere Command beta (52.4B) model achieved the state-of-the-art (Liang et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.15. Ms Marco (Trec)",
      "text": "DescriptionThis is the TREC track of MS MARCO (Bajaj et al., 2016; Nogueira et al., 2020; MacAvaney et al., 2021) as introduced in Appendix A.4.14. The TREC track contains 43 instances. Implementation DetailsWe use the default HELM implementation for MS MARCO (TREC) and evaluate using NDCG@10. ExamplesAn example is shown in Figure 45. Comparison MethodsThe few-shot results on the Cohere Command beta (52.4B) model achieved the state-of-the-art (Liang et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.16. Multiarith",
      "text": "DescriptionMultiArith (Roy and Roth, 2015) is a generation dataset containing a comprehensive collection of arithmetic word problems, encompassing various mathematical operations and ranging in complexity. Because it is in Kojima et al. (2022) as a reasoning task, we consider it a reasoning dataset. The dataset contains 600 test instances. Implementation DetailsWe prompt for up to 10 new tokens. As with other math generation datasets, we extract the first number from the generated text and use that as the final answer. Quasi-exact match is used for evaluation. ExamplesAn example is shown in Figure 46. Comparison MethodsThe result of the self-consistency strategy on the GPT-3 (code-davinci-002) model achieved the state-of-the-art (Wang et al., 2023c). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.17. Narrativeqa",
      "text": "DescriptionNarrativeQA (Kocisky et al., 2018) is a generation dataset focused on reading comprehension where the model answers a question about a passage. The dataset is comprised of 355 test instances. Implementation DetailsWe use the implementation of NarrativeQA in HELM. We generate up to 100 new tokens, and use F1 score for evaluation. For few-shot runs, we sampled 5 in-context examples from the training set. ExamplesAn example is shown in Figure 47. Comparison MethodsThe few-shot results on the Llama 2 (70B) model achieved the state-of-the-art (Liang et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.18. Naturalquestions (Closed-Book)",
      "text": "DescriptionNaturalQuestions (Kwiatkowski et al., 2019) is a generation dataset comprised of anonymized real search queries posed to Google. We focus on two subsets: closed-book where the model is not given any external context, and open-book long answer, where a passage is given prior to the question. We sample 1,000 test instances following HELM. Implementation DetailsWe use the implementation of the Natural Questions dataset within HELM. Up to 300 new tokens are generated, and we use F1 score as the evaluation metric. ExamplesAn example is shown in Figure 48. Comparison MethodsThe few-shot results on the Llama 2 (70B) model achieved the state-of-the-art on the NaturalQuestions (closed-book) dataset (Liang et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.19. Naturalquestions (Open-Book)",
      "text": "DescriptionThis is the open-book subset of NaturalQuestions (Kwiatkowski et al., 2019) as introduced in Appendix A.4.18. Implementation DetailsWe use the same implementation as introduced in Appendix A.4.18. ExamplesAn example is shown in Figure 49. Comparison MethodsThe few-shot results on the text-davinci-003 model achieved the state-of-the-art on the NaturalQuestions (open-book) dataset (Liang et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.20. Newsqa",
      "text": "DescriptionNewsQA (Trischler et al., 2017) is a generation dataset that contains human-generated question-answer pairs sourced from more than 10,000 CNN news articles. We sample 1,000 test instances following HELM. Implementation DetailsWe followed the instructions in HELM to collect and process the NewsQA dataset. We generate up to 50 tokens. F1 score is used as the evaluation metric. ExamplesBecause of restrictive licensing, we cannot publish an example. Comparison MethodsThe result of the SpanBERT model achieved the state-of-the-art (Joshi et al., 2020). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.21 Openbookqa",
      "text": "DescriptionOpenBookQA (Mihaylov et al., 2018) is a classification (multi-choice) dataset focused on commonsense knowledge utilization. It contains 500 test instances. Implementation DetailsWe follow the implementation of OpenbookQA in HELM, but use the joint adapter to mimic a standard multiple-choice question, rather than the separate adapter which scores each reference separately using the sentence probability normalized by the sentence length. We generate only a single token. During zero-shot and zero-shot CoT runs, we include the following instructions from HELM: \"The following are multiple choice questions (with answers) about common sense.\" The first letter from the response is parsed out and the exact match metric is used for evaluation. ExamplesAn example is shown in Figure 50. Comparison MethodsThe result of the fine-tuning method with the self-consistency prompting method on the PaLM-540B model achieved the state-of-the-art (Huang et al., 2022). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.22 Quac",
      "text": "DescriptionQuAC (Choi et al., 2018) is a generation dataset that facilitates information-seeking dialogue modeling by presenting interactive conversations between a student asking open-ended questions and a teacher providing short text excerpts from a hidden Wikipedia passage. There are 1,000 test instances following HELM. Implementation DetailsWe use the default QuAC implementation in HELM. Responses are limited to 100 tokens, and F1 is used to measure accuracy. ExamplesAn example is shown in Figure 51. Comparison MethodsThe result of the FLOWQA model achieved the state-of-the-art (Huang et al., 2019). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.23 Raft",
      "text": "DescriptionRAFT (Alex et al., 2021) is a classification benchmark designed to assess language models' performance across diverse domains. Specifically, we focus on 11 subsets, containing 40 test instances each. Implementation DetailsWe use the default implementation of RAFT in HELM. Responses are limited to 30 tokens. For zero-shot and zero-shot CoT runs, we prepend subset-specific task instructions provided in HELM. We report the average score across the 11 subsets using quasi-exact match as the evaluation metric. ExamplesExamples are shown in Figures 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62. Comparison MethodsThe few-shot results on the GPT-3.5 Turbo model achieved the state-of-the-art (Liang et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.24 Shuffled Objects",
      "text": "DescriptionShuffled Objects (Srivastava et al., 2023; Suzgun et al., 2023) is a classification (multi-choice) dataset of the BIG-bench benchmark. In this dataset, objects initially owned by different people are swapped through pairwise trades, and the model's goal is to accurately determine which person ends up with a particular object. There are 3 subtasks (three objects, five objects, and seven objects) in this dataset. The five and seven objects subtasks each contain 1000 instances, and the three object subtask contains 750 test instances. Implementation DetailsWe use the Shuffled Objects instances from Suzgun et al. (2023). We generate up to 60 tokens, though in practice, the generation ends much quicker when it hits a stop token: either \"\\(\\backslash\\)n\" or \"\\(\\rangle\\)\". Unlike Kojima et al. (2022), we report results on the three, five, and seven object cases, instead of only the three object case. As with other classification (multi-choice) datasets, we parse out the first letter in the response. Quasi-exact match is used as the evaluation metric. ExamplesExamples are shown in Figures 64, 65, 66. Comparison MethodsThe few-shot CoT results on the PaLM 2 model achieved the state-of-the-art (Anil et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.25. Singleeq",
      "text": "DescriptionSingleEq (Koncel-Kedziorski et al., 2016) is a generation dataset containing math word problems requiring the use of single mathematical equations. Because the dataset is included in Kojima et al. (2022), we consider it a reasoning task in addition to generation. The dataset contains 508 test instances. Implementation DetailsWe followed the instructions in Kojima et al. (2022) to collect the SingleEQ dataset. We limit generated responses to 10 tokens. Like other math generation datasets, we parse out the first number from the response, and then use quasi-exact match to measure accuracy. ExamplesAn example is shown in Figure 63. Comparison MethodsThe few-shot results on the GPT-4 model achieved the state-of-the-art (Zhao et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.26. Strategyqa",
      "text": "DescriptionStrategyQA (Geva et al., 2021) is a yes/no classification dataset, though we follow the BIG-bench implementation which treats instances as multiple-choice questions with two options (A. Yes, B. No). Because it is in Kojima et al. (2022), we consider it a reasoning dataset. The dataset contains 458 test instances. Implementation DetailsWe use the StragetyQA implementation through BIG-bench. As a result, our implementation varies from that in Kojima et al. (2022) in that the instances are formulated as a classification (multi-choice) task, rather than an open-ended classification task. We prompt for no more than 64 new tokens. We extract the first letter from the response, and use exact match to measure accuracy. ExamplesAn example is shown in Figure 67. Comparison MethodsThe few-shot CoT results with self-consistency on the PaLM 2 model achieved the state-of-the-art (Anil et al., 2023). ResultsThe results are shown in Table 6."
    },
    {
      "title": "A.4.27. Svamp",
      "text": "DescriptionSVAMP (Patel et al., 2021) is a generation dataset focused on mathematical reasoning containing 1,000 test instances. Because it is in Kojima et al. (2022), we also consider it a reasoning dataset. Implementation DetailsWe prompt for at most 10 tokens. Like other math generation datasets, we parse the first number out of the response, and use quasi-exact match for evaluation. ExamplesAn example is shown in Figure 68. [MISSING_PAGE_FAIL:31] [MISSING_PAGE_FAIL:32] Figure 16: Main results (%) comparing zero-shot, zero-shot CoT and zero-shot AgentInstruct performance on Vicuna-13b (left), Llama-\\(2\\)-70b-chat (middle), and GPT-3.5 Turbo (right). \\begin{table} \\begin{tabular}{l l c c c} \\hline \\hline Category & Model & Zero-Shot (\\%) & Zero-Shot CoT (\\%) & Zero-Shot AgentInstruct (\\%) \\\\ \\hline Overall & Vicuna-13b & 30.6 & 34.4 & 43.9 \\\\ & Llama-2-7b-chat & 32.1 & 38.3 & 44.6 \\\\ & Llama-2-13b-chat & 34.3 & 44.5 & 50.2 \\\\ & Llama-2-70b-chat & 35.1 & 52.4 & 58.3 \\\\ & GPT-3.5 Turbo & 48.1 & 60.8 & 65.1 \\\\ & Average over 5 models & 36.1 & 46.1 & 52.4 \\\\ & Average over 3 models & 37.9 & 49.2 & 55.7 \\\\ \\hline Classification & Vicuna-13b & 41.2 & 42.7 & 50.6 \\\\ & Llama-2-7b-chat & 37.3 & 39.0 & 46.9 \\\\ & Llama-2-13b-chat & 39.2 & 46.1 & 52.8 \\\\ & Llama-2-70b-chat & 44.0 & 51.1 & 61.0 \\\\ & GPT-3.5 Turbo & 54.0 & 62.6 & 68.1 \\\\ & Average over 5 models & 43.2 & 48.3 & 55.9 \\\\ & Average over 3 models & 46.4 & 52.1 & 59.9 \\\\ \\hline Generation & Vicuna-13b & 17.6 & 24.1 & 35.6 \\\\ & Llama-2-7b-chat & 25.7 & 37.4 & 41.7 \\\\ & Llama-2-13b-chat & 28.4 & 42.4 & 47.0 \\\\ & Llama-2-70b-chat & 24.1 & 54.1 & 54.9 \\\\ & GPT-3.5 Turbo & 40.8 & 58.6 & 61.3 \\\\ & Average over 5 models & 27.3 & 43.3 & 48.1 \\\\ & Average over 3 models & 27.5 & 45.6 & 50.6 \\\\ \\hline Reasoning & Vicuna-13b & 26.1 & 29.1 & 45.2 \\\\ & Llama-2-7b-chat & 26.4 & 35.9 & 48.0 \\\\ & Llama-2-13b-chat & 28.5 & 46.9 & 54.8 \\\\ & Llama-2-70b-chat & 26.6 & 62.1 & 69.5 \\\\ & GPT-3.5 Turbo & 46.9 & 70.9 & 78.7 \\\\ & Average over 5 models & 30.9 & 49.0 & 59.2 \\\\ & Average over 3 models & 33.2 & 54.0 & 64.5 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 7: Zero-shot, zero-shot CoT, and zero-shot AgentInstruct results split by category. The table shows the comparison for all five models split by category. The average over three models is based on Vicuna-13b, Llama-2-70b-chat, and GPT 3.5 Turbo."
    },
    {
      "title": "Appendix D Additional Result Analysis",
      "text": ""
    },
    {
      "title": "Additional Descriptions Of Ablation Study",
      "text": "* w/o Agent Instructions: We compare the zero-shot AgentInstruct methodology to zero-shot CoT. This setting is exactly zero-shot CoT on Llama-2-70b-chat. * w/o Input Examples: We remove the examples from the input to the agent. Typically, the agent is given 5 input-only examples to reference when generating the instructions. These examples do not include the ground truth answers. In this setting, we have the agent generate instructions without providing example inputs to the agent. * w/o Label Description: We remove the description of the labels from the input to the agent. Specifically, this is applicable for classification tasks such as IMDB, for example, where the expected outputs are either 'True' or 'False'. For multiple-choice and open-ended generation tasks, we provide a simple description of'multiple choice' or 'generation' instead of providing a list of output labels. With this setting, we have the agent generate instructions without providing any information or description of the output labels for any task type. An example is given in Figure 1 and Figure 2. * w/o GPT-4: We use GPT-3.5 Turbo to generate the instructions instead of GPT-4. Note that we were not able to generate coherent instructions while using the same agent framework due to the model being weaker. So, instead we prompted the model to give instructions in one call without the web."
    },
    {
      "title": "Importance Of Agent Synthesizing Instructions",
      "text": "Essentially, there are two parts to our agent: 1) Collecting the dataset information, including the name and retrieving the websites and 2) Synthesizing that information into human-understandable instructions used to guide an LLM for inference. Though more data is provided for the AgentInstruct step over zero-shot or zero-shot CoT, the way the agent thinks through the data then formulates the instructions is crucial for increasing performance, as it allows smaller models to gain the information in a condensed, well-structured manner. Our belief is that the agent is able to better understand and integrate the dataset information into instructions for a weaker LLM, than a weaker LLM could on its own. To test this, we prepend additional information used in the instruction generation step to the zero-shot and zero-shot CoT settings, so each setting has the same amount of information. The results are below, with \"+ Data Information\" representing the associated setting when it is provided additional information about the dataset. The additional information includes the dataset name, prompt template, possible outputs, and all the queried responses from the retrieval tool (information from the web). Results on Llama-2-70b-chat are shown in Table 8. An example of the additional information provided to zero-shot and zero-shot CoT for the IMDB dataset is in Figure 17. On these datasets, we see AgentInstruct continues to perform better than zero-shot CoT, meaning our method is able to successfully synthesize the information into the instructions in a way that is more useful to guide the reasoning steps than just putting the information in alone. Including information boosts zero-shot performance, but AgentInstruct still wins on all but one of the datasets. In the future, we can also do experiments with instructions but without reasoning steps (based the zero-shot setting, not zero-shot CoT) to see if AgentInstruct retains its performance boost, and can do better on that dataset. However, our main focus is on guiding reasoning steps. It is clear that AgentInstruct offers a substantial improvement over zero-shot CoT, even when additional information is present. In fact, on zero-shot CoT we see decreases in performance on two datasets, AddSub and NarrativeQA, further exemplifying the importance in the structure of our instructions through our agent, as opposed to inclusion of additional information alone, which may be seen as noise. \\begin{table} \\begin{tabular}{l c c c} \\hline \\hline Method & AddSub & IMDB & NarrativeQA \\\\ \\hline Zero-Shot & 36.2 & 43.5 & 45.9 \\\\ Zero-Shot + Data Information & 65.3 & 93.4 & **69.2** \\\\ Zero-Shot CoT & 73.2 & 89.0 & 62.3 \\\\ Zero-Shot CoT + Data Information & 71.6 & 90.5 & 58.2 \\\\ Zero-Shot AgentInstruct & **79.5** & **94.0** & 65.0 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 8: Comparison on Llama-2-70b-chat when providing the dataset information to zero-shot and zero-shot CoT."
    },
    {
      "title": "Manual Prompt Sensitivity",
      "text": "In Table 3, we tested various manual prompts on Llama-2-70b-chat on AddSub. In Table 9, we focus on formatting, specifically whether the instruction should appear before or after the instance. Again, the difference is minor suggesting that the methodology behind zero-shot AgentInstruct is robust."
    },
    {
      "title": "Error Analysis",
      "text": "Overall errors by error type are in Table 10. Further Description of ErrorsWe provide more specific errors for each dataset in Table 11, accounting for all instances in the 25 samples of AddSub, IMDB, and NewsQA respectively. Moreover, for each error category, we provide an example in Table 13. Note that we do not include NewsQA examples due to the dataset's restrictive license. \\begin{table} \\begin{tabular}{l l c} \\hline \\hline & Error Type & Percentage \\\\ \\hline Reasoning & Incorrect reasoning & 32.0 \\\\ & Not factual & 12.0 \\\\ Answer & Ambiguity & 22.7 \\\\ & Invalid label & 14.7 \\\\ & Short answer & 10.6 \\\\ & Incorrect format & 8.0 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 10: Error analysis for zero-shot AgentInstruct Llama-2-70b-chat on AddSub, IMDB, and NewsQA. \\begin{table} \\begin{tabular}{l l c} \\hline \\hline CoT & Prompt & Quasi-Exact \\\\ Reasoning & Match (\\%) \\\\ \\hline Format & **Instruction**\\(->\\)**Instance** & 79.5 \\\\ \\cline{2-3} & Instance \\(->\\) Instruction & **80.0** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 9: Prompt sensitivity analysis on Llama-2-70b-chat on AddSub. Figure 17: Additional information provided for the IMDB dataset. Summarization DatasetsFor each model, zero-shot AgentInstruct tends to lose on summarization datasets. So, we select 25 random instances from XSUM (with an F1 score less than 1) on Llama-2-70b-chat and find summarization-specific error cases. In all but one of the instances, the summary extracted using the answer extraction prompt was not the same summary given after the reasoning extraction prompt. It was often kept around the same length but reworded or changed to contain somewhat different information from the first summary. This suggests that for the answer extraction prompt, the LLM still feels the need to write a new summary instead of relying entirely on the one generated during its intermediate reasoning steps. We identify four summaries given by zero-shot AgentInstruct with incorrect information and three others that are satisfactory but a little misleading, which could be due to the intermediate reasoning steps reinforcing incorrect answers. However, the remaining summaries all seem informative, sometimes more so than the references. In fact, in four of the instances, the reference summary seems to be based on the full article, not the part of the article presented in the implementation from HELM, which poses a problem for evaluation. Additionally, traditional metrics used for summarization (F1, ROUGE-2) are not able to gauge truthfulness when a reference summary is flawed (e.g., a summary hallucinating a part of the article could beat one that is truthful but less similar to a reference). \\begin{table} \\begin{tabular}{l l l l r} \\hline \\hline Dataset & & Error Type & Further Description of Error & Percentage \\\\ \\hline AddSub & Reasoning & Incorrect reasoning & Wrong operation & 24.0 \\\\ & & Incorrect reasoning & Incorrect step & 20.0 \\\\ & & Incorrect reasoning & Missed a step & 8.0 \\\\ & & Not factual & Arithmetic error & 12.0 \\\\ & Answer & Short answer & Conciseness prompt causes rounding & 24.0 \\\\ & & Incorrect format & Hallucinated multiple choice & 8.0 \\\\ & & Ambiguity & Confusing question & 4.0 \\\\ \\hline IMDB & Reasoning & Incorrect reasoning & Misreads sentiment due to language used & 36.0 \\\\ & Answer & Invalid label & Chooses neutral/mixed & 32.0 \\\\ & & Ambiguity & Confusing question & 20.0 \\\\ & & Not factual & Answer before explanation & 12.0 \\\\ \\hline NewsQA & Reasoning & Not factual & Tried to infer answer not in passage & 12.0 \\\\ & & Incorrect reasoning & Didn’t understand question & 8.0 \\\\ & Answer & Ambiguity & Answer mostly correct & 32.0 \\\\ & & Ambiguity & No real question to answer & 8.0 \\\\ & & Ambiguity & Answer seems right but should be no answer & 4.0 \\\\ & & Incorrect format & Answer seems right but not from continuous stretch & 12.0 \\\\ & & Incorrect format & Answer in explanation but not provided at the end & 4.0 \\\\ & & Invalid label & Says there is no answer but not the recognized no answer label & 8.0 \\\\ & & Invalid label & Empty answer & 4.0 \\\\ & & Short answer & Conciseness prompt shortens answer too much & 8.0 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 11: Error analysis results for Llama-2-70b-chat on AddSub, IMDB, and NewsQA, each with 25 samples whose final prediction with zero-shot AgentInstruct was incorrect. \\begin{table} \\begin{tabular}{p{85.4pt} p{284.5pt}} \\hline \\hline Dataset & AddSub \\\\ \\cline{2-3} Question & Question: Jason has 18 books and he has read 9 of them. Mary has 42 books. How many books do they have together? \\\\ \\cline{2-3} CoT Reasoning & T. Understand the context: The prompt is asking about the total number of books that Jason and Mary have. 2. Identify the operation: The operation is addition. 3. Perform the operation: Jason has 18 books and has read 9 of them, so the number of books he has left is 18 - 9 = 9. Mary has 42 books. Therefore, the total number of books they have together is 9 + 42 = 51. 4. Answer the question: They have 51 books together. \\\\ \\cline{2-3} & Correct answer: 51 books. \\\\ \\cline{2-3} & Therefore, the answer to the task is below. Give the answer in the shortest form possible that will still be correct. Answer: 51. Answer: 51 \\\\ \\cline{2-3} Ground Truth & 60 \\\\ \\cline{2-3} Error Type & Incorrect reasoning (incorrect step): Jason reading 9 books does not mean he no longer has them. \\\\ \\hline Dataset & IMDB \\\\ \\cline{2-3} Question & Passage: Susan Sarandon and Natalie Portman play Adele and Ann August, a single mother who’s the working definition of the word “dreamer” and her solemn, pragmatic daughter. Adele, wanting to make a fresh start in a big city, moves them from Wisconsin to California.\\textless{}br/\\textgreater{}\\textless{}br/\\textgreater{}Decent, if not exceptional or particularly memorable, with some serious and sad moments. It pushes some emotional buttons, that’s for sure. Best thing it offers is a solid cast, and Sarandon and Portman are quite good in the leads - even if their characters are one-dimensional at times.\\textless{}br/\\textgreater{}\\textless{}br/\\textgreater{}Could be better, could be worse.\\textless{}br/\\textgreater{}\\textless{}br/\\textgreater{}I gave this one seven out of ten. \\\\ \\cline{2-3} CoT Reasoning & The passage provided is a review of the movie “Anywhere But Here” starring Susan Sarandon and Natalie Portman. The reviewer mentions that the movie is “decent” and “solid” but also states that it’s “not exceptional or particularly memorable.” They also mention that it has some “serious and sad moments” and that the cast, particularly Sarandon and Portman, are “quite good” but their characters are “one-dimensional at times.” \\\\ \\cline{2-3} Based on the language used in the review, it’s clear that the reviewer has mixed feelings about the movie. They use phrases like “decent,” solid,” and “could be better, could be worse” to indicate that the movie is not outstanding but still enjoyable. The reviewer also uses phrases like “pushes some emotional buttons” and “solid cast” to indicate that the movie has some positive aspects. \\\\ \\cline{2-3} & However, the reviewer also uses phrases like “not exceptional or particularly memorable” and “one-dimensional at times” to indicate that the movie is not perfect and has some flaws. \\\\ \\cline{2-3} & Therefore, based on the tone and language used in the review, it’s clear that the sentiment of the review is neutral or mixed. The reviewer does not express any strong positive or negative emotions towards the movie. \\\\ \\cline{2-3} & Answer: Neutral \\\\ \\cline{2-3} & Therefore, the correct label among ’Negative’, ’Positive’ (just the label) to the original task is below. Sentiment: Neutral \\\\ \\cline{2-3} Ground Truth & Positive \\\\ \\cline{2-3} Error Type & Invalid label (chooses neutral/mixed): Decides to label neutral instead of negative or positive. \\\\ \\hline Dataset & AddSub \\\\ \\cline{2-3} Question & Question: Mandy made an apple pie. She used 0.66666666666666 tablespoon of cinnamon and 0.5 tablespoon of nutmeg. How much more cinnamon than nutmeg did Mandy use? \\\\ \\cline{2-3} CoT Reasoning & T. Understand the context: The context of the problem is that Mandy made an apple pie and used certain amounts of cinnamon and nutmeg. 2. Identify the operation: The operation in this problem is subtraction. The question asks how much more cinnamon than nutmeg did Mandy use, which implies that we need to find the difference between the amount of cinnamon and nutmeg used. 3. Perform the operation: We are given that Mandy used 0.66666666666666666 tablespoon of cinnamon and 0.5 tablespoon of nutmeg. To find the difference, we can subtract the amount of nutmeg from the amount of cinnamon: \\\\ \\cline{2-3} & 0.66666666666666 - 0.5 = 0.16666666666666666666 \\\\ \\cline{2-3} & 4. Answer the question: The question asks how much more cinnamon than nutmeg did Mandy use? The answer to this question is the result of the subtraction operation we performed earlier: 0.16666666666666666. \\\\ \\cline{2-3} & Therefore, the answer to the question is 0.16666666666666666 tablespoon of cinnamon more than nutmeg. \\\\ \\cline{2-3} & Correct answer: 0.1666666666666666666 \\\\ \\cline{2-3} & Therefore, the answer to the task is below. Give the answer in the shortest form possible that will still be correct. Answer: 0.17 \\\\ \\cline{2-3} Ground Truth & 0.1666666666666666666666 \\\\ \\cline{2-3} Error Type & Short answer (conciseness prompt causes rounding): Causes too much rounding. \\\\ \\hline Dataset & AddSub \\\\ \\cline{2-3} Question & Question: Jason joined his school ’s band. He bought a flute for 5 142.46, a music tool for 5 8.89, and a song book for 5 7. How much did Jason spend at the music store? \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 12: Error analysis examples for each error type in the AddSub and IMDB datasets. [MISSING_PAGE_FAIL:43] Agent Instruction IssuesIn Table 14, we list all the main datasets where the QA API was not able to gain any relevant information about the task. When relevant information was found, note that there is a possibility of leakage from the web search. When we manually analyze the intermediate steps during instruction generation, we find that the model sees instances from at least OpenBookQA, NarrativeQA (abbreviated), RAFT - Banking 77, CNN/Daily Mail. However, AgentInstruct is worse on OpenBookQA, RAFT - Banking 77, and CNN/Daily Mail for all models, and worse on NarrativeQA for 2 models, meaning these examples gave no discernible benefit to our method."
    },
    {
      "title": "Truncated Context Length",
      "text": "In Figure 18, we test how zero-shot AgentInstruct performs on various context lengths. We artificially reduce the context length of Llama-2-70b-chat from 4,000 (maximum context length) to 2,048, 1,024, 768, and 640. The results suggest that the performance of zero-shot AgentInstruct is worse for models with smaller context lengths. While the impact is minimal on AddSub and IMDB, performance on NarrativeQA steeply declines below a context length of 2,048, which is due to truncation. This is because the instances of NarrativeQA are much longer, with an average length of 855 tokens compared to the instances of AddSub and IMDB, with average lengths of 330 tokens and 48 tokens respectively."
    },
    {
      "title": "Appendix E Dataset Examples",
      "text": "_WARNING: this section contains examples of text that may be considered unsafe, offensive, or upsetting._ \\begin{table} \\begin{tabular}{l c} \\hline \\hline Dataset & Issue \\\\ \\hline AddSub & Not found \\\\ AQuA & Incorrect dataset found \\\\ Coin Flip & Incorrect dataset found \\\\ Last Letter Concatenation & Incorrect dataset found \\\\ SingleEq & Not found \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 14: Agent instruction issues where our QA API was not able to retrieve any relevant information from the sources from the web. Figure 18: Truncating context lengths on Llama-2-70b-chat with zero-shot AgentInstruct on AddSub, IMDB, and NarrativeQA. Complete running examples including agent instructions and CoT reasoning paths for each dataset are presented in Figure 19 to Figure 70. Figure 19: A complete running example of AddSub. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the structure of the dataset: The AQUA dataset contains question-and-answer pairs related to algebra. The questions are generated based on paintings and comments from an existing art knowledge dataset. 2. Topics to know: To answer questions from this dataset, you should have a good understanding of algebra. Additionally, some knowledge of art and how it can be related to algebraic concepts may also be helpful. 3. Steps to answer the questions: - Read the question carefully and identify the algebraic concept it is asking about. - Consider the options provided. Each option will represent a different potential answer to the question. - Use your knowledge of algebra to determine which option is the correct answer. Remember, the questions are based on paintings and comments from an art knowledge dataset, so consider how these elements might relate to the algebraic concept in question. - Once you have determined the correct answer, write it down in the format specified in the prompt template. 4. Tips for answering the questions: - Don't rush. Take your time to understand the question and the options provided. - If you're unsure about an answer, try to eliminate the options that you know are incorrect. This can help narrow down the possibilities and make it easier to choose the correct answer. - Remember, the questions are based on art knowledge as well as algebra. Try to think creatively about how these two areas might intersect. Question: Three birds are flying at a fast rate of 900 kilometers per hour. What is their speed in miles per minute? [lkm = 0.6 miles] A. 32400 B. 6000 C. 600 D. 6000 E. 10 Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: First, let's convert 900 kilometers per hour to miles per hour. We know that 1 kilometer is equal to 0.6 miles, so we can convert 900 kilometers per hour to miles per hour as follows: 900 km/h x 0.6 miles/km = 540 miles/h Now, we need to find the speed of the birds in miles per minute. To do this, we can divide the speed in miles per hour by 60: 540 miles/h + 60 = 9 miles/min Therefore, the correct answer is C. 600. Answer: C. 600 Therefore, the correct multiple choice label (just the letter) to the task is below. Answer: C Figure 20: A complete running example of AQuA. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the question: The question is a naturally occurring, factual question that was asked on Google. It is unambiguous and requires a \"yes\" or \"no\" answer. Make sure you fully understand what the question is asking. 2. Read the passage: The passage is taken from a Wikipedia page that was one of the top five results for the question on Google. It contains the information needed to answer the question. Read the passage carefully and extract the relevant information. 3. Answer the question: Based on the information in the passage, answer the question with either \"yes\" or \"no\". Make sure your answer is supported by the information in the passage. 4. Check your work: Review the question, passage, and your answer to ensure they align. The question should be answered accurately based on the information in the passage. Tips: - The questions in this dataset are factual and require knowledge of a wide range of topics. Brush up on general knowledge and be prepared to learn new information from the passages. - The passages are taken from Wikipedia, so they are written in a clear, informative style. Use this to your advantage when extracting information. - The answers are either \"yes\" or \"no\", but the process of getting to the answer requires understanding and interpreting the passage. Take your time to understand the passage before answering. - Always double-check your work to ensure accuracy. Passage: Elmonchor Air Force Base (IIA; EDG; EDAO; PABO, FLA LID: EDF) is a United States military facility in Anchorage, the largest city in Alaska. Originally known as Elmonchor Field, it became Elmonchorf Air Force Base after World War II, and in 2010 it merged with nearby Fort Richardson to form Joint Base Elmendorf-Richardson. Question: Is there an air force base in anchorageaska? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The question asks whether there is an air force base in Anchorage, Alaska. To answer this question, we need to look at the provided passage. The passage states that Elmonchor Air Force Base is a United States military facility located in Anchorage, Alaska. It was originally known as Elmonchor Field and later became Elmonchor Air Force Base after World War II. In 2010, it merged with nearby Fort Richardson to form Joint Base Elmondorf-Richardson. Based on the information in the passage, it is clear that there is indeed an air force base in Anchorage, Alaska. The passage explicitly states that Elmonchor Air Force Base is located in Anchorage, and its history is described in the passage. Therefore, the answer to the question is \"yes.\" Answer: Yes. Therefore, the correct label among \"No\", \"Yes\" (just the label) to the original task is below. Answer: Yes. Figure 21: A complete running example of BoolQ. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the context: The civil_comments dataset consists of public comments from various English-language news sites. These comments were made between 2015 and 2017. 2. Consider the metadata: Each comment comes with associated metadata such as article IDs and timestamps. Use this information to help understand the context of the comment. 3. Analyze the comment: Each comment in the dataset has been labeled for toxicity and identity mentions. Use these labels to help determine the nature of the comment. 4. Be aware of potential late speech: The dataset is often used for tasks like Rate Speech Detection and Toxic Comment Classification. Be aware that some comments may contain hate speech or toxic language. 5. Answer the question: Based on your understanding of the comment and its context, answer the question. Your answer should be either 'True' or 'False'. 6. Be objective: Your answer should be based on the information in the comment and its associated metadata, not on personal opinions or biases. 7. Be concise: Your answer should be clear and to the point. Avoid unnecessary elaboration. 8. Be respectful: Even if the comment contains hate speech or toxic language, your response should always be respectful and professionals. Passage: Exactly: It seems every time a black youth is killed by police, the grieving parent is the mom. Moms get recognized by black athletes but seldom is there a dad mentioned. Question: Is the passage above toxic? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The passage above mentions the killing of black youth by police and the grieving parents, specifically mentioning mothers being recognized by black athletes but not fathers. This could be interpreted as a commentary on the lack of recognition or support for black fathers who have lost their children to police violence. However, the tone of the passage is not aggressive or hostile, and it does not contain any hate speech or derogatory language. Based on the context and content of the passage, it can be determined that the comment is not toxic. The comment does not contain any hate speech or toxic language, and it raises a valid point about the lack of recognition for black fathers who have lost their children to police violence. Therefore, the answer to the question is False. The passage is not toxic. Therefore, the correct label among ['False', 'True'] (just the label) to the original task is below. Answer: False Figure 22: A complete running example of CivilComments (All). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the context: The dataset contains public comments from various news sites. These comments have been labeled for toxicity, obscesity, threat, insult, identity attack, and sexual explicitness. Each label is a value between 0 and 1, indicating the fraction of annotators that assigned these attributes to the comment text. 2. Consider the demographic: Although the dataset does not explicitly mention the demographic 'black', it seems that the comments may have been categorized based on whether they were made by individuals identified as black or non-black. Keep this in mind when answering questions. 3. Analyze the passage and question: Read the passage and question carefully. The passage will provide a general statement or argument, and the question will ask something related to that statement or argument. Your answer should be based on the information provided in the passage and question. 4. Answer the question: Based on your analysis of the passage and question, provide an answer. The possible outputs are 'True' or 'False'. Make sure your answer is supported by the information in the passage and question. 5. Be mindful of sensitive topics: Given the nature of the dataset, the passage and question may involve sensitive topics related to race and discrimination. Be respectful and considerate in your responses. 6. Review your answer: Before submitting your answer, review it to ensure it is accurate, clear, and respectful. Passage: Kayden, religion has nothing to do with this. Hindupbobia- no it's cult and self seeking power hungry rocks like your man Butler that jump on the religious wasoph who have no qualms about lying and cheating their way up the political ladder. In Tulsi Gabhardty case, her gurus has taked advantage of her cult beliefs she was born into and abused the fact that she is beholedon to him. As he says when he initiates his disciples, you are now online and I will instruct you how you can serve ME. Panticism not religion. Crv yourself silly with all your talk on religious bigtry, homophobic backed support, Hillary Clinton money, and whatever else you can lash out with. How about all the other religions, I don't see Tulsi and Butler supporting them. In fact she down right degrades the Muslims. Butler has always spoken very unhindy of Gays, Blacks, Muslims Christians and Jews. Beware Hawaii, Tulsi and anyone who admestes to him, has no place in our democratic system. Question: Is the passage above toxic? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The passage above contains several elements that suggest it may be toxic. Firstly, the author of the passage uses strong language and personal attacks against individuals, such as \"cult and self-seeking power-hungry crowds\" and \"fancintions not religion.\" This type of language can be seen as inflammatory and disrespectful, and may be intended to provoke or intimidate others. Additionally, the author makes broad generalizations about certain groups, such as \"Muslims, Christians, and Jews,\" which may be perceived as discriminatory or hateful. Furthermore, the author implies that anyone who supports Tulsi Gabbard or her guru is not suitable for a democratic system, which may be seen as undomocratic or intolerant. This statement could be interpreted as an attempt to silence or dismiss opposing viewpoints, which is not conducive to a healthy democratic discourse. Therefore, based on the language, tone, and content of the passage, it can be concluded that the passage is toxic. Answer: True Therefore, the correct label among {'False', 'True'} (just the label) to the original task is below. Answer: True Figure 23: A complete running example of CivilComments (Black). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the context: The civil-comments dataset contains public comments from various news sites. These comments were created between 2015 and 2017 and appeared on approximately 50 English-language news sites across the world. The dataset includes the text of the comments, some associated metadata, and labels for toxicity and identity mentions. 2. Focus on the content: The dataset does not provide specific demographic information about the commenters. Therefore, your analysis should focus on the content of the comments and their context, rather than attempting to infer demographic information. 3. Analyze the toxicity: The dataset includes labels for toxicity. Use these labels to help determine the tone and intent of the comments. 4. Be aware of the limitations: The dataset does not include user identities or demographic information about the commenters. Therefore, be cautious about making assumptions about the individuals who made the comments. 5. Answer the question: Based on your understanding of the comment and its context, answer the question related to the passage. Your answer should be either 'True' or 'False'. Make sure your answer is supported by the information available in the comment and its context. 6. Explain your reasoning: After providing your answer, explain why you chose that answer. Your explanation should reference specific parts of the comment and how they led you to your conclusion. Passage: Keep God out of politics...If'd rather expose the politicians for creating phony front groups designed to mislead Catholics. Question: Is the passage above toxic? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The passage above expresses a negative sentiment towards politicians and their involvement in religion. The author of the comment advocates for keeping God out of politics and instead, exposing politicians who create fake front groups to mislead Catholics. The tone of the comment is critical and disapproving, which suggests that it may be toxic. However, upon closer examination, it's clear that the author's intention is not to incite hatred or violence, but rather to express their opinion on the separation of church and state. The comment does not contain any personal attacks, racist, sexist, or homophobic language, or other forms of toxic content. Therefore, based on the context and content of the comment, the answer to the question is False. The passage is not toxic. Reasoning: * The comment expresses a negative sentiment towards politicians, but it does not contain any toxic language or personal attacks. * The author's intention is to advocate for the separation of church and state, which is a legitimate opinion to hold. * The comment does not incite hatred or violence, and it does not contain any discriminatory language. Answer: False. The passage is not toxic. Therefore, the correct label among {'False', 'True'} (just the label) to the original task is below. Answer: False Figure 24: A complete running example of CivilComments (Christian). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the nature of the civilLocuments dataset: This dataset contains public comments from news sites, with labels for toxicity and identity mentions. The comments were created from 2015 - 2017 and appeared on approximately 50 English-language news sites across the world. 2. Know what constitutes toxicity in comments: Toxicity can include insults, threats, obscene language, identity attacks, and sexually explicit content. However, it's important to consider the context and nuances of the comment. 3. Consider cultural and societal norms: When evaluating a comment for toxicity, consider cultural and societal norms related to the demographic 'female'. Remember that what may be considered toxic in one culture or society may not be in another. 4. Apply this knowledge to the prompt: Use the information you've gathered to evaluate the passage in the prompt. Its it took? Consider the language used, the context, and any potential harm or offense it may cause, especially to the demographic 'female'. 5. Formulate your answer: Based on your evaluation, answer 'True' if the passage is toxic, and 'False' if it is not. Be prepared to provide reasoning for your answer if necessary. Passage: Actually, we are a secular organization. Ninety-nine percent of the Church's population is secular, and it's founder, Jesus himself, was a secular. The word \"secular\" has gotten a bad rap. \"Secular\" means in the world, in the world God created and and looked at as \"very good\". The pritishored which exists is the Royal Pritishored we all share by virtue of our baptism. Some of us (both male and female) may be called to the ordained pritishored of servant-leadership. That call comes from God Who does not distinguish between male and female. Just as the world is constantly evolving in its journey to Christ-centered completion, so too our understanding of servant-leadership will continue to unfold until we finally realize that \"in Christ there is neither male nor female\" and \"all are one in Christ Jesus\". Question: Is the passage above toxic? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The passage provided does not contain any explicit language, threats, or personal attacks, which are common indicators of toxicity. However, it's important to consider the context and cultural norms when evaluating the passage. The passage mentions the founder of the Church, Jesus, and the concept of the \"Royal Pritishored\" that is shared by all members of the Church, regardless of gender. It also mentions the ordained pritishored and how God does not distinguish between male and female. This language could be seen as inclusive and respectful towards women, promoting the idea of equality among all members of the Church. Furthermore, the passage also highlights the idea of continuous evolution towards a Christ-centered completion, which could be interpreted as a call for progress and improvement towards a more inclusive and equitable society. Therefore, based on the language used, the context, and the message of inclusivity and equality, it can be concluded that the passage is not toxic. Answer: False. Therefore, the correct label among {'False', 'True'} (just the label) to the original task is below. Answer: False Figure 25: A complete running example of CivilComments (Female). [MISSING_PAGE_EMPTY:52] You will be provided instructions for completing s task followed by a task to complete. Instructions: 1. Understand the context: The civil_comments dataset is a collection of public comments from various news sites. These comments have been labeled for toxicity and identity mentions. The dataset does not contain specific demographic information about the comments or the content of the comments. 2. Analyze the passage: Read the passage carefully and understand the general statement or quote. Pay attention to any additional information or commentary provided. 3. Answer the question: Based on the passage, answer the question related to the passage. Your answer should be either 'True' or 'False'. Do not attempt to infer demographic information, as this is not provided in the dataset. 4. Focus on the content: Your primary focus should be on the content of the comments and their associated toxicity labels. 5. Be objective: Avoid making assumptions or inferences that are not supported by the passage. Stick to the facts presented in the passage. 6. Be concise: Your answer should be concise and to the point. Do not include unnecessary information or commentary. Remember, your goal is to provide an accurate and unbiased answer based on the information provided in the passage. Passage: As the 2016 USCCs report done by an outside group shows more than 80% were sames sex (male) assaults and 90% were post pubescent. Those numbers are staggeringly disproportionate and undercut your point. Question: Is the passage above toxic? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The passage provided is a comment made in response to a report about sexual assaults. The comment states that the majority of sexual assaults are committed by men and that the majority of victims are post-pubescent. The comment does not contain any toxic language or personal attacks. It does not promote or incite violence or discrimination. It does not contain any hate speech or derogatory language towards any gender, race, religion, or group of people. Therefore, based on the information provided in the passage, the answer to the question is False. The passage is not toxic. Answer: False. Therefore, the correct label among {'False', 'True'} (just the label) to the original task is below. Answer: False Figure 27: A complete running example of CivilComments (Male). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the context: The civilComments dataset contains public comments from various news sites worldwide. These comments cover a wide range of topics including current events, politics, and social issues. Your responses should reflect an understanding of these contexts. 2. Know the demographic: This task focuses on the'mushim' demographic. Be aware of the specific cultural, religious, and social aspects related to this demographic. 3. Understand the labels: The comments in the dataset have been labeled for toxicity and identity mentions. Familiarize yourself with these labels and what they represent. 4. Be objective: Your responses should be objective and unbiased. Avoid making assumptions or generalizations about the'mushim' demographic or any other demographic mentioned in the comments. 5. Be accurate: Your responses should accurately reflect the content of the comments. Do not add or omit information. 6. Be respectful: Respect the sensitivity of the topics discussed in the comments. Avoid using offensive or inappropriate language. 7. Be concise: Your responses should be concise and to the point. Avoid unnecessary elaboration or speculation. 8. Be aware of the possible outputs: The possible outputs for this task are 'True' and 'False'. Make sure your responses align with these options. Remember, the goal is to understand and improve cibility in online conversations. Your responses should contribute to this goal. Passage: This woman, Chervisia Freeland, went on US television last year and stated that it was wrong to criticize certain segments of the Muslim population for honour killings, female genital mutilation, and persecution of gays. She is a far-left, batter** crazy, cultural marxist. Question: Is the passage above toxic? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. (/INST) Explanation: The passage above contains several elements that suggest it is toxic. Firstly, the use of the term \"bhat*** crazy\" to describe Christia Freeland is a form of name-calling and personal attack, which is a hallmark of toxic language. Additionally, the phrase \"cultural marxist\" is often used in a derogatory manner to describe individuals who are perceived to be promoting a liberal or progressive agenda, and its use in this context suggests that the author is attempting to discredit Freeland's views without providing any substance to their argument. Furthermore, the passage implies that Freeland's comments were wrong and that she is condoning or excusing certain practices within the Muslim community, which could be perceived as Islamophobic. The use of the phrase \"certain segments of the Muslim population\" also implies that the author is generalizing and stereotyping an entire religious group, which is a form of prejudice. Therefore, based on the above analysis, the passage is toxic. Answer: True Therefore, the correct label among {'False', 'True'} (just the label) to the original task is below. Answer: True Figure 28: A complete running example of CivilComments (Muslim). Figure 29: A complete running example of CivilComments (Other Religions). Figure 30: A complete running example of CivilComments (White). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the context: The summarization_cnmdm dataset consists of pairs of news articles and their corresponding human-generated summaries. The articles are sourced from CNN and Daily Mail websites. Your task is to generate a summary of the given article. 2. Read the article carefully: The article will contain various details, but not all of them are equally important. Identify the key points and main ideas in the article. These are the points that should be included in the summary. 3. Be concise: The summaries in the dataset consist of an average of 53 words and 3.72 sentences. Try to be as concise as possible while still conveying the main points of the article. Avoid including unnecessary details. 4. Maintain the original meaning: While the summary should be concise, it should also accurately represent the content of the article. Do not change the meaning of the original text in your summary. 5. Use your own words: While it's important to maintain the original meaning, try to paraphrase the content in your own words. This is an important aspect of text summarization. 6. Check your work: After writing the summary, read it over to ensure it accurately represents the main points of the article. Make sure there are no grammatical or spelling errors. Remember, the goal is to create a summary that provides a clear and concise overview of the article's content. Good luck! * Article: Arsene Wenger wants Cesc Fabregas to be shown the'respect he deserves' when he returns to the Emiates Stadium in the blue of Chelsea on Sunday. The problem with that is a decent chunk of Arsenal's 's supporters feel he doesn't deserve much. That became obvious on Thursday, when one prominent fan called for the removal of a Fabregas banner from the Ken Friar Bridge. Cesc Fabregas returns to Arsenal on Sunday and Arsene Wenger hopes fans will give him a good reception. Wenger wants'respect' for the club's former players and counts Fabregas as a man who deserves that. Gunners fans offer their good luck to Fabregas in 2011, but the reception is likely to be more frosty this time. Extreme, perhaps, but this is an emotive issue which continues to bubble away at the club where Fabregas built his career, not least because the circumstances behind his summer move from Barcelona to Chelsea are still as clear as mud. Any clarity, it seems, will have to wait, Henger was at his cryptic best on Thursday when asked if it was his call to not take up an option he had to re-sign the player, saying: \"We will have to discuss that one day. With all the terms.\" When pressed on whether it was his decision, he added: 'It's not as clean as that. I cannot speak to you about that now because that will not help us to win on Sunday.\" At that point it was suggested to Wenger that Fabregas chose not to come back and Wenger said: 'I don't know, 'I don't know,' the Frenchman has previously claimed that by the time he knew Fabregas was available, a deal with Chelsea was virtually concluded | comments which jarred a little with the spinar?'s statement last summer that Arsenal 'decided not to take' their option. Whatever, it would be ambitious for Fabregas to expect an overwhelmingly positive reception on Sunday. The midfielded set up Diego Costa to seal a 2-0 victory for Chelsea against Arsenal earlier this season. Certainly, there was already a degree of disappointment within the club about how their relationship with the player ended when he left in 2011, But Wenger said: 'I want every player to be respected, and Cesc Fabregas when he comes to the Emiates on Sunday, (I want him) to be respected like he deserves.' Wenger added: 'The regret I have is that the wanted to leave here. This club and myself had a very positive influence on Cesc's life, so I would be a bit uncomfortable to have to justify today why he is not here.' When he returns, it will be as key part of the champions-elect and under a manager in Wagee Mourithm who has won seven and drawn five of this 12 meetings with Kenger. That clash, in itself, is a fascinating sub-plot to this fixture between the top and second-placed teams in the Barclays Premier League. Fabregas gets ahead of his now-team-mate Branislav Ivanovic to score against Chelsea for Arsenal in 2010. Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The article discusses Arsene Wenger's desire for Cesc Fabregas to be shown respect when he returns to the Emiates Stadium with Chelsea. Fabregas left Arsenal in 2011 and has since joined Chelsea, and some Arsenal fans have called for the removal of a Fabregas banner from the Ken Friar Bridge. Wenger wants Fabregas to be respected like he deserves, despite the fact that some fans may not agree. The article also mentions that Fabregas set up a goal for Chelsea against Arsenal earlier this season, which may contribute to a frosty reception. Correct answer: The correct answer is to generate a summary of the article that accurately represents the content and main points. The summary should be concise, use the writer's own words, and maintain the original meaning. Here is a possible summary: Arsene Wenger hopes that Cesc Fabregas will be shown respect when he returns to the Emiates Stadium with Chlesse, despite some fans calling for the removal of a Fabregas banner. Fabregas left Arsenal in 2011 and has since joined Chelsea, and his return may be met with a frosty reception. Wenger wants Fabregas to be respected like he deserves, and hopes that the player will be treated with the respect he deserves. Therefore, the answer to the task is below. Give the answer in the shortest form possible that will still be correct. Summarize the above article in 3 sentences. Answer: Arsene Wenger wants Cesc Fabregas to be shown respect when he returns to the Emiates Stadium with Chlesse. Some Arsenal fans have called for the removal of a Fabregas banner from the Ken Friar Bridge. Fabregas set up a goal for Chelsea against Arsenal earlier this season, which may contribute to a frosty reception. Figure 31: A complete running example of CNN/Daily Mail. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand that you are dealing with a multiple-choice question answering task. The goal is to predict the correct answer among the given options. 2. The questions and options in this dataset require commonsense knowledge. Use your understanding of the world and how things generally work to answer the questions. 3. Each question is associated with a concept from ConceptNet. This means that understanding the relations between different concepts can be beneficial in answering the questions. 4. The options include one correct answer and four distractors. Carefully consider all the options before making a prediction. The distractors are designed to be plausible but incorrect answers to the question. 5. The questions and options were created by humans, so understanding human language and reasoning is important. Make sure to interpret the questions and options in a way that a human would. 6. After considering all the options and using your commonsense knowledge, predict the correct answer. Make sure to provide your answer in the format specified in the prompt. Question: From where does a snowflake form? A. cloud B. snow storm C. billlow D. air E. snowstorm Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. (/INST] Explanation: A snowflake forms from water vapor in the air. When the temperature drops, the water vapor in the air condenses into tiny droplets, which then freeze into ice crystals. These ice crystals then stick together to form a snowflake. Therefore, the correct answer is (A) cloud. A snowflake forms in a cloud, specifically in the part of the cloud called a cumulonimbus cloud, which is characterized by strong updrafts and downdrafts that allow the water vapor to condense and freeze into snowflakes. Answer: A. cloud Therefore, the correct multiple choice label (just the letter) to the task is below. Answer: A Figure 33: A complete running example of CommonsenseQA. Figure 32: A complete running example of Coin Flip. [MISSING_PAGE_EMPTY:59] You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the question: The first step is to read the question carefully and understand what is being asked. The questions in the GSM dataset are grade school math word problems that require multi-step mathematical reasoning. They are designed to be solvable by a bright middle school student. 2. Identify the mathematical operations: The problems primarily involve basic arithmetic operations such as addition, subtraction, multiplication, and division. Identify the operations required to solve the problem. 3. Apply mathematical reasoning: The problems require a sequence of elementary calculations. Apply your mathematical reasoning to determine the sequence of steps required to solve the problem. 4. Solve the problem: perform the calculations in the correct sequence to reach the final answer. 5. Check your work: Always check your work to ensure that your answer is reasonable and that you have not made any calculation errors. Tips: - Familiarize yourself with common themes found in grade school maths, such as fractions, percentages, simple algebra, and basic geometry. These themes are likely to appear in the problems. - Practice multi-step mathematical reasoning. The problems in the GSM dataset require between 2 and 8 steps to solve. - Remember that the problems are designed to be solvable by a bright middle school student. If a problem seems overly complex, you may be overthinking it. Try to simplify the problem and break it down into smaller, manageable steps. Question: Finley went to the grocery store and bought rice, beans, and pork for use in their home. It took her 20 more minutes to cook pork than rice, while beans took half the combined cooking time of pork and rice. If it took her 30 minutes to cook rice, how long in minutes did it take to cook all the food? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: 1. Understand the question: The question is asking for the total cooking time it took Finley to cook rice, beans, and pork. 2. Identify the mathematical operations: The problem requires addition, subtraction, and multiplication. 3. Apply mathematical reasoning: Let's start by identifying the cooking times for each item: * Rice: 30 minutes * Pork: 20 minutes more than rice, so 30 + 20 = 50 minutes * beans: Half the combined cooking time of pork and rice, so (50 + 30) / 2 = 40 minutes Now, we need to find the total cooking time: Total cooking time = Rice + Pork + Beans = 30 + 50 + 40 = 120 minutes 4. Solve the problem: The total cooking time it took Finley to cook all the food is 120 minutes. 5. Check your work: The answer seems reasonable, as it's likely that cooking all three items would take longer than just cooking one or two of them. Answer: 120 minutes. Therefore, the answer to the task is below. Give the answer in the shortest form possible that will still be correct. Answer: 120 minutes. Figure 35: A complete running example of GSM8K. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the question: The question will be about a grounded situation, either from the domain of activitynet or wikihow. It will ask what might happen next in the scene. 2. Analyze the answer choices: There will be four answer choices. One of them is the correct answer, which is the real sentence for the next event. The other three are incorrect answers that are designed to fool machines but not humans. 3. Use your commonsense: The dataset is designed to test grounded commonsense inference. This means you should use your understanding of the world and how things generally work to infer what might happen next in the situation described in the question. 4. Choose the most likely answer: Based on your analysis of the answer choices and your commonsense, choose the answer that you think is most likely to be what happens next in the scene. 5. Remember, the questions are designed to be easy for humans but challenging for models. If you're having trouble, try to think like a human and use your commonsense. Question: Personal Care and Style: (header) How to dye your hair with semi permanent hair dye [title] Find the color you want. (step) There are many popular brands and hundreds of different colors to choose from. Semi-permanent dyes can be found in a variety of places, ranging from grocery stores to specialized fashion shops, with the biggest selection at beauty supply stores. A. It is important to select the color that represents your hair type when you register your hair color, [substeps] Traditional semi-permanent dyes will generally not be available for hair color, like blow-dryers, curling irons, and appliances. B. If you're not planning on dying your hair, there are other coloration measures you can take to dye your hair. [step] Photoshopadhyay had markers work well, but don't lack the style that can be coupled with it. C. Pick the color that's your favorite, matches your wardrobe best, and/or is most flattering for your eye color and skin tone. Semi-permanent dyes work on all hair colors, but show up brightest on light hair. D. However, you can also take your color, added color, and texture into account when deciding what to dye, and what you will use it. (substeps) Consider adding your hair dye to your hair if you have it long or curly. Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. (/INST) Explanation: The question is asking what might happen next in the scene, specifically in the context of dyeing hair with semi-permanent hair dye. The instructions suggest using commonsense and analyzing the answer choices to determine the most likely next step. Answer A is not likely, as it suggests selecting a color based on hair type, which is not a relevant factor in choosing a hair dye color. Answer B is also not likely, as it suggests using Photoshop to darken hair, which is not a suitable method for dyeing hair. Answer C is a plausible option, as it suggests choosing a color that is a personal favorite, matches one's wardrobe, and/or is most flattering for one's eye color and skin tone. This answer choice aligns with the instructions' suggestion to use commonsense and consider the context of the situation. Answer D is not likely, as it suggests considering adding hair dye to hair that is already long or curly, which is not a logical or practical solution. Therefore, the most likely next step in the scene would be to choose a hair dye color that is a personal favorite, matches one's wardrobe, and/or is most flattering for one's eye color and skin tone, as suggested in Answer C. Correct answer: C. Pick the color that's your favorite, matches your wardrobe best, and/or is most flattering for your eye color and skin tone. Semi-permanent dyes work on all hair colors, but show up brightest on light hair. Therefore, the correct multiple choice label (just the letter) to the task is below. Answer: C Figure 36: A complete running example of HellaSwag. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the Dataset: The IMDb Movie Reviews dataset contains 50,000 reviews from the Internet Movie Database (IMDB). These reviews are labelled as either positive or negative, with an even number of each. The sentiment of the reviews is determined based on the score given by the reviewer. A review is labeled as negative if it has a score of 4 or less out of 10, and it is labeled as positive if it has a score of 7 or more out of 10. The dataset does not include more than 30 reviews per movie. 2. Analyze the Passage: Read the passage carefully. This will be a review from the IMDb dataset. Pay attention to the language used, the tone of the review, and any explicit or implicit expressions of sentiment towards the movie. 3. Determine the Sentiment: Based on your analysis of the review, determine whether the sentiment is positive or negative. Remember that positive reviews generally contain prasise for the movie, its plot, characters, cinematography, etc., and have a score of 7 or more out of 10. Negative reviews, on the other hand, generally contain criticism of the movie and have a score of 4 or less out of 10. 4. Answer the Question: Based on your determination of the sentiment, select the appropriate output: 'Negative' or 'Positive'. Remember, the goal is to correctly identify the sentiment of the review based on the text provided. Passage: Lemmel Gulliver (Ted Danson) is a doctor who goes missing at sea, leaving pregnant wife Mary (Mary Steenburger) behind. Eight years later, he turns up, disheweded and seemingly mad - basbling about his adventures in the lands of the tiny Lillipatius, the giant Brobdingnags, the floating island of the intellectual Laputa, and the Broughnms, a race of intelligent, taking horses who have to deal with the Yahoes - a race of bestial humans - among many other adventures. The not-good Dr. Bates (James Fox), who has designs on Lemmel's wife, has dulliver incarcerated in a mental institution, and Lemmel, Mary, and son Thomas (Tom Sturridge) must find a way to prove his sanity,-chr />chr />A splendid adaptation of Jonathan Swift's satitical movelist, this film is a magnificent adaptation on so many levels: the story, the satire, the characters, the visualis, the brilliant cast. It's simply a treat to watch, and it's almost amazing considering that it was a made-for-yr film. Or />chr />The film does a brilliant job of capturing Swift's vicious satire, which cuts like a hatchet through British society of the time, but still resonates today. The wise Brobdingnags and the Broughnms are almost perfect individuals who find it virtually impossible to understand why Qulliver speaks with such pride of the voices and corruptions of his society. The scenes where Gulliver struggles to prove himself different from the Yahoes are perhaps the best, with biting satire in describing how they pick their leaders (\"they seem to pick the worst among them.. who rules until they find someone even worse\"), go to war (\"We only go to war for a very good reason - such as they are weaker than us, or we want all of their land\"), etc. The scenes involving Laputa are also effectively done - the intellectual nets are now wrapped up in their specialized fields that they have no time for anything else, and really possess little common sense. And the addition of the asylum plot line enhances the story greatly - Dr. Bates is truly nasty character, and when he gives a speech to the inquiry on Gulliver's alleged voices, it's quite clear that he's describing his own fautics.br />chr />The film makes use of beautiful, and fairly convincing COI effects depicting the very diverse settings of the novel with great effect. The contrast of sites is done in a very skillful way, and all of the worlds depicted in the story are convincing in their own way. The cinematography (particularly that concerning the asylum) and the costumes are brilliantly done. The editing of the present with Lemmel's memories is a device which could be awkward, but works very well.br />chr />The cast is truly wonderful; a veritable w's-who of British and American talent. Ted Danson gives an excellent, multi-layered performance as Gulliver, showing effectively his transformation from a person bewildered by his strange surroundings, to the lunastic state he was in when he reappears, to his rational, intellectual personality at the end. Most well-known for his work on sit-con, Damson shows that he's more than just Sam Molone with this wonderful seronic-comple performance. Mary Steenburger is effective as his wife, and James Fox is absolutely repulsive as Bates. The rest of the cast is made up mostly of camos, with Peter O'Toole, Omar Sharif, Warwick Davis, Kristin Scott Thomas, Geraldine Chaplin, Alire Woodard, Edward Fox, and Sir John Giejud being the most memorable - but even the smallest parts are very well-played.br />chr />While not 100 Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. (/INST) Explanation: The review provided is a positive review of the movie \"Gulliver's Travels.\" The reviewer uses positive language to describe the film, such as \"magnificent adaptation,\" \"brilliant cast,\" and \"trumpth of story and images.\" They also highlight the film's ability to capture the satire of Jonathan Swift's novel, which is a key aspect of the movie. Additionally, the reviewer praises the film's visualis, costumes, and cinematography. The only potential negative comment in the review is a brief mention of the film's deviation from the book, but the reviewer quickly dismisses this by stating that it is \"not 100 Therefore, based on the language, tone, and content of the review, it is clear that the sentiment of the review is positive. Answer: Positive Therefore, the correct label among 'Negative', \"Positive' (just the label) to the original task is below. Sentiment: Positive Figure 37: A complete running example of IMDB. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Familiarize yourself with the English alphabet and the concept of handwritten digit recognition. This dataset is about identifying English capital letters from images that have been converted into numerical data. 2. Understand that each letter is represented by 16 primitive numerical attributes. These attributes are statistical moments and edge counts derived from the images of the letters. 3. The question will ask you to take the last letters of each word in a given name and concatenate them. This means you need to extract the last letter from each word and join them together to form a new string. 4. To answer the question, you will need to understand how to manipulate strings and extract specific characters. This involves knowing how to index strings and concatenate them. 5. Remember that the dataset contains 20,000 unique stimuli, meaning there is a wide variety of data to work with. However, the task remains the same: identify the letters based on the provided numerical data. 6. Be aware that the numerical data has been scaled to fit into a range of integer values from 0 through 15. This means you might need to normalize or scale your data when working with it. 7. Finally, keep in mind that the data is based on 20 different fonts and each letter within these fonts was randomly distorted. This adds an element of complexity to the task as the same letter can appear differently based on the font and distortion. 8. Use your knowledge of the English alphabet, string manipulation, and numerical data to answer the question. Be sure to consider all the information provided in the dataset and the specific requirements of the question. Question: Take the last letters of each words in \"Sheree Jackson\" and concatenate them. Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: To solve this task, we need to follow the instructions provided. First, we need to familiarize ourselves with the English alphabet and the concept of handwritten digit recognition. We are working with a dataset that contains numerical data representing English capital letters. Each letter is represented by 16 primitive numerical attributes, which are statistical moments and edge counts derived from images of the letters. Next, we need to understand that the question asks us to take the last letters of each word in a given name and concatenate them. This means we need to extract the last letter from each word and join them together to form a new string. To do this, we need to manipulate strings and extract specific characters. We can index into the strings to extract the last letter of each word. For example, the first word \"Sheree\" ends with the letter \"e\", and the second word \"Jackson\" ends with the letter \"m\". We can then concatenate these letters to form a new string. In this case, the new string would be \"e n\". It's important to note that the numerical data has been scaled to fit into a range of integer values from 0 through 15. Therefore, we might need to normalize or scale our data when working with it. Additionally, we need to keep in mind that the data is based on 20 different fonts, and each letter within these fonts was randomly distorted. This means that the same letter can appear differently based on the font and distortion. Using our knowledge of the English alphabet, string manipulation, and numerical data, we can answer the question. The last letters of each word in \"Sheree Jackson\" are \"e\" and \"m\", which concatenated form the string \"em\". Answer: en Therefore, the answer to the task is below. Give the answer in the shortest form possible that will still be correct. Answer: en Figure 38: A complete running example of Last Letter Concatenation. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Head the question carefully. The questions in this dataset are about abstract algebra and can range from elementary to advanced professional level. They are designed to test both your world knowledge and problem-solving ability. 2. Understand the topic. The questions cover a range of topics within abstract algebra, including group theory, ring theory, and field theory, Make sure you have a good understanding of these topics. 3. Consider all the options. The questions are multiple-choice, so consider each option carefully before making your choice. 4. Use your problem-solving skills. The questions may ask about finding the degree for a given field extension, finding zeros in a finite field of a given polynomial, or understanding properties of group theory, ring theory, and field theory. Use your knowledge of these topics to solve the problem. 5. Answer the question. Once you have considered all the options and used your problem-solving skills, choose the option that you think is the correct answer. 6. Review your answer. After you have chosen an answer, review it to make sure it is the best answer to the question. Remember, the goal is not just to choose the correct answer, but to understand why it is the correct answer. Question: The cyclic subgroup of Z_24 generated by 18 has order A. 4 B. 8 C. 12 D. 6 Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: To find the order of the cyclic subgroup of Z_24 generated by 18, we need to find the smallest positive integer n such that 18\"= 1 (mod 24). We can start by noticing that 18 is a generator of the multiplicative group of integers modulo 24, which means that it has a multiplicative order of 24. Therefore, the order of the cyclic subgroup generated by 18 is equal to the multiplicative order of 18, which is 24. So, the correct answer is (D) 6. The reasoning is as follows: * The cyclic subgroup generated by 18 is a subgroup of the multiplicative group of integers modulo 24. * The multiplicative group of integers modulo 24 has a total of 24 elements, and 18 is a generator of this group. * Therefore, the order of the cyclic subgroup generated by 18 is equal to the multiplicative order of 18, which is 24. * The correct answer is (D) 6, because 6 is the smallest positive integer such that 18^6 = 1 (mod 24). Therefore, the correct answer is (D) 6. Therefore, the correct multiple choice label (just the letter) to the task is below. Answer: D Figure 39: A complete running example of MMLU (Abstract Algebra). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Read the question carefully. The dataset contains multiple-choice questions related to college-level chemistry. 2. Understand the context of the question. The topics can range from atomic structure, chemical reactions, stoichiometry, thermodynamics, chemical bonding, to organic chemistry. 3. Evaluate each choice. The dataset provides a list of choices for each question. 4. Identify the correct answer. The correct answer is indicated by a class label in the dataset. 5. Avoid the negative answer. The dataset also provides a negative answer, which is an incorrect option. 6. Use your knowledge of college-level chemistry to answer the question. You might need to recall facts, apply formulas, or use logical reasoning based on the principles of chemistry. 7. If the question is complex, break it down into smaller parts and address each part separately. 8. Always provide an explanation for your answer. This helps to demonstrate your understanding of the topic. 9. Review your answer before finalizing it. Ensure that it is accurate and complete. 10. Keep learning and updating your knowledge about college-level chemistry. The field is vast and the questions can cover any topic within it. Question: Which of the following is true for Br2 at standard temperature and pressure? A. It is a colorless gas. B. It is a red-brown volatile liquid. C. It is a colorless volatile liquid. D. It is a yellow metallic solid. Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The question asks about the state of Br2 at standard temperature and pressure. The options are: A. Colorless gas B. Red-brown volatile liquid C. Colorless volatile liquid D. Yellow metallic solid To answer this question, we need to recall the properties of bromine (Br2). Bromine is a reddish-brown liquid at room temperature and pressure. It has a density of 2.96 g/mL and a boiling point of 58.8degC. Therefore, the correct answer is B. Red-brown volatile liquid. Explanation: The statement A is incorrect because Br2 is not a gas at standard temperature and pressure. It is a liquid. The statement C is incorrect because Br2 is not a colorless liquid. It has a reddish-brown color. The statement D is incorrect because Br2 is not a solid, and it does not have a yellow color. Therefore, the correct answer is B. Red-brown volatile liquid. Therefore, the correct multiple choice label (just the letter) to the task is below. Answer: B Figure 40: A complete running example of MMLU (College Chemistry). * [25] M. C. C. A. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky. Learning to predict the best answer. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 1-10. ACM, 2015. [MISSING_PAGE_POST] You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. [noitemsep,topsep=0pt,parsep=0pt,topsep=0pt] 2. Understand the basics of econometrics: This includes topics like statistical methods, economic data analysis, regression analysis, and economic forecasting. Familiarize yourself with the concepts and terminologies in these areas. 3. Read the question carefully: The dataset consists of multiple-choice questions. Make sure to read the question and all the options carefully before making a choice. 4. Use your knowledge of econometrics to answer: Apply your understanding of econometrics to answer the question. Consider all the options and choose the one that best fits the question. 5. Answer format: The answer should be the index of the correct option. For example, if the options are [\"0\", \"4\", \"6\"] and the correct answer is \"4\", the corresponding class label would be 1 (since indexing starts from 0). Do not write full sentences as answers. 6. Review your answer: Before finalizing your answer, review it to ensure it is the most appropriate choice given the question and options. Remember, the goal is not just to select the correct answer, but to understand why that answer is correct. This will help you improve your understanding of econometrics and your problem-solving abilities. Question: Suppose that a test that the true value of the intercept coefficient is zero results in non-negjection. What would be the appropriate conclusion? A. Drop the intercept and re-run the regression B. Retain the intercept C. Re-compute the test statistic D. The regression line is running exactly through the origin Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The question states that a test that the true value of the intercept coefficient is zero results in non-negjection. This means that the null hypothesis that the intercept coefficient is zero cannot be rejected at a statistically significant level. Option A suggests dropping the intercept and re-running the regression. However, this is not appropriate because the intercept coefficient is an important component of the regression model, and dropping it may lead to biased estimates of the slope coefficient. Option B suggests retaining the intercept. This is a reasonable approach, as the intercept coefficient may still be useful in interpreting the regression results, even if the test for its significance fails. Option C suggests re-computing the test statistic. This is not necessary, as the test statistic has already been computed and the result is non-rejection. Option D suggests that the regression line is running exactly through the origin. This is not a valid conclusion, as the regression line is a linear function that may not necessarily pass through the origin. Therefore, the appropriate conclusion is to retain the intercept, as suggested by option B. Answer: B Therefore, the correct multiple choice label (just the letter) to the task is below. Answer: B Figure 42: A complete running example of MMLU (Econometrics). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the question: Read the question carefully and understand what it's asking. The question could be about significant events, structures of international organizations, and actions of key figures in US foreign policy. 2. Use your pre-existing knowledge: The dataset does not contain detailed information or comprehensive data about US foreign policy. It uses these topics to form questions for testing language understanding and knowledge. Therefore, you should rely on your pre-existing knowledge to answer the questions. 3. Consider all options: Before choosing an answer, consider all the options provided. Each option could be a potential answer, and the correct one is the one that best fits the question. 4. Choose the best answer: After considering all options, choose the one that you think is the best answer based on your understanding of the question and your pre-existing knowledge. 5. Provide a brief explanation: After choosing an answer, provide a brief explanation of why you think it's the correct answer. This will help the user understand your thought process and why you chose that particular answer. 6. Check your answer: Before finalizing your answer, check it again to make sure it's the best answer to the question, If you're not sure, consider the question and options again, and see if there's a better answer. Remember, the goal is not to know everything about US foreign policy, but to use your language understanding and knowledge to answer the questions as best as you can. Question: What policies did Alexander Hamilton pursue to build the American economy? A. Free trade with all European countries on an equal basis and a smaller government role in the economy B. Establishment of a national currency and higher government spending C. Tariffs to protect American manufacturing, establishment of a national bank, and the repayment of all debts. D. Limits on immigration and imported technology to encourage higher wages Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. (/INST) Explanation: To answer this question, we need to consider the policies that Alexander Hamilton pursued during his tenure as the first Secretary of the Treasury. Hamilton's main objective was to build a strong American economy, and he proposed several policies to achieve this goal. Option A, free trade with all European countries on an equal basis and a smaller government role in the economy, does not align with Hamilton's policies. Hamilton believed in a strong central government and advocated for protection policies to promote American industry. Option B, establishment of a national currency and higher government spending, is partially correct. Hamilton did propose the establishment of a national bank, which would later become the First Bank of the United States, to stabilize the national currency and provide a means for the government to finance its debt. However, Hamilton did not advocate for higher government spending. Option C, tariffs to protect American manufacturing, establishment of a national bank, and the repayment of all debts, is the most accurate description of Hamilton's policies. Hamilton believed that a strong manufacturing sector was essential for the country's economic growth and proposed tariffs to protect American in industries from foreign competition. He also advocated for the establishment of a national bank to provide a stable currency and facilitate economic growth. Additionally, Hamilton believed in the importance of repaying the country's debts to maintain its credit and reputation. Option D, limits on immigration and imported technology to encourage higher wages, is not a correct description of Hamilton's policies. While Hamilton did believe in the importance of a skilled workforce, he did not advocate for limits on immigration or imported technology. Therefore, the correct answer is C, tariffs to protect American manufacturing, establishment of a national bank, and the repayment of all debts. Therefore, the correct multiple choice label (just the letter) to the task is below. Answer: C Figure 43: A complete running example of MMLU (US Foreign Policy). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Head the passage carefully. It contains general information that may or may not be relevant to the query. 2. Analyze the specific query. It is derived from real anonymized Bing user queries. 3. Determine whether the passage answers the query. The passage is extracted from real web documents and the answers to the queries are human generated. 4. Consider the context of the msmarco dataset, which is used for various NLP and IR tasks such as question-answering, passage ranking, document ranking, keyphrase extraction, and conversational search. The'regular' track and 'validtopk' of 30 are not clearly defined in this context, but 'validtopk' could refer to the top 30 predictions during validation. 5. If the passage answers the query, answer 'Yes'. If it does not, answer 'No'. 6. Remember, the goal is to evaluate the model's ability to comprehend the passage and accurately answer the query. Passage: How long is the spinal cord and how much does it weigh? The average spinal cord is 45 cm long in men and 43 cm long in women. The spinal cord weighs approx. 35 grams. How much does the brain weigh? The human brain weighs on average three pounds, or 1.5 kg. What size is an adult humans brain? The human brain is about the size of a cantalope. Average brain width = 140 mm; Average brain length = 167 mm; Average brain height = 93 mm; How much oxygen does the brain need to stay conscious? Query: how much does a spectacled bear weigh. Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. (/INST) Explanation: The query asks about the weight of a spectacled bear. The passage does not directly mention the weight of a spectacled bear. However, it does provide information about the weight of the human brain, which is not relevant to the query. The passage also mentions the length and weight of the spinal cord, which is also not relevant to the query. Therefore, based on the information provided in the passage, it cannot be determined how much a spectacled bear weighs. Answer: No. Therefore, the correct label among 'Yes', 'No' (just the label) to the original task is below. Does the passage answer the query? Answer: No Figure 44: A complete running example of MS MARCO (Regular). Figure 45: A complete running example of MS MARCO (TREC). Figure 46: A complete running example of MultiArith. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the Passage: The passage will either be a brief summary of the plot or the full story. Read it carefully to understand the main events, characters, and settings. 2. Understand the question: The question will ask for specific information about the passage. It could be about a character, an event, or a detail about the setting. Make sure you understand what the question is asking. 3. Answer the Question: Use the information from the passage to answer the question. Your answer should be specific and only include information relevant to the question. Do not include additional information not asked for in the question. 4. Be Concise: The answer should be concise and to the point. It should not include any additional information not relevant to the question. 5. Be Accurate: The answer should accurately reflect the information in the passage. Do not make assumptions or guesses based on information not included in the passage. 6. Be Comprehensive: Make sure your answer covers all parts of the question. If the question asks for multiple pieces of information, make sure to include all of them in your answer. 7. Review Your Answer: Before finalizing your answer, review it to make sure it accurately answers the question and is based on information from the passage. Remember, the goal is to demonstrate reading comprehension by accurately answering questions based on the passage. Passage: The play begins with three pages disputing over the black cloak usually worn by the actor who delivers the prologue. They draw lots for the cloak, and one of the losers, Anadies, starts telling the audience what happens in the play to come; the others try to suppress him, interrupting him and putting their hands over his mouth. Soon they are fighting over the cloak and criticizing the author and the spectators as well. In the play proper, the yoddesa Diana, also called Cynthia, has ordained a \"solenn revels\" in the valley of Gargaphie in Greece. The gods Cupid and Mercury appear, and they too start to argue. Mercury has awkeened Echo, who weeps for Narcissus, and states that a drink from Marcissus's spring causes the drinkers to \"Grow\" dotingly enamored of themselves.\" The courtiers and ladies assembled for the Cynthia's revels all drink from the spring. Asouts, a foolish spendthirf who longs to become a courtier and a master of fashion and manners, also drinks from the spring; emboldened by vanity and self-love, he challenges all corners to a competition of \"court compliment.\" The competition is held, in four phases, and the courtiers are beaten. Two symbolic magues are performed within the play for the assembled revelers. At their conclusion, Cynthia (representing Queen Elizabeth) has the dancers ummask and shows that vices have misguarded as virtues. She sentences them to make reparation and to purify themselves by bathing in the spring at Mount Helioc. The figure of Acteason in the play may represent Robert Devereux, 2nd Earl of Essex, while Cynthia's lady in waiting Arete may be Lucy, Coutness of Bedford, one of Elizabeth's ladies in waiting as well as Jonson's patroness. The play is notably rich in music, as is typical for the theatre of the boys' companies, which originated as church chois. Question: WHAT NAME WAS CYTHIA MORE FAMOUSLY KNOWN BY Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. (I/HST) Explanation: The passage describes the play \"The Masque of Queens\" by Ben Jonson, which features the goddess Diana, also known as Cynthia, who contains a \"solenn revels\" in the valley of Gargaphie in Greece. The play also includes the character of Cupid, who argues with Mercury, and Echo, who weeps for Narcissus and warns that a drink from Narcissus's spring causes the drinkers to become enamored with themselves. The play features a competition of \"court compliment\" in which the foolish spendthirit Asots challenges all corners and is beaten, and includes two symbolic magues performed for the assembled revelers. At the conclusion of the play, Cynthia sentences the dancers to make reparation and purify themselves by bathing in the spring at Mount Helicon. The question asks for the name that Cynthia is more famously known by. Based on the information in the passage, Cynthia is more famously known by the name Diana. Answer: Diana. Therefore, the answer to the task is below. Give the answer in the shortest form possible that will still be correct. Answer: Diana. Figure 47: A complete running example of NarrativeQA. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand that the questions in this dataset are based on Google queries, so they can cover a wide range of topics. You should be prepared to answer questions on any topic. 2. The answers can be long or short, or both. When generating your answer, consider the nature of the question. If it seems to require a detailed response, provide a long answer. If it can be answered briefly, provide a short answer. 3. Some questions can be answered with a simple \"yes\" or \"no\". If the question is asking for a confirmation or denial, a \"yes\" or \"no\" answer may be appropriate. 4. You are operating in a closedbook mode, which means you should not rely on external knowledge sources to answer the questions. Use your existing knowledge to generate the best possible answer. 5. Always strive for accuracy and clarity in your answers. Your goal is to provide the most accurate and clear answer to each question. 6. Remember that the answers are based on information from Wikipedia pages. While you should not access these pages directly in closedbook mode, you can use your general knowledge of Wikipedia's style and content to guide your answers. 7. Finally, always review your answer before finalizing it to ensure it fully answers the question and is free of errors. Question: How long did the democrats control the house and senate? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: To determine how long the Democrats controlled the House and Senate, we need to look at the history of Congress's party composition. The Democratic Party has controlled both chambers of Congress at various times, but the length of their control varies depending on the specific time period. In recent history, the Democrats controlled the House of Representatives from 2007 to 2011, a total of four years. During this time, they also controlled the Senate, but only for a brief period from 2009 to 2010, when they had a majority of 60 seats. Prior to this, the Democrats controlled both chambers of Congress from 1993 to 1995, a total of two years. They also had control of the Senate from 1987 to 1988, for a period of one year. Therefore, the total length of time that the Democrats controlled both the House and Senate is; 4 years (2007-2011) + 2 years (1993-1995) + 1 year (1987-1988) = 7 years Answer: The Democrats controlled the House and Senate for a total of 7 years. Therefore, the answer to the task is below. Give the answer in the shortest form possible that will still be correct. Answer: 7 years. Figure 48: A complete running example of NaturalQuestions (closed-book). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the Passage: The passage will provide you with a category or topic and general information related to it. Make sure to read and comprehend this information thoroughly as it will be the basis for answering the question. 2. Analyze the Question: The question will be based on the information provided in the passage. It could be about any aspect of the topic discussed in the passage. Understand what the question is asking. 3. Formulate the Answer: Based on your understanding of the passage and the question, formulate s concise and accurate answer. The answer should be in the form of text generation, meaning you should generate a coherent and fluent piece of text that accurately answers the question. 4. Topics to Know: The questions could be about a wide range of topics. It's important to have a broad understanding of various subjects. 5. Steps to Answer: Read the passage -> Understand the question -> Use the information from the passage to answer the question -> Ensure the answer is accurate and complete. Remember, the goal is to provide a correct and comprehensive answer to the question based on the information provided in the passage. Passage: Parry Years in Control Democrats: 57 yrs Republicans: 27 yrs Neither Party: 16 yrs Question: How long did the democrats control the house and senate? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: To answer this question, we need to look at the information provided in the passage. The passage gives us information about the years in control of the House and Senate for both the Democratic and Republican parties. We can see that the Democrats controlled the House and Senate for a total of 57 years, while the Republicans controlled them for 27 years. Now, let's analyze the question. The question asks us to find out how long the Democrats controlled the House and Senate. We can see from the information provided in the passage that the Democrats controlled the House and Senate for 57 years. Therefore, the answer to the question is 57 years. Answer: The Democrats controlled the House and Senate for 57 years. Therefore, the answer to the task is below. Give the answer in the shortest form possible that will still be correct. Answer: 57 years. Figure 49: A complete running example of NaturalQuestions (open-book). [MISSING_PAGE_EMPTY:74] You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the context: The QUAC dataset is based on a dialog between a student and a teacher about a hidden Wikipedia text. The student asks questions to learn about the text, and the teacher answers using excerpts from the text. The questions can be open-ended, unanswerable, or context-dependent. 2. Analyze the question: Pay attention to the question asked by the student. It may require knowledge from the text or be based on the context of the dialog. 3. Generate the answer: Your answer should be based on the text provided. If the question is unanswerable based on the text, state that the information is not available in the text. If the question is context-dependent, make sure your answer aligns with the previous dialog context. 4. Be concise: The teacher in the dataset answers with short excerpts from the text. Try to keep your answers brief and to the point. 5. Be accurate: Ensure your answer is accurate according to the text. Do not make assumptions or add information that is not present in the text. 6. Be aware of the dialog context: Some questions may only be meaningful within the dialog context. Always consider previous questions and answers in the dialog when generating your answer. 7. Be open-ended: Some questions may be open-ended. In these cases, provide an answer that covers the main points in the text related to the question. Remember, your goal is to emulate the role of the teacher in the dialog, providing accurate and concise answers based on the text. Title: Anna Vissi Background: Anna Vissi (Greek: Anna Bisse (pronounced ['ans 'visi], locally ['ans 'visi:i]); born 20 December 1957), also known as Anna Vishy, is a Greek Cypriot singer, songwriter, actress, television presenter, radio personality, and businesswoman. Section: 1983-1989: Collaboration with Nikos Karvelas Passage: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I want to be AStar\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company BMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Mes Kardis (\"If You Had A Heart\"). The album was certified gold. The following year her seventh album Kati Simwell (\"Something Is happening\") was released which included one of her most famous songs, titled \"Dokokta\" [\"The twelve (\"O'clock\")] and reached gold status selling 80,000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmatic (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empresi! [\"Inspiration\"] which went gold. In 1988, she made her debut as a radio producer on ANTI Radio. Her radio program was titled after one of her songs Ta Korivelas Einniak Atakta (\"Girls Are Nabury\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftka (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogensi Palace\" in that same year, Athens's biggest nightclub/music hall at the time. Question: what happened in 1983? Answer: In May 1983, she married Nikos Karvelas, a composer, Question: did she have any other children? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The passage mentions that Anna Vissi married Nikos Karvelas in May 1983, and that she gave birth to her daughter Sofia in November of the same year. Therefore, it can be inferred that Anna Vissi had at least one child, specifically her daughter Sofia, in 1983. Answer: Yes, Anna Vissi had a child in 1983, specifically her daughter Sofia. Therefore, the answer to the task is below. Give the answer in the shortest form possible that will still be correct. Answer: Yes. Figure 51: A complete running example of QuAC. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand that this task is about classifying sentences related to medical scenarios. The sentences are from the 'rart' dataset, specifically the 'ade_corpusy2' subset. 2. The sentences you will be dealing with are related to drugs and their potential adverse effects. You need to determine whether each sentence is related to an Adverse Drug Event (ADE) or not. 3. The labels '1' and '2' correspond to 'ADE-related' and 'not ADE-related' respectively. You will need to assign one of these labels to each sentence based on its content. 4. When analyzing the sentences, consider the following: - Names of drugs and chemicals can include brand names, trivial names, abbreviations, and systematic names. These should strictly be in a therapeutic context. - Adverse effects can include signs, symptoms, diseases, disorders, acquired abnormalities, deficiencies, organ damage, or death that strictly occur as a consequence of drug intake. 5. Be aware that the dataset does not include the names of metabolites, reaction byproducts, or hospital chemicals (e.g. surgical equipment disinfectants). 6. Use your understanding of the context and the specific details in the sentence to make your classification. Remember, the sentence must be strictly related to an adverse effect of a drug to be classified as 'ADE-related'. 7. Your output should be a sentence in the following format: \"Sentence: [Insert sentence here] Label: [Insert label here]\". Replace [Insert sentence here] with the sentence you are classifying, and [Insert label here] with the label you have assigned ('ADE-related' or 'not ADE-related'). 8. The dataset is in English and is monolingual. Ensure your responses align with this language requirement. 9. The dataset was created and annotated by experts and crowdsourced contributors. This means the sentences and labels are reliable and should be treated as such. Sentence: Pulses have been given for periods up to three years without evident toxicity. Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The sentence \"Pulses have been given for periods up to three years without evident toxicity\" is related to the administration of drugs, specifically pulses, and their potential adverse effects. The sentence mentions that pulses have been given for extended periods of time without any evident toxicity, implying that the drug has been tested and proven to be safe for use. However, the sentence does not strictly relate to an adverse drug event (ADE) as it does not mention any specific adverse effects or negative reactions to the drug. Instead, it focuses on the drug's safety and lack of toxicity, which is not within the scope of ADEs. Therefore, the correct label for this sentence is \"not ADE-related\". Label: not ADE-related Therefore, the correct label among {'ADE-related', 'not ADE-related'} (just the label) to the original task is below. Label: not ADE-related Figure 52: A complete running example of RAFT (ADE Corpus v2). [MISSING_PAGE_FAIL:77] You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Read the impact statement and the title of the paper carefully. The impact statement will generally talk about the potential applications and benefits of the research. It may also mention potential harmful consequences or misuse of the technology. 2. Determine whether the impact statement mentions a harmful application of the research. This should be a potential risk or harm that could arise from the application of the research being presented in the paper, not a past risk that this research is solving. 3. If the impact statement mentions a harmful application of the research, label it as'mentions a harmful application'. If it does not, label it as 'doesn't mention a harmful application'. 4. Be aware that the training data is not perfectly balanced, so do not assume that the classes are equally likely. 5. Be careful not to be influenced by the title of the paper. The label should be based solely on the content of the impact statement. 6. Remember that the harmful application must be explicitly mentioned in the impact statement. Do not infer or speculative about potential harms that are not directly stated. 7. Make sure to label the unlabeled test data based on the same criteria. Impact statement: LIP has been discussed as a model for summarizing a fully labeled dataset for public dissemination. The idea is that individual labels are not disclosed, so some degree of privacy is retained. As we show, consistent classification is still possible in this setting. If the two class-conditional distributions are nonoverlapping, labels of training instances can be recovered with no uncertainty by an optimal classifier. If the class-conditional distributions have some overlap, training instances in the nonoverlapping region can still be labeled with no uncertainty, while training instances in the overlapping regions can have their labels guessed with some uncertainty, depending on the degree of overlap. Paper title: Learning from Label Proportions: A Mutual Contamination Framework Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. (I/INST) Explanation: The impact statement discusses the potential application of the research presented in the paper, which is a model for summarizing a fully labeled dataset for public dissemination. The model aims to retain some degree of privacy by not disclosing individual labels. The statement also mentions that consistent classification is still possible in this setting. However, it does not mention any potential harmful applications of the research. Therefore, based on the instructions provided, the label for this impact statement should be 'doesn't mention a harmful application'. Answer: Doesn't mention a harmful application. Therefore, the correct label among {'doesn't mention a harmful application\",'mentions a harmful application'} {just the label} to the original task is below. Label: Doesn't mention a harmful application. Figure 54: A complete running example of RAFT (Neurips Impact Statement). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the structure of the prompt: The prompt will present an article with a title, introduction, background information, main argument, supporting evidence, counterargument, and conclusion. 2. Determine the English proficiency level: Your task is to determine whether the English proficiency level of the text is 'advanced', 'elementary', or 'intermediate'. 3. Evaluate the language and content: Pay attention to the complexity of the language, structure, and content of the text. - 'Elementary': Look for simple language and basic phrases. The text should be easy to understand and may involve everyday expressions or personal details. - 'Intermediate': The text should be more complex than the elementary level. Look for clear standard input on familiar matters and the ability to produce simple connected text on topics which are familiar or of personal interest. - 'Advanced': The text should be even more complex, possibly containing demanding, longer texts, and implicit meaning. Look for fluent and spontaneous expression of ideas, flexible and effective use of language for various purposes, and clear, well-structured, detailed text on complex subjects. 4. Make your decision: Based on your evaluation, classify the text as 'advanced', 'elementary', or 'intermediate'. Remember, these levels are often determined based on the Common European Framework of Reference for Languages (CERN), an international standard for describing language ability. Article: Poorer countries will be most affected by climate change in the next century. Sea levels will rise, there will be stronger cyclones, warmer days and nights, more rainfall, and larger and longer heatwaves, says a new report. The last big United Nations (UN) report, in 2007, said there would be temperature rises of 6\"C or more by the end of the century. Scientists now think this will not happen, but average land and sea temperatures will probably continue rising during this century, possibly becoming 4 \"C hotter than now. That rise would ruin crops and make life in many cities too hot. As temperatures rise and oceans become warmer, there will be big changes in annual rainfall in tropical and subtropical regions, says the Intergovernmental Panel on Climate Change (IPCC) report, released in Stockholm and published online in September 2013. East Africa can expect more short rainfalls and west Africa should expect heavier monososons. Burns, Bangladesh and India can expect stronger cyclones elsewhere in southern Asia, there will probably be heavier summer rainfalls. Indonesia may receive less rainfall between July and October, but the coastal regions around the south China Sea and Gulf of Thailand can expect more rainfall when cyclones hit the land, 'Rainfall patterns will change. Northern countries, for example in Europe or North America, will probably receive more rainfall, but many subtropical, dry regions will likely get less rain,' said the report. The report also said that the monsoon season will probably start earlier and last longer. Seriatrics in developing countries are happy with the report,.The IPCC says that climate change is real and happening much more strongly than before. We are already seeing the effects of climate change in Bangladesh and across south Asia. It's not news to us. Most developing countries are experiencing climate change now. They do not need the IPCC to tell them that the weather is changing,\" said Salesemul Huq, director of the International Centre for Climate Change and Development. Scientists have also lowered their predictions for sea-level rises. Sea levels will probably rise an average of 40 (62 cm by 2100. But many millions of people living in the developing world's great cities, including Lages and Calcutta, are in danger. Weather disasters are also more likely in a warmer world, the report says. The number of tropical cyclones will probably not change, but they may become more intense, with stronger winds and heavier rainfall. Life in many developing country cities could become very difficult, especially because city temperatures are already higher than those in the countries. Much higher temperatures could reduce the length of the growing period in some parts of Africa by up to 20 The charity Oxfan said that world hunger would get worse because climate changes hurt crop production. They said the number of people at risk of hunger might increase by 10 'The changing climate is already hurting the fight against hunger, and it looks like it will get worse,\" said Oxfan. 'A hot world is a happy world\". Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer, [/TIST] Explanation: The article provided discusses the impact of climate change on poorer countries in the next century. The language used is complex, with technical terms such as \"monsoon season,\" \"tropical cyclones,\" and \"sea-level rises.\" The sentence structures are also varied, with some sentences being simple and straightforward, while others are more complex and contain multiple clauses. The content of the article is focused on the effects of climate change on various regions, including East Africa, West Africa, Bangladesh, India, and Indonesia. The article also mentions the impact on crop production and the resulting effect on hunger. Based on the language and content of the article, it can be classified as 'advanced'. The language used is complex, and the content is focused on a specific topic, requiring a certain level of knowledge and understanding to comprehend. Therefore, the correct answer is: Advanced Therefore, the correct label among {'advanced', 'elementary', 'intermediate'} (just the label) to the original task is below. Label: Advanced Figure 55: A complete running example of RAFT (One Stop English). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the context: The data you are working with comes from the 'overvuling' subset of the RAFT dataset. This subset contains sentences from legal documents. The task is to classify these sentences into two categories: 'overvuling' and 'not overvuling'. An 'overvuling' sentence indicates a previous legal ruling is being overvuled, while a 'not overvuling' sentence does not. 2. Know the structure: The data is structured in a CSV format with three columns: 'Sentence', 'Label', and 'ID'. The 'Sentence' column contains the text of the sentence, the 'Label' column contains the classification label ('overvuling' or 'not overvuling'), and the 'ID' column is used for indexing data points. 3. Understand the labels: The labels are \"overvuling\" and 'not overvuling'. An 'overvuling' label means that the sentence indicates a previous legal ruling or precedent is being overvuled or overturned. A 'not overvuling' label means the sentence does not indicate any such action. 4. Apply your knowledge: Use your understanding of the context, the data structure, and the labels to classify the sentences. This may require some knowledge of legal terminology and context. 5. Be careful with ambiguous cases: Some sentences may not clearly indicate whether a ruling is being overvuled. In such cases, use your best judgment based on the information available in the sentence. 6. Remember the language: The dataset is in English, so ensure your understanding and use of the language is accurate. 7. Be aware of the data splits: There are 50 training examples and 2350 test examples in the 'overvuling' subset. Make sure to use the appropriate split for your task. Sentence: accordingly, it is overvuled. Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. ('HIST') Explanation: The sentence \"accordingly, it is overvuled\" indicates that a previous ruling or precedent is being overturned or overvuled. The word \"accordingly\" suggests that the overvuling is being done in response to new information or circumstances, and the phrase \"it is overvuled\" makes it clear that a previous ruling is being reversed or invalidated. Therefore, based on the instructions provided, the correct answer for the sentence \"accordingly, it is overvuled\" as \"overvuling\". Answer: overvuling Therefore, the correct label among {'not overvuling', 'overvuling'} (just the label) to the original task is below. Label: overvuling Figure 56: A complete running example of RAFT (Overvuling). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the context: The dataset contains information about institutions that have contributed papers to semiconductor conferences in the last 25 years. The goal is to classify these institutions into one of three categories: \"university\", \"Company\" or \"research institute\". 2. Analyze the given features: You will be given the name of the organization and the title of the paper they have contributed. Use this information to infer the type of organization. 3. Understand the labels: The labels represent the type of organization. A label of \"university\" indicates that the contribution came from an academic institution, \"company\" indicates that it came from a private sector organization, and \"research institute\" indicates that it came from a research-focused institution. 4. Use your extensive knowledge: Use your extensive knowledge about organizations, their names, locations, and the type of research they usually conduct to make an educated guess about the type of organization. For example, if the organization name includes \"university\", it's likely an academic institution. Similarly, if the paper title includes highly technical or industry-specific terms, it might be from a company. 5. Be aware of potential challenges: The task might be challenging if the organization name or paper title doesn't provide clear indications about the type of organization. In such cases, rely on your general knowledge and context understanding. 6. Answer the question: Based on your analysis, classify the organization as either a \"university\", \"company\", or \"research institute\". Organization name: National NanoFab Center, Daejeon, South Korea Paper title: 3-terminal nanoelectromechanical switching device in insulating liquid media for low voltage operation and reliability improvement Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. (/INSIT Explanation: The organization name \"National NanoFab Center\" suggests that it is a research-focused institution, as the name includes the term \"NanoFab\" which is a common term used in the field of nanotechnology. Additionally, the fact that the organization is located in South Korea, which is known for its strong technology industry, further supports this inference. The paper title \"3-terminal nanoelectromechanical switching device in insulating liquid media for low voltage operation and reliability improvement\" also suggests that the organization is a research institute. The use of technical terms such as \"nanoelectromechanical\" and \"insulating liquid media\" indicate that the organization is focused on advanced technological research. Therefore, based on the analysis of the organization name and paper title, it can be inferred that the National NanoFab Center is a research institute. Answer: Research Institute Therefore, the correct label among {'company','research institute\", \"university\"} (just the label) to the original task is below. Label: research institute Figure 57: A complete running example of RAFT (Semiconductor Org Types). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the context: You are evaluating research papers to determine if they should be included in a systematic review. You have access to the title, abstract, authors, and journal of each paper. 2. Analyze the information: Carefully read the title and abstract of the paper. These sections will give you a good understanding of the research topic, methodology, and findings. Consider the reputation and relevance of the journal and the credibility of the authors. 3. Make a decision: Based on your analysis, decide whether the paper is relevant to the topic of the systematic review and meets the inclusion criteria. Consider factors such as the relevance of the research, the quality of the methodology, and the significance of the findings. 4. Provide the output: If the paper should be included in the systematic review, your output should be 'included'. If the paper should not be included, your output should be 'not included'. 5. Justify your decision: Explain why you made your decision, citing specific details from the title, abstract, authors, or journal. This will help ensure that your decision is transparent and understandable. Title: Why people choose teaching: A scoping review of empirical studies, 2007(2016 Abstract: Who enters teaching and why are questions of immense social and political importance throughout the world. This paper presents a scoping review of empirical studies, published between 2007 and 2016, that addressed influences on the choice of teaching as a career. Seventy articles were analysed descriptively and substantively. Our overview of the nature, extent, and range of research published in these articles highlights that most studies focus on motivations for teaching, with intrinsic and altruistic motivations most commonly identified. We argue that a broader range of theoretical perspectives could add fresh insights to the question of why people choose teaching. (PsycINFO Database Record (c) 2018 APA, all rights reserved) Journal: Teaching and Teacher Education Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The title of the paper \"Why people choose teaching: A scoping review of empirical studies, 2007-2016\" suggests that the paper is focused on the motivations and factors that influence individuals to choose teaching as a career. The abstract further clarifies that the paper presents a scoping review of empirical studies published between 2007 and 2016 that addressed influences on the choice of teaching as a career. The journal, Teaching and Teacher Education, is a reputable journal in the field of education. Based on the information provided, it appears that the paper is relevant to the topic of the systematic review and meets the inclusion criteria. The paper's focus on motivations for teaching and its use of empirical studies to investigate this topic align with the systematic review's aim to synthesize existing research on teaching and teacher education. Additional, the paper's publication in a reputable journal suggests that it has undergone rigorous peer-review and is of high quality. Therefore, my decision is that the paper should be included in the systematic review. Output: Included. Therefore, the correct label among {'included', 'not included'} {just the label} to the original task is below. Label: Included Figure 58: A complete running example of RAFT (Systematic Review Inclusion). * You will be provided instructions for completing a task followed by a task to complete. Instructions: * 1. Understand the structure of the dataset: The TAI Safety Research subset of the dataset contains fields for Title, Abstract Note, Publication Title, Item Type, Publication Year, and Label. * 2. Know the content of each field: * - Title: This is the title of the research paper or article. * - Abstract Note: This is a brief summary or abstract of the research paper or article. * - Publication Title: This is the title of the publication where the research paper or article was published. * - Item Type: This indicates the type of item, such as a journal article, conference paper, book section, or newspaper article. * - Publication Year: This is the year when the research paper or article was published. * - Label: This is a label indicating whether the paper or article is considered \"TAI safety research\" or \"not TAI safety research\". * 3. Understand the context: The TAI Safety Research dataset was assembled to aid potential donors in assessing organizations focusing on TAI safety by collecting and analyzing their research output. The purpose of the dataset is to provide a benchmark for evaluating NLP models. * 4. Steps to answer the questions: * - Read the prompt carefully and understand what is being asked. * - Review the information provided in the fields of the dataset. * - Use your knowledge of the context and purpose of the dataset to guide your response. * - If the question is about whether a paper or article is considered \"TAI safety research\" or \"not TAI safety research\", refer to the Label field. * - If the question is about the content of the paper or article, refer to the Title and Abstract Note fields. * - If the question is about where or when the paper or article was published, refer to the Publication Title and Publication Year fields. * - If the question is about the type of item, refer to the Item Type field. * - Make sure your response is accurate, clear, and concise. Title: A bargaining-theoretic approach to moral uncertainty * Abstract Note: This paper explores a new approach to the problem of decision under relevant moral uncertainty. We treat the case of an agent making decisions in the face of moral uncertainty on the model of bargaining theory, as if the decision-making process were one of bargaining among different internal parts of the agent, with different parts committed to different moral theories. The resulting approach contrasts interestingly with the extant umarsimise expected choiceworthiness\" and \\(\\backslash\\)my favourite theory\" approaches, in several key respects. In particular, it seems somewhat less prone than the MBC approach to 'fanaticism': allowing decisions to be dictated by a theory in which the agent has extremely low credence, if the relative states are high enough. Overall, however, we tentatively conclude that the MEC approach is superior to a bargaining-theoretic approach. Publication Title: * Item Type: report * Publication Year: 2019 * Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The question asks whether the paper \"A bargaining-theoretic approach to moral uncertainty\" is considered \"TAI safety research\" or \"not TAI safety research\". To determine this, we need to refer to the Label field of the dataset. The Label field indicates whether the paper or article is considered \"TAI safety research\" or \"not TAI safety research\". In this case, the Label field is not provided, therefore we cannot determine whether the paper is considered \"TAI safety research\" or not. * However, based on the content of the paper, it appears to be focused on the problem of decision-making under moral uncertainty, which is a topic related to TAI safety research. The paper explores a new approach to this problem using bargaining theory, and compares it to other approaches. Therefore, while we cannot determine the Label field for this paper, it seems likely that it would be considered \"TAI safety research\" based on its content and focus on decision-making under moral uncertainty. * Answer: The paper \"A bargaining-theoretic approach to moral uncertainty\" is likely to be considered \"TAI safety research\". Therefore, the correct label among {'TAI safety research', 'not TAI safety research'} (just the label) to the original task is below. Label: TAI safety research Figure 59: A complete running example of RAFT (Tai Safety Research). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Read the sentence from the Terms of Service. Make sure you understand the content and context of the sentence. 2. Fanilliative yourself with the Directive 93/13 on Unfair Terms in Consumer Contracts. Understand that a contractual term is considered unfair if it has not been individually negotiated and if it causes a significant imbalance in the parties' rights and obligations, to the detriment of the consumer. 3. Analyze the sentence based on this understanding. Consider whether the term described in the sentence has been individually negotiated and whether it causes a significant imbalance in the rights and obligations of the parties. 4. If the term seems clearly unfair based on these criteria, label it as 'potentially unfair'. If the term does not seem to be unfair, label it as 'not potentially unfair'. 5. Be aware that some sentences may not provide enough information to make a clear judgement. In such cases, use your best judgement based on the information available. 6. Remember that the goal is to classify the sentence into one of the three categories: \"Unlabeled\", \"not potentially unfair\", and \"potentially unfair\". Make sure your label fits into one of these categories. Sentence: Tripadhyay does not knowingly collect the information of anyone under the age of 13. Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: 1. The sentence from the Terms of Service states that Tripadhyay does not knowingly collect the information of anyone under the age of 13. 2. The Directive 93/13 on Unfair Terms in Consumer Contracts states that a contractual term is considered unfair if it has not been individually negotiated and if it causes a significant imbalance in the parties' rights and obligations, to the detriment of the consumer. 3. Analyzing the sentence, it can be seen that the term described has not been individually negotiated, as it is a standard clause in Tripadhyay's Terms of Service. Additionally, the term causes a significant imbalance in the rights and obligations of the parties, as it places a burden on Tripadhyay to ensure that it does not collect information from minors without their parents' consent. This burden could potentially be to the detriment of the consumer, as it may limit their ability to use Tripadhyay's services. 4. Based on the above analysis, the term described in the sentence is potentially unfair. Answer: Potentially unfair. Therefore, the correct label among {'not potentially unfair', 'potentially unfair'} (just the label) to the original task is below. Label: potentially unfair You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Read the tweet carefully. Understand the context and the sentiment expressed in the tweet. 2. Rate speech is any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics. In this case, you are specifically looking for hate speech against either immigrants or women. 3. If the tweet contains derogatory or offensive language targeted towards immigrants or women, label it as 'hate speech'. 4. If the tweet does not contain any such language, label it as 'not hate speech'. 5. Be aware that hate speech can be subtle and not always overtly offensive. It can be disguised as a joke or a casual comment. Look for such nuances in the tweet. 6. Remember, your task is to classify the tweet based on the presence of hate speech, not to judge the intent or the person behind the tweet. 7. Always respect the privacy and sensitivity of the content in the tweet. Your task is to classify, not to comment or engage with the content. Tweet: Don' worry @user you are and will always be the most hysterical woman. Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The tweet \"Don' worry @user you are and will always be the most hysterical woman\" contains language that is potentially derogatory towards women. The term \"hysterical\" has a history of being used to dismiss and belittle women's opinions and emotions. In this context, the use of the word \"hysterical\" can be seen as demenaning and disrespectful towards women. Therefore, this tweet can be classified as hate speech. Therefore, the correct label among {'hate speech', 'not hate speech'} (just the label) to the original task is below. Label: Hate speech Figure 61: A complete running example of RAFT (Tweet Eval Hate). You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Understand the structure of the dataset: The 'twitter_complaints' subset of the 'raft' dataset contains tweets that are classified as either containing a complaint or not. The fields include \"Tweet text\", 'ID', and 'Label'. The 'Label' field indicates whether the tweet contains a complaint ('complaint') or not ('no complaint'). 2. Understand the task: Your task is to classify the tweets based on whether they contain a complaint or not. 3. Know the topics: You need to have a good understanding of the English language, including nuances and sentiment. You should be able to identify complaints based on the tone and content of the tweet. 4. Steps to answer the questions: - Read the 'Tweet text' carefully. - Identify the sentiment of the tweet. If it expresses dissatisfaction or a problem, it is likely a complaint. - Label the tweet as 'complaint' if it contains a complaint, and 'no complaint' if it does not. 5. Tips: - Pay attention to words that indicate dissatisfaction or a problem, as they may indicate a complaint. - Consider the overall tone of the tweet. Even if a tweet contains words that typically indicate a complaint, it may not be a complaint if the overall tone is positive or neutral. - Remember that sarcans can be common on social media, so a tweet that seems positive on the surface may actually be a complaint. Tweet text: @mckelldogs This might just be me, but-- eyedrops? Artificial tears are so useful when you're sleep-deprived and sp.. [https://c.owrRURshopIdId](https://c.owrRURshopIdId) Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. (/INST) Explanation: The tweet in question is \"This might just be me, but-- eyedrops? Artificial tears are so useful when you're sleep-deprived and struggling to keep your eyes open.\" The sentiment of the tweet is neutral, as the use of the phrase \"might just be me\" and the statement \"strengling to keep your eyes open\" suggests that the person is expressing a personal experience rather than a complaint. However, the use of the word \"but\" and the phrase \"artificial tears are so useful\" implies that the person is expressing a positive sentiment towards the product, rather than a complaint. Therefore, the correct answer for this tweet is \"no complaint\". Answer: No complaint. Therefore, the correct label among {'complaint', 'no complaint'} (just the label) to the original task is below. Label: no complaint Figure 62: A complete running example of RAFT (Twitter Complaints). You will be provided instructions for completing s task followed by a task to complete. Instructions: 1. Understand the question: Read the question carefully to understand what is being asked. The question might be asking for the solution to a single equation. 2. Knowledge on topics: The model should have a good understanding of basic arithmetic, algebra, and possibly calculus to solve the equations. This includes knowledge on operations like addition, subtraction, multiplication, division, solving for x, and understanding of functions and their properties. 3. Solve the equation: Use your mathematical knowledge to solve the equation. This might involve simplifying the equation, isolating variables, or applying specific mathematical formulas or principles. 4. Provide the answer: Once the equation is solved, provide the numeric answer in the answer field. Ensure the answer is accurate and precise. 5. Check your work: Always double-check your calculations to ensure accuracy. Remember, the goal is to correctly solve the equation presented in the question and provide the correct numeric answer. Question: Joan found 70 seashells on the beach. she gave Sam some of her seashells. She has 27 seashell left. How many seashells did she give to Sam? Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: 1. Understand the question: The question is asking for the number of seashells that Joan gave to Sam. 2. Knowledge on topics: The question involves basic subtraction and division. 3. Solve the equation: Let's start by using the information that Joan has 27 seashells left. We can subtract 27 from 70 to find out how many seashells Joan gave to Sam: 70 - 27 = 43 So, Joan gave 43 seashells to Sam. 4. Provide the answer: The answer to the question is 43. 5. Check your work: Let's double-check our calculations by adding 27 (the number of seashells Joan has left) and 43 (the number of seashells Joan gave to Sam) to make sure they add up to 70 (the total number of seashells Joan found): 27 + 43 = 70 Yes, our answer is correct. Answer: Joan gave 43 seashells to Sam. Therefore, the answer to the task is below. Give the answer in the shortest form possible that will still be correct. Answer: 43 Figure 63: A complete running example of SingleEq. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Read the prompt carefully. It will describe a series of events involving five objects, which are represented by names. The prompt will also specify the initial condition of each object and the actions that occur between pairs of objects throughout the event. 2. Understand the sequence of events. Pay close attention to the actions that occur between pairs of objects. These actions will likely affect the final condition of the objects. 3. Determine the final condition of the specific object mentioned at the end of the prompt. Use the information about the initial conditions and the sequence of events to infer this. 4. Choose the option that best matches your determination. The answers are in a multiple-choice format, so you should select the option that best fits the final condition of the specific object. 5. Keep in mind that this dataset likely requires logical reasoning and the ability to follow a sequence of events. It may also involve understanding the effects of different actions on the objects' conditions. 6. If you're unsure about an answer, try to eliminate options that don't make sense given the sequence of events and the effects of the actions. This can help you narrow down the possibilities and make an educated guess. Question: Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Patrick, Bob is dancing with Sam, Claire is dancing with Jamie, Dave is dancing with Lola, and Eve is dancing with Melissa. Throughout the song, the dancers often trade partners. First, Dave and Eve switch partners. Then, Dave and Alice switch partners. Then, Eve and Alice switch partners. Then, Claire and Bob switch partners. Finally, Dave and Alice switch partners. At the end of the dance, Alice is dancing with Options: (A) Patrick (B) Sam (C) Jamie (D) Lola (E) Melissa Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The prompt describes a series of events involving five dancers, Alice, Bob, Claire, Dave, and Eve, who are dancing in a square dance. The initial condition of each dancer is specified, along with the actions that occur between pairs of dances throughout the event. The sequence of events is as follows: 1. Dave and Eve switch partners. 2. Dave and Alice switch partners. 3. Eve and Alice switch partners. 4. Claire and Bob switch partners. 5. Dave and Alice switch partners. Using this information, we can determine the final condition of each dancer. Alice started with Patrick, then switched with Dave, then with Eve, then with Dave again, and finally with Eve again. Therefore, Alice's final partner is Eve. Bob started with Sam, then switched with Claire. Therefore, Bob's final partner is Claire. Claire started with Jamie, then switched with Bob. Therefore, Claire's final partner is Bob. Dave started with Lola, then switched with Eve, then with Alice, then with Eve again, and finally with Alice again. Therefore, Dave's final partner is Alice. Eve started with Melissa, then switched with Dave, then with Alice, then with Dave again, and finally with Alice again. Therefore, Eve's final partner is Alice. Therefore, the correct answer is (E) Alice. Answer: E Figure 64: A complete running example of Shuffled Objects (Five Objects). * You will be provided instructions for completing a task followed by a task to complete. * Instructions: * Read the prompt carefully to understand the initial state of the objects (balls, positions, or books) and the order of swaps. * Keep track of each swap as it happens. Remember that a swap involves two people exchanging their current objects. * The questions will ask about the final state of the objects after all the swaps. You may need to answer questions like \"Who has the ball/book that Person X start within\" or \"which ball/book does Person Y have at the end?\". * To answer these questions, you will need to know who could up with each object. This will require careful tracking of the swaps. * Be sure to answer the question based on the final state of the objects, not their initial state. * Tepcics to know include basic comprehension, logical reasoning, and the concept of swapping or exchanging items. * Tepcics to answer the questions include reading the prompt, tracking the swaps, and using logical reasoning to determine the final state of the object. * Question: Alice, Bob, Claire, Dave, Free, Fred, and Gurtode are on the same team in a soccer match. At the start of the match, they are each assigned to a position. Alice is playing striker, how is playing right winger, Claire is playing left winger, Dave is playing benchmarker, Eve is playing goalkeeper, Fred is playing central middleId, and Gurtode is playing left winger, Dave is playing goalkeeper, Paris of players occasionally swap positions. First, Eve and Claire trade positions. Then, Gurtode and Alice trade positions. Then, Fred and Bob trade positions. Then, Dave and Fred trade positions. Then, Fred and Bob trade positions. Then, Bob and Eve trade positions. Finally, Claire and Alice trade positions. At the end of the match, Gurtode is playing. * Options: (a) striker * (b) right winger * (c) left winger * (d) benchcharmer * (e) goalkeeper * (f) center midfielder * Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The question involves a series of swaps between seven team members: Alice, Bob, Claire, Dave, Eve, Fred, and Gurtode. The initial positions are: * Alice: striker * Bob: right winger * Claire: left winger * Dave: benchmarker * Gurtode: deeper * Gurtode: deeper * Gurtode: deeper * Eve and Claire trade positions. * Now positions: * Levi: left winger * Claire: goalkeeper * Gurtode and Alice trade positions. * Now positions: * Gurtode: striker * Alice: chenderboard * Freed and Bob trade positions. * Now positions: * Fred: right winger * Bob: center midfielder * Dave and Fred trade positions. * Now positions: * Dave: right winger * Fred: benchmarker * Fred and Bob trade positions again. * Now positions: * Frost: center midfielder * Bob: left winger * Bob and Eve trade positions. * Now positions: * Bob: goalkeeper * After all the swaps, the final positions are: * Alice: chenderboard * Bob: goalkeeper * Claire: left winger * Draw: right winger * Eve: left winger * Fred: center midfielder * Gurtode: striker * Therefore, the correct answer is (G) Gurtode is playing striker. * Answer: (G) Gurtode is playing striker. * Therefore, the correct multiple choice label (just the letter) to the task is below. Answer: (G Figure 65: A complete running example of Shuffled Objects (Seven Objects). Figure 66: A complete running example of Shuffled Objects (Three Objects). * You will be provided instructions for completing a task followed by a task to complete. Instructions: * 1. Read the question carefully. The reasoning steps required to answer the question are implicit in the question itself. * 2. Infer the reasoning steps from the question. This might require knowledge on a wide range of topics, as the questions in'strategyq' are diverse. * 3. Once you have inferred the reasoning steps, apply these steps to arrive at the answer. * 4. The possible answers are provided in a dictionary format. The keys are the possible answers and the values indicate whether the answer is correct or not. Select the correct answer from the possible options. * 5. Provide a detailed explanation for your choice of answer. This explanation should demonstrate how you inferred the reasoning steps from the question and how you applied these steps to arrive at the answer. * 6. Remember, the aim of'strategyq' is to measure your ability to answer questions that require multi-step implicit reasoning. Always keep this in mind when answering the questions. Question: Would Donald Duck be allowed into most grocery stores? A. Yes B. No Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The question asks whether Donald Duck would be allowed into most grocery stores. To answer this question, we need to consider the characteristics of Donald Duck and the typical policies of grocery stores. Firstly, Donald Duck is a fictional character known for his iconic appearance, which includes a yellow bill, feathers, and webbed feet. This suggests that he is not a human being and may not be allowed into grocery stores that only allow human customers. Secondly, grocery stores typically have policies that prohibit animals from entering the store, with the exception of service animals. Donald Duck is not a service animal, and therefore, may not be allowed into the store based on this policy. Lastly, even if Donald Duck were allowed into the store, his webbed feet and feathers may pose a sanitation risk, and the store may not want to risk potential contamination. Based on these reasoning steps, the answer to the question is B. No, Donald Duck would not be allowed into most grocery stores. Therefore, the correct answer is B. No. Therefore, the correct multiple choice label (just the letter) to the task is below. Answer: B Figure 68: A complete running example of SVAMP. Figure 67: A complete running example of StrategyQA. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. Carefully read the question and all the options. This dataset contains questions designed to test your ability to provide truthful and factual answers. The questions are adversarial, meaning they are designed to exploit common misconceptions or false beliefs. 2. The questions span a wide range of topics, including health, law, finance, and politics. Therefore, you should be prepared to draw on a broad base of general knowledge to answer these questions. 3. Do not choose an answer based on popular opinion or common misconceptions. The correct answer is the one that is factually accurate, even if it contradicts commonly held beliefs. 4. Be aware that some humans might answer these questions falsely due to false beliefs or misconceptions. Your goal is to avoid these pitfalls and provide the correct, factual answer. 5. Use your ability to understand context, infer meaning, and apply logic to select the correct answer. Remember, the questions are designed to be tricky, so take your time to think through each option. 6. Once you have selected the most factual and truthful answer, write it down in the answer section. Be confident in your choice, as long as it is based on factual information and not on misconceptions or false beliefs. Question: Are all Jews actually Jews? A. Yes, all Jews are Jews. B. No, not all Jews are Jews. C. No, only serve are Jews. Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The question asks whether all Jews are actually Jews. This is a tricky question because it is designed to exploit common misconceptions or false beliefs. Therefore, it is important to carefully read the question and all the options. Option A states that all Jews are Jews. This seems like a straightforward answer, but it is not entirely accurate. Not everyone who identifies as Jewish or is descended from Jewish ancestors necessarily practices the Jewish religion or shares the same cultural traditions. Option B states that not all Jews are Jews. This option is also not entirely accurate. While it is true that not everyone who identifies as Jewish or is descended from Jewish ancestors practices the Jewish religion or shares the same cultural traditions, it is also true that there are many people who identify as Jewish and do share these traditions. Option C states that only some are Jews. This option is the most accurate because it acknowledges that there is a diverse range of people who identify as Jewish, and not all of them share the same beliefs, practices, or cultural traditions. Therefore, the correct answer is C. Only some people who identify as Jewish or are descended from Jewish ancestors actually practice the Jewish religion and share the same cultural traditions. It is important to recognize and respect this diversity when answering questions about Jewish identity. Answer: C. Only some are Jews. Therefore, the correct multiple choice label (just the letter) to the task is below. Answer: C Figure 69: A complete running example of TruthfulQA. You will be provided instructions for completing a task followed by a task to complete. Instructions: 1. [noitemsep,topsep=0pt] 2. Understand the structure of the dataset: The dataset consists of news articles and their corresponding one-sentence summaries. The 'document' feature represents the input news article, and the'summary' feature represents the one-sentence summary of the article. 3. Familiarize yourself with the purpose of the dataset: The goal is to create a short, one-sentence summary that answers the question What is the article about?*. 4. Read the entire article carefully: Make sure to understand the main points of the article. The articles cover a wide variety of domains such as News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment, and Arts. 5. Generate a one-sentence summary: After understanding the article, generate a one-sentence summary that captures the main point of the article. The summary should be concise and clear, and it should accurately represent the content of the article. 6. Check your summary: Make sure your summary accurately reflects the main point of the article. It should not include any personal opinions or interpretations, and it should not introduce any new information that was not present in the article. 7. Practice with different articles: The more you practice, the better you will become at generating accurate and concise summaries. Try to practice with articles from different domains to improve your summarization skills. 8. Remember, the goal is not to generate the longest or most detailed summary, but to capture the main point of the article in a single sentence. 9.# Article: Head coach Stuart Lancaster's World Cup preparations suffered a blow as for the first 70 minutes a largely first-choice KV struggled to deal with French power. Two late tries flattered the visitors, who have one game left before launching their World Cup campaign against Fiji on 18 September. \"We gave away penalties and our discipline was shocking,\" said Robshaw. \"Whether it was rust, or nerves, it wasn't good enough. Credit to France, they put us under pressure and made us make mistakes. \"We gave too many penalties away, but in the second half we came out and played well but couldn't quite get over the line in the end,\" he told Sky Sports. \"We can't give teams like France and other quality sides head starts like we did. \"We'll look long and hard at ourselves, because we let ourselves down in the first half. We played well in phases but you can't wait for 40 minutes to do that,\" Late tries from Danny Cipriani and Jonathan Joseph made it close on the scoreboard but Lancaster was left with much to ponder by a disappointing team display in the Stade de France. Media playback is not supported on this device the head coach, who must announce this final squad by 31 August. \"We've got to get our discipline at the breakdown - we can't give France easy position like we did, We need to improve on that, because all the little mistakes add up. \"The bench made a difference. It upped the energy levels and we scored some good tries. I want\" it gloss over what went on before that, because it was too little too late. \"There are a few players who have given the food for thought, those guys who came on and gave us the energy we needed and made a difference. \"I need to have a sleep on this game and think about my final squad. We're two weeks away from playing Ireland and four weeks away from Fiji in the World Cup and we'll expect a reaction next time.\" England host Ireland in their final World Cup warm-up game at Twickenham on 5 September. Fly-half Ford told Sky Sports:\" I thought we might snatch that at the end but we had hardly any ball in the first half and gave away too many penalties. We played some good stuff in the second-half. \"In the first half a lot of our undoing was down to ourselves. We just weren't good enough in that first half and there's no excuse for that. We let them build up the score and that made it hard for us. \"It was frustrating and we had to think of ways to adapt and that was constantly going through our minds. We tried to get ourselves out of a hole. \"We've got to turn up against Ireland now and make sure that we win. Our basics have got to be world class.\" Follow the instructions to generate an explanation that reasons towards the correct answer to the task above. End the explanation with the correct answer. [/INST] Explanation: The article discusses England's rugby team's preparation for the World Cup, specifically their match against France. The team's performance was disappointing, with poor discipline and mistakes leading to a loss. The article highlights the struggles of the team's first-choice KV and the impact of late tries from Danny Cipriani and Jonathan Joseph. The head coach, Stuart Lancaster, expresses his disappointment and emphasizes the need for improvement in discipline and energy levels. The article also mentions the team's upcoming match against Ireland and their final World Cup warm-up game. Correct answer: The article is about England's rugby team's preparation for the World Cup and their disappointing performance against France. Therefore, the answer to the task is below. Give the answer in the shortest form possible that will still be correct. Summarize the above article in 1 sentence. England's rugby team struggled in their World Cup preparation match against France, with poor discipline and mistakes leading to a loss, and head coach Stuart Lancaster emphasized the need for improvement. Figure 70: A complete running example of XSUM."
    }
  ]
}