{
  "title": "Modality-invariant and Specific Prompting for Multimodal Human Perception Understanding",
  "authors": [
    "Hao Sun",
    "Ziwei Niu",
    "Xinyao Yu",
    "Jiaqing Liu",
    "Yen-Wei Chen",
    "Lanfen Lin"
  ],
  "abstract": "\n Understanding human perceptions presents a formidable multimodal challenge for computers, encompassing aspects such as sentiment tendencies and sense of humor. While various methods have recently been introduced to extract modality-invariant and specific information from diverse modalities, with the goal of enhancing the efficacy of multimodal learning, few works emphasize this aspect in large language models. In this paper, we introduce a novel multimodal prompt strategy tailored for tuning large language models. Our method assesses the correlation among different modalities and isolates the modality-invariant and specific components, which are then utilized for prompt tuning. This approach enables large language models to efficiently and effectively assimilate information from various modalities. Furthermore, our strategy is designed with scalability in mind, allowing the integration of features from any modality into pretrained large language models. Experimental results on public datasets demonstrate that our proposed method significantly improves performance compared to previous methods. \n",
  "references": [
    {
      "id": null,
      "title": "Modality-invariant and Specific Prompting for Multimodal Human Perception Understanding",
      "authors": [
        "Hao Sun",
        "Ziwei Niu",
        "Xinyao Yu",
        "Jiaqing Liu",
        "Yen-Wei Chen",
        "Lanfen Lin"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
      "authors": [
        "Martín Abadi",
        "Ashish Agarwal",
        "Paul Barham",
        "Eugene Brevdo",
        "Zhifeng Chen",
        "Craig Citro",
        "Greg S Corrado",
        "Andy Davis",
        "Jeffrey Dean",
        "Matthieu Devin"
      ],
      "year": "2016",
      "venue": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Role of chat gpt in public health",
      "authors": [
        "S Som",
        "Biswas"
      ],
      "year": "2023",
      "venue": "Annals of biomedical engineering",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Data distributional properties drive emergent few-shot learning in transformers",
      "authors": [
        "C Y Stephanie",
        "Adam Chan",
        "Santoro",
        "Jane X Andrew K Lampinen",
        "Aaditya Wang",
        "Pierre H Singh",
        "Jay Richemond",
        "Felix Mc-Clelland",
        "Hill"
      ],
      "year": "2022",
      "venue": "Data distributional properties drive emergent few-shot learning in transformers",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "Kyunghyun Cho",
        "Bart Merrienboer",
        "Caglar Gulcehre",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "EMNLP",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp)",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "Noé Tits",
        "Mathilde Brousmiche",
        "Stéphane Dupont"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "Paul Ekman",
        "Erika L Rosenberg"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Modelling emotional trajectories of individuals in an online chat",
      "authors": [
        "Maros Galik",
        "Stefan Rank"
      ],
      "year": "2012",
      "venue": "Multiagent System Technologies: 10th German Conference, MATES 2012",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Multimodal affective analysis using hierarchical attention strategy with word-level alignment",
      "authors": [
        "Yue Gu",
        "Kangning Yang",
        "Shiyu Fu",
        "Shuhong Chen",
        "Xinyu Li",
        "Ivan Marsic"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Alexander Gelbukh",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Soujanya Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 International Conference on Multimodal Interaction",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Soujanya Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Ur-funny: A multimodal language dataset for understanding humor",
      "authors": [
        "Md Kamrul Hasan",
        "Wasifur Rahman",
        "Amirali Bagher Zadeh",
        "Jianyuan Zhong",
        "Md Iftekhar Tanveer",
        "Louis-Philippe Morency",
        "Mohammed Ehsan Hoque"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2006",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Lora: Lowrank adaptation of large language models",
      "authors": [
        "J Edward",
        "Phillip Hu",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Multi-modal prompt learning",
      "authors": [
        "Muhammad Uzair Khattak",
        "Hanoona Rasheed",
        "Muhammad Maaz",
        "Salman Khan",
        "Fahad Shahbaz Khan",
        "Maple"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Grounding language models to images for multimodal inputs and outputs",
      "authors": [
        "Jing Yu Koh",
        "Ruslan Salakhutdinov",
        "Daniel Fried"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "authors": [
        "Lisa Xiang",
        "Percy Li",
        "Liang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Fewshot parameter-efficient fine-tuning is better and cheaper than in-context learning",
      "authors": [
        "Haokun Liu",
        "Derek Tam",
        "Mohammed Muqeeth",
        "Jay Mohta",
        "Tenghao Huang",
        "Mohit Bansal",
        "Colin A Raffel"
      ],
      "year": "1950",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Osan: A one-stage alignment network to unify multimodal alignment and unsupervised domain adaptation",
      "authors": [
        "Ye Liu",
        "Lingfeng Qiao",
        "Changchong Lu",
        "Di Yin",
        "Chen Lin",
        "Haoyuan Peng",
        "Bo Ren"
      ],
      "year": "2007",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Linearly mapping from image to text space",
      "authors": [
        "Jack Merullo",
        "Louis Castricato",
        "Carsten Eickhoff",
        "Ellie Pavlick"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach",
      "authors": [
        "Sunghyun Park",
        "Suk Han",
        "Moitreya Shim",
        "Kenji Chatterjee",
        "Louis-Philippe Sagae",
        "Morency"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-Physical Systems",
      "authors": [
        "Partha Pratim",
        "Ray"
      ],
      "year": "2023",
      "venue": "Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-Physical Systems",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Supervised feature selection via dependence estimation",
      "authors": [
        "Le Song",
        "Alex Smola",
        "Arthur Gretton",
        "Karsten M Borgwardt",
        "Justin Bedo"
      ],
      "year": "2007",
      "venue": "Proceedings of the 24th international conference on Machine learning",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Cubemlp: An mlp-based model for multimodal sentiment analysis and depression estimation",
      "authors": [
        "Hongyi Hao Sun",
        "Jiaqing Wang",
        "Yen-Wei Liu",
        "Lanfen Chen",
        "Lin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Modality-invariant temporal representation learning for multimodal sentiment classification",
      "authors": [
        "Jiaqing Hao Sun",
        "Yen-Wei Liu",
        "Lanfen Chen",
        "Lin"
      ],
      "year": "2023",
      "venue": "Information Fusion",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Zhongkai Sun",
        "Prathusha Sarma",
        "William Sethares",
        "Yingyu Liang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2007",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2008",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2006",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Re3: Generating longer stories with recursive reprompting and revision",
      "authors": [
        "Kevin Yang",
        "Yuandong Tian",
        "Nanyun Peng",
        "Dan Klein"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Pu Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Pu Liang",
        "Soujanya Poria",
        "Prateek Vij",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Pu Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Central moment discrepancy (cmd) for domain-invariant representation learning",
      "authors": [
        "Werner Zellinger",
        "Thomas Grubinger",
        "Edwin Lughofer",
        "Thomas Natschläger",
        "Susanne Saminger-Platz"
      ],
      "year": "2017",
      "venue": "Proceedings of the 5th International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "authors": [
        "Renrui Zhang",
        "Jiaming Han",
        "Aojun Zhou",
        "Xiangfei Hu",
        "Shilin Yan",
        "Pan Lu",
        "Hongsheng Li",
        "Peng Gao",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Opt: Open pre-trained transformer language models",
      "authors": [
        "Susan Zhang",
        "Stephen Roller",
        "Naman Goyal",
        "Mikel Artetxe",
        "Moya Chen",
        "Shuohui Chen",
        "Christopher Dewan",
        "Mona Diab",
        "Xian Li",
        "Xi Victoria Lin"
      ],
      "year": "2022",
      "venue": "Opt: Open pre-trained transformer language models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Modality-Invariant And Specific Prompting For Multimodal Human Perception Understanding",
      "text": "Hao Sun Zhejiang University Hangzhou, China sunhaoxx@zju.edu.cn Ziwei Niu Zhejiang University Hangzhou, China nzw@zju.edu.cn Xinyao Yu Zhejiang University Hangzhou, China xinyaoyu@zju.edu.cn Jiaqing Liu Ritsumeikan University Shiga, Japan liu-j@fc.ritsumei.ac.jp Yen-Wei Chen Ritsumeikan University Shiga, Japan chen@is.ritsumei.ac.jp Lanfen Lin Zhejiang University Hangzhou, China llf@zju.edu.cn"
    },
    {
      "title": "Abstract",
      "text": "Understanding human perceptions presents a formidable multimodal challenge for computers, encompassing aspects such as sentiment tendencies and sense of humor. While various methods have recently been introduced to extract modality-invariant and specific information from diverse modalities, with the goal of enhancing the efficacy of multimodal learning, few works emphasize this aspect in large language models. In this paper, we introduce a novel multimodal prompt strategy tailored for tuning large language models. Our method assesses the correlation among different modalities and isolates the modality-invariant and specific components, which are then utilized for prompt tuning. This approach enables large language models to efficiently and effectively assimilate information from various modalities. Furthermore, our strategy is designed with scalability in mind, allowing the integration of features from any modality into pretrained large language models. Experimental results on public datasets demonstrate that our proposed method significantly improves performance compared to previous methods."
    },
    {
      "title": "1 Introduction",
      "text": "Understanding human perceptions, such as sentiment tendency and humor sense, encompasses a variety of emerging applications, such as online chatting [9, 24] and dialogue systems [2]. The primary challenge in this field lies in effectively leveraging human psychological information across various modalities, including text, acoustics, and facial cues. In recent years, numerous approaches have emerged to decouple multimodal signals into modality-invariant and specific information for perception understanding [14, 32] (as illustrated in Figure 1(a)). For instance, Hazarika et al.[14] decomposed involved modalities into several pieces and utilized central moment discrepancy[39] to bring invariant parts closer while pushing specific parts further apart. These methods effectively capture human perception from multimodal features, remaining a key focus in multimodal Figure 1: The comparison of previous modality-invariant (\\(\\oplus\\)) and specific (\\(\\Delta\\)\\(\\bigtriangledown\\)) information learning(a) and our proposed multimodal prompting strategy(b), in which the modality-invariant and specific information are integrated. research. On another front, pretrained large language models (LLMs) have demonstrated excellent generalization abilities in various downstream tasks [3, 33]. However, due to their massive scale (e.g., 10 billion or 70 billion parameters), fine-tuning the entire model becomes challenging. Effectively prompting LLMs has thus become a prominent research topic in related fields [19, 20, 40]. While many methods have endowed LLMs with multimodal processing capabilities [17, 18], fewer studies focus on enabling LLMs to leverage modality-invariant and specific information. To address this gap, we propose a new multimodal prompting strategy with modality information disentangling for human perception estimation (as depicted in Figure 1(b)). Instead of directly extracting modality-invariant and specific information for final predictions, we emphasize these aspects in tunable prefix tokens. Our approach introduces the Parameter-Free Invariant and Specific prompt generation module (PaFIS) to generate tunable multimodal prompts with corresponding information from other modalities. Specifically, we calculate channel-wise correlations between text tokens and tokens from other modalities. We posit that less relevant parts contain more specific information, while more relevant parts bring more invariant information. By integrating these different parts into prompt tokens, LLMs can acquire the ability to process multimodal information. The entire process is parameter-free and does not introduce new learning parameters. Moreover, our strategy is designed with scalability, enabling the resolution of any modality features for LLM prompting. Experiments on four public datasets demonstrate that our approach significantly improves performance compared with other state-of-the-art methods. Through further analysis, our method also demonstrates the ability to handle modal absence cases. To the best of our knowledge, this is the first application that employs LLMs for modality information disentangling in human perception understanding. In summary, our contributions can be summarized as follows: * We propose a new multimodal prompting strategy, granting LLMs the ability to extract information from various modalities, such as facial and acoustic features. * We introduce PaFIS, a parameter-free modality-invariant and specific prompt generation module. Through PaFIS, LLMs can leverage modality-invariant and specific information from other modalities. * Our approach achieves state-of-the-art performance on four evaluated datasets, outperforming other methods by a large margin."
    },
    {
      "title": "2 Related Works",
      "text": "Our work encompasses two primary research areas: multimodal learning and LLM tuning. Multimodal learning seeks to refine or discover relationships between multiple modalities, while LLM tuning aims to uncover the knowledge implicit in language models with efficient parameter approaches."
    },
    {
      "title": "Multimodal Representation Learning",
      "text": "One research topic in multimodality focuses on learning effective representations from diverse modalities. Zadeh et al.[35] introduced the Tensor Fusion Network (TFN), employing outer product to blend multimodal features into a compact representation. Gu et al.[10] proposed a hierarchical attention network to analyze multimodal signals. More recently, researchers have utilized the self-attention mechanism [31] to fuse multimodal signals [6, 27, 30]. Liu et al.[21] applied unsupervised domain adaptation to align involved modalities in a common space, and Sun et al.[26] introduced a pure multilayer perceptron network for fusing multimodal features. Besides direct multimodal fusion, some researchers argue that modality-invariant and specific information are crucial for multimodal understanding. For instance, Hazarika et al.[14] used central moment discrepancy, while Yang et al.[32] employed Hilbert-Schmidt Independence Criterion [25] to explicitly extract invariant and specific information for final predictions. These works highlight the importance of invariant and specific information in multimodal representation learning. Building upon this, our approach disentangles modality information with adaptability tailored for the fine-tuning of LLMs."
    },
    {
      "title": "Llms For Language And Multimodal Learning",
      "text": "In recent years, LLMs have gained significant attention in natural language processing and computer vision communities, demonstrating prowess in downstream tasks such as long-form generation and summarization [3, 33]. Most LLMs are based on the Transformer [31] structure, boasting vast parameters and training data [29, 41]. Directly tuning LLMs is infeasible due to their scale. Various approaches propose prompting large-scale models with few or no parameters, such as LoRA [16], IA3 [20], prefix tuning [19], and LLMA-Adapter [40]. Researchers have also explored equipping LLMs with the ability to process information from other modalities. Merullo [22] found similarities in latent representations between LLMs and large vision models (LVM), correlated through linear mappings. Jing et al.[18] combined LLMs and LVMs with linear mappings for multimodal inputs and outputs. Khattak et al.[17] used a projection function to map textual tokens to visual prompts. Our approach also adopts prompting for multimodal processing but places greater emphasis on extracting invariant and specific information from any other modalities."
    },
    {
      "title": "3 Methodology",
      "text": "Our approach concerns with fine-tuning a LLM with different kind of modalities while keeping its parameters frozen. As we focus on perception understanding, we mainly involve textual(\\(t\\)), facial(visual, \\(v\\)), and acoustic(\\(a\\)) features. We try to let the LLM leverage the modality-invariant and specific information during prompting, so as to further excavate the capabilities obtained from large scale pretraining."
    },
    {
      "title": "Model Architecture And Pipeline",
      "text": "The framework overview is presented in Figure 2. Text inputs are first tokenized and embedded before being fed into the pretrained LLM. The hidden states for the \\(i\\)th layer are represented as: \\[h_{t}^{i}=[h_{t,1}^{i},h_{t,2}^{i},...,h_{t,l_{t}}^{i}]\\in\\mathbb{R}^{l_{t} \\times d_{t}}, \\tag{1}\\] where \\(d_{t}\\) is the embedding dimension, and \\(l_{t}\\) is the number of tokens in the textual input. Simultaneously, facial and acoustic features undergo context modeling through respective Transformer encoders [31]. The encoded states for the \\(i\\)th layer are expressed as: \\[h_{m}^{i}=[h_{m,1}^{i},h_{m,2}^{i},...,h_{m,l_{m}}^{i}]\\in\\mathbb{R}^{l_{m} \\times d_{m}}, \\tag{2}\\] where \\(m\\in\\{a,v\\}\\), \\(l_{m}\\) is the sequential length of modality \\(m\\), and \\(d_{m}(d_{m}<d_{t})\\) is the corresponding feature dimension. To generate multimodal prompts for the \\(i\\)th layer of the LLM, we introduce a parameter-free invariant and specific prompt generation module (PaFIS): \\[p^{i}=\\text{PaFIS}(h_{t}^{i},h_{a}^{i},h_{v}^{i})\\in\\mathbb{R}^{l_{p}\\times d_{ t}}, \\tag{3}\\] where \\(p^{i}\\) is the prompt for the \\(i\\)th layer, and \\(l_{p}\\) is the length of prompt tokens. Subsequently, we prefix the prompt tokens to the textual hidden states for the \\(i\\)th layer of the LLM: \\[h_{t}^{{}^{\\prime}i}=[p_{1}^{i},...,p_{l_{p}}^{i};h_{t,1}^{i},...,h_{t,l_{t}}^ {i}]\\in\\mathbb{R}^{(l_{p}+l_{t})\\times d_{t}}. \\tag{4}\\] During processing, we maintain frozen LLM parameters and exclusively train the facial and acoustic Transformers for context modeling."
    },
    {
      "title": "Parameter-Free Invariant And Specific Prompt Generation(Pafis)",
      "text": "The PaFIS module aims to generate learnable prompts where modality-invariant and specific information are emphasized. In the \\(i\\)th layer of the LLM, the PaFIS module takes modality hidden states (\\(h_{t}^{i}\\), \\(h_{a}^{i}\\), \\(h_{v}^{i}\\)) as inputs and outputs multimodal prompts \\(p^{i}\\). For each PaFIS module, we first introduce the learnable prompts \\(\\tilde{p}^{i}\\in\\mathbb{R}^{l_{p}\\times d_{t}}\\), which are not only used to tune the LLM but also serve as containers for modality-invariant and specific information. **Aligned Cases.** When acoustic or visual features are strictly aligned with textual tokens (\\(l_{m}=l_{t}=l\\)), we calculate the channel-wise correlation coefficients1 between \\(h_{m}^{i}\\) and distinct channels of \\(h_{t}^{i}\\): Footnote 1: The Pearson Correlation Coefficient is employed in our work. \\[K=\\text{Corr}(h_{m}^{i},h_{t}^{i}[j:j+d_{m}])\\in \\mathbb{R}^{l\\times(d_{t}-d_{m})}, \\tag{5}\\] \\[\\text{for }j\\in [0,\\;d_{t}-d_{m}],\\] where \\(h_{t}^{i}[j:j+d_{m}]\\in\\mathbb{R}^{l\\times d_{m}}\\) is the sub-channels of \\(h_{t}\\) used to calculate coefficients with \\(h_{m}^{i}\\). Next, we identify Figure 2: The pipeline of our proposed method. The whole LLM is frozen while the facial and acoustic features are context modeled by respective transformer encoders. In each prompting layer \\(i\\), textual(\\(h_{t}^{i}\\)), facial(\\(h_{v}^{i}\\)) and acoustic(\\(h_{a}^{i}\\)) features are fed to Parameter-free invariant and specific prompt generation module(PaFIS). In PaFIS, the modality-invariant and specific information are integrated into learnable tokens \\(p^{i}\\). the distinct sub-channels of \\(h_{t}^{i}\\) with the highest correlation: \\[\\begin{split} k_{m}^{max}&=\\text{argmax}(K,\\text{axis} =1)\\in\\mathbb{N}^{l},\\\\ c_{m}^{max}&=[k_{m}^{max}[j]:k_{m}^{max}[j]+d_{m}] \\in\\mathbb{N}^{l\\times d_{m}},j\\in[0,l],\\\\ h_{t}^{i,inv}&=h_{t}^{i}[c_{m}^{max}]\\in\\mathbb{R}^{l \\times d_{m}}.\\end{split} \\tag{6}\\] Similarly, we identify those with the lowest correlation: \\[\\begin{split} k_{m}^{min}&=\\text{argmin}(K,\\text{axis }=1)\\in\\mathbb{N}^{l},\\\\ c_{m}^{min}&=[k_{m}^{min}[j]:k_{m}^{min}[j]+d_{m}] \\in\\mathbb{N}^{l\\times d_{m}},j\\in[0,l],\\\\ h_{t}^{i,spe}&=h_{t}^{i}[c_{m}^{min}]\\in\\mathbb{R}^{l \\times d_{m}}.\\end{split} \\tag{7}\\] We posit that \\(h_{t}^{i,inv}\\) channels (of high correlation with \\(h_{m}^{i}\\)) bring more modality-invariant information, while \\(h_{t}^{i,spe}\\) channels (of low correlation with \\(h_{m}^{i}\\)) contain more modality-specific information. To enable the LLM to leverage this information from other modalities, we integrate them into the learnable prompts: \\[\\begin{split} p^{i}[c_{m}^{max}]&=\\tilde{p}^{i}[c_{ m}^{max}]+(h_{m}^{i}+h_{t}^{i,inv}),\\\\ p^{i}[c_{m}^{min}]&=\\tilde{p}^{i}[c_{m}^{min}]+(h_ {m}^{i}-h_{t}^{i,spe}).\\end{split} \\tag{8}\\] In Equation 8, modality-invariant information(between modality \\(t\\) and \\(m\\)) is emphasized by \\((h_{m}^{i}+h_{t}^{i,inv})\\) while specific information(from modality \\(m\\)) is emphasized by \\((h_{m}^{i}-h_{t}^{i,spe})\\). **Unaligned Cases.** In cases where acoustic or facial features are not aligned with textual tokens, we average-pool the hidden states on the temporal level before calculating the correlation: \\[\\begin{split} K=\\text{Corr}(\\bar{h}_{m}^{i},\\bar{h}_{t}^{i}[j:j +d_{m}])\\in\\mathbb{R}^{(d_{t}-d_{m})},\\\\ \\text{for }j\\in[0,\\;d_{t}-d_{m}].\\end{split} \\tag{9}\\] where \\(\\bar{h}_{m}^{i}\\) and \\(\\bar{h}_{t}^{i}\\) are temporally pooled features. Correspondingly, the sub-channels of \\(\\bar{h}_{t}^{i}\\) with the highest correlation are calculated by: \\[\\begin{split} k_{m}^{max}&=argmax(K)\\in\\mathbb{N}, \\\\ c_{m}^{max}&=[k_{m}^{max}:k_{m}^{max}+d_{m}]\\in \\mathbb{N}^{d_{m}},\\\\ \\bar{h}_{t}^{i,inv}&=\\bar{h}_{t}^{i}[c_{m}^{max}] \\in\\mathbb{R}^{d_{m}},\\end{split} \\tag{10}\\] The same is true for the lowest correlated channels \\(\\bar{h}_{t}^{i,spe}\\). Likewise, modality-invariant and specific information will be emphasized in learnable prompts: \\[\\begin{split} p^{i}[j][c_{m}^{max}]&=\\tilde{p}^{i}[j ][c_{m}^{max}]+(\\bar{h}_{m}^{i}+\\bar{h}_{t}^{i,inv}),\\\\ p^{i}[j][c_{m}^{min}]&=\\tilde{p}^{i}[j][c_{m}^{min} ]+(\\bar{h}_{m}^{i}-\\bar{h}_{t}^{i,spe}),\\\\ \\text{for }j\\in[0,\\;l_{t}].\\end{split} \\tag{11}\\]"
    },
    {
      "title": "Training Target",
      "text": "Our framework addresses two types of tasks: regression and binary classification. However, LLMs are typically designed and pretrained with language generation tasks. Therefore, we append a fully-connected layer after the last token hidden state \\(h_{t,l_{t}}^{L}\\) in the last layer for predictions. For regression tasks, we utilize the rooted mean square error as the loss function: \\[\\mathcal{L}=\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_{n}-\\hat{y}_{n})^{2}}, \\tag{12}\\] where \\(N\\) is the number of samples in a training batch, \\(y_{n}\\) is the ground truth, and \\(\\hat{y}_{n}\\) is the prediction. For classification tasks, the binary cross-entropy loss is used: \\[\\mathcal{L}=\\frac{1}{N}\\sum_{i=1}^{N}[y_{n}\\log\\hat{y}_{n}+(1-y_{n})\\log(1-\\hat {y}_{n})]. \\tag{13}\\]"
    },
    {
      "title": "4 Experiments",
      "text": ""
    },
    {
      "title": "Datasets Evaluated",
      "text": "To assess the effectiveness of our proposed method, we conducted experiments on four publicly available datasets: MOSI [34], MOSEI [38], URFunny [13], and POM [23]. Each of these datasets provides textual, facial, and acoustic modalities. **MOSI & MOSEI:** These datasets consist of utterances gathered from a public social platform. MOSI comprises 1283 training utterances, 229 validation utterances, and 686 testing utterances. In MOSEI, there are 16315 training samples, 1817 validation samples, and 4654 testing samples. Labels range from \\([-3,+3]\\), where -3 and +3 denote the most negative and positive sentiments, respectively. Although we approach the task as a regression, we can also provide 2-class and 7-class metrics by rounding the labels. **URFunny:** This dataset is designed for humor detection in video clips, with clips sourced from the Internet and annotated as 0 or 1 based on the presence of humor punchlines. It consists of 10598 training samples, 2626 validation samples, and 3290 testing samples. **POM:** The POM dataset includes 600 training samples, 100 validation samples, and 203 testing samples. Each sample offers 18 human perceptions, covering sentiment, confidence, passion, voice pleasantness, dominance, credibility, vividness, expertise, entertainment, reserve, trust, laziness, relaxation, extroversion, thoroughness, nervousness, humor, and persuasion. All labels fall within the range of \\([1,7]\\), where a higher value indicates a stronger tendency."
    },
    {
      "title": "Experimental Settings",
      "text": "In our experiments, we employed the popular LLaMA2-7B [29] as the foundational model, which stands as one [MISSING_PAGE_FAIL:5] an hour when training on the MOSI dataset. 2 Footnote 2: The codes for our method will be made public after publication."
    },
    {
      "title": "5 Results And Analysis",
      "text": "Our results, along with comparisons to other approaches, are presented in Table 1, Table 2, and Table 3. We benchmark our method against current state-of-the-art approaches, including Tensor Fusion Network [35], MISA [14], and OSAN [21]. As evident from the tables, our method significantly outperforms the current state-of-the-art approaches. Specifically, we achieve a Mean Absolute Error (MAE) of 0.619 on the MOSI dataset and 0.501 on the MOSI dataset for sentiment tendency prediction. In humor detection, we attain an accuracy of 74.2 on the URFunny dataset. Furthermore, our method achieves state-of-the-art performance across all 18 tasks on the POM dataset."
    },
    {
      "title": "Ablation And Modality Robustness",
      "text": "To further assess the effectiveness of our approach, we conducted ablation studies on the MOSI and URFunny datasets. Our focus was on studying the impact of the PaFIS module and the involved modalities. In scenarios where the PaFIS module is not employed, the modality features \\(h_{a}^{i}\\) and \\(h_{v}^{i}\\) are directly added to the textual hidden states \\(h_{t}^{i}\\) at arbitrary positions, implying that modality-invariant and specific information is not emphasized. The results are presented in Table 4. The observed performance drop without the PaFIS module emphasizes the crucial role of modality-invariant and specific information in multimodal learning. Through the metrics, we observe that the enhancement in our performance is not solely attributed to LLM; however, the contributions of PaFIS and other modalities should not be underestimated. Furthermore, we explored the significance of different modalities in human perception estimation. As evident from the results, both acoustic and facial information (\\(h_{a}\\), \\(h_{v}\\)) play crucial roles in perceptual understanding. However, acoustic features tend to contribute more to the predictions than facial features. Additionally, we experimented with fine-tuning the LLM with full modalities but only utilizing texts during inference (as illustrated in Figure 3). Surprisingly, the performance (shown in Figure 4 with Test\\({}_{a}\\)/Test\\({}_{v}\\) ablation) remained superior to previous work, as illustrated in Table 1 and Table 3. This indicates the robustness of our proposed multimodal prompting strategy in scenarios where modalities are absent, which is common in real-time applications."
    },
    {
      "title": "Cross Dataset Evaluation",
      "text": "As some datasets provide the same kind of acoustic and facial features, we conducted cross-dataset evaluation experiments. Two models were trained on the MOSEI dataset: model1 was trained with COVAREP [5] (\\(a\\)) and Facet [7] (\\(v\\)) features, while model2 was trained with COVAREP [5] (\\(a\\)) and OpenSMILE [8] (\\(v\\)) features. After training, the models were directly tested on the MOSI and POM datasets, which provide the same type of features as the two models above, respectively. As indicated in Table 5, the results remained competitive with previous approaches. This suggests that our proposed approach facilitates generalization capabilities in perception understanding."
    },
    {
      "title": "Impact Of Prompt Scales",
      "text": "As the LLM is frozen when performing multimodal prompting, the prompts play a crucial role in the final performance. Therefore, we studied the influence of prompt scales, including prompt length and prompt depth. **Prompt Length:** Figure 4(a) illustrates the effect of prompt length on our proposed method. The experiments were conducted on the MOSI and URFunny datasets. \\begin{table} \\begin{tabular}{c|c c c} \\hline \\hline \\multirow{2}{*}{Models} & \\multicolumn{3}{c}{URFunny} \\\\ & Acc-2/F1(\\(\\uparrow\\)) & Pre(\\(\\uparrow\\)) & Rec(\\(\\uparrow\\)) \\\\ \\hline TFN[35]\\({}_{\\Diamond}\\) & 68.5/68.6 & 67.9 & 61.1 \\\\ MuIT[30]\\({}_{\\Diamond}\\) & 70.5/70.4 & 66.4 & 70.0 \\\\ MISA[30]\\({}_{\\Diamond}\\) & 70.6/70.0 & 68.9 & 73.5* \\\\ BBFN[11]\\({}_{\\Diamond}\\) & 71.6/72.0* & 69.2* & 72.8 \\\\ DRL[32] & 71.8/ - & - & - \\\\ \\hline Ours & **74.2/74.2** & **71.8** & **77.7** \\\\ \\(\\Delta_{SOTA}(\\uparrow)\\) & 2.6/62.2\\% & 2.6\\% & 4.2\\% \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: The results of our method on URFunny dataset. \\(\\uparrow\\): the higher the better. _Pre_ means the precision and _Rec_ means the recall. \\(\\Diamond\\): some of the metrics are not provided by presented papers, we rerun the methods based on public codes. *: SOTA performance. Figure 3: The illustration of modality absence inference(the case in figure is for sentiment analysis). The model is tuned in with multimodal inputs(up) but without multimodal signals for inference(down). Longer prompt lengths indicate stronger fitting ability but come with a greater computational burden and a risk of overfitting. We observed that the best results were achieved with a prompt length (\\(l_{p}\\)) set between 8 and 10. **Prompt Depth:** In Figure 4(b), we showcase the effect of different prompt depths on predictions. In LLM, we consistently prompt the last \\(D\\) layers, as deeper features contain richer semantic information and are more conducive to multimodal learning. The experiments were conducted on the MOSI dataset. Results demonstrate that performance is optimal when the prompt depth is set to 3. Any increase or decrease in the depth of prompts results in reduced performance. An increase in prompt depth leads to overfitting, while shallower prompts can lead to underfitting. We present not only the performance on the test set but also the results on the training set. Interestingly, the model fails to fit on the training set when prompts are deeper than 15 (\\(D>15\\)). We attribute this to the scale of data in downstream tasks. A surplus of tunable parameters relative to the data scale eventually hampers model fitting."
    },
    {
      "title": "Selection Of Acoustic & Facial Encoders",
      "text": "As evident from the ablation studies presented in Table 4, acoustic and facial features play a significant role in perception understanding. Consequently, we investigated the impact of different encoders for modality features other than \\begin{table} \\begin{tabular}{c|c c c c} \\hline \\hline Encoders & MAE(\\(\\downarrow\\)) & Corr(\\(\\uparrow\\)) & Acc-2/F1(\\(\\uparrow\\)) & Acc-7(\\(\\uparrow\\)) \\\\ \\hline Convolution & 0.710 & 0.839 & 81.8/81.7 & 46.7 \\\\ LSTM & 0.625 & 0.851 & 85.2/85.1 & 47.8 \\\\ GRU & 0.622 & 0.849 & 85.0/85.1 & 48.0 \\\\ \\hline Transformer & **0.619** & **0.860** & **86.5/86.5** & **49.3** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: The impact of different encoders for acoustic and facial encoders. The experiments are conducted on MOSI dataset. \\begin{table} \\begin{tabular}{c c c|c c|c c c|c c} \\hline \\hline \\multicolumn{6}{c|}{Ablations} & \\multicolumn{5}{c|}{MOSI} & \\multicolumn{2}{c}{URFunny} \\\\ \\hline PaFIS & \\(h_{a}\\) & \\(h_{v}\\) & Testa & Test\\(v\\) & MAE(\\(\\downarrow\\)) & Corr(\\(\\uparrow\\)) & Acc-2/F1(\\(\\uparrow\\)) & Acc-7(\\(\\uparrow\\)) & Acc-2(\\(\\uparrow\\)) & F1(\\(\\uparrow\\)) \\\\ \\hline & & & & & 0.689 & 0.840 & 82.2/82.2 & 46.9 & 72.1 & 72.1 \\\\ ✓ & ✓ & & ✓ & & 0.639 & 0.844 & 83.7/83.7 & 48.2 & 73.0 & 72.9 \\\\ ✓ & & ✓ & & ✓ & 0.644 & 0.841 & 82.2/83.9 & 47.1 & 71.1 & 71.1 \\\\ & ✓ & ✓ & ✓ & ✓ & 0.630 & 0.843 & 84.4/84.3 & 47.9 & 72.9 & 71.4 \\\\ \\hline ✓ & ✓ & ✓ & & & 0.655 & 0.820 & 82.8/82.6 & 45.4 & 71.8 & 71.8 \\\\ ✓ & ✓ & ✓ & ✓ & & 0.638 & 0.839 & 83.4/83.5 & 46.9 & 73.4 & 73.3 \\\\ ✓ & ✓ & ✓ & & ✓ & 0.641 & 0.830 & 83.0/83.0 & 46.1 & 72.6 & 72.5 \\\\ ✓ & ✓ & ✓ & ✓ & ✓ & **0.619** & **0.860** & **86.5/86.5** & **49.3** & **74.2** & **74.2** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: The ablation studies on MOSI and URFunny dataset. \\(h_{a}\\) and \\(h_{v}\\) means whether we use the acoustic or facial modality features. PaFIS means whether the modality-invariant and specific information are addressed during tuning. Testa or Testv mean whether to use acoustic or visual features during inference and testing. Figure 4: The impact of prompt scales to the final predictions. (a) The impact of prompt length. (b) The impact of prompt depth(\\(D\\)). \\begin{table} \\begin{tabular}{c|c c c c} \\hline \\hline Methods & MAE(\\(\\downarrow\\)) & Corr(\\(\\uparrow\\)) & Acc-2/F1(\\(\\uparrow\\)) & Acc-7(\\(\\uparrow\\)) \\\\ \\hline MMIM[12] & **0.700** & 0.800 & **84.1/84.0** & **46.6** \\\\ OSAAN[21] & 0.713 & 0.801 & 83.1/83.0 & 46.3 \\\\ \\hline Ours & 0.719 & **0.802** & 83.2/83.3 & 46.4 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: The results of cross dataset evaluation on MOSI and POM dataset. On POM dataset, we only evaluate on the sentiment analysis task. texts. Four types of encoders were employed: LSTM [15], GRU [4], convolution, and Transformer encoders [31]. The findings in Table 6 indicate that Transformer yields the best results, while LSTM and GRU exhibit slightly inferior performance. Interestingly, the convolution encoders produce the poorest results, even worse than the text-only model in Table 4. This suggests that convolution not only fails to extract effective information but also, at times, has a negative impact on LLM."
    },
    {
      "title": "6 Conclusion",
      "text": "This paper introduced a novel multimodal prompting strategy for Large Language Models (LLMs), leveraging the Parameter-Free Invariant and Specific prompt generation module (PaFIS). The PaFIS module generates learnable prompts that integrate modality-invariant and specific information, enabling LLMs to effectively utilize information from various modalities. Experimental results on four public datasets, including MOSI, MOSEI, URFunny, and POM, showcased the effectiveness of our proposed method. Despite these positive outcomes, our approach is susceptible to overfitting due to the potent representation capability of LLMs. Future research will focus on refining strategies to seamlessly integrate multimodal signals into LLMs while addressing overfitting concerns."
    },
    {
      "title": "Acknowledgments",
      "text": "The acknowledgments and sponsors are hidden for double-blind review."
    },
    {
      "title": "References",
      "text": "* [1] Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. _arXiv preprint arXiv:1603.04467_, 2016. * [2] Som S Biswas. Role of chat gpt in public health. _Annals of biomedical engineering_, 51(5):868-869, 2023. * [3] Stephanie CY Chan, Adam Santoro, Andrew K Lampinen, Jane X Wang, Aaditya Singh, Pierre H Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent few-shot learning in transformers. _arXiv preprint arXiv:2205.05055_, 2022. * [4] Kyunghyun Cho, Bart Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In _EMNLP_, 2014. * [5] Gilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio, and Stefan Scherer. Covarep--a collaborative voice analysis repository for speech technologies. In _2014 ieee international conference on acoustics, speech and signal processing (icassp)_, pages 960-964. IEEE, 2014. * [6] Jean-Benoit Delbrouck, Noe Tits, Mathilde Brousmiche, and Stephane Dupont. A transformer-based joint-encoding for emotion recognition and sentiment analysis. In _Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)_, pages 1-7, 2020. * [7] Paul Ekman and Erika L Rosenberg. _What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)_. Oxford University Press, USA, 1997. * [8] Florian Eyben, Martin Wollmer, and Bjorn Schuller. Opensmile: the munich versatile and fast open-source audio feature extractor. In _Proceedings of the 18th ACM international conference on Multimedia_, pages 1459-1462, 2010. * [9] Maros Galik and Stefan Rank. Modelling emotional trajectories of individuals in an online chat. In _Multiagent System Technologies: 10th German Conference, MATES 2012, Trier, Germany, October 10-12, 2012. Proceedings 10_, pages 96-105. Springer, 2012. * [10] Yue Gu, Kangning Yang, Shiyu Fu, Shuhong Chen, Xinyu Li, and Ivan Marsic. Multimodal affective analysis using hierarchical attention strategy with word-level alignment. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2225-2235, Melbourne, Australia, 2018. Association for Computational Linguistics. * [11] Wei Han, Hui Chen, Alexander Gelbukh, Amir Zadeh, Louis-philippe Morency, and Soujanya Poria. Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis. In _Proceedings of the 2021 International Conference on Multimodal Interaction_, pages 6-15, 2021. * [12] Wei Han, Hui Chen, and Soujanya Poria. Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 9180-9192, 2021. * [13] Md Kamrul Hasan, Wasifur Rahman, AmirAli Bagher Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer, Louis-Philippe Morency, and Mohammed Ehsan Hoque. Ur-funny: A multimodal language dataset for understanding humor. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2046-2056, 2019. * [14] Devamanyu Hazarika, Roger Zimmermann, and Soujanya Poria. Misa: Modality-invariant and-specific representations for multimodal sentiment analysis. In _Proceedings of the 28th ACM International Conference on Multimedia_, pages 1122-1131, 2020. * [15] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997. * [16] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2021. * [17] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19113-19122, 2023. * [18] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal inputs and outputs. In _Proceedings of the 40th International Conference on Machine Learning_. JMLR.org, 2023. * [19] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 4582-4597, 2021. * [20] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. _Advances in Neural Information Processing Systems_, 35:1950-1965, 2022. * [21] Ye Liu, Lingfeng Qiao, Changchong Lu, Di Yin, Chen Lin, Haoyuan Peng, and Bo Ren. Osan: A one-stage alignment network to unify multimodal alignment and unsupervised domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3551-3560, 2023. * [22] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. Linearly mapping from image to text space. In _The Eleventh International Conference on Learning Representations_, 2022. * [23] Sunghyun Park, Han Suk Shim, Moitreya Chatterjee, Kenji Sagae, and Louis-Philippe Morency. Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach. In _Proceedings of the 16th International Conference on Multimodal Interaction_, pages 50-57, 2014. * [24] Partha Pratim Ray. Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. _Internet of Things and Cyber-Physical Systems_, 2023. * [25] Le Song, Alex Smola, Arthur Gretton, Karsten M Borgwardt, and Justin Bedo. Supervised feature selection via dependence estimation. In _Proceedings of the 24th international conference on Machine learning_, pages 823-830, 2007. * [26] Hao Sun, Hongyi Wang, Jiaqing Liu, Yen-Wei Chen, and Lanfen Lin. Cubemlp: An mlp-based model for multimodal sentiment analysis and depression estimation. In _Proceedings of the 30th ACM International Conference on Multimedia_, pages 3722-3729, New York, NY, USA, 2022. Association for Computing Machinery. * [27] Hao Sun, Jiaqing Liu, Yen-Wei Chen, and Lanfen Lin. Modality-invariant temporal representation learning for multimodal sentiment classification. _Information Fusion_, 91:504-514, 2023. * [28] Zhongkai Sun, Prathusha Sarma, William Sethares, and Yingyu Liang. Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 8992-8999, 2020. * [29] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023. * [30] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. In _Proceedings of the conference. Association for Computational Linguistics. Meeting_, pages 6558-6569. NIH Public Access, 2019. * [31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems_, pages 5998-6008, 2017. * [32] Dingkang Yang, Shuai Huang, Haopeng Kuang, Yangtao Du, and Lihua Zhang. Disentangled representation learning for multimodal emotion recognition. In _Proceedings of the 30th ACM International Conference on Multimedia_, pages 1642-1651, 2022. * [33] Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with recursive reporting and revision. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 4393-4479, 2022. * [34] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages. _IEEE Intelligent Systems_, 31(6):82-88, 2016. * [35] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for multimodal sentiment analysis. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 1103-1114, 2017. * [36] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Memory fusion network for multi-view sequential learning. In _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence_, pages 5634-5641. AAAI Press, 2018. * [37] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, and Louis-Philippe Morency. Multi-attention recurrent network for human communication compensation. In _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence_, pages 5642-5649. AAAI Press, 2018. * [38] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2236-2246, 2018. * [39] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschlager, and Susanne Saminger-Platz. Centralmoment discrepancy (cmd) for domain-invariant representation learning. In _Proceedings of the 5th International Conference on Learning Representations_, 2017. * [40] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. _arXiv preprint arXiv:2303.16199_, 2023. * [41] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022."
    }
  ]
}