{
  "title": "Bootstrapping Cognitive Agents with a Large Language Model",
  "authors": [
    "Gavin Zhu",
    "Reid Simmons"
  ],
  "abstract": "\n Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune. On the other hand cognitive architectures have excellent interpretability and are flexible to update but require a lot of manual work to instantiate. In this work, we combine the best of both worlds: bootstrapping a cognitive-based model with the noisy knowledge encoded in large language models. Through an embodied agent doing kitchen tasks, we show that our proposed framework yields better efficiency compared to an agent based entirely on large language models. Our experiments indicate that large language models are a good source of information for cognitive architectures, and the cognitive architecture in turn can verify and update the knowledge of large language models to a specific domain. \n",
  "references": [
    {
      "id": null,
      "title": "Bootstrapping Cognitive Agents with a Large Language Model",
      "authors": [
        "Gavin Zhu",
        "Reid Simmons"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Do as i can, not as i say: Grounding language in robotic affordances",
      "authors": [
        "M Ahn",
        "A Brohan",
        "N Brown",
        "Y Chebotar",
        "O Cortes",
        "B David",
        "C Finn",
        "C Fu",
        "K Gopalakrishnan",
        "K Hausman"
      ],
      "year": "2022",
      "venue": "Do as i can, not as i say: Grounding language in robotic affordances",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "How can the human mind occur in the physical universe?",
      "authors": [
        "J R Anderson"
      ],
      "year": "2009",
      "venue": "How can the human mind occur in the physical universe?",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "On evaluation of embodied navigation agents",
      "authors": [
        "P Anderson",
        "A Chang",
        "D S Chaplot",
        "A Dosovitskiy",
        "S Gupta",
        "V Koltun",
        "J Kosecka",
        "J Malik",
        "R Mottaghi",
        "M Savva"
      ],
      "year": "2018",
      "venue": "On evaluation of embodied navigation agents",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "",
      "authors": [
        "S Andrew",
        "Y Karmesh",
        "C Alex",
        "B Vincent-Pierre",
        "G Aaron",
        "C Angel",
        "S Manolis",
        "K Zsolt",
        "B Dhruv"
      ],
      "year": "2022",
      "venue": "Habitat Rearrangement Challenge",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
      "authors": [
        "S Casper",
        "X Davies",
        "C Shi",
        "T K Gilbert",
        "J Scheurer",
        "J Rando",
        "R Freedman",
        "T Korbak",
        "D Lindner",
        "P Freire"
      ],
      "year": "2023",
      "venue": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Towards a unified agent with foundation models",
      "authors": [
        "N Di Palo",
        "A Byravan",
        "L Hasenclever",
        "M Wulfmeier",
        "N Heess",
        "M Riedmiller"
      ],
      "year": "2023",
      "venue": "Workshop on Reincarnating Reinforcement Learning at ICLR 2023",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Palm-e: An embodied multimodal language model",
      "authors": [
        "D Driess",
        "F Xia",
        "M S Sajjadi",
        "C Lynch",
        "A Chowdhery",
        "B Ichter",
        "A Wahid",
        "J Tompson",
        "Q Vuong",
        "T Yu"
      ],
      "year": "2023",
      "venue": "Palm-e: An embodied multimodal language model",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
      "authors": [
        "K Ellis",
        "C Wong",
        "M Nye",
        "M Sabl√©-Meyer",
        "L Morales",
        "L Hewitt",
        "L Cary",
        "A Solar-Lezama",
        "J B Tenenbaum"
      ],
      "year": "2021",
      "venue": "Proceedings of the 42nd acm sigplan international conference on programming language design and implementation",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Inferring a Cognitive Architecture from Multitask Neuroimaging Data: A Data-Driven Test of the Common Model of Cognition Using Granger Causality",
      "authors": [
        "H S Hake",
        "C Sibert",
        "A Stocco"
      ],
      "year": "2022",
      "venue": "Topics in Cognitive Science",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
      "authors": [
        "W Huang",
        "P Abbeel",
        "D Pathak",
        "I Mordatch"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Learning Hierarchical Symbolic Representations to Support Interactive Task Learning and Knowledge Transfer",
      "authors": [
        "J R Kirk",
        "J E Laird"
      ],
      "year": "2019",
      "venue": "IJCAI",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks",
      "authors": [
        "J R Kirk",
        "R E Wray",
        "P Lindes",
        "J E Laird"
      ],
      "year": "2023",
      "venue": "Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "An Interactive 3D Environment for Visual AI",
      "authors": [
        "E Kolve",
        "R Mottaghi",
        "W Han",
        "E Vanderbilt",
        "L Weihs",
        "A Herrasti",
        "D Gordon",
        "Y Zhu",
        "A Gupta",
        "A Farhadi"
      ],
      "year": "2017",
      "venue": "An Interactive 3D Environment for Visual AI",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "",
      "authors": [
        "J E Laird"
      ],
      "year": "2017",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Introduction to Soar",
      "authors": [
        "J E Laird"
      ],
      "year": "2022",
      "venue": "Introduction to Soar",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Interactive task learning",
      "authors": [
        "J E Laird",
        "K Gluck",
        "J Anderson",
        "K D Forbus",
        "O C Jenkins",
        "C Lebiere",
        "D Salvucci",
        "M Scheutz",
        "A Thomaz",
        "G Trafton"
      ],
      "year": "2017",
      "venue": "IEEE Intelligent Systems",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics",
      "authors": [
        "J E Laird",
        "C Lebiere",
        "P S Rosenbloom",
        "J Liang",
        "W Huang",
        "F Xia",
        "P Xu",
        "K Hausman",
        "B Ichter",
        "P Florence",
        "A Zeng"
      ],
      "year": "2017",
      "venue": "2023 IEEE International Conference on Robotics and Automation (ICRA)",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Improving Knowledge Extraction from LLMs for Robotic Task Learning through Agent Analysis",
      "authors": [
        "J R Lindes",
        "W Peter"
      ],
      "year": "2023",
      "venue": "Improving Knowledge Extraction from LLMs for Robotic Task Learning through Agent Analysis",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "LLM+ P: Empowering Large Language Models with Optimal Planning Proficiency",
      "authors": [
        "B Liu",
        "Y Jiang",
        "X Zhang",
        "Q Liu",
        "S Zhang",
        "J Biswas",
        "P Stone"
      ],
      "year": "2023",
      "venue": "LLM+ P: Empowering Large Language Models with Optimal Planning Proficiency",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Lost in the middle: How language models use long contexts",
      "authors": [
        "N F Liu",
        "K Lin",
        "J Hewitt",
        "A Paranjape",
        "M Bevilacqua",
        "F Petroni",
        "P Liang"
      ],
      "year": "2023",
      "venue": "Lost in the middle: How language models use long contexts",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Language models of code are few-shot commonsense learners",
      "authors": [
        "A Madaan",
        "S Zhou",
        "U Alon",
        "Y Yang",
        "G Neubig"
      ],
      "year": "2022",
      "venue": "Language models of code are few-shot commonsense learners",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "A survey of explainable reinforcement learning",
      "authors": [
        "S Milani",
        "N Topin",
        "M Veloso",
        "F Fang"
      ],
      "year": "2022",
      "venue": "A survey of explainable reinforcement learning",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "A Demonstration of Compositional, Hierarchical Interactive Task Learning",
      "authors": [
        "A Mininger",
        "J E Laird"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Unified theories of cognition",
      "authors": [
        "A Newell"
      ],
      "year": "1994",
      "venue": "Unified theories of cognition",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Demystifying GPT Self-Repair for Code Generation",
      "authors": [
        "T X Olausson",
        "J P Inala",
        "C Wang",
        "J Gao",
        "A Solar-Lezama",
        "J S Park",
        "J C O'brien",
        "C J Cai",
        "M R Morris",
        "P Liang",
        "M S Bernstein"
      ],
      "year": "2023",
      "venue": "Generative agents: Interactive simulacra of human behavior",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Tidee: Tidying up novel rooms using visuo-semantic commonsense priors",
      "authors": [
        "J Qiu",
        "M Xu",
        "W Han",
        "S Moon",
        "D Zhao",
        "G Sarch",
        "Z Fang",
        "A W Harley",
        "P Schydlo",
        "M J Tarr",
        "S Gupta",
        "K Fragkiadaki"
      ],
      "year": "2022",
      "venue": "Embodied Executable Policy Learning with Language-based Scene Summarization",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models",
      "authors": [
        "I Singh",
        "V Blukis",
        "A Mousavian",
        "A Goyal",
        "D Xu",
        "J Tremblay",
        "D Fox",
        "J Thomason",
        "A Garg"
      ],
      "year": "2023",
      "venue": "International Conference on Robotics and Automation",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Llm-planner: Few-shot grounded planning for embodied agents with large language models",
      "authors": [
        "C H Song",
        "J Wu",
        "C Washington",
        "B M Sadler",
        "W.-L Chao",
        "Y Su"
      ],
      "year": "2022",
      "venue": "Llm-planner: Few-shot grounded planning for embodied agents with large language models",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Reinforcement learning: An introduction",
      "authors": [
        "R S Sutton",
        "A G Barto"
      ],
      "year": "2018",
      "venue": "Reinforcement learning: An introduction",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Towards Modeling and Influencing the Dynamics of Human Learning",
      "authors": [
        "R Tian",
        "M Tomizuka",
        "A D Dragan",
        "A Bajcsy"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "A Simple Approach for Visual Room Rearrangement: 3D Mapping and Semantic Search",
      "authors": [
        "B Trabucco",
        "G A Sigurdsson",
        "R Piramuthu",
        "G S Sukhatme",
        "R Salakhutdinov"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "ChatGPT for Robotics: Design Principles and Model Abilities",
      "authors": [
        "S Vemprala",
        "R Bonatti",
        "A Bucker",
        "A Kapoor"
      ],
      "year": "2023",
      "venue": "ChatGPT for Robotics: Design Principles and Model Abilities",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
      "authors": [
        "G Wang",
        "Y Xie",
        "Y Jiang",
        "A Mandlekar",
        "C Xiao",
        "Y Zhu",
        "L Fan",
        "A Anandkumar"
      ],
      "year": "2023",
      "venue": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Chain-ofthought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q V Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Tidybot: Personalized robot assistance with large language models",
      "authors": [
        "J Wu",
        "R Antonova",
        "A Kan",
        "M Lepert",
        "A Zeng",
        "S Song",
        "J Bohg",
        "S Rusinkiewicz",
        "T Funkhouser"
      ],
      "year": "2023",
      "venue": "Tidybot: Personalized robot assistance with large language models",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Language Models Meet World Models: Embodied Experiences Enhance Language Models",
      "authors": [
        "J Xiang",
        "T Tao",
        "Y Gu",
        "T Shu",
        "Z Wang",
        "Z Yang",
        "Z Hu"
      ],
      "year": "2023",
      "venue": "Language Models Meet World Models: Embodied Experiences Enhance Language Models",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Translating natural language to planning goals with largelanguage models",
      "authors": [
        "Y Xie",
        "C Yu",
        "T Zhu",
        "J Bai",
        "Z Gong",
        "H Soh"
      ],
      "year": "2023",
      "venue": "Translating natural language to planning goals with largelanguage models",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "AmadeusGPT: a natural language interface for interactive animal behavioral analysis",
      "authors": [
        "S Ye",
        "J Lauer",
        "M Zhou",
        "A Mathis",
        "M W Mathis"
      ],
      "year": "2023",
      "venue": "AmadeusGPT: a natural language interface for interactive animal behavioral analysis",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Large Language Models as Zero-Shot Human Models for Human-Robot Interaction",
      "authors": [
        "B Zhang",
        "H Soh"
      ],
      "year": "2023",
      "venue": "Large Language Models as Zero-Shot Human Models for Human-Robot Interaction",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory",
      "authors": [
        "X Zhu",
        "Y Chen",
        "H Tian",
        "C Tao",
        "W Su",
        "C Yang",
        "G Huang",
        "B Li",
        "L Lu",
        "X Wang"
      ],
      "year": "2023",
      "venue": "Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": [
        "A Zou",
        "Z Wang",
        "J Z Kolter",
        "M Fredrikson"
      ],
      "year": "2023",
      "venue": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Bootstrapping Cognitive Agents With A Large Language Model",
      "text": "Feiyu (Gavin) Zhu, Reid Simmons Carnegie Mellon University feiyuz@andrew.cmu.edu, rsimmons@andrew.cmu.edu"
    },
    {
      "title": "Abstract",
      "text": "Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune. On the other hand cognitive architectures have excellent interpretability and are flexible to update but require a lot of manual work to instantiate. In this work, we combine the best of both worlds: bootstrapping a cognitive-based model with the noisy knowledge encoded in large language models. Through an embodied agent doing kitchen tasks, we show that our proposed framework yields better efficiency compared to an agent based entirely on large language models. Our experiments indicate that large language models are a good source of information for cognitive architectures, and the cognitive architecture in turn can verify and update the knowledge of large language models to a specific domain."
    },
    {
      "title": "Introduction",
      "text": "Large language models (LLM) such as GPT-4 (OpenAI 2023), have shown emerging capabilities after training on internet-scale text data with human feedback, and have been employed in robot planning [14], animal behavior analysis [23], human proxies [22], and many more. However, they have also been criticized for being susceptible to adversarial attacks [24], hallucination [15], and having diminishing returns for scaling (OpenAI 2023). Cognitive architectures are another approach in the pursuit of artificial general intelligence that attempt to unify all aspects of human cognition computationally [21]. Despite the variety of architectures developed, most of them share the same central components, consisting of declarative memory reflecting knowledge of the world, procedural memory dictating the agent's behavior given certain scenarios, and short-term working memory that assists reasoning and planning [16]. The procedural memory is represented by a set of production rules, each with a precondition and an effect. Agents operate in perceive-plan-act cycles, dynamically matching relevant features of the environment to the production rules and applying their effects. Unlike operators in symbolic planning, production rules do not represent alternative actions, but instead reflect different contextual knowledge [16]. These rules can be reinforced and modified throughout the agent's learning process. Despite some pioneering work on data-driven cognitive model creation [13], almost all previous work generate their initial set of production rules manually, limiting their application to simple environments such as blocks world or psychology experiments [15]. In this work we combine the two approaches in a complementary fashion (Figure 1). LLMs encode the common sense knowledge of the world [10] that can be used in place of human labor for constructing agents in the cognitive architecture. And the reasoning and learning capabilities in the cognitive architecture can identify and filter the noise in LLMs, while converting the knowledge in language to actionable productions of an embodied agent. This combined framework separates knowledge generation and knowledge application, and this modularity is the key to generalization. The LLM is responsible only for generating general knowledge, such as \"if the task is to find an Figure 1: Overview of agent framework. It is showing the agent executing the production of attending to a new sub-task of finding a tomato when the original task is to slice a tomato and the tomato is not in the gripper nor on the table. Dotted lines represent the information a production rule may condition on. Solid lines represent information flow. object, the agent should explore the places where that object is commonly stored\". Since such knowledge can be applied to almost all objects and environments, the LLM needs to generate these only once, and it is the role of the cognitive architecture to dynamically match the environment to the generated knowledge. This is significantly different from using LLMs to generate plans directly, as the plans are grounded to the specific instance of the task (e.g., finding a specific object in the specific environment), and are non-trivial to generalize to novel environments without re-generation. The contribution of this paper is threefold: 1) we propose an agent framework that combines LLMs with customized cognitive architecture, 2) we demonstrate how it can learn to perform various kitchen tasks from bootstrapping, and 3) we show that, when applied to new environments, it requires significantly fewer tokens than querying LLM for actions."
    },
    {
      "title": "Related Work",
      "text": ""
    },
    {
      "title": "Learning Through Program Synthesis",
      "text": "Interactive Task Learning (ITL) [10] aims at teaching robots new skills in a one-shot fashion. Previous work implements this in the SOAR cognitive architecture and has shown effective task and environment transferability in domains such as board games [13] and embodied agents [14]. To reduce the need for extensive human input, recent research explores using LLM as the knowledge source [11, 12], shifting human labor from specifying the goal conditions to answering yes/no questions. In contrast, our approach uses strategic prompting and self-reflection mechanisms to eliminate the need for human supervision. Our work shares some high-level ideas with DreamCoder [15], which learns to solve new problems by program generation and reflection. Instead of formulating it as an informed search problem, we accelerate this process by querying LLMs for their existing knowledge. Madaan et al. (2022) extract common sense knowledge from LLMs into code form similar to how we extract productions. But they only address the general task decomposition, not applying the information to an embodied agent."
    },
    {
      "title": "Large Language Model For Embodied Agents",
      "text": "Many studies have explored using LLMs to generate code that performs robotics tasks [19, 10, 12] and game environments [10], which is similar to the procedural memory in the cognitive architectures. In addition to script-based code, other works explored generating PDDL specifications [10, 11]. Unlike the situation-grounded code produced by these methods, our approach generates parameterized productions with learnable weights. This allows more generalization capabilities and choosing the best plan among multiple applicable plans. Others let LLMs select the action directly [13, 14] with the help of other auxiliary components such as affordance evaluation [1], memory stream [3], visual summarization [15], and knowledge base [16]. Some others explored multi-modal foundation models tailored for embodied agents [12, 10]. As LLMs are non-trivial to update from a single instance, using more explicit production systems in our approach enables persistent one-shot updates and more interpretability. As we will show in our experiments, relying on LLMs for every action is also not very cost-effective."
    },
    {
      "title": "Method",
      "text": ""
    },
    {
      "title": "Architecture Overview",
      "text": "Figure 1 illustrates the architecture and workflow of the agent. The agent has four main components. A world knowledge base that contains general knowledge, such as \"Toma-toes are commonly stored in the Fridge\". An environment knowledge that reflects what the agent knows about the environment from past observations, including both information about the agent itself (e.g., the gripper is empty) and about the external world (e.g., the table is clear). These two components form the declarative memories of the agent. Another essential component is the procedural memory that contains all the production rules. In our work, however, we integrate the working memory into each production by exploiting the Python class structure, so there is no centralized working memory. And finally inspired by the goal module of ACT-R [1] and the impasse mechanism of SOAR [10], the agent manages a stack of tasks. At each time step, the agent searches in its procedural memory for any applicable production rule, considering the current task and environment knowledge. If there is no production applicable, the agent will summarize the current knowledge and query an LLM for both an action suggestion, and a corresponding production rule, such that the agent knows what to do in similar scenarios in the future. When at least one production applicable, it will sample an applicable production rule, based on its utility, and execute the proposed action, which can be either in the environment or internally, such as adding a subtask to its task stack."
    },
    {
      "title": "Bootstrapping Procedures",
      "text": "The bootstrapping process starts with a _curriculum_. We took inspiration from [10], which uses an LLM to automatically construct the curriculum for the game of Minecraft. As the simulator we are using is not as popular as Minecraft, and the robot has some very specific affordance model (e.g., can only hold one object at a time), we find it better to specify the curriculum manually. Another difference is that our curriculum consists of families of tasks (e.g., find a/an <object>) instead of specific instances (e.g., find a/an egg). We follow the SOAR syntax and keep all variables in angle brackets. Unlike previous works that require human input on the next steps and/or goal condition for the tasks [10], we only require the names of the task families, so designing the curriculum is not very labor intensive. With a given curriculum, the following steps are used to bootstrap a single task in the curriculum (using find a/an <object> as an example). 1. Fill in the variables randomly from the environment to instantiate a concrete task (e.g., find a/an Egg); 2. Attempt the task with the existing production rules; 3. (**Action Selection**) If there is no production rule for a state, or there is a cycle detected through the production application, query an LLM for an action; 4. (**Production Generation**) Generate the corresponding production rule to the action, and load it into the agent; 5. Repeat step 1-4 sufficient times until the robot can perform the task with only production rules; 6. (**Production Improvement**) Use a critic to summarize the end condition of the task for future use and improve the generated productions. The above procedures are repeated for all task families in the curriculum. While the agent might not fully learn every scenario of a task before moving on to the next one, it can still query LLM to generate a production rule for a previous task later. The training of a task is considered complete as long as it has sufficient experience with the task to generate a reasonable end condition such that future tasks can reuse the previously learned tasks."
    },
    {
      "title": "Action Selection",
      "text": "The LLM is prompted with the current task, a summary of the current state, and a list of options available to the robot, which include both motor actions on the environment (e.g., move to a specific location) and internal action (e.g., attend to a new subtask). For each previously trained subtask, we provide the end condition generated by the critic for the LLM to evaluate its relevance. Like the task names, the actions can also be parameterized (e.g., move to <receptacle>) and the LLM can replace <receptacle> with anything as it sees fit. We use chain-of-thought prompting [20], which explicitly instructs the LLM to respond to the prompt in a step-by-step manner, probing it to make the most informed decision. The LLM is instructed to reflect on common strategies for approaching the task, analyze the current situation, and evaluate the usefulness of each action before suggesting one option for the robot to take. The LLM is also prompted to state the purpose of the chosen action, which will inform the production rule generation later."
    },
    {
      "title": "Production Generation",
      "text": "Although the production rule is generated based on the current state, they are not plans for the current task but instead should be the underlying decision-making principle for all similar scenarios. For example, if the current task is to find a/an egg, instead of suggesting the action sequence of exploring every cabinet in the current environment, a desirable production rule would suggest \"whenever you need to find something, you should first explore the unexplored places where that object is commonly stored\". This is a systematic generalization that can be applied to finding any objects, not just eggs or food, and also to other novel environments with different layouts and receptacle types. To generate desired production rules, we separate the production generation into two steps. The first step summarizes the action selection process and generates the English description of the production rule; the second step converts the description into executable Python code (Listing 1). This separation is inspired by how human beginners are instructed to build cognitive models [13], and has two benefits. First, it allows each query to the LLM to be of reasonable length (\\(\\sim 5k\\) tokens), preventing LLMs to lose focus on overly long prompts [13]. Second, it enables a modular design, which allows generating code from English descriptions generated from other sources, including human feedback and post-generation self-reflection. For each step, we also use the chain-of-thought prompting technique. For English description generation, the LLM is given the entire history of the action selection process, and is instructed to take four steps: 1) identify relevant information that leads to choosing the action, 2) generate a specific production rule that describes the current situation, 3) identify the potentially generalizable components in the specific rule and what they can be generalized to, and 4) replace the components to form the generalized production description. For code generation, the LLM is given the Python interface of querying declarative memory and the current task, and is instructed to take another four steps: 1) plan what variable bindings are needed, and how their values should be assigned, 2) analyze the predicates in the precondition and associate them with relevant variables, 3) plan how each predicate should be tested using the provided function interfaces, and 4) fill in the production template. The code snippet is parsed from the LLM output, saved as a Python file, and dynamically imported into the agent."
    },
    {
      "title": "Production Improvement",
      "text": "We use three mechanisms to monitor and improve the common interface mismatch, over-constraining, and over-generalization problems of the LLM-generated productions. Similar to the iterative prompting design in Voyager [21], we replay the generated production rule on the state that it is generated, and ensure that its pre-condition check passes the existing declarative knowledge. This mostly fixes errors regarding function interfaces, as the generated production has to comply with a specific naming scheme and the interface of the declarative knowledge. Passing the precondition test for a single instance does not guarantee that production is ideal. As the LLM has access to accumulated observations from the past during the action selection process, it might include unnecessary conditionsthat happen to be true in the precondition of the production, making it over-constraining. This is handled by a critic LLM that summarizes the end condition of the task and provides suggestions on the existing productions. Specifically, the critic LLM is given the name of the task family (e.g., find a/an <object>), and the English descriptions of the existing production rules for that task. The LLM is instructed to first analyze all the production rules whose effect is the done action, and summarize the end condition of the given task in a sentence (e.g., the robot is holding the desired object in its gripper). These end conditions summarize the behavior of the previously learned tasks to inform the action selection process for future tasks. As mentioned in the action selection section, this summary will be added to the prompt when querying for tasks later in the curriculum to incentivize reusing previously learned skills. Next, the LLM will go through all the production rules, and suggest modifications for each of them. The LLM is also given the choice of keeping a production rule as is or removing it entirely. The modifications are in the English description space for the critic, and we make use of the two-step modularity of production generation to update the production rules. Over-generalization happens when important features are left out of the production's precondition. For example, for the pick and place task, the LLM might generate a production rule that says: IF task is pick and place <object> AND <object> in field of view AND gripper is empty THEN pick <object> This will make the robot pick up the object even when the object is already in the target receptacle. To prevent the agent from being stuck in an infinite loop, it will keep a state transition graph during the execution process and query the LLM for an alternative action once a cycle is detected using a depth-first search on the transition graph. Coupled with the production reinforcement (described below), the agent will prioritize loop-breaking productions."
    },
    {
      "title": "Production Reinforcement",
      "text": "Following previous work in visual navigation [1], the agent has to explicitly choose the special done action to indicate that it has completed the current task. We further extend this and give the agent a quit option to indicate that it believes the given task is impossible for the given environment. This is important as we allow the architecture to choose to attend to any subtask as it wants, and it should be able to realize when a task is impossible. As we do not pre-define the goal condition during the bootstrapping process, we give a unit reward whenever the agent decides it is done with the current task. The reward propagates back through the shortest path to the starting state. For example, if the state transition is \\[S_{0}\\xrightarrow{P_{1}}S_{1}\\xrightarrow{P_{2}}S_{2}\\xrightarrow{P_{3}}S_{ 0}\\xrightarrow{P_{4}}S_{4}\\xrightarrow{P_{5}}S_{5}\\xrightarrow{P_{\\text{ done}}}\\] where \\(S_{0}\\) is the start state and \\(P_{\\text{done}}\\) is the production that yields the done action. Then the shortest path is \\[S_{0}\\xrightarrow{P_{4}}S_{4}\\xrightarrow{P_{5}}S_{5}\\xrightarrow{P_{\\text{ time}}}\\] Therefore only \\(P_{4},P_{5},P_{\\text{done}}\\) will receive a utility update, using the bellman backup [12]. \\[U_{\\text{after}}(P)\\leftarrow\\frac{1}{N(P)+1}\\left(N(P)\\cdot U_{\\text{before}} (P)+\\gamma^{\\Delta_{t}}\\right) \\tag{1}\\] Where \\(U(P)\\) is the utility of production \\(P\\), \\(N(P)\\) is the number of times \\(P\\) gets applied, \\(\\Delta_{t}\\) is the time difference from production application to the done action, and \\(\\gamma\\) is the discount factor (which is set to \\(0.95\\) for our experiments). When a subtask is involved, the utility is updated with respect to each task. For example, if the state transition is \\[A_{0}\\xrightarrow{P_{1}}A_{1}\\xrightarrow{P_{2}}\\underbrace{B_{3} \\xrightarrow{Q_{3}}B_{4}\\xrightarrow{Q_{4}}B_{5}\\xrightarrow{Q_{\\text{aux}}}}_ {\\text{a subtask initiated by }P_{2}}A_{6}\\xrightarrow{P_{\\text{ none}}}\\] Where \\(A\\) and \\(P\\) correspond to the states and productions of the original task respectively and \\(B\\) and \\(Q\\) correspond to the states and productions of the subtask respectively. This will be treated as two separate utility update pathways \\[A_{0}\\xrightarrow{P_{1}}A_{1}\\xrightarrow{P_{2}}A_{6} \\xrightarrow{P_{\\text{done}}}\\] \\[B_{3}\\xrightarrow{Q_{3}}B_{4}\\xrightarrow{Q_{4}}B_{5} \\xrightarrow{Q_{\\text{done}}}\\] If a subtask ends up with quit then there will be no utility update, not even negative ones. Because the task might be impossible due to environmental constraints, which has nothing to do with the production rules. Intuitively the closer a production brings the agent to choose done for its current task, the higher its utility will be. This process is not provided to the LLM, so it has no incentive to \"cheat\" by proposing the done action all the time. We also explicitly tell the LLM to avoid selecting done or quit action unless it is \"absolutely certain\" about it. This works empirically in our experiments. This utility update process helps reduce the impact of hallucination in LLMs as the knowledge is aggregated. For example, when tasked with \"explore the countertops\", the LLM may hallucinate and propose a production \\(P_{\\text{bad}}\\) that keeps the agent exploring the cabinets after all countertops have been explored, instead of proposing the done action, as it should. However, when tasked with \"explore the sink\" in the same bootstrapping section, the LLM may generate a production \\(P_{\\text{good}}\\) that correctly identifies the termination condition and proposes done when all receptacles of the desired type have been explored. Then later, when the agent needs to explore all the countertops (potentially as a subtask of another task) and all of the countertops have been explored, both \\(P_{\\text{bad}}\\) and \\(P_{\\text{good}}\\) will be applicable. The agent will prioritize \\(P_{\\text{good}}\\) because it is guaranteed to have a higher utility value than \\(P_{\\text{bad}}\\). On the other hand, if we use LLM to generate plans for each task, we would get a correct plan for the sink but an incorrect one for the countertops. When multiple productions are applicable given the same environment knowledge, we resolve the conflict using the definition of noisy-optimal in previous works Tian et al. (2023). Where the probability of production \\(P_{i}\\) being selected and applied, given the current knowledge \\(\\mathcal{K}\\), is \\[\\mathbb{P}(P_{i}\\mid\\mathcal{K})\\propto\\mathbf{I}_{\\mathcal{K}}(P_{i})\\cdot \\exp(U(P_{i})) \\tag{2}\\] where \\(\\mathbf{I}_{\\mathcal{K}}(p)\\) indicates that the preconditions of production \\(p\\) hold, given knowledge \\(\\mathcal{K}\\)."
    },
    {
      "title": "World Knowledge Base",
      "text": "For the sake of simplicity, we implemented the world knowledge of the agent as a dictionary that maps natural language statements to either true or false. Unlike many existing cognitive architectures that assume an absence of knowledge means the negation is true, we explicitly differentiate between not knowing and knowing to be false. When a production rule is conditioned on a statement not previously known to the agent, the LLM is used to evaluate whether the statement is true, and the result will be saved to the knowledge base to be reused later. For instance, when bootstrapping the task of finding an egg, the agent will learn the production rule that says \"If there is an unexplored receptacle where the object is commonly stored, explore that receptacle\". But the agent does not know whether \"egg is commonly stored in the fridge\" is true or not initially, so it will query the LLM and memorize the positive response in its world knowledge base. Later when the agent is tasked to put things in their common storage place, the agent can reuse the knowledge and place eggs into the fridge. In addition to transferring to new tasks, the knowledge can be applied to new environments as well (e.g., eggs are commonly stored in fridges in most American households). This knowledge base could be easily replaced by connecting it to an existing knowledge graph or ontology, but for the purpose of this paper, we are bootstrapping it from scratch."
    },
    {
      "title": "Experiments",
      "text": ""
    },
    {
      "title": "Setup",
      "text": "Following previous works in the embodied agents domain Sarch et al. (2022); Trabucco et al. (2023), we evaluate our method in the kitchen environments in the AI2THOR simulator Kolve et al. (2017), shown in Figure 2. As shown in Figure 2, the agent has access to classification labels and attributes (e.g., \"is opened\") for objects that are close enough (within \\(1.6m\\)) or large enough (more than \\(5\\%\\) of the frame). We also assume the agent already knows the names and locations of the large receptacles (e.g., cabinets, fridges, etc.) but does not know what objects are in the receptacles until the agent actively explores them. We use three different tasks for evaluation: * find a/an <object>: the goal is to have the specified object in the robot's field of view. This is a fundamental skill that is often overlooked or directly assumed in many of the previous works Singh et al. (2023). We want to show that our framework can bootstrap very basic skills in addition to composite actions. * slice a/an <object>: the goal is to use a knife to slice an object. Because the robot can hold at most one item at a time, slicing involves a sequence of actions including finding the target object and the knife, putting them in the same place, and the final slice action. We want to show that our framework can handle tasks that involve multiple steps and tool use. * clear the countertops: the goal is to have all the objects on the countertops moved to suitable storage places. This is a common household task that is investigated a lot in previous work Andrew et al. (2022); Sarch et al. (2022). We want to show that our framework can handle tasks that involve repeating similar subtasks. The goal conditions listed above are only used for evaluation purposes, but are not provided to the LLM during training or testing. The LLM has to infer the goal condition from the task description only. For find and slice, \\(5\\) target objects are chosen for each task, and we run \\(3\\) trials for each object where the initial locations of the objects are shuffled. For clear the countertops we run \\(3\\) trials each with \\(5\\) objects on the countertops that need to be put away. The specific objects and locations vary between trials, and the success of the agent is evaluated based on how many objects originally on the countertops have been relocated to other places. This results in \\(15\\) specific goal instances for each task family. We use GPT4-0613 OpenAI (2023) for our experiments as previous works have shown that GPT3.5 is insufficient for code generation Olausson et al. (2023); Wang et al. (2023). We set temperature to the \\(0\\) for the most deterministic response. Figure 2: Screenshots of the AI2THOR simulator"
    },
    {
      "title": "Conditions",
      "text": "For the experiment condition, we bootstrapped our agent with the following curriculum in the training floor plan: 1. explore <receptacle> 2. find a/an <object> 3. pick and place a/an <object> in/on a/an <receptacle> 4. slice a/an <object> 5. put things on the countertop away This process generated \\(27\\) production rules in total. During test time, the agent can query the LLM for an immediate action if it does not have an applicable production rule for the current situation, but it cannot learn new production rules. For the baseline condition of using LLMs to query only the actions, we omit the production generation steps and only use the action selection process within our framework. This ensures the prompts used by both conditions are the same, so LLM should suggest actions of similar quality. If the action proposed by the LLM leads to an affordance error, we query LLM another two times, and if none of the actions are viable by the agent, then it raises a failure. Although many works address the rearrangement task [1, 2], they are not appropriate baselines as their architectures already encode the general strategies (e.g., first determine the target receptacle for each object, then navigate to the target area, etc.) while our approach bootstraps everything from scratch. Other code generation works cannot handle multiple instances of the same kind [2] or understand the slicing preconditions [2] without non-trivial modifications."
    },
    {
      "title": "Results",
      "text": "Table 1 shows the quantitative results of different types of agents performing each kitchen task. The action-only baseline successfully completes all tasks but one, where it assumes find a/an mug is equivalent to find a/an cup, and ends the search pre-maturely without exploring the sink where the mug is actually placed. On the other hand, our bootstrapped agent is able to finish most tasks completely using its learned production rule. The only exceptions are when it is tasked to find an object that does not exist in the scene, which is not part of its training. But with very limited queries, the bootstrapped agent is able to successfully complete those tasks as well. This shows that the knowledge in the bootstrapped agent can be easily transferred to new objects in new environments. The success rate and number of query tokens show two advantages of our framework. First, it is verifiable such that it won't make false assumptions (e.g., confusing mugs with cups). Second, it is much more efficient to be deployed into new environments as the production rules it learned can be easily transferred and require minimal further assistance from the LLM, saving computations and costs. We use paired sample t-test for means to compare the number of steps taken by both agents. No significant evidence suggests that the two agents perform differently in find nor slice task (p-values \\(0.446\\) and \\(0.347\\)). This is not surprising as the knowledge source of both agents is the same LLM. However, the bootstrapped agent is taking longer in the clearing task with significance (p-value \\(0.001\\)), which results from a stylistic difference between the two agents. As shown in Figures 2(b) and 2(d), the bootstrapped agent places everything into an individual cabinet while the baseline places multiple objects in the same cabinets. This is because one of the productions generated is \"if there is an object on the countertop and there is an empty receptacle, attend to the subtask pick up the object and place it into the empty receptacle\". This production gets reused repeatedly, requiring the agent to seek an unique empty receptacle before placing each object instead of putting every object in the same cabinet. By contrast, the baseline agent is making decisions on a case-by-case basis, so it does not enforce that the target receptacle has to be empty. A similar difference is also found in the slice task where the bootstrapped agent always moves the objects to the countertops before slicing while the baseline agent slices objects at their current location (Figures 2(a) and 2(c))."
    },
    {
      "title": "Production Analysis",
      "text": "The following are some learned productions: * IF the current task is to find a/an <object> AND the <object> is located on <location> AND the robot is not at <location> THEN choose motor action: move to <location>. Figure 3: Examples of task execution. The first row shows the bootstrapped agent sliced the apple on the countertop, and put each object in their own cabinet. The second row shows the baseline agent sliced the apple at its current location (fridge), and put multiple objects in the same cabinet. * IF the current task is to slice a/an <sliceable> AND the robot is holding a/an <sliceable> AND there is no <tool> in the spatial knowledge or object knowledge THEN choose 'attend to subtask: find a/an <tool>'. * IF the current task is to clear objects from a/an <receptacle_type> AND all the <receptacle_type> are empty THEN choose special action: 'done'. These show that the agent is able to represent different aspects of the given tasks using production rules. The first represents a common strategy for finding things, namely how to find things with a known location. The second represents decomposing complex tasks and reusing previously learned tasks. The third is a correct termination condition, which is not directly provided, for the exploration task from the LLM. Figure 4 shows the task hierarchy learned by the agent after training on the given curriculum. It shows how previously learned tasks are used to perform new tasks. This reduces the number of queries needed for the LLM, fosters generality, and ensures the scalability of our approach."
    },
    {
      "title": "Discussion",
      "text": ""
    },
    {
      "title": "Explainability",
      "text": "Our framework touches upon all three aspects of explainability as defined by Milani et al. (2022). The preconditions of the productions directly specify the feature that is being used (feature importance). Each production rule corresponds to a specific scenario during the bootstrapping process when it is created, which helps determine the training points that influence the learned policy (learning process). Lastly, the production application process can be easily converted to a verifiable decision tree by merging the precondition checks of productions (policy-level explainability)."
    },
    {
      "title": "Limitations",
      "text": "In this work, we explore only the high-level decision-making process of the agent and rely heavily on having a well-defined interface for low-level actions, such as navigation and object manipulation. There will likely be a considerable sim-to-real gap when applying this to physical agents. Additionally, the English description generation step requires the decision-making process to be articulable to be converted to production rules. This is hard for skills that cannot be fully expressed using language (e.g., sculpting)."
    },
    {
      "title": "Future Work",
      "text": "There are more learning opportunities in cognitive architectures such as updating the preconditions of productions or using separate productions for conflict resolutions. These would help better extract the existing knowledge from LLMs to fit the specific agent and environment configurations. Additionally, it is well-acknowledged that human values and preferences are hard to represent with a single reward function (Casper et al., 2023). But the production rules are interpretable and can be easily modified to suit each individual without extensive computation. Therefore it would interesting to examine whether this framework will facilitate personalization in human-AI collaboration tasks."
    },
    {
      "title": "Conclusion",
      "text": "This paper presents a framework for bootstrapping a cognitive architecture from the existing noisy knowledge in LLMs, with minimal human inputs. We demonstrated how such an agent could efficiently learn to perform kitchen tasks and be applied to new environments. This work generalizes using LLMs to generate plans and provides an alternative to purely data-driven foundation models. And finally, we shed light on how it will benefit personalized agents in the future. \\begin{table} \\begin{tabular}{c l l l l} \\hline \\hline Task & Agent & Success \\(\\uparrow\\) & Success w/o LLM \\(\\uparrow\\) & Steps \\(\\downarrow\\) & Tokens \\(\\downarrow\\) \\\\ \\hline find a/an \\textless{}object> & action-only & \\(14/15\\) & - & \\(15.67\\) & \\(54754.20\\) \\\\ & bootstrapped (ours) & \\(15/15\\) & \\(12/15\\) & \\(15.80\\) & \\(916.87\\) \\\\ \\hline slice a/an \\textless{}object> & action-only & \\(15/15\\) & - & \\(28.20\\) & \\(102806.60\\) \\\\ & bootstrapped (ours) & \\(15/15\\) & \\(15/15\\) & \\(29.13\\) & \\(0.00\\) \\\\ \\hline clear the countertops & action-only & \\(15/15\\) & - & \\(5.13\\) & \\(18924.87\\) \\\\ & bootstrapped (ours) & \\(15/15\\) & \\(15/15\\) & \\(7.47\\) & \\(0.00\\) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Result of experiments on household tasks. Completion steps and tokens are averaged over all task instances Figure 4: The hierarchy of tasks learned. Gray nodes denote the built-in functions of the robot, and white nodes represent the tasks learned from the curriculum. For built-in actions that involve an object (e.g., close), the object has to be within the field of view for the action to be taken. Special actions (i.e., done and quit) are omitted due to space constraints."
    },
    {
      "title": "References",
      "text": "* M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al. (2022)Do as i can, not as i say: grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. Cited by: SS1. * J. R. Anderson (2009)How can the human mind occur in the physical universe?. Oxford University Press. Cited by: SS1. * P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, et al. (2018)On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757. Cited by: SS1. * S. Andrew, Y. Karmesh, C. Alex, B. Vincent-Pierre, G. Aaron, C. Angel, S. Manolis, K. Zsolt, and B. Dhruv (2022)Habitat rearrangement challenge. Note: [https://aihabitat.org/challenge/rearrange_2022](https://aihabitat.org/challenge/rearrange_2022) Cited by: SS1. * S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak, D. Lindner, P. Freire, et al. (2023)Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217. Cited by: SS1. * N. Di Palo, A. Byravan, L. Hasenclever, M. Wulfmeier, N. Heess, and M. Riedmiller (2023)Towards a unified agent with foundation models. In Workshop on Reincarning Reinforcement Learning at ICLR 2023, Cited by: SS1. * D. Driess, F. Xia, M. S. Sajladi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. (2023)Palm-e: an embodied multimodal language model. arXiv preprint arXiv:2303.03378. Cited by: SS1. * K. Ellis, C. Wong, M. Nye, M. Sable-Meyer, L. Morales, L. Hewitt, L. Cary, A. Solar-Lezama, and J. B. Tenenbaum (2021)Dreamcoder: bootstrapping inductive program synthesis with wake-sleep library learning. In Proceedings of the 42nd acm sigplan international conference on programming language design and implementation, pp. 835-850. Cited by: SS1. * H. S. Hake, C. Sibert, and A. Stocco (2022)Inferring a cognitive architecture from multitask neuroimaging data: a data-driven test of the common model of cognition using Granger causality. Topics in Cognitive Science14 (4), pp. 845-859. Cited by: SS1. * W. Huang, P. Abbeel, D. Pathak, and I. Mordatch (2022)Language models as zero-shot planners: extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. Cited by: SS1. * J. R. Kirk and J. E. Laird (2019)Learning hierarchical symbolic representations to support interactive task learning and knowledge transfer. In IJCAI, pp. 6095-6102. Cited by: SS1. * J. R. Kirk, R. E. Wray, P. Lindes, and J. E. Laird (2023)Integrating diverse knowledge sources for online one-shot learning of novel tasks. arXiv:2208.09554. Cited by: SS1. * E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi (2017)AI2-thOR: an interactive 3d environment for visual ai. arXiv. Cited by: SS1. * J. E. Laird (2017)SOR 9.6.0 Tutorial. Cited by: SS1. * J. E. Laird (2022)Introduction to soar. arXiv preprint arXiv:2205.03854. Cited by: SS1. * J. E. Laird, K. Gluck, J. Anderson, K. D. Forbus, O. C. Jenkins, C. Lebiere, D. Salvucci, M. Scheutz, A. Thomaz, G. Trafton, et al. (2017)Interactive task learning. IEEE Intelligent Systems32 (4), pp. 6-21. Cited by: SS1. * J. E. Laird, C. Lebiere, and P. S. Rosenbloom (2017)A standard model of the mind: toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. Ai Magazine38 (4), pp. 13-26. Cited by: SS1. * J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng (2023)Code as policies: language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 9493-9500. Cited by: SS1. * J. R. Lindes and W. Peter (2023)Improving knowledge extraction from LLMs for robotic task learning through agent analysis. arXiv preprint arXiv:2306.06770. Cited by: SS1. * B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone (2023)LLM+ p: empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477. Cited by: SS1. * N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang (2023)Lost in the middle: how language models use long contexts. arXiv preprint arXiv:2307.03172. Cited by: SS1. * A. Madaan, S. Zhou, U. Alon, Y. Yang, and G. Neubig (2022)Language models of code are few-shot commonsense learners. arXiv preprint arXiv:2210.07128. Cited by: SS1. * S. Milani, N. Topin, M. Veloso, and F. Fang (2022)A survey of explainable reinforcement learning. arXiv preprint arXiv:2202.08434. Cited by: SS1. * A. Mininger and J. E. Laird (2022)A demonstration of compositional, hierarchical interactive task learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36, pp. 13203-13205. Cited by: SS1. * A. Newell (1994)Unified theories of cognition. Harvard University Press. Cited by: SS1. * T. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama (2023)Demystifying GPT Self-Repair for code generation. arXiv:2306.09896. Cited by: SS1. * A. OpenAI (2023)GPT-4 technical report. Note: arXiv:2303.08774 Cited by: SS1. * J. S. Park, J. C. O'Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein (2023)Generative agents: interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442. Cited by: SS1. * J. Qiu, M. Xu, W. Han, S. Moon, and D. Zhao (2023)Embodied executable policy learning with language-based scene summarization. arXiv preprint arXiv:2306.05696. Cited by: SS1. * G. Sarch, Z. Fang, A. W. Harley, P. Schydlo, M. J. Tarr, S. Gupta, and K. Fragkiadaki (2022)Tidee: tidying up novel rooms using visuo-semantic commonsense priors. In European Conference on Computer Vision, pp. 480-496. Cited by: SS1. * I. Singh, V. Blukis, A. Mousavian, D. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg (2023)ProgPrompt: generating situated robot task plans usingLarge Language Models. In _International Conference on Robotics and Automation (ICRA)_. * Song et al. (2022) Song, C. H.; Wu, J.; Washington, C.; Sadler, B. M.; Chao, W.-L.; and Su, Y. 2022. Llm-planner: Few-shot grounded planning for embodied agents with large language models. _arXiv preprint arXiv:2212.04088_. * Sutton and Barto (2018) Sutton, R. S.; and Barto, A. G. 2018. _Reinforcement learning: An introduction_. MIT press. * Tian et al. (2023) Tian, R.; Tomizuka, M.; Dragan, A. D.; and Bajcsy, A. 2023. Towards Modeling and Influencing the Dynamics of Human Learning. In _Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction_, 350-358. * Trabucco et al. (2023) Trabucco, B.; Sigurdsson, G. A.; Piramuthu, R.; Sukhatme, G. S.; and Salakhutdinov, R. 2023. A Simple Approach for Visual Room Rearrangement: 3D Mapping and Semantic Search. In _The Eleventh International Conference on Learning Representations_. * Vemprala et al. (2023) Vemprala, S.; Bonatti, R.; Bucker, A.; and Kapoor, A. 2023. ChatGPT for Robotics: Design Principles and Model Abilities. Technical Report MSR-TR-2023-8, Microsoft. * Wang et al. (2023) Wang, G.; Xie, Y.; Jiang, Y.; Mandlekar, A.; Xiao, C.; Zhu, Y.; Fan, L.; and Anandkumar, A. 2023. Voyager: An Open-Ended Embodied Agent with Large Language Models. arXiv:2305.16291. * Wei et al. (2022) Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35: 24824-24837. * Wu et al. (2023) Wu, J.; Antonova, R.; Kan, A.; Lepert, M.; Zeng, A.; Song, S.; Bohg, J.; Rusinkiewicz, S.; and Funkhouser, T. 2023. Tidybot: Personalized robot assistance with large language models. _arXiv preprint arXiv:2305.05658_. * Xiang et al. (2023) Xiang, J.; Tao, T.; Gu, Y.; Shu, T.; Wang, Z.; Yang, Z.; and Hu, Z. 2023. Language Models Meet World Models: Embodied Experiences Enhance Language Models. _arXiv preprint arXiv:2305.10626_. * Xie et al. (2023) Xie, Y.; Yu, C.; Zhu, T.; Bai, J.; Gong, Z.; and Soh, H. 2023. Translating natural language to planning goals with large-language models. _arXiv preprint arXiv:2302.05128_. * Ye et al. (2023) Ye, S.; Lauer, J.; Zhou, M.; Mathis, A.; and Mathis, M. W. 2023. AmadeusGPT: a natural language interface for interactive animal behavioral analysis. _arXiv preprint arXiv:2307.04858_. * Zhang and Soh (2023) Zhang, B.; and Soh, H. 2023. Large Language Models as Zero-Shot Human Models for Human-Robot Interaction. arXiv:2303.03548. * Zhu et al. (2023) Zhu, X.; Chen, Y.; Tian, H.; Tao, C.; Su, W.; Yang, C.; Huang, G.; Li, B.; Lu, L.; Wang, X.; et al. 2023. Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory. _arXiv preprint arXiv:2305.17144_. * Zou et al. (2023) Zou, A.; Wang, Z.; Kolter, J. Z.; and Fredrikson, M. 2023. Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043."
    },
    {
      "title": "Technical Appendix",
      "text": ""
    },
    {
      "title": "Step-By-Step Example Of Learning One Production Rule",
      "text": "This section shows an example of learning a new production rule for slice a/an <object> task. In the interest of space, only relevant information is kept. The original complete prompt, along with the responses from the LLM, are provided in the code and data supplementary material."
    },
    {
      "title": "System Prompt For Action Selection And Production Description In English",
      "text": "The system prompt mainly describes the robot's affordance model and explains the input of future user prompts. It is the same prompt for all action selections."
    },
    {
      "title": "User Prompt For Action Selection",
      "text": "As mentioned in the methods section, the production generation is grounded to a specific instance, and the LLM is first asked to choose an action. The user prompt for action selection has a fixed template, where the information will be dynamically filled according to the actual knowledge of the agent. Below is an example of the information provided in the prompt. Unimportant information and static instructions are omitted in the interest of space. The user prompt below and the system prompt will be provided to the LLM in the same request for the action selection. [Current Task] slice a/an lettuce [Current Location] in front of SinkBasin_28084e25 [Spatial Knowledge] * (0.0 meters away) RobotGripper(Gripper) has Lettuce_895e9ec5(Lettuce), and nothing else * (0.0 meters away) SinkBasin_28084e25(SinkBasin) has been fully explored: it has Cup_26e78d79(Cup), Egg_113844f2(Egg), and nothing else * (0.9 meters away) CounterTop4(CounterTop) has been explored: it is empty * <<OTHER RECEPTACLES OMITTED>> [Object Knowledge] * Apple_969aaa5d(Apple) at Fridge_683c888d * Cabinet_2fee59ce(Cabinet): opened, * Faucet_63e88a3b(Faucet):turned off, * Lettuce_895e9ec5(Lettuce) at RobotGripper * <<OTHER OBJECTS OMITTED>> [Previous Tasks] * go to explore a/an sinkbasin_28084e25: True * find a/an lettuce: True * <<OTHER TASKS OMITTED>> [Action History] * <<OTHER HISTORY OMITTED>> * (time 20) motor action: pick up lettuce (purpose: IF the current task is to find a/an <object> AND the <object> is located in a/an <receptacle> AND the robot is in front of the <receptacle> AND the robot's gripper is empty THEN choose motor action: pick up <object>.) * (time 21) special action: done (purpose: IF the current task is to find a/an <object> AND the robot's gripper has <object> THEN choose special action: 'done'.) [Possible Options] * attend to subtask: find a/an <object> (Apply anytime. End condition: the robot has found the object and has it inits gripper.) * motor action: put Lettuce_895e9ec5 on CounterTop4 * special action: 'done' * <<OTHER OPTIONS OMITTED>> [Blacklisted Options] * attend to subtask: slice a/an lettuce"
    },
    {
      "title": "Gpt4 Response For Action Selection",
      "text": "The response of GPT4 to the prompt above is recreated below. [Current Task] Slice a lettuce. [Common Strategies for \"slice a lettuce\"] * Find a lettuce and a knife. * Pick up the lettuce. * Move to a countertop or a similar flat surface. * Put down the lettuce. * Pick up the knife. * Slice the lettuce. [Goal Conditions] * The lettuce is sliced. [Current Situation Analysis] The robot is currently holding a lettuce and is located in front of the sink. The robot has explored several areas of the kitchen, including the sink, countertop4, countertop5, and a few cabinets. However, there are still several unexplored areas, including countertop1, countertop2, countertop3, and several cabinets. The robot has not found a knife yet, which is necessary for slicing the lettuce. The robot's previous actions indicate that it has successfully found and picked up the lettuce, which is part of the task. There is no existing plan provided. [Task Analysis] The current task is to slice a lettuce. The robot has already found and picked up the lettuce, which is part of the task. However, the robot has not found a knife yet, which is necessary for slicing the lettuce. The robot also needs to find a suitable place to put down the lettuce for slicing, such as a countertop. [Plan] Step1: Put down the lettuce on a countertop. Step2: Find and pick up a knife. Step3: Slice the lettuce. [Option Evaluation] \"attend to subtask: find a/an <object>\": This option is not useful right now because the robot already has the lettuce in its gripper. \"motor action: put Lettuce_895e9ec5 on CounterTop4\": This option is useful because it allows the robot to free its gripper so it can pick up a knife. \"special action: 'done'\": This option is not appropriate because the task is not completed yet. <<OTHER EVALUATION OMITTED>> [Option Suggestion] \"motor action: put Lettuce_895e9ec5 on CounterTop4\" [Purpose] The purpose of the suggested option is to free the robot's gripper so it can pick up a knife. [End] User prompt for generating English description of the production ruleAfter parsing the suggested option from the previous response, we prompt the LLM again for the English description of the production rule. We combine all the previous queries and responses and send all of them to the LLM in a single request. **GPT4 response for production rule in English** [Relevant Information] * The current task is to slice a lettuce. * The robot is currently holding a lettuce in its gripper. * The robot is currently in front of the sink, which is not an ideal place for slicing. * A countertop is a suitable place for slicing. * The robot has the option to put the lettuce on CounterTop4. [Specific Rule] IF the current task is to slice a lettuce AND the robot is holding a lettuce in its gripper AND the robot is not at a suitable place for slicing AND there is a countertop available THEN choose motor action: put Lettuce_895e9ec5 on CounterTop4. [Generalizable Constants] * Lettuce_895e9ec5 could be any object that needs to be sliced. * CounterTop4 could be any countertop or similar flat surface. [Generalized Rule] IF the current task is to slice a/an <object> AND the robot is holding the <object> in its gripper AND the robot is not at a suitable place for slicing AND there is a countertop available THEN choose motor action: put <object> on <countertop>. [Correspondence] * <object> is Lettuce_895e9ec5. * <countertop> is CounterTop4. Notes: This rule is designed to ensure that the robot places the object to be sliced on a suitable surface before attempting to slice it. It is applicable to any task that involves slicing an object. System prompt for production rule code generationThe system prompt mainly describes the Python interfaces for the productions and information stored in the agent. It is the same prompt for all code generation. User prompt for production rule code generationThe response of the production rule in English is copied verbatim along with the instructions. GPT4 response for production code generationThe response generated by GPT4 is recreated below. The code has been reformatted to fit the page. [Variable Bindings] * target_object: the object that needs to be sliced. Its value should be parsed from the current task. * target_receptacle: a suitable place for slicing. Its value should be assigned by iterating through all receptacles and finding the countertop that is closest to the robot. [Precondition Relation] * \"the current task is to slice a/an <object>\" is about the current task * \"the robot is holding the <object> in its gripper\" is about the object type of target_object and the hosting attribute of the robot's gripper * \"the robot is not at a suitable place for slicing\" is about the current location * \"there is a countertop available\" is about the existence of target_receptacle [Precondition Test] * The current task is to slice a/an <object> can be tested by checking if * re.match(r\"slice a/an (.+)\", current_task) ' * The robot is holding the <object> in its gripper can be tested by checking if * spatial_knowledge[\"RobotGripper\"].hosting and name_equal( * spatial_knowledge[\"RobotGripper\"].hosting[0].object_type, target_object) ' * The robot is not at a suitable place for slicing can be tested by checking if * 'not OracleQuery(f\"(current_location) is a suitable place for slicing\") ' * There is a countertop available can be tested by checking if 'target_receptacle is not None'[Python Implementation] \"\"py import re from typing import Dict, Tuple from.knowledge import SpatialKnowledge, ObjectKnowledge class SliceObject: target_task: str = \"slice a/an <sliceable>\" def precondition( self, current_task: str, current_location: str, previous_tasks: Dict[str, bool], spatial_knowledge: SpatialKnowledge, object_knowledge: ObjectKnowledge, ) -> Tuple[bool, str]: match = re.match(r\"slice a/an (.+)\", current_task) if not match: return False, \"The current task is not to slice an object\" target_object = match.group(1) if not spatial_knowledge[\"RobotGripper\"].hosting or not name_equal( spatial_knowledge[\"RobotGripper\"].hosting[0].object_type, target_object, ): return ( False, \"The robot is not holding the target object in its gripper\", ) if OracleQuery(f\"{current_location} is a suitable place for slicing\"): return False, \"The robot is already at a suitable place for slicing\" target_receptacle = min( ( r for r in spatial_knowledge.locations.values() if name_equal(r.object_type, \"CounterTop\") ), key=spatial_knowledge.get_distance ) if target_receptacle is None: return False, \"There is no countertop available\" setattr(self, \"target_object\", target_object) setattr(self, \"target_receptacle\", target_receptacle.object_name) return True, \"\" def apply(self) -> str: return f\"motor action: put {self.target_object} on {self.target_receptacle}\" \"\" User prompt for code revisionAs described in the production improvement section, if the code generated does not pass the current state knowledge, we query it again with the current knowledge and the error raised, which in this example is the world knowledge cannot determine whether \"SinkBasin_28084e25 is a suitable place for slicing\" is true or not. <<INFORMATION FROM THE ACTION SELECTION STEP>> [Expected Return of 'precondition() '](True, \"\") [Your Code Return] ValueError: got unknown statement: SinkBasin_28084e25 is a suitable place for slicing **GPT4 response for code revision** [Error Interpretation] The message indicates that the OracleQuery function has failed. [Error Attribution] The error is because the OracleQuery function does not understand the statement \"SinkBasin_28084e25 is a suitable place for slicing\". The OracleQuery function is designed to answer general knowledge questions, not specific queries about the current environment. [Things to Change] The OracleQuery test should be replaced with a check against a list of suitable places for slicing. [Python Implementation] \"'py import re from typing import Dict, Tuple from.knowledge import SpatialKnowledge, ObjectKnowledge class SliceObject: target_task: str = \"slice a/an <sliceable>\" def precondition( self, current_task: str, current_location: str, previous_tasks: Dict[str, bool], spatial_knowledge: SpatialKnowledge, object_knowledge: ObjectKnowledge, ) -> Tuple[bool, str]: match = re.match(r\"slice a/an (.+)\", current_task) if not match: return False, \"The current task is not to slice an object\" target_object = match.group(1) if not spatial_knowledge[\"RobotGripper\"].hosting or not name_equal( spatial_knowledge[\"RobotGripper\"].hosting[0].object_type, target_object, ): return ( False, \"The robot is not holding the target object in its gripper\", ) suitable_places_for_slicing = [\"CounterTop\"] if not any( name_equal(current_location, place) for place in suitable_places_for_slicing ): return False, \"The robot is not at a suitable place for slicing\"target_receptacle = min( ( r for r in spatial_knowledge.locations.values() if name_equal(r.object_type, \"CounterTop\") ), key=spatial_knowledge.get_distance, ) if target_receptacle is None: return False, \"There is no countertop available\" setattr(self, \"target_object\", target_object) setattr(self, \"target_receptacle\", target_receptacle.object_name) return True, \"\" def apply(self) -> str: return f\"motor action: put {self.target_object} on {self.target_receptacle}\" ..."
    },
    {
      "title": "Production Rules Generated",
      "text": "Figures 5 and 6 illustrate the production rules learned for each task in the form of decision trees. These decision trees are for illustrations purpose and are not completely equivalent to the agent because of the sampling mechanism in the production selection process. The order of the nodes is chosen based on the utility of the production. That is, productions with higher utility (e.g., productions leading to the done action) will be closer to the root. This demonstrates how the production rules can be converted to verifiable decision trees as mentioned in the discussion section in the main paper. The actual production rules and their Python implementations can be found in the code and data supplementary material. Figure 5: Productions learned through bootstrapping for exploring, finding, and placing. Gray nodes are the effects, and white nodes are the features being conditioned on."
    },
    {
      "title": "Tokens Usage",
      "text": "As shown in Figure 7, the number of tokens needed to train each task is roughly the same. So as the curriculum expands, the number of tokens needed will only grow linearly. Additionally, the number of tokens needed to train one task is less than one single trial of slicing objects of the action-only agent as reflected in Table 1. This shows that our framework is much more cost-effective. The testing experiments on the baseline action-only agent cost around \\(\\$120\\) in total while the bootstrapping of our framework costs less than \\(\\$40\\)."
    },
    {
      "title": "Step-By-Step Example Of Completing One Task",
      "text": "Figure 8 and Table 2 shows the trajectory of the agent completing the task of \"pick up and place a/an kettle in/on a/an sinkbasin\" after bootstrapping. The agent first attends to the subtask of finding a kettle, during which process it also uses the explore subtasks, and finally moves to the sink basin and places the kettle as instructed. The main task column in the table reflects the management of the task stack in the agent: it attends to a single main task at a time and releases it when a production rule determines the current task is done."
    },
    {
      "title": "Task End Conditions Generated",
      "text": "Here is a list of end conditions for the tasks families in our curriculum. * explore a/an <receptacle>: \"the robot has fully explored the receptacle.\" * find a/an <object>: \"the robot has found the object and has it in its gripper.\" * pick up and place a/an <object> in/on a/an <receptacle>: \"the robot has successfully picked up the specified object and placed it in/on the specified receptacle, and the robot's gripper is empty.\" * slice a/an <sliceable>: \"the sliceable object is already sliced and the robot's gripper is holding a knife.\" Figure 6: Productions learned through bootstrapping for slicing and putting things away. Gray nodes are the effects, and white nodes are the features being conditioned on. Figure 7: Number of tokens used during bootstrapping* put things on the countertops away: \"all objects on the countertops have been put away in the cabinets and there are no more unexplored countertops or cabinets.\" They might not be fully aligned with the human's intention (e.g., someone may think having an object in view already satisfies the goal of \"find\", but the agent believes the task is not done until it picks the object up), but they reflect what the agent would do if the subtask is chosen. This is very helpful for reusing previously learned tasks."
    },
    {
      "title": "Reproducibility Checklist",
      "text": "We answer \"partial\" to the following question on the reproducibility checklist [Computational experiments] If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. This is because our experiments involve the use of GPT-4 and an Unity-based simulator, whose internal mechanism is not fully disclosed to the best of our knowledge. We have done our best to set the temperature of GPT-4 to \\(0\\), but empirical experiments and experiences from other users suggest that its behavior is still not deterministic when the temperature is set to \\(0\\). Additionally, there is physics simulation in the simulator (e.g., when slicing a lettuce, the slices will fall apart). These motions are not deterministic according to our observation, and the documentation of the simulator does not provide a way to make the result deterministic. Because the bootstrapping process contains multiple steps (\\(\\sim 1400\\) steps), small discrepancies at the beginning may accumulate and result in very different production rules learned. Despite we cannot guarantee whether anyone bootstrapping the agent from scratch will generate the same production rules as we do, we attach (in the code and data supplementary material) our bootstrapped production rules, the logs generated during the bootstrapping process, and the logs generated during the testing process such that one can use them to verify the results we reported in the experiments section of the main paper. Figure 8: Trajectory of the agent completing ‚Äúpick up and place a/an kettle in/on a/an sinkbasin‚Äù \\begin{table} \\begin{tabular}{l|l l l l} \\hline Main Task & Figure & Action & Production Preconditions \\\\ \\hline \\hline pick and place a/an kettle & 8a & find a/an kettle & the robot‚Äôs gripper is empty AND the \\textless{}object\\textgreater{} has not been located \\\\ in/on a/an sinkbasin & & & & \\\\ \\hline & 8a & open fridge & the robot is in front of a closed \\textless{}receptacle\\textgreater{} AND the robot‚Äôs gripper is empty \\\\ \\cline{2-5} find a/an kettle & & & & \\\\ \\cline{2-5} & 8b & move to countertop2 & the robot‚Äôs gripper is empty AND there is an unexplored \\textless{}receptacle\\textgreater{} that is commonly associated with the \\textless{}object\\textgreater{} \\\\ \\cline{2-5} & 8c & explore a/an cabinet1 & the robot‚Äôs gripper is empty AND there are unexplored receptacles in the kitchen \\\\ \\hline & 8c & move to cabinet1 & the \\textless{}receptacle\\textgreater{} is not at the current location AND the robot‚Äôs gripper is empty \\\\ \\cline{2-5} explore a/an cabinet1 & & & & \\\\ \\cline{2-5} & 8d & open cabinet1 & the robot is at the location of the \\textless{}receptacle\\textgreater{} AND the robot‚Äôs gripper is empty \\\\ \\cline{2-5} & 8e & done & the robot is in front of the \\textless{}receptacle\\textgreater{} AND the \\textless{}receptacle\\textgreater{} has been fully explored \\\\ \\hline find a/an kettle & 8e & explore a/an cabinet2 & the robot‚Äôs gripper is empty AND there are unexplored receptacles in the kitchen \\\\ \\hline & 8e & move to cabinet2 & the \\textless{}receptacle\\textgreater{} is not at the current location AND the robot‚Äôs gripper is empty \\\\ \\cline{2-5} explore a/an cabinet2 & & & & \\\\ \\cline{2-5} & 8f & open cabinet2 & the robot is at the location of the \\textless{}receptacle\\textgreater{} AND the \\textless{}receptacle\\textgreater{} is closed and unexplored AND the robot‚Äôs gripper is empty \\\\ \\cline{2-5} & 8g & done & the robot is in front of the \\textless{}receptacle\\textgreater{} AND the \\textless{}receptacle\\textgreater{} has been fully explored \\\\ \\hline & 8g & move to countertop1 & the robot‚Äôs gripper is empty AND there is an unexplored \\textless{}receptacle\\textgreater{} that is commonly associated with the \\textless{}object\\textgreater{} \\\\ \\cline{2-5} find a/an kettle & & & & \\\\ \\cline{2-5} & 8h & move to countertop3 & the robot‚Äôs gripper is empty AND there is an unexplored \\textless{}receptacle\\textgreater{} that is commonly associated with the \\textless{}object\\textgreater{} \\\\ \\cline{2-5} & 8i & pick up kettle & the \\textless{}object\\textgreater{} is located in a/an \\textless{}receptacle\\textgreater{} AND the robot is in front of the \\textless{}receptacle\\textgreater{} robot is in front of the \\textless{}receptacle\\textgreater{} \\\\ \\cline{2-5} & 8j & done & the robot‚Äôs gripper has \\textless{}object\\textgreater{} \\\\ \\hline pick and place a/an kettle & 8j & move to sinkbasin & the robot‚Äôs gripper has the \\textless{}object\\textgreater{} AND the robot is not at the \\textless{}receptacle\\textgreater{} at the \\textless{}receptacle\\textgreater{} \\\\ \\cline{2-5} in/on a/an sinkbasin & & & & \\\\ \\cline{2-5} & 8k & put on sinkbasin & the robot is holding the \\textless{}object\\textgreater{} AND the robot is in front of the \\textless{}receptacle\\textgreater{} AND the \\textless{}receptacle\\textgreater{} is empty \\\\ \\cline{2-5} & 8l & done & the \\textless{}object\\textgreater{} is already in the \\textless{}receptacle\\textgreater{} AND the robot‚Äôs gripper is empty \\\\ \\hline \\end{tabular} \\end{table} Table 2: Action history of the agent completing ‚Äúpick up and place a/an kettle in/on a/an sinkbasin‚Äù. Due to space constraints, the object names are simplified and the task matching is omitted from the preconditions."
    }
  ]
}