{
  "title": "Advancing bioinformatics with large language models: components, applications and perspectives",
  "authors": [
    "Jiajia Liu",
    "Mengyuan Yang",
    "Yankai Yu",
    "Haixia Xu",
    "Tiangang Wang",
    "Kang Li",
    "Ph.D Xiaobo Zhou",
    "Fannin St"
  ],
  "abstract": "\n Large language models (LLMs) are a class of artificial intelligence models based on deep learning, which have great performance in various tasks, especially in natural language processing (NLP). Large language models typically consist of artificial neural networks with numerous parameters, trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. However, their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language. In this review, we will provide a comprehensive overview of the essential components of large language models (LLMs) in bioinformatics, spanning genomics, transcriptomics, proteomics, drug discovery, and single-cell analysis. Key aspects covered include tokenization methods for diverse data types, the architecture of transformer models, the core attention mechanism, and the pre-training processes underlying these models. Additionally, we will introduce currently available foundation models and highlight their downstream applications across various bioinformatics domains. Finally, drawing from our experience, we will offer practical guidance for both LLM users and developers, emphasizing strategies to optimize their use and foster further innovation in the field. Additionally, specialized tokens such as ' [IND]' can be introduced to mark the start or end of sequences or to handle unknown characters or gaps, as demonstrated in RNAErnie [9]. In protein language models, the input data primarily includes multiple sequence alignments (MSAs), protein sequences, biomedical/biological text, and cDNA. The basic units of MSAs and protein sequences are amino acids, leading most protein language models to use Single Amino Acid Tokenization, where protein sequences are segmented into individual amino acids. This approach is akin to the k-mers method used for DNA and RNA sequences and is employed in models such as , ProtTrans [11], and ProGen [12]. For biomedical and biological text, including general descriptions, conditioning tags in generative models, and resources like Gene Ontology (GO), tokenization methods from natural language processing (NLP) are widely used. Methods like WordPiece Tokenization build vocabulary using frequency-based greedy algorithms and segment text into discrete tokens, as demonstrated in ProtST [13]. For cDNA data, tokenization is similar to that of protein sequences but differs in the basic unit. Instead of amino acids, sequences are tokenized into codons, or triplets of nucleotides, as seen in CaLM [14]. In drug discovery, small molecule drugs account for 98% of commonly used medications [1]. LLMs leverage four main tokenization methods to uncover molecular patterns and drug-target interactions. Atom-level tokenization treats molecules as sequences of individual atoms, analogous to character-level text representation, as seen in K-BERT [15]. MolGPT [16]  utilizes a SMILES tokenizer that segments molecular structures into units such as atoms, bond types, and ring markers. A Graph-based VQ-VAE approach enhances this by encoding atoms into context-aware discrete values, distinguishing roles like aldehyde versus ester carbons, based on latent codes derived from a graph-based Vector Quantized Variational Autoencoder (VQ-VAE). This method categorizes atoms into chemically meaningful sub-classes, enriching the molecular vocabulary. Fingerprint tokens, another method, represent molecules through binary or numerical vectors summarizing molecular properties or structural patterns, as seen in SMILES-BERT [17]. Tokenization methods for single-cell profiles include four main strategies. Gene ranking/reindexing-based methods rank genes by expression levels and create tokens using ranked gene symbols or unique integer identifiers, as seen in Geneformer [18] and tGPT [19]. Binningbased methods divide gene expression into predefined intervals, assigning tokens based on the corresponding bin, used in models like scBERT [20] and scGPT [21]. Gene set or pathway-based methods group genes into biologically meaningful sets, such as pathways or Gene Ontology terms, with tokens representing the activation of these sets, exemplified by TOSICA [22]. Patch-based methods segment gene expression vectors into equal-sized sub-vectors, as seen in CIForm [152]. Alternatively, convolutional neural networks (CNNs) can be used to transform the reshaped gene expression matrix into several flattened 2D patches, as demonstrated by scTranSort [23]. Another variation involves reshaping the sub-vectors into a gene expression matrix after segmentation, as employed in scCLIP [24]. In addition to the four methods mentioned above, a more direct approach involves projecting gene expression directly, as seen in models like scFoundation [25], and scMulan [26]. Alternatively, some methods tokenize cells instead of genes, as exemplified by models such as CellPLM [27], ScRAT [28], and mcBERT [29], which utilize cell tokens during model training (Table  1 ). These strategies allow models to capture biological structure and variability, tailoring tokenization to single-cell data characteristics. After tokenization, embedding converts tokens into continuous vector representations, capturing the semantic relationships between them. Positional encoding represents the token order by adding vectors that encode the relative or absolute positions of tokens in the sequence. The final step involves combining the token embeddings with the positional embeddings to create a unified input embedding, which is then fed into the model for further processing (Figure  2b ). \n Architecture of transformer models Transformers are the foundational architecture in large language models (LLMs) and consist of two main components: the encoder and the decoder. The encoder takes the input data and processes it in parallel across multiple layers to capture relationships within the sequence. The decoder, on the other hand, generates output sequences based on the encoder's processed information, typically used in tasks like translation or text generation. Each component is built on layers of multi-head attention, add and norm layer, and feed-forward layer (Figure  2c ). \n Attention Mechanism: A key innovation of the transformer is the attention mechanism, particularly self-attention [3], which allows the model to weigh the importance of different tokens in a sequence relative to each other. In self-attention, each token computes a score based on how much attention it should pay to other tokens in the sequence. This is done by calculating three key components: Query (Q), Key (K), and Value (V) vectors for each token (Figure  2d ). The attention score is computed as the dot product between the Query of one token and the Key of another token, followed by a softmax operation to normalize the scores. These scores are then used to weight the \n",
  "references": [
    {
      "id": null,
      "title": "Advancing bioinformatics with large language models: components, applications and perspectives",
      "authors": [
        "Jiajia Liu",
        "Mengyuan Yang",
        "Yankai Yu",
        "Haixia Xu",
        "Tiangang Wang",
        "Kang Li",
        "Xiaobo Zhou",
        "Fannin St"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Advances in neural information processing systems",
      "authors": [
        "A Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Interpretable RNA foundation model from unannotated data for highly accurate RNA structure and function predictions",
      "authors": [
        "J Chen"
      ],
      "year": "2022",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Multiple sequence alignment-based RNA language model and its application to structural inference",
      "authors": [
        "Y Zhang"
      ],
      "year": "2024",
      "venue": "Nucleic Acids Research",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
      "authors": [
        "Y Ji"
      ],
      "year": "2021",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks",
      "authors": [
        "D Zhang"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Informative RNA base embedding for RNA structural alignment and clustering by deep representation learning",
      "authors": [
        "M Akiyama",
        "Y Sakakibara"
      ],
      "year": "2022",
      "venue": "NAR genomics and bioinformatics",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Multi-purpose RNA language modelling with motif-aware pretraining and typeguided fine-tuning",
      "authors": [
        "N Wang"
      ],
      "year": "2024",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
      "authors": [
        "A Rives"
      ],
      "year": "2019",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "ProtTrans: Towards Cracking the Language of Lifes Code Through Self-Supervised Deep Learning and High Performance Computing",
      "authors": [
        "A Elnaggar"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Progen: Language modeling for protein generation",
      "authors": [
        "A Madani"
      ],
      "year": "2020",
      "venue": "Progen: Language modeling for protein generation",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Protst: Multi-modality learning of protein sequences and biomedical texts",
      "authors": [
        "M Xu"
      ],
      "year": "2023",
      "venue": "Protst: Multi-modality learning of protein sequences and biomedical texts",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Codon language embeddings provide strong signals for use in protein engineering",
      "authors": [
        "C Outeiral",
        "C M Deane"
      ],
      "year": "2024",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Knowledge-based BERT: a method to extract molecular features like computational chemists",
      "authors": [
        "Z Wu"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "MolGPT: molecular generation using a transformer-decoder model",
      "authors": [
        "V Bagal"
      ],
      "year": "2021",
      "venue": "Journal of Chemical Information and Modeling",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction",
      "authors": [
        "S Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Transfer learning enables predictions in network biology",
      "authors": [
        "C V Theodoris"
      ],
      "year": "2023",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Generative pretraining from large-scale transcriptomes for single-cell deciphering. iScience",
      "authors": [
        "H Shen"
      ],
      "year": "2023",
      "venue": "Generative pretraining from large-scale transcriptomes for single-cell deciphering. iScience",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data",
      "authors": [
        "F Yang"
      ],
      "year": "",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "scGPT: toward building a foundation model for single-cell multi-omics using generative AI",
      "authors": [
        "H Cui"
      ],
      "year": "2024",
      "venue": "Nat Methods",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Transformer for one stop interpretable cell type annotation",
      "authors": [
        "J Chen"
      ],
      "year": "2023",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "scTransSort: Transformers for Intelligent Annotation of Cell Types by Gene Embeddings",
      "authors": [
        "L Jiao"
      ],
      "year": "2023",
      "venue": "Biomolecules",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "scCLIP: Multi-modal Single-cell Contrastive Learning Integration Pre-training",
      "authors": [
        "L Xiong",
        "T Chen",
        "M Kellis"
      ],
      "year": "",
      "venue": "NeurIPS 2023 AI for Science Workshop",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Large-scale foundation model on single-cell transcriptomics",
      "authors": [
        "M Hao"
      ],
      "year": "2024",
      "venue": "Nat Methods",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "scMulan: a multitask generative pre-trained language model for single-cell analysis",
      "authors": [
        "H Bian"
      ],
      "year": "2024",
      "venue": "International Conference on Research in Computational Molecular Biology",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "CellPLM: pre-training of cell language model beyond single cells",
      "authors": [
        "H Wen"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Phenotype prediction from single-cell RNA-seq data using attention-based neural networks",
      "authors": [
        "Y Mao"
      ],
      "year": "2024",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "mcBERT: Patient-Level Single-cell Transcriptomics Data Representation",
      "authors": [
        "B V Querfurth"
      ],
      "year": "2024",
      "venue": "mcBERT: Patient-Level Single-cell Transcriptomics Data Representation",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Decoding\" coding\": Information and DNA",
      "authors": [
        "S Sarkar"
      ],
      "year": "1996",
      "venue": "BioScience",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "The nucleotide transformer: Building and evaluating robust foundation models for human genomics",
      "authors": [
        "H Dalla-Torre"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "DNA language models are powerful predictors of genomewide variant effects",
      "authors": [
        "G Benegas",
        "S S Batra",
        "Y S Song"
      ],
      "year": "2023",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Dnabert-2: Efficient foundation model and benchmark for multi-species genome",
      "authors": [
        "Z Zhou"
      ],
      "year": "2023",
      "venue": "Dnabert-2: Efficient foundation model and benchmark for multi-species genome",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "DNA language model GROVER learns sequence context in the human genome",
      "authors": [
        "M Sanabria"
      ],
      "year": "2024",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "UNI-RNA: universal pre-trained models revolutionize RNA research",
      "authors": [
        "X Wang"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Self-supervised learning on millions of primary RNA sequences from 72 vertebrates improves sequence-based RNA splicing prediction",
      "authors": [
        "K Chen"
      ],
      "year": "2024",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Deciphering 3'UTR Mediated Gene Regulation Using Interpretable Deep Representation Learning",
      "authors": [
        "Y Yang"
      ],
      "year": "2024",
      "venue": "Advanced Science",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "A 5â€² UTR language model for decoding untranslated regions of mRNA and function predictions",
      "authors": [
        "Y Chu"
      ],
      "year": "2024",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Evaluating protein transfer learning with TAPE. Advances in neural information processing systems",
      "authors": [
        "R Rao"
      ],
      "year": "2019",
      "venue": "Evaluating protein transfer learning with TAPE. Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
      "authors": [
        "A Rives"
      ],
      "year": "2021",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "ProtGPT2 is a deep unsupervised language model for protein design",
      "authors": [
        "N Ferruz",
        "S Schmidt",
        "B Hcker"
      ],
      "year": "2022",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "ProteinBERT: a universal deep-learning model of protein sequence and function",
      "authors": [
        "N Brandes"
      ],
      "year": "2022",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling",
      "authors": [
        "H.-Y Zhou"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Varnek, Estimation of the size of drug-like chemical space based on GDB-17 data",
      "authors": [
        "P G Polishchuk",
        "T I Madzhidov",
        "A J"
      ],
      "year": "2013",
      "venue": "Varnek, Estimation of the size of drug-like chemical space based on GDB-17 data",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "RETHINKING PRE-TRAINING GRAPH NEURAL NETWORKS FOR MOLECULES",
      "authors": [
        "Mole-Bert"
      ],
      "year": "",
      "venue": "RETHINKING PRE-TRAINING GRAPH NEURAL NETWORKS FOR MOLECULES",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Smiles-Bert",
      "authors": [
        "S Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
      "authors": [
        "V Bagal"
      ],
      "year": "2022",
      "venue": "J Chem Inf Model",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design",
      "authors": [],
      "year": "",
      "venue": "SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Universal cell embeddings: A foundation model for cell biology",
      "authors": [
        "Y Rosen"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "CancerFoundation: A single-cell RNA sequencing foundation model to decipher drug resistance in cancer",
      "authors": [
        "A Theus"
      ],
      "year": "2024",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "GeneCompass: deciphering universal gene regulatory mechanisms with a knowledge-informed cross-species foundation model",
      "authors": [
        "X Yang"
      ],
      "year": "2024",
      "venue": "Cell Res",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "scPRINT: pre-training on 50 million cells allows robust gene network predictions",
      "authors": [
        "J Kalfon"
      ],
      "year": "2024",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Nicheformer: a foundation model for single-cell and spatial omics",
      "authors": [
        "A Schaar"
      ],
      "year": "2024",
      "venue": "Nicheformer: a foundation model for single-cell and spatial omics",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "DNA structure, mutations, and human genetic disease",
      "authors": [
        "R R Sinden",
        "R D Wells"
      ],
      "year": "1992",
      "venue": "Current opinion in biotechnology",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Cis-regulatory elements: molecular mechanisms and evolutionary processes underlying divergence",
      "authors": [
        "P J Wittkopp",
        "G Kalay"
      ],
      "year": "2012",
      "venue": "Nature Reviews Genetics",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Identification of putative promoters in 48 eukaryotic genomes on the basis of DNA free energy",
      "authors": [
        "V R Yella",
        "A Kumar",
        "M Bansal"
      ],
      "year": "2018",
      "venue": "Scientific reports",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "BERT-Promoter: An improved sequence-based predictor of DNA promoter using BERT pre-trained model and SHAP feature selection",
      "authors": [
        "N Q K Le"
      ],
      "year": "2022",
      "venue": "Computational Biology and Chemistry",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Enhancers in disease: molecular basis and emerging treatment strategies",
      "authors": [
        "A Claringbould",
        "J B Zaugg"
      ],
      "year": "2021",
      "venue": "Trends in Molecular Medicine",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Genome-wide enhancer maps link risk variants to disease genes",
      "authors": [
        "J Nasser"
      ],
      "year": "2021",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "iEnhancer-BERT: A novel transfer learning architecture based on DNA-Language model for identifying enhancers and their strength",
      "authors": [
        "H Luo"
      ],
      "year": "2022",
      "venue": "International Conference on Intelligent Computing",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "DNA-protein interaction studies: a historical and comparative analysis",
      "authors": [
        "R A C Ferraz"
      ],
      "year": "2021",
      "venue": "Plant Methods",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Improving language model of human genome for DNA-protein binding prediction based on task-specific pre-training",
      "authors": [
        "H Luo"
      ],
      "year": "2023",
      "venue": "Interdisciplinary Sciences: Computational Life Sciences",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "MoDNA: motif-oriented pre-training for DNA language model",
      "authors": [
        "W An"
      ],
      "year": "2022",
      "venue": "Proceedings of the 13th ACM International Conference on Bioinformatics",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "DNA methylation and its basic function",
      "authors": [
        "L D Moore",
        "T Le",
        "G Fan"
      ],
      "year": "2013",
      "venue": "Neuropsychopharmacology",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Comprehensive analysis of DNA 5-methylcytosine and N6-adenine methylation by nanopore sequencing in hepatocellular carcinoma",
      "authors": [
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Frontiers in cell and developmental biology",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "BERT6mA: prediction of DNA N6-methyladenine site using deep learningbased approaches",
      "authors": [
        "S Tsukiyama"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "iDNA-ABT: advanced deep learning model for detecting DNA methylation with adaptive features and transductive information maximization",
      "authors": [
        "Y Yu"
      ],
      "year": "2021",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "iDNA-ABF: multi-scale deep biological language learning model for the interpretable prediction of DNA methylations",
      "authors": [
        "J Jin"
      ],
      "year": "2022",
      "venue": "Genome biology",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "MuLan-Methyl-Multiple Transformer-based Language Models for Accurate DNA Methylation Prediction",
      "authors": [
        "W Zeng",
        "A Gautam",
        "D H Huson"
      ],
      "year": "",
      "venue": "MuLan-Methyl-Multiple Transformer-based Language Models for Accurate DNA Methylation Prediction",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh"
      ],
      "year": "2019",
      "venue": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "Albert: A lite bert for self-supervised learning of language representations",
      "authors": [
        "Z Lan"
      ],
      "year": "2019",
      "venue": "Albert: A lite bert for self-supervised learning of language representations",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang"
      ],
      "year": "2019",
      "venue": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "Electra: Pre-training text encoders as discriminators rather than generators",
      "authors": [
        "K Clark"
      ],
      "year": "2020",
      "venue": "Electra: Pre-training text encoders as discriminators rather than generators",
      "doi": ""
    },
    {
      "id": "b73",
      "title": "RNA splicing by the spliceosome. Annual review of biochemistry",
      "authors": [
        "M E Wilkinson",
        "C Charenton",
        "K Nagai"
      ],
      "year": "2020",
      "venue": "RNA splicing by the spliceosome. Annual review of biochemistry",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "Advances and opportunities in RNA structure experimental determination and computational modeling",
      "authors": [
        "J Zhang"
      ],
      "year": "2022",
      "venue": "Nature Methods",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "Dynamic methylome of internal mRNA N 7-methylguanosine and its regulatory role in translation",
      "authors": [
        "L Malbec"
      ],
      "year": "2019",
      "venue": "Cell research",
      "doi": ""
    },
    {
      "id": "b76",
      "title": "LncCat: An ORF attention model to identify LncRNA based on ensemble learning strategy and fused sequence information",
      "authors": [
        "H Feng"
      ],
      "year": "2023",
      "venue": "Computational and Structural Biotechnology Journal",
      "doi": ""
    },
    {
      "id": "b77",
      "title": "A multi-granularity information-enhanced pre-training method for predicting the coding potential of sORFs in plant lncRNAs",
      "authors": [
        "S Xia"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
      "doi": ""
    },
    {
      "id": "b78",
      "title": "Prediction of RNA-protein interactions using a nucleotide language model",
      "authors": [
        "K Yamada",
        "M Hamada"
      ],
      "year": "2022",
      "venue": "Bioinformatics Advances",
      "doi": ""
    },
    {
      "id": "b79",
      "title": "Recent deep learning methodology development for RNA-RNA interaction prediction. Symmetry",
      "authors": [
        "Y Fang",
        "X Pan",
        "H.-B Shen"
      ],
      "year": "2022",
      "venue": "Recent deep learning methodology development for RNA-RNA interaction prediction. Symmetry",
      "doi": ""
    },
    {
      "id": "b80",
      "title": "The functional role of long non-coding RNA in human carcinomas",
      "authors": [
        "E A Gibb",
        "C J Brown",
        "W L Lam"
      ],
      "year": "2011",
      "venue": "Molecular cancer",
      "doi": ""
    },
    {
      "id": "b81",
      "title": "BERT-m7G: a transformer architecture based on BERT and stacking ensemble to identify RNA N7-Methylguanosine sites from sequence information",
      "authors": [
        "L Zhang"
      ],
      "year": "2021",
      "venue": "Computational and Mathematical Methods in Medicine",
      "doi": ""
    },
    {
      "id": "b82",
      "title": "BERT2OME: Prediction of 2'-O-methylation Modifications from RNA Sequence by Transformer Architecture Based on BERT",
      "authors": [
        "N N Soylu",
        "E Sefer"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
      "doi": ""
    },
    {
      "id": "b83",
      "title": "mRNA vaccines-a new era in vaccinology",
      "authors": [
        "N Pardi"
      ],
      "year": "2018",
      "venue": "Nature reviews Drug discovery",
      "doi": ""
    },
    {
      "id": "b84",
      "title": "CodonBERT: Using BERT for Sentiment Analysis to Better Predict Genes with Low Expression",
      "authors": [
        "A N Babjac",
        "Z Lu",
        "S J Emrich"
      ],
      "year": "2023",
      "venue": "Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics",
      "doi": ""
    },
    {
      "id": "b85",
      "title": "Integrated mRNA sequence optimization using deep learning",
      "authors": [
        "H Gong"
      ],
      "year": "2023",
      "venue": "Brief Bioinform",
      "doi": ""
    },
    {
      "id": "b86",
      "title": "Protein design via deep learning",
      "authors": [
        "W Ding",
        "K Nakai",
        "H Gong"
      ],
      "year": "2022",
      "venue": "Briefings in bioinformatics",
      "doi": ""
    },
    {
      "id": "b87",
      "title": "Artificial intelligence-aided protein engineering: from topological data analysis to deep protein language models",
      "authors": [
        "Y Qiu",
        "G.-W Wei"
      ],
      "year": "2023",
      "venue": "Artificial intelligence-aided protein engineering: from topological data analysis to deep protein language models",
      "doi": ""
    },
    {
      "id": "b88",
      "title": "Observed antibody space: a resource for data mining next-generation sequencing of antibody repertoires",
      "authors": [
        "A Kovaltsuk"
      ],
      "year": "2018",
      "venue": "The Journal of Immunology",
      "doi": ""
    },
    {
      "id": "b89",
      "title": "AI-based protein structure prediction in drug discovery: impacts and challenges",
      "authors": [
        "M Schauperl",
        "R A Denny"
      ],
      "year": "2022",
      "venue": "Journal of Chemical Information and Modeling",
      "doi": ""
    },
    {
      "id": "b90",
      "title": "The AlphaFold database of protein structures: a biologist's guide",
      "authors": [
        "A David"
      ],
      "year": "2022",
      "venue": "Journal of molecular biology",
      "doi": ""
    },
    {
      "id": "b91",
      "title": "MSA transformer",
      "authors": [
        "R M Rao"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b92",
      "title": "Transformer-xl: Attentive language models beyond a fixed-length context",
      "authors": [
        "Z Dai"
      ],
      "year": "2019",
      "venue": "Transformer-xl: Attentive language models beyond a fixed-length context",
      "doi": ""
    },
    {
      "id": "b93",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b94",
      "title": "UniProt: the universal protein knowledgebase in 2021",
      "authors": [],
      "year": "",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b95",
      "title": "Clustering huge protein sequence sets in linear time",
      "authors": [
        "M Steinegger",
        "J Sding"
      ],
      "year": "2018",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b96",
      "title": "Deep generative modeling for protein design",
      "authors": [
        "A Strokach",
        "P M Kim"
      ],
      "year": "2022",
      "venue": "Current opinion in structural biology",
      "doi": ""
    },
    {
      "id": "b97",
      "title": "Controllable protein design with language models",
      "authors": [
        "N Ferruz",
        "B Hcker"
      ],
      "year": "",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b98",
      "title": "ColabFold: making protein folding accessible to all",
      "authors": [
        "M Mirdita"
      ],
      "year": "2022",
      "venue": "Nature methods",
      "doi": ""
    },
    {
      "id": "b99",
      "title": "Highly accurate protein structure prediction with AlphaFold",
      "authors": [
        "J Jumper"
      ],
      "year": "2021",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b100",
      "title": "I-TASSER-MTD: a deep-learning-based platform for multi-domain protein structure and function prediction",
      "authors": [
        "X Zhou"
      ],
      "year": "2022",
      "venue": "Nature Protocols",
      "doi": ""
    },
    {
      "id": "b101",
      "title": "From sequence to function through structure: Deep learning for protein design",
      "authors": [
        "N Ferruz"
      ],
      "year": "2023",
      "venue": "Computational and Structural Biotechnology Journal",
      "doi": ""
    },
    {
      "id": "b102",
      "title": "Protst: Multi-modality learning of protein sequences and biomedical texts",
      "authors": [
        "M Xu"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b103",
      "title": "Codon-specific Ramachandran plots show amino acid backbone conformation depends on identity of the translated codon",
      "authors": [
        "A A Rosenberg",
        "A Marx",
        "A M Bronstein"
      ],
      "year": "2022",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b104",
      "title": "Protein post-translational modifications in the regulation of cancer hallmarks",
      "authors": [
        "H Wang"
      ],
      "year": "2023",
      "venue": "Cancer Gene Therapy",
      "doi": ""
    },
    {
      "id": "b105",
      "title": "Current status of PTMs structural databases: applications, limitations and prospects",
      "authors": [
        "A G De Brevern",
        "J Rebehmed"
      ],
      "year": "2022",
      "venue": "Amino Acids",
      "doi": ""
    },
    {
      "id": "b106",
      "title": "Insertions and deletions in protein evolution and engineering",
      "authors": [
        "S Savino",
        "T Desmet",
        "J Franceus"
      ],
      "year": "2022",
      "venue": "Biotechnology Advances",
      "doi": ""
    },
    {
      "id": "b107",
      "title": "Recent advances in machine learning variant effect prediction tools for protein engineering. Industrial \\& engineering chemistry research",
      "authors": [
        "J Horne",
        "D Shukla"
      ],
      "year": "2022",
      "venue": "Recent advances in machine learning variant effect prediction tools for protein engineering. Industrial \\& engineering chemistry research",
      "doi": ""
    },
    {
      "id": "b108",
      "title": "Unified rational protein engineering with sequence-based deep representation learning",
      "authors": [
        "E C Alley"
      ],
      "year": "2019",
      "venue": "Nature methods",
      "doi": ""
    },
    {
      "id": "b109",
      "title": "PLMSearch: Protein language model powers accurate and fast sequence search for remote homology",
      "authors": [
        "W Liu"
      ],
      "year": "2024",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b110",
      "title": "Fast, sensitive detection of protein homologs using deep dense retrieval",
      "authors": [
        "L Hong"
      ],
      "year": "2024",
      "venue": "Nature Biotechnology",
      "doi": ""
    },
    {
      "id": "b111",
      "title": "Artificial intelligence challenges for predicting the impact of mutations on protein stability. Current opinion in structural biology",
      "authors": [
        "F Pucci",
        "M Schwersensky",
        "M Rooman"
      ],
      "year": "2022",
      "venue": "Artificial intelligence challenges for predicting the impact of mutations on protein stability. Current opinion in structural biology",
      "doi": ""
    },
    {
      "id": "b112",
      "title": "Multi-level Protein Structure Pre-training via Prompt Learning",
      "authors": [
        "Z Wang"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b113",
      "title": "Machine learning on protein--protein interaction prediction: models, challenges and trends",
      "authors": [
        "T Tang"
      ],
      "year": "2023",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b114",
      "title": "Recent advances in predicting and modeling protein--protein interactions",
      "authors": [
        "J Durham"
      ],
      "year": "2023",
      "venue": "Trends in biochemical sciences",
      "doi": ""
    },
    {
      "id": "b115",
      "title": "Ontoprotein: Protein pretraining with gene ontology embedding",
      "authors": [
        "N Zhang"
      ],
      "year": "2022",
      "venue": "Ontoprotein: Protein pretraining with gene ontology embedding",
      "doi": ""
    },
    {
      "id": "b116",
      "title": "Immunobiology: the immune system in health and disease",
      "authors": [
        "C Janeway"
      ],
      "year": "2001",
      "venue": "Immunobiology: the immune system in health and disease",
      "doi": ""
    },
    {
      "id": "b117",
      "title": "T cell epitope predictions",
      "authors": [
        "B Peters",
        "M Nielsen",
        "A J A R O I Sette"
      ],
      "year": "2020",
      "venue": "T cell epitope predictions",
      "doi": ""
    },
    {
      "id": "b118",
      "title": "MHCflurry 2.0: improved pan-allele prediction of MHC class I-presented peptides by incorporating antigen processing",
      "authors": [
        "T J O'donnell",
        "A Rubinsteyn",
        "U J C S Laserson"
      ],
      "year": "2020",
      "venue": "MHCflurry 2.0: improved pan-allele prediction of MHC class I-presented peptides by incorporating antigen processing",
      "doi": ""
    },
    {
      "id": "b119",
      "title": "MHCRoBERTa: pan-specific peptide-MHC class I binding prediction through transfer learning with label-agnostic protein sequences",
      "authors": [
        "F Wang"
      ],
      "year": "2022",
      "venue": "Brief Bioinform",
      "doi": ""
    },
    {
      "id": "b120",
      "title": "BERTMHC: improved MHC-peptide class II interaction prediction with transformer and multiple instance learning",
      "authors": [
        "J Cheng"
      ],
      "year": "2021",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b121",
      "title": "TCR-BERT: learning the grammar of T-cell receptors for flexible antigenbinding analyses",
      "authors": [
        "K Wu"
      ],
      "year": "2021",
      "venue": "TCR-BERT: learning the grammar of T-cell receptors for flexible antigenbinding analyses",
      "doi": ""
    },
    {
      "id": "b122",
      "title": "SC-AIR-BERT: a pre-trained single-cell model for predicting the antigen-binding specificity of the adaptive immune receptor",
      "authors": [
        "Y Zhao"
      ],
      "year": "2023",
      "venue": "Brief Bioinform",
      "doi": ""
    },
    {
      "id": "b123",
      "title": "AntiFormer: graph enhanced large language model for binding affinity prediction",
      "authors": [
        "Q Wang"
      ],
      "year": "2024",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b124",
      "title": "AbLang: an antibody language model for completing antibody sequences",
      "authors": [
        "T H Olsen",
        "I H Moal",
        "C M Deane"
      ],
      "year": "2022",
      "venue": "Bioinformatics Advances",
      "doi": ""
    },
    {
      "id": "b125",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "doi": ""
    },
    {
      "id": "b126",
      "title": "Deciphering the language of antibodies using self-supervised learning",
      "authors": [
        "J Leem"
      ],
      "year": "2022",
      "venue": "Patterns",
      "doi": ""
    },
    {
      "id": "b127",
      "title": "On pre-trained language models for antibody",
      "authors": [
        "D Wang",
        "F Ye",
        "H Zhou"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b128",
      "title": "Deep learning in drug discovery: an integrative review and future challenges",
      "authors": [
        "H Askr"
      ],
      "year": "2023",
      "venue": "Artificial Intelligence Review",
      "doi": ""
    },
    {
      "id": "b129",
      "title": "High content cellular imaging for drug development",
      "authors": [
        "Z Xiaobo",
        "S T C Wong"
      ],
      "year": "2006",
      "venue": "IEEE Signal Processing Magazine",
      "doi": ""
    },
    {
      "id": "b130",
      "title": "Multi-scale agent-based brain cancer modeling and prediction of TKI treatment response: incorporating EGFR signaling pathway and angiogenesis",
      "authors": [
        "X Sun"
      ],
      "year": "2012",
      "venue": "BMC Bioinformatics",
      "doi": ""
    },
    {
      "id": "b131",
      "title": "The evolution of commercial drug delivery technologies",
      "authors": [
        "A M Vargason",
        "A C Anselmo",
        "S J N B E Mitragotri"
      ],
      "year": "2021",
      "venue": "The evolution of commercial drug delivery technologies",
      "doi": ""
    },
    {
      "id": "b132",
      "title": "The influence of drug-like concepts on decision-making in medicinal chemistry",
      "authors": [
        "P D Leeson",
        "B J N R D D Springthorpe"
      ],
      "year": "2007",
      "venue": "The influence of drug-like concepts on decision-making in medicinal chemistry",
      "doi": ""
    },
    {
      "id": "b133",
      "title": "Structure-Based Drug Discovery with Deep Learning",
      "authors": [
        "R Ozcelik"
      ],
      "year": "2023",
      "venue": "Chembiochem",
      "doi": ""
    },
    {
      "id": "b134",
      "title": "Deep learning methods for molecular representation and property prediction",
      "authors": [
        "Z Li"
      ],
      "year": "2022",
      "venue": "Drug Discovery Today",
      "doi": ""
    },
    {
      "id": "b135",
      "title": "Artificial intelligence for drug discovery: Resources, methods, and applications",
      "authors": [
        "W Chen"
      ],
      "year": "2023",
      "venue": "Molecular Therapy-Nucleic Acids",
      "doi": ""
    },
    {
      "id": "b136",
      "title": "ChemBERTa: large-scale self-supervised pretraining for molecular property prediction",
      "authors": [
        "S Chithrananda",
        "G Grand",
        "B Ramsundar"
      ],
      "year": "2020",
      "venue": "ChemBERTa: large-scale self-supervised pretraining for molecular property prediction",
      "doi": ""
    },
    {
      "id": "b137",
      "title": "ChemBERTa-2: Towards Chemical Foundation Models",
      "authors": [],
      "year": "",
      "venue": "ChemBERTa-2: Towards Chemical Foundation Models",
      "doi": ""
    },
    {
      "id": "b138",
      "title": "Mole-bert: Rethinking pre-training graph neural networks for molecules",
      "authors": [
        "J Xia"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b139",
      "title": "Generative models for molecular discovery: Recent advances and challenges",
      "authors": [
        "C Bilodeau"
      ],
      "year": "2022",
      "venue": "Wiley Interdisciplinary Reviews: Computational Molecular Science",
      "doi": ""
    },
    {
      "id": "b140",
      "title": "De novo molecular design and generative models",
      "authors": [
        "J Meyers",
        "B Fabian",
        "N Brown"
      ],
      "year": "2021",
      "venue": "Drug Discovery Today",
      "doi": ""
    },
    {
      "id": "b141",
      "title": "Deep learning in drug target interaction prediction: current and future perspectives",
      "authors": [
        "K Abbasi"
      ],
      "year": "2021",
      "venue": "Current Medicinal Chemistry",
      "doi": ""
    },
    {
      "id": "b142",
      "title": "Graph neural network approaches for drug-target interactions",
      "authors": [
        "Z Zhang"
      ],
      "year": "2022",
      "venue": "Current Opinion in Structural Biology",
      "doi": ""
    },
    {
      "id": "b143",
      "title": "DTI-BERT: identifying drug-target interactions in cellular networking based on BERT and deep learning method",
      "authors": [
        "J Zheng",
        "X Xiao",
        "W.-R Qiu"
      ],
      "year": "2022",
      "venue": "Frontiers in Genetics",
      "doi": ""
    },
    {
      "id": "b144",
      "title": "TransDTI: transformer-based language models for estimating DTIs and building a drug recommendation workflow",
      "authors": [
        "Y Kalakoti",
        "S Yadav",
        "D Sundar"
      ],
      "year": "2022",
      "venue": "ACS omega",
      "doi": ""
    },
    {
      "id": "b145",
      "title": "Fine-tuning of bert model to accurately predict drug--target interactions",
      "authors": [
        "H Kang"
      ],
      "year": "2022",
      "venue": "Pharmaceutics",
      "doi": ""
    },
    {
      "id": "b146",
      "title": "Mitigating cold-start problems in drug-target affinity prediction with interaction knowledge transferring",
      "authors": [
        "T M Nguyen",
        "T Nguyen",
        "T Tran"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b147",
      "title": "Protein--ligand scoring with convolutional neural networks",
      "authors": [
        "M Ragoza"
      ],
      "year": "2017",
      "venue": "Journal of chemical information and modeling",
      "doi": ""
    },
    {
      "id": "b148",
      "title": "Structure-aware interactive graph neural networks for the prediction of protein-ligand binding affinity",
      "authors": [
        "S Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \\& Data Mining",
      "doi": ""
    },
    {
      "id": "b149",
      "title": "InteractionGraphNet: a novel and efficient deep graph representation learning framework for accurate protein--ligand interaction predictions",
      "authors": [
        "D Jiang"
      ],
      "year": "2021",
      "venue": "Journal of medicinal chemistry",
      "doi": ""
    },
    {
      "id": "b150",
      "title": "A point cloud-based deep learning strategy for protein--ligand binding affinity prediction",
      "authors": [
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b151",
      "title": "A randomized phase IIIB trial of chemotherapy, bevacizumab, and panitumumab compared with chemotherapy and bevacizumab alone for metastatic colorectal cancer",
      "authors": [
        "J R Hecht"
      ],
      "year": "2009",
      "venue": "A randomized phase IIIB trial of chemotherapy, bevacizumab, and panitumumab compared with chemotherapy and bevacizumab alone for metastatic colorectal cancer",
      "doi": ""
    },
    {
      "id": "b152",
      "title": "Chemotherapy, bevacizumab, and cetuximab in metastatic colorectal cancer",
      "authors": [
        "J Tol"
      ],
      "year": "2009",
      "venue": "Chemotherapy, bevacizumab, and cetuximab in metastatic colorectal cancer",
      "doi": ""
    },
    {
      "id": "b153",
      "title": "DCE-DForest: a deep forest model for the prediction of anticancer drug combination effects",
      "authors": [
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Computational and Mathematical Methods in Medicine",
      "doi": ""
    },
    {
      "id": "b154",
      "title": "DFFNDDS: prediction of synergistic drug combinations with dual feature fusion networks",
      "authors": [
        "M Xu"
      ],
      "year": "2023",
      "venue": "Journal of Cheminformatics",
      "doi": ""
    },
    {
      "id": "b155",
      "title": "A universal approach for integrating super large-scale single-cell transcriptomes by exploring gene rankings",
      "authors": [
        "H Shen"
      ],
      "year": "2022",
      "venue": "Brief Bioinform",
      "doi": ""
    },
    {
      "id": "b156",
      "title": "Large-scale cell representation learning via divide-and-conquer contrastive learning",
      "authors": [
        "S Zhao",
        "J Zhang",
        "Z Nie"
      ],
      "year": "2023",
      "venue": "Large-scale cell representation learning via divide-and-conquer contrastive learning",
      "doi": ""
    },
    {
      "id": "b157",
      "title": "GEARS: Predicting transcriptional outcomes of novel multigene perturbations",
      "authors": [
        "Y Roohani",
        "K Huang",
        "J Leskovec"
      ],
      "year": "2022",
      "venue": "BioRxiv",
      "doi": ""
    },
    {
      "id": "b158",
      "title": "A deep generative model for multi-view profiling of single-cell RNA-seq and ATAC-seq data",
      "authors": [
        "G Li"
      ],
      "year": "2022",
      "venue": "Genome Biol",
      "doi": ""
    },
    {
      "id": "b159",
      "title": "Single-cell biological network inference using a heterogeneous graph transformer",
      "authors": [
        "A Ma"
      ],
      "year": "2023",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b160",
      "title": "A pre-trained large language model for translating single-cell transcriptome to proteome",
      "authors": [
        "L Linjing"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b161",
      "title": "Single-cell multimodal prediction via transformers",
      "authors": [
        "W Tang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 32nd ACM International Conference on Information and Knowledge Management",
      "doi": ""
    },
    {
      "id": "b162",
      "title": "The STRING database in 2023: protein-protein association networks and functional enrichment analyses for any sequenced genome of interest",
      "authors": [
        "D Szklarczyk"
      ],
      "year": "2023",
      "venue": "Nucleic Acids Res",
      "doi": ""
    },
    {
      "id": "b163",
      "title": "Single cells are spatial tokens: Transformers for spatial transcriptomic data imputation",
      "authors": [
        "H Wen"
      ],
      "year": "2023",
      "venue": "Single cells are spatial tokens: Transformers for spatial transcriptomic data imputation",
      "doi": ""
    },
    {
      "id": "b164",
      "title": "Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis",
      "authors": [
        "W Hou",
        "Z Ji"
      ],
      "year": "2024",
      "venue": "Nat Methods",
      "doi": ""
    },
    {
      "id": "b165",
      "title": "GenePT: a simple but effective foundation model for genes and cells built from ChatGPT",
      "authors": [
        "Y Chen",
        "J Zou"
      ],
      "year": "2024",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b166",
      "title": "Cell2Sentence: teaching large language models the language of biology",
      "authors": [
        "D Levine"
      ],
      "year": "2023",
      "venue": "BioRxiv",
      "doi": ""
    },
    {
      "id": "b167",
      "title": "Langcell: Language-cell pre-training for cell identity understanding",
      "authors": [
        "S Zhao"
      ],
      "year": "2024",
      "venue": "Langcell: Language-cell pre-training for cell identity understanding",
      "doi": ""
    },
    {
      "id": "b168",
      "title": "scChat: A Large Language Model-Powered Co-Pilot for Contextualized Single-Cell RNA Sequencing Analysis",
      "authors": [
        "Y.-C Lu"
      ],
      "year": "2024",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b169",
      "title": "scelmo: Embeddings from language models are good learners for single-cell data analysis",
      "authors": [
        "T Liu"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b170",
      "title": "Scalable querying of human cell atlases via a foundational model reveals commonalities across fibrosis-associated macrophages",
      "authors": [
        "G Heimberg"
      ],
      "year": "2023",
      "venue": "BioRxiv",
      "doi": ""
    },
    {
      "id": "b171",
      "title": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
      "authors": [
        "Y Ji"
      ],
      "year": "2021",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b172",
      "title": "EPD and EPDnew, high-quality promoter resources in the next-generation sequencing era",
      "authors": [
        "R Dreos"
      ],
      "year": "2013",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b173",
      "title": "An integrated encyclopedia of DNA elements in the human genome",
      "authors": [
        "E P Consortium"
      ],
      "year": "2012",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b174",
      "title": "Nucleic acids research",
      "authors": [
        "F Cunningham"
      ],
      "year": "2019",
      "venue": "Ensembl",
      "doi": ""
    },
    {
      "id": "b175",
      "title": "dbSNP: the NCBI database of genetic variation",
      "authors": [
        "S T Sherry"
      ],
      "year": "2001",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b176",
      "title": "Dnabert-2: Efficient foundation model and benchmark for multi-species genome",
      "authors": [
        "Z Zhou"
      ],
      "year": "2023",
      "venue": "Dnabert-2: Efficient foundation model and benchmark for multi-species genome",
      "doi": ""
    },
    {
      "id": "b177",
      "title": "Convolutional neural network architectures for predicting DNA-protein binding",
      "authors": [
        "H Zeng"
      ],
      "year": "2016",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b178",
      "title": "Capturing large genomic contexts for accurately predicting enhancer-promoter interactions",
      "authors": [
        "K Chen",
        "H Zhao",
        "Y Yang"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b179",
      "title": "The nucleotide transformer: Building and evaluating robust foundation models for human genomics",
      "authors": [
        "H Dalla-Torre"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b180",
      "title": "",
      "authors": [
        "K L Howe"
      ],
      "year": "2021",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b181",
      "title": "Insights into human genetic variation and population history from 929 diverse genomes",
      "authors": [
        "A BergstrÃ¶m"
      ],
      "year": "2020",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b182",
      "title": "135 genomes reveal the global pattern of polymorphism in Arabidopsis thaliana",
      "authors": [
        "C Alonso-Blanco"
      ],
      "year": "2016",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b183",
      "title": "DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks",
      "authors": [
        "D Zhang"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b184",
      "title": "DeepGSR: an optimized deep-learning structure for the recognition of genomic signals and regions",
      "authors": [
        "M Kalkatawi"
      ],
      "year": "2019",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b185",
      "title": "Predicting mRNA abundance directly from genomic sequence using deep convolutional neural networks",
      "authors": [
        "V Agarwal",
        "J Shendure"
      ],
      "year": "2020",
      "venue": "Cell reports",
      "doi": ""
    },
    {
      "id": "b186",
      "title": "DNA language model GROVER learns sequence context in the human genome",
      "authors": [
        "M Sanabria"
      ],
      "year": "2024",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b187",
      "title": "The ENCODE project",
      "authors": [
        "N De Souza"
      ],
      "year": "2012",
      "venue": "Nature methods",
      "doi": ""
    },
    {
      "id": "b188",
      "title": "DNA language models are powerful zero-shot predictors of genome-wide variant effects",
      "authors": [
        "G Benegas",
        "S S Batra",
        "Y S Song"
      ],
      "year": "2022",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b189",
      "title": "BERT-Promoter: An improved sequence-based predictor of DNA promoter using BERT pre-trained model and SHAP feature selection",
      "authors": [
        "N Q K Le"
      ],
      "year": "2022",
      "venue": "Computational Biology and Chemistry",
      "doi": ""
    },
    {
      "id": "b190",
      "title": "RegulonDB version 9.0: high-level integration of gene regulation, coexpression, motif clustering and beyond",
      "authors": [
        "S Gama-Castro"
      ],
      "year": "2016",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b191",
      "title": ")-PseKNC: A two-layer predictor for identifying promoters and their strength by hybrid features via pseudo Ktuple nucleotide composition",
      "authors": [
        "X Xiao"
      ],
      "year": "2019",
      "venue": "Genomics",
      "doi": ""
    },
    {
      "id": "b192",
      "title": "Improving language model of human genome for DNA-protein binding prediction based on task-specific pre-training",
      "authors": [
        "H Luo"
      ],
      "year": "2023",
      "venue": "Interdisciplinary Sciences: Computational Life Sciences",
      "doi": ""
    },
    {
      "id": "b193",
      "title": "MoDNA: motif-oriented pre-training for DNA language model",
      "authors": [
        "W An"
      ],
      "year": "2022",
      "venue": "Proceedings of the 13th ACM International Conference on Bioinformatics",
      "doi": ""
    },
    {
      "id": "b194",
      "title": "iEnhancer-BERT: A novel transfer learning architecture based on DNA-Language model for identifying enhancers and their strength",
      "authors": [
        "H Luo"
      ],
      "year": "2022",
      "venue": "International Conference on Intelligent Computing",
      "doi": ""
    },
    {
      "id": "b195",
      "title": "Mapping and analysis of chromatin state dynamics in nine human cell types",
      "authors": [
        "J Ernst"
      ],
      "year": "2011",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b196",
      "title": "BERT6mA: prediction of DNA N6-methyladenine site using deep learning-based approaches",
      "authors": [
        "S Tsukiyama"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b197",
      "title": "N6-methyladenine DNA modification in the human genome",
      "authors": [
        "C.-L Xiao"
      ],
      "year": "2018",
      "venue": "Molecular cell",
      "doi": ""
    },
    {
      "id": "b198",
      "title": "De novo genome assembly of the stress tolerant forest species Casuarina equisetifolia provides insight into secondary growth",
      "authors": [
        "G Ye"
      ],
      "year": "2019",
      "venue": "The Plant Journal",
      "doi": ""
    },
    {
      "id": "b199",
      "title": "MethSMRT: an integrative database for DNA N6-methyladenine and N4-methylcytosine generated by single-molecular real-time sequencing",
      "authors": [
        "P Ye"
      ],
      "year": "2016",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b200",
      "title": "MDR: an integrative DNA N6-methyladenine and N4-methylcytosine modification database for Rosaceae",
      "authors": [
        "Z.-Y Liu"
      ],
      "year": "2019",
      "venue": "Horticulture research",
      "doi": ""
    },
    {
      "id": "b201",
      "title": "N6-adenine DNA methylation is associated with the linker DNA of H2A. Z-containing well-positioned nucleosomes in Pol II-transcribed genes in Tetrahymena",
      "authors": [
        "Y Wang"
      ],
      "year": "2017",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b202",
      "title": "iDNA-ABF: multi-scale deep biological language learning model for the interpretable prediction of DNA methylations",
      "authors": [
        "J Jin"
      ],
      "year": "2022",
      "venue": "Genome biology",
      "doi": ""
    },
    {
      "id": "b203",
      "title": "New developments on the Encyclopedia of DNA Elements (ENCODE) data portal",
      "authors": [
        "Y Luo"
      ],
      "year": "2020",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b204",
      "title": "An integrative ENCODE resource for cancer genomics",
      "authors": [
        "J Zhang"
      ],
      "year": "2020",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b205",
      "title": "iDNA-MS: an integrated computational tool for detecting DNA modification sites in multiple genomes. Iscience",
      "authors": [
        "H Lv"
      ],
      "year": "2020",
      "venue": "iDNA-MS: an integrated computational tool for detecting DNA modification sites in multiple genomes. Iscience",
      "doi": ""
    },
    {
      "id": "b206",
      "title": "iDNA-ABT: advanced deep learning model for detecting DNA methylation with adaptive features and transductive information maximization",
      "authors": [
        "Y Yu"
      ],
      "year": "2021",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b207",
      "title": "MuLan-Methyl-Multiple Transformer-based Language Models for Accurate DNA Methylation Prediction",
      "authors": [
        "W Zeng",
        "A Gautam",
        "D H Huson"
      ],
      "year": "",
      "venue": "MuLan-Methyl-Multiple Transformer-based Language Models for Accurate DNA Methylation Prediction",
      "doi": ""
    },
    {
      "id": "b208",
      "title": "Multiple sequence alignment-based RNA language model and its application to structural inference",
      "authors": [
        "Y Zhang"
      ],
      "year": "2024",
      "venue": "Nucleic Acids Research",
      "doi": ""
    },
    {
      "id": "b209",
      "title": "RNA secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning",
      "authors": [
        "J Singh"
      ],
      "year": "2019",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b210",
      "title": "Interpretable RNA foundation model from unannotated data for highly accurate RNA structure and function predictions",
      "authors": [
        "J Chen"
      ],
      "year": "2022",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b211",
      "title": "TurboFold II: RNA structural alignment and secondary structure prediction informed by multiple homologs",
      "authors": [
        "Z Tan"
      ],
      "year": "2017",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b212",
      "title": "Exact calculation of loop formation probability identifies folding motifs in RNA secondary structures",
      "authors": [
        "M F Sloma",
        "D H Mathews"
      ],
      "year": "2016",
      "venue": "RNA",
      "doi": ""
    },
    {
      "id": "b213",
      "title": "A new coronavirus associated with human respiratory disease in China",
      "authors": [
        "F Wu"
      ],
      "year": "2020",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b214",
      "title": "Predicting dynamic cellular protein-RNA interactions by deep learning using in vivo RNA structures",
      "authors": [
        "L Sun"
      ],
      "year": "2021",
      "venue": "Cell research",
      "doi": ""
    },
    {
      "id": "b215",
      "title": "Human 5â€² UTR design and variant effect prediction from a massively parallel translation assay",
      "authors": [
        "P J Sample"
      ],
      "year": "2019",
      "venue": "Nature biotechnology",
      "doi": ""
    },
    {
      "id": "b216",
      "title": "Informative RNA base embedding for RNA structural alignment and clustering by deep representation learning",
      "authors": [
        "M Akiyama",
        "Y Sakakibara"
      ],
      "year": "2022",
      "venue": "NAR genomics and bioinformatics",
      "doi": ""
    },
    {
      "id": "b217",
      "title": "Self-supervised learning on millions of primary RNA sequences from 72 vertebrates improves sequence-based RNA splicing prediction",
      "authors": [
        "K Chen"
      ],
      "year": "2024",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b218",
      "title": "The UCSC genome browser database: 2019 update",
      "authors": [
        "M Haeussler"
      ],
      "year": "2019",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b219",
      "title": "BERT-m7G: a transformer architecture based on BERT and stacking ensemble to identify RNA N7-Methylguanosine sites from sequence information",
      "authors": [
        "L Zhang"
      ],
      "year": "2021",
      "venue": "Computational and Mathematical Methods in Medicine",
      "doi": ""
    },
    {
      "id": "b220",
      "title": "Iterative feature representation algorithm to improve the predictive performance of N7-methylguanosine sites",
      "authors": [
        "C Dai"
      ],
      "year": "2021",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b221",
      "title": "M6A-BERT-Stacking: A Tissue-Specific Predictor for Identifying RNA N6-Methyladenosine Sites Based on BERT and Stacking Strategy. Symmetry",
      "authors": [
        "Q Li"
      ],
      "year": "2023",
      "venue": "M6A-BERT-Stacking: A Tissue-Specific Predictor for Identifying RNA N6-Methyladenosine Sites Based on BERT and Stacking Strategy. Symmetry",
      "doi": ""
    },
    {
      "id": "b222",
      "title": "Computational identification of N6-methyladenosine sites in multiple tissues of mammals. Computational and structural biotechnology journal",
      "authors": [
        "F.-Y Dao"
      ],
      "year": "2020",
      "venue": "Computational identification of N6-methyladenosine sites in multiple tissues of mammals. Computational and structural biotechnology journal",
      "doi": ""
    },
    {
      "id": "b223",
      "title": "BERT2OME: Prediction of 2'-O-methylation Modifications from RNA Sequence by Transformer Architecture Based on BERT",
      "authors": [
        "N N Soylu",
        "E Sefer"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
      "doi": ""
    },
    {
      "id": "b224",
      "title": "RMBase v2. 0: deciphering the map of RNA modifications from epitranscriptome sequencing data",
      "authors": [
        "J.-J Xuan"
      ],
      "year": "2018",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b225",
      "title": "Rm-LR: A long-range-based deep learning model for predicting multiple types of RNA modifications",
      "authors": [
        "S Liang"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine",
      "doi": ""
    },
    {
      "id": "b226",
      "title": "Attention-based multi-label neural networks for integrated prediction and interpretation of twelve widely occurring RNA modifications",
      "authors": [
        "Z Song"
      ],
      "year": "2021",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b227",
      "title": "NCBI GEO: archive for functional genomics data sets-update",
      "authors": [
        "T Barrett"
      ],
      "year": "2012",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b228",
      "title": "RADAR: a rigorously annotated database of A-to-I RNA editing",
      "authors": [
        "G Ramaswami",
        "J B Li"
      ],
      "year": "2014",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b229",
      "title": "BertNDA: a Model Based on Graph-Bert and Multi-scale Information Fusion for ncRNA-disease Association Prediction",
      "authors": [
        "Z Ning"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b230",
      "title": "HMDD v2. 0: a database for experimentally supported human microRNA and disease associations",
      "authors": [
        "Y Li"
      ],
      "year": "2014",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b231",
      "title": "miR2Disease: a manually curated database for microRNA deregulation in human disease",
      "authors": [
        "Q Jiang"
      ],
      "year": "2009",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b232",
      "title": "LncRNADisease 2.0: an updated database of long non-coding RNA-associated diseases",
      "authors": [
        "Z Bao"
      ],
      "year": "2019",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b233",
      "title": "Lnc2Cancer 3.0: an updated resource for experimentally supported lncRNA/circRNA cancer associations and web tools based on RNA-seq and scRNA-seq data",
      "authors": [
        "Y Gao"
      ],
      "year": "2021",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b234",
      "title": "LncCat: An ORF attention model to identify LncRNA based on ensemble learning strategy and fused sequence information",
      "authors": [
        "H Feng"
      ],
      "year": "2023",
      "venue": "Computational and Structural Biotechnology Journal",
      "doi": ""
    },
    {
      "id": "b235",
      "title": "Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation",
      "authors": [
        "N A O'leary"
      ],
      "year": "2016",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b236",
      "title": "A multi-granularity information-enhanced pre-training method for predicting the coding potential of sORFs in plant lncRNAs",
      "authors": [
        "S Xia"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
      "doi": ""
    },
    {
      "id": "b237",
      "title": "GreeNC 2.0: a comprehensive database of plant long non-coding RNAs",
      "authors": [
        "M Di Marsico"
      ],
      "year": "2022",
      "venue": "Nucleic Acids Research",
      "doi": ""
    },
    {
      "id": "b238",
      "title": "CodonBERT: Using BERT for Sentiment Analysis to Better Predict Genes with Low Expression",
      "authors": [
        "A N Babjac",
        "Z Lu",
        "S J Emrich"
      ],
      "year": "2023",
      "venue": "Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics",
      "doi": ""
    },
    {
      "id": "b239",
      "title": "Revealing determinants of translation efficiency via whole-gene codon randomization and machine learning",
      "authors": [
        "T Nieuwkoop"
      ],
      "year": "2023",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b240",
      "title": "High-coverage whole-genome sequencing of the expanded 1000 Genomes Project cohort including 602 trios",
      "authors": [
        "M Byrska-Bishop"
      ],
      "year": "2022",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b241",
      "title": "RNA-TorsionBERT: leveraging language models for RNA 3D torsion angles prediction",
      "authors": [
        "C Bernard"
      ],
      "year": "2024",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b242",
      "title": "UNI-RNA: universal pre-trained models revolutionize RNA research",
      "authors": [
        "X Wang"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b243",
      "title": "RNAcentral: a hub of information for non-coding RNA sequences",
      "authors": [],
      "year": "2019",
      "venue": "Nucleic Acids Research",
      "doi": ""
    },
    {
      "id": "b244",
      "title": "Database resources of the national center for biotechnology information",
      "authors": [
        "E W Sayers"
      ],
      "year": "2022",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b245",
      "title": "Genome Warehouse: a public repository housing genome-scale data",
      "authors": [
        "M Chen"
      ],
      "year": "2021",
      "venue": "Proteomics and Bioinformatics",
      "doi": ""
    },
    {
      "id": "b246",
      "title": "A 5â€² UTR language model for decoding untranslated regions of mRNA and function predictions",
      "authors": [
        "Y Chu"
      ],
      "year": "2024",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b247",
      "title": "Deciphering 3'UTR Mediated Gene Regulation Using Interpretable Deep Representation Learning",
      "authors": [
        "Y Yang"
      ],
      "year": "2024",
      "venue": "Advanced Science",
      "doi": ""
    },
    {
      "id": "b248",
      "title": "Multi-purpose RNA language modelling with motif-aware pretraining and type-guided fine-tuning",
      "authors": [
        "N Wang"
      ],
      "year": "2024",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b249",
      "title": "MSA transformer",
      "authors": [
        "R M Rao"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b250",
      "title": "Continuous Automated Model EvaluatiOn (CAMEO) complementing the critical assessment of structure prediction in CASP12",
      "authors": [
        "J Haas"
      ],
      "year": "2018",
      "venue": "Proteins: Structure, Function, and Bioinformatics",
      "doi": ""
    },
    {
      "id": "b251",
      "title": "Assessing the accuracy of contact predictions in CASP13",
      "authors": [
        "R Shrestha"
      ],
      "year": "2019",
      "venue": "Proteins: Structure, Function, and Bioinformatics",
      "doi": ""
    },
    {
      "id": "b252",
      "title": "Improved protein structure prediction using predicted interresidue orientations",
      "authors": [
        "J Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": ""
    },
    {
      "id": "b253",
      "title": "Evaluation and improvement of multiple sequence methods for protein secondary structure prediction",
      "authors": [
        "J A Cuff",
        "G J Barton"
      ],
      "year": "1999",
      "venue": "Proteins: Structure, Function, and Bioinformatics",
      "doi": ""
    },
    {
      "id": "b254",
      "title": "NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning",
      "authors": [
        "M S Klausen"
      ],
      "year": "2019",
      "venue": "Proteins: Structure, Function, and Bioinformatics",
      "doi": ""
    },
    {
      "id": "b255",
      "title": "The protein data bank",
      "authors": [
        "H M Berman"
      ],
      "year": "2000",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b256",
      "title": "Unified rational protein engineering with sequence-based deep representation learning",
      "authors": [
        "E C Alley"
      ],
      "year": "2019",
      "venue": "Nature methods",
      "doi": ""
    },
    {
      "id": "b257",
      "title": "UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches",
      "authors": [
        "B E Suzek"
      ],
      "year": "2015",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b258",
      "title": "Global analysis of protein folding using massively parallel design, synthesis, and testing",
      "authors": [
        "G J Rocklin"
      ],
      "year": "2017",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b259",
      "title": "Quantitative missense variant effect prediction using large-scale mutagenesis data",
      "authors": [
        "V E Gray"
      ],
      "year": "2018",
      "venue": "Cell systems",
      "doi": ""
    },
    {
      "id": "b260",
      "title": "Local fitness landscape of the green fluorescent protein",
      "authors": [
        "K S Sarkisyan"
      ],
      "year": "2016",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b261",
      "title": "Evaluating protein transfer learning with TAPE. Advances in neural information processing systems",
      "authors": [
        "R Rao"
      ],
      "year": "2019",
      "venue": "Evaluating protein transfer learning with TAPE. Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b262",
      "title": "ProteinNet: a standardized data set for machine learning of protein structure",
      "authors": [
        "M Alquraishi"
      ],
      "year": "2019",
      "venue": "BMC bioinformatics",
      "doi": ""
    },
    {
      "id": "b263",
      "title": "DeepSF: deep convolutional neural network for mapping protein sequences to folds",
      "authors": [
        "J Hou",
        "B Adhikari",
        "J Cheng"
      ],
      "year": "2018",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b264",
      "title": "SCOP: a structural classification of proteins database for the investigation of sequences and structures",
      "authors": [
        "A G Murzin"
      ],
      "year": "1995",
      "venue": "Journal of molecular biology",
      "doi": ""
    },
    {
      "id": "b265",
      "title": "CASP9 target classification. PROTEINS: structure, function, and bioinformatics",
      "authors": [
        "L N Kinch"
      ],
      "year": "2011",
      "venue": "CASP9 target classification. PROTEINS: structure, function, and bioinformatics",
      "doi": ""
    },
    {
      "id": "b266",
      "title": "CASP 11 target classification",
      "authors": [
        "L N Kinch"
      ],
      "year": "2016",
      "venue": "Proteins: Structure, Function, and Bioinformatics",
      "doi": ""
    },
    {
      "id": "b267",
      "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
      "authors": [
        "A Rives"
      ],
      "year": "2021",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": ""
    },
    {
      "id": "b268",
      "title": "SCOPe: Structural Classification of Proteins-extended, integrating SCOP and ASTRAL data and classification of new structures",
      "authors": [
        "N K Fox",
        "S E Brenner",
        "J.-M Chandonia"
      ],
      "year": "2014",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b269",
      "title": "Critical assessment of methods of protein structure prediction: Progress and new directions in round XI",
      "authors": [
        "J Moult"
      ],
      "year": "2016",
      "venue": "Critical assessment of methods of protein structure prediction: Progress and new directions in round XI",
      "doi": ""
    },
    {
      "id": "b270",
      "title": "Deep generative models of genetic variation capture the effects of mutations",
      "authors": [
        "A J Riesselman",
        "J B Ingraham",
        "D S Marks"
      ],
      "year": "2018",
      "venue": "Nature methods",
      "doi": ""
    },
    {
      "id": "b271",
      "title": "ProtTrans: Towards Cracking the Language of Lifes Code Through Self-Supervised Deep Learning and High Performance Computing",
      "authors": [
        "A Elnaggar"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b272",
      "title": "Assessment of hard target modeling in CASP12 reveals an emerging role of alignment-based contact prediction methods",
      "authors": [
        "L A Abriata"
      ],
      "year": "2018",
      "venue": "Proteins: Structure, Function, and Bioinformatics",
      "doi": ""
    },
    {
      "id": "b273",
      "title": "DeepLoc: prediction of protein subcellular localization using deep learning",
      "authors": [
        "J J Almagro Armenteros"
      ],
      "year": "2017",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b274",
      "title": "SCOPe: classification of large macromolecular structures in the structural classification of proteins-extended database",
      "authors": [
        "J.-M Chandonia",
        "N K Fox",
        "S E Brenner"
      ],
      "year": "2019",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b275",
      "title": "SPRoBERTa: protein embedding learning with local fragment modeling",
      "authors": [
        "L Wu"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b276",
      "title": "Structure-based protein function prediction using graph convolutional networks",
      "authors": [
        "V Gligorijevi"
      ],
      "year": "2021",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b277",
      "title": "Multi-level Protein Structure Pre-training via Prompt Learning",
      "authors": [
        "Z Wang"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b278",
      "title": "FLIP: Benchmark tasks in fitness landscape inference for proteins",
      "authors": [
        "C Dallago"
      ],
      "year": "2021",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b279",
      "title": "Progen: Language modeling for protein generation",
      "authors": [
        "A Madani"
      ],
      "year": "2020",
      "venue": "Progen: Language modeling for protein generation",
      "doi": ""
    },
    {
      "id": "b280",
      "title": "UniProt archive",
      "authors": [
        "R Leinonen"
      ],
      "year": "2004",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b281",
      "title": "The universal protein resource (UniProt)",
      "authors": [
        "A Bairoch"
      ],
      "year": "2005",
      "venue": "The universal protein resource (UniProt)",
      "doi": ""
    },
    {
      "id": "b282",
      "title": "Swiss-Prot: juggling between evolution and stability",
      "authors": [
        "A Bairoch"
      ],
      "year": "2004",
      "venue": "Briefings in bioinformatics",
      "doi": ""
    },
    {
      "id": "b283",
      "title": "The SWISS-PROT protein knowledgebase and its supplement TrEMBL in 2003",
      "authors": [
        "B Boeckmann"
      ],
      "year": "2003",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b284",
      "title": "The Pfam protein families database",
      "authors": [
        "A Bateman"
      ],
      "year": "2004",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b285",
      "title": "The NCBI taxonomy database",
      "authors": [
        "S Federhen"
      ],
      "year": "2012",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b286",
      "title": "Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval",
      "authors": [
        "P Notin"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b287",
      "title": "ProtGPT2 is a deep unsupervised language model for protein design",
      "authors": [
        "N Ferruz",
        "S Schmidt",
        "B Hcker"
      ],
      "year": "2022",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b288",
      "title": "UniProt: the universal protein knowledgebase in 2021",
      "authors": [],
      "year": "",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b289",
      "title": "ProteinBERT: a universal deep-learning model of protein sequence and function",
      "authors": [
        "N Brandes"
      ],
      "year": "2022",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b290",
      "title": "Critical assessment of methods of protein structure prediction (CASP)-Round XII",
      "authors": [
        "J Moult"
      ],
      "year": "2018",
      "venue": "Proteins: Structure, Function, and Bioinformatics",
      "doi": ""
    },
    {
      "id": "b291",
      "title": "SCOP2 prototype: a new approach to protein structure mining",
      "authors": [
        "A Andreeva"
      ],
      "year": "2014",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b292",
      "title": "The SCOP database in 2020: expanded classification of representative family and superfamily domains of known protein structures",
      "authors": [
        "A Andreeva"
      ],
      "year": "2020",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b293",
      "title": "SignalP 5.0 improves signal peptide predictions using deep neural networks",
      "authors": [
        "J J A Armenteros"
      ],
      "year": "2019",
      "venue": "Nature biotechnology",
      "doi": ""
    },
    {
      "id": "b294",
      "title": "PhosphoSitePlus, 2014: mutations, PTMs and recalibrations",
      "authors": [
        "P V Hornbeck"
      ],
      "year": "2015",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b295",
      "title": "ProFET: Feature engineering captures high-level protein functions",
      "authors": [
        "D Ofer",
        "M"
      ],
      "year": "2015",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b296",
      "title": "ASAP: a machine learning framework for local protein properties",
      "authors": [
        "N Brandes",
        "D Ofer",
        "M Linial"
      ],
      "year": "2016",
      "venue": "Database",
      "doi": ""
    },
    {
      "id": "b297",
      "title": "Protst: Multi-modality learning of protein sequences and biomedical texts",
      "authors": [
        "M Xu"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b298",
      "title": "The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000",
      "authors": [
        "A Bairoch",
        "R Apweiler"
      ],
      "year": "2000",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b299",
      "title": "Protst: Multi-modality learning of protein sequences and biomedical texts",
      "authors": [
        "M Xu"
      ],
      "year": "2023",
      "venue": "Protst: Multi-modality learning of protein sequences and biomedical texts",
      "doi": ""
    },
    {
      "id": "b300",
      "title": "Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling",
      "authors": [
        "H.-Y Zhou"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b301",
      "title": "Ontoprotein: Protein pretraining with gene ontology embedding",
      "authors": [
        "N Zhang"
      ],
      "year": "2022",
      "venue": "Ontoprotein: Protein pretraining with gene ontology embedding",
      "doi": ""
    },
    {
      "id": "b302",
      "title": "Codon language embeddings provide strong signals for use in protein engineering",
      "authors": [
        "C Outeiral",
        "C M Deane"
      ],
      "year": "2024",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b303",
      "title": "Meltome atlas-thermal proteome stability across the tree of life",
      "authors": [
        "A Jarzab"
      ],
      "year": "2020",
      "venue": "Nature methods",
      "doi": ""
    },
    {
      "id": "b304",
      "title": "DeepLoc 2.0: multi-label subcellular localization prediction using protein language models",
      "authors": [
        "V Thumuluri"
      ],
      "year": "2022",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b305",
      "title": "Proteome-wide solubility and thermal stability profiling reveals distinct regulatory roles for ATP",
      "authors": [
        "S Sridharan"
      ],
      "year": "2019",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b306",
      "title": "Learning functional properties of proteins with language models",
      "authors": [
        "S Unsal"
      ],
      "year": "",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b307",
      "title": "Tissue-based map of the human proteome",
      "authors": [
        "M Uhln"
      ],
      "year": "2015",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b308",
      "title": "PaxDb, a database of protein abundance averages across all three domains of life. Molecular \\& cellular proteomics",
      "authors": [
        "M Wang"
      ],
      "year": "2012",
      "venue": "PaxDb, a database of protein abundance averages across all three domains of life. Molecular \\& cellular proteomics",
      "doi": ""
    },
    {
      "id": "b309",
      "title": "PLMSearch: Protein language model powers accurate and fast sequence search for remote homology",
      "authors": [
        "W Liu"
      ],
      "year": "2024",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b310",
      "title": "SCOPe: improvements to the structural classification of proteins--extended database to facilitate variant interpretation and machine learning",
      "authors": [
        "J.-M Chandonia"
      ],
      "year": "2022",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b311",
      "title": "CATH: increased structural coverage of functional space",
      "authors": [
        "I Sillitoe"
      ],
      "year": "2021",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b312",
      "title": "Fast, sensitive detection of protein homologs using deep dense retrieval",
      "authors": [
        "L Hong"
      ],
      "year": "2024",
      "venue": "Nature Biotechnology",
      "doi": ""
    },
    {
      "id": "b313",
      "title": "MHCRoBERTa: pan-specific peptide-MHC class I binding prediction through transfer learning with label-agnostic protein sequences",
      "authors": [
        "F Wang"
      ],
      "year": "2022",
      "venue": "Brief Bioinform",
      "doi": ""
    },
    {
      "id": "b314",
      "title": "UniProtKB/Swiss-Prot: the manually annotated section of the UniProt KnowledgeBase",
      "authors": [
        "E Boutet"
      ],
      "year": "2007",
      "venue": "Plant bioinformatics: methods and protocols",
      "doi": ""
    },
    {
      "id": "b315",
      "title": "The Immune Epitope Database (IEDB): 2018 update",
      "authors": [
        "R Vita"
      ],
      "year": "2019",
      "venue": "Nucleic Acids Res",
      "doi": ""
    },
    {
      "id": "b316",
      "title": "BERTMHC: improved MHC-peptide class II interaction prediction with transformer and multiple instance learning",
      "authors": [
        "J Cheng"
      ],
      "year": "2021",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b317",
      "title": "Improved methods for predicting peptide binding affinity to MHCclass II molecules",
      "authors": [],
      "year": "2017",
      "venue": "Improved methods for predicting peptide binding affinity to MHCclass II molecules",
      "doi": ""
    },
    {
      "id": "b318",
      "title": "TCR-BERT: learning the grammar of T-cell receptors for flexible antigenbinding analyses",
      "authors": [
        "K Wu"
      ],
      "year": "2021",
      "venue": "TCR-BERT: learning the grammar of T-cell receptors for flexible antigenbinding analyses",
      "doi": ""
    },
    {
      "id": "b319",
      "title": "PIRD: Pan Immune Repertoire Database",
      "authors": [
        "W Zhang"
      ],
      "year": "2020",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b320",
      "title": "VDJdb in 2019: database extension, new analysis infrastructure and a T-cell receptor motif compendium",
      "authors": [
        "D V Bagaev"
      ],
      "year": "2020",
      "venue": "Nucleic Acids Research",
      "doi": ""
    },
    {
      "id": "b321",
      "title": "TCRdb: a comprehensive database for T-cell receptor sequences with powerful search function",
      "authors": [
        "S Y Chen"
      ],
      "year": "2021",
      "venue": "Nucleic Acids Res",
      "doi": ""
    },
    {
      "id": "b322",
      "title": "AntiFormer: graph enhanced large language model for binding affinity prediction",
      "authors": [
        "Q Wang"
      ],
      "year": "2024",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b323",
      "title": "Observed Antibody Space: A diverse database of cleaned, annotated, and translated unpaired and paired antibody sequences",
      "authors": [
        "T H Olsen",
        "F Boyles",
        "C M J P S Deane"
      ],
      "year": "2022",
      "venue": "Observed Antibody Space: A diverse database of cleaned, annotated, and translated unpaired and paired antibody sequences",
      "doi": ""
    },
    {
      "id": "b324",
      "title": "SC-AIR-BERT: a pre-trained single-cell model for predicting the antigen-binding specificity of the adaptive immune receptor",
      "authors": [
        "Y Zhao"
      ],
      "year": "2023",
      "venue": "Brief Bioinform",
      "doi": ""
    },
    {
      "id": "b325",
      "title": "huARdb: human Antigen Receptor database for interactive clonotype-transcriptome analysis at the single-cell level",
      "authors": [
        "L Wu"
      ],
      "year": "2022",
      "venue": "Nucleic Acids Res",
      "doi": ""
    },
    {
      "id": "b326",
      "title": "CoV-AbDab: the coronavirus antibody database",
      "authors": [
        "M I J Raybould"
      ],
      "year": "2021",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b327",
      "title": "AbLang: an antibody language model for completing antibody sequences",
      "authors": [
        "T H Olsen",
        "I H Moal",
        "C M Deane"
      ],
      "year": "2022",
      "venue": "Bioinformatics Advances",
      "doi": ""
    },
    {
      "id": "b328",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "doi": ""
    },
    {
      "id": "b329",
      "title": "Observed antibody space: a resource for data mining next-generation sequencing of antibody repertoires",
      "authors": [
        "A Kovaltsuk"
      ],
      "year": "2018",
      "venue": "The Journal of Immunology",
      "doi": ""
    },
    {
      "id": "b330",
      "title": "Different B cell subpopulations show distinct patterns in their IgH repertoire metrics",
      "authors": [
        "M Ghraichy"
      ],
      "year": "2021",
      "venue": "Different B cell subpopulations show distinct patterns in their IgH repertoire metrics",
      "doi": ""
    },
    {
      "id": "b331",
      "title": "SAbDab: the structural antibody database",
      "authors": [
        "J Dunbar"
      ],
      "year": "2014",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b332",
      "title": "Humanization of antibodies using a machine learning approach on large-scale repertoire data",
      "authors": [
        "C Marks"
      ],
      "year": "2021",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b333",
      "title": "On pre-trained language models for antibody",
      "authors": [
        "D Wang",
        "F Ye",
        "H Zhou"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b334",
      "title": "Optimization of therapeutic antibodies by predicting antigen specificity from antibody sequence via deep learning",
      "authors": [
        "D M Mason"
      ],
      "year": "2021",
      "venue": "Nature Biomedical Engineering",
      "doi": ""
    },
    {
      "id": "b335",
      "title": "Parapred: antibody paratope prediction using convolutional and recurrent neural networks",
      "authors": [
        "E Liberis"
      ],
      "year": "2018",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b336",
      "title": "Differences in the composition of the human antibody repertoire by B cell subsets in the blood",
      "authors": [
        "E S Mroczek"
      ],
      "year": "2014",
      "venue": "Frontiers in immunology",
      "doi": ""
    },
    {
      "id": "b337",
      "title": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction",
      "authors": [
        "S Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics",
      "doi": ""
    },
    {
      "id": "b338",
      "title": "ZINC: a free tool to discover chemistry for biology",
      "authors": [
        "J J Irwin"
      ],
      "year": "2012",
      "venue": "Journal of chemical information and modeling",
      "doi": ""
    },
    {
      "id": "b339",
      "title": "ChemBERTa: large-scale self-supervised pretraining for molecular property prediction",
      "authors": [
        "S Chithrananda",
        "G Grand",
        "B Ramsundar"
      ],
      "year": "2020",
      "venue": "ChemBERTa: large-scale self-supervised pretraining for molecular property prediction",
      "doi": ""
    },
    {
      "id": "b340",
      "title": "PubChem 2019 update: improved access to chemical data",
      "authors": [
        "S Kim"
      ],
      "year": "2019",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b341",
      "title": "ChemBERTa-2: Towards Chemical Foundation Models",
      "authors": [],
      "year": "",
      "venue": "ChemBERTa-2: Towards Chemical Foundation Models",
      "doi": ""
    },
    {
      "id": "b342",
      "title": "Knowledge-based BERT: a method to extract molecular features like computational chemists",
      "authors": [
        "Z Wu"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b343",
      "title": "ChEMBL: towards direct deposition of bioassay data",
      "authors": [
        "D Mendez"
      ],
      "year": "2019",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b344",
      "title": "Mole-bert: Rethinking pre-training graph neural networks for molecules",
      "authors": [
        "J Xia"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b345",
      "title": "ZINC 15--ligand discovery for everyone",
      "authors": [
        "T Sterling",
        "J J Irwin"
      ],
      "year": "2015",
      "venue": "Journal of chemical information and modeling",
      "doi": ""
    },
    {
      "id": "b346",
      "title": "MolGPT: molecular generation using a transformer-decoder model",
      "authors": [
        "V Bagal"
      ],
      "year": "2021",
      "venue": "Journal of Chemical Information and Modeling",
      "doi": ""
    },
    {
      "id": "b347",
      "title": "Molecular sets (MOSES): a benchmarking platform for molecular generation models",
      "authors": [
        "D Polykovskiy"
      ],
      "year": "2020",
      "venue": "Frontiers in pharmacology",
      "doi": ""
    },
    {
      "id": "b348",
      "title": "GuacaMol: benchmarking models for de novo molecular design",
      "authors": [
        "N Brown"
      ],
      "year": "2019",
      "venue": "Journal of chemical information and modeling",
      "doi": ""
    },
    {
      "id": "b349",
      "title": "DTI-BERT: identifying drug-target interactions in cellular networking based on BERT and deep learning method",
      "authors": [
        "J Zheng",
        "X Xiao",
        "W.-R Qiu"
      ],
      "year": "2022",
      "venue": "Frontiers in Genetics",
      "doi": ""
    },
    {
      "id": "b350",
      "title": "DrugBank 5.0: a major update to the DrugBank database for 2018",
      "authors": [
        "D S Wishart"
      ],
      "year": "2018",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b351",
      "title": "GPCR--drug interactions prediction using random forest with drug-association-matrix-based post-processing procedure",
      "authors": [
        "J Hu"
      ],
      "year": "2016",
      "venue": "Computational biology and chemistry",
      "doi": ""
    },
    {
      "id": "b352",
      "title": "TransDTI: transformer-based language models for estimating DTIs and building a drug recommendation workflow",
      "authors": [
        "Y Kalakoti",
        "S Yadav",
        "D Sundar"
      ],
      "year": "2022",
      "venue": "ACS omega",
      "doi": ""
    },
    {
      "id": "b353",
      "title": "Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis",
      "authors": [
        "J Tang"
      ],
      "year": "2014",
      "venue": "Journal of Chemical Information and Modeling",
      "doi": ""
    },
    {
      "id": "b354",
      "title": "DTI-MLCD: predicting drug-target interactions using multi-label learning with community detection method",
      "authors": [
        "Y Chu"
      ],
      "year": "2021",
      "venue": "Briefings in bioinformatics",
      "doi": ""
    },
    {
      "id": "b355",
      "title": "Mitigating cold-start problems in drug-target affinity prediction with interaction knowledge transferring",
      "authors": [
        "T M Nguyen",
        "T Nguyen",
        "T Tran"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b356",
      "title": "The STRING database in 2021: customizable protein--protein networks, and functional characterization of user-uploaded gene/measurement sets",
      "authors": [
        "D Szklarczyk"
      ],
      "year": "2021",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b357",
      "title": "STITCH: interaction networks of chemicals and proteins",
      "authors": [
        "M Kuhn"
      ],
      "year": "2007",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b358",
      "title": "Comprehensive analysis of kinase inhibitor selectivity",
      "authors": [
        "M I Davis"
      ],
      "year": "2011",
      "venue": "Nature Biotechnology",
      "doi": ""
    },
    {
      "id": "b359",
      "title": "The database: Collection of binding affinities for protein-ligand complexes with known three-dimensional structures",
      "authors": [
        "R Wang"
      ],
      "year": "2004",
      "venue": "Journal of medicinal chemistry",
      "doi": ""
    },
    {
      "id": "b360",
      "title": "The PDBbind database: methodologies and updates",
      "authors": [
        "R Wang"
      ],
      "year": "2005",
      "venue": "Journal of medicinal chemistry",
      "doi": ""
    },
    {
      "id": "b361",
      "title": "Fine-tuning of bert model to accurately predict drug--target interactions",
      "authors": [
        "H Kang"
      ],
      "year": "2022",
      "venue": "Pharmaceutics",
      "doi": ""
    },
    {
      "id": "b362",
      "title": "BioSNAP Datasets: Stanford biomedical network dataset collection",
      "authors": [
        "M Zitnik",
        "R Sosic",
        "J Leskovec"
      ],
      "year": "2018",
      "venue": "BioSNAP Datasets: Stanford biomedical network dataset collection",
      "doi": ""
    },
    {
      "id": "b363",
      "title": "BindingDB: a web-accessible database of experimentally determined protein--ligand binding affinities",
      "authors": [
        "T Liu"
      ],
      "year": "2007",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b364",
      "title": "scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data",
      "authors": [
        "F Yang"
      ],
      "year": "",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b365",
      "title": "PanglaoDB: a web server for exploration of mouse and human single-cell RNA sequencing data",
      "authors": [
        "O Franzen",
        "L M Gan",
        "J L M Bjorkegren"
      ],
      "year": "2019",
      "venue": "Database",
      "doi": ""
    },
    {
      "id": "b366",
      "title": "Massively parallel digital transcriptional profiling of single cells",
      "authors": [
        "G X Zheng"
      ],
      "year": "2017",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b367",
      "title": "A Single-Cell Transcriptomic Map of the Human and Mouse Pancreas Reveals Inter-and Intra-cell Population Structure",
      "authors": [
        "M Baron"
      ],
      "year": "2016",
      "venue": "Cell Syst",
      "doi": ""
    },
    {
      "id": "b368",
      "title": "A Single-Cell Transcriptome Atlas of the Human Pancreas",
      "authors": [
        "M J Muraro"
      ],
      "year": "2016",
      "venue": "Cell Syst",
      "doi": ""
    },
    {
      "id": "b369",
      "title": "Single-Cell Transcriptome Profiling of Human Pancreatic Islets in Health and Type 2 Diabetes",
      "authors": [
        "A Segerstolpe"
      ],
      "year": "2016",
      "venue": "Cell Metab",
      "doi": ""
    },
    {
      "id": "b370",
      "title": "RNA Sequencing of Single Human Islet Cells Reveals Type 2 Diabetes Genes",
      "authors": [
        "Y Xin"
      ],
      "year": "2016",
      "venue": "Cell Metab",
      "doi": ""
    },
    {
      "id": "b371",
      "title": "Single cell RNA sequencing of human liver reveals distinct intrahepatic macrophage populations",
      "authors": [
        "S A Macparland"
      ],
      "year": "2018",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b372",
      "title": "Cells of the adult human heart",
      "authors": [
        "M Litvinukova"
      ],
      "year": "2020",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b373",
      "title": "Transcriptional and Cellular Diversity of the Human Heart",
      "authors": [
        "N R Tucker"
      ],
      "year": "2020",
      "venue": "Circulation",
      "doi": ""
    },
    {
      "id": "b374",
      "title": "SARS-CoV-2 receptor ACE2 and TMPRSS2 are primarily expressed in bronchial transient secretory cells",
      "authors": [
        "S Lukassen"
      ],
      "year": "2020",
      "venue": "EMBO J",
      "doi": ""
    },
    {
      "id": "b375",
      "title": "Single-cell transcriptome profiling of an adult human cell atlas of 15 major organs",
      "authors": [
        "S He"
      ],
      "year": "2020",
      "venue": "Genome Biol",
      "doi": ""
    },
    {
      "id": "b376",
      "title": "scGPT: toward building a foundation model for single-cell multi-omics using generative AI",
      "authors": [
        "H Cui"
      ],
      "year": "2024",
      "venue": "Nat Methods",
      "doi": ""
    },
    {
      "id": "b377",
      "title": "CZ CELLxGENE Discover",
      "authors": [
        "N D )",
        "C Z I"
      ],
      "year": "2022",
      "venue": "CZ CELLxGENE Discover",
      "doi": ""
    },
    {
      "id": "b378",
      "title": "A Python library for probabilistic analysis of single-cell omics data",
      "authors": [
        "A Gayoso"
      ],
      "year": "2022",
      "venue": "Nat Biotechnol",
      "doi": ""
    },
    {
      "id": "b379",
      "title": "Benchmarking atlas-level data integration in single-cell genomics",
      "authors": [
        "M D Luecken"
      ],
      "year": "2022",
      "venue": "Nat Methods",
      "doi": ""
    },
    {
      "id": "b380",
      "title": "Transformer for one stop interpretable cell type annotation",
      "authors": [
        "J Chen"
      ],
      "year": "2023",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b381",
      "title": "Single-cell transcriptomes identify human islet cell signatures and reveal cell-type-specific expression changes in type 2 diabetes",
      "authors": [
        "N Lawlor"
      ],
      "year": "2017",
      "venue": "Genome Res",
      "doi": ""
    },
    {
      "id": "b382",
      "title": "A Multiplexed Single-Cell CRISPR Screening Platform Enables Systematic Dissection of the Unfolded Protein Response",
      "authors": [
        "B Adamson"
      ],
      "year": "2016",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b383",
      "title": "Exploring genetic interaction manifolds constructed from rich single-cell phenotypes",
      "authors": [
        "T M Norman"
      ],
      "year": "2019",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b384",
      "title": "Multiplex single cell profiling of chromatin accessibility by combinatorial cellular indexing",
      "authors": [
        "D A Cusanovich"
      ],
      "year": "2015",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b385",
      "title": "Scalable, multimodal profiling of chromatin accessibility, gene expression and protein levels in single cells",
      "authors": [
        "E P Mimitou"
      ],
      "year": "2021",
      "venue": "Nat Biotechnol",
      "doi": ""
    },
    {
      "id": "b386",
      "title": "CIForm as a Transformer-based model for cell-type annotation of large-scale single-cell RNA-seq data",
      "authors": [
        "J Xu"
      ],
      "year": "2023",
      "venue": "Brief Bioinform",
      "doi": ""
    },
    {
      "id": "b387",
      "title": "A Bayesian mixture model for clustering droplet-based single-cell transcriptomic data from population studies",
      "authors": [
        "Z Sun"
      ],
      "year": "2019",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b388",
      "title": "Human bone marrow assessment by single-cell RNA sequencing, mass cytometry, and flow cytometry",
      "authors": [
        "K A Oetjen"
      ],
      "year": "2018",
      "venue": "JCI Insight",
      "doi": ""
    },
    {
      "id": "b389",
      "title": "A single-cell hematopoietic landscape resolves 8 lineage trajectories and defects in Kit mutant mice",
      "authors": [
        "J S Dahlin"
      ],
      "year": "2018",
      "venue": "Blood",
      "doi": ""
    },
    {
      "id": "b390",
      "title": "Molecular Architecture of the Mouse Nervous System",
      "authors": [
        "A Zeisel"
      ],
      "year": "2018",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b391",
      "title": "Molecular Diversity and Specializations among the Cells of the Adult Mouse Brain",
      "authors": [
        "A Saunders"
      ],
      "year": "2018",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b392",
      "title": "Single-cell profiling of the developing mouse brain and spinal cord with split-pool barcoding",
      "authors": [
        "A B Rosenberg"
      ],
      "year": "2018",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b393",
      "title": "Single-cell transcriptomics of 20 mouse organs creates a Tabula Muris",
      "authors": [
        "C Tabula Muris"
      ],
      "year": "2018",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b394",
      "title": "Lineage tracking reveals dynamic relationships of T cells in colorectal cancer",
      "authors": [
        "L Zhang"
      ],
      "year": "2018",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b395",
      "title": "Shared and distinct transcriptomic cell types across neocortical areas",
      "authors": [
        "B Tasic"
      ],
      "year": "2018",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b396",
      "title": "Synovial cell cross-talk with cartilage plays a major role in the pathogenesis of osteoarthritis",
      "authors": [
        "C H Chou"
      ],
      "year": "2020",
      "venue": "Sci Rep",
      "doi": ""
    },
    {
      "id": "b397",
      "title": "Decoding the transcriptome of calcified atherosclerotic plaque at single-cell resolution",
      "authors": [
        "T Alsaigh"
      ],
      "year": "2022",
      "venue": "Commun Biol",
      "doi": ""
    },
    {
      "id": "b398",
      "title": "Cell types in the mouse cortex and hippocampus revealed by single-cell RNA-seq",
      "authors": [
        "A Zeisel"
      ],
      "year": "2015",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b399",
      "title": "Comprehensive single cell mRNA profiling reveals a detailed roadmap for pancreatic endocrinogenesis",
      "authors": [
        "A Bastidas-Ponce"
      ],
      "year": "2019",
      "venue": "Development",
      "doi": ""
    },
    {
      "id": "b400",
      "title": "A single-cell transcriptomic atlas characterizes ageing tissues in the mouse",
      "authors": [
        "C Tabula Muris"
      ],
      "year": "2020",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b401",
      "title": "scTransSort: Transformers for Intelligent Annotation of Cell Types by Gene Embeddings",
      "authors": [
        "L Jiao"
      ],
      "year": "2023",
      "venue": "Biomolecules",
      "doi": ""
    },
    {
      "id": "b402",
      "title": "scDeepSort: a pre-trained cell-type annotation method for single-cell transcriptomics using deep learning with a weighted graph neural network",
      "authors": [
        "X Shao"
      ],
      "year": "2021",
      "venue": "Nucleic Acids Res",
      "doi": ""
    },
    {
      "id": "b403",
      "title": "TransCluster: A Cell-Type Identification Method for single-cell RNA-Seq data using deep learning based on transformer",
      "authors": [
        "T Song"
      ],
      "year": "2022",
      "venue": "TransCluster: A Cell-Type Identification Method for single-cell RNA-Seq data using deep learning based on transformer",
      "doi": ""
    },
    {
      "id": "b404",
      "title": "Construction of a human cell landscape at single-cell level",
      "authors": [
        "X Han"
      ],
      "year": "2020",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b405",
      "title": "Transfer learning enables predictions in network biology",
      "authors": [
        "C V Theodoris"
      ],
      "year": "2023",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b406",
      "title": "Datasets: A community library for natural language processing",
      "authors": [
        "Q Lhoest"
      ],
      "year": "2021",
      "venue": "Datasets: A community library for natural language processing",
      "doi": ""
    },
    {
      "id": "b407",
      "title": "Generative pretraining from large-scale transcriptomes for single-cell deciphering. iScience",
      "authors": [
        "H Shen"
      ],
      "year": "2023",
      "venue": "Generative pretraining from large-scale transcriptomes for single-cell deciphering. iScience",
      "doi": ""
    },
    {
      "id": "b408",
      "title": "The human cell atlas white paper",
      "authors": [
        "A Regev"
      ],
      "year": "2018",
      "venue": "The human cell atlas white paper",
      "doi": ""
    },
    {
      "id": "b409",
      "title": "Molecular Classification and Comparative Taxonomics of Foveal and Peripheral Cells in Primate Retina",
      "authors": [
        "Y R Peng"
      ],
      "year": "2019",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b410",
      "title": "Adjusting batch effects in microarray expression data using empirical Bayes methods",
      "authors": [
        "W E Johnson",
        "C Li",
        "A Rabinovic"
      ],
      "year": "2007",
      "venue": "Biostatistics",
      "doi": ""
    },
    {
      "id": "b411",
      "title": "The immune landscape of esophageal cancer",
      "authors": [
        "T X Huang",
        "L Fu"
      ],
      "year": "2019",
      "venue": "Cancer Commun (Lond)",
      "doi": ""
    },
    {
      "id": "b412",
      "title": "Single-cell biological network inference using a heterogeneous graph transformer",
      "authors": [
        "A Ma"
      ],
      "year": "2023",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b413",
      "title": "A sandbox for prediction and integration of DNA, RNA, and proteins in single cells",
      "authors": [
        "M D Luecken"
      ],
      "year": "",
      "venue": "Thirty-fifth conference on neural information processing systems datasets and benchmarks track",
      "doi": ""
    },
    {
      "id": "b414",
      "title": "A deep generative model for multi-view profiling of single-cell RNA-seq and ATAC-seq data",
      "authors": [
        "G Li"
      ],
      "year": "2022",
      "venue": "Genome Biol",
      "doi": ""
    },
    {
      "id": "b415",
      "title": "Joint profiling of chromatin accessibility and gene expression in thousands of single cells",
      "authors": [
        "J Cao"
      ],
      "year": "2018",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b416",
      "title": "An ultra high-throughput method for single-cell joint analysis of open chromatin and transcriptome",
      "authors": [
        "C Zhu"
      ],
      "year": "2019",
      "venue": "Nat Struct Mol Biol",
      "doi": ""
    },
    {
      "id": "b417",
      "title": "High-throughput sequencing of the transcriptome and chromatin accessibility in the same cell",
      "authors": [
        "S Chen",
        "B B Lake",
        "K Zhang"
      ],
      "year": "2019",
      "venue": "Nat Biotechnol",
      "doi": ""
    },
    {
      "id": "b418",
      "title": "Chromatin Potential Identified by Shared Single-Cell Profiling of RNA and Chromatin",
      "authors": [
        "S Ma"
      ],
      "year": "2020",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b419",
      "title": "A pre-trained large language model for translating single-cell transcriptome to proteome",
      "authors": [
        "L Linjing"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b420",
      "title": "Comprehensive molecular characterization of clear cell renal cell carcinoma",
      "authors": [
        "Cancer Genome",
        "Atlas Research",
        "N"
      ],
      "year": "2013",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b421",
      "title": "Comprehensive Molecular Portraits of Invasive Lobular Breast Cancer",
      "authors": [
        "G Ciriello"
      ],
      "year": "2015",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b422",
      "title": "Comprehensive Molecular Characterization of Pheochromocytoma and Paraganglioma",
      "authors": [
        "L Fishbein"
      ],
      "year": "2017",
      "venue": "Cancer Cell",
      "doi": ""
    },
    {
      "id": "b423",
      "title": "Comprehensive Analysis of Alternative Splicing Across Tumors from 8,705 Patients",
      "authors": [
        "A Kahles"
      ],
      "year": "2018",
      "venue": "Cancer Cell",
      "doi": ""
    },
    {
      "id": "b424",
      "title": "Proteogenomic characterization of pancreatic ductal adenocarcinoma",
      "authors": [
        "L Cao"
      ],
      "year": "2021",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b425",
      "title": "Proteogenomic Characterization of Endometrial Carcinoma",
      "authors": [
        "Y Dou"
      ],
      "year": "2020",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b426",
      "title": "Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma",
      "authors": [
        "M A Gillette"
      ],
      "year": "2020",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b427",
      "title": "Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy",
      "authors": [
        "K Krug"
      ],
      "year": "2020",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b428",
      "title": "Integrated Proteogenomic Characterization across Major Histological Types of Pediatric Brain Cancer. Cell",
      "authors": [
        "F Petralia"
      ],
      "year": "2020",
      "venue": "Integrated Proteogenomic Characterization across Major Histological Types of Pediatric Brain Cancer. Cell",
      "doi": ""
    },
    {
      "id": "b429",
      "title": "A proteogenomic portrait of lung squamous cell carcinoma",
      "authors": [
        "S Satpathy"
      ],
      "year": "2021",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b430",
      "title": "Proteogenomic and metabolomic characterization of human glioblastoma",
      "authors": [
        "L B Wang"
      ],
      "year": "2021",
      "venue": "Cancer Cell",
      "doi": ""
    },
    {
      "id": "b431",
      "title": "Consistency of drug profiles and predictors in large-scale cancer cell line data",
      "authors": [
        "T C C L Encyclopedia"
      ],
      "year": "2015",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b432",
      "title": "Quantitative Proteomics of the Cancer Cell Line Encyclopedia",
      "authors": [
        "D P Nusinow"
      ],
      "year": "2020",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b433",
      "title": "Genomic Differences Between \"Primary\" and \"Secondary\" Muscle-invasive Bladder Cancer as a Basis for Disparate Outcomes to Cisplatin-based Neoadjuvant Chemotherapy",
      "authors": [
        "E J Pietzak"
      ],
      "year": "2019",
      "venue": "Eur Urol",
      "doi": ""
    },
    {
      "id": "b434",
      "title": "Integrated analysis of multimodal single-cell data",
      "authors": [
        "Y Hao"
      ],
      "year": "2021",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b435",
      "title": "Multiplexed quantification of proteins and transcripts in single cells",
      "authors": [
        "V M Peterson"
      ],
      "year": "2017",
      "venue": "Nat Biotechnol",
      "doi": ""
    },
    {
      "id": "b436",
      "title": "Simultaneous epitope and transcriptome measurement in single cells",
      "authors": [
        "M Stoeckius"
      ],
      "year": "2017",
      "venue": "Nat Methods",
      "doi": ""
    },
    {
      "id": "b437",
      "title": "A pan-cancer single-cell transcriptional atlas of tumor infiltrating myeloid cells",
      "authors": [
        "S Cheng"
      ],
      "year": "2021",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b438",
      "title": "Large-scale foundation model on single-cell transcriptomics",
      "authors": [
        "M Hao"
      ],
      "year": "2024",
      "venue": "Nat Methods",
      "doi": ""
    },
    {
      "id": "b439",
      "title": "DeepCDR: a hybrid graph convolutional network for predicting cancer drug response",
      "authors": [
        "Q Liu"
      ],
      "year": "2020",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b440",
      "title": "Predicting transcriptional outcomes of novel multigene perturbations with GEARS",
      "authors": [
        "Y Roohani",
        "K Huang",
        "J Leskovec"
      ],
      "year": "2023",
      "venue": "Nat Biotechnol",
      "doi": ""
    },
    {
      "id": "b441",
      "title": "Single-cell multimodal prediction via transformers",
      "authors": [
        "W Tang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 32nd ACM International Conference on Information and Knowledge Management",
      "doi": ""
    },
    {
      "id": "b442",
      "title": "GeneCompass: deciphering universal gene regulatory mechanisms with a knowledge-informed cross-species foundation model",
      "authors": [
        "X Yang"
      ],
      "year": "2024",
      "venue": "Cell Res",
      "doi": ""
    },
    {
      "id": "b443",
      "title": "Synergistic activation of inflammatory cytokine genes by interferon-gamma-induced chromatin remodeling and toll-like receptor signaling",
      "authors": [
        "Y Qiao"
      ],
      "year": "2013",
      "venue": "Immunity",
      "doi": ""
    },
    {
      "id": "b444",
      "title": "Massively multiplex chemical transcriptomics at single-cell resolution",
      "authors": [
        "S R Srivatsan"
      ],
      "year": "2020",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b445",
      "title": "The BTB transcription factors ZBTB11 and ZFP131 maintain pluripotency by repressing pro-differentiation genes",
      "authors": [
        "G Garipler"
      ],
      "year": "2022",
      "venue": "Cell Rep",
      "doi": ""
    },
    {
      "id": "b446",
      "title": "scMulan: a multitask generative pre-trained language model for single-cell analysis",
      "authors": [
        "H Bian"
      ],
      "year": "2024",
      "venue": "International Conference on Research in Computational Molecular Biology",
      "doi": ""
    },
    {
      "id": "b447",
      "title": "Single-nucleus RNA sequencing in ischemic cardiomyopathy reveals common transcriptional profile underlying endstage heart failure",
      "authors": [
        "B Simonson"
      ],
      "year": "2023",
      "venue": "Cell Rep",
      "doi": ""
    },
    {
      "id": "b448",
      "title": "Mapping the developing human immune system across organs",
      "authors": [
        "C Suo"
      ],
      "year": "2022",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b449",
      "title": "Mapping single-cell data to reference atlases by transfer learning",
      "authors": [
        "M Lotfollahi"
      ],
      "year": "2022",
      "venue": "Nat Biotechnol",
      "doi": ""
    },
    {
      "id": "b450",
      "title": "Universal cell embeddings: A foundation model for cell biology",
      "authors": [
        "Y Rosen"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b451",
      "title": "Cross-tissue immune cell analysis reveals tissue-specific features in humans",
      "authors": [
        "C Dominguez Conde"
      ],
      "year": "2022",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b452",
      "title": "Single-cell RNA sequencing reveals SARS-CoV-2 infection dynamics in lungs of African green monkeys",
      "authors": [
        "E Speranza"
      ],
      "year": "2021",
      "venue": "Sci Transl Med",
      "doi": ""
    },
    {
      "id": "b453",
      "title": "Single-cell transcriptomics of the naked mole-rat reveals unexpected features of mammalian immunity",
      "authors": [
        "H G Hilton"
      ],
      "year": "2019",
      "venue": "PLoS Biol",
      "doi": ""
    },
    {
      "id": "b454",
      "title": "A cell atlas of the chick retina based on single-cell transcriptomics",
      "authors": [
        "M Yamagata",
        "W Yan",
        "J R Sanes"
      ],
      "year": "2021",
      "venue": "A cell atlas of the chick retina based on single-cell transcriptomics",
      "doi": ""
    },
    {
      "id": "b455",
      "title": "Integration of eQTL and a Single-Cell Atlas in the Human Eye Identifies Causal Genes for Age-Related Macular Degeneration",
      "authors": [
        "L D Orozco"
      ],
      "year": "2020",
      "venue": "Cell Rep",
      "doi": ""
    },
    {
      "id": "b456",
      "title": "Large-scale cell representation learning via divide-and-conquer contrastive learning",
      "authors": [
        "S Zhao",
        "J Zhang",
        "Z Nie"
      ],
      "year": "2023",
      "venue": "Large-scale cell representation learning via divide-and-conquer contrastive learning",
      "doi": ""
    },
    {
      "id": "b457",
      "title": "Single-cell transcriptional changes associated with drug tolerance and response to combination therapies in cancer",
      "authors": [
        "A F Aissa"
      ],
      "year": "2021",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b458",
      "title": "Longitudinal single-cell RNA sequencing of patient-derived primary cells reveals drug-induced infidelity in stem cell hierarchy",
      "authors": [
        "A Sharma"
      ],
      "year": "2018",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b459",
      "title": "Single-cell analysis of EphA clustering phenotypes to probe cancer cell heterogeneity",
      "authors": [
        "A Ravasio"
      ],
      "year": "2020",
      "venue": "Commun Biol",
      "doi": ""
    },
    {
      "id": "b460",
      "title": "Predicting heterogeneity in clone-specific therapeutic vulnerabilities using single-cell transcriptomic signatures",
      "authors": [
        "C Suphavilai"
      ],
      "year": "2021",
      "venue": "Genome Med",
      "doi": ""
    },
    {
      "id": "b461",
      "title": "The Cancer Cell Line Encyclopedia enables predictive modelling of anticancer drug sensitivity",
      "authors": [
        "J Barretina"
      ],
      "year": "2012",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b462",
      "title": "A Landscape of Pharmacogenomic Interactions in Cancer",
      "authors": [
        "F Iorio"
      ],
      "year": "2016",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b463",
      "title": "scCLIP: Multi-modal Single-cell Contrastive Learning Integration Pre-training",
      "authors": [
        "L Xiong",
        "T Chen",
        "M Kellis"
      ],
      "year": "",
      "venue": "NeurIPS 2023 AI for Science Workshop",
      "doi": ""
    },
    {
      "id": "b464",
      "title": "A human cell atlas of fetal gene expression",
      "authors": [
        "J Cao"
      ],
      "year": "2020",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b465",
      "title": "A human cell atlas of fetal chromatin accessibility",
      "authors": [
        "S Domcke"
      ],
      "year": "2020",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b466",
      "title": "Single nucleus multiomics identifies ZEB1 and MAFB as candidate regulators of Alzheimer's disease-specific cisregulatory elements",
      "authors": [
        "A G Anderson"
      ],
      "year": "2023",
      "venue": "Cell Genom",
      "doi": ""
    },
    {
      "id": "b467",
      "title": "A universal approach for integrating super large-scale single-cell transcriptomes by exploring gene rankings",
      "authors": [
        "H Shen"
      ],
      "year": "2022",
      "venue": "Brief Bioinform",
      "doi": ""
    },
    {
      "id": "b468",
      "title": "Multiplexed droplet single-cell RNA-sequencing using natural genetic variation",
      "authors": [
        "H M Kang"
      ],
      "year": "2018",
      "venue": "Nat Biotechnol",
      "doi": ""
    },
    {
      "id": "b469",
      "title": "Global characterization of T cells in non-small-cell lung cancer by single-cell sequencing",
      "authors": [
        "X Guo"
      ],
      "year": "2018",
      "venue": "Nat Med",
      "doi": ""
    },
    {
      "id": "b470",
      "title": "Landscape of Infiltrating T Cells in Liver Cancer Revealed by Single-Cell Sequencing",
      "authors": [
        "C Zheng"
      ],
      "year": "2017",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b471",
      "title": "CellPLM: pre-training of cell language model beyond single cells",
      "authors": [
        "H Wen"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b472",
      "title": "Single-Cell Transcriptome Analysis Reveals Dynamic Cell Populations and Differential Gene Expression Patterns in Control and Aneurysmal Human Aortic Tissue",
      "authors": [
        "Y Li"
      ],
      "year": "2020",
      "venue": "Circulation",
      "doi": ""
    },
    {
      "id": "b473",
      "title": "Neuronal vulnerability and multilineage diversity in multiple sclerosis",
      "authors": [
        "L Schirmer"
      ],
      "year": "2019",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b474",
      "title": "scGREAT: Transformer-based deep-language model for gene regulatory network inference from single-cell transcriptomics. iScience",
      "authors": [
        "Y Wang"
      ],
      "year": "2024",
      "venue": "scGREAT: Transformer-based deep-language model for gene regulatory network inference from single-cell transcriptomics. iScience",
      "doi": ""
    },
    {
      "id": "b475",
      "title": "Single-cell RNA-seq reveals novel regulators of human embryonic stem cell differentiation to definitive endoderm",
      "authors": [
        "L F Chu"
      ],
      "year": "2016",
      "venue": "Genome Biol",
      "doi": ""
    },
    {
      "id": "b476",
      "title": "Differences and similarities between human and chimpanzee neural progenitors during cerebral cortex development",
      "authors": [
        "F Mora-Bermudez"
      ],
      "year": "2016",
      "venue": "Differences and similarities between human and chimpanzee neural progenitors during cerebral cortex development",
      "doi": ""
    },
    {
      "id": "b477",
      "title": "Multilineage communication regulates human liver bud development from pluripotency",
      "authors": [
        "J G Camp"
      ],
      "year": "2017",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b478",
      "title": "Single-cell RNA-seq reveals dynamic paracrine control of cellular variation",
      "authors": [
        "A K Shalek"
      ],
      "year": "2014",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b479",
      "title": "Single-cell full-length total RNA sequencing uncovers dynamics of recursive splicing and enhancer RNAs",
      "authors": [
        "T Hayashi"
      ],
      "year": "2018",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b480",
      "title": "A single-cell resolution map of mouse hematopoietic stem and progenitor cell differentiation",
      "authors": [
        "S Nestorowa"
      ],
      "year": "2016",
      "venue": "Blood",
      "doi": ""
    },
    {
      "id": "b481",
      "title": "BIOFORMERS: A SCALABLE FRAMEWORK FOR EXPLORING BIOSTATES USING TRANSFORMERS",
      "authors": [
        "S Amara-Belgadi"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b482",
      "title": "Massively parallel digital transcriptional profiling of single cells",
      "authors": [
        "G X Zheng"
      ],
      "year": "2017",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b483",
      "title": "scPRINT: pre-training on 50 million cells allows robust gene network predictions",
      "authors": [
        "J Kalfon"
      ],
      "year": "2024",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b484",
      "title": "The landscape of immune dysregulation in Crohn's disease revealed through single-cell transcriptomic profiling in the ileum and colon",
      "authors": [
        "L Kong"
      ],
      "year": "2023",
      "venue": "Immunity",
      "doi": ""
    },
    {
      "id": "b485",
      "title": "Single-cell multiome of the human retina and deep learning nominate causal variants in complex eye diseases",
      "authors": [
        "S K Wang"
      ],
      "year": "2022",
      "venue": "Cell Genom",
      "doi": ""
    },
    {
      "id": "b486",
      "title": "High-resolution Slide-seqV2 spatial transcriptomics enables discovery of disease-specific cell neighborhoods and pathways. iScience",
      "authors": [
        "J L Marshall"
      ],
      "year": "2022",
      "venue": "High-resolution Slide-seqV2 spatial transcriptomics enables discovery of disease-specific cell neighborhoods and pathways. iScience",
      "doi": ""
    },
    {
      "id": "b487",
      "title": "Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens",
      "authors": [
        "A Dixit"
      ],
      "year": "2016",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b488",
      "title": "ChIP-seq: advantages and challenges of a maturing technology",
      "authors": [
        "P J Park"
      ],
      "year": "2009",
      "venue": "Nat Rev Genet",
      "doi": ""
    },
    {
      "id": "b489",
      "title": "A Proximal-to-Distal Survey of Healthy Adult Human Small Intestine and Colon Epithelium by Single-Cell Transcriptomics",
      "authors": [
        "J Burclaff"
      ],
      "year": "2022",
      "venue": "Cell Mol Gastroenterol Hepatol",
      "doi": ""
    },
    {
      "id": "b490",
      "title": "Cell atlas of the human ocular anterior segment: Tissue-specific and shared cell types",
      "authors": [
        "T Van Zyl"
      ],
      "year": "",
      "venue": "Proc Natl Acad Sci",
      "doi": ""
    },
    {
      "id": "b491",
      "title": "Single-cell analysis of mouse and human prostate reveals novel fibroblasts with specialized distribution and microenvironment interactions",
      "authors": [
        "D B Joseph"
      ],
      "year": "2021",
      "venue": "J Pathol",
      "doi": ""
    },
    {
      "id": "b492",
      "title": "Phenotype prediction from single-cell RNA-seq data using attention-based neural networks",
      "authors": [
        "Y Mao"
      ],
      "year": "2024",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b493",
      "title": "A blood atlas of COVID-19 defines hallmarks of disease severity and specificity",
      "authors": [
        "Julian",
        "C O B A C E -M.-O",
        "C O B A -M.-O",
        "Consortium"
      ],
      "year": "2022",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b494",
      "title": "Single-cell multi-omics analysis of the immune response in COVID-19",
      "authors": [
        "E Stephenson"
      ],
      "year": "2021",
      "venue": "Nat Med",
      "doi": ""
    },
    {
      "id": "b495",
      "title": "COVID-19 immune features revealed by a large-scale single-cell transcriptome atlas",
      "authors": [
        "X Ren"
      ],
      "year": "2021",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b496",
      "title": "CancerFoundation: A single-cell RNA sequencing foundation model to decipher drug resistance in cancer",
      "authors": [
        "A Theus"
      ],
      "year": "2024",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b497",
      "title": "An Integrative Model of Cellular States, Plasticity, and Genetics for Glioblastoma",
      "authors": [
        "C Neftel"
      ],
      "year": "2019",
      "venue": "Cell",
      "doi": ""
    },
    {
      "id": "b498",
      "title": "Survboard: standardised benchmarking for multi-omics cancer survival models",
      "authors": [
        "D Wissel"
      ],
      "year": "2022",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b499",
      "title": "The Cancer Genome Atlas Pan-Cancer analysis project",
      "authors": [
        "N"
      ],
      "year": "2013",
      "venue": "Cancer Genome Atlas Research",
      "doi": ""
    },
    {
      "id": "b500",
      "title": "mcBERT: Patient-Level Single-cell Transcriptomics Data Representation",
      "authors": [
        "B V Querfurth"
      ],
      "year": "2024",
      "venue": "mcBERT: Patient-Level Single-cell Transcriptomics Data Representation",
      "doi": ""
    },
    {
      "id": "b501",
      "title": "Single-nucleus profiling of human dilated and hypertrophic cardiomyopathy",
      "authors": [
        "M Chaffin"
      ],
      "year": "2022",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b502",
      "title": "Single-cell transcriptomics reveals cell-type-specific diversification in human heart failure",
      "authors": [
        "A L Koenig"
      ],
      "year": "2022",
      "venue": "Nat Cardiovasc Res",
      "doi": ""
    },
    {
      "id": "b503",
      "title": "Pathogenic variants damage cell composition and single cell transcription in cardiomyopathies",
      "authors": [
        "D Reichart"
      ],
      "year": "1984",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b504",
      "title": "Spatial multi-omic map of human myocardial infarction",
      "authors": [
        "C Kuppe"
      ],
      "year": "2022",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b505",
      "title": "An atlas of healthy and injured cell states and niches in the human kidney",
      "authors": [
        "B B Lake"
      ],
      "year": "2023",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b506",
      "title": "Decoding myofibroblast origins in human kidney fibrosis",
      "authors": [
        "C Kuppe"
      ],
      "year": "2021",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b507",
      "title": "Defining cellular complexity in human autosomal dominant polycystic kidney disease by multimodal single cell analysis",
      "authors": [
        "Y Muto"
      ],
      "year": "2022",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b508",
      "title": "Multimodal single cell sequencing implicates chromatin accessibility and genetic background in diabetic kidney disease progression",
      "authors": [
        "P C Wilson"
      ],
      "year": "2022",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b509",
      "title": "Single cell transcriptional and chromatin accessibility profiling redefine cellular heterogeneity in the adult human kidney",
      "authors": [
        "Y Muto"
      ],
      "year": "2021",
      "venue": "Nat Commun",
      "doi": ""
    },
    {
      "id": "b510",
      "title": "Single-cell RNA-seq reveals cell type-specific molecular and genetic associations to lupus",
      "authors": [
        "R K Perez"
      ],
      "year": "1970",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b511",
      "title": "Local and systemic responses to SARS-CoV-2 infection in children and adults",
      "authors": [
        "M Yoshida"
      ],
      "year": "2022",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b512",
      "title": "An integrated cell atlas of the lung in health and disease",
      "authors": [
        "L Sikkema"
      ],
      "year": "2023",
      "venue": "Nat Med",
      "doi": ""
    },
    {
      "id": "b513",
      "title": "Nicheformer: a foundation model for single-cell and spatial omics",
      "authors": [
        "A Schaar"
      ],
      "year": "2024",
      "venue": "Nicheformer: a foundation model for single-cell and spatial omics",
      "doi": ""
    },
    {
      "id": "b514",
      "title": "A high-resolution transcriptomic and spatial atlas of cell types in the whole mouse brain",
      "authors": [
        "Z Yao"
      ],
      "year": "2023",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b515",
      "title": "High-plex multiomic analysis in FFPE at subcellular level by spatial molecular imaging",
      "authors": [
        "S He"
      ],
      "year": "2021",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b516",
      "title": "Single cells are spatial tokens: Transformers for spatial transcriptomic data imputation",
      "authors": [
        "H Wen"
      ],
      "year": "2023",
      "venue": "Single cells are spatial tokens: Transformers for spatial transcriptomic data imputation",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Abstract",
      "text": "Large language models (LLMs) are a class of artificial intelligence models based on deep learning, which have great performance in various tasks, especially in natural language processing (NLP). Large language models typically consist of artificial neural networks with numerous parameters, trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. However, their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language. In this review, we will provide a comprehensive overview of the essential components of large language models (LLMs) in bioinformatics, spanning genomics, transcriptomics, proteomics, drug discovery, and single-cell analysis. Key aspects covered include tokenization methods for diverse data types, the architecture of transformer models, the core attention mechanism, and the pre-training processes underlying these models. Additionally, we will introduce currently available foundation models and highlight their downstream applications across various bioinformatics domains. Finally, drawing from our experience, we will offer practical guidance for both LLM users and developers, emphasizing strategies to optimize their use and foster further innovation in the field."
    },
    {
      "title": "1 Introduction",
      "text": "Significant progress has been made in the field of natural language processing with the advent of large language models. Examples of these models include OpenAI's GPT-X [1] and Google's BERT [2] models. These models are transformative because they can understand, generate, and manipulate human language at an unprecedented scale. Vast Large language models are typically trained on datasets that encompass a significant portion of the internet's text, enabling them to learn the complexities of language and context. These models are built upon a neural network architecture called transformers [3]. The transformer architecture revolutionized NLP due to its parallelization, scalability, and ability to capture long-range dependencies in text. Instead of relying on recurrent or convolutional layers, transformers use self-attention mechanisms, as previously described, which allow them to assess the importance of every word in a sentence when understanding context. This innovation is key to their remarkable performance. The training regimen for large language models comprises two phases: pre-training and fine-tuning. During pre-training, the model is trained on an extensive corpus of text data to acquire proficiency in grammar, factual knowledge, reasoning abilities, and word understanding. Fine-tuning tailorsthese models for specific tasks like translation, summarization, or question-answering. The adaptability of large language models is a major advantage; they can excel at various NLP tasks without task-specific architectures. However, they have found applications in diverse fields beyond NLP, including biology, healthcare, education, finance, customer service, and more. In particular, there have been many successful applications of large language models in the field of bioinformatics. In this manuscript, we focus on the applications of large language models to several bioinformatic tasks through five areas: DNA level, RNA level, protein level, drug discovery and single-cell analysis. Applications of LLMs in genomics focus on LLMs using DNA sequence; applications of LLMs focus on in transcriptomics using RNA sequence; applications of LLMs in proteomics focus on LLMs using protein sequence; applications of LLMs in drug discovery focus on LLMs using Molecular SMILES (seq) and applications of LLMs in single-cell analysis focus on LLMs using scRNA-seq, scMulti-omics and spatial transcriptomics data (**Figure 1**)."
    },
    {
      "title": "2 Understanding The Building Blocks Of Large Language Models In Bioinformatics",
      "text": "Building large language models involves several critical components, including tokenization methods, embedding techniques, attention mechanisms, transformer architectures, and the training processes for large-scale models. Each of these elements plays a vital role in enabling the models to process, understand, and generate complex data."
    },
    {
      "title": "Tokenization And Input Embedding",
      "text": "Tokenization methods are essential for processing raw input data, breaking it down into smaller, manageable units (tokens) that can be analyzed and processed by models. The choice of tokenization method varies depending on the type of data being handled (**Figure 2a, Table 1**). In DNA and RNA sequence data, tokenization converts raw nucleotide sequences (A, T, C, G for DNA or A, U, C, G for RNA) into a numerical format suitable for computational models. A common method is one-hot encoding, where each nucleotide is represented as a binary vector with a '1' indicating its position (e.g., [1, 0, 0, 0] for A in DNA), as used in RNA-FM [4] and RNA-MSM [5]. Another widely adopted approach is k-mer tokenization, which segments sequences into overlapping substrings of fixed length 'k' (e.g., for k=3, \"ATGC\" becomes \"ATG\" and \"TGC\"). This method is employed in models like DNABERT[6], DNAGPT [7], and RNABERT [8]. Additionally, specialized tokens such as '[IND]' can be introduced to mark the start or end of sequences or to handle unknown characters or gaps, as demonstrated in RNAErnie [9]. In protein language models, the input data primarily includes multiple sequence alignments (MSAs), protein sequences, biomedical/biological text, and cDNA. The basic units of MSAs and protein sequences are amino acids, leading most protein language models to use Single Amino Acid Tokenization, where protein sequences are segmented into individual amino acids. This approach is akin to the k-mers method used for DNA and RNA sequences and is employed in models such as ESM-1b [10], ProtTrans [11], and ProGen [12]. For biomedical and biological text, including general descriptions, conditioning tags in generative models, and resources like Gene Ontology (GO), tokenization methods from natural language processing (NLP) are widely used. Methods like WordPiece Tokenization build vocabulary using frequency-based greedy algorithms and segment text into discrete tokens, as demonstrated in ProtST [13]. For cDNA data, tokenization is similar to that of protein sequences but differs in the basic unit. Instead of amino acids, sequences are tokenized into codons, or triplets of nucleotides, as seen in CaLM [14]. In drug discovery, small molecule drugs account for 98% of commonly used medications [1]. LLMs leverage four main tokenization methods to uncover molecular patterns and drug-target interactions. Atom-level tokenization treats molecules as sequences of individual atoms, analogous to character-level text representation, as seen in K-BERT [15]. MolGPT [16] utilizes a SMILES tokenizer that segments molecular structures into units such as atoms, bond types, and ring markers. A Graph-based VQ-VAE approach enhances this by encoding atoms into context-aware discrete values, distinguishing roles like aldehyde versus ester carbons, based on latent codes derived from a graph-based Vector Quantized Variational Autoencoder (VQ-VAE). This method categorizes atoms into chemically meaningful sub-classes, enriching the molecular vocabulary. Fingerprint tokens, another method, represent molecules through binary or numerical vectors summarizing molecular properties or structural patterns, as seen in SMILES-BERT [17]. Tokenization methods for single-cell profiles include four main strategies. Gene ranking/reindexing-based methods rank genes by expression levels and create tokens using ranked gene symbols or unique integer identifiers, as seen in Geneformer [18] and tGPT [19]. Binning-based methods divide gene expression into predefined intervals, assigning tokens based on the corresponding bin, used in models like scBERT [20] and scGPT [21]. Gene set or pathway-based methods group genes into biologically meaningful sets, such as pathways or Gene Ontology terms,with tokens representing the activation of these sets, exemplified by TOSICA [22]. Patch-based methods segment gene expression vectors into equal-sized sub-vectors, as seen in CIForm [152]. Alternatively, convolutional neural networks (CNNs) can be used to transform the reshaped gene expression matrix into several flattened 2D patches, as demonstrated by scTranSort [23]. Another variation involves reshaping the sub-vectors into a gene expression matrix after segmentation, as employed in scCLIP [24]. In addition to the four methods mentioned above, a more direct approach involves projecting gene expression directly, as seen in models like scFoundation [25], and scMulan [26]. Alternatively, some methods tokenize cells instead of genes, as exemplified by models such as CellPLM [27], ScRAT [28], and mcBERT [29], which utilize cell tokens during model training (**Table 1**). These strategies allow models to capture biological structure and variability, tailoring tokenization to single-cell data characteristics. After tokenization, embedding converts tokens into continuous vector representations, capturing the semantic relationships between them. Positional encoding represents the token order by adding vectors that encode the relative or absolute positions of tokens in the sequence. The final step involves combining the token embeddings with the positional embeddings to create a unified input embedding, which is then fed into the model for further processing (**Figure 2b**)."
    },
    {
      "title": "Architecture Of Transformer Models",
      "text": "Transformers are the foundational architecture in large language models (LLMs) and consist of two main components: the encoder and the decoder. The encoder takes the input data and processes it in parallel across multiple layers to capture relationships within the sequence. The decoder, on the other hand, generates output sequences based on the encoder's processed information, typically used in tasks like translation or text generation. Each component is built on layers of multi-head attention, add and norm layer, and feed-forward layer (**Figure 2c**). **Attention Mechanism:** A key innovation of the transformer is the attention mechanism, particularly self-attention [3], which allows the model to weigh the importance of different tokens in a sequence relative to each other. In self-attention, each token computes a score based on how much attention it should pay to other tokens in the sequence. This is done by calculating three key components: Query (Q), Key (K), and Value (V) vectors for each token (**Figure 2d**). The attention score is computed as the dot product between the Query of one token and the Key of another token, followed by a softmax operation to normalize the scores. These scores are then used to weight the Value vectors, which are aggregated to form the output representation for each token as following [3]: \\[Attention(Q,K,V)=softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}})V \\tag{1}\\] **Multi-head attention** extends this idea by running multiple attention mechanisms (or \"heads\") in parallel. Each attention head processes the input tokens in a slightly different way by using different sets of learned weights for the Q, K, and V vectors. The results of all heads are concatenated and linearly transformed, allowing the model to capture different aspects of relationships between tokens simultaneously. This mechanism enables the model to focus on various parts of the input sequence at once, learning different types of interactions between tokens. For example, in single-cell foundation models, self-attention can help identify important gene interactions by determining which genes (tokens) should focus on each other during processing. In this way, multi-head attention allows the model to capture complex relationships between genes in single-cell RNA-seq data, where multiple aspects of gene expression (such as co-expression patterns or functional relationships) need to be captured simultaneously. **Add and norm layer:** The add and norm layer performs layer normalization and residual connections, which help stabilize training by ensuring that the output from each layer is added to the input before being normalized. This allows for smoother gradient flow and avoids the vanishing gradient problem. **Feed-forward layer:** After the attention mechanism, the feed-forward network is a fully connected neural network (**Figure 2e**), helping the model learn complex mappings and capture more abstract representations of the input data. **2.3 BERT and GPT models** BERT and GPT stand as two exceptional language models. Both BERT and GPT leverage the transformer architecture, employing attention mechanisms to grasp dependencies within input data. **BERT (Bidirectional Encoder Representations from Transformers).** BERT is basically an encoder stack of transformer architecture, which was introduced by Google in 2018 [2]. BERT is trained using a bidirectional approach, meaning it considers context from both the left and right of each token during training. This enables BERT to capture richer, more context-aware representations. BERT is typically pre-trained using a masked language model (MLM) task, where random tokens in a sequence are masked, and the model is tasked with predicting them. This bidirectional training allows BERT to better understand the full context of a sequence or biological sentence, such as a cell (**Figure 2f**). For example, scBERT, a single-cell adaptation of BERT, applies this approach to single-cell RNA-seq data. By masking random gene tokens and predicting them during pretraining, scBERT learns complex dependencies and co-expression patterns between genes. This enables it to capture the full transcriptional context of individual cells, improving downstream tasks like cell type classification. **GPT (Generative Pretrained Transformer).** Introduced by OpenAI [1], GPT is based on a decoder stack of transformer architecture. Unlike BERT, GPT uses a unidirectional training approach, processing the input sequence from left to right (**Figure 2f**). It is trained using autoregressive learning, where each token is predicted based on the previous ones, making it particularly suited for generational tasks. GPT excels in zero-shot learning, where they perform tasks without needing task-specific training data. For example, DNAGPT leverages its pretrained knowledge to perform tasks like predicting DNA motifs or identifying regulatory elements without explicit task-specific training. When prompted with a sequence such as \"Find the transcription factor binding motif in the following DNA sequence: AGCTTAGGCC...\", DNAGPT can identify or generate plausible motifs based on its understanding of DNA patterns learned during pretraining."
    },
    {
      "title": "3 Foundation Models In Bioinformatics",
      "text": ""
    },
    {
      "title": "Key Components Of Biological Foundation Models",
      "text": "Foundation models are a category of large-scale, pre-trained models designed to be versatile and adaptable to various downstream tasks. They are built upon several fundamental components that enable their widespread applicability and effectiveness across domains. First, foundation models are trained on extensive and diverse datasets to capture broad, generalizable patterns. In single-cell biology, for example, datasets with millions of cells spanning multiple tissues and conditions are often used. Second, the architecture of foundation models is typically designed for flexibility and scalability. Their architecture, often transformer-based (e.g., GPT and BERT), are specifically designed for flexibility and scalability. Third, Self-supervised learning is a core training strategy for foundation models. By creating tasks such as masked prediction, contrastive learning, or next-token prediction, models can learn representations without requiring labeled data. Fourth, foundation models exhibit multi-task transferability, leveraging a two-step process of pre-training and fine-tuning (**Figure 3**). During pre-training, these models are trained on large-scale datasets to develop robust generalization capabilities by capturing broad patterns and knowledge. Finetuning involves adapting the pre-trained model to specific tasks by exposing it to unique data and additional training. This approach enables foundation models to adjust effectively to diverse applications while maintaining their versatility across a wide range of domains. Last but not least, training foundation models requires significant computational resources, often involving GPU or TPU clusters. Foundation models typically feature billions or even trillions of parameters. **3.2 Foundation models in different biological domains** **DNA foundation models.** Currently, DNA sequence-based foundation models are powerful tools that leverage advanced deep learning architectures to analyze and interpret genomic data [30]. These models are built on frameworks like BERT and GPT, which have been adapted for the specific challenges of genomic sequences (**Table 2**). For example, DNABERT [6]is a BERT-based model trained on the human reference genome, enabling it to capture the contextual relationships between nucleotides and perform tasks such as sequence classification and variant prediction. Expanding beyond a single species, Nucleotide Transformer [31] and Genomic Pre-trained Network (GPN) [32] are transformer-based models that incorporate the multiple species reference genomes, providing a broader understanding of genomic diversity. DNABERT-2 [33] takes this a step further by training on multi-species genomic data from 135 species, allowing for cross-species genomic analysis. Similarly, GROVER [34], another BERT-based model, is focused on the human reference genome and is designed for applications such as understanding gene expression and functional genomics. On the other hand, DNAGPT, based on the GPT architecture, is trained not only on the human reference genome but also on reference genomes from nine other species, facilitating tasks such as sequence generation and evolutionary analysis. Together, these DNA sequence-based foundation models represent a leap forward in computational genomics, enabling more accurate predictions, better understanding of genetic variation, and advancements in personalized medicine. **RNA foundation models.** RNA sequence-based language models, particularly BERT-based and Transformer-based models, have gained significant traction in the analysis of RNA sequences due to their ability to understand the complex patterns and structures of RNA. These models are trained using a wide variety of RNA types, including non-coding RNAs (ncRNAs), coding RNA, and untranslated regions (UTRs), across diverse organisms (**Table 2**). For instance, RNABERT [8], RNA-FM [4], RNA-MSM [5], and UNI-RNA [35] focus on all ncRNA types from a broad range of species, enabling insights into RNA function and interactions. Models like SpliceBERT [36]specialize in coding RNA sequences from 72 vertebrates, while 3UTRBERT [37] is specifically designed for human mRNA transcripts, particularly the 3' untranslated regions. Additionally, UTR-LM [38] focuses on 5' UTR sequences from five species, and RNAErnie [9], a Transformer-based model, covers a wide range of ncRNAs. These models are part of a rapidly growing field aimed at advancing RNA sequence analysis, facilitating the study of RNA biology and its role in various biological processes and diseases. Through the use of these RNA-based language models, researchers can make significant strides in understanding RNA structure, function, and regulatory mechanisms. **Protein foundation models.** Foundation models for proteins can be directly utilized to obtain high-quality protein embeddings and support various downstream applications. The foundational protein models listed in **Table 2** not only fulfill these requirements but also exhibit unique characteristics. For example, TAPE [39] made a significant contribution by introducing a comprehensive benchmark for protein bioinformatics tasks. ESM-1b [40] applied the transformer architecture of large language models in a highly standardized manner to protein representation learning. This model has since been widely used to generate protein sequence embeddings, and its variants can also be found via the same link provided in **Table 2**. ProtTrans [11], compared to ESM-1b, significantly expanded the model architecture, the number of parameters, and the size of the training dataset. It has been widely adopted as a frozen encoder for protein sequences. ProtGPT2 [41], as its name suggests, extends GPT-2 into the protein domain (with links providing details on the GPT-2 training framework). Recent foundation models like ProtBert [42] and KeAP [43] integrate biomedical text information alongside protein sequences. Notably, KeAP incorporates a knowledge graph to enhance this integration. Both models demonstrate that multimodal fusion within proteomics often produces more expressive features. CaLM [14], on the other hand, represents proteins using cDNA, embedding cross-omics biological information. From the perspective of algorithmic advancements, the integration of multimodal information within a single omics domain, as well as cross-omics data fusion, represents key strategies for constructing unified large-scale biological models. **Drug discovery foundation models.** It has been postulated that the total number of potential drug like candidates range from \\(10^{23}\\) to \\(10^{60}\\) molecules[44]. Foundation models leverage diverse tokenization strategies, embedding techniques, and pre-training mechanisms to enhance molecular representation learning, facilitating the optimization of various downstream tasks (**Table 2**). For instance, Mol-BERT [45] employs a context-aware tokenizer to encode atoms into chemically meaningful discrete values, although this approach results in an unbalanced atom vocabulary. SMILES-BERT [46], a semi-supervised model incorporating an attention-based Transformer architecture, utilizes datasets such as LogP, PM2, and PCBA-686978 to pre-train the model via a Masked SMILES Recovery (MSR) task. This model demonstrates strong generalization capabilities, enabling its application to diverse molecular property prediction tasks through fine-tuning. Similarly, Mol-GPT [47] facilitates the generation of molecules with specific scaffolds and desired molecular properties by conditioning the generation process on scaffold SMILES strings and property values. Notably, SynerGPT [48] enables a pre-trained GPT model to perform in-context learning of \"drug synergy functions\", showcasing potential for future advancements in personalized drug discovery. These foundation models developed based on distinct strategies, effectively learn representations from raw sequence data and molecular descriptors. They provide significant insights into the design of small-molecule drugs, drug-drug interactions, and drug-target interactions. **Single-cell foundation models.** Foundation models in single-cell analysis are revolutionizing the field by offering scalable and versatile solutions for a wide range of tasks, leveraging both cell and gene-level representations (**Table 2**). Models like scBERT [20], tGPT [19], scMulan [26], UCE [49] and CancerFoundation [50] focus on learning robust cell representations, effectively supporting applications such as cell clustering, cell type annotation, batch effect correction, trajectory inference and drug response prediction. These models excel at analyzing heterogeneous cellular populations and uncovering cellular dynamics. In contrast, models like scGPT [21], scFoundation [25], Geneformer [18] GeneCompass [51] and scPRINT [52] combine the ability to learn both cell and gene-level representations. They capture inter-gene relationships and regulatory networks, making them highly effective for tasks such as gene expression profiling, gene regulatory network (GRN) inference, gene perturbation prediction, and drug dose-response prediction. Notably, scGPT can also handle single-cell multi-omics data, facilitating tasks like scRNA-seq and scATAC-seq integration. Another notable model is Nicheformer [53], a foundation model specifically designed for spatial transcriptomics. It focuses on learning cell representations while being highly adaptable to various downstream tasks in spatial transcriptomics, such as spatial label prediction (e.g., cell type, niche, and region labels), niche composition analysis, and neighborhood density prediction. Additionally, Nicheformer can generate joint embeddings of scRNA-seq and spatial transcriptomics data, facilitating the integration of these modalities for a more comprehensive understanding of cellular and spatial interactions."
    },
    {
      "title": "4 Applications Of Large Language Models In Bioinformatics",
      "text": "Large language models (LLMs) have seen numerous successful applications in bioinformatics, addressing a wide array of tasks across DNA, RNA, protein, drug discovery, and single-cell analysis (**Figure 4**). These applications highlight the adaptability and potential of LLMs in overcoming bioinformatic challenges, enabling deeper insights into complex biological systems and fostering advancements across multiple domains."
    },
    {
      "title": "Applications Of Large Language Models In Genomics",
      "text": "The DNA language models take DNA sequence as input, use transformer, BERT, GPT models to solve multiple biological tasks, including genome-wide variant effects prediction, DNA cis-regulatory regions prediction, DNA-protein interaction prediction, DNA methylation (6mA,4mC 5hmC) prediction, splice sites prediction from DNA sequence (**Table 3, Supplementary Figure 1**). A detailed list of DNA language models, their downstream tasks, and the datasets used can be found in **Supplementary Table 1.** **Genome-wide variant effects prediction.** Genome-wide variant effects prediction is crucial for understanding the role of DNA mutations in species diversity. Genome-wide association studies (GWAS) provide valuable insights but often struggle to identify specific causal variants [30, 54]. The Genome Prediction Network (GPN) [32] addresses this by using unsupervised pre-training on genomic DNA sequences. During this process, GPN predicts nucleotides at masked positions within a 512-bp DNA sequence. This model is particularly effective at predicting rare variant effects, often missed by traditional GWAS methods. Additionally, models like DNABERT, DNABERT-2, and the Nucleotide Transformer also predict variant effects from DNA sequences. These advancements highlight ongoing efforts to better understand how DNA mutations contribute to biological diversity. **Cis-regulatory regions prediction.** Cis-regulatory sequences, such as enhancers and promoters, play crucial roles in gene expression regulation, influencing development and physiology [55]. However, identifying these sequences remains a major challenge [56]. Pre-trained models like DNABERT, DNABERT-2, GROVER, and DNAGPT have been developed to predict promoter regions and their activities with high accuracy. BERT-Promoter [57] utilizes a pre-trained BERT model for feature representation and SHAP analysis to filter data, improving prediction performance and generalization over traditional methods. Enhancers, which bind transcription factors to regulate gene expression [58, 59], are predicted by iEnhancer-BERT [60], which leverages DNABERT and uses a novel transfer learning approach. This model employs output from all transformer encoder layers and classifies features with a Convolutional Neural Network (CNN). These advancements highlight the growing trend of treating biological sequences as a natural language for computational modeling, offering new tools for identifying cis-regulatory regions and understanding their roles in diseases. **DNA-protein interaction prediction.** Accurate identification of DNA-protein interactions is crucial for gene expression regulation and understanding evolutionary processes [61]. Several DNA language models, including DNABERT, DNABERT-2, and GROVER, have been developed to predict protein-DNA binding from ChIP-seq data. TFBert [62] is a pre-trained model specifically designed for DNA-protein binding prediction, which treats DNA sequences as natural sentences and k-mer nucleotides as words, allowing effective context extraction. Pre-trained on 690 ChIP-seq datasets, TFBert delivers strong performance with minimal fine-tuning. The MoDNA [63] framework introduces domain knowledge by incorporating common DNA functional motifs. During self-supervised pre-training, MoDNA performs tasks such as k-mer and motif prediction. Pre-training on extensive unlabeled genome data, MoDNA acquires semantic-level genome representations, enhancing predictions for promoter regions and transcription factor binding sites. Essentially, MoDNA functions as a biological language model for DNA-protein binding prediction. **DNA methylation prediction.** DNA methylation is a key biological process in epigenetic regulation and is linked to various medical conditions and applications, such as metagenomic binning [64]. DNA methylation types depend on the nucleotide where the methyl group attaches [65]. Several models predict DNA methylation with varying accuracy. BERT6mA [66] is designed for predicting 6-methyladenine (6mA) sites, while iDNA-ABT [67], iDNA-ABF [68], and MuLan-Methyl [69] are versatile models predicting various methylation types (6mA, 5hmC, 4mC). iDNA-ABT, a deep learning model, integrates BERT with transductive information maximization (TIM), though it has yet to fully explore feature representation. iDNA-ABF uses a multi-scale architecture, applying multiple tokenizers for diverse embeddings, and MuLan-Methyl employs four transformer-based models (DistilBERT [70], ALBERT[71], XLNet [72], and ELECTRA [73]) to predict methylation sites, enhancing performance through joint model utilization. **DNA level splice site identification.** Accurate pre-mRNA splicing is essential for proper protein translation, driven by splice site selection. Identifying splice sites is challenging, particularly with prevalent GT-AG sequences [74]. To address this, DNABERT and DNABERT-2 were developed, trained on 10,000 donors, acceptor, and non-splice site sequences from the human reference genome to predict splice sites. DNABERT showed high attention to intronic regions, suggesting the functional role of intronic splicing enhancers and silencers as cis-regulatory elements in splicing regulation. This highlights DNABERT's potential in understanding splicing mechanisms. **4.2 Applications of large language models in transcriptomics** The RNA language models take RNA sequences as input, use transformer, BERT, GPT models to solve multiple biological tasks, including RNA 2D/3D structure prediction, RNA structural alignment,, RNA family clustering, RNA splice sites prediction from RNA sequence, RNA N7-methylguanosine modification prediction, RNA 2'-O-methylation modifications prediction, multiple types of RNA modifications prediction, predicting the association between miRNA, lncRNA and disease, identifying lncRNAs, lncRNAs' coding potential prediction, protein expression and mRNA degradation prediction (**Table 3, Supplementary Figure 1**). A detailed list of RNA language models, their downstream tasks, and the datasets used can be found in **Supplementary Table 1.** **Secondary structure prediction.** RNA secondary structure prediction is a major challenge for RNA structural biologists, with models holding potential for RNA-targeting drug development [75]. Several RNA language models, such as RNABERT [8], RNA-MSM [5], RNA-FM [4], and UNI-RNA [35], have been developed to predict RNA structures with varying sophistication. RNABERT uses BERT architecture to predict structural features like base-pairing and stem loops. RNA-MSM integrates sequence and structural information to predict local and long-range folding patterns. RNA-FM focuses on RNA folding, stability, and energetics, including pseudoknots. UNI-RNA combines sequence and structure predictions across various RNA types. These models advance RNA structure prediction by applying deep learning and advanced techniques to improve understanding of RNA folding and function. **RNA splicing prediction**. RNA splicing is crucial for gene expression in eukaryotes, and advancements have been made in sequence-based splicing modeling through models like SpliceBERT [36] and UNI-RNA [35]. SpliceBERT, based on BERT, is trained to predict RNA splicing events by capturing long-range dependencies, identifying splice sites, and predicting alternative splicing events. UNI-RNA, a more generalized model, integrates multiple RNA tasks, including splicing, and combines sequence and structural data to predict splicing regulatory elements and interactions with splicing factors. These models enhance the understanding of RNA splicing, gene regulation, and its role in diseases, providing powerful tools for studying splicing defects and mutations. **lncRNAs identification and lncRNAs' coding potential prediction.** Long non-coding RNAs (lncRNAs) play significant regulatory roles in cancer and diseases, and their small Open Reading Frames (sORFs), once thought weak in protein translation, are now known to encode peptides [76]. Identifying lncRNAs with sORFs is crucial for discovering new regulatory factors. LncCat [77] addresses this challenge by using category boosting and ORF-attention features, including BERT for peptide sequence representation, to improve prediction accuracy for both long ORF and sORF datasets. It demonstrates effectiveness across multiple species and Ribo-seq datasets in identifying lncRNAs with sORFs. In predicting translatable sORFs in lncRNAs (lncRNA-sORFs), LSCPP-BERT [78] is a novel method designed for plants, leveraging pre-trained transformer models for reliable coding potential prediction. LSCPP-BERT is poised to impact drug development and agriculture by enhancing understanding of lncRNA coding potential. **RNA-RBP interactions prediction.** RNA sequences differ from DNA sequences by a single base (thymine to uracil), maintaining largely congruent syntax and semantics. BERT's versatility extends to Cross-linking and Immunoprecipitation data, particularly in predicting RNA-binding protein (RBP) binding preferences. BERT-RBP [79] is a model pre-trained on a human reference genome, designed to forecast RNA-RBP interactions. It outperforms existing models when tested on eCLIP-seq data from 154 RBPs and can identify transcript regions and RNA secondary structures based on sequence alone. BERT-RBP demonstrates BERT's adaptability in biological contexts and its potential to advance RNA-protein interaction understanding. **RNA-RNA interaction prediction**. RNA-RNA interactions occur between various RNA species, including long non-coding RNAs, mRNAs, and small RNAs (e.g., miRNAs and lncRNAs), driven by complementary sequences, secondary structures, and other motifs [80]. Accurate prediction of these interactions provides insights into RNA-mediated regulation, enhancing understanding of biological processes like gene expression, splicing, and translation. RNAErnie, used for this purpose, employs a TBTH architecture combining RNAEmie with a hybrid network (CNN, BiLSTM, and MLP) to predict RNA-RNA interactions. This approach demonstrates RNAEmie's potential in advancing RNA-based regulatory network studies. **RNA modification prediction.** Post-transcriptional RNA modifications, such as N7-methylguanosine (m7G) and 2'-O-methylation (Nm), regulate gene expression and are linked to diseases [76, 81]. Identifying modification sites is essential but challenging due to the high cost and time required by experimental methods. Computational tools like BERT-m7G [82] and Bert2Ome [83] address this issue. BERT-m7G uses a stacking ensemble approach to identify m7G sites directly from RNA sequences, offering an efficient, cost-effective alternative. Bert2Om combines BERT and CNN to predict 2'-O-methylation sites, outperforming existing methods across datasets and species. These tools enhance the accuracy, scalability, and efficiency of RNA modification site identification, advancing research into RNA modifications and their roles in gene regulation and disease. **Protein expression and mRNA degradation prediction.** mRNA vaccines are a cost-effective, rapid, and safe alternative to traditional vaccines, showing high potency [84]. These vaccines work by introducing mRNA that encodes a viral protein. CodonBERT [85] is a model specifically designed for mRNA sequences to predict protein expression. It uses a multi-head attention transformer architecture and was pre-trained on 10 million mRNA sequences from various organisms. This pre-training enables CodonBERT to excel in tasks like protein expression and mRNA degradation prediction. Its ability to integrate new biological information makes it a valuable tool for mRNA vaccine development. CodonBERT surpasses existing methods, optimizing mRNA vaccine design and improving efficacy and applicability in immunization. Its strength in predicting protein expression enhances mRNA vaccine development efficiency and effectiveness. **5' UTR-based mean ribosome loading prediction and mRNA subcellular localization prediction**. The 5' UTR sequence plays a critical role in regulating translation efficiency. RNA sequence models like 3UTRBERT, UNI-RNA, UTR-LM, RNA-FM, and Nucleotide Transformer have been developed to predict key features of the 5' UTR, focusing on ribosome loading efficiency and mRNA localization. These models use Transformer-based architecture to analyze sequence patterns, motifs, and structural elements. For example, 3UTRBERT [37] and RNA-FM [4] predict ribosome loading efficiency, identifying regions likely to recruit ribosomes for translation initiation. UTR-LM [38], UNI-RNA [35], and Nucleotide Transformer [31] predict mRNA subcellular localization, determining where mRNA will localize in the cell (cytoplasm, ribosomes, or nucleus), which is crucial for regulating mRNA stability and translation. Together, these models provide valuable insights into gene expression, translation control, and RNA localization, advancing molecular biology research. **4.3 Applications of large language models in proteomics** Protein is an indispensable molecule in life, assuming a pivotal role in the construction and sustenance of vital processes. As the field of protein research advances, there has been a substantial surge in the accumulation of protein data [86]. In this context, the utilization of large language models emerges as a viable approach to extract pertinent and valuable information from these vast reservoirs of data. Several pre-trained protein language models (PPLMs) have been proposed to learn the characteristic representations of proteins data (e.g., protein sequences, gene ontology annotations, property descriptions), then applied to different tasks by fine-tuning, adding or altering downstream networks, such as protein structure, post-translational modifications (PTMs), and biophysical properties, which align with corresponding downstream tasks like secondary structure prediction, major PTMs prediction, and stability prediction [87, 88]. Even though antibodies are classified as proteins, the datasets of antibodies and subsequent tasks differ significantly from those of proteins. Through the establishment and continuous updates of the Observed Antibody Space (OAS) database [89], a substantial amount of antibody sequence data has become available, which can be utilized to facilitate the development of pre-trained antibody large language models (PALMs). PALMs primarily delve into downstream topics encompassing therapeutic antibody binding mechanisms, immune evolution, and antibody discovery, which correspond to tasks like paratope prediction, B cell maturation analysis, and antibody sequence classification (**Table 3, Supplementary Figure 2**). In this section, some of the popular protein-related large language models of recent years are introduced, as well as corresponding important downstream tasks. It is important to emphasize that both PPLM and PALM are capable of handling not only the downstream tasks introduced in this section. For further details, additional information can be referenced within **Supplementary Table 2**. **Secondary structure and contact prediction.** Protein structure is critical to its function and interactions [90]. However, traditional experimental techniques for protein structure analysis are time-consuming and labor-intensive. With the rise of deep learning, large language models have demonstrated significant advantages in computational efficiency and prediction accuracy for protein structure prediction [91]. MSA Transformer [92] introduces a protein language model that processes MSAs using a unique mechanism of interleaved row and column attention. Trained with a MLM objective across diverse protein families, it outperformed earlier unsupervised approaches and showed greater parameter efficiency than previous models. Drawing on insights from BERT, large parameter models tend to achieve better performance for predicting secondary structures and contacts. Few models have more parameters than the largest models in ProtTrans [11], which includes a series of autoregressive models (Transformer-XL [93], XLNet [72]) and four encoder (BERT [2], Albert [71], Electra [73], T5 [94]) trained on datasets like UniRef [95] and BFD [96], comprising up to 393 billion amino acids. Model sizes vary from millions to billions of parameters. Notably, ProtTrans made a significant breakthrough in per-residue predictions. **Protein sequence generation.** Protein sequence generation holds significant potential in drug design and protein engineering [97]. Using machine learning or deep learning, generated sequences aim for good foldability, stable 3D structures, and specific functional properties, such as enzyme activity and antibody binding. The development of large language models, combined with conditional models, has greatly advanced protein generation [98]. ProGen [12] incorporates UniprotKB keywords as conditional tags, covering over 1,100 categories like 'biological process' and'molecular function'. Proteins generated by ProGen, assessed for sequence similarity, secondary structure, and conformational energy, exhibit desirable structural properties. In 2022, ProtGPT2 [41] inspired by GPT-x was developed. ProtGPT2-generated proteins show amino acid propensities like natural proteins. Prediction of disorder and secondary structure reveals that 88% of these proteins are globular, resembling natural sequences. Employing AlphaFold [99, 100] on ProtGPT2 sequences produces well-folded, non-idealized structures with unique topologies not seen in current databases, suggesting ProtGPT2 has effectively learned \"protein language\". **Protein function prediction.** Proteins are essential in cellular metabolism, signal transduction, and structural support, making their function critical for drug development and disease analysis. However, predicting and annotating protein functions is challenging due to their complexity. PPLMs offer effective solutions to these challenges [101, 102]. ProtST [103] introduced a multimodal framework combining a PPLM for sequences and a biomedical language model (BLM) for protein property descriptions. Through three pre-training tasks, unimodal mask prediction,multimodal representation alignment, and multimodal mask prediction, the model excels in tasks like protein function annotation, zero-shot classification, and functional protein retrieval from large databases. While most methods focus on increasing model parameters to improve performance, CaLM [14] introduces an alternative representation, the cDNA sequence, akin to an amino acid sequence, as input. The core idea lies in the relationship between synonymous codon usage and protein structure [104], and the information encoded in codons is no less than that of amino acids. Experimental results demonstrate that even with a small parameter language model, using cDNA sequences as input enhances performance in tasks such as protein function prediction, species recognition, prediction of protein and transcript abundance, and melting point estimation. **Major post-translational modification prediction.** Post-translational modifications (PTMs) are chemical changes, such as phosphorylation, methylation, and acetylation, that alter protein structure and function after translation. PTMs influence protein stability, localization, interactions, and function, making their study crucial for disease diagnosis and therapeutic strategies [105, 106]. Language models can effectively predict PTMs and related tasks like signal peptide prediction. ProteinBERT [42], with only ~16M parameters, is not large enough but performs well due to its inclusion of Gene Ontology (GO) annotation tasks. By incorporating GO interactions with protein sequences, ProteinBERT achieves strong performance on PTM prediction and other protein property benchmarks, outperforming models with larger parameter sizes. **Evolution and mutation prediction.** Protein evolution and mutation drive functional diversity, aiding adaptation to environmental changes and offering insights into protein function origin, which can inform drug development and disease treatment [107, 108]. UniRep [109], built on the LSTM architecture, was trained on the UniRef50 [95] and excelled in tasks like remote homology detection and mutation effect prediction. ESM-1b [40], a deep transformer model trained on 250 million sequences, with 33 layers and 650 million parameters, captures essential protein sequence patterns through self-supervised learning. ESM-1b is also integral to frameworks like PLMSearch [110] and DHR [111], which enable fast, sensitive homology searches. PLMSearch uses supervised training, while DHR relies on unsupervised contrastive learning and enhances structure prediction models like AlphaFold2 [100]. **Biophysical properties prediction.** Biophysical properties of proteins, such as fluorescence and stability landscapes [112], are crucial for understanding protein folding, stability, and conformational changes, with significant implications for drug design, protein engineering, and enzyme engineering. Deep learning advancements have enabled more accurate prediction of these properties using PPLMs. TAPE benchmark [39] established standardized tasks for evaluating protein, including fluorescence and stability landscape prediction. In 2022, PromptProtein [113], a prompt-based pre-trained model, incorporated multi-task pre-training and a fine-tuning module to improve task-specific performance. It outperformed existing methods in function and biophysical properties prediction, demonstrating substantial gains in predictive accuracy. **Protein-protein interaction and binding affinity prediction.** Protein-protein interactions (PPIs) are crucial for biological functions, and their prediction is also vital for drug discovery and design. PPLMs provide efficient, accurate predictions of PPI types and binding affinities [114, 115]. KeAP model [43], like ProtST, aims to integrate fine-grained information beyond OntoProtein [116]. KeAP uses a triplet format (Protein, Relation, Attribute) as input, processed by encoders and a cascaded decoder based on the Transformer architecture. Using MLM for pre-training, KeAP employs a cross-attention fusion mechanism to capture detailed protein information, achieving superior performance on tasks such as PPI identification and binding affinity estimation. **Antigen-receptor binding and antigen-antibody binding prediction.** Antigen proteins are processed into neoantigen peptides that bind to the Major Histocompatibility Complex (MHC), forming pMHC complexes. These complexes are presented to T-cells, stimulating antibody production by B-cells, which triggers an immune response [117]. Predicting peptide binding to MHC molecules is a key focus of language models in this process [118, 119]. MHCRoBERTa [120] uses a pretrained BERT model to predict pMHC-I binding by learning the biological meaning of amino acid sequences. BERTMHC [121], trained on 2,413 MHC-peptide pairs, focuses on pMHC-II binding prediction, filling a gap in this area. Another goal is predicting the binding specificity of adaptive immune receptors (AIRs), particularly TCRs. TCR-BERT [122] learns TCR CDR3 sequences to predict antigen specificity but lacks the ability to model the interaction between TCR chains. SC-AIR-BERT [123] addresses this by pre-training a model that outperforms others in TCR and BCR binding specificity. Additionally, the Antiformer [124] integrates RNA-seq and BCT-seq data in a graph-based framework to improve antibody development. In antibody modeling, three recent models focus on unique tasks. AbLang [125], built on RoBERTa [126], excels at restoring lost residues during sequencing and outperforms other models in accuracy and efficiency. AntiBERTa [127] understands antibody \"language\" through tasks like predicting immunogenicity and binding sites. EATLM [128], with its unique pre-training tasks (Ancestor Germline Prediction and Mutation Position Prediction), contributes a reliable benchmark for antibody language models. **4.4 Applications of large language models in drug discovery** Drug discovery is an expensive and long-term process that exhibits a low success rate. During the early stages of drug discovery, computer-aided drug discovery, employing empirical or expert knowledge algorithms, machine learning algorithms, and deep learning algorithms, serve to accelerate the generation and screening of drug molecules and their lead compounds [129-131]. It speeds up the entire drug discovery process, especially the development of small molecule drugs. Among commonly used medications, small molecule drugs can account for up to 98% of the total [132]. The structure of small molecule drugs exhibits excellent spatial dispersibility, and their chemical properties determine their good drug-like properties and pharmacokinetic properties [133]. With the development of deep learning and the proposal of large language models, it has become easy to apply these methods to discover hidden patterns of molecules and interactions between molecules for drugs (such as small molecules) and targets (such as proteins and RNA) that can be easily represented as sequence data. The Simplified Molecular-Input Line-Entry System (SMILES) string and chemical fingerprint are commonly used to represent molecules. Additionally, through the pooling process of graph neural networks(GNN), small molecules can be transformed into sequential representations [134]. With the protein sequence, large language models can engage in drug discovery through various inputs. Within this section, key tasks within the early drug discovery process that have effectively leveraged large language models will be introduced (**Table 3, Supplementary Figure 3**). A detailed list of drug discovery language models, their downstream tasks, and the datasets used can be found in **Supplementary Table 3**. **Drug-like molecular properties prediction**. In drug discovery, significant focus is placed on properties like ADMET and PK to develop more effective, accessible, and safe drugs[135, 136]. Large language models (LLMs) are used for molecular property prediction, including these properties. Since molecular SMILES representations are consistent, models can be easily improved and fine-tuned for specific tasks based on researchers' requirements. SMILES-BERT [17] departed from the usage of knowledge-based molecular fingerprints as input. Instead, it adopted a representation method where molecules were encoded as SMILES sequences and employed as input for both pre-training and fine-tuning within a BERT-based model. This novel approach yielded superior outcomes across various downstream molecular property prediction tasks,surpassing the performance of previous models reliant on molecular fingerprints. ChemBERTa [137] is a BERT-based model that focuses on the scalability of large language models, exploring the impact of pre-training dataset size, tokenizer, and string representation. Subsequently, ChemBERTa-2[138] improved upon ChemBERTa by using a larger dataset of 77 million compounds from PubChem, enhancing its ability to learn from diverse chemical structures. It also integrates advanced self-supervised learning techniques and fine-tuning strategies, resulting in better generalization performance across various downstream tasks. K-BERT [15] stands out by using three pre-training tasks: atom feature prediction, molecular feature prediction, and contrastive learning. This approach enables the model to understand the essence of SMILES representations, resulting in exceptional performance across 15 drug datasets, highlighting its effectiveness in drug discovery. Given the importance of graph neural networks in the development of molecular pre-training models, Mole-BERT [139] introduces atom-level Masked Atoms Modeling (MAM) task and graph-level Triplet Masked Contrastive Learning (TMCL) task. These tasks enable the network to acquire a comprehensive understanding of the \"language\" embedded within molecular graphs. By adopting this approach, the network demonstrates exceptional performance across eight downstream tasks, showcasing its adaptability and effectiveness in diverse applications. **Drug-like molecules generation.** It is very difficult to chase the full coverage of the enormous drug-like chemical space (estimated at more than 10\\({}^{63}\\) compounds), and traditional virtual screening libraries usually contain less than 10\\({}^{7}\\) compounds and are sometimes not available. In such circumstances, the utilization of deep learning methods to generate molecules exhibiting drug-like properties emerges as a viable approach [140, 141]. Inspired by the generative pre-training model GPT, MolGPT [16] model was introduced. In addition to performing the next token prediction task, MolGPT incorporates an extra training task for conditional prediction, facilitating the capability of conditional generation. Beyond its capacity to generate innovative and efficacious molecules, the model has demonstrated an enhanced ability to capture the statistical characteristics within the dataset. **Drug-target interaction predictions.** The investigation of Drug-Target Interaction (DTI) holds paramount significance in the realm of drug development and the optimization of drug therapy. Understanding drug-target interactions aids in pharmaceutical design, accelerates drug development, and reduces time and resource costs in lab experimentation and trial-and-errormethods [142, 143]. During the exploration of DTI, diligent focus is placed on the prediction of drug-target binding affinity. DTI-BERT employs a fine-tuned ProtBERT [144] model to process protein sequences and applies discrete wavelet transform to drug molecular fingerprints.. TransDTI [145] is a multi-class classification and regression workflow. This model not only uses fine-tuned SMILES-BERT to extract drug features, but also expands the selection of fine-tuned large protein models. After acquiring potential representations of drug-target pairs, the authors subject the representations to downstream neural networks for the completion of a multi-classification task. Additionally, The Chemical-Chemical Protein-Protein Transferred DTA (C2P2) [146] method uses pre-trained protein and molecular large language models to capture the interaction information within molecules. Given the relatively limited scale of the DTI dataset, C2P2 leverages protein-protein interaction (PPI) and chemical-chemical interaction (CCI) tasks to acquire knowledge of intermolecular interactions and subsequently transfer this knowledge to affinity prediction tasks [147]. It is worth highlighting that in scenarios involving the docking or when emphasizing the spatial structure of a complex, methodologies incorporating 3D convolution networks, point clouds-based networks, and graph networks are often employed [148-151]. In situations where the molecular structure is unknown, but the sequence is available, the prediction of DTI using large-scale models still holds significant promise. **Drug synergistic effects predictions.** Combination therapy is common for complex diseases like cancer, infections, and neurological disorders, often surpassing single-drug treatments. Predicting drug pair synergy, where combining drugs boosts therapeutic effects, is vital in drug development. However, it's challenging due to many drug combinations and complex biology [152, 153]. Various computational methods, including machine learning, help predict drug pair synergy. Carl Edwards et al. introduced SynerGPT [48], which is based on GPT trained to in-context learn drug synergy functions without relying on domain-specific knowledge. Wei Zhang et al. [154] introduced DCE-DForest [154], a model for predicting drug combination synergies. It uses a pretrained drug BERT model to encode the drug SMILES and then predicts synergistic effects based on the embedding vectors of drugs and cell lines using the deep forest method. Mengdie Xua et al. [155] utilized a fine-tuned pre-trained large language model and a dual feature fusion mechanism to predict synergistic drug combinations. Its input includes hashed atom pair molecular fingerprints of drugs, SMILES string encodings, and cell line gene expressions. They conducted ablation analyses on the dual feature fusion network for drug-drug synergy prediction, highlightingthe significant role of fingerprint inputs in ensuring high-quality drug synergy predictions. **4.5 Applications of large language models in single-cell analysis** Large language models have demonstrated significant applications in single-cell analysis, including cell-level tasks such as identifying cell types, determining cell states, and discovering novel cell populations; gene-level tasks like inferring gene regulatory networks; and multi-omics tasks, such as integrating single-cell multi-omics (scMulti-omics) data (**Supplementary Figure 4**). Additionally, this section will explore emerging language models based on spatial transcriptomics (**Table 3**).** A detailed list of single-cell large language models, their downstream tasks, and the datasets used can be found in **Supplementary Table 4. **Cell-level tasks.** Cell-level tasks, such as cell clustering, cell type annotation, novel cell type discovery, batch effect removal and trajectory inference, are critical in single-cell analysis. These tasks often rely on cell representations learned during pretraining, which are subsequently fine-tuned for different tasks. Single-cell language models derive cell representations in two primary ways. The first method utilizes a special class token (\\(<\\)cls\\(>\\)) appended to the input sequence; its embedding is updated through the transformer layers, and the final embedding at the \\(<\\)cls\\(>\\) position serves as the cell representation. The second method generates a cell embedding matrix from the model output, where each row represents a specific cell. Both approaches facilitate downstream tasks, as demonstrated by TOSICA [22], which uses the \\(<\\)cls\\(>\\) token to predict cell type probabilities using the whole conjunction neural network cell type classifier to annotate single cells, and iSEEK [156], which generates cell embedding for cell clustering, cell type annotation, and developmental trajectory exploration. Models like scBERT [20] and UCE [49] leverage multi-head attention mechanisms to extract information from diverse representation subspaces, discerning subtle differences between novel and known cell types. Their large receptive fields capture long-range gene-gene interactions, enabling comprehensive characterization of novel cellular states. Addressing batch effects, which arise from variations in species, tissues, operators, and experimental protocols, remains a significant challenge in single-cell analysis. Large language models, pretrained on extensive datasets, utilize attention mechanisms to incorporate prior biological knowledge, enabling batch-insensitive data annotation. Without relying on explicit batch information, models such as CIForm [152] have demonstrated effectiveness in both intra-dataset and inter-dataset scenarios. They handle annotations across diverse species, organs, tissues, and technologies while also supporting the integration of reference and query data from various sequencing platforms or studies. This capability allows them to address batch effects in single-cell analysis. Drug response or sensitivity prediction is a classification task akin to cell type annotation, where a classifier is appended to the learned cell embeddings to predict whether a cell will respond to or exhibit sensitivity to a specific drug. Models like scFoundation [25] and CellLM [157] effectively utilize this approach, leveraging the robust cell representations learned during pretraining to enhance prediction accuracy. **Gene-level tasks.** Gene-level tasks, such as gene expression prediction, gene regulatory network (GRN) inference, gene perturbation prediction, and drug dose-response prediction, are integral to understanding single-cell transcriptomics. Self-attention mechanisms have transformed deep learning by enabling context-aware models that prioritize relevant elements in large input spaces. These models, particularly transformers, are well-suited for modeling the context-dependent dynamics of gene regulatory networks. By focusing on key interactions, transformers can effectively capture the complexities of regulatory relationships, such as the attention matrix in Geneformer [18] and scGPT [21] reflect which genes that gene pays attention to and which genes pay attention to that gene, aiding to infer gene regulation network. Geneformer is pretrained on a vast repository of single-cell transcriptomes to learn gene relationships for diverse downstream applications, including predicting dosage-sensitive disease genes, identifying downstream targets, forecasting chromatin dynamics, and modeling network dynamics. In addition, after pretraining and fine-tuning, single-cell language models output gene embeddings that can be utilized for functional analysis of scRNA-seq data. For instance, scGPT [21] serves as a generalizable feature extractor leveraging zero-shot learning, enabling applications in gene expression prediction and genetic perturbation prediction. Similarly, in scFoundation [25], zero-expressed genes and masked genes are combined with the output from the transformer-based encoder. This combined information is then input into the decoder and projected to gene expression values through a multilayer perceptron (MLP). The gene context expression is employed to formulate a cell-specific gene graph, facilitating the prediction of perturbations using the GEARS [158] model. It is worth noting that genes have a lot of prior knowledge that can be used to enhance many gene-level tasks. For example, GeneCompass [51] incorporates four types of biological prior knowledge, including GRN, promoter information, gene family annotation and gene-co-expressed relationship, making it capable for various gene tasks. scMulti-omics tasks.** Studying single-cell multi-omics data requires integrating diverse information from genomics, transcriptomics, epigenomics, and proteomics at the single-cell level. The adaptability, generalization capabilities, and feature extraction strengths of large language models make them effective in addressing challenges such as feature variance, data sparsity, and cell heterogeneity inherent in single-cell multi-omics datasets. scMulti-omics integration can be viewed as a specialized form of batch effect removal. For example, scGPT [21] treats each modality as a distinct batch and incorporates a special modality token to represent input features (such as genes, regions, or proteins) associated with each modality. This approach helps the transformer model balance attention across modalities, preventing overemphasis on intra-modality features while integrating inter-modality relationships effectively. Another approach involves processing different modalities through separate transformers before projecting their embeddings into a common latent space. Models like scMVP [159] use mask attention-based encoders for scRNA-seq data and transformer-based multi-head self-attention encoders for scATAC-seq. By aligning variations between different omics in this latent space, scMVP captures joint profiling of scRNA-seq and scATAC-seq, achieving paired integration where gene expression and chromatin accessibility are studied within the same cells. Graphs are increasingly recognized as powerful tools for characterizing feature heterogeneity in scMulti-omics integration. For example, DeepMAPS [160] leverages graph transformers to construct cell and gene graphs, learning both local and global features that build cell-cell and gene-gene relationships for data integration, inference of biological networks from scMulti-omics data and cell-cell communication. Recent advances in sequencing technologies that capture multiple modalities within the same cell have enabled the development of computational tools for cross-modality prediction. One approach involves training large language models on paired datasets to predict one modality from another. For instance, scTranslator [161], pre-trained on paired bulk and single-cell data, fine-tunes to infer protein abundance from scRNA-seq data by minimizing the mean squared error (MSE) between predicted and actual protein levels. Another strategy leverages graph learning with prior knowledge to model feature relationships. For example, scMoFormer [162] can not only translate gene expression to protein abundance, but is also applicable to multi-omics predictions, including protein abundance to gene expression, chromatin accessibility to gene expression, gene expression to chromatin accessibility using graph transformers. Taking protein prediction task as an example, scMoFormer constructs cell-gene graph, gene-gene graph, protein-protein graph, and gene-proteingraph based on gene expression profiles and prior knowledge from STRING database [163]. Each modality has a separate transformer to learn the global information that may not be included in prior knowledge. Message-passing graph neural networks (GNNs) link nodes across various graphs, while transformers are employed to precisely map gene expression to protein abundance. **Spatial transcriptomics tasks.** The rapid development of single-cell and spatial transcriptomics has advanced our understanding of cellular heterogeneity and tissue architecture. Spatial transcriptomics retains cells' native spatial context, enabling insights into cellular interactions. Large language models address the challenge of high-dimensional spatial data analysis by integrating spatial and molecular information, enhancing tissue-specific pattern interpretation. For example, Nicheformer [53] is the latest large language model in spatial transcriptomics. It integrates extensive spatial transcriptomics and single-cell transcriptomics data, leveraging metadata across multiple modalities, species, and sequencing technologies. By doing so, Nicheformer is capable of learning joint information from single-cell and spatial transcriptomics, enabling the resolution of various spatial prediction tasks even with limited data. Spaformer [164] is another transformer-based model designed for spatial transcriptomics data. Spaformer is designed to address two key challenges: how to encode spatial information of cells into a transformer model and how to train a transformer to overcome the sparsity of spatial transcriptomics data, enabling data imputation. Spatial transcriptomics, as one of the most popular technologies in recent years, focuses on integrating single-cell resolution gene expression data with tissue spatial information to reveal spatial relationships and functional characteristics among cells. However, large language models (LLMs) specifically designed for spatial transcriptomics are still in their early stages of development. The creation of these models faces unique challenges, such as effectively integrating high-dimensional gene expression data with complex spatial information and addressing the sparsity and irregularity of the data. In addition to the single-cell large language models discussed above, another category of single-cell prediction models leverages natural language, utilizing textual data such as human-readable descriptions of gene functions and biological features to support various single-cell analyses. For example, GPT-4 [165] leverages its strong contextual understanding for interpreting high-dimensional single-cell analysis for accurate cell type annotation. GenePT [166] utilizes OpenAI's ChatGPT text embedding to classify gene properties and cell types effectively. More and more models demonstrate that natural language pretraining can significantly boost performance on single-cell downstream tasks, including cell generation [167], cell identity (e.g., cell type, pathway, and disease information) [167-171], and gene enrichment analysis [169]. These models demonstrate significant potential in advancing single-cell analysis by integrating natural language processing techniques. However, the reliance on textual data may constrain performance in less-annotated or novel datasets. **5. Conclusion and Suggestions on large language models in bioinformatics** **5.1 Summary of large language models in bioinformatics** Large language models (LLMs) have catalyzed transformative progress across biological disciplines, including genomics, transcriptomics, proteomics, drug discovery, and single-cell analysis. These models, trained on vast datasets, address challenges like the sparsity, high dimensionality, and heterogeneity of biological data while capturing the complexity of sequence relationships. Tokenization methods are pivotal, converting sequences into manageable formats, such as, for genomics and transcriptomics, k-mer encoding is prevalent, segmenting DNA/RNA sequences into overlapping units. In proteomics, amino acid residue-based tokenization captures protein structure and function. These preprocessing strategies enable LLMs to interpret biological language effectively. Representation learning allows LLMs to uncover contextual and hierarchical relationships within biological data, forming the basis for various downstream applications. These tasks can be grouped into four primary categories: 1) Classification/Prediction Tasks: Examples include identifying functional genomic elements (e.g., promoters, enhancers), predicting protein structures and interactions, and cell type annotation in single-cell data. 2) Generation Tasks: LLMs can create biologically relevant sequences, such as gene expression imputation and synthetic DNA, RNA, or protein sequences, aiding in vaccine development or enzyme engineering. 3) Interaction Tasks: These involve modeling interactions like drug-target binding, cell-cell interaction, protein-protein interactions, or cross-omics relationships (e.g., gene expression to protein abundance). 4) Transfer Learning Tasks: Pretrained LLMs, such as DNABERT and scGPT, are fine-tuned for specific applications, including single-cell data annotation or predicting RNA modifications like N6-methyladenosine sites. Despite their capabilities, challenges persist. Biological data often exhibit sparsity, as seen in single-cell and spatial transcriptomics, and irregularity due to sequencing errors or noise. To address this, LLMs must effectively integrate multi-modal data, balance computational efficiency, and ensure interpretability of their outputs. As foundational modelsevolve, their ability to unify diverse biological datasets into a single framework for prediction, generation, interaction, and transfer learning tasks will continue to reshape our understanding and applications of biological systems. **5.2 Guidance on how to use and develop LLMs in practice** Large Language Models offer immense potential in bioinformatics and other fields, but their effective utilization and development require distinct approaches for end-users and developers (**Figure 5**). For LLM users, the process begins by clearly defining the research domain and task, specifying the relevant omics level (e.g., genomics, transcriptomics, proteomics) and identifying whether the objective involves classification or prediction, generation, interaction, or transfer learning. A well-defined objective streamlines the selection of appropriate models and workflows. Next, users should choose models pretrained on data relevant to their domain, as detailed in **Table 2**, which includes information on foundation models, their training data types, and availability. For instance, DNABERT is ideal for genomics tasks, while scGPT is tailored for single-cell analysis. Additionally, users must assess computational requirements and ensure compatibility with their dataset size and complexity. Proper data preparation is critical, including aligning data with model requirements, addressing missing values, and incorporating metadata like cell types or genomic regions. **Table 1** provides common tokenization methods for reference. To leverage transfer learning, users can fine-tune foundation models listed in **Table 2** for their specific dataset, optimizing performance through hyperparameter tuning, early stopping, and cross-validation. Alternatively, users can utilize predeveloped models listed in **Supplementary Tables 1-4** for similar tasks to obtain results directly. Finally, rigorous evaluation using metrics like accuracy, precision, and recall is essential, complemented by interpretation tools such as attention maps or feature embeddings to extract meaningful biological insights (**Figure 5**). For LLM developers, it is essential to **first** understand domain-specific challenges to address issues like sparsity, heterogeneity, and high dimensionality. For example, single-cell and spatial transcriptomics datasets often suffer from sparsity and noise, necessitating innovative solutions in model architecture. **Second,** developers should choose or develop tokenization strategies tailored to biological data. For instance, k-mer encoding works well for DNA/RNA sequences, while gene ranking-based tokenization is effective for scRNA-seq data. Exploring hybrid tokenization can enhance cross-modal understanding. **Third,** in model development, developers should employ or design novel transformer structures. For example, scBERT utilizes Performer to improve scalability. Incorporating knowledge-based information into model training can further enhance performance. For instance, GeneCompass integrates four types of biological prior knowledge including GRNs, promoter information, gene family annotation, and gene co-expression relationships, making it versatile for various gene-related tasks. Similarly, basic protein language models, which are often limited to MSA and protein sequences, can be improved by incorporating additional modalities like 3D structural data. This can be achieved by converting such modalities into sequence formats or integrating large models to collectively capture multi-modal information using fusion techniques. Moreover, combining Graph Neural Networks (GNNs) with transformers has led to significant advancements. For example, scMoFormer constructs cell-gene, gene-gene, protein-protein, and gene-protein graphs for multi-omics predictions, while DeepMAPS uses cell-gene graphs to estimate gene importance. GNNs excel in capturing local interactions, while transformers effectively model long-range dependencies, enabling comprehensive representations of intricate relationships in single-cell data. **Fourth,** novel tasks that can be explored in developing LLMs for bioinformatics include causal inference in multi-omics, such as determining how DNA variations influence mRNA abundance or protein expression. Spatial transcriptomics interpretation can model cell spatial organization within tissues. Epigenetic modulation prediction focuses on regulatory roles of histone modifications, DNA methylation, or chromatin accessibility. Synthetic biology applications can involve generating optimized gene or protein sequences, while cross-species genomics identifies conserved functional genomic elements. These tasks exemplify how LLMs can tackle emerging challenges in biological research. **Fifth,** developers should expand LLMs to accommodate emerging data types, such as CODEX imaging data and long-read sequencing data, which bring unique challenges in terms of data structure, preprocessing, and representation. **Lastly,** validation, application, and interpretability should be prioritized. Developers should not only evaluate models on specific tasks but also ensure that foundational challenges, such as the impact of sparsity in scRNA-seq data on cell type annotation performance, are fully addressed to enhance the robustness and utility of the models (**Figure 5**)."
    },
    {
      "title": "Acknowledgements",
      "text": "We would like to express our gratitude to our colleagues and friends who provided invaluable advice and support throughout the duration of this study."
    },
    {
      "title": "Funding",
      "text": "This work was partially supported by the National Institutes of Health [R01LM014156, R01GM153822, R01CA241930 to X.Z] and the National Science Foundation [2217515, 2326879 to X.Z]. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript. Funding for open access charge: Dr & Mrs Carl V. Vartian Chair Professorship Funds to Dr. Zhou from the University of Texas Health Science Center at Houston. _Conflict of interest statement._ None declared."
    },
    {
      "title": "References",
      "text": "* [1] Radford, A., et al., _Improving language understanding by generative pre-training._ 2018. * [2] Devlin, J., et al., _Bert: Pre-training of deep bidirectional transformers for language understanding._ arXiv preprint arXiv:1810.04805, 2018. * [3] Vaswani, A., et al., _Attention is all you need._ Advances in neural information processing systems, 2017. **30**. * [4] Chen, J., et al., _Interpretable RNA foundation model from unannotated data for highly accurate RNA structure and function predictions._ bioRxiv, 2022: p. 2022.08. 06.503062. * [5] Zhang, Y., et al., _Multiple sequence alignment-based RNA language model and its application to structural inference._ Nucleic Acids Research, 2024. **52**(1): p. e3-e3. * [6] Ji, Y., et al., _DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome._ Bioinformatics, 2021. **37**(15): p. 2112-2120. * [7] Zhang, D., et al., _DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks._ bioRxiv, 2023: p. 2023.07. 11.548628. * [8] Akiyama, M. and Y. Sakakibara, _Informative RNA base embedding for RNA structural alignment and clustering by deep representation learning._ NAR genomics and bioinformatics, 2022. **4**(1): p. lqac012. * [9] Wang, N., et al., _Multi-purpose RNA language modelling with motif-aware pretraining and type-guided fine-tuning._ Nature Machine Intelligence, 2024: p. 1-10. * [10] Rives, A., et al., _Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. bioRxiv._ 2019. * [11] Elnaggar, A., et al., _ProtTrans: Towards Cracking the Language of Lifes Code Through Self-Supervised Deep Learning and High Performance Computing._ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021: p. 1-1. * [12] Madani, A., et al., _Progen: Language modeling for protein generation._ arXiv preprint arXiv:2004.03497, 2020. * [13] Xu, M., et al., _Protist: Multi-modality learning of protein sequences and biomedical texts._ arXiv preprint arXiv:2301.12040, 2023. * [14] Outeiral, C. and C.M. Deane, _Codon language embeddings provide strong signals for use in protein engineering._ Nature Machine Intelligence, 2024. **6**(2): p. 170-179. * [15] Wu, Z., et al., _Knowledge-based BERT: a method to extract molecular features like computational chemists._ Briefings in Bioinformatics, 2022. **23**(3): p. bbac131. * [16] Bagal, V., et al., _MolGPT: molecular generation using a transformer-decoder model._ Journal of Chemical Information and Modeling, 2021. **62**(9): p. 2064-2076. * [17] Wang, S., et al. _Smiles-bert: large scale unsupervised pre-training for molecular property prediction_. in _Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics._ 2019. * [18] Theodoris, C.V., et al., _Transfer learning enables predictions in network biology._ Nature, 2023. **618**(7965): p. 616-624. * [19] Shen, H., et al., _Generative pretraining from large-scale transcriptomes for single-cell deciphering._ iScience, 2023. **26**(5): p. 106536. * [20] Yang, F., et al., _scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data._ Nature Machine Intelligence, 2022. **4**(10): p. 852-866. * [21] Cui, H., et al., _scGPT: toward building a foundation model for single-cell multi-omics using generative AI._ Nat Methods, 2024. **21**(8): p. 1470-1480. * [22] Chen, J., et al., _Transformer for one stop interpretable cell type annotation._ Nat Commun, 2023. **14**(1): p. 223. * [23] Jiao, L., et al., _scTransSort: Transformers for Intelligent Annotation of Cell Types by Gene Embeddings._ Biomolecules, 2023. **13**(4). * [24] Xiong, L., T. Chen, and M. Kellis. _scCLIP: Multi-modal Single-cell Contrastive Learning Integration Pre-training._ in _NeurIPS 2023 AI for Science Workshop._ * [25] Hao, M., et al., _Large-scale foundation model on single-cell transcriptomics._ Nat Methods, 2024. **21**(8): p. 1481-1491. * [26] Bian, H., et al. _scMulan: a multitask generative pre-trained language model for single-cell analysis._ in _International Conference on Research in Computational Molecular Biology_. 2024. Springer. * [27] Wen, H., et al., _CellPLM: pre-training of cell language model beyond single cells._ bioRxiv, 2023: p. 2023.10. 03.560734. * [28] Mao, Y., et al., _Phenotype prediction from single-cell RNA-seq data using attention-based neural networks._ Bioinformatics, 2024. **40**(2). * [29] Querfurth, B.v., et al., _mcBERT: Patient-Level Single-cell Transcriptomics Data Representation._ bioRxiv, 2024: p. 2024.11. 04.621897. * [30] Sarkar, S., _Decoding\" coding\": Information and DNA._ BioScience, 1996. **46**(11): p. 857-864. * [31] Dalla-Torre, H., et al., _The nucleotide transformer: Building and evaluating robust foundation models for human genomics._ bioRxiv, 2023: p. 2023.01. 11.523679. * [32] Benegas, G., S.S. Batra, and Y.S. Song, _DNA language models are powerful predictors of genome-wide variant effects._ Proceedings of the National Academy of Sciences, 2023. **120**(44): p. e2311219120. * [33] Zhou, Z., et al., _Dnabert-2: Efficient foundation model and benchmark for multi-species genome._ arXiv preprint arXiv:2306.15006, 2023. * [34] Sanabria, M., et al., _DNA language model GROVER learns sequence context in the human genome._ Nature Machine Intelligence, 2024. **6**(8): p. 911-923. * [35] Wang, X., et al., _UNI-RNA: universal pre-trained models revolutionize RNA research._ bioRxiv, 2023: p. 2023.07. 11.548588. * [36] Chen, K., et al., _Self-supervised learning on millions of primary RNA sequences from 72 vertebrates improves sequence-based RNA splicing prediction._ Briefings in Bioinformatics, 2024. **25**(3): p. bbae163. * [37] Yang, Y., et al., _Deciphering 3'UTR Mediated Gene Regulation Using Interpretable Deep Representation Learning._ Advanced Science, 2024. **11**(39): p. 2407013. * [38] Chu, Y., et al., _A 5'UTR language model for decoding untranslated regions of mRNA and function predictions._ Nature Machine Intelligence, 2024. **6**(4): p. 449-460. * [39] Rao, R., et al., _Evaluating protein transfer learning with TAPE_. Advances in neural information processing systems, 2019. **32**. * [40] Rives, A., et al., _Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences_. Proceedings of the National Academy of Sciences, 2021. **118**(15): p. e2016239118. * [41] Ferruz, N., S. Schmidt, and B. Hucker, _ProtGPT2 is a deep unsupervised language model for protein design_. Nature communications, 2022. **13**(1): p. 4348. * [42] Brandes, N., et al., _ProteinBERT: a universal deep-learning model of protein sequence and function_. Bioinformatics, 2022. **38**(8): p. 2102-2110. * [43] Zhou, H.-Y., et al., _Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling_. bioRxiv, 2023: p. 2023-01. * [44] Polishchuk, P.G., T.I. Madzhidov, and A.J.J.o.c.-a.m.d. Varnek, _Estimation of the size of drug-like chemical space based on GDB-17 data_. 2013. **27**: p. 675-679. * [45]_MOLE-BERT: RETHINKING PRE-TRAINING GRAPH NEURAL NETWORKS FOR MOLECULES_. * [46] Wang, S., et al., _Smiles-Bert_, in _Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics_. 2019. p. 429-436. * [47] Bagal, V., et al., _MolGPT: Molecular Generation Using a Transformer-Decoder Model_. J Chem Inf Model, 2022. **62**(9): p. 2064-2076. * [48]_SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design_. * [49] Rosen, Y., et al., _Universal cell embeddings: A foundation model for cell biology_. bioRxiv, 2023: p. 2023.11. 28.568918. * [50] Theus, A., et al., _CancerFoundation: A single-cell RNA sequencing foundation model to decipher drug resistance in cancer_. bioRxiv, 2024: p. 2024.11. 01.621087. * [51] Yang, X., et al., _GeneCompass: deciphering universal gene regulatory mechanisms with a knowledge-informed cross-species foundation model_. Cell Res, 2024. **34**(12): p. 830-845. * [52] Kalfon, J., et al., _scPRINT: pre-training on 50 million cells allows robust gene network predictions_. bioRxiv, 2024: p. 2024.07. 29.605556. * [53] Schaar, A., et al., _Nicheformer: a foundation model for single -cell and spatial omics. 2024_. Preprint at bioRxiv, 2024. **4**: p. 589472. * [54] Sinden, R.R. and R.D. Wells, _DNA structure, mutations, and human genetic disease_. Current opinion in biotechnology, 1992. **3**(6): p. 612-622. * [55] Wittkopp, P.J. and G. Kalay, _Cis-regulatory elements: molecular mechanisms and evolutionary processes underlying divergence_. Nature Reviews Genetics, 2012. **13**(1): p. 59-69. * [56] Yella, V.R., A. Kumar, and M. Bansal, _Identification of putative promoters in 48 eukaryotic genomes on the basis of DNA free energy_. Scientific reports, 2018. **8**(1): p. 4520. * [57] Le, N.Q.K., et al., _BERT-Promoter: An improved sequence-based predictor of DNA promoter using BERT pre-trained model and SHAP feature selection_. Computational Biology and Chemistry, 2022. **99**: p. 107732. * [58] Claringbould, A. and J.B. Zaugg, _Enhancers in disease: molecular basis and emerging treatment strategies_. Trends in Molecular Medicine, 2021. **27**(11): p. 1060-1073. * [59] Nasser, J., et al., _Genome-wide enhancer maps link risk variants to disease genes_. Nature, 2021. **593**(7858): p. 238-243. * [60] Luo, H., et al. _iEnhancer-BERT: A novel transfer learning architecture based on DNA-Language model for identifying enhancers and their strength_. in _International Conference on Intelligent Computing_. 2022. Springer. * [61] Ferraz, R.A.C., et al., _DNA-protein interaction studies: a historical and comparative analysis_. Plant Methods, 2021. **17**(1): p. 1-21. * [62] Luo, H., et al., _Improving language model of human genome for DNA-protein binding prediction based on task-specific pre-training._ Interdisciplinary Sciences: Computational Life Sciences, 2023. **15**(1): p. 32-43. * [63] An, W., et al. _MoDNA: motif-oriented pre-training for DNA language model._ in _Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics._ 2022. * [64] Moore, L.D., T. Le, and G. Fan, _DNA methylation and its basic function._ Neuropsychopharmacology, 2013. **38**(1): p. 23-38. * [65] Zhang, L., et al., _Comprehensive analysis of DNA 5-methylcytosine and N6-adenine methylation by nanopore sequencing in hepatocellular carcinoma._ Frontiers in cell and developmental biology, 2022. **10**: p. 827 391. * [66] Tsukiyama, S., et al., _BERT6mA: prediction of DNA N6-methyladenine site using deep learning-based approaches._ Briefings in Bioinformatics, 2022. **23**(2): p. bbac053. * [67] Yu, Y., et al., _iDNA-ABT: advanced deep learning model for detecting DNA methylation with adaptive features and transductive information maximization._ Bioinformatics, 2021. **37**(24): p. 4603-4610. * [68] Jin, J., et al., _iDNA-ABF: multi-scale deep biological language learning model for the interpretable prediction of DNA methylations._ Genome biology, 2022. **23**(1): p. 1-23. * [69] Zeng, W., A. Gautam, and D.H. Huson, _MuLan-Methyl-Multiple Transformer-based Language Models for Accurate DNA Methylation Prediction._ bioRxiv, 2023: p. 2023.01. 04.522704. * [70] Sanh, V., et al., _DistilBERT, a distilled version of BERT: smaller, foster, cheaper and lighter._ arXiv preprint arXiv:1910.01108, 2019. * [71] Lan, Z., et al., _Albert: A lite bert for self-supervised learning of language representations._ arXiv preprint arXiv:1909.11942, 2019. * [72] Yang, Z., et al., _Xlnet: Generalized autoregressive pretraining for language understanding._ Advances in neural information processing systems, 2019. **32**. * [73] Clark, K., et al., _Electra: Pre-training text encoders as discriminators rather than generators._ arXiv preprint arXiv:2003.10555, 2020. * [74] Wilkinson, M.E., C. Charenton, and K. Nagai, _RNA splicing by the spliceosome._ Annual review of biochemistry, 2020. **89**: p. 359-388. * [75] Zhang, J., et al., _Advances and opportunities in RNA structure experimental determination and computational modeling._ Nature Methods, 2022. **19**(10): p. 1193-1207. * [76] Malbec, L., et al., _Dynamic methylome of internal mRNA N 7-methylguanosine and its regulatory role in translation._ Cell research, 2019. **29**(11): p. 927-941. * [77] Feng, H., et al., _IncCat: An ORF attention model to identify lncRNA based on ensemble learning strategy and fused sequence information._ Computational and Structural Biotechnology Journal, 2023. **21**: p. 1433-1447. * [78] Xia, S., et al. _A multi-granularity information-enhanced pre-training method for predicting the coding potential of sORFs in plant lncRNAs._ in _2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)._ 2023. IEEE. * [79] Yamada, K. and M. Hamada, _Prediction of RNA-protein interactions using a nucleotide language model._ Bioinformatics Advances, 2022. **2**(1): p. vbaac023. * [80] Fang, Y., X. Pan, and H.-B. Shen, _Recent deep learning methodology development for RNA-RNA interaction prediction._ Symmetry, 2022. **14**(7): p. 1302. * [81] Gibb, E.A., C.J. Brown, and W.L. Lam, _The functional role of long non-coding RNA in human carcinomas._ Molecular cancer, 2011. **10**(1): p. 1-17. * [82] Zhang, L., et al., _BERT-m7G: a transformer architecture based on BERT and stacking ensemble to identify RNA N7-Methylguanosine sites from sequence information._ Computational and Mathematical Methods in Medicine, 2021. **2021**. * [83] Soylu, N.N. and E. Sefer, _BERT2OME: Prediction of 2'-O-methylation Modifications from RNA Sequence by Transformer Architecture Based on BERT._ IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2023. * [84] Pardi, N., et al., _mRNA vaccines--a new era in vaccinology._ Nature reviews Drug discovery, 2018. **17**(4): p. 261-279. * [85] Babjac, A.N., Z. Lu, and S.J. Emrich. _CodonBERT: Using BERT for Sentiment Analysis to Better Predict Genes with Low Expression._ in _Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics._ 2023. * [86] Gong, H., et al., _Integrated mRNA sequence optimization using deep learning._ Brief Bioinform, 2023. **24**(1). * [87] Ding, W., K. Nakai, and H. Gong, _Protein design via deep learning._ Briefings in bioinformatics, 2022. **23**(3): p. bbac102. * [88] Qiu, Y. and G.-W. Wei, _Artificial intelligence-aided protein engineering: from topological data analysis to deep protein language models._ arXiv preprint arXiv:2307.14587, 2023. * [89] Kovaltsuk, A., et al., _Observed antibody space: a resource for data mining next-generation sequencing of antibody repertoires._ The Journal of Immunology, 2018. **201**(8): p. 2502-2509. * [90] Schauperl, M. and R.A. Denny, _AI-based protein structure prediction in drug discovery: impacts and challenges._ Journal of Chemical Information and Modeling, 2022. **62**(13): p. 3142-3156. * [91] David, A., et al., _The AlphaFold database of protein structures: a biologist's guide._ Journal of molecular biology, 2022. **434**(2): p. 167336. * [92] Rao, R.M., et al. _MSA transformer._ in _International Conference on Machine Learning_. 2021. * [93] Dai, Z., et al., _Transformer-xl: Attentive language models beyond a fixed-length context._ arXiv preprint arXiv:1901.02860, 2019. * [94] Raffel, C., et al., _Exploring the limits of transfer learning with a unified text-to-text transformer._ The Journal of Machine Learning Research, 2020. **21**(1): p. 5485-5551. * [95]_UniProt: the universal protein knowledgebase in 2021._ Nucleic acids research, 2021. **49**(D1): p. D480-D489. * [96] Steinegger, M. and J. Sding, _Clustering huge protein sequence sets in linear time._ Nature communications, 2018. **9**(1): p. 2542. * [97] Strokach, A. and P.M. Kim, _Deep generative modeling for protein design._ Current opinion in structural biology, 2022. **72**: p. 226-236. * [98] Ferruz, N. and B. Hucker, _Controllable protein design with language models._ Nature Machine Intelligence, 2022. **4**(6): p. 521-532. * [99] Mirdita, M., et al., _ColabFold: making protein folding accessible to all._ Nature methods, 2022. **19**(6): p. 679-682. * [100] Jumper, J., et al., _Highly accurate protein structure prediction with AlphaFold._ Nature, 2021. **596**(7873): p. 583-589. * [101] Zhou, X., et al., _I-TASSER-MTD: a deep-learning-based platform for multi-domain protein structure and function prediction._ Nature Protocols, 2022. **17**(10): p. 2326-2353. * [102] Ferruz, N., et al., _From sequence to function through structure: Deep learning for protein design._ Computational and Structural Biotechnology Journal, 2023. **21**: p. 238-250. * [103] Xu, M., et al. _Prost: Multi-modality learning of protein sequences and biomedical texts._ in _International Conference on Machine Learning._ 2023. PMLR. * [104] Rosenberg, A.A., A. Marx, and A.M. Bronstein, _Codon-specific Ramachandran plots show amino acid backbone conformation depends on identity of the translated codon._ Nature communications, 2022. **13**(1): p. 2815. * [105] Wang, H., et al., _Protein post-translational modifications in the regulation of cancer hallmarks._ Cancer Gene Therapy, 2023. **30**(4): p. 529-547. * [106] de Brevern, A.G. and J. Rebehmed, _Current status of PTMs structural databases: applications, limitations and prospects._ Amino Acids, 2022. **54**(4): p. 575-590. * [107] Savino, S., T. Desmet, and J. Francesus, _Insertions and deletions in protein evolution and engineering._ Biotechnology Advances, 2022. **60**: p. 108010. * [108] Horne, J. and D. Shukla, _Recent advances in machine learning variant effect prediction tools for protein engineering._ Industrial \\(\\backslash\\)& engineering chemistry research, 2022. **61**(19): p. 6235-6245. * [109] Alley, E.C., et al., _Unified rational protein engineering with sequence-based deep representation learning._ Nature methods, 2019. **16**(12): p. 1315-1322. * [110] Liu, W., et al., _PLMSearch: Protein language model powers accurate and fast sequence search for remote homology._ Nature communications, 2024. **15**(1): p. 2775. * [111] Hong, L., et al., _Fast, sensitive detection of protein homologs using deep dense retrieval._ Nature Biotechnology, 2024: p. 1-13. * [112] Pucci, F., M. Schwersensky, and M. Rooman, _Artificial intelligence challenges for predicting the impact of mutations on protein stability._ Current opinion in structural biology, 2022. **72**: p. 161-168. * [113] Wang, Z., et al. _Multi-level Protein Structure Pre-training via Prompt Learning._ in _The Eleventh International Conference on Learning Representations._ 2022. * [114] Tang, T., et al., _Machine learning on protein-protein interaction prediction: models, challenges and trends._ Briefings in Bioinformatics, 2023. **24**(2): p. bbad076. * [115] Durham, J., et al., _Recent advances in predicting and modeling protein-protein interactions._ Trends in biochemical sciences, 2023. * [116] Zhang, N., et al., _Ontoprotein: Protein pretraining with gene ontology embedding._ arXiv preprint arXiv:2201.11147, 2022. * [117] Janeway, C., et al., _Immunobiology: the immune system in health and disease._ Vol. 2. 2001: Garland Pub. New York. * [118] Peters, B., M. Nielsen, and A.J.A.R.o.l. Sette, _T cell epitope predictions._ 2020. **38**: p. 123-145. * [119] O'Donnell, T.J., A. Rubinsteyn, and U.J.C.s. Laserson, _MHC/flurry 2.0: improved pan-allele prediction of MHC class I-presented peptides by incorporating antigen processing._ 2020. **11**(1): p. 42-48. e7. * [120] Wang, F., et al., _MHCRoBERTa: pan-specific peptide-MHC class I binding prediction through transfer learning with label-agnostic protein sequences._ Brief Bioinform, 2022. **23**(3). * [121] Cheng, J., et al., _BERTMHC: improved MHC-peptide class II interaction prediction with transformer and multiple instance learning._ Bioinformatics, 2021. **37**(22): p. 4172-4179. * [122] Wu, K., et al., _TCR-BERT: learning the grammar of T-cell receptors for flexible antigenbinding analyses._ 2021. * [123] Zhao, Y., et al., _SC-AIR-BERT: a pre-trained single-cell model for predicting the antigen-binding specificity of the adaptive immune receptor._ Brief Bioinform, 2023. **24**(4). * [124] Wang, Q., et al., _AntiFormer: graph enhanced large language model for binding affinity prediction._ Briefings in Bioinformatics, 2024. **25**(5). * [125] Olsen, T.H., I.H. Moal, and C.M. Deane, _AbLang: an antibody language model for completing antibody sequences._ Bioinformatics Advances, 2022. **2**(1): p. vbac046. * [126] Liu, Y., et al., _Roberta: A robustly optimized bert pretraining approach._ arXiv preprint arXiv:1907.11692, 2019. * [127] Leem, J., et al., _Deciphering the language of antibodies using self-supervised learning_. Patterns, 2022. **3**(7). * [128] Wang, D., F. Ye, and H. Zhou, _On pre-trained language models for antibody_. bioRxiv, 2023: p. 2023-01. * [129] Askr, H., et al., _Deep learning in drug discovery: an integrative review and future challenges_. Artificial Intelligence Review, 2023. **56**(7): p. 5975-6037. * [130] Xiaobo, Z. and S.T.C. Wong, _High content cellular imaging for drug development_. IEEE Signal Processing Magazine, 2006. **23**(2): p. 170-174. * [131] Sun, X., et al., _Multi-scale agent-based brain cancer modeling and prediction of TKI treatment response: incorporating EGFR signaling pathway and angiogenesis_. BMC Bioinformatics, 2012. **13**: p. 218. * [132] Vargason, A.M., A.C. Anselmo, and S.J.N.b.e. Mitragotri, _The evolution of commercial drug delivery technologies_. 2021. **5**(9): p. 951-967. * [133] Leeson, P.D. and B.J.N.r.D.d. Springthorpe, _The influence of drug-like concepts on decision-making in medicinal chemistry_. 2007. **6**(11): p. 881-890. * [134] Ozcelik, R., et al., _Structure-Based Drug Discovery with Deep Learning_. Chembiochem, 2023. **24**(13): p. e202200776. * [135] Li, Z., et al., _Deep learning methods for molecular representation and property prediction_. Drug Discovery Today, 2022: p. 103373. * [136] Chen, W., et al., _Artificial intelligence for drug discovery: Resources, methods, and applications_. Molecular Therapy-Nucleic Acids, 2023. * [137] Chithrananda, S., G. Grand, and B. Ramsundar, _ChemBERTa: large-scale self-supervised pretraining for molecular property prediction_. arXiv preprint arXiv:2010.09885, 2020. * [138]_ChemBERTa-2: Towards Chemical Foundation Models_. * [139] Xia, J., et al. _Mole-bert: Rethinking pre-training graph neural networks for molecules_. in _The Eleventh International Conference on Learning Representations_. 2022. * [140] Bilodeau, C., et al., _Generative models for molecular discovery: Recent advances and challenges_. Wiley Interdisciplinary Reviews: Computational Molecular Science, 2022. **12**(5): p. e1608. * [141] Meyers, J., B. Fabian, and N. Brown, _De novo molecular design and generative models_. Drug Discovery Today, 2021. **26**(11): p. 2707-2715. * [142] Abbasi, K., et al., _Deep learning in drug target interaction prediction: current and future perspectives_. Current Medicinal Chemistry, 2021. **28**(11): p. 2100-2113. * [143] Zhang, Z., et al., _Graph neural network approaches for drug-target interactions_. Current Opinion in Structural Biology, 2022. **73**: p. 102327. * [144] Zheng, J., X. Xiao, and W.-R. Qiu, _DTI-BERT: identifying drug-target interactions in cellular networking based on BERT and deep learning method_. Frontiers in Genetics, 2022. **13**: p. 859188. * [145] Kalakoti, Y., S. Yadav, and D. Sundar, _TransDTI: transformer-based language models for estimating DTIs and building a drug recommendation workflow_. ACS omega, 2022. **7**(3): p. 2706-2717. * [146] Kang, H., et al., _Fine-tuning of bert model to accurately predict drug-target interactions_. Pharmaceutics, 2022. **14**(8): p. 1710. * [147] Nguyen, T.M., T. Nguyen, and T. Tran, _Mitigating cold-start problems in drug-target affinity prediction with interaction knowledge transferring_. Briefings in Bioinformatics, 2022. **23**(4): p. bbac269. * [148] Ragoza, M., et al., _Protein-ligand scoring with convolutional neural networks_. Journal of chemical information and modeling, 2017. **57**(4): p. 942-957. * [149] Li, S., et al. _Structure-aware interactive graph neural networks for the prediction of protein-ligand binding affinity_. in _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_. 2021. * [150] Jiang, D., et al., _InteractionGraphNet: a novel and efficient deep graph representation learning framework for accurate protein--ligand interaction predictions._ Journal of medicinal chemistry, 2021. **64**(24): p. 18209-18232. * [151] Wang, Y., et al., _A point cloud-based deep learning strategy for protein--ligand binding affinity prediction._ Briefings in Bioinformatics, 2022. **23**(1): p. bbab474. * [152] Hecht, J.R., et al., _A randomized phase IIIB trial of chemotherapy, bevacizumab, and panitumamb compared with chemotherapy and bevacizumab alone for metastatic colorectal cancer._ 2009. **27**(5): p. 672-680. * [153] Tol, J., et al., _Chemotherapy, bevacizumab, and cetuximab in metastatic colorectal cancer._ 2009. **360**(6): p. 563-572. * [154] Zhang, W., et al., _DCE-DForest: a deep forest model for the prediction of anticancer drug combination effects._ Computational and Mathematical Methods in Medicine, 2022. **2022**. * [155] Xu, M., et al., _DFFNDDS: prediction of synergistic drug combinations with dual feature fusion networks._ Journal of Cheminformatics, 2023. **15**(1): p. 1-12. * [156] Shen, H., et al., _A universal approach for integrating super large-scale single-cell transcriptomes by exploring gene rankings._ Brief Bioinform, 2022. **23**(2). * [157] Zhao, S., J. Zhang, and Z. Nie, _Large-scale cell representation learning via divide-and-conquer contrastive learning._ arXiv preprint arXiv:2306.04371, 2023. * [158] Roohani, Y., K. Huang, and J. Leskovec, _GEARS: Predicting transcriptional outcomes of novel multi-gene perturbations._ BioRxiv, 2022: p. 2022.07. 12.499735. * [159] Li, G., et al., _A deep generative model for multi-view profiling of single-cell RNA-seq and ATAC-seq data._ Genome Biol, 2022. **23**(1): p. 20. * [160] Ma, A., et al., _Single-cell biological network inference using a heterogeneous graph transformer._ Nat Commun, 2023. **14**(1): p. 964. * [161] Linjing, L., et al., _A pre-trained large language model for translating single-cell transcriptome to proteome._ bioRxiv, 2023: p. 2023.07.04.547619. * [162] Tang, W., et al. _Single-cell multimodal prediction via transformers._ in _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management._ 2023. * [163] Szklarczyk, D., et al., _The STRING database in 2023: protein-protein association networks and functional enrichment analyses for any sequenced genome of interest._ Nucleic Acids Res, 2023. **51**(D1): p. D638-D646. * [164] Wen, H., et al., _Single cells are spatial tokens: Transformers for spatial transcriptomic data imputation._ arXiv preprint arXiv:2302.03038, 2023. * [165] Hou, W. and Z. Ji, _Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis._ Nat Methods, 2024. **21**(8): p. 1462-1465. * [166] Chen, Y. and J. Zou, _GenePT: a simple but effective foundation model for genes and cells built from ChatGPT._ bioRxiv, 2024: p. 2023.10. 16.562533. * [167] Levine, D., et al., _Cell2Sentence: teaching large language models the language of biology._ BioRxiv, 2023: p. 2023.09. 11.557287. * [168] Zhao, S., et al., _Longcell: Language-cell pre-training for cell identity understanding._ arXiv preprint arXiv:2405.06708, 2024. * [169] Lu, Y.-C., et al., _scChat: A Large Language Model-Powered Co-Pilot for Contextualized Single-Cell RNA Sequencing Analysis._ bioRxiv, 2024: p. 2024.10. 01.616063. * [170] Liu, T., et al., _scelmo: Embeddings from language models are good learners for single-cell data analysis._ bioRxiv, 2023: p. 2023.12. 07.569910. * [171] Heimberg, G., et al., _Scalable querying of human cell atlases via a foundational model reveals commonalities across fibrosis-associated macrophages._ BioRxiv, 2023: p. 2023.07. 18.549537."
    },
    {
      "title": "Main Figures",
      "text": "**Figure 1. Summary of the application of large language models in bioinformatics in this review.** Applications of large language models in bioinformatics include applications in genomics, transcriptomics, proteomics, drug discovery and single-cell analysis. Applications of LLMs in genomics focus on LLMs using DNA sequence; applications of LLMs in transcriptomics focus on using RNA sequence; applications of LLMs in proteomics focus on LLMs using protein sequence; applications of LLMs in drug discovery focus on LLMs using molecular data and applications of LLMs in single-cell analysis focus on LLMs using scRNA-seq, scRNA-omics and spatial transcriptomics data. Each corresponds to a variety of biological downstream tasks. Figure 2: **Building blocks of large language models in bioinformatics.****a,** tokenization methods tailored to various data types, including DNA/RNA sequences, proteins, small molecules, and single-cell data. **b,** input embedding strategies used in large language models to encode tokenized data. **c,** schematic representation of the transformer architecture, a foundational structure in LLMs. **d,** the attention mechanism, enabling models to focus on important features in sequences. **e,** the feed-forward network, a critical component of transformers for learning hierarchical representations. **f,** pre-training processes for BERT and GPT-based models, highlighting BERT's bidirectional prediction approach and GPT's left-to-right prediction strategy. Figure 3: **Schematic diagram of the large language model pretraining and fine-tuning process.** The workflow begins with tokenizing the input data, which is then fed into the embedding layer and transformer models. The training process comprises two stages: pretraining and fine-tuning. Pretraining employs self-supervised learning on large-scale, unlabeled reference datasets to develop a general-purpose model with robust generalization capabilities. Fine-tuning builds upon the pretrained model, involving task-specific training to optimize performance for designated applications. [MISSING_PAGE_EMPTY:42] Figure 4: **Downstream tasks of large language models in bioinformatics.** Large language models (LLMs) have seen numerous successful applications in bioinformatics, addressing a wide array of tasks across DNA, RNA, protein, drug discovery, and single-cell analysis. Figure 5: **Guidance for LLM users and developers on how to use and develop LLM in practice.** Guidance for LLM users includes steps such as clarifying the task, selecting an appropriate model, preparing the dataset, training the model, and evaluating its performance. For LLM developers, the focus involves identifying domain-specific challenges, designing tokenization strategies, advancing model architectures, exploring novel tasks and data types, and assessing model capabilities comprehensively. \\begin{table} \\begin{tabular}{c c c c} \\hline \\hline **Application area** & **Data type** & **Method** & **Example** \\\\ \\hline \\multirow{4}{*}{Genomics/Transcriptomics} & \\multirow{4}{*}{DNA/RNA sequence} & One-hot encoding & RNA-FM, RNA-MSM \\\\ & & & DNABERT, Nucleotide Transformer, DNABERT-2, \\\\ & & Fixed-length k-mers & DNAGPT, RNABERT \\\\ & & Special â€™[IND]â€™ token & RNAEmie \\\\ \\hline \\multirow{4}{*}{Proteomics} & MSAs/Protein sequences & Single Amino Acid Tokenization & MSA Transformer/TAPE, ESM-1b, ProfTrans, Progen \\\\ & Biomedical text & WordPiece & ProtST \\\\ & cDNA & Single condo Tokenization & CaLM \\\\ \\hline \\multirow{4}{*}{Drug discovery} & Simplified & Random token & K-BERT \\\\ & Molecular-Input & SmilesTokenizer & ChemBERTa, ChemBERTa-2, MolGPT \\\\ & Line-Entry system & Graph VQ-VAE & Mole-BERT \\\\ & (SMILES) & fingerprint & SMILES-BERT \\\\ \\hline \\multirow{4}{*}{Single-cell analysis} & \\multirow{4}{*}{Expression profiles} & Gene expression Ranking & Generomer, tGPT, iSEEEK \\\\ & & & scBERT, scGPT, scFormer, CellLM, BioFormers, \\\\ & & & CancerFoundation \\\\ \\cline{1-1} & & Gene set/Pathway tokens & TOSICA \\\\ \\cline{1-1} & & Patches & CIForm, scTranSort, scCLIP \\\\ \\cline{1-1} & & Gene value projection & scTranslator, scFoundation, scMulan, scGREAT \\\\ \\cline{1-1} & & Cell tokens & CellPLM, ScRAT, mcBERT \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: **Tokenization methods for different types of data** \\begin{table} \\begin{tabular}{c c c c c} \\hline **Applicatio** & **Model** & **Architecture** & **Pre-training Data** & **Code available** \\\\ \\hline \\multirow{9}{*}{Genomics} & GPN & Transformer-based & Reference genomes from 8 species & _[https://github.com/songlab-call/gen_](https://github.com/songlab-call/gen_) \\\\ & Nucleotide & & 3.2 billion nucleotides in GRCh38/hg38 reference assembly, 20.5 & _[https://github.com/instadden/indexidle_](https://github.com/instadden/indexidle_) \\\\ & Transformer & Transformer-based & trillion nucleotides including 125 million mutations (111 million SNPs, & _[https://github.com/instadden/indexidle_](https://github.com/instadden/indexidle_) \\\\ & Transformer & & 14 million indels), and 174 billion nucleotides from 850 species & _transformer_ \\\\ & DNABERT & BERT-based & 2.75 billion nucleotide based human genome dataset & _[https://github.com/jerrv1/993/DNABERT_](https://github.com/jerrv1/993/DNABERT_) \\\\ & DNABERT-2 & BERT-based & 2.75 billion nucleotide based human genome dataset and 32.49 billion & _[https://github.com/dIGCS_](https://github.com/dIGCS_) \\\\ & DNABERT-based & & nucleotide bases from 135 species, spread across 6 categories & _LAB/DDABERT_ 2 \\\\ & MoDNA & BERT-based & Same as Nucleotide Transformer & _[https://github.com/vitz-emite/MoDNA_](https://github.com/vitz-emite/MoDNA_) \\\\ & GROVER & BERT-based & Homo sapiens (human) genome assembly GRCh37 (hg19) & _[https://github.com/mons/gror_](https://github.com/mons/gror_) \\\\ & MuLan-Methyl & BERT-based & 3 main types of DNA methylation sites (66mA, 4mC, and 5hCm) across & _[https://github.com/hussonlab/mulan-methyl_](https://github.com/hussonlab/mulan-methyl_) \\\\ & iDNA-ABF & BERT-based & & Same as MuLan-Methyl & _[https://github.com/FakeEnd/DNA_ABF_](https://github.com/FakeEnd/DNA_ABF_) \\\\ & iDNA-ABT & BERT-based & Same as MuLan-Methyl & _[https://github.com/YUTINO7/DNA_ABT_](https://github.com/YUTINO7/DNA_ABT_) \\\\ & DNAGPT & GPT-based & Reference genomes from the Ensembl database include 3 billion bps, with a total of 1,594,129,992 bps across 9 species & _[https://github.com/TennGT/AllHealthHealth_](https://github.com/TennGT/AllHealthHealth_) \\\\ & DNABERT & BERT-based & 76 237 human-derived small mRNA from RNAcentral & _[https://github.com/mann438/DNABERT_](https://github.com/mann438/DNABERT_) \\\\ & RNA-FM & BERT-based & About 27 million nCRNA sequences across 47 different databases & _[https://github.com/m4tho/RNA-FM_](https://github.com/m4tho/RNA-FM_) \\\\ & RNA-MSM & BERT-based & 4069 RNA families from fram_ & _[https://github.com/vikhuplu/RNA-MSM_](https://github.com/vikhuplu/RNA-MSM_) \\\\ & SpileeBERT & BERT-based & 2 million sequences and approximately covering 65 billion nucleotides & _[https://github.com/binned-t/SUglieBERT_](https://github.com/binned-t/SUglieBERT_) \\\\ & UNI-RNA & BERT-based & 72 vertices from USCC genome browser & _[https://github.com/ComDec/inrima-tools_](https://github.com/ComDec/inrima-tools_) \\\\ & 3UTRBERT & BERT-based & 108,573 unique mRNA transcripts from the GENCODE and each contains 3,754 nucleotides (median 3048 nts) on average. & _[https://github.com/yangv833/UTREB_](https://github.com/yangv833/UTREB_) \\\\ & UTR-LM & BERT-based & 214,349 unlabeled 5' UTR sequences from Ensembl across 5 species & _[https://github.com/a96123155/](https://github.com/a96123155/) UTR-LM_ \\\\ & RNAErnie & Transformer-based & 23 million nCRNA sequences obtained from the RNAcentral database & _[https://github.com/CallVIII/RNAErnie_](https://github.com/CallVIII/RNAErnie_) \\\\ \\hline \\multirow{9}{*}{Protoomics} & TAPE & Transformer-based & 31 million protein sequences from Pfam & _[https://github.com/songlab-call/nape_](https://github.com/songlab-call/nape_) \\\\ & ESM-1b & Transformer-based & 250 million protein sequences from UniRef50 & _[https://github.com/facebookresearch/esm_](https://github.com/facebookresearch/esm_) \\\\ & Transformer-XL & & About 2.3 billion protein sequences from UniRef and BFD & _[https://github.com/aagemagician/ProTrans_](https://github.com/aagemagician/ProTrans_) \\\\ & ProGTP2 & GPT-based & 50 million protein sequences from UniRef50 & _[https://huggingface.co/docs/transformers/main_classes/trainer_](https://huggingface.co/docs/transformers/main_classes/trainer_) \\\\ & ProteinBERT & BERT-based & 106 million protein sequences with GO annotations from UniRef50 & _[https://github.com/hadra/protein_bert_](https://github.com/hadra/protein_bert_) \\\\ & KeAP & BERT-based & 5 million Triplet in the format of (Protein, Relation, Attribute) with nearly 600k protein, 50k attribute terms, and 31 relation terms included & _[https://github.com/RL-M/Ke_LP_](https://github.com/RL-M/Ke_LP_) \\\\ \\hline \\end{tabular} \\end{table} Table 2: **Foundation models in bioinformatics** \\begin{table} \\begin{tabular}{c l l} \\hline \\hline **Input data** & \\multicolumn{1}{c}{**Biological tasks**} & \\multicolumn{1}{c}{**Models**} \\\\ \\hline & Genome-wide variant effects prediction & DNABERT, DNABERT-2, GPN, Nucleotide Transformer \\\\ & DNA cis-regulatory regions prediction & DNABERT, DNABERT-2, BERT-Promoter, Einhancer- \\\\ & & BERT, Nucleotide Transformer \\\\ DNA sequence & DNA-protein interaction prediction & DNABERT, DNABERT-2, TFBert, GROVER, and MoDNA \\\\ & DNA methylation (6mA,4mc 5bmC) prediction & BERT6mA, iDNA-ABF, iDNA-ABT, and MuLan-Methyl \\\\ & RNA splice sites prediction from DNA sequence & DNABERT, DNABERT-2 \\\\ \\hline & RNA 2D/3D structure prediction & RNA-FM, RNA-MSM, and RNA-FM \\\\ & RNA structural alignment, RNA family clustering & RNABERT \\\\ & RNA splice sites prediction from RNA sequence & SpliceBERT \\\\ & RNA N7-Methylguanosine modification prediction & BERT-m7G \\\\ RNA sequence & RNA 2â€™-O-methylation Modifications prediction & Bert20me \\\\ & Multiple types of RNA modifications prediction & Rm-LR \\\\ & Predicting the association between miRNA, IncRNA and & BertNDA \\\\ & Identifies lncRNAs & LncCat \\\\ & Protein expression and mRNA degradation prediction & CodonBERT \\\\ \\hline & Secondary structure and contact prediction & MSA Transformer, ProtTrans, SPRoBERTa, TAPE, KeAP \\\\ & Protein sequence generation & ProGen, ProtGPT2 \\\\ Protein sequences & Protein function prediction & SPRoBERTa, ProtST, PromptProtein, CalLM \\\\ MSAs & Major PTMs prediction & ProteinBERT \\\\ Gene ontology annotations & Evolution and mutation prediction & SPRoBERTa, UniRep, ESM-1b, TAPE, PLMsearch, DHR \\\\ Triplets of protein-relation-attribute & Biophysical properties prediction & TAPE, PromptProtein \\\\ Protein property descriptions & Protein-protein interaction and binding affinity prediction & KeAP \\\\ cDNA sequences & Antigen-Receptor binding prediction & MHCRoBERTa, BERTMHC, TCR-BERT, SC-AIR-BERT, Antiformer \\\\ & Antigen-Antibody binding prediction & AlphAng, AntiBERTa, EATLM \\\\ \\hline Molecular SMILES & Predicting Molecular Properties & SMILES-BERT, ChemBERTa, K-BERT \\\\ & Generating Molecules & MoiGPT \\\\ Molecular graphs & Predicting Molecular Properties & MOLE-BERT \\\\ Molecular fingerprints and protein sequences & Predicting Drug-Target Interaction & TransDTI, FG-BERT \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Large language models for downstream tasks in bioinformatics"
    },
    {
      "title": "Supplementary Figures",
      "text": "**Supplementary figure 1. Applications of large language models in genomics and transcriptomics.** DNA language models take DNA sequence as input, use transformer, BERT,GPT models to solve multiple biological tasks, including genome-wide variant effects prediction, DNA cis-regulatory regions prediction, DNA-protein interaction prediction, DNA methylation (6mA,4mC 5hmC) prediction, splice sites prediction from DNA sequence. The RNA language models take RNA sequences as input, use transformer, BERT, GPT models to solve multiple biological tasks, including RNA 2D/3D structure prediction, RNA structural alignment,, RNA family clustering, RNA splice sites prediction from RNA sequence, RNA N7-methylguanosine modification prediction, RNA 2'-O-methylation modifications prediction, multiple types of RNA modifications prediction, predicting the association between miRNA, lncRNA and disease, identifying lncRNAs, lncRNAs' coding potential prediction, protein expression and mRNA degradation prediction. **Supplementary figure 2. Applications of large language models in proteomics.** The protein language models take multiple sequence alignment, protein sequence, gene ontology and protein-relation-attribute as input, use transformer, BERT, GPT models to solve multiple biological tasks, including predicting secondary structure, predicting protein generation, predicting protein function,predicting post-translational modifications, predicting evolution and mutation, predicting biophysical properties, predicting protein-protein interaction and predicting antigen-receptor or antigen-antibody binding. **Supplementary figure 3. Applications of large language models in drug discovery.** The language models for drug discovery take molecular SMILES, protein sequence, molecular fingerprints and molecular graphs as input, use transformer, BERT, GPT models to solve multiplebiological tasks, including predicting molecular properties, predicting drug-target interaction, generating molecules and predicting synergistic effects. **Supplementary figure 4. Applications of large language models in single-cell analysis.** The single-cell language models take gene expression or single-cell multi-omics data as input, use transformer, BERT, GPT models to solve multiple biological tasks, including cell type annotation, batch effect removal, multi-omics integration, gene regulation network inference perturbation prediction, dropout imputation. **Supplementary Tables** **Supplementary Table 1. Detailed information of large language models for genomic and transcriptomic tasks** \\begin{tabular}{|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|} \\hline \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline & & & & & 60\\%As and non60\\%As) and \\%. & \\multicolumn{1}{p{113.8pt}|}{} & \\multicolumn{1}{p{113. [MISSING_PAGE_FAIL:62]"
    },
    {
      "title": "References",
      "text": "* (1) Ji, Y., et al., _DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome_. Bioinformatics, 2021. **37**(15): p. 2112-2120. * (2) Dreos, R., et al., _EPD and EPDnew, high-quality promoter resources in the next-generation sequencing era_. Nucleic acids research, 2013. **41**(D1): p. D157-D164. * (3) Consortium, E.P., _An integrated encyclopedia of DNA elements in the human genome_. Nature, 2012. **489**(7414): p. 57. * (4) Cunningham, F., et al., _Ensembl 2019_. Nucleic acids research, 2019. **47**(D1): p. D745-D751. * (5) Sherry, S.T., et al., _dbSNP: the NCBI database of genetic variation_. Nucleic acids research, 2001. **29**(1): p. 308-311. * (6) Zhou, Z., et al., _Dnabert-2: Efficient foundation model and benchmark for multi-species genome_. arXiv preprint arXiv:2306.15006, 2023. * (7) Zeng, H., et al., _Convolutional neural network architectures for predicting DNA-protein binding_. Bioinformatics, 2016. **32**(12): p. i121-i127. * (8) Chen, K., H. Zhao, and Y. Yang, _Capturing large genomic contexts for accurately predicting enhancer-promoter interactions_. Briefings in Bioinformatics, 2022. **3**(2): p. bbab577. * (9) Dalla-Torre, H., et al., _The nucleotide transformer: Building and evaluating robust foundation models for human genomics_. bioRxiv, 2023: p. 2023.01. 11.523679. * (10) Howe, K.L., et al., _Ensembl 2021_. Nucleic acids research, 2021. **49**(D1): p. D884-D891. * (11) Bergstrom, A., et al., _Insights into human genetic variation and population history from 929 diverse genomes_. Science, 2020. **367**(6484): p. eaay5012. * (12) Alonso-Blanco, C., et al., _1,135 genomes reveal the global pattern of polymorphism in Arabidopsis thaliana_. Cell, 2016. **166**(2): p. 481-491. * (13) Zhang, D., et al., _DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks_. bioRxiv, 2023: p. 2023.07. 11.548628. * (14) Kalkatawi, M., et al., _DeepGSR: an optimized deep-learning structure for the recognition of genomic signals and regions_. Bioinformatics, 2019. **35**(7): p. 1125-1132. * (15) Agarwal, V. and J. Shendure, _Predicting mRNA abundance directly from genomic sequence using deep convolutional neural networks_. Cellreports, 2020. **31**(7). * [16] Sanabria, M., et al., _DNA language model GROVER learns sequence context in the human genome._ Nature Machine Intelligence, 2024. **6(8)**: p. 911-923. * [17] de Souza, N., _The ENCODE project._ Nature methods, 2012. **9**(11): p. 1046-1046. * [18] Benegas, G., S.S. Batra, and Y.S. Song, _DNA language models are powerful zero-shot predictors of genome-wide variant effects._ bioRxiv, 2022: p. 2022.08. 22.504706. * [19] Le, N.Q.K., et al., _BERT-Promoter: An improved sequence-based predictor of DNA promoter using BERT pre-trained model and SHAP feature selection._ Computational Biology and Chemistry, 2022. **99**: p. 107732. * [20] Gama-Castro, S., et al., _RegulonDB version 9.0: high-level integration of gene regulation, coexpression, motif clustering and beyond._ Nucleic acids research, 2016. **44**(D1): p. D133-D143. * [21] Xiao, X., et al., _IPSW (2L)-PseKNC: A two-layer predictor for identifying promoters and their strength by hybrid features via pseudo K-tuple nucleotide composition._ Genomics, 2019. **111**(6): p. 1785-1793. * [22] Luo, H., et al., _Improving language model of human genome for DNA-protein binding prediction based on task-specific pre-training._ Interdisciplinary Sciences: Computational Life Sciences, 2023. **15**(1): p. 32-43. * [23] An, W., et al. _MoDNA: motif-oriented pre-training for DNA language model._ in _Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics._ 2022. * [24] Luo, H., et al. _iEnhanced-BERT: A novel transfer learning architecture based on DNA-Language model for identifying enhancers and their strength._ in _International Conference on Intelligent Computing._ 2022. Springer. * [25] Ernst, J., et al., _Mapping and analysis of chromatin state dynamics in nine human cell types._ Nature, 2011. **473**(7345): p. 43-49. * [26] Tsukiyama, S., et al., _BERT6mA: prediction of DNA N6-methyladenosine site using deep learning-based approaches._ Briefings in Bioinformatics, 2022. **23**(2): p. bbac053. * [27] Xiao, C.-L., et al., _N6-methyladenosine DNA modification in the human genome._ Molecular cell, 2018. **71**(2): p. 306-318. e7. * [28] Ye, G., et al., _De novo genome assembly of the stress tolerant forest species Casaurina equisetifolia provides insight into secondary growth._ The Plant Journal, 2019. **97**(4): p. 779-794. * [29] Ye, P., et al., _MehSMRT: an integrative database for DNA N6-methyladenine and N4-methylcytosine generated by single-molecular real time sequencing_. Nucleie acids research, 2016: p. gkw950. * [30] Liu, Z.-Y., et al., _MDR: an integrative DNA N6-methyladenine and N4-methylcytosine modification database for Rosaceae_. Horticulture research, 2019. **6**. * [31] Wang, Y., et al., _N6-adenine DNA methylation is associated with the linker DNA of H2A. Z-containing well-positioned nucleosomes in Pol II-transcribed genes in Terathymena_. Nucleie acids research, 2017. **45**(20): p. 11594-11606. * [32] Jin, J., et al., _iDNA-ABF: multi-scale deep biological language learning model for the interpretable prediction of DNA methylations_. Genome biology, 2022. **23**(1): p. 1-23. * [33] Luo, Y., et al., _New developments on the Encyclopedia of DNA Elements (ENCODE) data portal_. Nucleic acids research, 2020. **48**(D1): p. D882-D889. * [34] Zhang, J., et al., _An integrative ENCODE resource for cancer genomics_. Nature communications, 2020. **11**(1): p. 3696. * [35] Lv, H., et al., _iDNA-MS: an integrated computational tool for detecting DNA modification sites in multiple genomes_. licence, 2020. **23**(4). * [36] Yu, Y., et al., _iDNA-ABF: advanced deep learning model for detecting DNA methylation with adaptive features and transductive information maximization_. Bioinformatics, 2021. **37**(24): p. 4603-4610. * [37] Zeng, W., A. Gautam, and D.H. Huson, _MuLan-Methyl-Multiple Transformer-based Language Models for Accurate DNA Methylation Prediction_. bioRxiv, 2023: p. 2023.01. 04.522704. * [38] Zhang, Y., et al., _Multiple sequence alignment-based RNA language model and its application to structural inference_. Nucleic Acids Research, 2024. **52**(1): p. e3-e3. * [39] Singh, J., et al., _RNA secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning_. Nature communications, 2019. **10**(1): p. 5407. * [40] Chen, J., et al., _Interpretable RNA foundation model from unannotated data for highly accurate RNA structure and function predictions_. bioRxiv, 2022: p. 2022.08. 06.503062. * [41] Tan, Z., et al., _TurboFold II: RNA structural alignment and secondary structure prediction informed by multiple homologs_. Nucleic acids research, 2017. **45**(20): p. 11570-11581. * [42] Sloma, M.F. and D.H. Mathews, _Exact calculation of loop formation probability identifies folding motifs in RNA secondary structures_. RNA, 2016. **22**(12): p. 1808-1818. * [43] Wu, F., et al., _A new coronavirus associated with human respiratory disease in China_. Nature, 2020. **579**(7798): p. 265-269. * [44] Sun, L., et al., _Predicting dynamic cellular protein-RNA interactions by deep learning using in vivo RNA structures_. Cell research, 2021. **31**(5): p. 495-516. * [45] Sample, P.J., et al., _Human 5 \\({}^{\\prime}\\) UTR design and variant effect prediction from a massively parallel translation assay_. Nature biotechnology, 2019. **37**(7): p. 803-809. * [46] Akiyama, M. and Y. Sakakibara, _Informative RNA base embedding for RNA structural alignment and clustering by deep representation learning_. NAR genomics and bioinformatics, 2022. **4**(1): p. lqae012. * [47] Chen, K., et al., _Self-supervised learning on millions of primary RNA sequences from 72 vertebrates improves sequence-based RNA splicing prediction_. Briefings in Bioinformatics, 2024. **25**(3): p. bbea163. * [48] Haeusler, M., et al., _The UCSC genome browser database: 2019 update_. Nucleic acids research, 2019. **47**(D1): p. D853-D858. * [49] Zhang, L., et al., _BERT-m7G: a transformer architecture based on BERT and stacking ensemble to identify RNA N7-Methylguanosine sites from sequence information_. Computational and Mathematical Methods in Medicine, 2021. **2021**. * [50] Dai, C., et al., _Iterative feature representation algorithm to improve the predictive performance of N7-methylguanosine sites_. Briefings in Bioinformatics, 2021. **22**(4): p. bbaa278. * [51] Li, Q., et al., _M64-BERT-Stacking: A Tissue-Specific Predictor for Identifying RNA N6-Methyladenosine Sites Based on BERT and Stacking Strategy_. Symmetry, 2023. **15**(3): p. 731. * [52] Dao, F.-Y., et al., _Computational identification of N6-methyladenosine sites in multiple tissues of mammals_. Computational and structural biotechnology journal, 2020. **18**: p. 1084-1091. * [53] Soylu, N.N. and E. Sefer, _BERT2OME: Prediction of 2'-O-methylation Modifications from RNA Sequence by Transformer Architecture Based on BERT_. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2023. * [54] Xuan, J.-J., et al., _RMBase v2. 0: deciphering the map of RNA modifications from epistranscriptome sequencing data_. Nucleic acids research, 2018. **46**(D1): p. D327-D334. * [55] Liang, S., et al., _Rm-LR: A long-range-based deep learning model for predicting multiple types of RNA modifications_. Computers in Biology and Medicine, 2023. **164**: p. 107238. * [56] Song, Z., et al., _Attention-based multi-label neural networks for integrated prediction and interpretation of twelve widely occurring RNAmodifications_. Nature communications, 2021. **12**(1): p. 4011. * [57] Barrett, T., et al., _NCBI GEO: archive for functional genomics data sets--update_. Nucleic acids research, 2012. **41**(D1): p. D991-D995. * [58] Ramaswami, G. and J.B. Li, _RADAR: a rigorously annotated database of A-to-I RNA editing_. Nucleic acids research, 2014. **42**(D1): p. D109-D113. * [59] Ning, Z., et al., _BertNDA: a Model Based on Graph-Bert and Multi-scale Information Fusion for ncRNA-disease Association Prediction_. bioRxiv, 2023: p. 2023.05. 18.541387. * [60] Li, Y., et al., _HMDD v2. 0: a database for experimentally supported human microRNA and disease associations_. Nucleic acids research, 2014. **42**(D1): p. D1070-D1074. * [61] Jiang, Q., et al., _miRNA2Disease: a manually curated database for microRNA deregulation in human disease_. Nucleic acids research, 2009. **37**(suppl_1): p. D98-D104. * [62] Bao, Z., et al., _IncRNADisease 2.0: an updated database of long non-coding RNA-associated diseases_. Nucleic acids research, 2019. **47**(D1): p. D1034-D1037. * [63] Gao, Y., et al., _Inc2Cancer 3.0: an updated resource for experimentally supported lncRNA/circRNA cancer associations and web tools based on RNd-seq and scRNA-seq data_. Nucleic acids research, 2021. **49**(D1): p. D1251-D1258. * [64] Feng, H., et al., _IncCat: An ORF attention model to identify lncRNA based on ensemble learning strategy and fused sequence information_. Computational and Structural Biotechnology Journal, 2023. **21**: p. 1433-1447. * [65] O'Leary, N.A., et al., _Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation_. Nucleic acids research, 2016. **44**(D1): p. D733-D745. * [66] Xia, S., et al. _A multi-granularity information-enhanced pre-training method for predicting the coding potential of sORFs in plant lncRNAs_. in _2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)_. 2023. IEEE. * [67] Di Marsico, M., et al., _GreeNC 2.0: a comprehensive database of plant long non-coding RNAs_. Nucleic Acids Research, 2022. **50**(D1): p. D1442-D1447. * [68] Babjac, A.N., Z. Lu, and S.J. Emrich. _CodonBERT: Using BERT for Sentiment Analysis to Better Predict Genes with Low Expression_. in _Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics_. 2023. * [69] Nieuwkoop, T., et al., _Revealing determinants of translation efficiency via whole-gene codon randomization and machine learning_. Nucleicacids research, 2023. **51**(5): p. 2363-2376. * [70] Byrska-Bishop, M., et al., _High-coverage whole-genome sequencing of the expanded 1000 Genomes Project cohort including 602 trios_. Cell, 2022. **185**(18): p. 3426-3440. e19. * [71] Bernard, C., et al., _RNA-TorsionBERT: leveraging language models for RNA 3D torsion angles prediction_. bioRxiv, 2024: p. 2024.06. 06.597803. * [72] Wang, X., et al., _UNI-RNA: universal pre-trained models revolutionize RNA research._ bioRxiv, 2023: p. 2023.07. 11.548588. * [73]_RNAcentral: a hub of information for non-coding RNA sequences_. Nucleic Acids Research, 2019. **47**(D1): p. D221-D229. * [74] Sayers, E.W., et al., _Database resources of the national center for biotechnology information._ Nucleic acids research, 2022. **50**(D1): p. D20-D26. * [75] Chen, M., et al., _Genome Warehouse: a public repository housing genome-scale data._ Genomics, Proteomics and Bioinformatics, 2021. **19**(4): p. 584-589. * [76] Chu, Y., et al., _4 5'UTR language model for decoding untranslated regions of mRNA and function predictions_. Nature Machine Intelligence, 2024. **6**(4): p. 449-460. * [77] Yang, Y., et al., _Deciphering 3'UTR Mediated Gene Regulation Using Interpretable Deep Representation Learning._ Advanced Science, 2024. **11**(39): p. 2407013. * [78] Wang, N., et al., _Multi-purpose RNA language modelling with motif-aware pretraining and type-guided fine-tuning._ Nature Machine Intelligence, 2024: p. 1-10. [MISSING_PAGE_EMPTY:70]"
    },
    {
      "title": "References",
      "text": "* [1] Rao, R.M., et al. _MSA transformer_. in _International Conference on Machine Learning_. 2021. * [2] Haas, J., et al., _Continuous Automated Model EvaluatiOn (CAMEO) complementing the critical assessment of structure prediction in CASP12_. Proteins: Structure, Function, and Bioinformatics, 2018. **86**: p. 387-398. * [3] Shrestha, R., et al., _Assessing the accuracy of contact predictions in CASP13_. Proteins: Structure, Function, and Bioinformatics, 2019. **87**(12): p. 1058-1068. * [4] Yang, J., et al., _Improved protein structure prediction using predicted interresidue orientations_. Proceedings of the National Academy of Sciences, 2020. **117**(3): p. 1496-1503. * [5] Cuff, J.A. and G.J. Barton, _Evaluation and improvement of multiple sequence methods for protein secondary structure prediction_. Proteins: Structure, Function, and Bioinformatics, 1999. **34**(4): p. 508-519. * [6] Klausen, M.S., et al., _NetSurP-2.0: Improved prediction of protein structural features by integrated deep learning_. Proteins: Structure, Function, and Bioinformatics, 2019. **87**(6): p. 520-527. * [7] Berman, H.M., et al., _The protein data bank_. Nucleic acids research, 2000. **28**(1): p. 235-242. * [8] Alley, E.C., et al., _Unified rational protein engineering with sequence-based deep representation learning_. Nature methods, 2019. **16**(12): p. 1315-1322. * [9] Suzek, B.E., et al., _UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches_. Bioinformatics, 2015. **31**(6): p. 926-932. * [10] Rocklin, G.J., et al., _Global analysis of protein folding using massively parallel design, synthesis, and testing_. Science, 2017. **357**(6347): p. 168-175. * [11] Gray, V.E., et al., _Quantitative missense variant effect prediction using large-scale mutagenesis data_. Cell systems, 2018. **6**(1): p. 116-124. * [12] Sarkisyan, K.S., et al., _Local fitness landscape of the green fluorescent protein_. Nature, 2016. **533**(7603): p. 397-401. * [13] Rao, R., et al., _Evaluating protein transfer learning with TAPE_. Advances in neural information processing systems, 2019. **32**. * [14] AlQuraishi, M., _ProteinNet: a standardized data set for machine learning of protein structure_. BMC bioinformatics, 2019. **20**: p. 1-10. * [15] Hou, J., B. Adhikari, and J. Cheng, _DeepSF: deep convolutional neural network for mapping protein sequences to folds_. Bioinformatics, 2018. **34**(8): p. 1295-1303. * [16] Murzin, A.G., et al., _SCOP: a structural classification of proteins database for the investigation of sequences and structures._ Journal of molecular biology, 1995. **247**(4): p. 536-540. * [17] Kinch, L.N., et al., _CASP9 target classification._ PROTEINS: structure, function, and bioinformatics, 2011. **79**(S10): p. 21-36. * [18] Kinch, L.N., et al., _CASP 11 target classification._ Proteins: Structure, Function, and Bioinformatics, 2016. **84**: p. 20-33. * [19] Rives, A., et al., _Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences._ Proceedings of the National Academy of Sciences, 2021. **118**(15): p. e2016239118. * [20] Fox, N.K., S.E. Brenner, and J.-M. Chandonia, _SCOPe: Structural Classification of Proteins--extended, integrating SCOP and ASTRAL data and classification of new structures._ Nucleic acids research, 2014. **42**(D1): p. D304-D309. * [21] Moult, J., et al., _Critical assessment of methods of protein structure prediction: Progress and new directions in round XI._ Proteins: Structure, Function, and Bioinformatics, 2016. **84**: p. 4-14. * [22] Riesselman, A.J., J.B. Ingraham, and D.S. Marks, _Deep generative models of genetic variation capture the effects of mutations._ Nature methods, 2018. **15**(10): p. 816-822. * [23] Einaggar, A., et al., _ProtTrans: Towards Cracking the Language of Lifes Code Through Self-Supervised Deep Learning and High Performance Computing._ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021: p. 1-1. * [24] Abriata, L.A., et al., _Assessment of hard target modeling in CASP12 reveals an emerging role of alignment-based contact prediction methods._ Proteins: Structure, Function, and Bioinformatics, 2018. **86**: p. 97-112. * [25] Almagro Armenteros, J.J., et al., _DeepLoc: prediction of protein subcellular localization using deep learning._ Bioinformatics, 2017. **33**(21): p. 3387-3395. * [26] Chandonia, J.-M., N.K. Fox, and S.E. Brenner, _SCOPe: classification of large macromolecular structures in the structural classification of proteins--extended database._ Nucleic acids research, 2019. **47**(D1): p. D475-D481. * [27] Wu, L., et al., _SPRoBERTa: protein embedding learning with local fragment modeling._ Briefings in Bioinformatics, 2022. **23**(6): p. bbaac401. * [28] Gligorijevic, V., et al., _Structure-based protein function prediction using graph convolutional networks._ Nature communications, 2021. **12**(1): p. 3168. * [29] Wang, Z., et al. _Multi-level Protein Structure Pre-training via Prompt Learning._ in _The Eleventh International Conference on Learning Representations_. 2022. * [30] Dallago, C., et al., _FLIP: Benchmark tasks in fitness landscape inference for proteins_. bioRxiv, 2021: p. 2021-11. * [31] Madani, A., et al., _Progen: Language modeling for protein generation_. arXiv preprint arXiv:2004.03497, 2020. * [32] Leinonen, R., et al., _UniProt archive_. Bioinformatics, 2004. **20**(17): p. 3236-3237. * [33] Bairoch, A., et al., _The universal protein resource (UniProt)_. Nucleic acids research, 2005. **33**(suppl_1): p. D154-D159. * [34] Bairoch, A., et al., _Swiss-Prot: jinggling between evolution and stability_. Briefings in bioinformatics, 2004. **5**(1): p. 39-55. * [35] Boeckmann, B., et al., _The SWISS-PROT protein knowledgebase and its supplement TrEMBL in 2003_. Nucleic acids research, 2003. **31**(1): p. 365-370. * [36] Bateman, A., et al., _The Pfam protein families database_. Nucleic acids research, 2004. **32**(suppl_1): p. D138-D141. * [37] Federhen, S., _The NCBI taxonomy database_. Nucleic acids research, 2012. **40**(D1): p. D136-D143. * [38] Notin, P., et al. _Transception: protein fitness prediction with autoregressive transformers and inference-time retrieval_. in _International Conference on Machine Learning_. 2022. * [39] Ferruz, N., S. Schmidt, and B. Hcker, _ProtGPT2 is a deep unsupervised language model for protein design_. Nature communications, 2022. **13**(1): p. 4348. * [40]_UniProt: the universal protein knowledgebase in 2021_. Nucleic acids research, 2021. **49**(D1): p. D480-D489. * [41] Brandes, N., et al., _ProteinBERT: a universal deep-learning model of protein sequence and function_. Bioinformatics, 2022. **38**(8): p. 2102-2110. * [42] Moult, J., et al., _Critical assessment of methods of protein structure prediction (CASP)--Round XII_. Proteins: Structure, Function, and Bioinformatics, 2018. **86**: p. 7-15. * [43] Andreeva, A., et al., _SCOP2 prototype: a new approach to protein structure mining_. Nucleic acids research, 2014. **42**(D1): p. D310-D314. * [44] Andreeva, A., et al., _The SCOP database in 2020: expanded classification of representative family and superfamily domains of known protein structures_. Nucleic acids research, 2020. **48**(D1): p. D376-D382. * [45] Armenteros, J.J.A., et al., _SignalP 5.0 improves signal peptide predictions using deep neural networks_. Nature biotechnology, 2019. **37**: p. 420-423. * [46] Hornbeck, P.V., et al., _PhosphoSitePlus, 2014: mutations, PTMs and recalibrations_. Nucleic acids research, 2015. **43**(D1): p. D512-D520. * [47] Ofer, D. and M. Linial, _ProFET: Feature engineering captures high-level protein functions_. Bioinformatics, 2015. **31**(21): p. 3429-3436. * [48] Brandes, N., D. Ofer, and M. Linial, _ASAP: a machine learning framework for local protein properties._ Database, 2016. **2016**: p. baw133. * [49] Xu, M., et al. _Prost: Multi-modality learning of protein sequences and biomedical texts._ in _International Conference on Machine Learning_. 2023. PMLR. * [50] Bairoch, A. and R. Appweiler, _The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000._ Nucleic acids research, 2000. **28**(1): p. 45-48. * [51] Xu, M., et al., _Prost: Multi-modality learning of protein sequences and biomedical texts._ arXiv preprint arXiv:2301.12040, 2023. * [52] Zhou, H.-Y., et al., _Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling._ bioRxiv, 2023: p. 2023-01. * [53] Zhang, N., et al., _Ontoprotein: Protein pretraining with gene ontology embedding._ arXiv preprint arXiv:2201.11147, 2022. * [54] Outerial, C. and C.M. Deane, _Codon language embeddings provide strong signals for use in protein engineering._ Nature Machine Intelligence, 2024. **6**(2): p. 170-179. * [55] Jarzab, A., et al., _Melome atlas--thermal proteome stability across the tree of life._ Nature methods, 2020. **17**(5): p. 495-503. * [56] Thumuluri, V., et al., _DeepLoc 2.0: multi-label subcellular localization prediction using protein language models._ Nucleic acids research, 2022. **50**(W1): p. W228-W234. * [57] Sridharan, S., et al., _Proteome-wide solubility and thermal stability profiling reveals distinct regulatory roles for ATP._ Nature communications, 2019. **10**(1): p. 1155. * [58] Unsal, S., et al., _Learning functional properties of proteins with language models._ Nature Machine Intelligence, 2022. **4**(3): p. 227-245. * [59] Uhln, M., et al., _Tissue-based map of the human proteome._ Science, 2015. **347**(6220): p. 1260419. * [60] Wang, M., et al., _PaxDb, a database of protein abundance averages across all three domains of life._ Molecular \\& cellular proteomics, 2012. **11**(8): p. 492-500. * [61] Liu, W., et al., _PLMSearch: Protein language model powers accurate and fast sequence search for remote homology._ Nature communications, 2024. **15**(1): p. 2775. * [62] Chandonia, J.-M., et al., _SCOPE: improvements to the structural classification of proteins--extended database to facilitate variant interpretation and machine learning._ Nucleic acids research, 2022. **50**(D1): p. D553-D559. * [63] Sillitoe, I., et al., _CATH: increased structural coverage of functional space._ Nucleic acids research, 2021. **49**(D1): p. D266-D273. * [64] Hong, L., et al., _Fast, sensitive detection of protein homologs using deep dense retrieval._ Nature Biotechnology, 2024: p. 1-13. * [65] Wang, F., et al., _MHCRoBERTa: pan-specific peptide-MHC class I binding prediction through transfer learning with label-agnostic protein sequences_. Brief Bioinform, 2022. **23**(3). * [66] Boutet, E., et al., _UniProtKB/Swiss-Prot: the manually annotated section of the UniProt KnowledgesBase_, in _Plant bioinformatics: methods and protocols_. 2007, Springer. p. 89-112. * [67] Vita, R., et al., _The Immune Epitope Database (IEDB): 2018 update_. Nucleic Acids Res, 2019. **47**(D1): p. D339-D343. * [68] Cheng, J., et al., _BERTMHC: improved MHC-peptide class II interaction prediction with transformer and multiple instance learning_. Bioinformatics, 2021. **37**(22): p. 4172-4179. * [69]_Improved methods for predicting peptide binding affinity to MHCclass II molecules_. 2017. * [70] Wu, K., et al., _TCR-BERT: learning the grammar of T-cell receptors for flexible antigenbinding analyses_. 2021. * [71] Zhang, W., et al., _PIRD: Pan Immune Repertoire Database_. Bioinformatics, 2020. **36**(3): p. 897-903. * [72] Bagaev, D.V., et al., _YD4db in 2019: database extension, new analysis infrastructure and a T-cell receptor motif compendium_. Nucleic Acids Research, 2020. **48**(D1): p. D1057-D1062. * [73] Chen, S.Y., et al., _TCRdb: a comprehensive database for T-cell receptor sequences with powerful search function_. Nucleic Acids Res, 2021. **49**(D1): p. D468-D474. * [74] Wang, Q., et al., _AntiFormer: graph enhanced large language model for binding affinity prediction_. Briefings in Bioinformatics, 2024. **25**(5). * [75] Olsen, T.H., F. Boyles, and C.M.J.P.S. Deane, _Observed Antibody Space: A diverse database of cleaned, annotated, and translated unpaired and paired antibody sequences_. 2022. **31**(1): p. 141-146. * [76] Zhao, Y., et al., _SC-AIR-BERT: a pre-trained single-cell model for predicting the antigen-binding specificity of the adaptive immune receptor_. Brief Bioinform, 2023. **24**(4). * [77] Wu, L., et al., _hu4Rdb: human Antigen Receptor database for interactive clonotype-transcriptome analysis at the single-cell level_. Nucleic Acids Res, 2022. **50**(D1): p. D1244-D1254. * [78] Raybould, M.I.J., et al., _CoV-dbDab: the coronavirus antibody database_. Bioinformatics, 2021. **37**(5): p. 734-735. * [79] Olsen, T.H., I.H. Moal, and C.M. Deane, _AbLang: an antibody language model for completing antibody sequences_. Bioinformatics, Advances, 2022. **2**(1): p. vbao046. * [80] Liu, Y., et al., _Roberta: A robustly optimized bert pretraining approach._ arXiv preprint arXiv:1907.11692, 2019. * [81] Kovaltsuk, A., et al., _Observed antibody space: a resource for data mining next-generation sequencing of antibody repertoires._ The Journal of Immunology, 2018. **201**(8): p. 2502-2509. * [82] Ghraichy, M., et al., _Different B cell subpopulations show distinct patterns in their IgH repertoire metrics._ Elife, 2021. **10**: p. e73111. * [83] Dunbar, J., et al., _SAbDab: the structural antibody database._ Nucleic acids research, 2014. **42**(D1): p. D1140-D1146. * [84] Marks, C., et al., _Humanization of antibodies using a machine learning approach on large-scale repertoire data._ Bioinformatics, 2021. **37**(22): p. 4041-4047. * [85] Wang, D., F. Ye, and H. Zhou, _On pre-trained language models for antibody._ bioRxiv, 2023: p. 2023-01. * [86] Mason, D.M., et al., _Optimization of therapeutic antibodies by predicting antigen specificity from antibody sequence via deep learning._ Nature Biomedical Engineering, 2021. **5**(6): p. 600-612. * [87] Liberis, E., et al., _Parapred: antibody paratope prediction using convolutional and recurrent neural networks._ Bioinformatics, 2018. **34**(17): p. 2944-2950. * [88] Mroczek, E.S., et al., _Differences in the composition of the human antibody repertoire by B cell subsets in the blood._ Frontiers in immunology, 2014. **5**: p. 96. \\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \\hline **Application** & \\multirow{2}{*}{**Models**} & \\multirow{2}{*}{**Ref**} & \\multirow{2}{*}{**Publication**} & \\multirow{2}{*}{**Parameters**} & \\multirow{2}{*}{**Architecture**} & \\multicolumn{3}{c|}{**Datasets**} & \\multirow{2}{*}{**Downstream**} \\\\ \\cline{4-4} \\cline{6-10} _area_ & & & & & & **Data type** & & & **Source** & **Size** & \\\\ \\hline \\multirow{9}{*}{ChernBERTA} & SMILES- & [1] & Sep 2019 & 6 Transformer encoder layers, the feed-forward hidden units are 1024 and the attention heads are 4 & BERT-based & SMILES & ZINC [2] & 18.69 M used, totally more than 35 M & Log prediction, PM2 prediction, and molecular property prediction \\\\ \\cline{2-10} & SMILES- & [1] & Sep 2019 & 6 Transformer encoder layers, the feed-forward hidden units are 1024 and the attention heads are 4 & BERT-based & SMILES & ZINC [2] & 18.69 M used, totally more than 35 M & Log prediction, PM2 prediction, and molecular property prediction \\\\ \\cline{2-10} & SMILES- & [1] & Sep 2019 & 6 Transformer encoder layers, the feed-forward hidden units are 1024 and attention heads are 4 & BERT-based & SMILES & Curted a dataset of 777 & Binary classification prediction of barrier permeability properties, of unique SMILES from binary classification of clinical trial & \\\\ \\cline{2-10} & ChemicalBERTA & [3] & Oct 2020 & Implementation of RoBERTta uses 12 attention heads and 6 layers, resulting in 72 distinct attention mechanisms. & BERT-based & SMILES & PubChem [4] & PubChem, divided data dataset into subsets of 100 K, 250 K, 1 M, and 10 M. & \\begin{tabular}{c} Binary classification prediction of barrier permeability properties, \\\\ binary classification of clinical trial \\\\ variable, whether the compound inhibits HIV \\\\ component for binary classification between native and inactive \\\\ inactive \\\\ \\end{tabular} & [5] & Sep 2022 & Implementation of RoBERTta uses 12 attention heads and 6 layers, resulting in 72 distinct attention mechanisms. & BERT-based & SMILES & PubChem & \\begin{tabular}{c} Our a large corpus of \\\\ 77 million SMILES \\\\ strings \\\\ \\end{tabular} & \\begin{tabular}{c} brain \\\\ 77 million SMILES \\\\ and co-target inhibition \\\\ \\end{tabular} & \\begin{tabular}{c} brain \\\\ 77 million SMILES \\\\ and co-target inhibition \\\\ \\end{tabular} & \\begin{tabular}{c} brain \\\\ 77 million SMILES \\\\ and co-target inhibition \\\\ \\end{tabular} \\\\ \\hline \\multirow{9}{*}{K-BERT} & [6] & Apr 2022 & The hidden size of the transformer encoder is 768, and the number of the attention heads is 12.5x transformer encoders were used in KBERT. & BERT-based & SMILES & CHEMBL [7] & 1.8 M used & \\begin{tabular}{c} Related tasks on 15 drug-discovery-related datasets \\\\ including carcinogenicity, respiratory toxicity and drug \\\\ induced liver injury. \\\\ \\end{tabular} \\\\ \\cline{1-1} \\cline{2-10} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\\\ \\end{tabular} \\end{table} Table 3: Detailed information of large language models for drug-discovery tasks [MISSING_PAGE_FAIL:84]"
    },
    {
      "title": "References",
      "text": "* [1] Wang, S., et al. _Smiles-bert: large scale unsupervised pre-training for molecular property prediction_. in _Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics_. 2019. * [2] Irwin, J.J., et al., _ZINC: a free tool to discover chemistry for biology_. Journal of chemical information and modeling, 2012. **52**(7): p. 1757-1768. * [3] Chithrananda, S., G. Grand, and B. Ramsundar, _ChemBERTa: large-scale self-supervised pretraining for molecular property prediction_. arXiv preprint arXiv:2010.09885, 2020. * [4] Kim, S., et al., _PubChem Chem 2019 update: improved access to chemical data_. Nucleic acids research, 2019. **47**(D1): p. D1102-D1109. * [5]_ChemBERTa-2: Towards Chemical Foundation Models_. * [6] Wu, Z., et al., _Knowledge-based BERT: a method to extract molecular features like computational chemists_. Briefings in Bioinformatics, 2022. **23**(3): p. bbac131. * [7] Mendez, D., et al., _CHEMBL: towards direct deposition of bioassay data_. Nucleic acids research, 2019. **47**(D1): p. D930-D940. * [8] Xia, J., et al. _Mole-bert: Rethinking pre-training graph neural networks for molecules_. in _The Eleventh International Conference on Learning Representations_. 2022. * [9] Sterling, T. and J.J. Irwin, _ZINC 15-ligand discovery_, for everyone. Journal of chemical information and modeling, 2015. **55**(11): p. 2324-2337. * [10] Bagal, V., et al., _MoGPU: molecular generation using a transformer-decoder model_. Journal of Chemical Information and Modeling, 2021. **62**(9): p. 2064-2076. * [11] Polykovskiy, D., et al., _Molecular sets (MOSES): a benchmarking platform for molecular generation models_. Frontiers in pharmacology, 2020. **11**: p. 565644. * [12] Brown, N., et al., _GuacaMol: benchmarking models for de novo molecular design_. Journal of chemical information and modeling, 2019. **59**(3): p. 1096-1108. * [13] Zheng, J., X. Xiao, and W.-R. Qiu, _DT-BERT: identifying drug-target interactions in cellular networking based on BERT and deep learning method_. Frontiers in Genetics, 2022. **13**: p. 859188. * [14] Wishart, D.S., et al., _DrugBank 5.0: a major update to the DrugBank database for 2018_. Nucleic acids research, 2018. **46**(D1): p. D1074-D1082. * [15] Hu, J., et al., _GPCR-drug interactions prediction using random forest with drug-association-matrix-based post-processing procedure_. Computational biology and chemistry, 2016. **60**: p. 59-71. * [16] Kalakoti, Y., S. Yadav, and D. Sundar, _TransDTI: transformer-based language models for estimating DTs and building a drug recommendation workflow_. ACS omega, 2022. **7**(3): p. 2706-2717. * [17] Tang, J., et al., _Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis_. Journal of Chemical Information and Modeling, 2014. **54**(3): p. 735-743. * [18] Chu, Y., et al., _DTI-MLCD: predicting drug-target interactions using multi-label learning with community detection method_. Briefings in bioinformatics, 2021. **22**(3):p. bbaa205. * [19] Nguyen, T.M., T. Nguyen, and T. Tran, _Mitigating cold-start problems in drug-target affinity prediction with interaction knowledge transferring_. Briefings in Bioinformatics, 2022. **23**(4): p. bbaa269. * [20] Szklarczyk, D., et al., _The STRING database in 2021: customizable protein-protein networks, and functional characterization of user-uploaded gene/measurement sets_. Nucleic acids research, 2021. **49**(D1): p. D605-D612. * [21] Kuhn, M., et al., _STITCH: interaction networks of chemicals and proteins_. Nucleic acids research, 2007. **36**(suppl_): p. D684-D688. * [22] Davis, M.I., et al., _Comprehensive analysis of kinase inhibitor selectivity_. Nature Biotechnology, 2011. **29**(11): p. 1046-1051. * [23] Wang, R., et al., _The PDBbind database: Collection of binding affinities for protein- ligand complexes with known three-dimensional structures_. Journal of medicinal chemistry, 2004. **47**(12): p. 2977-2980. * [24] Wang, R., et al., _The PDBbind database: methodologies and updates_. Journal of medicinal chemistry, 2005. **48**(12): p. 4111-4119. * [25] Kang, H., et al., _Fine-tuning of bert model to accurately predict drug-target interactions_. Pharmaceutics, 2022. **14**(8): p. 1710. * [26] Zitnik, M., R. Sosic, and J. Leskovec, _BioSNAP Datasets: Stanford biomedical network dataset collection_. Note: http://snap. stanford. edu/biodata Cited by, 2018. **5**(1). * [27] Liu, T., et al., _BindingDB: a web-accessible database of experimentally determined protein-ligand binding affinities_. Nucleic acids research, 2007. **35**(suppl_): p. D198-D201. [MISSING_PAGE_FAIL:88]"
    },
    {
      "title": "References",
      "text": "* [1] Yang, F., et al., _scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data_. Nature Machine Intelligence, 2022. **4**(10): p. 852-866. * [2] Franzen, O., L.M. Gan, and J.L.M. Bjorkegren, _PanglaoDB: a web server for exploration of mouse and human single-cell RNA sequencing data_. Database (Oxford), 2019. **2019**. * [3] Zheng, G.X., et al., _Massively parallel digital transcriptional profiling of single cells_. Nat Commun, 2017. **8**: p. 14049. * [4] Baron, M., et al., _A Single-Cell Transcriptomic Map of the Human and Mouse Pancreas Reveals Inter- and Intra-cell Population Structure_. Cell Syst, 2016. **3**(4): p. 346-360 e4. * [5] Muraro, M.J., et al., _A Single-Cell Transcriptome Atlas of the Human Pancreas_. Cell Syst, 2016. **3**(4): p. 385-394 e3. * [6] Segerstolpe, A., et al., _Single-Cell Transcriptome Profiling of Human Pancreatic Ilets in Health and Type 2 Diabetes_. Cell Metab, 2016. **24**(4): p. 593-607. * [7] Xin, Y., et al., _RNA Sequencing of Single Human Islet Cells Reveals Type 2 Diabetes Genes_. Cell Metab, 2016. **24**(4): p. 608-615. * [8] MacParland, S.A., et al., _Single cell RNA sequencing of human liver reveals distinct intrahepatic macrophage populations_. Nat Commun, 2018. **9**(1): p. 4383. * [9] Litvinukova, M., et al., _Cells of the adult human heart_. Nature, 2020. **588**(7838): p. 466-472. * [10] Tucker, N.R., et al., _Transcriptional and Cellular Diversity of the Human Heart_. Circulation, 2020. **142**(5): p. 466-482. * [11] Lukassen, S., et al., _SARS-CoV-2 receptor ACE2 and TMPRSS2 are primarily expressed in bronchial transient secretory cells_. EMBO J, 2020. **39**(10): p. e105114. * [12] He, S., et al., _Single-cell transcriptome profiling of an adult human cell atlas of 15 major organs_. Genome Biol, 2020. **21**(1): p. 294. * [13] Cui, H., et al., _scGPT: toward building a foundation model for single-cell multi-omics using generative AI_. Nat Methods, 2024. **21**(8): p. 1470-1480. * [14] (n.d.)., C.Z.I., _CZ CELLARGENE Discover_. 2022, [https://cellkggene.cziscience.com/](https://cellkggene.cziscience.com/). * [15] Gayoso, A., et al., _A Python library for probabilistic analysis of single-cell omics data_. Nat Biotechnol, 2022. **40**(2): p. 163-166. * [16] Luecken, M.D., et al., _Benchmarking atlas-level data integration in single-cell genomics_. Nat Methods, 2022. **19**(1): p. 41-50. * [17] Chen, J., et al., _Transformer for one stop interpretable cell type annotation_. Nat Commun, 2023. **14**(1): p. 223. * [18] Lawlor, N., et al., _Single-cell transcriptomes identify human islet cell signatures and reveal cell-type-specific expression changes in type 2 diabetes_. Genome Res, 2017. **27**(2): p. 208-222. * [19] Adamson, B., et al., _A Multiplexed Single-Cell CRISPR Screening Platform Enables Systematic Dissection of the Unfolded Protein Response_. Cell, 2016. **167**(7): p. 1867-1882 e21. * [20] Norman, T.M., et al., _Exploring genetic interaction manifolds constructed from rich single-cell phenotypes_. Science, 2019. **365**(6455): p. 786-793. * [21] Cusanovich, D.A., et al., _Multiplex single cell profiling of chromatin accessibility by combinatorial cellular indexing_. Science, 2015. **348**(6237): p. 910-4. * [22] Mimitou, E.P., et al., _Scalable, multimodal profiling of chromatin accessibility, gene expression and protein levels in single cells_. Nat Biotechnol, 2021. **39**(10): p. 1246-1258. * [23] Xu, J., et al., _CIForm as a Transformer-based model for cell-type annotation of large-scale single-cell RNA-seq data_. Brief Bioinform, 2023. **24**(4). * [24] Sun, _Z., et al., _A Bayesian mixture model for clustering droplet-based single-cell transcriptomic data from population studies_. Nat Commun, 2019. **10**(1): p. 1649. * [25] Oetjen, K.A., et al., _Human bone marrow assessment by single-cell RNA sequencing, mass cytometry, and flow cytometry_. JCI Insight, 2018. **3**(23). * [26] Dahlin, J.S., et al., _A single-cell hematopoietic landscape resolves 8 lineage trajectories and defects in Kit mutant mice_. Blood, 2018. **131**(21): p. e1-e11. * [27] Zeisel, A., et al., _Molecular Architecture of the Mouse Nervous System_. Cell, 2018. **174**(4): p. 999-1014 e22. * [28] Saunders, A., et al., _Molecular Diversity and Specializations among the Cells of the Adult Mouse Brain_. Cell, 2018. **174**(4): p. 1015-1030 e16. * [29] Rosenberg, A.B., et al., _Single-cell profiling of the developing mouse brain and spinal cord with split-pool barcoding_. Science, 2018. **360**(6385): p. 176-182. * [30] Tabula Muris, C., et al., _Single-cell transcriptomics of 20 mouse organs creates a Tabula Muris_. Nature, 2018. **562**(7727): p. 367-372. * [31] Zhang, L., et al., _Lineage tracking reveals dynamic relationships of T cells in colorectal cancer_. Nature, 2018. **564**(7735): p. 268-272. * [32] Tasic, B., et al., _Shared and distinct transcriptomic cell types across neocortical areas_. Nature, 2018. **563**(7729): p. 72-78. * [33] Chou, C.H., et al., _Synovial cell cross-talk with cartilage plays a major role in the pathogenesis of osteoarthritis._ Sci Rep, 2020. **10**(1): p. 10868. * [34] Alsaigh, T., et al., _Decoding the transcriptome of calcified atherosclerotic plaque at single-cell resolution._ Commun Biol, 2022. **5**(1): p. 1084. * [35] Zeisel, A., et al., _Brain structure. Cell types in the mouse cortex and hippocampus revealed by single-cell RNA-seq._ Science, 2015. **347**(6226): p. 1138-42. * [36] Bastidas-Ponce, A., et al., _Comprehensive single cell mRNA profiling reveals a detailed roadmap for pancreatic endocrinogenesis._ Development, 2019. **146**(12). * [37] Tabula Muris, C., _A single-cell transcriptomic atlas characterizes ageing tissues in the mouse._ Nature, 2020. **583**(7817): p. 590-595. * [38] Jiao, L., et al., _scTransSort: Transformers for Intelligent Annotation of Cell Types by Gene Embeddings._ Biomolecules, 2023. **13**(4). * [39] Shao, X., et al., _scDeepSort: a pre-trained cell-type annotation method for single-cell transcriptomics using deep learning with a weighted graph neural network._ Nucleic Acids Res, 2021. **49**(21): p. e122. * [40] Song, T., et al., _TransCluster: A Cell-Type Identification Method for single-cell RNA-Seq data using deep learning based on transformer._ Front Genet, 2022. **13**: p. 1038919. * [41] Han, X., et al., _Construction of a human cell landscape at single-cell level._ Nature, 2020. **581**(7808): p. 303-309. * [42] Theodoris, C.V., et al., _Transfer learning enables predictions in network biology._ Nature, 2023. **618**(7965): p. 616-624. * [43] Lhoest, Q., et al., _Datasets: A community library for natural language processing._ arXiv preprint arXiv:2109.02846, 2021. * [44] Shen, H., et al., _Generative pretraining from large-scale transcriptomes for single-cell deciphering._ iScience, 2023. **26**(5): p. 106536. * [45] Regev, A., et al., _The human cell atlas white paper._ arXiv preprint arXiv:1810.05192, 2018. * [46] Peng, Y.R., et al., _Molecular Classification and Comparative Taxonomics of Foveal and Peripheral Cells in Primate Retina._ Cell, 2019. **176**(5): p. 1222-1237 e22. * [47] Johnson, W.E., C. Li, and A. Rabinovic, _Adjusting batch effects in microarray expression data using empirical Bayes methods._ Biostatistics, 2007. **8**(1): p. 118-27. * [48] Huang, T.X. and L. Fu, _The immune landscape of esophageal cancer._ Cancer Commun (Lond), 2019. **39**(1): p. 79. * [49] Ma, A., et al., _Single-cell biological network inference using a heterogeneous graph transformer._ Nat Commun, 2023. **14**(1): p. 964. * [50] Luecken, M.D., et al. _A sandbox for prediction and integration of DNA, RNA, and proteins in single cells._ in _Thirty-fifth conference on neural information processing systems datasets and benchmarks track (Round 2)._ 2021. * [51] Li, G., et al., _A deep generative model for multi-view profiling of single-cell RNA-seq and ATAC-seq data._ Genome Biol, 2022. **23**(1): p. 20. * [52] Cao, J., et al., _Joint profiling of chromatin accessibility and gene expression in thousands of single cells._ Science, 2018. **361**(6409): p. 1380-1385. * [53] Zhu, C., et al., _An ultra high-throughput method for single-cell joint analysis of open chromatin and transcriptome._ Nat Struct Mol Biol, 2019. **26**(11): p. 1063-1070. * [54] Chen, S., B.B. Lake, and K. Zhang, _High-throughput sequencing of the transcriptome and chromatin accessibility in the same cell._ Nat Biotechnol, 2019. **37**(12): p. 1452-1457. * [55] Ma, S., et al., _Chromatin Potential Identified by Shared Single-Cell Profiling of RNA and Chromatin._ Cell, 2020. **183**(4): p. 1103-1116 e20. * [56] Linjing, L., et al., _A pre-trained large language model for translating single-cell transcriptome to proteome._ bioRxiv, 2023: p. 2023.07.04.547619. * [57] Cancer Genome Atlas Research, N., _Comprehensive molecular characterization of clear cell renal cell carcinoma._ Nature, 2013. * [58] Ciriello, G., et al., _Comprehensive Molecular Portraits of Invasive Lobular Breast Cancer._ Cell, 2015. **163**(2): p. 506-19. * [59] Fishbein, L., et al., _Comprehensive Molecular Characterization of Pheochromocytoma and Paraganglioma._ Cancer Cell, 2017. **31**(2): p. 181-193. * [60] Kahles, A., et al., _Comprehensive Analysis of Alternative Splicing Across Tumors from 8,705 Patients._ Cancer Cell, 2018. **34**(2): p. 211-224 e6. * [61] Cao, L., et al., _Proteogenomic characterization of pancreatic ductal adenocarcinoma._ Cell, 2021. **184**(19): p. 5031-5052 e26. * [62] Dou, Y., et al., _Proteogenomic Characterization of Endometrial Carcinoma._ Cell, 2020. **180**(4): p. 729-748 e26. * [63] Gillette, M.A., et al., _Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma._ Cell, 2020. **182**(1):p. 200-225 e35. * [64] Krug, K., et al., _Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy_. Cell, 2020. **183**(5): p. 1436-1456 e31. * [65] Petralia, F., et al., _Integrated Proteobacteria Characterization across Major Histological Types of Pediatric Brain Cancer_. Cell, 2020. **183**(7): p. 1962-1985 e31. * [66] Satpathy, S., et al., _d proteogenomic portrait of lung squamous cell carcinoma_. Cell, 2021. **184**(16): p. 4348-4371 e40. * [67] Wang, L.B., et al., _Proteogenomic and metabolomic characterization of human glioblastoma_. Cancer Cell, 2021. **39**(4): p. 509-528 e20. * [68] Encyclopedia, T.C.C.L., et al., _Consistency of drug profiles and predictors in large-scale cancer cell line data_. Nature, 2015. **528**(7580): p. 84. * [69] Nusinow, D.P., et al., _Quantitative Proteomics of the Cancer Cell Line Encyclopedia_. Cell, 2020. **180**(2): p. 387-402 e16. * [70] Pietzak, E.J., et al., _Genomic Differences Between \"Primary\" and \"Secondary\" Muscle-invasive Bladder Cancer as a Basis for Disparate Outcomes to Cisplatin-based Neoadjuvant Chemotherapy_. Eur Urol, 2019. **75**(2): p. 231-239. * [71] Hao, Y., et al., _Integrated analysis of multimodal single-cell data_. Cell, 2021. **184**(13): p. 3573-3587 e29. * [72] Peterson, V.M., et al., _Multiplexed quantification of proteins and transcripts in single cells_. Nat Biotechnol, 2017. **35**(10): p. 936-939. * [73] Stoeckius, M., et al., _Simultaneous epitope and transcriptome measurement in single cells_. Nat Methods, 2017. **14**(9): p. 865-868. * [74] Cheng, S., et al., _A pan-cancer single-cell transcriptional atlas of tumor infiltrating myeloid cells_. Cell, 2021. **184**(3): p. 792-809 e23. * [75] Hao, M., et al., _Large-scale foundation model on single-cell transcriptomics_. Nat Methods, 2024. **21**(8): p. 1481-1491. * [76] Liu, Q., et al., _DeepCDR: a hybrid graph convolutional network for predicting cancer drug response_. Bioinformatics, 2020. **36**(Suppl_2): p. 911-i918. * [77] Roohani, Y., K. Huang, and J. Leskovec, _Predicting transcriptional outcomes of novel multigene perturbations with GEARS_. Nat Biotechnol, 2023. * [78] Tang, W., et al. _Single-cell multimodal prediction via transformers_. in _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_. 2023. * [79] Yang, X., et al., _GeneCompass: deciphering universal gene regulatory mechanisms with a knowledge-informed cross-species foundation model_. Cell Res, 2024. **34**(12): p. 830-845. * [80] Qiao, Y., et al., _Synergistic activation of inflammatory cytokine genes by interferon-gamma-induced chromatin remodeling and toll-likereceptor signaling_. Immunity, 2013. **39**(3): p. 454-69. * [81] Srivatsan, S.R., et al., _Massively multiplex chemical transcriptomics at single-cell resolution_. Science, 2020. **367**(6473): p. 45-51. * [82] Garipler, G., et al., _The BTB transcription factors ZBTB11 and ZFP131 maintain pluripotency by repressing pro-differentiation genes_. Cell Rep, 2022. **38**(11): p. 110524. * [83] Bian, H., et al. _scMulan: a multitask generative pre-trained language model for single-cell analysis_. in _International Conference on Research in Computational Molecular Biology_. 2024. Springer. * [84] Simonson, B., et al., _Single-nucleus RNA sequencing in ischemic cardiomyopathy reveals common transcriptional profile underlying end-stage heart failure_. Cell Rep, 2023. **42**(2): p. 112086. * [85] Suo, C., et al., _Mapping the developing human immune system across organs_. Science, 2022. **376**(6597): p. eabo0510. * [86] Lotfollahi, M., et al., _Mapping single-cell data to reference atlases by transfer learning_. Nat Biotechnol, 2022. **40**(1): p. 121-130. * [87] Rosen, Y., et al., _Universal cell embeddings: A foundation model for cell biology_. bioRxiv, 2023: p. 2023.11. 28.568918. * [88] Dominguez Conde, C., et al., _Cross-tissue immune cell analysis reveals tissue-specific features in humans_. Science, 2022. **376**(6594): p. eab15197. * [89] Speranza, E., et al., _Single-cell RNA sequencing reveals SARS-CoV-2 infection dynamics in lungs of African green monkeys_. Sci Transl Med, 2021. **13**(578). * [90] Hilton, H.G., et al., _Single-cell transcriptomics of the naked mole-rat reveals unexpected features of mammalian immunity_. PLoS Biol, 2019. **17**(11): p. e3000528. * [91] Yamagata, M., W. Yan, and J.R. Sanes, _A cell atlas of the chick retina based on single-cell transcriptomics_. Elife, 2021. **10**. * [92] Orozco, L.D., et al., _Integration of eQTL and a Single-Cell Atlas in the Human Eye Identifies Causal Genes for Age-Related Macular Degeneration_. Cell Rep, 2020. **30**(4): p. 1246-1259 e6. * [93] Zhao, S., J. Zhang, and Z. Nie, _Large-scale cell representation learning via divide-and-conquer contrastive learning_. arXiv preprint arXiv:2306.04371, 2023. * [94] Aissa, A.F., et al., _Single-cell transcriptional changes associated with drug tolerance and response to combination therapies in cancer_. Nat Commun, 2021. **12**(1): p. 1628. * [95] Sharma, A., et al., _Longitudinal single-cell RNA sequencing of patient-derived primary cells reveals drug-induced infidelity in stem cell hierarchy_. Nat Commun, 2018. **9**(1): p. 4931. * [96] Ravasio, A., et al., _Single-cell analysis of Eph4 clustering phenotypes to probe cancer cell heterogeneity_. Commun Biol, 2020. **3**(1): p. 429. * [97] Suhavilai, C., et al., _Predicting heterogeneity in clone-specific therapeutic vulnerabilities using single-cell transcriptomic signatures_. Genome Med, 2021. **13**(1): p. 189. * [98] Barretina, J., et al., _The Cancer Cell Line Encyclopedia enables predictive modelling of anticancer drug sensitivity_. Nature, 2012. **483**(7391): p. 603-7. * [99] Iorio, F., et al., _A Landscape of Pharmacogenomic Interactions in Cancer_. Cell, 2016. **166**(3): p. 740-754. * [100] Xiong, L., T. Chen, and M. Kellis. _scCLIP: Multi-modal Single-cell Contrastive Learning Integration Pre-training_. in _NeurIPS 2023 AI for Science Workshop_. * [101] Cao, J., et al., _human cell atlas of fetal gene expression_. Science, 2020. **370**(6518). * [102] Domcke, S., et al., _A human cell atlas of fetal chromatin accessibility_. Science, 2020. **370**(6518). * [103] Anderson, A.G., et al., _Single nucleus multiomics identifies ZEB1 and MAFB as candidate regulators of Alzheimer's disease-specific cis-regulatory elements_. Cell Genom, 2023. **3**(3): p. 100263. * [104] Shen, H., et al., _A universal approach for integrating super large-scale single-cell transcriptomes by exploring gene rankings_. Brief Bioinform, 2022. **23**(2). * [105] Kang, H.M., et al., _Multiplexed droplet single-cell RNA-sequencing using natural genetic variation_. Nat Biotechnol, 2018. **36**(1): p. 89-94. * [106] Guo, X., et al., _Global characterization of T cells in non-small-cell lung cancer by single-cell sequencing_. Nat Med, 2018. **24**(7): p. 978-985. * [107] Zheng, C., et al., _Landscape of Infiltrating T Cells in Liver Cancer Revealed by Single-Cell Sequencing_. Cell, 2017. **169**(7): p. 1342-1356. e16. * [108] Wen, H., et al., _CellPLM: pre-training of cell language model beyond single cells_. bioRxiv, 2023: p. 2023.10. 03.560734. * [109] Li, Y., et al., _Single-Cell Transcriptome Analysis Reveals Dynamic Cell Populations and Differential Gene Expression Patterns in Control and Aneurysmal Human Aortic Tissue_. Circulation, 2020. **142**(14): p. 1374-1388. * [110] Schirmer, L., et al., _Neuronal vulnerability and multilineage diversity in multiple sclerosis_. Nature, 2019. **573**(7772): p. 75-82. * [111] Wang, Y., et al., _scGREAT: Transformer-based deep-language model for gene regulatory network inference from single-cell transcriptomics_. iScience, 2024. **27**(4): p. 109352. * [112] Chu, L.F., et al., _Single-cell RNA-seq reveals novel regulators of human embryonic stem cell differentiation to definitive endoderm._ Genome Biol, 2016. **17**(1): p. 173. * [113] Mora-Bermudez, F., et al., _Differences and similarities between human and chimpanzee neural progenitors during cerebral cortex development._ Elife, 2016. **5**. * [114] Camp, J.G., et al., _Multilineage communication regulates human liver bud development from pluripotency._ Nature, 2017. **546**(7659): p. 533-538. * [115] Shalek, A.K., et al., _Single-cell RNA-seq reveals dynamic paracrine control of cellular variation._ Nature, 2014. **510**(7505): p. 363-9. * [116] Hayashi, T., et al., _Single-cell full-length total RNA sequencing uncovers dynamics of recursive splicing and enhancer RNAs._ Nat Commun, 2018. **9**(1): p. 619. * [117] Nestorowa, S., et al., _A single-cell resolution map of mouse hematopoietic stem and progenitor cell differentiation._ Blood, 2016. **128**(8): p. e20-31. * [118] Amara-Belgadi, S., et al., _BIOFORMERS: A SCALABLE FRAMEWORK FOR EXPLORING BIOSTATES USING TRANSFERORERS._ bioRxiv, 2023: p. 2023.11. 29.569320. * [119] Zheng, G.X., et al., _Massively parallel digital transcriptional profiling of single cells._ Nature communications, 2017. **8**(1): p. 14049. * [120] Kalfon, J., et al., _scPRINT: pre-training on 50 million cells allows robust gene network predictions._ bioRxiv, 2024: p. 2024.07. * [121] Kong, L., et al., _The landscape of immune dysregulation in Crohn's disease revealed through single-cell transcriptomic profiling in the ileum and colon._ Immunity, 2023. **56**(2): p. 444-458 e5. * [122] Wang, S.K., et al., _Single-cell multiome of the human retina and deep learning nominate causal variants in complex eye diseases._ Cell Genom, 2022. **2**(8). * [123] Marshall, J.L., et al., _High-resolution Slide-seqV2 spatial transcriptomics enables discovery of disease-specific cell neighborhoods and pathways._ iScience, 2022. **25**(4): p. 104097. * [124] Dixit, A., et al., _Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens._ Cell, 2016. **167**(7): p. 1853-1866 e17. * [125] Park, P.J., _ChIP-seq: advantages and challenges of a maturing technology._ Nat Rev Genet, 2009. **10**(10): p. 669-80. * [126] Burclaff, J., et al., _A Proximal-to-Distal Survey of Healthy Adult Human Small Intestine and Colon Epithelium by Single-Cell Transcriptomics._ Cell Mol Gastroenterol Hepatol, 2022. **13**(5): p. 1554-1589. * [127] van Zyl, T., et al., _Cell atlas of the human ocular anterior segment: Tissue-specific and shared cell types._ Proc Natl Acad Sci U S A, 2022. **119**(29): p. e2200914119. * [128] Joseph, D.B., et al., _Single-cell analysis of mouse and human prostate reveals novel fibroblasts with specialized distribution and microenvironment interactions._ J Pathol, 2021. **255**(2): p. 141-154. * [129] Mao, Y., et al., _Phenotype prediction from single-cell RNA-seq data using attention-based neural networks._ Bioinformatics, 2024. **40**(2). * [130] julian.knight(@well.ox.ac.uk, C.O.-M.-o.B.A.C.E.a. and C.O.-M.-o.B.A. Consortium, _A blood atlas of COVID-19 defines hallmarks of disease severity and specificity._ Cell, 2022. **185**(5): p. 916-938 e58. * [131] Stephenson, E., et al., _Single-cell multi-omics analysis of the immune response in COVID-19._ Nat Med, 2021. **27**(5): p. 904-916. * [132] Ren, X., et al., _COVID-19 immune features revealed by a large-scale single-cell transcriptome atlas._ Cell, 2021. **184**(7): p. 1895-1913 e19. * [133] Theus, A., et al., _Cancer Foundation: 4 single-cell RNA sequencing foundation model to decipher drug resistance in cancer._ bioRxiv, 2024: p. 2024.11. 01.621087. * [134] Neftel, C., et al., _An Integrative Model of Cellular States, Plasticity, and Genetics for Glioblastoma._ Cell, 2019. **178**(4): p. 835-849 e21. * [135] Wissel, D., et al., _Survboard: standardised benchmarking for multi-omics cancer survival models._ bioRxiv, 2022: p. 2022.11. 18.517043. * [136] Cancer Genome Atlas Research, N., et al., _The Cancer Genome Atlas Pan-Cancer analysis project._ Nat Genet, 2013. **45**(10): p. 1113-20. * [137] Querfurth, B.v., et al., _mcBERT: Patient-Level Single-cell Transcriptomics Data Representation._ bioRxiv, 2024: p. 2024.11. 04.621897. * [138] Chaffin, M., et al., _Single-nucleus profiling of human dilated and hypertrophic cardiomyopathy._ Nature, 2022. **608**(7921): p. 174-180. * [139] Koenig, A.L., et al., _Single-cell transcriptomics reveals cell-type-specific diversification in human heart failure._ Nat Cardiovasc Res, 2022. **1**(3): p. 263-280. * [140] Reichart, D., et al., _Pathogenic variants damage cell composition and single cell transcription in cardiomyopathies._ Science, 2022. **377**(6606): p. eabo1984. * [141] Kuppe, C., et al., _Spatial multi-omic map of human myocardial infarction._ Nature, 2022. **608**(7924): p. 766-777. * [142] Lake, B.B., et al., _An atlas of healthy and injured cell states and niches in the human kidney._ Nature, 2023. **619**(7970): p. 585-594. * [143] Kuppe, C., et al., _Decoding myofibral origins in human kidney fibrosis._ Nature, 2021. **589**(7841): p. 281-286. * [144] Muto, Y., et al., _Defining cellular complexity in human autosomal dominant polycystic kidney disease by multimodal single cell analysis._ Nat Commun, 2022. **13**(1): p. 6497. * [145] Wilson, P.C., et al., _Multimodal single cell sequencing implicates chromatin accessibility and genetic background in diabetic kidney disease progression._ Nat Commun, 2022. **13**(1): p. 5253. * [146] Muto, Y., et al., _Single cell transcriptional and chromatin accessibility profiling redefine cellular heterogeneity in the adult human kidney._ Nat Commun, 2021. **12**(1): p. 2190. * [147] Perez, R.K., et al., _Single-cell RNA-seq reveals cell type-specific molecular and genetic associations to lupus._ Science, 2022. **376**(6589): p. eabf1970. * [148] Yoshida, M., et al., _Local and systemic responses to SARS-CoV-2 infection in children and adults._ Nature, 2022. **602**(7896): p. 321-327. * [149] Sikkema, L., et al., _An integrated cell atlas of the lung in health and disease._ Nat Med, 2023. **29**(6): p. 1563-1577. * [150] Schaar, A., et al., _Nicheformer: a foundation model for single-cell and spatial omics. 2024._ Preprint at bioRxiv, 2024. **4**: p. 589472. * [151] Yao, Z., et al., _A high-resolution transcriptomic and spatial atlas of cell types in the whole mouse brain._ Nature, 2023. **624**(7991): p. 317-332. * [152] He, S., et al., _High-plex multiomic analysis in FFPE at subcellular level by spatial molecular imaging. bioRxiv 467020._ 2021. * [153] Wen, H., et al., _Single cells are spatial tokens: Transformers for spatial transcriptomic data imputation._ arXiv preprint arXiv:2302.03038, 2023."
    }
  ]
}