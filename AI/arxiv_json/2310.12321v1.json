{
  "title": "A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4",
  "authors": [
    "Katikapalli Subramanyam"
  ],
  "abstract": "\n Large language models (LLMs) are a special class of pretrained language models obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI's GPT-3 model, and the popularity of LLMs is increasing exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GPT-3 family large language models. \n",
  "references": [
    {
      "id": null,
      "title": "A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4",
      "authors": [
        "Katikapalli Subramanyam"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Ammus: A survey of transformer-based pretrained models in natural language processing",
      "authors": [
        "K S Kalyan",
        "A Rajasekharan",
        "S Sangeetha"
      ],
      "year": "2021",
      "venue": "Ammus: A survey of transformer-based pretrained models in natural language processing",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Ammu: a survey of transformer-based biomedical pretrained language models",
      "authors": [
        "K S Kalyan",
        "A Rajasekharan",
        "S Sangeetha"
      ],
      "year": "2022",
      "venue": "Journal of biomedical informatics",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J D Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C D Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Enriching word vectors with subword information",
      "authors": [
        "P Bojanowski",
        "E Grave",
        "A Joulin",
        "T Mikolov"
      ],
      "year": "2017",
      "venue": "Transactions of the association for computational linguistics",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "A convolutional neural network for modelling sentences",
      "authors": [
        "N Kalchbrenner",
        "E Grefenstette",
        "P Blunsom"
      ],
      "year": "2014",
      "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Recent advances in recurrent neural networks",
      "authors": [
        "H Salehinejad",
        "S Sankar",
        "J Barfett",
        "E Colak",
        "S Valaee"
      ],
      "year": "2017",
      "venue": "Recent advances in recurrent neural networks",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "NIPS 2014 Workshop on Deep Learning",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Sequence to sequence learning with neural networks",
      "authors": [
        "I Sutskever",
        "O Vinyals",
        "Q V Le"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K H Cho",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Effective approaches to attention-based neural machine translation",
      "authors": [
        "M.-T Luong",
        "H Pham",
        "C D Manning"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A N Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G E Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "",
      "venue": "Improving language understanding by generative pre-training",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Self-supervised learning: Generative or contrastive",
      "authors": [
        "X Liu",
        "F Zhang",
        "Z Hou",
        "L Mian",
        "Z Wang",
        "J Zhang",
        "J Tang"
      ],
      "year": "2021",
      "venue": "IEEE transactions on knowledge and data engineering",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "A survey of self-supervised learning from multiple perspectives: Algorithms, theory, applications and future trends",
      "authors": [
        "J Gui",
        "T Chen",
        "Q Cao",
        "Z Sun",
        "H Luo",
        "D Tao"
      ],
      "year": "2023",
      "venue": "A survey of self-supervised learning from multiple perspectives: Algorithms, theory, applications and future trends",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R R Salakhutdinov",
        "Q V Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Electra: Pretraining text encoders as discriminators rather than generators",
      "authors": [
        "K Clark",
        "M.-T Luong",
        "Q V Le",
        "C D Manning"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Albert: A lite bert for self-supervised learning of language representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman",
        "K Gimpel",
        "P Sharma",
        "R Soricut"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing",
      "authors": [
        "P He",
        "J Gao",
        "W Chen"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Deberta: Decoding-enhanced bert with disentangled attention",
      "authors": [
        "P He",
        "X Liu",
        "J Gao",
        "W Chen"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P J Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "A Chowdhery",
        "S Narang",
        "J Devlin",
        "M Bosma",
        "G Mishra",
        "A Roberts",
        "P Barham",
        "H W Chung",
        "C Sutton",
        "S Gehrmann"
      ],
      "year": "2022",
      "venue": "Palm: Scaling language modeling with pathways",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Training compute-optimal large language models",
      "authors": [
        "J Hoffmann",
        "S Borgeaud",
        "A Mensch",
        "E Buchatskaya",
        "T Cai",
        "E Rutherford",
        "D D L Casas",
        "L A Hendricks",
        "J Welbl",
        "A Clark"
      ],
      "year": "2022",
      "venue": "Training compute-optimal large language models",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Glam: Efficient scaling of language models with mixture-of-experts",
      "authors": [
        "N Du",
        "Y Huang",
        "A M Dai",
        "S Tong",
        "D Lepikhin",
        "Y Xu",
        "M Krikun",
        "Y Zhou",
        "A W Yu",
        "O Firat"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Lamda: Language models for dialog applications",
      "authors": [
        "R Thoppilan",
        "D De Freitas",
        "J Hall",
        "N Shazeer",
        "A Kulshreshtha",
        "H.-T Cheng",
        "A Jin",
        "T Bos",
        "L Baker",
        "Y Du"
      ],
      "year": "2022",
      "venue": "Lamda: Language models for dialog applications",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Scaling language models: Methods, analysis & insights from training gopher",
      "authors": [
        "J W Rae",
        "S Borgeaud",
        "T Cai",
        "K Millican",
        "J Hoffmann",
        "F Song",
        "J Aslanides",
        "S Henderson",
        "R Ring",
        "S Young"
      ],
      "year": "2021",
      "venue": "Scaling language models: Methods, analysis & insights from training gopher",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model",
      "authors": [
        "S Smith",
        "M Patwary",
        "B Norick",
        "P Legresley",
        "S Rajbhandari",
        "J Casper",
        "Z Liu",
        "S Prabhumoye",
        "G Zerveas",
        "V Korthikanti"
      ],
      "year": "2022",
      "venue": "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Bloom: A 176b-parameter open-access multilingual language model",
      "authors": [
        "T L Scao",
        "A Fan",
        "C Akiki",
        "E Pavlick",
        "S Ilić",
        "D Hesslow",
        "R Castagné",
        "A S Luccioni",
        "F Yvon",
        "M Gallé"
      ],
      "year": "2022",
      "venue": "Bloom: A 176b-parameter open-access multilingual language model",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Galactica: A large language model for science",
      "authors": [
        "R Taylor",
        "M Kardas",
        "G Cucurull",
        "T Scialom",
        "A Hartshorn",
        "E Saravia",
        "A Poulton",
        "V Kerkez",
        "R Stojnic"
      ],
      "year": "2022",
      "venue": "Galactica: A large language model for science",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Opt: Open pre-trained transformer language models",
      "authors": [
        "S Zhang",
        "S Roller",
        "N Goyal",
        "M Artetxe",
        "M Chen",
        "S Chen",
        "C Dewan",
        "M Diab",
        "X Li",
        "X V Lin"
      ],
      "year": "2022",
      "venue": "Opt: Open pre-trained transformer language models",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "N Bashlykov",
        "S Batra",
        "P Bhargava",
        "S Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "authors": [
        "S Bubeck",
        "V Chandrasekaran",
        "R Eldan",
        "J Gehrke",
        "E Horvitz",
        "E Kamar",
        "P Lee",
        "Y T Lee",
        "Y Li",
        "S Lundberg"
      ],
      "year": "2023",
      "venue": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "A survey of large language models",
      "authors": [
        "W X Zhao",
        "K Zhou",
        "J Li",
        "T Tang",
        "X Wang",
        "Y Hou",
        "Y Min",
        "B Zhang",
        "J Zhang",
        "Z Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "A survey for in-context learning",
      "authors": [
        "Q Dong",
        "L Li",
        "D Dai",
        "C Zheng",
        "Z Wu",
        "B Chang",
        "X Sun",
        "J Xu",
        "Z Sui"
      ],
      "year": "2022",
      "venue": "A survey for in-context learning",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "A survey on evaluation of large language models",
      "authors": [
        "Y Chang",
        "X Wang",
        "J Wang",
        "Y Wu",
        "K Zhu",
        "H Chen",
        "L Yang",
        "X Yi",
        "C Wang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "A survey on evaluation of large language models",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Through the lens of core competency: Survey on evaluation of large language models",
      "authors": [
        "Z Zhuang",
        "Q Chen",
        "L Ma",
        "M Li",
        "Y Han",
        "Y Qian",
        "H Bai",
        "Z Feng",
        "W Zhang",
        "T Liu"
      ],
      "year": "2023",
      "venue": "Through the lens of core competency: Survey on evaluation of large language models",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Aligning large language models with human: A survey",
      "authors": [
        "Y Wang",
        "W Zhong",
        "L Li",
        "F Mi",
        "X Zeng",
        "W Huang",
        "L Shang",
        "X Jiang",
        "Q Liu"
      ],
      "year": "2023",
      "venue": "Aligning large language models with human: A survey",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Trustworthy llms: a survey and guideline for evaluating large language models' alignment",
      "authors": [
        "Y Liu",
        "Y Yao",
        "J.-F Ton",
        "X Zhang",
        "R G H Cheng",
        "Y Klochkov",
        "M F Taufiq",
        "H Li"
      ],
      "year": "2023",
      "venue": "Trustworthy llms: a survey and guideline for evaluating large language models' alignment",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "A survey of safety and trustworthiness of large language models through the lens of verification and validation",
      "authors": [
        "X Huang",
        "W Ruan",
        "W Huang",
        "G Jin",
        "Y Dong",
        "C Wu",
        "S Bensalem",
        "R Mu",
        "Y Qi",
        "X Zhao"
      ],
      "year": "2023",
      "venue": "A survey of safety and trustworthiness of large language models through the lens of verification and validation",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Towards reasoning in large language models: A survey",
      "authors": [
        "J Huang",
        "K C",
        "-C Chang"
      ],
      "year": "2022",
      "venue": "Towards reasoning in large language models: A survey",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Challenges and applications of large language models",
      "authors": [
        "J Kaddour",
        "J Harris",
        "M Mozes",
        "H Bradley",
        "R Raileanu",
        "R Mchardy"
      ],
      "year": "2023",
      "venue": "Challenges and applications of large language models",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "A survey on model compression for large language models",
      "authors": [
        "X Zhu",
        "J Li",
        "Y Liu",
        "C Ma",
        "W Wang"
      ],
      "year": "2023",
      "venue": "A survey on model compression for large language models",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "A survey on multimodal large language models",
      "authors": [
        "S Yin",
        "C Fu",
        "S Zhao",
        "K Li",
        "X Sun",
        "T Xu",
        "E Chen"
      ],
      "year": "2023",
      "venue": "A survey on multimodal large language models",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Recent trends in deep learning based natural language processing",
      "authors": [
        "T Young",
        "D Hazarika",
        "S Poria",
        "E Cambria"
      ],
      "year": "2018",
      "venue": "ieee Computational intelligenCe magazine",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merrienboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "A comprehensive survey on transfer learning",
      "authors": [
        "F Zhuang",
        "Z Qi",
        "K Duan",
        "D Xi",
        "Y Zhu",
        "H Zhu",
        "H Xiong",
        "Q He"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "A survey of the usages of deep learning for natural language processing",
      "authors": [
        "D W Otter",
        "J R Medina",
        "J K Kalita"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Pre-trained models: Past, present and future",
      "authors": [
        "X Han",
        "Z Zhang",
        "N Ding",
        "Y Gu",
        "X Liu",
        "Y Huo",
        "J Qiu",
        "Y Yao",
        "A Zhang",
        "L Zhang"
      ],
      "year": "2021",
      "venue": "AI Open",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "A survey on transfer learning",
      "authors": [
        "S J Pan",
        "Q Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on knowledge and data engineering",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "authors": [
        "J Blitzer",
        "M Dredze",
        "F Pereira"
      ],
      "year": "2007",
      "venue": "Proceedings of the 45th annual meeting of the association of computational linguistics",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "A survey on semi-supervised learning",
      "authors": [
        "J E Van Engelen",
        "H H Hoos"
      ],
      "year": "2020",
      "venue": "Machine learning",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "A survey on multi-task learning",
      "authors": [
        "Y Zhang",
        "Q Yang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "Unsupervised learning of sentence embeddings using compositional n-gram features",
      "authors": [
        "M Pagliardini",
        "P Gupta",
        "M Jaggi"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "Deep contextualized word representations",
      "authors": [
        "M E Peters",
        "M Neumann",
        "M Iyyer",
        "M Gardner",
        "C Clark",
        "K Lee",
        "L Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "Palm 2 technical report",
      "authors": [
        "R Anil",
        "A M Dai",
        "O Firat",
        "M Johnson",
        "D Lepikhin",
        "A Passos",
        "S Shakeri",
        "E Taropa",
        "P Bailey",
        "Z Chen"
      ],
      "year": "2023",
      "venue": "Palm 2 technical report",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "A survey of transformers",
      "authors": [
        "T Lin",
        "Y Wang",
        "X Liu",
        "X Qiu"
      ],
      "year": "2022",
      "venue": "AI Open",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "Pre-trained models for natural language processing: A survey",
      "authors": [
        "X Qiu",
        "T Sun",
        "Y Xu",
        "Y Shao",
        "N Dai",
        "X Huang"
      ],
      "year": "2020",
      "venue": "Science China Technological Sciences",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
      "authors": [
        "J Zhang",
        "Y Zhao",
        "M Saleh",
        "P Liu"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "A primer on pretrained multilingual language models",
      "authors": [
        "S Doddapaneni",
        "G Ramesh",
        "M M Khapra",
        "A Kunchukuttan",
        "P Kumar"
      ],
      "year": "2021",
      "venue": "A primer on pretrained multilingual language models",
      "doi": ""
    },
    {
      "id": "b73",
      "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
      "authors": [
        "L Xue",
        "N Constant",
        "A Roberts",
        "M Kale",
        "R Al-Rfou",
        "A Siddhant",
        "A Barua",
        "C Raffel"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "Multilingual denoising pretraining for neural machine translation",
      "authors": [
        "Y Liu",
        "J Gu",
        "N Goyal",
        "X Li",
        "S Edunov",
        "M Ghazvininejad",
        "M Lewis",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "Indicnlpsuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for indian languages",
      "authors": [
        "D Kakwani",
        "A Kunchukuttan",
        "S Golla",
        "N Gokul",
        "A Bhattacharyya",
        "M M Khapra",
        "P Kumar"
      ],
      "year": "",
      "venue": "Indicnlpsuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for indian languages",
      "doi": ""
    },
    {
      "id": "b76",
      "title": "Cross-lingual language model pretraining",
      "authors": [
        "A Conneau",
        "G Lample"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b77",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "A Conneau",
        "K Khandelwal",
        "N Goyal",
        "V Chaudhary",
        "G Wenzek",
        "F Guzmán",
        "É Grave",
        "M Ott",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b78",
      "title": "Bertweet: A pre-trained language model for english tweets",
      "authors": [
        "D Q Nguyen",
        "T Vu",
        "A T Nguyen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": ""
    },
    {
      "id": "b79",
      "title": "Tweeteval: Unified benchmark and comparative evaluation for tweet classification",
      "authors": [
        "F Barbieri",
        "J Camacho-Collados",
        "L E Anke",
        "L Neves"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": ""
    },
    {
      "id": "b80",
      "title": "Finbert: A pretrained language model for financial communications",
      "authors": [
        "Y Yang",
        "M C S Uy",
        "A Huang"
      ],
      "year": "2020",
      "venue": "Finbert: A pretrained language model for financial communications",
      "doi": ""
    },
    {
      "id": "b81",
      "title": "Finbert: Financial sentiment analysis with pre-trained language models",
      "authors": [
        "D Araci"
      ],
      "year": "2019",
      "venue": "Finbert: Financial sentiment analysis with pre-trained language models",
      "doi": ""
    },
    {
      "id": "b82",
      "title": "Finbert: A pretrained financial language representation model for financial text mining",
      "authors": [
        "Z Liu",
        "D Huang",
        "K Huang",
        "Z Li",
        "J Zhao"
      ],
      "year": "2021",
      "venue": "Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence",
      "doi": ""
    },
    {
      "id": "b83",
      "title": "Legal-bert: The muppets straight out of law school",
      "authors": [
        "I Chalkidis",
        "M Fergadiotis",
        "P Malakasiotis",
        "N Aletras",
        "I Androutsopoulos"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": ""
    },
    {
      "id": "b84",
      "title": "A benchmark for lease contract review",
      "authors": [
        "S Leivaditi",
        "J Rossi",
        "E Kanoulas"
      ],
      "year": "2020",
      "venue": "A benchmark for lease contract review",
      "doi": ""
    },
    {
      "id": "b85",
      "title": "Codebert: A pre-trained model for programming and natural languages",
      "authors": [
        "Z Feng",
        "D Guo",
        "D Tang",
        "N Duan",
        "X Feng",
        "M Gong",
        "L Shou",
        "B Qin",
        "T Liu",
        "D Jiang"
      ],
      "year": "",
      "venue": "Codebert: A pre-trained model for programming and natural languages",
      "doi": ""
    },
    {
      "id": "b86",
      "title": "Codet5: Identifieraware unified pre-trained encoder-decoder models for code understanding and generation",
      "authors": [
        "Y Wang",
        "W Wang",
        "S Joty",
        "S C Hoi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b87",
      "title": "Codet5+: Open code large language models for code understanding and generation",
      "authors": [
        "Y Wang",
        "H Le",
        "A D Gotmare",
        "N D Bui",
        "J Li",
        "S C Hoi"
      ],
      "year": "2023",
      "venue": "Codet5+: Open code large language models for code understanding and generation",
      "doi": ""
    },
    {
      "id": "b88",
      "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "authors": [
        "J Lee",
        "W Yoon",
        "S Kim",
        "D Kim",
        "S Kim",
        "C H So",
        "J Kang"
      ],
      "year": "2020",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b89",
      "title": "Domain-specific language model pretraining for biomedical natural language processing",
      "authors": [
        "Y Gu",
        "R Tinn",
        "H Cheng",
        "M Lucas",
        "N Usuyama",
        "X Liu",
        "T Naumann",
        "J Gao",
        "H Poon"
      ],
      "year": "2020",
      "venue": "Domain-specific language model pretraining for biomedical natural language processing",
      "doi": ""
    },
    {
      "id": "b90",
      "title": "Bioelectra: pretrained biomedical text encoder using discriminators",
      "authors": [
        "K Kanakarajan",
        "B Kundumani",
        "M Sankarasubbu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 20th Workshop on Biomedical Language Processing",
      "doi": ""
    },
    {
      "id": "b91",
      "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh",
        "L Debut",
        "J Chaumond",
        "T Wolf"
      ],
      "year": "2019",
      "venue": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "doi": ""
    },
    {
      "id": "b92",
      "title": "Tinybert: Distilling bert for natural language understanding",
      "authors": [
        "X Jiao",
        "Y Yin",
        "L Shang",
        "X Jiang",
        "X Chen",
        "L Li",
        "F Wang",
        "Q Liu"
      ],
      "year": "",
      "venue": "Tinybert: Distilling bert for natural language understanding",
      "doi": ""
    },
    {
      "id": "b93",
      "title": "Mobilebert: a compact task-agnostic bert for resource-limited devices",
      "authors": [
        "Z Sun",
        "H Yu",
        "X Song",
        "R Liu",
        "Y Yang",
        "D Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b94",
      "title": "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers",
      "authors": [
        "W Wang",
        "F Wei",
        "L Dong",
        "H Bao",
        "N Yang",
        "M Zhou"
      ],
      "year": "2020",
      "venue": "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers",
      "doi": ""
    },
    {
      "id": "b95",
      "title": "Longformer: The longdocument transformer",
      "authors": [
        "I Beltagy",
        "M E Peters",
        "A Cohan"
      ],
      "year": "2020",
      "venue": "Longformer: The longdocument transformer",
      "doi": ""
    },
    {
      "id": "b96",
      "title": "Big bird: Transformers for longer sequences",
      "authors": [
        "M Zaheer",
        "G Guruganesh",
        "K A Dubey",
        "J Ainslie",
        "C Alberti",
        "S Ontanon",
        "P Pham",
        "A Ravula",
        "Q Wang",
        "L Yang"
      ],
      "year": "2020",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b97",
      "title": "Selfalignment pretraining for biomedical entity representations",
      "authors": [
        "F Liu",
        "E Shareghi",
        "Z Meng",
        "M Basaldella",
        "N Collier"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter",
      "doi": ""
    },
    {
      "id": "b98",
      "title": "Umlsbert: Clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathesaurus",
      "authors": [
        "G Michalopoulos",
        "Y Wang",
        "H Kaka",
        "H Chen",
        "A Wong"
      ],
      "year": "2020",
      "venue": "Umlsbert: Clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathesaurus",
      "doi": ""
    },
    {
      "id": "b99",
      "title": "Artificial general intelligence: concept, state of the art, and future prospects",
      "authors": [
        "B Goertzel"
      ],
      "year": "2014",
      "venue": "Journal of Artificial General Intelligence",
      "doi": ""
    },
    {
      "id": "b100",
      "title": "Emergent abilities of large language models",
      "authors": [
        "J Wei",
        "Y Tay",
        "R Bommasani",
        "C Raffel",
        "B Zoph",
        "S Borgeaud",
        "D Yogatama",
        "M Bosma",
        "D Zhou",
        "D Metzler"
      ],
      "year": "2022",
      "venue": "Transactions on Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b101",
      "title": "Are emergent abilities of large language models a mirage?",
      "authors": [
        "R Schaeffer",
        "B Miranda",
        "S Koyejo"
      ],
      "year": "2023",
      "venue": "Are emergent abilities of large language models a mirage?",
      "doi": ""
    },
    {
      "id": "b102",
      "title": "Evaluating large language models trained on code",
      "authors": [
        "M Chen",
        "J Tworek",
        "H Jun",
        "Q Yuan",
        "H P D O Pinto",
        "J Kaplan",
        "H Edwards",
        "Y Burda",
        "N Joseph",
        "G Brockman"
      ],
      "year": "2021",
      "venue": "Evaluating large language models trained on code",
      "doi": ""
    },
    {
      "id": "b103",
      "title": "Competition-level code generation with alphacode",
      "authors": [
        "Y Li",
        "D Choi",
        "J Chung",
        "N Kushman",
        "J Schrittwieser",
        "R Leblond",
        "T Eccles",
        "J Keeling",
        "F Gimeno",
        "A Lago"
      ],
      "year": "2022",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b104",
      "title": "Improving alignment of dialogue agents via targeted human judgements",
      "authors": [
        "A Glaese",
        "N Mcaleese",
        "M Trebacz",
        "J Aslanides",
        "V Firoiu",
        "T Ewalds",
        "M Rauh",
        "L Weidinger",
        "M Chadwick",
        "P Thacker"
      ],
      "year": "2022",
      "venue": "Improving alignment of dialogue agents via targeted human judgements",
      "doi": ""
    },
    {
      "id": "b105",
      "title": "Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation",
      "authors": [
        "S Wang",
        "Y Sun",
        "Y Xiang",
        "Z Wu",
        "S Ding",
        "W Gong",
        "S Feng",
        "J Shang",
        "Y Zhao",
        "C Pang"
      ],
      "year": "2021",
      "venue": "Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation",
      "doi": ""
    },
    {
      "id": "b106",
      "title": "Jurassic-1: Technical details and evaluation",
      "authors": [
        "O Lieber",
        "O Sharir",
        "B Lenz",
        "Y Shoham"
      ],
      "year": "2021",
      "venue": "White Paper. AI21 Labs",
      "doi": ""
    },
    {
      "id": "b107",
      "title": "Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model",
      "authors": [
        "S Soltan",
        "S Ananthakrishnan",
        "J Fitzgerald",
        "R Gupta",
        "W Hamza",
        "H Khan",
        "C Peris",
        "S Rawls",
        "A Rosenbaum",
        "A Rumshisky"
      ],
      "year": "2022",
      "venue": "Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model",
      "doi": ""
    },
    {
      "id": "b108",
      "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "authors": [
        "S Iyer",
        "X V Lin",
        "R Pasunuru",
        "T Mihaylov",
        "D Simig",
        "P Yu",
        "K Shuster",
        "T Wang",
        "Q Liu",
        "P S Koura"
      ],
      "year": "2022",
      "venue": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "doi": ""
    },
    {
      "id": "b109",
      "title": "Crosslingual generalization through multitask finetuning",
      "authors": [
        "N Muennighoff",
        "T Wang",
        "L Sutawika",
        "A Roberts",
        "S Biderman",
        "T L Scao",
        "M S Bari",
        "S Shen",
        "Z.-X Yong",
        "H Schoelkopf"
      ],
      "year": "2022",
      "venue": "Crosslingual generalization through multitask finetuning",
      "doi": ""
    },
    {
      "id": "b110",
      "title": "Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models",
      "authors": [
        "N Sengupta",
        "S K Sahu",
        "B Jia",
        "S Katipomu",
        "H Li",
        "F Koto",
        "O M Afzal",
        "S Kamboj",
        "O Pandit",
        "R"
      ],
      "year": "2023",
      "venue": "Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models",
      "doi": ""
    },
    {
      "id": "b111",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "authors": [
        "A Zeng",
        "X Liu",
        "Z Du",
        "Z Wang",
        "H Lai",
        "M Ding",
        "Z Yang",
        "Y Xu",
        "W Zheng",
        "X Xia"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b112",
      "title": "Flm-101b: An open llm and how to train it with 100 k budget",
      "authors": [
        "X Li",
        "Y Yao",
        "X Jiang",
        "X Fang",
        "X Meng",
        "S Fan",
        "P Han",
        "J Li",
        "L Du",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Flm-101b: An open llm and how to train it with 100 k budget",
      "doi": ""
    },
    {
      "id": "b113",
      "title": "Fingpt: Open-source financial large language models",
      "authors": [
        "H Yang",
        "X.-Y Liu",
        "C D Wang"
      ],
      "year": "2023",
      "venue": "Fingpt: Open-source financial large language models",
      "doi": ""
    },
    {
      "id": "b114",
      "title": "Bloomberggpt: A large language model for finance",
      "authors": [
        "S Wu",
        "O Irsoy",
        "S Lu",
        "V Dabravolski",
        "M Dredze",
        "S Gehrmann",
        "P Kambadur",
        "D Rosenberg",
        "G Mann"
      ],
      "year": "2023",
      "venue": "Bloomberggpt: A large language model for finance",
      "doi": ""
    },
    {
      "id": "b115",
      "title": "Large language models encode clinical knowledge",
      "authors": [
        "K Singhal",
        "S Azizi",
        "T Tu",
        "S S Mahdavi",
        "J Wei",
        "H W Chung",
        "N Scales",
        "A Tanwani",
        "H Cole-Lewis",
        "S Pfohl"
      ],
      "year": "2023",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b116",
      "title": "Towards expertlevel medical question answering with large language models",
      "authors": [
        "K Singhal",
        "T Tu",
        "J Gottweis",
        "R Sayres",
        "E Wulczyn",
        "L Hou",
        "K Clark",
        "S Pfohl",
        "H Cole-Lewis",
        "D Neal"
      ],
      "year": "2023",
      "venue": "Towards expertlevel medical question answering with large language models",
      "doi": ""
    },
    {
      "id": "b117",
      "title": "Starcoder: may the source be with you!",
      "authors": [
        "R Li",
        "L B Allal",
        "Y Zi",
        "N Muennighoff",
        "D Kocetkov",
        "C Mou",
        "M Marone",
        "C Akiki",
        "J Li",
        "J Chim"
      ],
      "year": "2023",
      "venue": "Starcoder: may the source be with you!",
      "doi": ""
    },
    {
      "id": "b118",
      "title": "Code llama: Open foundation models for code",
      "authors": [
        "B Rozière",
        "J Gehring",
        "F Gloeckle",
        "S Sootla",
        "I Gat",
        "X E Tan",
        "Y Adi",
        "J Liu",
        "T Remez",
        "J Rapin"
      ],
      "year": "2023",
      "venue": "Code llama: Open foundation models for code",
      "doi": ""
    },
    {
      "id": "b119",
      "title": "Codegen: An open large language model for code with multi-turn program synthesis",
      "authors": [
        "E Nijkamp",
        "B Pang",
        "H Hayashi",
        "L Tu",
        "H Wang",
        "Y Zhou",
        "S Savarese",
        "C Xiong"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b120",
      "title": "Codegen2: Lessons for training llms on programming and natural languages",
      "authors": [
        "E Nijkamp",
        "H Hayashi",
        "C Xiong",
        "S Savarese",
        "Y Zhou"
      ],
      "year": "2023",
      "venue": "Codegen2: Lessons for training llms on programming and natural languages",
      "doi": ""
    },
    {
      "id": "b121",
      "title": "Learning to generate reviews and discovering sentiment",
      "authors": [
        "A Radford",
        "R Jozefowicz",
        "I Sutskever"
      ],
      "year": "2017",
      "venue": "Learning to generate reviews and discovering sentiment",
      "doi": ""
    },
    {
      "id": "b122",
      "title": "Semi-supervised sequence learning",
      "authors": [
        "A M Dai",
        "Q V Le"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b123",
      "title": "Universal language model fine-tuning for text classification",
      "authors": [
        "J Howard",
        "S Ruder"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b124",
      "title": "Investigating chain-of-thought with chatgpt for stance detection on social media",
      "authors": [
        "B Zhang",
        "X Fu",
        "D Ding",
        "H Huang",
        "Y Li",
        "L Jing"
      ],
      "year": "2023",
      "venue": "Investigating chain-of-thought with chatgpt for stance detection on social media",
      "doi": ""
    },
    {
      "id": "b125",
      "title": "Evaluation of chatgpt for nlp-based mental health applications",
      "authors": [
        "B Lamichhane"
      ],
      "year": "2023",
      "venue": "Evaluation of chatgpt for nlp-based mental health applications",
      "doi": ""
    },
    {
      "id": "b126",
      "title": "On the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis",
      "authors": [
        "K Yang",
        "S Ji",
        "T Zhang",
        "Q Xie",
        "S Ananiadou"
      ],
      "year": "2023",
      "venue": "On the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis",
      "doi": ""
    },
    {
      "id": "b127",
      "title": "Is chatgpt a good sentiment analyzer? a preliminary study",
      "authors": [
        "Z Wang",
        "Q Xie",
        "Z Ding",
        "Y Feng",
        "R Xia"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good sentiment analyzer? a preliminary study",
      "doi": ""
    },
    {
      "id": "b128",
      "title": "Can chatgpt forecast stock price movements? return predictability and large language models",
      "authors": [
        "A Lopez-Lira",
        "Y Tang"
      ],
      "year": "2023",
      "venue": "Can chatgpt forecast stock price movements? return predictability and large language models",
      "doi": ""
    },
    {
      "id": "b129",
      "title": "Can large language models transform computational social science?",
      "authors": [
        "C Ziems",
        "W Held",
        "O Shaikh",
        "J Chen",
        "Z Zhang",
        "D Yang"
      ],
      "year": "2023",
      "venue": "Can large language models transform computational social science?",
      "doi": ""
    },
    {
      "id": "b130",
      "title": "Chatgpt: Beginning of an end of manual annotation? use case of automatic genre identification",
      "authors": [
        "T Kuzman",
        "N Ljubešić",
        "I Mozetič"
      ],
      "year": "2023",
      "venue": "Chatgpt: Beginning of an end of manual annotation? use case of automatic genre identification",
      "doi": ""
    },
    {
      "id": "b131",
      "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "authors": [
        "Y Bang",
        "S Cahyawijaya",
        "N Lee",
        "W Dai",
        "D Su",
        "B Wilie",
        "H Lovenia",
        "Z Ji",
        "T Yu",
        "W Chung"
      ],
      "year": "2023",
      "venue": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "doi": ""
    },
    {
      "id": "b132",
      "title": "Chatgpt: Jack of all trades, master of none",
      "authors": [
        "J Koco",
        "I Cichecki",
        "O Kaszyca",
        "M Kochanek",
        "D Szydło",
        "J Baran",
        "J Bielaniewicz",
        "M Gruza",
        "A Janz",
        "K Kanclerz"
      ],
      "year": "2023",
      "venue": "Chatgpt: Jack of all trades, master of none",
      "doi": ""
    },
    {
      "id": "b133",
      "title": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
      "authors": [
        "Q Zhong",
        "L Ding",
        "J Liu",
        "B Du",
        "D Tao"
      ],
      "year": "2023",
      "venue": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
      "doi": ""
    },
    {
      "id": "b134",
      "title": "A comprehensive capability analysis of gpt-3 and gpt-3.5 series models",
      "authors": [
        "J Ye",
        "X Chen",
        "N Xu",
        "C Zu",
        "Z Shao",
        "S Liu",
        "Y Cui",
        "Z Zhou",
        "C Gong",
        "Y Shen"
      ],
      "year": "2023",
      "venue": "A comprehensive capability analysis of gpt-3 and gpt-3.5 series models",
      "doi": ""
    },
    {
      "id": "b135",
      "title": "Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? an examination on several typical tasks",
      "authors": [
        "X Li",
        "X Zhu",
        "Z Ma",
        "X Liu",
        "S Shah"
      ],
      "year": "2023",
      "venue": "Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? an examination on several typical tasks",
      "doi": ""
    },
    {
      "id": "b136",
      "title": "Exploring the trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology nli task",
      "authors": [
        "Z Wu",
        "L Zhang",
        "C Cao",
        "X Yu",
        "H Dai",
        "C Ma",
        "Z Liu",
        "L Zhao",
        "G Li",
        "W Liu"
      ],
      "year": "2023",
      "venue": "Exploring the trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology nli task",
      "doi": ""
    },
    {
      "id": "b137",
      "title": "Are large language models ready for healthcare? a comparative study on clinical language understanding",
      "authors": [
        "Y Wang",
        "Y Zhao",
        "L Petzold"
      ],
      "year": "2023",
      "venue": "Are large language models ready for healthcare? a comparative study on clinical language understanding",
      "doi": ""
    },
    {
      "id": "b138",
      "title": "Detecting hate speech with gpt-3",
      "authors": [
        "K.-L Chiu",
        "A Collins",
        "R Alexander"
      ],
      "year": "2021",
      "venue": "Detecting hate speech with gpt-3",
      "doi": ""
    },
    {
      "id": "b139",
      "title": "Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech",
      "authors": [
        "F Huang",
        "H Kwak",
        "J An"
      ],
      "year": "2023",
      "venue": "Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech",
      "doi": ""
    },
    {
      "id": "b140",
      "title": "Evaluation of chatgpt family of models for biomedical reasoning and classification",
      "authors": [
        "S Chen",
        "Y Li",
        "S Lu",
        "H Van",
        "H J Aerts",
        "G K Savova",
        "D S Bitterman"
      ],
      "year": "2023",
      "venue": "Evaluation of chatgpt family of models for biomedical reasoning and classification",
      "doi": ""
    },
    {
      "id": "b141",
      "title": "Will affective computing emerge from foundation models and general ai? a first evaluation on chatgpt",
      "authors": [
        "M M Amin",
        "E Cambria",
        "B W Schuller"
      ],
      "year": "",
      "venue": "IEEE Intelligent Systems",
      "doi": ""
    },
    {
      "id": "b142",
      "title": "Exploring zero and few-shot techniques for intent classification",
      "authors": [
        "S Parikh",
        "Q Vohra",
        "P Tumbade",
        "M Tiwari"
      ],
      "year": "2023",
      "venue": "Exploring zero and few-shot techniques for intent classification",
      "doi": ""
    },
    {
      "id": "b143",
      "title": "Text classification via large language models",
      "authors": [
        "X Sun",
        "X Li",
        "J Li",
        "F Wu",
        "S Guo",
        "T Zhang",
        "G Wang"
      ],
      "year": "2023",
      "venue": "Text classification via large language models",
      "doi": ""
    },
    {
      "id": "b144",
      "title": "A survey on text classification: From traditional to deep learning",
      "authors": [
        "Q Li",
        "H Peng",
        "J Li",
        "C Xia",
        "R Yang",
        "L Sun",
        "P S Yu",
        "L He"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "doi": ""
    },
    {
      "id": "b145",
      "title": "Yes but.. can chatgpt identify entities in historical documents?",
      "authors": [
        "C.-E González-Gallardo",
        "E Boros",
        "N Girdhar",
        "A Hamdi",
        "J G Moreno",
        "A Doucet"
      ],
      "year": "2023",
      "venue": "Yes but.. can chatgpt identify entities in historical documents?",
      "doi": ""
    },
    {
      "id": "b146",
      "title": "Zero-shot clinical entity recognition using chatgpt",
      "authors": [
        "Y Hu",
        "I Ameer",
        "X Zuo",
        "X Peng",
        "Y Zhou",
        "Z Li",
        "Y Li",
        "J Li",
        "X Jiang",
        "H Xu"
      ],
      "year": "2023",
      "venue": "Zero-shot clinical entity recognition using chatgpt",
      "doi": ""
    },
    {
      "id": "b147",
      "title": "Zero-shot information extraction via chatting with chatgpt",
      "authors": [
        "X Wei",
        "X Cui",
        "N Cheng",
        "X Wang",
        "X Zhang",
        "S Huang",
        "P Xie",
        "J Xu",
        "Y Chen",
        "M Zhang"
      ],
      "year": "2023",
      "venue": "Zero-shot information extraction via chatting with chatgpt",
      "doi": ""
    },
    {
      "id": "b148",
      "title": "Thinking about gpt-3 in-context learning for biomedical ie? think again",
      "authors": [
        "B J Gutiérrez",
        "N Mcneal",
        "C Washington",
        "Y Chen",
        "L Li",
        "H Sun",
        "Y Su"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022",
      "doi": ""
    },
    {
      "id": "b149",
      "title": "Exploring the feasibility of chatgpt for event extraction",
      "authors": [
        "J Gao",
        "H Zhao",
        "C Yu",
        "R Xu"
      ],
      "year": "2023",
      "venue": "Exploring the feasibility of chatgpt for event extraction",
      "doi": ""
    },
    {
      "id": "b150",
      "title": "Evaluation of gpt and bert-based models on identifying protein-protein interactions in biomedical text",
      "authors": [
        "H Rehana",
        "N B",
        "M Basmaci",
        "Y He",
        "A Özg",
        "J Hur"
      ],
      "year": "2023",
      "venue": "Evaluation of gpt and bert-based models on identifying protein-protein interactions in biomedical text",
      "doi": ""
    },
    {
      "id": "b151",
      "title": "Zero-shot temporal relation extraction with chatgpt",
      "authors": [
        "C Yuan",
        "Q Xie",
        "S Ananiadou"
      ],
      "year": "2023",
      "venue": "Zero-shot temporal relation extraction with chatgpt",
      "doi": ""
    },
    {
      "id": "b152",
      "title": "Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness",
      "authors": [
        "B Li",
        "G Fang",
        "Y Yang",
        "Q Wang",
        "W Ye",
        "W Zhao",
        "S Zhang"
      ],
      "year": "2023",
      "venue": "Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness",
      "doi": ""
    },
    {
      "id": "b153",
      "title": "Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations",
      "authors": [
        "C Chan",
        "J Cheng",
        "W Wang",
        "Y Jiang",
        "T Fang",
        "X Liu",
        "Y Song"
      ],
      "year": "2023",
      "venue": "Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations",
      "doi": ""
    },
    {
      "id": "b154",
      "title": "How to unleash the power of large language models for few-shot relation extraction?",
      "authors": [
        "X Xu",
        "Y Zhu",
        "X Wang",
        "N Zhang"
      ],
      "year": "2023",
      "venue": "How to unleash the power of large language models for few-shot relation extraction?",
      "doi": ""
    },
    {
      "id": "b155",
      "title": "Gpt-re: In-context learning for relation extraction using large language models",
      "authors": [
        "Z Wan",
        "F Cheng",
        "Z Mao",
        "Q Liu",
        "H Song",
        "J Li",
        "S Kurohashi"
      ],
      "year": "2023",
      "venue": "Gpt-re: In-context learning for relation extraction using large language models",
      "doi": ""
    },
    {
      "id": "b156",
      "title": "Is chatgpt a general-purpose natural language processing task solver?",
      "authors": [
        "C Qin",
        "A Zhang",
        "Z Zhang",
        "J Chen",
        "M Yasunaga",
        "D Yang"
      ],
      "year": "2023",
      "venue": "Is chatgpt a general-purpose natural language processing task solver?",
      "doi": ""
    },
    {
      "id": "b157",
      "title": "Large language model is not a good few-shot information extractor, but a good reranker for hard samples",
      "authors": [
        "Y Ma",
        "Y Cao",
        "Y Hong",
        "A Sun"
      ],
      "year": "2023",
      "venue": "Large language model is not a good few-shot information extractor, but a good reranker for hard samples",
      "doi": ""
    },
    {
      "id": "b158",
      "title": "Gpt-ner: Named entity recognition via large language models",
      "authors": [
        "S Wang",
        "X Sun",
        "X Li",
        "R Ouyang",
        "F Wu",
        "T Zhang",
        "J Li",
        "G Wang"
      ],
      "year": "2023",
      "venue": "Gpt-ner: Named entity recognition via large language models",
      "doi": ""
    },
    {
      "id": "b159",
      "title": "Heroes, villains, and victims, and gpt-3: Automated extraction of character roles without training data",
      "authors": [
        "D Stammbach",
        "M Antoniak",
        "E Ash"
      ],
      "year": "2022",
      "venue": "Proceedings of the 4th Workshop of Narrative Understanding",
      "doi": ""
    },
    {
      "id": "b160",
      "title": "Revisiting relation extraction in the era of large language models",
      "authors": [
        "S Wadhwa",
        "S Amir",
        "B C Wallace"
      ],
      "year": "2023",
      "venue": "Revisiting relation extraction in the era of large language models",
      "doi": ""
    },
    {
      "id": "b161",
      "title": "Codeie: Large code generation models are better few-shot information extractors",
      "authors": [
        "P Li",
        "T Sun",
        "Q Tang",
        "H Yan",
        "Y Wu",
        "X Huang",
        "X Qiu"
      ],
      "year": "2023",
      "venue": "Codeie: Large code generation models are better few-shot information extractors",
      "doi": ""
    },
    {
      "id": "b162",
      "title": "Aligning instruction tasks unlocks large language models as zero-shot relation extractors",
      "authors": [
        "K Zhang",
        "B J Gutiérrez",
        "Y Su"
      ],
      "year": "2023",
      "venue": "Aligning instruction tasks unlocks large language models as zero-shot relation extractors",
      "doi": ""
    },
    {
      "id": "b163",
      "title": "Unified structure generation for universal information extraction",
      "authors": [
        "Y Lu",
        "Q Liu",
        "D Dai",
        "X Xiao",
        "H Lin",
        "X Han",
        "L Sun",
        "H Wu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b164",
      "title": "Learning from sibling mentions with scalable graph inference in fine-grained entity typing",
      "authors": [
        "Y Chen",
        "J Cheng",
        "H Jiang",
        "L Liu",
        "H Zhang",
        "S Shi",
        "R Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b165",
      "title": "Container: Few-shot named entity recognition via contrastive learning",
      "authors": [
        "S S S Das",
        "A Katiyar",
        "R J Passonneau",
        "R Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b166",
      "title": "Enriching pre-trained language model with entity information for relation classification",
      "authors": [
        "S Wu",
        "Y He"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th ACM international conference on information and knowledge management",
      "doi": ""
    },
    {
      "id": "b167",
      "title": "Packed levitated marker for entity and relation extraction",
      "authors": [
        "D Ye",
        "Y Lin",
        "P Li",
        "M Sun"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b168",
      "title": "Knowledgeenhanced self-supervised prototypical network for few-shot event detection",
      "authors": [
        "K Zhao",
        "X Jin",
        "L Bai",
        "J Guo",
        "X Cheng"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022",
      "doi": ""
    },
    {
      "id": "b169",
      "title": "Prompt for extraction? paie: Prompting argument interaction for event argument extraction",
      "authors": [
        "Y Ma",
        "Z Wang",
        "Y Cao",
        "M Li",
        "M Chen",
        "K Wang",
        "J Shao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b170",
      "title": "Event extraction by answering (almost) natural questions",
      "authors": [
        "X Du",
        "C Cardie"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b171",
      "title": "Calibrate before use: Improving few-shot performance of language models",
      "authors": [
        "Z Zhao",
        "E Wallace",
        "S Feng",
        "D Klein",
        "S Singh"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b172",
      "title": "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks",
      "authors": [
        "Y Wang",
        "S Mishra",
        "P Alipoormolabashi",
        "Y Kordi",
        "A Mirzaei",
        "A Naik",
        "A Ashok",
        "A S Dhanasekaran",
        "A Arunkumar",
        "D Stap"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b173",
      "title": "Conversational question answering: A survey",
      "authors": [
        "M Zaib",
        "W E Zhang",
        "Q Z Sheng",
        "A Mahmood",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "Knowledge and Information Systems",
      "doi": ""
    },
    {
      "id": "b174",
      "title": "Improving graph-based random walks for complex question answering using syntactic, shallow semantic and extended string subsequence kernels",
      "authors": [
        "Y Chali",
        "S A Hasan",
        "S R Joty"
      ],
      "year": "2011",
      "venue": "Information Processing & Management",
      "doi": ""
    },
    {
      "id": "b175",
      "title": "Natural language processing advancements by deep learning: A survey",
      "authors": [
        "A Torfi",
        "R A Shirvani",
        "Y Keneshloo",
        "N Tavaf",
        "E A Fox"
      ],
      "year": "2020",
      "venue": "Natural language processing advancements by deep learning: A survey",
      "doi": ""
    },
    {
      "id": "b176",
      "title": "Evaluating gpt-3.5 and gpt-4 models on brazilian university admission exams",
      "authors": [
        "D Nunes",
        "R Primi",
        "R Pires",
        "R Lotufo",
        "R Nogueira"
      ],
      "year": "2023",
      "venue": "Evaluating gpt-3.5 and gpt-4 models on brazilian university admission exams",
      "doi": ""
    },
    {
      "id": "b177",
      "title": "Evaluation of chatgpt as a question answering system for answering complex questions",
      "authors": [
        "Y Tan",
        "D Min",
        "Y Li",
        "W Li",
        "N Hu",
        "Y Chen",
        "G Qi"
      ],
      "year": "2023",
      "venue": "Evaluation of chatgpt as a question answering system for answering complex questions",
      "doi": ""
    },
    {
      "id": "b178",
      "title": "An empirical study of gpt-3 for few-shot knowledge-based vqa",
      "authors": [
        "Z Yang",
        "Z Gan",
        "J Wang",
        "X Hu",
        "Y Lu",
        "Z Liu",
        "L Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b179",
      "title": "Towards zero-shot and few-shot table question answering using gpt-3",
      "authors": [
        "P Srivastava",
        "T Ganu",
        "S Guha"
      ],
      "year": "2022",
      "venue": "Towards zero-shot and few-shot table question answering using gpt-3",
      "doi": ""
    },
    {
      "id": "b180",
      "title": "Why does chatgpt fall short in answering questions faithfully?",
      "authors": [
        "S Zheng",
        "J Huang",
        "K C",
        "-C Chang"
      ],
      "year": "2023",
      "venue": "Why does chatgpt fall short in answering questions faithfully?",
      "doi": ""
    },
    {
      "id": "b181",
      "title": "Assessing the accuracy of responses by the language model chatgpt to questions regarding bariatric surgery",
      "authors": [
        "J S Samaan",
        "Y H Yeo",
        "N Rajeev",
        "L Hawley",
        "S Abel",
        "W H Ng",
        "N Srinivasan",
        "J Park",
        "M Burch",
        "R Watson"
      ],
      "year": "2023",
      "venue": "Obesity surgery",
      "doi": ""
    },
    {
      "id": "b182",
      "title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
      "authors": [
        "J Holmes",
        "Z Liu",
        "L Zhang",
        "Y Ding",
        "T T Sio",
        "L A Mcgee",
        "J B Ashman",
        "X Li",
        "T Liu",
        "J Shen"
      ],
      "year": "",
      "venue": "Frontiers in Oncology",
      "doi": ""
    },
    {
      "id": "b183",
      "title": "Chatgpt-a blessing or a curse for undergraduate computer science students and instructors?",
      "authors": [
        "I Joshi",
        "R Budhiraja",
        "H Dev",
        "J Kadia",
        "M O Ataullah",
        "S Mitra",
        "D Kumar",
        "H D Akolekar"
      ],
      "year": "2023",
      "venue": "Chatgpt-a blessing or a curse for undergraduate computer science students and instructors?",
      "doi": ""
    },
    {
      "id": "b184",
      "title": "Capabilities of gpt-4 on medical challenge problems",
      "authors": [
        "H Nori",
        "N King",
        "S M Mckinney",
        "D Carignan",
        "E Horvitz"
      ],
      "year": "2023",
      "venue": "Capabilities of gpt-4 on medical challenge problems",
      "doi": ""
    },
    {
      "id": "b185",
      "title": "Evaluation of ai chatbots for patientspecific ehr questions",
      "authors": [
        "A Hamidi",
        "K Roberts"
      ],
      "year": "2023",
      "venue": "Evaluation of ai chatbots for patientspecific ehr questions",
      "doi": ""
    },
    {
      "id": "b186",
      "title": "Large language models (gpt) struggle to answer multiple-choice questions about code",
      "authors": [
        "J Savelka",
        "A Agarwal",
        "C Bogart",
        "M Sakr"
      ],
      "year": "2023",
      "venue": "Large language models (gpt) struggle to answer multiple-choice questions about code",
      "doi": ""
    },
    {
      "id": "b187",
      "title": "Gpt takes the bar exam",
      "authors": [
        "M Bommarito",
        "D M Katz"
      ],
      "year": "2022",
      "venue": "Gpt takes the bar exam",
      "doi": ""
    },
    {
      "id": "b188",
      "title": "Visconde: Multi-document qa with gpt-3 and neural reranking",
      "authors": [
        "J Pereira",
        "R Fidalgo",
        "R Lotufo",
        "R Nogueira"
      ],
      "year": "2023",
      "venue": "European Conference on Information Retrieval",
      "doi": ""
    },
    {
      "id": "b189",
      "title": "Performance of chatgpt on the plastic surgery inservice training examination",
      "authors": [
        "R Gupta",
        "I Herzog",
        "J B Park",
        "J Weisberger",
        "P Firouzbakht",
        "V Ocon",
        "J Chao",
        "E S Lee",
        "B A Mailey"
      ],
      "year": "2023",
      "venue": "Aesthetic surgery journal",
      "doi": ""
    },
    {
      "id": "b190",
      "title": "Performance of generative pretrained transformer on the national medical licensing examination in japan",
      "authors": [
        "Y Tanaka",
        "T Nakata",
        "K Aiga",
        "T Etani",
        "R Muramatsu",
        "S Katagiri",
        "H Kawai",
        "F Higashino",
        "M Enomoto",
        "M Noda"
      ],
      "year": "2023",
      "venue": "medRxiv",
      "doi": ""
    },
    {
      "id": "b191",
      "title": "Leveraging large language models for multiple choice question answering",
      "authors": [
        "J Robinson",
        "D Wingate"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b192",
      "title": "Large language models need holistically thought in medical conversational qa",
      "authors": [
        "Y Weng",
        "B Li",
        "F Xia",
        "M Zhu",
        "B Sun",
        "S He",
        "K Liu",
        "J Zhao"
      ],
      "year": "2023",
      "venue": "Large language models need holistically thought in medical conversational qa",
      "doi": ""
    },
    {
      "id": "b193",
      "title": "Truthfulqa: Measuring how models mimic human falsehoods",
      "authors": [
        "S Lin",
        "J Hilton",
        "O Evans"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b194",
      "title": "Evaluating gpt-4 and chatgpt on japanese medical licensing examinations",
      "authors": [
        "J Kasai",
        "Y Kasai",
        "K Sakaguchi",
        "Y Yamada",
        "D Radev"
      ],
      "year": "2023",
      "venue": "Evaluating gpt-4 and chatgpt on japanese medical licensing examinations",
      "doi": ""
    },
    {
      "id": "b195",
      "title": "Linguistically informed chatgpt prompts to enhance japanese-chinese machine translation: A case study on attributive clauses",
      "authors": [
        "W Gu"
      ],
      "year": "2023",
      "venue": "Linguistically informed chatgpt prompts to enhance japanese-chinese machine translation: A case study on attributive clauses",
      "doi": ""
    },
    {
      "id": "b196",
      "title": "Towards making the most of chatgpt for machine translation",
      "authors": [
        "K Peng",
        "L Ding",
        "Q Zhong",
        "L Shen",
        "X Liu",
        "M Zhang",
        "Y Ouyang",
        "D Tao"
      ],
      "year": "2023",
      "venue": "Towards making the most of chatgpt for machine translation",
      "doi": ""
    },
    {
      "id": "b197",
      "title": "Is chatgpt a good translator? yes with gpt-4 as the engine",
      "authors": [
        "W Jiao",
        "W Wang",
        "J Huang",
        "X Wang",
        "Z Tu"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good translator? yes with gpt-4 as the engine",
      "doi": ""
    },
    {
      "id": "b198",
      "title": "How good are gpt models at machine translation? a comprehensive evaluation",
      "authors": [
        "A Hendy",
        "M Abdelrehim",
        "A Sharaf",
        "V Raunak",
        "M Gabr",
        "H Matsushita",
        "Y J Kim",
        "M Afify",
        "H H Awadalla"
      ],
      "year": "2023",
      "venue": "How good are gpt models at machine translation? a comprehensive evaluation",
      "doi": ""
    },
    {
      "id": "b199",
      "title": "How to design translation prompts for chatgpt: An empirical study",
      "authors": [
        "Y Gao",
        "R Wang",
        "F Hou"
      ],
      "year": "2023",
      "venue": "How to design translation prompts for chatgpt: An empirical study",
      "doi": ""
    },
    {
      "id": "b200",
      "title": "Document-level machine translation with large language models",
      "authors": [
        "L Wang",
        "C Lyu",
        "T Ji",
        "Z Zhang",
        "D Yu",
        "S Shi",
        "Z Tu"
      ],
      "year": "2023",
      "venue": "Document-level machine translation with large language models",
      "doi": ""
    },
    {
      "id": "b201",
      "title": "Multilingual machine translation with large language models: Empirical results and analysis",
      "authors": [
        "W Zhu",
        "H Liu",
        "Q Dong",
        "J Xu",
        "L Kong",
        "J Chen",
        "L Li",
        "S Huang"
      ],
      "year": "2023",
      "venue": "Multilingual machine translation with large language models: Empirical results and analysis",
      "doi": ""
    },
    {
      "id": "b202",
      "title": "New trends in machine translation using large language models: Case examples with chatgpt",
      "authors": [
        "C Lyu",
        "J Xu",
        "L Wang"
      ],
      "year": "2023",
      "venue": "New trends in machine translation using large language models: Case examples with chatgpt",
      "doi": ""
    },
    {
      "id": "b203",
      "title": "Large language models effectively leverage document-level context for literary translation, but critical errors persist",
      "authors": [
        "M Karpinska",
        "M Iyyer"
      ],
      "year": "2023",
      "venue": "Large language models effectively leverage document-level context for literary translation, but critical errors persist",
      "doi": ""
    },
    {
      "id": "b204",
      "title": "Adaptive machine translation with large language models",
      "authors": [
        "Y Moslem",
        "R Haque",
        "A Way"
      ],
      "year": "2023",
      "venue": "Adaptive machine translation with large language models",
      "doi": ""
    },
    {
      "id": "b205",
      "title": "Exploring human-like translation strategy with large language models",
      "authors": [
        "Z He",
        "T Liang",
        "W Jiao",
        "Z Zhang",
        "Y Yang",
        "R Wang",
        "Z Tu",
        "S Shi",
        "X Wang"
      ],
      "year": "2023",
      "venue": "Exploring human-like translation strategy with large language models",
      "doi": ""
    },
    {
      "id": "b206",
      "title": "Leveraging gpt-4 for automatic translation post-editing",
      "authors": [
        "V Raunak",
        "A Sharaf",
        "H H Awadallah",
        "A Menezes"
      ],
      "year": "2023",
      "venue": "Leveraging gpt-4 for automatic translation post-editing",
      "doi": ""
    },
    {
      "id": "b207",
      "title": "Do gpts produce less literal translations?",
      "authors": [
        "V Raunak",
        "A Menezes",
        "M Post",
        "H H Awadallah"
      ],
      "year": "2023",
      "venue": "Do gpts produce less literal translations?",
      "doi": ""
    },
    {
      "id": "b208",
      "title": "Neural machine translation: A review",
      "authors": [
        "F Stahlberg"
      ],
      "year": "2020",
      "venue": "Journal of Artificial Intelligence Research",
      "doi": ""
    },
    {
      "id": "b209",
      "title": "A survey of deep learning techniques for neural machine translation",
      "authors": [
        "S Yang",
        "Y Wang",
        "X Chu"
      ],
      "year": "2020",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b210",
      "title": "Neural machine translation: A review of methods, resources, and tools",
      "authors": [
        "Z Tan",
        "S Wang",
        "Z Yang",
        "G Chen",
        "X Huang",
        "M Sun",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "AI Open",
      "doi": ""
    },
    {
      "id": "b211",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "CoRR",
      "doi": ""
    },
    {
      "id": "b212",
      "title": "Multilingual translation with extensible multilingual pretraining and finetuning",
      "authors": [
        "Y Tang",
        "C Tran",
        "X Li",
        "P.-J Chen",
        "N Goyal",
        "V Chaudhary",
        "J Gu",
        "A Fan"
      ],
      "year": "2020",
      "venue": "Multilingual translation with extensible multilingual pretraining and finetuning",
      "doi": ""
    },
    {
      "id": "b213",
      "title": "Beyond english-centric multilingual machine translation",
      "authors": [
        "A Fan",
        "S Bhosale",
        "H Schwenk",
        "Z Ma",
        "A El-Kishky",
        "S Goyal",
        "M Baines",
        "O ¸elebi",
        "G Wenzek",
        "V Chaudhary",
        "N Goyal",
        "T Birch",
        "V Liptchinsky",
        "S Edunov",
        "E Grave",
        "M Auli",
        "A Joulin"
      ],
      "year": "2020",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b214",
      "title": "No language left behind: Scaling human-centered machine translation",
      "authors": [
        "M R Costa-Jussà",
        "J Cross",
        "O ¸elebi",
        "M Elbayad",
        "K Heafield",
        "K Heffernan",
        "E Kalbassi",
        "J Lam",
        "D Licht",
        "J Maillard"
      ],
      "year": "2022",
      "venue": "No language left behind: Scaling human-centered machine translation",
      "doi": ""
    },
    {
      "id": "b215",
      "title": "Chatgpt vs state-of-the-art models: A benchmarking study in keyphrase generation task",
      "authors": [
        "R Martínez-Cruz",
        "A J Ópez-L Ópez",
        "J Portela"
      ],
      "year": "2023",
      "venue": "Chatgpt vs state-of-the-art models: A benchmarking study in keyphrase generation task",
      "doi": ""
    },
    {
      "id": "b216",
      "title": "Is chatgpt a good keyphrase generator? a preliminary study",
      "authors": [
        "M Song",
        "H Jiang",
        "S Shi",
        "S Yao",
        "S Lu",
        "Y Feng",
        "H Liu",
        "L Jing"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good keyphrase generator? a preliminary study",
      "doi": ""
    },
    {
      "id": "b217",
      "title": "A preliminary evaluation of chatgpt for zero-shot dialogue understanding",
      "authors": [
        "W Pan",
        "Q Chen",
        "X Xu",
        "W Che",
        "L Qin"
      ],
      "year": "2023",
      "venue": "A preliminary evaluation of chatgpt for zero-shot dialogue understanding",
      "doi": ""
    },
    {
      "id": "b218",
      "title": "Is chatgpt equipped with emotional dialogue capabilities?",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu",
        "S Wang",
        "Y Tong",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Is chatgpt equipped with emotional dialogue capabilities?",
      "doi": ""
    },
    {
      "id": "b219",
      "title": "Medically aware gpt-3 as a data generator for medical dialogue summarization",
      "authors": [
        "B Chintagunta",
        "N Katariya",
        "X Amatriain",
        "A Kannan"
      ],
      "year": "2021",
      "venue": "Machine Learning for Healthcare Conference",
      "doi": ""
    },
    {
      "id": "b220",
      "title": "Prompt scoring system for dialogue summarization using gpt-3",
      "authors": [
        "G P Prodan",
        "E Pelican"
      ],
      "year": "2022",
      "venue": "ACM Transaction on Audio, Speech, and Language Processing",
      "doi": ""
    },
    {
      "id": "b221",
      "title": "Understanding the effectiveness of very large language models on dialog evaluation",
      "authors": [
        "J Huynh",
        "C Jiao",
        "P Gupta",
        "S Mehri",
        "P Bajaj",
        "V Chaudhary",
        "M Eskenazi"
      ],
      "year": "2023",
      "venue": "Understanding the effectiveness of very large language models on dialog evaluation",
      "doi": ""
    },
    {
      "id": "b222",
      "title": "Uncovering the potential of chatgpt for discourse analysis in dialogue: An empirical study",
      "authors": [
        "Y Fan",
        "F Jiang"
      ],
      "year": "2023",
      "venue": "Uncovering the potential of chatgpt for discourse analysis in dialogue: An empirical study",
      "doi": ""
    },
    {
      "id": "b223",
      "title": "Chain-of-thought prompting for responding to in-depth dialogue questions with llm",
      "authors": [
        "H Wang",
        "R Wang",
        "F Mi",
        "Z Wang",
        "R Xu",
        "K.-F Wong"
      ],
      "year": "2023",
      "venue": "Chain-of-thought prompting for responding to in-depth dialogue questions with llm",
      "doi": ""
    },
    {
      "id": "b224",
      "title": "An empirical study on neural keyphrase generation",
      "authors": [
        "R Meng",
        "X Yuan",
        "T Wang",
        "S Zhao",
        "A Trischler",
        "D He"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": ""
    },
    {
      "id": "b225",
      "title": "One size does not fit all: Generating and evaluating variable number of keyphrases",
      "authors": [
        "X Yuan",
        "T Wang",
        "R Meng",
        "K Thaker",
        "P Brusilovsky",
        "D He",
        "A Trischler"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b226",
      "title": "Learning rich representation of keyphrases from text",
      "authors": [
        "M Kulkarni",
        "D Mahata",
        "R Arora",
        "R Bhowmik"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2022",
      "doi": ""
    },
    {
      "id": "b227",
      "title": "A survey of available corpora for building data-driven dialogue systems: The journal version",
      "authors": [
        "I V Serban",
        "R Lowe",
        "P Henderson",
        "L Charlin",
        "J Pineau"
      ],
      "year": "2018",
      "venue": "Dialogue & Discourse",
      "doi": ""
    },
    {
      "id": "b228",
      "title": "A survey of intent classification and slot-filling datasets for task-oriented dialog",
      "authors": [
        "S Larson",
        "K Leach"
      ],
      "year": "2022",
      "venue": "A survey of intent classification and slot-filling datasets for task-oriented dialog",
      "doi": ""
    },
    {
      "id": "b229",
      "title": "Is chatgpt good at search? investigating large language models as reranking agent",
      "authors": [
        "W Sun",
        "L Yan",
        "X Ma",
        "P Ren",
        "D Yin",
        "Z Ren"
      ],
      "year": "2023",
      "venue": "Is chatgpt good at search? investigating large language models as reranking agent",
      "doi": ""
    },
    {
      "id": "b230",
      "title": "Large language models are built-in autoregressive search engines",
      "authors": [
        "N Ziems",
        "W Yu",
        "Z Zhang",
        "M Jiang"
      ],
      "year": "2023",
      "venue": "Large language models are built-in autoregressive search engines",
      "doi": ""
    },
    {
      "id": "b231",
      "title": "Explainable information retrieval: A survey",
      "authors": [
        "A Anand",
        "L Lyu",
        "M Idahl",
        "Y Wang",
        "J Wallat",
        "Z Zhang"
      ],
      "year": "2022",
      "venue": "Explainable information retrieval: A survey",
      "doi": ""
    },
    {
      "id": "b232",
      "title": "Document ranking with a pretrained sequence-to-sequence model",
      "authors": [
        "R Nogueira",
        "Z Jiang",
        "R Pradeep",
        "J Lin"
      ],
      "year": "",
      "venue": "Document ranking with a pretrained sequence-to-sequence model",
      "doi": ""
    },
    {
      "id": "b233",
      "title": "Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
      "authors": [
        "N Thakur",
        "N Reimers",
        "A Ücklé",
        "A Srivastava",
        "I Gurevych"
      ],
      "year": "",
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track",
      "doi": ""
    },
    {
      "id": "b234",
      "title": "Dense text retrieval based on pretrained language models: A survey",
      "authors": [
        "W X Zhao",
        "J Liu",
        "R Ren",
        "J.-R Wen"
      ],
      "year": "2022",
      "venue": "Dense text retrieval based on pretrained language models: A survey",
      "doi": ""
    },
    {
      "id": "b235",
      "title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions",
      "authors": [
        "G Adomavicius",
        "A Tuzhilin"
      ],
      "year": "2005",
      "venue": "IEEE transactions on knowledge and data engineering",
      "doi": ""
    },
    {
      "id": "b236",
      "title": "A survey on modern recommendation system based on big data",
      "authors": [
        "Y Peng"
      ],
      "year": "2022",
      "venue": "A survey on modern recommendation system based on big data",
      "doi": ""
    },
    {
      "id": "b237",
      "title": "A survey of attack detection approaches in collaborative filtering recommender systems",
      "authors": [
        "F Rezaimehr",
        "C Dadkhah"
      ],
      "year": "2021",
      "venue": "Artificial Intelligence Review",
      "doi": ""
    },
    {
      "id": "b238",
      "title": "Rethinking multi-interest learning for candidate matching in recommender systems",
      "authors": [
        "Y Xie",
        "J Gao",
        "P Zhou",
        "Q Ye",
        "Y Hua",
        "J Kim",
        "F Wu",
        "S Kim"
      ],
      "year": "2023",
      "venue": "Rethinking multi-interest learning for candidate matching in recommender systems",
      "doi": ""
    },
    {
      "id": "b239",
      "title": "An interactive knowledge-based recommender system for fashion product design in the big data environment",
      "authors": [
        "M Dong",
        "X Zeng",
        "L Koehl",
        "J Zhang"
      ],
      "year": "2020",
      "venue": "Information Sciences",
      "doi": ""
    },
    {
      "id": "b240",
      "title": "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
      "authors": [
        "Y Gao",
        "T Sheng",
        "Y Xiang",
        "Y Xiong",
        "H Wang",
        "J Zhang"
      ],
      "year": "2023",
      "venue": "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
      "doi": ""
    },
    {
      "id": "b241",
      "title": "Crossdomain recommendation: challenges, progress, and prospects",
      "authors": [
        "F Zhu",
        "Y Wang",
        "C Chen",
        "J Zhou",
        "L Li",
        "G Liu"
      ],
      "year": "2021",
      "venue": "Crossdomain recommendation: challenges, progress, and prospects",
      "doi": ""
    },
    {
      "id": "b242",
      "title": "Zero-shot next-item recommendation using large pretrained language models",
      "authors": [
        "L Wang",
        "E.-P Lim"
      ],
      "year": "2023",
      "venue": "Zero-shot next-item recommendation using large pretrained language models",
      "doi": ""
    },
    {
      "id": "b243",
      "title": "Bookgpt: A general framework for book recommendation empowered by large language model",
      "authors": [
        "A Zhiyuli",
        "Y Chen",
        "X Zhang",
        "X Liang"
      ],
      "year": "2023",
      "venue": "Bookgpt: A general framework for book recommendation empowered by large language model",
      "doi": ""
    },
    {
      "id": "b244",
      "title": "Is chatgpt a good recommender? a preliminary study",
      "authors": [
        "J Liu",
        "C Liu",
        "R Lv",
        "K Zhou",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good recommender? a preliminary study",
      "doi": ""
    },
    {
      "id": "b245",
      "title": "Uncovering chatgpt's capabilities in recommender systems",
      "authors": [
        "S Dai",
        "N Shao",
        "H Zhao",
        "W Yu",
        "Z Si",
        "C Xu",
        "Z Sun",
        "X Zhang",
        "J Xu"
      ],
      "year": "2023",
      "venue": "Uncovering chatgpt's capabilities in recommender systems",
      "doi": ""
    },
    {
      "id": "b246",
      "title": "Do llms understand user preferences? evaluating llms on user rating prediction",
      "authors": [
        "W.-C Kang",
        "J Ni",
        "N Mehta",
        "M Sathiamoorthy",
        "L Hong",
        "E Chi",
        "D Z Cheng"
      ],
      "year": "2023",
      "venue": "Do llms understand user preferences? evaluating llms on user rating prediction",
      "doi": ""
    },
    {
      "id": "b247",
      "title": "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
      "authors": [
        "J Zhang",
        "K Bao",
        "Y Zhang",
        "W Wang",
        "F Feng",
        "X He"
      ],
      "year": "2023",
      "venue": "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
      "doi": ""
    },
    {
      "id": "b248",
      "title": "Large language models are zero-shot rankers for recommender systems",
      "authors": [
        "Y Hou",
        "J Zhang",
        "Z Lin",
        "H Lu",
        "R Xie",
        "J Mcauley",
        "W X Zhao"
      ],
      "year": "2023",
      "venue": "Large language models are zero-shot rankers for recommender systems",
      "doi": ""
    },
    {
      "id": "b249",
      "title": "Large language model augmented narrative driven recommendations",
      "authors": [
        "S Mysore",
        "A Mccallum",
        "H Zamani"
      ],
      "year": "2023",
      "venue": "Large language model augmented narrative driven recommendations",
      "doi": ""
    },
    {
      "id": "b250",
      "title": "Keep the conversation going: Fixing 162 out of 337 bugs for 0.42 each using chatgpt",
      "authors": [
        "C S Xia",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "Keep the conversation going: Fixing 162 out of 337 bugs for 0.42 each using chatgpt",
      "doi": ""
    },
    {
      "id": "b251",
      "title": "Evaluation of chatgpt model for vulnerability detection",
      "authors": [
        "A Cheshkov",
        "P Zadorozhny",
        "R Levichev"
      ],
      "year": "2023",
      "venue": "Evaluation of chatgpt model for vulnerability detection",
      "doi": ""
    },
    {
      "id": "b252",
      "title": "Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt",
      "authors": [
        "B Yetis ¸tiren",
        "I Özsoy",
        "M Ayerdem",
        "E T Üz Ün"
      ],
      "year": "2023",
      "venue": "Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt",
      "doi": ""
    },
    {
      "id": "b253",
      "title": "Finding failure-inducing test cases with chatgpt",
      "authors": [
        "T.-O Li",
        "W Zong",
        "Y Wang",
        "H Tian",
        "Y Wang",
        "S.-C Cheung"
      ],
      "year": "2023",
      "venue": "Finding failure-inducing test cases with chatgpt",
      "doi": ""
    },
    {
      "id": "b254",
      "title": "Improving chatgpt prompt for code generation",
      "authors": [
        "C Liu",
        "X Bao",
        "H Zhang",
        "N Zhang",
        "H Hu",
        "X Zhang",
        "M Yan"
      ],
      "year": "2023",
      "venue": "Improving chatgpt prompt for code generation",
      "doi": ""
    },
    {
      "id": "b255",
      "title": "Ai-assisted coding: Experiments with gpt-4",
      "authors": [
        "R A Poldrack",
        "T Lu",
        "G Beguš"
      ],
      "year": "2023",
      "venue": "Ai-assisted coding: Experiments with gpt-4",
      "doi": ""
    },
    {
      "id": "b256",
      "title": "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
      "authors": [
        "J Liu",
        "C S Xia",
        "Y Wang",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
      "doi": ""
    },
    {
      "id": "b257",
      "title": "Gptutor: a chatgpt-powered programming tool for code explanation",
      "authors": [
        "E Chen",
        "R Huang",
        "H.-S Chen",
        "Y.-H Tseng",
        "L.-Y Li"
      ],
      "year": "2023",
      "venue": "Gptutor: a chatgpt-powered programming tool for code explanation",
      "doi": ""
    },
    {
      "id": "b258",
      "title": "Comparing software developers with chatgpt: An empirical investigation",
      "authors": [
        "N Nascimento",
        "P Alencar",
        "D Cowan"
      ],
      "year": "2023",
      "venue": "Comparing software developers with chatgpt: An empirical investigation",
      "doi": ""
    },
    {
      "id": "b259",
      "title": "Automatic code documentation generation using gpt-3",
      "authors": [
        "J Y Khan",
        "G Uddin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering",
      "doi": ""
    },
    {
      "id": "b260",
      "title": "Comparing code explanations created by students and large language models",
      "authors": [
        "J Leinonen",
        "P Denny",
        "S Macneil",
        "S Sarsa",
        "S Bernstein",
        "J Kim",
        "A Tran",
        "A Hellas"
      ],
      "year": "2023",
      "venue": "Comparing code explanations created by students and large language models",
      "doi": ""
    },
    {
      "id": "b261",
      "title": "Think outside the code: Brainstorming boosts large language models in code generation",
      "authors": [
        "X.-Y Li",
        "J.-T Xue",
        "Z Xie",
        "M Li"
      ],
      "year": "2023",
      "venue": "Think outside the code: Brainstorming boosts large language models in code generation",
      "doi": ""
    },
    {
      "id": "b262",
      "title": "Automatic program repair with openai's codex: Evaluating quixbugs",
      "authors": [
        "J A Prenner",
        "R Robbes"
      ],
      "year": "2021",
      "venue": "Automatic program repair with openai's codex: Evaluating quixbugs",
      "doi": ""
    },
    {
      "id": "b263",
      "title": "Exploring the effectiveness of large language models in generating unit tests",
      "authors": [
        "M L Siddiq",
        "J C S Santos",
        "R H Tanvir",
        "N Ulfat",
        "F A Rifat",
        "V C Lopes"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b264",
      "title": "Is chatgpt the ultimate programming assistant-how far is it?",
      "authors": [
        "H Tian",
        "W Lu",
        "T O Li",
        "X Tang",
        "S.-C Cheung",
        "J Klein",
        "T F Bissyandé"
      ],
      "year": "2023",
      "venue": "Is chatgpt the ultimate programming assistant-how far is it?",
      "doi": ""
    },
    {
      "id": "b265",
      "title": "An empirical study on using large language models for multi-intent comment generation",
      "authors": [
        "M Geng",
        "S Wang",
        "D Dong",
        "H Wang",
        "G Li",
        "Z Jin",
        "X Mao",
        "X Liao"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b266",
      "title": "Explainable automated debugging via large language model-driven scientific debugging",
      "authors": [
        "S Kang",
        "B Chen",
        "S Yoo",
        "J.-G Lou"
      ],
      "year": "2023",
      "venue": "Explainable automated debugging via large language model-driven scientific debugging",
      "doi": ""
    },
    {
      "id": "b267",
      "title": "Chatgpt for programming numerical methods",
      "authors": [
        "A Kashefi",
        "T Mukerji"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b268",
      "title": "A preliminary analysis on the code generation capabilities of gpt-3.5 and bard ai models for java functions",
      "authors": [
        "G Destefanis",
        "S Bartolucci",
        "M Ortu"
      ],
      "year": "2023",
      "venue": "A preliminary analysis on the code generation capabilities of gpt-3.5 and bard ai models for java functions",
      "doi": ""
    },
    {
      "id": "b269",
      "title": "No more manual tests? evaluating and improving chatgpt for unit test generation",
      "authors": [
        "Z Yuan",
        "Y Lou",
        "M Liu",
        "S Ding",
        "K Wang",
        "Y Chen",
        "X Peng"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b270",
      "title": "Generative ai for programming education: Benchmarking chatgpt, gpt-4, and human tutors",
      "authors": [
        "T Phung",
        "V.-A Padurean",
        "J P Cambronero",
        "S Gulwani",
        "T Kohn",
        "R Majumdar",
        "A K Singla",
        "G Soares"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b271",
      "title": "Large language models for software engineering: A systematic literature review",
      "authors": [
        "X Hou",
        "Y Zhao",
        "Y Liu",
        "Z Yang",
        "K Wang",
        "L Li",
        "X Luo",
        "D Lo",
        "J Grundy",
        "H Wang"
      ],
      "year": "2023",
      "venue": "Large language models for software engineering: A systematic literature review",
      "doi": ""
    },
    {
      "id": "b272",
      "title": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
      "authors": [
        "S Lu",
        "D Guo",
        "S Ren",
        "J Huang",
        "A Svyatkovskiy",
        "A Blanco",
        "C Clement",
        "D Drain",
        "D Jiang",
        "D Tang"
      ],
      "year": "",
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track",
      "doi": ""
    },
    {
      "id": "b273",
      "title": "Cotext: Multi-task learning with code-text transformer",
      "authors": [
        "L Phan",
        "H Tran",
        "D Le",
        "H Nguyen",
        "J Annibal",
        "A Peltekian",
        "Y Ye"
      ],
      "year": "2021",
      "venue": "Proceedings of the 1st Workshop on Natural Language Processing for Programming",
      "doi": ""
    },
    {
      "id": "b274",
      "title": "Graphcodebert: Pretraining code representations with data flow",
      "authors": [
        "D Guo",
        "S Ren",
        "S Lu",
        "Z Feng",
        "D Tang",
        "L Shujie",
        "L Zhou",
        "N Duan",
        "A Svyatkovskiy",
        "S Fu"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b275",
      "title": "Unified pre-training for program understanding and generation",
      "authors": [
        "W Ahmad",
        "S Chakraborty",
        "B Ray",
        "K.-W Chang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter",
      "doi": ""
    },
    {
      "id": "b276",
      "title": "Cert: Continual pre-training on sketches for library-oriented code generation",
      "authors": [
        "D Zan",
        "B Chen",
        "D Yang",
        "Z Lin",
        "M Kim",
        "B Guan",
        "Y Wang",
        "W Chen",
        "J.-G Lou"
      ],
      "year": "2022",
      "venue": "Cert: Continual pre-training on sketches for library-oriented code generation",
      "doi": ""
    },
    {
      "id": "b277",
      "title": "Defects4j: A database of existing faults to enable controlled testing studies for java programs",
      "authors": [
        "R Just",
        "D Jalali",
        "M D Ernst"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 international symposium on software testing and analysis",
      "doi": ""
    },
    {
      "id": "b278",
      "title": "Quixbugs: A multi-lingual program repair benchmark set based on the quixey challenge",
      "authors": [
        "D Lin",
        "J Koppel",
        "A Chen",
        "A Solar-Lezama"
      ],
      "year": "2017",
      "venue": "Proceedings Companion of the 2017 ACM SIGPLAN international conference on systems, programming, languages, and applications: software for humanity",
      "doi": ""
    },
    {
      "id": "b279",
      "title": "Multimodal conversational ai: A survey of datasets and approaches",
      "authors": [
        "A Sundar",
        "L Heck"
      ],
      "year": "2022",
      "venue": "Proceedings of the 4th Workshop on NLP for Conversational AI",
      "doi": ""
    },
    {
      "id": "b280",
      "title": "Multimodal learning with transformers: A survey",
      "authors": [
        "P Xu",
        "X Zhu",
        "D A Clifton"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b281",
      "title": "Prompting large language models with answer heuristics for knowledge-based visual question answering",
      "authors": [
        "Z Shao",
        "Z Yu",
        "M Wang",
        "J Yu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": ""
    },
    {
      "id": "b282",
      "title": "Revive: Regional visual representation matters in knowledge-based visual question answering",
      "authors": [
        "Y Lin",
        "Y Xie",
        "D Chen",
        "Y Xu",
        "C Zhu",
        "L Yuan"
      ],
      "year": "2022",
      "venue": "Revive: Regional visual representation matters in knowledge-based visual question answering",
      "doi": ""
    },
    {
      "id": "b283",
      "title": "Kat: A knowledge augmented transformer for visionand-language",
      "authors": [
        "L Gui",
        "B Wang",
        "Q Huang",
        "A G Hauptmann",
        "Y Bisk",
        "J Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter",
      "doi": ""
    },
    {
      "id": "b284",
      "title": "Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation",
      "authors": [
        "Y Lu",
        "X Yang",
        "X Li",
        "X E Wang",
        "W Y Wang"
      ],
      "year": "2023",
      "venue": "Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation",
      "doi": ""
    },
    {
      "id": "b285",
      "title": "Collaborative generative ai: Integrating gpt-k for efficient editing in text-to-image generation",
      "authors": [
        "W Zhu",
        "X Wang",
        "Y Lu",
        "T.-J Fu",
        "X E Wang",
        "M Eckstein",
        "W Y Wang"
      ],
      "year": "2023",
      "venue": "Collaborative generative ai: Integrating gpt-k for efficient editing in text-to-image generation",
      "doi": ""
    },
    {
      "id": "b286",
      "title": "Controllable text-to-image generation with gpt-4",
      "authors": [
        "T Zhang",
        "Y Zhang",
        "V Vineet",
        "N Joshi",
        "X Wang"
      ],
      "year": "2023",
      "venue": "Controllable text-to-image generation with gpt-4",
      "doi": ""
    },
    {
      "id": "b287",
      "title": "Large language models are frame-level directors for zero-shot text-to-video generation",
      "authors": [
        "S Hong",
        "J Seo",
        "S Hong",
        "H Shin",
        "S Kim"
      ],
      "year": "2023",
      "venue": "Large language models are frame-level directors for zero-shot text-to-video generation",
      "doi": ""
    },
    {
      "id": "b288",
      "title": "Audiogpt: Understanding and generating speech, music, sound, and talking head",
      "authors": [
        "R Huang",
        "M Li",
        "D Yang",
        "J Shi",
        "X Chang",
        "Z Ye",
        "Y Wu",
        "Z Hong",
        "J Huang",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Audiogpt: Understanding and generating speech, music, sound, and talking head",
      "doi": ""
    },
    {
      "id": "b289",
      "title": "Retrieval augmented chest x-ray report generation using openai gpt models",
      "authors": [
        "M Ranjit",
        "G Ganapathy",
        "R Manuel",
        "T Ganu"
      ],
      "year": "2023",
      "venue": "Retrieval augmented chest x-ray report generation using openai gpt models",
      "doi": ""
    },
    {
      "id": "b290",
      "title": "Action-gpt: Leveraging large-scale language models for improved and generalized zero shot action generation",
      "authors": [
        "S S Kalakonda",
        "S Maheshwari",
        "R K Sarvadevabhatla"
      ],
      "year": "2022",
      "venue": "Action-gpt: Leveraging large-scale language models for improved and generalized zero shot action generation",
      "doi": ""
    },
    {
      "id": "b291",
      "title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
      "authors": [
        "C Wu",
        "S Yin",
        "W Qi",
        "X Wang",
        "Z Tang",
        "N Duan"
      ],
      "year": "2023",
      "venue": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
      "doi": ""
    },
    {
      "id": "b292",
      "title": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
      "authors": [
        "Z Yang",
        "L Li",
        "J Wang",
        "K Lin",
        "E Azarnasab",
        "F Ahmed",
        "Z Liu",
        "C Liu",
        "M Zeng",
        "L Wang"
      ],
      "year": "2023",
      "venue": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
      "doi": ""
    },
    {
      "id": "b293",
      "title": "Prompt chatgpt in mner: Improved multimodal named entity recognition method based on auxiliary refining knowledge from chatgpt",
      "authors": [
        "J Li",
        "H Li",
        "Z Pan",
        "G Pan"
      ],
      "year": "2023",
      "venue": "Prompt chatgpt in mner: Improved multimodal named entity recognition method based on auxiliary refining knowledge from chatgpt",
      "doi": ""
    },
    {
      "id": "b294",
      "title": "Images in language space: Exploring the suitability of large language models for vision & language tasks",
      "authors": [
        "S Hakimov",
        "D Schlangen"
      ],
      "year": "2023",
      "venue": "Images in language space: Exploring the suitability of large language models for vision & language tasks",
      "doi": ""
    },
    {
      "id": "b295",
      "title": "Layoutgpt: Compositional visual planning and generation with large language models",
      "authors": [
        "W Feng",
        "W Zhu",
        "T -J. Fu",
        "V Jampani",
        "A Akula",
        "X He",
        "S Basu",
        "X E Wang",
        "W Y Wang"
      ],
      "year": "2023",
      "venue": "Layoutgpt: Compositional visual planning and generation with large language models",
      "doi": ""
    },
    {
      "id": "b296",
      "title": "Improving clip training with language rewrites",
      "authors": [
        "L Fan",
        "D Krishnan",
        "P Isola",
        "D Katabi",
        "Y Tian"
      ],
      "year": "2023",
      "venue": "Improving clip training with language rewrites",
      "doi": ""
    },
    {
      "id": "b297",
      "title": "Llava-med: Training a large language-and-vision assistant for biomedicine in one day",
      "authors": [
        "C Li",
        "C Wong",
        "S Zhang",
        "N Usuyama",
        "H Liu",
        "J Yang",
        "T Naumann",
        "H Poon",
        "J Gao"
      ],
      "year": "2023",
      "venue": "Llava-med: Training a large language-and-vision assistant for biomedicine in one day",
      "doi": ""
    },
    {
      "id": "b298",
      "title": "A video is worth 4096 tokens: Verbalize story videos to understand them in zero shot",
      "authors": [
        "A Bhattacharya",
        "Y K Singla",
        "B Krishnamurthy",
        "R R Shah",
        "C Chen"
      ],
      "year": "2023",
      "venue": "A video is worth 4096 tokens: Verbalize story videos to understand them in zero shot",
      "doi": ""
    },
    {
      "id": "b299",
      "title": "Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research",
      "authors": [
        "X Mei",
        "C Meng",
        "H Liu",
        "Q Kong",
        "T Ko",
        "C Zhao",
        "M D Plumbley",
        "Y Zou",
        "W Wang"
      ],
      "year": "2023",
      "venue": "Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research",
      "doi": ""
    },
    {
      "id": "b300",
      "title": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
      "authors": [
        "D Zhang",
        "S Li",
        "X Zhang",
        "J Zhan",
        "P Wang",
        "Y Zhou",
        "X Qiu"
      ],
      "year": "2023",
      "venue": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
      "doi": ""
    },
    {
      "id": "b301",
      "title": "Chatbridge: Bridging modalities with large language model as a language catalyst",
      "authors": [
        "Z Zhao",
        "L Guo",
        "T Yue",
        "S Chen",
        "S Shao",
        "X Zhu",
        "Z Yuan",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Chatbridge: Bridging modalities with large language model as a language catalyst",
      "doi": ""
    },
    {
      "id": "b302",
      "title": "Can gpt-4 perform neural architecture search?",
      "authors": [
        "M Zheng",
        "X Su",
        "S You",
        "F Wang",
        "C Qian",
        "C Xu",
        "S Albanie"
      ],
      "year": "2023",
      "venue": "Can gpt-4 perform neural architecture search?",
      "doi": ""
    },
    {
      "id": "b303",
      "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
      "authors": [
        "Y Shen",
        "K Song",
        "X Tan",
        "D Li",
        "W Lu",
        "Y Zhuang"
      ],
      "year": "2023",
      "venue": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
      "doi": ""
    },
    {
      "id": "b304",
      "title": "Mlcopilot: Unleashing the power of large language models in solving machine learning tasks",
      "authors": [
        "L Zhang",
        "Y Zhang",
        "K Ren",
        "D Li",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "Mlcopilot: Unleashing the power of large language models in solving machine learning tasks",
      "doi": ""
    },
    {
      "id": "b305",
      "title": "Automlgpt: Automatic machine learning with gpt",
      "authors": [
        "S Zhang",
        "C Gong",
        "L Wu",
        "X Liu",
        "M Zhou"
      ],
      "year": "2023",
      "venue": "Automlgpt: Automatic machine learning with gpt",
      "doi": ""
    },
    {
      "id": "b306",
      "title": "Automated machine learning: methods, systems, challenges",
      "authors": [
        "F Hutter",
        "L Kotthoff",
        "J Vanschoren"
      ],
      "year": "2019",
      "venue": "Automated machine learning: methods, systems, challenges",
      "doi": ""
    },
    {
      "id": "b307",
      "title": "Gpt3-toplan: Extracting plans from text using gpt-3",
      "authors": [
        "A Olmo",
        "S Sreedharan",
        "S Kambhampati"
      ],
      "year": "2021",
      "venue": "Gpt3-toplan: Extracting plans from text using gpt-3",
      "doi": ""
    },
    {
      "id": "b308",
      "title": "Large language models as zero-shot human models for human-robot interaction",
      "authors": [
        "B Zhang",
        "H Soh"
      ],
      "year": "2023",
      "venue": "Large language models as zero-shot human models for human-robot interaction",
      "doi": ""
    },
    {
      "id": "b309",
      "title": "Translating natural language to planning goals with large-language models",
      "authors": [
        "Y Xie",
        "C Yu",
        "T Zhu",
        "J Bai",
        "Z Gong",
        "H Soh"
      ],
      "year": "2023",
      "venue": "Translating natural language to planning goals with large-language models",
      "doi": ""
    },
    {
      "id": "b310",
      "title": "Chain-ofsymbol prompting elicits planning in large langauge models",
      "authors": [
        "H Hu",
        "H Lu",
        "H Zhang",
        "W Lam",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Chain-ofsymbol prompting elicits planning in large langauge models",
      "doi": ""
    },
    {
      "id": "b311",
      "title": "Large language models still can't plan (a benchmark for llms on planning and reasoning about change)",
      "authors": [
        "K Valmeekam",
        "A Olmo",
        "S Sreedharan",
        "S Kambhampati"
      ],
      "year": "2022",
      "venue": "NeurIPS 2022 Foundation Models for Decision Making Workshop",
      "doi": ""
    },
    {
      "id": "b312",
      "title": "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",
      "authors": [
        "K M Collins",
        "C Wong",
        "J Feng",
        "M Wei",
        "J B Tenenbaum"
      ],
      "year": "2022",
      "venue": "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",
      "doi": ""
    },
    {
      "id": "b313",
      "title": "Dissociating language and thought in large language models: a cognitive perspective",
      "authors": [
        "K Mahowald",
        "A A Ivanova",
        "I A Blank",
        "N Kanwisher",
        "J B Tenenbaum",
        "E Fedorenko"
      ],
      "year": "2023",
      "venue": "Dissociating language and thought in large language models: a cognitive perspective",
      "doi": ""
    },
    {
      "id": "b314",
      "title": "Medical concept normalization in user-generated texts by learning target concept embeddings",
      "authors": [
        "K S Kalyan",
        "S Sangeetha"
      ],
      "year": "2020",
      "venue": "Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis",
      "doi": ""
    },
    {
      "id": "b315",
      "title": "The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
      "authors": [],
      "year": "2020",
      "venue": "Proceedings of Deep Learning Inside Out",
      "doi": ""
    },
    {
      "id": "b316",
      "title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
      "authors": [
        "J Holmes",
        "Z Liu",
        "L Zhang",
        "Y Ding",
        "T T Sio",
        "L A Mcgee",
        "J B Ashman",
        "X Li",
        "T Liu",
        "J Shen"
      ],
      "year": "2023",
      "venue": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
      "doi": ""
    },
    {
      "id": "b317",
      "title": "Deid-gpt: Zero-shot medical text deidentification by gpt-4",
      "authors": [
        "Z Liu",
        "X Yu",
        "L Zhang",
        "Z Wu",
        "C Cao",
        "H Dai",
        "L Zhao",
        "W Liu",
        "D Shen",
        "Q Li"
      ],
      "year": "2023",
      "venue": "Deid-gpt: Zero-shot medical text deidentification by gpt-4",
      "doi": ""
    },
    {
      "id": "b318",
      "title": "Wanglab at mediqa-chat 2023: Clinical note generation from doctor-patient conversations using large language models",
      "authors": [
        "J Giorgi",
        "A Toma",
        "R Xie",
        "S Chen",
        "K An",
        "G Zheng",
        "B Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
      "doi": ""
    },
    {
      "id": "b319",
      "title": "Capabilities of gpt-4 on medical challenge problems",
      "authors": [
        "H Nori",
        "N King",
        "S M Mckinney",
        "D Carignan",
        "E Horvitz"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b320",
      "title": "Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",
      "authors": [
        "Q Chen",
        "J Du",
        "Y Hu",
        "V K Keloth",
        "X Peng",
        "K Raja",
        "R Zhang",
        "Z Lu",
        "H Xu"
      ],
      "year": "2023",
      "venue": "Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",
      "doi": ""
    },
    {
      "id": "b321",
      "title": "Performance of generative pretrained transformer on the national medical licensing examination in japan",
      "authors": [
        "Y Tanaka",
        "T Nakata",
        "K Aiga",
        "T Etani",
        "R Muramatsu",
        "S Katagiri",
        "H Kawai",
        "F Higashino",
        "M Enomoto",
        "M Noda",
        "M Kometani",
        "M Takamura",
        "T Yoneda",
        "H Kakizaki",
        "A Nomura"
      ],
      "year": "2023",
      "venue": "Performance of generative pretrained transformer on the national medical licensing examination in japan",
      "doi": ""
    },
    {
      "id": "b322",
      "title": "Benchmarking large language models on cmexam-a comprehensive chinese medical exam dataset",
      "authors": [
        "J Liu",
        "P Zhou",
        "Y Hua",
        "D Chong",
        "Z Tian",
        "A Liu",
        "H Wang",
        "C You",
        "Z Guo",
        "L Zhu"
      ],
      "year": "2023",
      "venue": "Benchmarking large language models on cmexam-a comprehensive chinese medical exam dataset",
      "doi": ""
    },
    {
      "id": "b323",
      "title": "Data augmentation for radiology report simplification",
      "authors": [
        "Z Yang",
        "S Cherian",
        "S Vucetic"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EACL 2023",
      "doi": ""
    },
    {
      "id": "b324",
      "title": "Impressiongpt: an iterative optimizing framework for radiology report summarization with chatgpt",
      "authors": [
        "C Ma",
        "Z Wu",
        "J Wang",
        "S Xu",
        "Y Wei",
        "Z Liu",
        "L Guo",
        "X Cai",
        "S Zhang",
        "T Zhang"
      ],
      "year": "2023",
      "venue": "Impressiongpt: an iterative optimizing framework for radiology report summarization with chatgpt",
      "doi": ""
    },
    {
      "id": "b325",
      "title": "Gpt-3 models are poor few-shot learners in the biomedical domain",
      "authors": [
        "M Moradi",
        "K Blagec",
        "F Haberl",
        "M Samwald"
      ],
      "year": "2021",
      "venue": "Gpt-3 models are poor few-shot learners in the biomedical domain",
      "doi": ""
    },
    {
      "id": "b326",
      "title": "Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports",
      "authors": [
        "K Jeblick",
        "B Schachtner",
        "J Dexl",
        "A Mittermeier",
        "A T St Über",
        "J Topalis",
        "T Weber",
        "P Wesp",
        "B Sabel",
        "J Ricke"
      ],
      "year": "2022",
      "venue": "Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports",
      "doi": ""
    },
    {
      "id": "b327",
      "title": "Gersteinlab at mediqachat 2023: Clinical note summarization from doctor-patient conversations through fine-tuning and in-context learning",
      "authors": [
        "X Tang",
        "A Tran",
        "J Tan",
        "M Gerstein"
      ],
      "year": "2023",
      "venue": "Gersteinlab at mediqachat 2023: Clinical note summarization from doctor-patient conversations through fine-tuning and in-context learning",
      "doi": ""
    },
    {
      "id": "b328",
      "title": "Large language models are few-shot clinical information extractors",
      "authors": [
        "M Agrawal",
        "S Hegselmann",
        "H Lang",
        "Y Kim",
        "D Sontag"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b329",
      "title": "Generating medicallyaccurate summaries of patient-provider dialogue: A multistage approach using large language models",
      "authors": [
        "V Nair",
        "E Schumacher",
        "A Kannan"
      ],
      "year": "2023",
      "venue": "Generating medicallyaccurate summaries of patient-provider dialogue: A multistage approach using large language models",
      "doi": ""
    },
    {
      "id": "b330",
      "title": "Summarizing, simplifying, and synthesizing medical evidence using gpt-3 (with varying success)",
      "authors": [
        "C Shaib",
        "M L Li",
        "S Joseph",
        "I J Marshall",
        "J J Li",
        "B C Wallace"
      ],
      "year": "2023",
      "venue": "Summarizing, simplifying, and synthesizing medical evidence using gpt-3 (with varying success)",
      "doi": ""
    },
    {
      "id": "b331",
      "title": "Medgpteval: A dataset and benchmark to evaluate responses of large language models in medicine",
      "authors": [
        "J Xu",
        "L Lu",
        "S Yang",
        "B Liang",
        "X Peng",
        "J Pang",
        "J Ding",
        "X Shi",
        "L Yang",
        "H Song"
      ],
      "year": "2023",
      "venue": "Medgpteval: A dataset and benchmark to evaluate responses of large language models in medicine",
      "doi": ""
    },
    {
      "id": "b332",
      "title": "Chatgpt performs on the chinese national medical licensing examination",
      "authors": [
        "X Wang",
        "Z Gong",
        "G Wang",
        "J Jia",
        "Y Xu",
        "J Zhao",
        "Q Fan",
        "S Wu",
        "W Hu",
        "X Li"
      ],
      "year": "2023",
      "venue": "Chatgpt performs on the chinese national medical licensing examination",
      "doi": ""
    },
    {
      "id": "b333",
      "title": "Using gpt-3 to build a lexicon of drugs of abuse synonyms for social media pharmacovigilance",
      "authors": [
        "K A Carpenter",
        "R B Altman"
      ],
      "year": "2023",
      "venue": "Biomolecules",
      "doi": ""
    },
    {
      "id": "b334",
      "title": "Do we still need clinical language models?\" in Conference on Health, Inference, and Learning",
      "authors": [
        "E Hernandez",
        "D Mahajan",
        "J Wulff",
        "M J Smith",
        "Z Ziegler",
        "D Nadler",
        "P Szolovits",
        "A Johnson",
        "E Alsentzer"
      ],
      "year": "2023",
      "venue": "Do we still need clinical language models?\" in Conference on Health, Inference, and Learning",
      "doi": ""
    },
    {
      "id": "b335",
      "title": "Assessing the utility of chatgpt throughout the entire clinical workflow",
      "authors": [
        "A S Rao",
        "M Pang",
        "J Kim",
        "M Kamineni",
        "W Lie",
        "A K Prasad",
        "A Landman",
        "K Dryer",
        "M D Succi"
      ],
      "year": "2023",
      "venue": "medRxiv",
      "doi": ""
    },
    {
      "id": "b336",
      "title": "Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models",
      "authors": [
        "T H Kung",
        "M Cheatham",
        "A Medenilla",
        "C Sillos",
        "L De Leon",
        "C Elepa Ño",
        "M Madriaga",
        "R Aggabao",
        "G Diaz-Candido",
        "J Maningo"
      ],
      "year": "2023",
      "venue": "PLoS digital health",
      "doi": ""
    },
    {
      "id": "b337",
      "title": "Chatgpt-versus humangenerated answers to frequently asked questions about diabetes: a turing test-inspired survey among employees of a danish diabetes center",
      "authors": [
        "A Hulman",
        "O L Dollerup",
        "J F Mortensen",
        "M Fenech",
        "K Norman",
        "H Stoevring",
        "T K Hansen"
      ],
      "year": "2023",
      "venue": "medRxiv",
      "doi": ""
    },
    {
      "id": "b338",
      "title": "Diagnostic accuracy of differential-diagnosis lists generated by generative pretrained transformer 3 chatbot for clinical vignettes with common chief complaints: A pilot study",
      "authors": [
        "T Hirosawa",
        "Y Harada",
        "M Yokose",
        "T Sakamoto",
        "R Kawamura",
        "T Shimizu"
      ],
      "year": "2023",
      "venue": "International journal of environmental research and public health",
      "doi": ""
    },
    {
      "id": "b339",
      "title": "Assessing the value of chatgpt for clinical decision support optimization",
      "authors": [
        "S Liu",
        "A P Wright",
        "B L Patterson",
        "J P Wanderer",
        "R W Turer",
        "S D Nelson",
        "A B Mccoy",
        "D F Sittig",
        "A Wright"
      ],
      "year": "2023",
      "venue": "MedRxiv",
      "doi": ""
    },
    {
      "id": "b340",
      "title": "How does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment",
      "authors": [
        "A Gilson",
        "C W Safranek",
        "T Huang",
        "V Socrates",
        "L Chi",
        "R A Taylor",
        "D Chartash"
      ],
      "year": "2023",
      "venue": "JMIR Medical Education",
      "doi": ""
    },
    {
      "id": "b341",
      "title": "Evaluating the performance of chatgpt in ophthalmology: An analysis of its successes and shortcomings",
      "authors": [
        "F Antaki",
        "S Touma",
        "D Milad",
        "J El-Khoury",
        "R Duval"
      ],
      "year": "2023",
      "venue": "Ophthalmology Science",
      "doi": ""
    },
    {
      "id": "b342",
      "title": "Translating radiology reports into plain language using chatgpt and gpt-4 with prompt learning: results, limitations, and potential",
      "authors": [
        "Q Lyu",
        "J Tan",
        "M E Zapadka",
        "J Ponnatapura",
        "C Niu",
        "K J Myers",
        "G Wang",
        "C T Whitlow"
      ],
      "year": "2023",
      "venue": "Biomedicine, and Art",
      "doi": ""
    },
    {
      "id": "b343",
      "title": "Legal prompting: Teaching a language model to think like a lawyer",
      "authors": [
        "F Yu",
        "L Quartey",
        "F Schilder"
      ],
      "year": "2022",
      "venue": "Legal prompting: Teaching a language model to think like a lawyer",
      "doi": ""
    },
    {
      "id": "b344",
      "title": "A brief report on lawgpt 1.0: A virtual legal assistant based on gpt-3",
      "authors": [
        "H.-T Nguyen"
      ],
      "year": "2023",
      "venue": "A brief report on lawgpt 1.0: A virtual legal assistant based on gpt-3",
      "doi": ""
    },
    {
      "id": "b345",
      "title": "Chatgpt may pass the bar exam soon, but has a long way to go for the lexglue benchmark",
      "authors": [
        "I Chalkidis"
      ],
      "year": "2023",
      "venue": "Chatgpt may pass the bar exam soon, but has a long way to go for the lexglue benchmark",
      "doi": ""
    },
    {
      "id": "b346",
      "title": "Chatgpt goes to law school",
      "authors": [
        "J H Choi",
        "K E Hickman",
        "A Monahan",
        "D Schwarcz"
      ],
      "year": "2023",
      "venue": "Chatgpt goes to law school",
      "doi": ""
    },
    {
      "id": "b347",
      "title": "Chestxraybert: A pretrained language model for chest radiology report summarization",
      "authors": [
        "X Cai",
        "S Liu",
        "J Han",
        "L Yang",
        "Z Liu",
        "T Liu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia",
      "doi": ""
    },
    {
      "id": "b348",
      "title": "Doctorglm: Fine-tuning your chinese doctor is not a herculean task",
      "authors": [
        "H Xiong",
        "S Wang",
        "Y Zhu",
        "Z Zhao",
        "Y Liu",
        "Q Wang",
        "D Shen"
      ],
      "year": "2023",
      "venue": "Doctorglm: Fine-tuning your chinese doctor is not a herculean task",
      "doi": ""
    },
    {
      "id": "b349",
      "title": "Overview of the mediqa-chat 2023 shared tasks on the summarization & generation of doctor-patient conversations",
      "authors": [
        "A B Abacha",
        "W -W. Yim",
        "G Adams",
        "N Snider",
        "M Yetisgen-Yildiz"
      ],
      "year": "2023",
      "venue": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
      "doi": ""
    },
    {
      "id": "b350",
      "title": "One embedder, any task: Instruction-finetuned text embeddings",
      "authors": [
        "H Su",
        "J Kasai",
        "Y Wang",
        "Y Hu",
        "M Ostendorf",
        "W -T. Yih",
        "N A Smith",
        "L Zettlemoyer",
        "T Yu"
      ],
      "year": "2022",
      "venue": "One embedder, any task: Instruction-finetuned text embeddings",
      "doi": ""
    },
    {
      "id": "b351",
      "title": "Chinese finegrained financial sentiment analysis with large language models",
      "authors": [
        "Y Lan",
        "Y Wu",
        "W Xu",
        "W Feng",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Chinese finegrained financial sentiment analysis with large language models",
      "doi": ""
    },
    {
      "id": "b352",
      "title": "Transforming sentiment analysis in the financial domain with chatgpt",
      "authors": [
        "G Fatouros",
        "J Soldatos",
        "K Kouroumali",
        "G Makridis",
        "D Kyriazis"
      ],
      "year": "2023",
      "venue": "Transforming sentiment analysis in the financial domain with chatgpt",
      "doi": ""
    },
    {
      "id": "b353",
      "title": "Sentiment spin: Attacking financial sentiment with gpt-3",
      "authors": [
        "M Leippold"
      ],
      "year": "2023",
      "venue": "Finance Research Letters",
      "doi": ""
    },
    {
      "id": "b354",
      "title": "Promptshots at the finnlp-2022 erai task: Pairwise comparison and unsupervised ranking",
      "authors": [
        "P Wiriyathammabhum"
      ],
      "year": "2022",
      "venue": "Proceedings of the Fourth Workshop on Financial Technology and Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b355",
      "title": "Zero is not hero yet: Benchmarking zero-shot performance of llms for financial tasks",
      "authors": [
        "A Shah",
        "S Chava"
      ],
      "year": "2023",
      "venue": "Zero is not hero yet: Benchmarking zero-shot performance of llms for financial tasks",
      "doi": ""
    },
    {
      "id": "b356",
      "title": "Fineval: A chinese financial domain knowledge evaluation benchmark for large language models",
      "authors": [
        "L Zhang",
        "W Cai",
        "Z Liu",
        "Z Yang",
        "W Dai",
        "Y Liao",
        "Q Qin",
        "Y Li",
        "X Liu",
        "Z Liu"
      ],
      "year": "2023",
      "venue": "Fineval: A chinese financial domain knowledge evaluation benchmark for large language models",
      "doi": ""
    },
    {
      "id": "b357",
      "title": "Gpt-finre: In-context learning for financial relation extraction using large language models",
      "authors": [
        "P K Rajpoot",
        "A Parikh"
      ],
      "year": "2023",
      "venue": "Gpt-finre: In-context learning for financial relation extraction using large language models",
      "doi": ""
    },
    {
      "id": "b358",
      "title": "Breaking the bank with chatgpt: Few-shot text classification for finance",
      "authors": [
        "L Loukas",
        "I Stogiannidis",
        "P Malakasiotis",
        "S Vassos"
      ],
      "year": "2023",
      "venue": "Breaking the bank with chatgpt: Few-shot text classification for finance",
      "doi": ""
    },
    {
      "id": "b359",
      "title": "Lexglue: A benchmark dataset for legal language understanding in english",
      "authors": [
        "I Chalkidis",
        "A Jana",
        "D Hartung",
        "M Bommarito",
        "I Androutsopoulos",
        "D Katz",
        "N Aletras"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b360",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q V Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b361",
      "title": "Finqa: A dataset of numerical reasoning over financial data",
      "authors": [
        "Z Chen",
        "W Chen",
        "C Smiley",
        "S Shah",
        "I Borova",
        "D Langdon",
        "R Moussa",
        "M Beane",
        "T.-H Huang",
        "B R Routledge"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b362",
      "title": "Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning",
      "authors": [
        "V D Lai",
        "N T Ngo",
        "A P B Veyseh",
        "H Man",
        "F Dernoncourt",
        "T Bui",
        "T H Nguyen"
      ],
      "year": "2023",
      "venue": "Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning",
      "doi": ""
    },
    {
      "id": "b363",
      "title": "Is chatgpt a highly fluent grammatical error correction system? a comprehensive evaluation",
      "authors": [
        "T Fang",
        "S Yang",
        "K Lan",
        "D F Wong",
        "J Hu",
        "L S Chao",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Is chatgpt a highly fluent grammatical error correction system? a comprehensive evaluation",
      "doi": ""
    },
    {
      "id": "b364",
      "title": "On the multilingual capabilities of very large-scale english language models",
      "authors": [
        "J Armengol-Estapé",
        "O De Gibert",
        "M Bonet",
        "Melero"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
      "doi": ""
    },
    {
      "id": "b365",
      "title": "Mega: Multilingual evaluation of generative ai",
      "authors": [
        "K Ahuja",
        "R Hada",
        "M Ochieng",
        "P Jain",
        "H Diddee",
        "S Maina",
        "T Ganu",
        "S Segal",
        "M Axmed",
        "K Bali"
      ],
      "year": "2023",
      "venue": "Mega: Multilingual evaluation of generative ai",
      "doi": ""
    },
    {
      "id": "b366",
      "title": "Don't trust gpt when your question is not in english",
      "authors": [
        "X Zhang",
        "S Li",
        "B Hauer",
        "N Shi",
        "G Kondrak"
      ],
      "year": "2023",
      "venue": "Don't trust gpt when your question is not in english",
      "doi": ""
    },
    {
      "id": "b367",
      "title": "Evaluating chatgpt's performance for multilingual and emoji-based hate speech detection",
      "authors": [
        "M Das",
        "S K Pandey",
        "A Mukherjee"
      ],
      "year": "2023",
      "venue": "Evaluating chatgpt's performance for multilingual and emoji-based hate speech detection",
      "doi": ""
    },
    {
      "id": "b368",
      "title": "Are large language model-based evaluators the solution to scaling up multilingual evaluation?",
      "authors": [
        "R Hada",
        "V Gumma",
        "A De Wynter",
        "H Diddee",
        "M Ahmed",
        "M Choudhury",
        "K Bali",
        "S Sitaram"
      ],
      "year": "2023",
      "venue": "Are large language model-based evaluators the solution to scaling up multilingual evaluation?",
      "doi": ""
    },
    {
      "id": "b369",
      "title": "Bhasa: A holistic southeast asian linguistic and cultural evaluation suite for large language models",
      "authors": [
        "W Q Leong",
        "J G Ngui",
        "Y Susanto",
        "H Rengarajan",
        "K Sarveswaran",
        "W C Tjhi"
      ],
      "year": "2023",
      "venue": "Bhasa: A holistic southeast asian linguistic and cultural evaluation suite for large language models",
      "doi": ""
    },
    {
      "id": "b370",
      "title": "Holistic evaluation of language models",
      "authors": [
        "R Bommasani",
        "P Liang",
        "T Lee"
      ],
      "year": "2023",
      "venue": "Annals of the New York Academy of Sciences",
      "doi": ""
    },
    {
      "id": "b371",
      "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
      "authors": [
        "A Srivastava",
        "A Rastogi",
        "A Rao",
        "A A M Shoeb",
        "A Abid",
        "A Fisch",
        "A R Brown",
        "A Santoro",
        "A Gupta",
        "A Garriga-Alonso"
      ],
      "year": "2023",
      "venue": "Transactions on Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b372",
      "title": "Chatgpt outperforms crowd-workers for text-annotation tasks",
      "authors": [
        "F Gilardi",
        "M Alizadeh",
        "M Kubli"
      ],
      "year": "2023",
      "venue": "Chatgpt outperforms crowd-workers for text-annotation tasks",
      "doi": ""
    },
    {
      "id": "b373",
      "title": "Annollm: Making large language models to be better crowdsourced annotators",
      "authors": [
        "X He",
        "Z Lin",
        "Y Gong",
        "A Jin",
        "H Zhang",
        "C Lin",
        "J Jiao",
        "S M Yiu",
        "N Duan",
        "W Chen"
      ],
      "year": "2023",
      "venue": "Annollm: Making large language models to be better crowdsourced annotators",
      "doi": ""
    },
    {
      "id": "b374",
      "title": "Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning",
      "authors": [
        "P T Örnberg"
      ],
      "year": "2023",
      "venue": "Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning",
      "doi": ""
    },
    {
      "id": "b375",
      "title": "Can chatgpt reproduce human-generated labels? a study of social computing tasks",
      "authors": [
        "Y Zhu",
        "P Zhang",
        "E.-U Haq",
        "P Hui",
        "G Tyson"
      ],
      "year": "2023",
      "venue": "Can chatgpt reproduce human-generated labels? a study of social computing tasks",
      "doi": ""
    },
    {
      "id": "b376",
      "title": "hot\" chatgpt: The promise of chatgpt in detecting and discriminating hateful, offensive, and toxic comments on social media",
      "authors": [
        "L Li",
        "L Fan",
        "S Atreja",
        "L Hemphill"
      ],
      "year": "2023",
      "venue": "hot\" chatgpt: The promise of chatgpt in detecting and discriminating hateful, offensive, and toxic comments on social media",
      "doi": ""
    },
    {
      "id": "b377",
      "title": "Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events",
      "authors": [
        "Y Gu",
        "S Zhang",
        "N Usuyama",
        "Y Woldesenbet",
        "C Wong",
        "P Sanapathi",
        "M Wei",
        "N Valluri",
        "E Strandberg",
        "T Naumann"
      ],
      "year": "2023",
      "venue": "Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events",
      "doi": ""
    },
    {
      "id": "b378",
      "title": "Want to reduce labeling cost? gpt-3 can help",
      "authors": [
        "S Wang",
        "Y Liu",
        "Y Xu",
        "C Zhu",
        "M Zeng"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": ""
    },
    {
      "id": "b379",
      "title": "Is gpt-3 a good data annotator?",
      "authors": [
        "B Ding",
        "C Qin",
        "L Liu",
        "L Bing",
        "S Joty",
        "B Li"
      ],
      "year": "2022",
      "venue": "Is gpt-3 a good data annotator?",
      "doi": ""
    },
    {
      "id": "b380",
      "title": "Large language models as instructors: A study on multilingual clinical entity extraction",
      "authors": [
        "S Meoni",
        "E De La Clergerie",
        "T Ryffel"
      ],
      "year": "2023",
      "venue": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
      "doi": ""
    },
    {
      "id": "b381",
      "title": "Inheritsumm: A general, versatile and compact summarizer by distilling from gpt",
      "authors": [
        "Y Xu",
        "R Xu",
        "D Iter",
        "Y Liu",
        "S Wang",
        "C Zhu",
        "M Zeng"
      ],
      "year": "2023",
      "venue": "Inheritsumm: A general, versatile and compact summarizer by distilling from gpt",
      "doi": ""
    },
    {
      "id": "b382",
      "title": "Open-source large language models outperform crowd workers and approach chatgpt in textannotation tasks",
      "authors": [
        "M Alizadeh",
        "M Kubli",
        "Z Samei",
        "S Dehghani",
        "J D Bermeo",
        "M Korobeynikova",
        "F Gilardi"
      ],
      "year": "2023",
      "venue": "Open-source large language models outperform crowd workers and approach chatgpt in textannotation tasks",
      "doi": ""
    },
    {
      "id": "b383",
      "title": "From humans to machines: can chatgpt-like llms effectively replace human annotators in nlp tasks",
      "authors": [
        "S Thapa",
        "U Naseem",
        "M Nasim"
      ],
      "year": "2023",
      "venue": "Workshop Proceedings of the 17th International AAAI Conference on Web and Social Media",
      "doi": ""
    },
    {
      "id": "b384",
      "title": "Twitsenti: a realtime twitter sentiment analysis and visualization framework",
      "authors": [
        "J S Murthy",
        "G Siddesh",
        "K Srinivasa"
      ],
      "year": "2019",
      "venue": "Journal of Information & Knowledge Management",
      "doi": ""
    },
    {
      "id": "b385",
      "title": "The validity of sentiment analysis: Comparing manual annotation, crowd-coding, dictionary approaches, and machine learning algorithms",
      "authors": [
        "W Van Atteveldt",
        "M A Van Der Velden",
        "M Boukes"
      ],
      "year": "2021",
      "venue": "Communication Methods and Measures",
      "doi": ""
    },
    {
      "id": "b386",
      "title": "An mturk crisis? shifts in data quality and the impact on study results",
      "authors": [
        "M Chmielewski",
        "S C Kucker"
      ],
      "year": "2020",
      "venue": "Social Psychological and Personality Science",
      "doi": ""
    },
    {
      "id": "b387",
      "title": "Z-code++: A pre-trained language model optimized for abstractive summarization",
      "authors": [
        "P He",
        "B Peng",
        "L Lu",
        "S Wang",
        "J Mei",
        "Y Liu",
        "R Xu",
        "H H Awadalla",
        "Y Shi",
        "C Zhu"
      ],
      "year": "2022",
      "venue": "Z-code++: A pre-trained language model optimized for abstractive summarization",
      "doi": ""
    },
    {
      "id": "b388",
      "title": "Scal-ing instruction-finetuned language models",
      "authors": [
        "H W Chung",
        "L Hou",
        "S Longpre",
        "B Zoph",
        "Y Tay",
        "W Fedus",
        "E Li",
        "X Wang",
        "M Dehghani",
        "S Brahma"
      ],
      "year": "2022",
      "venue": "Scal-ing instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b389",
      "title": "Chatgpt to replace crowdsourcing of paraphrases for intent classification: Higher diversity and comparable model robustness",
      "authors": [
        "J Cegin",
        "J Simko",
        "P Brusilovsky"
      ],
      "year": "2023",
      "venue": "Chatgpt to replace crowdsourcing of paraphrases for intent classification: Higher diversity and comparable model robustness",
      "doi": ""
    },
    {
      "id": "b390",
      "title": "Data augmentation for neural machine translation using generative language model",
      "authors": [
        "S Oh",
        "W Jung"
      ],
      "year": "2023",
      "venue": "Data augmentation for neural machine translation using generative language model",
      "doi": ""
    },
    {
      "id": "b391",
      "title": "Systematic review of effect of data augmentation using paraphrasing on named entity recognition",
      "authors": [
        "S Sharma",
        "A Joshi",
        "N Mukhija",
        "Y Zhao",
        "H Bhathena",
        "P Singh",
        "S Santhanam",
        "P Biswas"
      ],
      "year": "2022",
      "venue": "NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research",
      "doi": ""
    },
    {
      "id": "b392",
      "title": "Dr. llama: Improving small language models in domain-specific qa via generative data augmentation",
      "authors": [
        "Z Guo",
        "P Wang",
        "Y Wang",
        "S Yu"
      ],
      "year": "2023",
      "venue": "Dr. llama: Improving small language models in domain-specific qa via generative data augmentation",
      "doi": ""
    },
    {
      "id": "b393",
      "title": "Lm-cppf: Paraphrasing-guided data augmentation for contrastive promptbased few-shot fine-tuning",
      "authors": [
        "A Abaskohi",
        "S Rothe",
        "Y Yaghoobzadeh"
      ],
      "year": "2023",
      "venue": "Lm-cppf: Paraphrasing-guided data augmentation for contrastive promptbased few-shot fine-tuning",
      "doi": ""
    },
    {
      "id": "b394",
      "title": "Medical data augmentation via chatgpt: A case study on medication identification and medication event classification",
      "authors": [
        "S Sarker",
        "L Qian",
        "X Dong"
      ],
      "year": "2023",
      "venue": "Medical data augmentation via chatgpt: A case study on medication identification and medication event classification",
      "doi": ""
    },
    {
      "id": "b395",
      "title": "Auggpt: Leveraging chatgpt for text data augmentation",
      "authors": [
        "H Dai",
        "Z Liu",
        "W Liao",
        "X Huang",
        "Y Cao",
        "Z Wu",
        "L Zhao",
        "S Xu",
        "W Liu",
        "N Liu"
      ],
      "year": "2023",
      "venue": "Auggpt: Leveraging chatgpt for text data augmentation",
      "doi": ""
    },
    {
      "id": "b396",
      "title": "Chatgpt as data augmentation for compositional generalization: A case study in open intent detection",
      "authors": [
        "Y Fang",
        "X Li",
        "S W Thomas",
        "X Zhu"
      ],
      "year": "2023",
      "venue": "Chatgpt as data augmentation for compositional generalization: A case study in open intent detection",
      "doi": ""
    },
    {
      "id": "b397",
      "title": "A survey on image data augmentation for deep learning",
      "authors": [
        "C Shorten",
        "T M Khoshgoftaar"
      ],
      "year": "2019",
      "venue": "Journal of big data",
      "doi": ""
    },
    {
      "id": "b398",
      "title": "Data augmentation approaches in natural language processing: A survey",
      "authors": [
        "B Li",
        "Y Hou",
        "W Che"
      ],
      "year": "2022",
      "venue": "Ai Open",
      "doi": ""
    },
    {
      "id": "b399",
      "title": "A survey of text data augmentation",
      "authors": [
        "P Liu",
        "X Wang",
        "C Xiang",
        "W Meng"
      ],
      "year": "2020",
      "venue": "2020 International Conference on Computer Communication and Network Security (CCNS)",
      "doi": ""
    },
    {
      "id": "b400",
      "title": "A survey of data augmentation approaches for nlp",
      "authors": [
        "S Y Feng",
        "V Gangal",
        "J Wei",
        "S Chandar",
        "S Vosoughi",
        "T Mitamura",
        "E Hovy"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
      "doi": ""
    },
    {
      "id": "b401",
      "title": "A survey on data augmentation for text classification",
      "authors": [
        "M Bayer",
        "M.-A Kaufhold",
        "C Reuter"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys",
      "doi": ""
    },
    {
      "id": "b402",
      "title": "Synthetic and natural noise both break neural machine translation",
      "authors": [
        "Y Belinkov",
        "Y Bisk"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b403",
      "title": "Text data augmentation made simple by leveraging nlp cloud apis",
      "authors": [
        "C Coulombe"
      ],
      "year": "2018",
      "venue": "Text data augmentation made simple by leveraging nlp cloud apis",
      "doi": ""
    },
    {
      "id": "b404",
      "title": "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
      "authors": [
        "J Wei",
        "K Zou"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b405",
      "title": "That's so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using# petpeeve tweets",
      "authors": [
        "W Y Wang",
        "D Yang"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing",
      "doi": ""
    },
    {
      "id": "b406",
      "title": "Improving neural machine translation models with monolingual data",
      "authors": [
        "R Sennrich",
        "B Haddow",
        "A Birch"
      ],
      "year": "2016",
      "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b407",
      "title": "Question classification using limited labelled data",
      "authors": [
        "C Mallikarjuna",
        "S Sivanesan"
      ],
      "year": "2022",
      "venue": "Information Processing & Management",
      "doi": ""
    },
    {
      "id": "b408",
      "title": "Socialdial: A benchmark for socially-aware dialogue systems",
      "authors": [
        "H Zhan",
        "Z Li",
        "Y Wang",
        "L Luo",
        "T Feng",
        "X Kang",
        "Y Hua",
        "L Qu",
        "L.-K Soon",
        "S Sharma"
      ],
      "year": "2023",
      "venue": "Socialdial: A benchmark for socially-aware dialogue systems",
      "doi": ""
    },
    {
      "id": "b409",
      "title": "Umass bionlp at mediqa-chat 2023: Can llms generate high-quality synthetic note-oriented doctor-patient conversations?",
      "authors": [
        "J Wang",
        "Z Yao",
        "A Mitra",
        "S Osebe",
        "Z Yang",
        "H Yu"
      ],
      "year": "2023",
      "venue": "Umass bionlp at mediqa-chat 2023: Can llms generate high-quality synthetic note-oriented doctor-patient conversations?",
      "doi": ""
    },
    {
      "id": "b410",
      "title": "Textbooks are all you need",
      "authors": [
        "S Gunasekar",
        "Y Zhang",
        "J Aneja",
        "C C T Mendes",
        "A D Giorno",
        "S Gopi",
        "M Javaheripi",
        "P C Kauffmann",
        "G De Rosa",
        "O Saarikivi",
        "A Salim",
        "S Shah",
        "H S Behl",
        "X Wang",
        "S Bubeck",
        "R Eldan",
        "A T Kalai",
        "Y T Lee",
        "Y.-F Li"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b411",
      "title": "Llm-powered data augmentation for enhanced crosslingual performance",
      "authors": [
        "C Whitehouse",
        "M Choudhury",
        "A F Aji"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b412",
      "title": "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
      "authors": [
        "T Hartvigsen",
        "S Gabriel",
        "H Palangi",
        "M Sap",
        "D Ray",
        "E Kamar"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b413",
      "title": "A holistic approach to undesired content detection in the real world",
      "authors": [
        "T Markov",
        "C Zhang",
        "S Agarwal",
        "F E Nekoul",
        "T Lee",
        "S Adler",
        "A Jiang",
        "L Weng"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b414",
      "title": "Dr. llama: Improving small language models on pubmedqa via generative data augmentation",
      "authors": [
        "Z Guo",
        "P Wang",
        "Y Wang",
        "S Yu"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b415",
      "title": "Tinystories: How small can language models be and still speak coherent english?",
      "authors": [
        "R Eldan",
        "Y Li"
      ],
      "year": "2023",
      "venue": "Tinystories: How small can language models be and still speak coherent english?",
      "doi": ""
    },
    {
      "id": "b416",
      "title": "Logicot: Logical chain-of-thought instruction-tuning data collection with gpt-4",
      "authors": [
        "H Liu",
        "Z Teng",
        "L Cui",
        "C Zhang",
        "Q Zhou",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Logicot: Logical chain-of-thought instruction-tuning data collection with gpt-4",
      "doi": ""
    },
    {
      "id": "b417",
      "title": "Instruction tuning with gpt-4",
      "authors": [
        "B Peng",
        "C Li",
        "P He",
        "M Galley",
        "J Gao"
      ],
      "year": "2023",
      "venue": "Instruction tuning with gpt-4",
      "doi": ""
    },
    {
      "id": "b418",
      "title": "Gpt-calls: Enhancing call segmentation and tagging by generating synthetic conversations via large language models",
      "authors": [
        "I Malkiel",
        "U Alon",
        "Y Yehuda",
        "S Keren",
        "O Barkan",
        "R Ronen",
        "N Koenigstein"
      ],
      "year": "2023",
      "venue": "Gpt-calls: Enhancing call segmentation and tagging by generating synthetic conversations via large language models",
      "doi": ""
    },
    {
      "id": "b419",
      "title": "How large language models are transforming machine-paraphrase plagiarism",
      "authors": [
        "J P Wahle",
        "T Ruas",
        "F Kirstein",
        "B Gipp"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b420",
      "title": "Uzh clyp at semeval-2023 task 9: Head-first fine-tuning and chatgpt data generation for cross-lingual learning in tweet intimacy prediction",
      "authors": [
        "A Michail",
        "S Konstantinou",
        "S Clematide"
      ],
      "year": "2023",
      "venue": "Uzh clyp at semeval-2023 task 9: Head-first fine-tuning and chatgpt data generation for cross-lingual learning in tweet intimacy prediction",
      "doi": ""
    },
    {
      "id": "b421",
      "title": "Does synthetic data generation of llms help clinical text mining?",
      "authors": [
        "R Tang",
        "X Han",
        "X Jiang",
        "X Hu"
      ],
      "year": "2023",
      "venue": "Does synthetic data generation of llms help clinical text mining?",
      "doi": ""
    },
    {
      "id": "b422",
      "title": "Large language model as attributed training data generator: A tale of diversity and bias",
      "authors": [
        "Y Yu",
        "Y Zhuang",
        "J Zhang",
        "Y Meng",
        "A Ratner",
        "R Krishna",
        "J Shen",
        "C Zhang"
      ],
      "year": "2023",
      "venue": "Large language model as attributed training data generator: A tale of diversity and bias",
      "doi": ""
    },
    {
      "id": "b423",
      "title": "Neural machine translation data generation and augmentation using chatgpt",
      "authors": [
        "W Yang",
        "G Nicolai"
      ],
      "year": "2023",
      "venue": "Neural machine translation data generation and augmentation using chatgpt",
      "doi": ""
    },
    {
      "id": "b424",
      "title": "Robut: A systematic study of table qa robustness against human-annotated adversarial perturbations",
      "authors": [
        "Y Zhao",
        "C Zhao",
        "L Nan",
        "Z Qi",
        "W Zhang",
        "X Tang",
        "B Mi",
        "D Radev"
      ],
      "year": "2023",
      "venue": "Robut: A systematic study of table qa robustness against human-annotated adversarial perturbations",
      "doi": ""
    },
    {
      "id": "b425",
      "title": "Instructscore: Towards explainable text generation evaluation with automatic feedback",
      "authors": [
        "W Xu",
        "D Wang",
        "L Pan",
        "Z Song",
        "M Freitag",
        "W Y Wang",
        "L Li"
      ],
      "year": "2023",
      "venue": "Instructscore: Towards explainable text generation evaluation with automatic feedback",
      "doi": ""
    },
    {
      "id": "b426",
      "title": "Data augmentation using backtranslation for context-aware neural machine translation",
      "authors": [
        "A Sugiyama",
        "N Yoshinaga"
      ],
      "year": "2019",
      "venue": "Proceedings of the fourth workshop on discourse in machine translation",
      "doi": ""
    },
    {
      "id": "b427",
      "title": "Smaller language models are better black-box machine-generated text detectors",
      "authors": [
        "F Mireshghallah",
        "J Mattern",
        "S Gao",
        "R Shokri",
        "T Berg-Kirkpatrick"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b428",
      "title": "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
      "authors": [
        "B Guo",
        "X Zhang",
        "Z Wang",
        "M Jiang",
        "J Nie",
        "Y Ding",
        "J Yue",
        "Y Wu"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b429",
      "title": "Regulating chatgpt and other large generative ai models",
      "authors": [
        "P Hacker",
        "A Engel",
        "M Mauer"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",
      "doi": ""
    },
    {
      "id": "b430",
      "title": "Chatgpt and the rise of large language models: the new ai-driven infodemic threat in public health",
      "authors": [
        "L De Angelis",
        "F Baglivo",
        "G Arzilli",
        "G P Privitera",
        "P Ferragina",
        "A E Tozzi",
        "C Rizzo"
      ],
      "year": "2023",
      "venue": "Frontiers in Public Health",
      "doi": ""
    },
    {
      "id": "b431",
      "title": "Chatgpt or human? detect and explain. explaining decisions of machine learning model for detecting short chatgpt-generated text",
      "authors": [
        "S Mitrovi'c",
        "D Andreoletti",
        "O Ayoub"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b432",
      "title": "Comparing scientific abstracts generated by chatgpt to real abstracts with detectors and blinded human reviewers",
      "authors": [
        "C A Gao",
        "F M Howard",
        "N S Markov",
        "E C Dyer",
        "S Ramesh",
        "Y Luo",
        "A T Pearson"
      ],
      "year": "2023",
      "venue": "NPJ Digital Medicine",
      "doi": ""
    },
    {
      "id": "b433",
      "title": "Chatting and cheating: Ensuring academic integrity in the era of chatgpt",
      "authors": [
        "D R Cotton",
        "P A Cotton",
        "J R Shipway"
      ],
      "year": "2023",
      "venue": "Innovations in Education and Teaching International",
      "doi": ""
    },
    {
      "id": "b434",
      "title": "Detection of fake generated scientific abstracts",
      "authors": [
        "P C Theocharopoulos",
        "P Anagnostou",
        "A Tsoukala",
        "S V Georgakopoulos",
        "S K Tasoulis",
        "V P Plagianakos"
      ],
      "year": "2023",
      "venue": "Detection of fake generated scientific abstracts",
      "doi": ""
    },
    {
      "id": "b435",
      "title": "Distinguishing chatgpt (-3.5,-4)-generated and human-written papers through japanese stylometric analysis",
      "authors": [
        "W Zaitsu",
        "M Jin"
      ],
      "year": "2023",
      "venue": "Distinguishing chatgpt (-3.5,-4)-generated and human-written papers through japanese stylometric analysis",
      "doi": ""
    },
    {
      "id": "b436",
      "title": "Cheat: A large-scale dataset for detecting chatgpt-written abstracts",
      "authors": [
        "P Yu",
        "J Chen",
        "X Feng",
        "Z Xia"
      ],
      "year": "2023",
      "venue": "Cheat: A large-scale dataset for detecting chatgpt-written abstracts",
      "doi": ""
    },
    {
      "id": "b437",
      "title": "Dnagpt: Divergent n-gram analysis for training-free detection of gptgenerated text",
      "authors": [
        "X Yang",
        "W Cheng",
        "L Petzold",
        "W Y Wang",
        "H Chen"
      ],
      "year": "2023",
      "venue": "Dnagpt: Divergent n-gram analysis for training-free detection of gptgenerated text",
      "doi": ""
    },
    {
      "id": "b438",
      "title": "Argugpt: evaluating, understanding and identifying argumentative essays generated by gpt models",
      "authors": [
        "Y Liu",
        "Z Zhang",
        "W Zhang",
        "S Yue",
        "X Zhao",
        "X Cheng",
        "Y Zhang",
        "H Hu"
      ],
      "year": "2023",
      "venue": "Argugpt: evaluating, understanding and identifying argumentative essays generated by gpt models",
      "doi": ""
    },
    {
      "id": "b439",
      "title": "Detecting llm-generated text in computing education: A comparative study for chatgpt cases",
      "authors": [
        "M S Orenstrakh",
        "O Karnalim",
        "C A Suarez",
        "M Liut"
      ],
      "year": "2023",
      "venue": "Detecting llm-generated text in computing education: A comparative study for chatgpt cases",
      "doi": ""
    },
    {
      "id": "b440",
      "title": "Differentiate chatgpt-generated and humanwritten medical texts",
      "authors": [
        "W Liao",
        "Z Liu",
        "H Dai",
        "S Xu",
        "Z Wu",
        "Y Zhang",
        "X Huang",
        "D Zhu",
        "H Cai",
        "T Liu"
      ],
      "year": "2023",
      "venue": "Differentiate chatgpt-generated and humanwritten medical texts",
      "doi": ""
    },
    {
      "id": "b441",
      "title": "G3detector: General gpt-generated text detector",
      "authors": [
        "H Zhan",
        "X He",
        "Q Xu",
        "Y Wu",
        "P Stenetorp"
      ],
      "year": "2023",
      "venue": "G3detector: General gpt-generated text detector",
      "doi": ""
    },
    {
      "id": "b442",
      "title": "All that's 'human'is not gold: Evaluating human evaluation of generated text",
      "authors": [
        "E Clark",
        "T August",
        "S Serrano",
        "N Haduong",
        "S Gururangan",
        "N A Smith"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b443",
      "title": "To chatgpt, or not to chatgpt: That is the question!",
      "authors": [
        "A Pegoraro",
        "K Kumari",
        "H Fereidooni",
        "A.-R Sadeghi"
      ],
      "year": "2023",
      "venue": "To chatgpt, or not to chatgpt: That is the question!",
      "doi": ""
    },
    {
      "id": "b444",
      "title": "Red teaming language model detectors with language models",
      "authors": [
        "Z Shi",
        "Y Wang",
        "F Yin",
        "X Chen",
        "K.-W Chang",
        "C.-J Hsieh"
      ],
      "year": "2023",
      "venue": "Red teaming language model detectors with language models",
      "doi": ""
    },
    {
      "id": "b445",
      "title": "Will chatgpt get you caught? rethinking of plagiarism detection",
      "authors": [
        "M Khalil",
        "E Er"
      ],
      "year": "2023",
      "venue": "Will chatgpt get you caught? rethinking of plagiarism detection",
      "doi": ""
    },
    {
      "id": "b446",
      "title": "Mgtbench: Benchmarking machine-generated text detection",
      "authors": [
        "X He",
        "X Shen",
        "Z Chen",
        "M Backes",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Mgtbench: Benchmarking machine-generated text detection",
      "doi": ""
    },
    {
      "id": "b447",
      "title": "Bot or human? detecting chatgpt imposters with a single question",
      "authors": [
        "H Wang",
        "X Luo",
        "W Wang",
        "X Yan"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b448",
      "title": "Gpt-sentinel: Distinguishing human and chatgpt generated content",
      "authors": [
        "Y Chen",
        "H Kang",
        "V Zhai",
        "L Li",
        "R Singh",
        "B Ramakrishnan"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b449",
      "title": "Gpt paternity test: Gpt generated text detection with gpt genetic inheritance",
      "authors": [
        "X Yu",
        "Y Qi",
        "K Chen",
        "G Chen",
        "X Yang",
        "P Zhu",
        "W Zhang",
        "N H Yu"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b450",
      "title": "Is chatgpt involved in texts? measure the polish ratio to detect chatgpt-generated text",
      "authors": [
        "L Yang",
        "F Jiang",
        "H Li"
      ],
      "year": "2023",
      "venue": "ArXiv",
      "doi": ""
    },
    {
      "id": "b451",
      "title": "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
      "authors": [
        "K Krishna",
        "Y Song",
        "M Karpinska",
        "J Wieting",
        "M Iyyer"
      ],
      "year": "2023",
      "venue": "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
      "doi": ""
    },
    {
      "id": "b452",
      "title": "Automatic detection of generated text is easiest when humans are fooled",
      "authors": [
        "D Ippolito",
        "D Duckworth",
        "C Callison-Burch",
        "D Eck"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b453",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S M Lundberg",
        "S.-I Lee"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b454",
      "title": "How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks",
      "authors": [
        "X Chen",
        "J Ye",
        "C Zu",
        "N Xu",
        "R Zheng",
        "M Peng",
        "J Zhou",
        "T Gui",
        "Q Zhang",
        "X Huang"
      ],
      "year": "2023",
      "venue": "How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks",
      "doi": ""
    },
    {
      "id": "b455",
      "title": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective",
      "authors": [
        "J Wang",
        "X Hu",
        "W Hou",
        "H Chen",
        "R Zheng",
        "Y Wang",
        "L Yang",
        "H Huang",
        "W Ye",
        "X Geng"
      ],
      "year": "2023",
      "venue": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective",
      "doi": ""
    },
    {
      "id": "b456",
      "title": "On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex",
      "authors": [
        "T Y Zhuo",
        "Z Li",
        "Y Huang",
        "Y.-F Li",
        "W Wang",
        "G Haffari",
        "F Shiri"
      ],
      "year": "2023",
      "venue": "On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex",
      "doi": ""
    },
    {
      "id": "b457",
      "title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts",
      "authors": [
        "K Zhu",
        "J Wang",
        "J Zhou",
        "Z Wang",
        "H Chen",
        "Y Wang",
        "L Yang",
        "W Ye",
        "N Z Gong",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts",
      "doi": ""
    },
    {
      "id": "b458",
      "title": "Exploring the robustness of large language models for solving programming problems",
      "authors": [
        "A Shirafuji",
        "Y Watanobe",
        "T Ito",
        "M Morishita",
        "Y Nakamura",
        "Y Oda",
        "J Suzuki"
      ],
      "year": "2023",
      "venue": "Exploring the robustness of large language models for solving programming problems",
      "doi": ""
    },
    {
      "id": "b459",
      "title": "Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors",
      "authors": [
        "R Han",
        "T Peng",
        "C Yang",
        "B Wang",
        "L Liu",
        "X Wan"
      ],
      "year": "2023",
      "venue": "Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors",
      "doi": ""
    },
    {
      "id": "b460",
      "title": "Evaluating the logical reasoning ability of chatgpt and gpt-4",
      "authors": [
        "H Liu",
        "R Ning",
        "Z Teng",
        "J Liu",
        "Q Zhou",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Evaluating the logical reasoning ability of chatgpt and gpt-4",
      "doi": ""
    },
    {
      "id": "b461",
      "title": "A comprehensive evaluation of chatgpt's zero-shot text-to-sql capability",
      "authors": [
        "A Liu",
        "X Hu",
        "L Wen",
        "P S Yu"
      ],
      "year": "2023",
      "venue": "A comprehensive evaluation of chatgpt's zero-shot text-to-sql capability",
      "doi": ""
    },
    {
      "id": "b462",
      "title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
      "authors": [
        "E Mitchell",
        "Y Lee",
        "A Khazatsky",
        "C D Manning",
        "C Finn"
      ],
      "year": "2023",
      "venue": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
      "doi": ""
    },
    {
      "id": "b463",
      "title": "A survey of adversarial defences and robustness in nlp",
      "authors": [
        "S Goyal",
        "S Doddapaneni",
        "M M Khapra",
        "B Ravindran"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys",
      "doi": ""
    },
    {
      "id": "b464",
      "title": "Adversarial attack and defense technologies in natural language processing: A survey",
      "authors": [
        "S Qiu",
        "Q Liu",
        "S Zhou",
        "W Huang"
      ],
      "year": "2022",
      "venue": "Neurocomputing",
      "doi": ""
    },
    {
      "id": "b465",
      "title": "Towards out-of-distribution generalization: A survey",
      "authors": [
        "Z Shen",
        "J Liu",
        "Y He",
        "X Zhang",
        "R Xu",
        "H Yu",
        "P Cui"
      ],
      "year": "2021",
      "venue": "Towards out-of-distribution generalization: A survey",
      "doi": ""
    },
    {
      "id": "b466",
      "title": "Textflint: Unified multilingual robustness evaluation toolkit for natural language processing",
      "authors": [
        "X Wang",
        "Q Liu",
        "T Gui",
        "Q Zhang",
        "Y Zou",
        "X Zhou",
        "J Ye",
        "Y Zhang",
        "R Zheng",
        "Z Pang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
      "doi": ""
    },
    {
      "id": "b467",
      "title": "Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study",
      "authors": [
        "Y Chen",
        "R Wang",
        "H Jiang",
        "S Shi",
        "R Xu"
      ],
      "year": "2023",
      "venue": "Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study",
      "doi": ""
    },
    {
      "id": "b468",
      "title": "A survey of evaluation metrics used for nlg systems",
      "authors": [
        "A B Sai",
        "A K Mohankumar",
        "M M Khapra"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys (CSUR)",
      "doi": ""
    },
    {
      "id": "b469",
      "title": "Large language models are state-of-the-art evaluators of code generation",
      "authors": [
        "T Y Zhuo"
      ],
      "year": "2023",
      "venue": "Large language models are state-of-the-art evaluators of code generation",
      "doi": ""
    },
    {
      "id": "b470",
      "title": "Multidimensional evaluation for text style transfer using chatgpt",
      "authors": [
        "H Lai",
        "A Toral",
        "M Nissim"
      ],
      "year": "2023",
      "venue": "Multidimensional evaluation for text style transfer using chatgpt",
      "doi": ""
    },
    {
      "id": "b471",
      "title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
      "authors": [
        "Y Liu",
        "D Iter",
        "Y Xu",
        "S Wang",
        "R Xu",
        "C Zhu"
      ],
      "year": "2023",
      "venue": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
      "doi": ""
    },
    {
      "id": "b472",
      "title": "Large language models are state-of-the-art evaluators of translation quality",
      "authors": [
        "T Kocmi",
        "C Federmann"
      ],
      "year": "2023",
      "venue": "Large language models are state-of-the-art evaluators of translation quality",
      "doi": ""
    },
    {
      "id": "b473",
      "title": "Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt",
      "authors": [
        "Q Lu",
        "B Qiu",
        "L Ding",
        "L Xie",
        "D Tao"
      ],
      "year": "2023",
      "venue": "Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt",
      "doi": ""
    },
    {
      "id": "b474",
      "title": "Chatgpt as a factual inconsistency evaluator for text summarization",
      "authors": [
        "Z Luo",
        "Q Xie",
        "S Ananiadou"
      ],
      "year": "2023",
      "venue": "Chatgpt as a factual inconsistency evaluator for text summarization",
      "doi": ""
    },
    {
      "id": "b475",
      "title": "Are large language models good evaluators for abstractive summarization?",
      "authors": [
        "C Shen",
        "L Cheng",
        "Y You",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Are large language models good evaluators for abstractive summarization?",
      "doi": ""
    },
    {
      "id": "b476",
      "title": "Gptscore: Evaluate as you desire",
      "authors": [
        "J Fu",
        "S.-K Ng",
        "Z Jiang",
        "P Liu"
      ],
      "year": "2023",
      "venue": "Gptscore: Evaluate as you desire",
      "doi": ""
    },
    {
      "id": "b477",
      "title": "On learning to summarize with large language models as references",
      "authors": [
        "Y Liu",
        "A R Fabbri",
        "P Liu",
        "D Radev",
        "A Cohan"
      ],
      "year": "2023",
      "venue": "On learning to summarize with large language models as references",
      "doi": ""
    },
    {
      "id": "b478",
      "title": "Humanlike summarization evaluation with chatgpt",
      "authors": [
        "M Gao",
        "J Ruan",
        "R Sun",
        "X Yin",
        "S Yang",
        "X Wan"
      ],
      "year": "2023",
      "venue": "Humanlike summarization evaluation with chatgpt",
      "doi": ""
    },
    {
      "id": "b479",
      "title": "Not all metrics are guilty: Improving nlg evaluation with llm paraphrasing",
      "authors": [
        "T Tang",
        "H Lu",
        "Y E Jiang",
        "H Huang",
        "D Zhang",
        "W X Zhao",
        "F Wei"
      ],
      "year": "2023",
      "venue": "Not all metrics are guilty: Improving nlg evaluation with llm paraphrasing",
      "doi": ""
    },
    {
      "id": "b480",
      "title": "Large language models are not fair evaluators",
      "authors": [
        "P Wang",
        "L Li",
        "L Chen",
        "D Zhu",
        "B Lin",
        "Y Cao",
        "Q Liu",
        "T Liu",
        "Z Sui"
      ],
      "year": "2023",
      "venue": "Large language models are not fair evaluators",
      "doi": ""
    },
    {
      "id": "b481",
      "title": "Multi-dimensional evaluation of text summarization with in-context learning",
      "authors": [
        "S Jain",
        "V Keshava",
        "S M Sathyendra",
        "P Fernandes",
        "P Liu",
        "G Neubig",
        "C Zhou"
      ],
      "year": "2023",
      "venue": "Multi-dimensional evaluation of text summarization with in-context learning",
      "doi": ""
    },
    {
      "id": "b482",
      "title": "Is chatgpt a good nlg evaluator? a preliminary study",
      "authors": [
        "J Wang",
        "Y Liang",
        "F Meng",
        "H Shi",
        "Z Li",
        "J Xu",
        "J Qu",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good nlg evaluator? a preliminary study",
      "doi": ""
    },
    {
      "id": "b483",
      "title": "Benchmarking foundation models with language-model-as-an-examiner",
      "authors": [
        "Y Bai",
        "J Ying",
        "Y Cao",
        "X Lv",
        "Y He",
        "X Wang",
        "J Yu",
        "K Zeng",
        "Y Xiao",
        "H Lyu"
      ],
      "year": "2023",
      "venue": "Benchmarking foundation models with language-model-as-an-examiner",
      "doi": ""
    },
    {
      "id": "b484",
      "title": "Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages",
      "authors": [
        "W Yang",
        "C Li",
        "J Zhang",
        "C Zong"
      ],
      "year": "2023",
      "venue": "Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages",
      "doi": ""
    },
    {
      "id": "b485",
      "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "authors": [
        "L Zheng",
        "W.-L Chiang",
        "Y Sheng",
        "S Zhuang",
        "Z Wu",
        "Y Zhuang",
        "Z Lin",
        "Z Li",
        "D Li",
        "E Xing"
      ],
      "year": "2023",
      "venue": "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "doi": ""
    },
    {
      "id": "b486",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "K Papineni",
        "S Roukos",
        "T Ward",
        "W.-J Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b487",
      "title": "Rouge: A package for automatic evaluation of summaries",
      "authors": [
        "C.-Y Lin"
      ],
      "year": "2004",
      "venue": "Text summarization branches out",
      "doi": ""
    },
    {
      "id": "b488",
      "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "authors": [
        "S Banerjee",
        "A Lavie"
      ],
      "year": "2005",
      "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization",
      "doi": ""
    },
    {
      "id": "b489",
      "title": "To ship or not to ship: An extensive evaluation of automatic metrics for machine translation",
      "authors": [
        "T Kocmi",
        "C Federmann",
        "R Grundkiewicz",
        "M Junczys-Dowmunt",
        "H Matsushita",
        "A Menezes"
      ],
      "year": "2021",
      "venue": "Proceedings of the Sixth Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b490",
      "title": "Bertscore: Evaluating text generation with bert",
      "authors": [
        "T Zhang",
        "V Kishore",
        "F Wu",
        "K Q Weinberger",
        "Y Artzi"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b491",
      "title": "Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance",
      "authors": [
        "W Zhao",
        "M Peyrard",
        "F Liu",
        "Y Gao",
        "C M Meyer",
        "S Eger"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b492",
      "title": "Bartscore: Evaluating generated text as text generation",
      "authors": [
        "W Yuan",
        "G Neubig",
        "P Liu"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b493",
      "title": "Codebertscore: Evaluating code generation with pretrained models of code",
      "authors": [
        "S Zhou",
        "U Alon",
        "S Agarwal",
        "G Neubig"
      ],
      "year": "2023",
      "venue": "Codebertscore: Evaluating code generation with pretrained models of code",
      "doi": ""
    },
    {
      "id": "b494",
      "title": "Ctrlsum: Towards generic controllable text summarization",
      "authors": [
        "J He",
        "W Kryści Ński",
        "B Mccann",
        "N Rajani",
        "C Xiong"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b495",
      "title": "Sentbs: Sentencelevel beam search for controllable summarization",
      "authors": [
        "C Shen",
        "L Cheng",
        "L Bing",
        "Y You",
        "L Si"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b496",
      "title": "Brio: Bringing order to abstractive summarization",
      "authors": [
        "Y Liu",
        "P Liu",
        "D Radev",
        "G Neubig"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b497",
      "title": "Toward human-like evaluation for natural language generation with error analysis",
      "authors": [
        "Q Lu",
        "L Ding",
        "L Xie",
        "K Zhang",
        "D F Wong",
        "D Tao"
      ],
      "year": "2022",
      "venue": "Toward human-like evaluation for natural language generation with error analysis",
      "doi": ""
    },
    {
      "id": "b498",
      "title": "Red-teaming large language models using chain of utterances for safety-alignment",
      "authors": [
        "R Bhardwaj",
        "S Poria"
      ],
      "year": "2023",
      "venue": "Red-teaming large language models using chain of utterances for safety-alignment",
      "doi": ""
    },
    {
      "id": "b499",
      "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
      "authors": [
        "D Ganguli",
        "L Lovitt",
        "J Kernion",
        "A Askell",
        "Y Bai",
        "S Kadavath",
        "B Mann",
        "E Perez",
        "N Schiefer",
        "K Ndousse"
      ],
      "year": "2022",
      "venue": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
      "doi": ""
    },
    {
      "id": "b500",
      "title": "Flirt: Feedback loop in-context red teaming",
      "authors": [
        "N Mehrabi",
        "P Goyal",
        "C Dupuy",
        "Q Hu",
        "S Ghosh",
        "R Zemel",
        "K.-W Chang",
        "A Galstyan",
        "R Gupta"
      ],
      "year": "2023",
      "venue": "Flirt: Feedback loop in-context red teaming",
      "doi": ""
    },
    {
      "id": "b501",
      "title": "Red teaming language models with language models",
      "authors": [
        "E Perez",
        "S Huang",
        "F Song",
        "T Cai",
        "R Ring",
        "J Aslanides",
        "A Glaese",
        "N Mcaleese",
        "G Irving"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b502",
      "title": "Frugalgpt: How to use large language models while reducing cost and improving performance",
      "authors": [
        "L Chen",
        "M Zaharia",
        "J Zou"
      ],
      "year": "2023",
      "venue": "Frugalgpt: How to use large language models while reducing cost and improving performance",
      "doi": ""
    },
    {
      "id": "b503",
      "title": "Batch prompting: Efficient inference with large language model apis",
      "authors": [
        "Z Cheng",
        "J Kasai",
        "T Yu"
      ],
      "year": "2023",
      "venue": "Batch prompting: Efficient inference with large language model apis",
      "doi": ""
    },
    {
      "id": "b504",
      "title": "Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering",
      "authors": [
        "Y Li"
      ],
      "year": "2023",
      "venue": "Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering",
      "doi": ""
    },
    {
      "id": "b505",
      "title": "Leancontext: Cost-efficient domain-specific question answering using llms",
      "authors": [
        "M A Arefeen",
        "B Debnath",
        "S Chakradhar"
      ],
      "year": "2023",
      "venue": "Leancontext: Cost-efficient domain-specific question answering using llms",
      "doi": ""
    },
    {
      "id": "b506",
      "title": "Time travel in llms: Tracing data contamination in large language models",
      "authors": [
        "S Golchin",
        "M Surdeanu"
      ],
      "year": "2023",
      "venue": "Time travel in llms: Tracing data contamination in large language models",
      "doi": ""
    },
    {
      "id": "b507",
      "title": "Can we trust the evaluation on chatgpt",
      "authors": [
        "R Aiyappa",
        "J An",
        "H Kwak",
        "Y.-Y Ahn"
      ],
      "year": "2023",
      "venue": "Can we trust the evaluation on chatgpt",
      "doi": ""
    },
    {
      "id": "b508",
      "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "authors": [
        "A Wang",
        "A Singh",
        "J Michael",
        "F Hill",
        "O Levy",
        "S R Bowman"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b509",
      "title": "Character-level convolutional networks for text classification",
      "authors": [
        "X Zhang",
        "J Zhao",
        "Y Lecun"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b510",
      "title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "authors": [
        "S Narayan",
        "S B Cohen",
        "M Lapata"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b511",
      "title": "Siren's song in the ai ocean: A survey on hallucination in large language models",
      "authors": [
        "Y Zhang",
        "Y Li",
        "L Cui",
        "D Cai",
        "L Liu",
        "T Fu",
        "X Huang",
        "E Zhao",
        "Y Zhang",
        "Y Chen"
      ],
      "year": "2023",
      "venue": "Siren's song in the ai ocean: A survey on hallucination in large language models",
      "doi": ""
    },
    {
      "id": "b512",
      "title": "A survey of hallucination in large foundation models",
      "authors": [
        "V Rawte",
        "A Sheth",
        "A Das"
      ],
      "year": "2023",
      "venue": "A survey of hallucination in large foundation models",
      "doi": ""
    },
    {
      "id": "b513",
      "title": "Chain-of-verification reduces hallucination in large language models",
      "authors": [
        "S Dhuliawala",
        "M Komeili",
        "J Xu",
        "R Raileanu",
        "X Li",
        "A Celikyilmaz",
        "J Weston"
      ],
      "year": "2023",
      "venue": "Chain-of-verification reduces hallucination in large language models",
      "doi": ""
    },
    {
      "id": "b514",
      "title": "Med-halt: Medical domain hallucination test for large language models",
      "authors": [
        "L K Umapathi",
        "A Pal",
        "M Sankarasubbu"
      ],
      "year": "2023",
      "venue": "Med-halt: Medical domain hallucination test for large language models",
      "doi": ""
    },
    {
      "id": "b515",
      "title": "Halueval: A large-scale hallucination evaluation benchmark for large language models",
      "authors": [
        "J Li",
        "X Cheng",
        "W X Zhao",
        "J.-Y Nie",
        "J.-R Wen"
      ],
      "year": "2023",
      "venue": "Halueval: A large-scale hallucination evaluation benchmark for large language models",
      "doi": ""
    },
    {
      "id": "b516",
      "title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
      "authors": [
        "B Peng",
        "M Galley",
        "P He",
        "H Cheng",
        "Y Xie",
        "Y Hu",
        "Q Huang",
        "L Liden",
        "Z Yu",
        "W Chen"
      ],
      "year": "2023",
      "venue": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "A Survey Of Gpt-3 Family Large Language Models Including Chatgpt And Gpt-4",
      "text": "Katikapalli Subramanyam Kalyan Akmmus Al, Trichy, India _Email: kalyan@akmmusai.pro, Website: [https://www.akmmusai.pro/kalyanksmlp_](https://www.akmmusai.pro/kalyanksmlp_)"
    },
    {
      "title": "Abstract",
      "text": "Large language models (LLMs) are a special class of pretrained language models obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI's GPT-3 model, and the popularity of LLMs is increasing exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-signing popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GPT-3 family large language models. Large Language Models, GPT-3, ChatGPT, GPT-4, Transformers, Survey."
    },
    {
      "title": "Contents",
      "text": "* 1 Introduction * 2 Foundation Concepts * 2.1 Transformer * 2.1.1 Traditional Deep Learning Models * 2.1.2 Drawbacks of Traditional Deep Learning Models * 2.1.3 Transformer Description * 2.2 Transfer Learning * 2.2.1 Why Transfer Learning? * 2.2.2 What is Transfer Learning? * 2.2.3 Transfer Learning vs Other Learning Paradigms * 2.3 Self-Supervised Learning * 2.3.1 Why Self-Supervised Learning? * 2.3.2 What is Self-Supervised Learning? * 2.3.3 Evolution of Self-Supervised Learning * 2.3.4 _Katibapalli Subramanyam Kalyan is with Abmmus AI as NLP Researcher and Founder, Tricho, Tamil Nadu, India, 620015. E-mail: kalyan@akmmusai.pro, Website: [https://www.akmmusai.pro/kalyanksmlp_](https://www.akmmusai.pro/kalyanksmlp_) * 3.4 Self-Supervised Learning * 3.5 Self-Supervised Learning * 3.6 Self-Supervised Learning * 3.7 Self-Supervised Learning * 3.8 Self-Supervised Learning * 3.9 Self-Supervised Learning * 4.1 Text Classification * 4.2 Information Extraction * 4.3 Question Answering * 4.4 Machine Translation * 4.5 Keyphrase Generation * 4.6 Dialogue Tasks * 4.7 Information Retrieval * 4.8 Recommendation Systems * 4.9 Coding Tasks * 4.10 Multimodal AI Tasks * 4.11 Machine Learning Tasks * 4.12 Planning"
    },
    {
      "title": "1 Introduction",
      "text": "Large Language Models (LLMs), the recent buzz in Artificial Intelligence, have garnered a lot of attention in both academic and industry circles with their remarkable performances in most of the natural language processing (NLP) tasks. These models are essentially deep learning models, specifically transformer-based, pretrained on large volumes of text data and then aligned to human preferences using meta-training. Pretraining provides universal language knowledge to the model [1], while meta-training aligns the model to act based on the user's intentions. Here user's intention includes both explicit intentions, like following instructions, and implicit intentions, like maintaining truthfulness and avoiding bias, toxicity, or any harmful behaviour [2]. Large language models (LLMs) are a special class of pretrained language models obtained by scaling model size, pretraining corpus and computation. For downstream task usage, pre-trained language models leverage supervised learning paradigm, which involves task-specific fine-tuning and hundreds or thousands of labelled instances [1, 3]. LLMs leverage in-context learning (ICL), a new learning paradigm which doesn't require task-specific fine-tuning and a large number of labelled instances [4]. LLMs treat any NLP task as a conditional text generation problem and generate the desired text output just by conditioning on the input prompt, which includes task description, test input and optionally, a few examples. Figure 1 shows the evolution of artificial intelligence from machine learning to large language models. In the beginning, NLP systems are predominantly rule-based. These rule-based models are built on top of domain expert-framed rules. As manual rule framing is a laborious, expensive process and also requires frequent changes, rules-based models are gradually replaced by machine models, which learn the rules automatically from the training data and completely avoid manual rule framing [1]. However, machine learning models require human intervention in the form of domain experts for feature engineering. The evolution of dense text vector representation models like Word2Vec [5], Glove [6], FastText [7] and the advancement of computer hardware like GPUs, NLP systems are built using traditional deep learning models like CNN [8], RNN [9], LSTM [10], GRU [11], Seq2Seq [12] and Attention-based Seq2Seq models [13, 14]. However, the drawbacks of these models like the inability to (i) capture long-term dependencies and (ii) leverage GPUs fully because of sequential processing (except in the case of CNN), resulted in the evolution of advanced deep learning models like Transformers [15], which are fully attention based without any recurrent and convolution layers. Inspired by the success of image-pretrained models [16, 17, 18] built on top of transfer learning and large convolution models, the research community focused on building pretrained language models (PLMs) like BERT [19] and GPT-1 [20] with transformers as the backbone and pretrained based on a new learning paradigm called self-supervised learning [1, 21, 22]. Unlike traditional deep learning models and vanilla transformers, which require training from scratch for downstream usage, pretrained language models can be easily adapted to downstream tasks with fine-tuning. The huge success of BERT and GPT-1 models triggered the development of other pretrained language models like RoBERTa, XLNet [23], ELECTRA [24], ALBERT [25], DeBERTa [26, 27], GPT-2 [28], T5 [29], BART [30] etc. Although PLMs have many advantages compared to traditional deep learning and vanilla transformer models, they still suffer from drawbacks like the inability to generalize to unseen tasks without task-specific training. So, the research community focused on developing more advanced models like large language models which can generalize to unseen tasks without any task-specific training. The era of LLMs started with GPT-3 [4], and the success of GPT-3 inspired the development of other LLMs like PaLM [31], Chinchilla [32], GLaM[33], LaMDA [34], Gopher [35], Megatron-Turing NLG [36][181], BLOOM [37], Galactica [38], OPT [39], LLaMA [40, 41] etc. The popularity of LLMs is increasing exponentially after the recent launch of Open AI's models like ChatGPT and GPT-4 [42]. For example, ChatGPT has garnered millions of users within a few weeks of its launch. Because of the ability to generalize to unseen tasks based on the task description and a few examples without requiring any task-specific training, just like humans, LLMs can be considered as a baby step towards Artificial General Intelligence [43]. In this survey paper, we mainly focus on Open AI LLMs like GPT-3 models, GPT-3.5 models (InstructGPT, ChatGPT etc.) and GPT-4, which we refer to as GPT-3 family large language models (GLLMs). This survey paper provides a comprehensive review of research works related to GLLMs in multiple dimensions. **Contributions.** The key contributions of this survey paper are * First survey paper to present a comprehensive review of GPT-3 family large language models (GLLMs) in multiple dimensions covering more than 350 recent research papers. * We discuss various foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. * We discuss GPT-3 family large language models in detail, starting from GPT-3 to the latest ChatGPT and GPT-4. * We discuss the performances of GLLMs in various downstream tasks and present a thorough discussion on the data labelling, and data augmentation abilities of GLLMs. * We discuss the robustness and the evaluation abilities of GLLMs. * We present multiple insightful future research directions which will guide the research community to improve the performances of GLLMs further. **Comparison with existing surveys.** The existing survey papers provide a review of large language models [44] and the relevant concepts like in-context learning [45], evaluation [46, 47], alignment with human values [48, 49], safety and trustworthiness [50], reasoning [51], challenges and applications [52], LLM compression [53] and multi-modal LLMs [54]. For example, Zhao et al. [44] are the first to provide a comprehensive of large language models. Unlike Zhao et al. [44], the other existing survey papers focus on specific concepts of LLMs. For example, the survey papers written by Dong et al. [45], Chang et al. [46], Wang et al. [48] and Huang et al. [51] focus on in-context learning, evaluation of LLMs, alignment of LLMs with human values and reasoning ability of LLMs respectively. Similarly, the survey papers written by Yin et al. [54] and Huan et al. [50] provide a review of multi-modal LLMs and the safety and trustworthiness of LLMs, respectively. However, there is no existing survey paper which provides a comprehensive survey of GPT-3 family large language models. With the ever-rising popularity of GPT-3 family large language models like GPT-3, InstructGPT, ChatGPT, GPT-4 etc. and a lot of research works using these models, there is a strong need for a survey paper which focuses exclusively on GPT-3 family large language models. **Papers collection.** For this survey paper, we gathered over 350 research papers that appeared online in the period of June 2020 to September 2023. Initially, we selected GLLMs like GPT-3, InstructGPT, Codex and GPT-4 papers as seed papers and collected all the citing papers. We also collected papers from popular venues like ACL, Fig. 1: Evolution of artificial intelligence from machine learning to large language models. EMNLP, COLING, AAAI, ICML, ICLR, NeurIPS etc and popular databases like Google Scholar and ScienceDirect using the keywords GPT-3, ChatGPT, GPT-3.5, Instruct-GPT, Codex and GPT-4. After removing the duplicate papers, we did a manual review to arrive at a final set of over 350 relevant research papers. **Survey paper organization.** The survey paper is organized as follows: Section 2 presents a brief overview of various foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. Section 3 presents GPT-3 family large language models in detail, starting from GPT-3 to the latest ChatGPT and GPT-4. Sections 4, 5, and 6 discuss the performances of GLLMs in various downstream tasks, specific domains and multilingual scenarios, respectively. Section 7 presents the data labelling and data augmentation abilities of GLLMs. Section 8 discusses various research works presenting approaches to detect text generated by GLLMs. Sections 9 and 10 discuss the robustness and evaluation abilities of GLLMs, respectively. Section 11 presents multiple insightful future research directions."
    },
    {
      "title": "2 Foundation Concepts",
      "text": ""
    },
    {
      "title": "_Transformer_",
      "text": ""
    },
    {
      "title": "2.1.1 Traditional Deep Learning Models",
      "text": "Before the evolution of the transformer model, most of the research in natural language processing involved deep learning models like multi-layer perceptron (MLP), convolutional neural network (CNN), recurrent neural network (RNN), long short-term memory (LSTM) network, gated recurrent unit (GRU), sequence-to-sequence and attention-based sequence-to-sequence [55]. MLP is a feed-forward neural network with three or more layers (input layer, one or more hidden layers, and output layer), and the neurons in these layers are fully connected. MLPs are easy to understand and simple to implement. However, as MLPs ignore the sequence information and struggle to capture the semantic relationships, these models are subsequently replaced by advanced models like CNN and RNN. CNN, originally developed to process images, is also explored for natural language processing tasks by treating text as a one-dimensional image [56, 8]. CNNs can learn local features (n-grams) effectively using convolution layers but struggle to capture long-term dependencies. RNNs evolved as a deep learning model exclusively to process sequential data like text, time series, etc [9]. RNNs can handle input with varying lengths and process sequential data by maintaining a hidden state to capture the context from previous inputs. However, RNNs suffer from vanishing gradients problems and struggle to capture long-term dependencies. LSTM [10] and GRU [11, 57] evolved as advanced RNN variants to address the issues with the vanilla RNN model. The gating mechanism in these models helps to regulate the flow of information along the sequence and retain the most important information. Compared to LSTM, which includes three gates (input, forget and output gates), GRU is more parameter efficient as it includes only two gates, namely the input and the reset gates. RNN and its variants like LSTM and GRU expect the input and output sequences to be the same length. However, in the case of natural language generation tasks like machine translation, text summarization, etc., the input and output sequences can be of different lengths. So, the researchers introduced the sequence-to-sequence (Seq2Seq) model to handle tasks with different input and output sequence lengths [12]. The Seq2Seq model is originally developed for machine translation and later explored for other NLP tasks. The Seq2Seq model consists of an encoder and decoder based on RNN, LSTM or GRU to process the input sequence and generate the output sequence. The encoder processes the input sequence to generate a fixed-size context vector based on which the decoder generates the output sequence. However, the fixed-size context vector fails to encode the entire information in the input sequence, especially when the input sequence is long [13]. The attention mechanism is introduced to address this issue, allowing the decoder to focus on the relevant input tokens at each decoding step [13, 14]. However, as the encoder and decoder of the Seq2Seq model are based on RNN and its variants, the Seq2Seq model suffers from vanishing gradients and struggles to capture long-term dependencies."
    },
    {
      "title": "2.1.2 Drawbacks Of Traditional Deep Learning Models",
      "text": "Here are the drawbacks of traditional deep learning models * _Lack of sequence and semantic understanding -_ MLPs ignore sequence information, treating all input tokens as independent. Moreover, MLPs can learn statistical patterns but struggle to capture semantic information in the input sequence. * _Computationally expensive -_ CNNs require a large number of parameters to achieve good results. Although LSTM and GRU address the limitations of vanilla RNNs to some extent, these models include a gating mechanism which significantly increases the number of model parameters. The large number of parameters makes these models computationally expensive to train and use. * _Vanishing gradients -_ RNN suffer from vanishing gradients problem. Although LSTM and GRU address this problem to some extent, these models also suffer from vanishing gradient problem and have difficulties in capturing long-term dependencies. * _Sequential Computation -_ RNN and its variants process the input sequence token by token, i.e. sequentially. This sequential computation is a bottleneck for these models to leverage parallel computing capability in advanced computing hardware like GPUs and TPUs. This sequential computation also slows down training and inference processes, especially for long sequences."
    },
    {
      "title": "2.1.3 Transformer Description",
      "text": "The transformer model evolved as an effective alternative to traditional deep learning models and addressed most associated issues [15]. In no time, the transformer model, with its novel and efficient architecture, gained a lot of popularity and became a de facto choice for building pretrained language models and large language models using self-supervised learning paradigm [44, 1]. The key ingredient behind the massive success of the transformer model is its self-attention mechanism. The self-attention mechanism allows the transformer model to process the input sequence without using recurrent or convolution layers. This attention mechanism also allows the model to effectively capture long-range dependencies in the input sequence, making it highly effective for natural language understanding and generation tasks. The transformer consists of encoder and decoder components. The encoder processes the input text using a stack of encoder layers and then produces rich contextualized vector representations for each token in the input sequence, which are later used by the decoder. Each encoder layer consists of a self-attention mechanism and a feedforward neural network. The self-attention mechanism adds contextual information to the token vectors by allowing each token to attend to all other input tokens, and this helps the model to capture long-term dependencies better. After the self-attention mechanism, the token vectors are passed through a feedforward neural network, which introduces non-linearity and further transforms the representations. In this way, each encoder layer applies self-mechanism and feed-forward network to add more contextual information to the token vector representations. The decoder receives the output from the last encoder layer and processes it sequentially by applying a stack of layers, with each decoder layer having masked self-attention, encoder-decoder self-attention and feed-forward neural network. The masked self-attention allows each token to attend to the previously generated tokens only and prevents the model from attending to future tokens. The encoder-decoder self-attention allows the decoder to attend to the encoded input sequence and helps the decoder focus on relevant input sequence tokens to generate the output tokens. The self-attention mechanism in the Transformer uses multiple attention heads, which allow the model to learn different aspects of relationships between tokens and encode more contextual information in the token representations. The encoder and decoder layers also include the embedding layer, residual connections and layer normalization. The embedding layer transforms input tokens into vector representations where each vector representation encodes both the meaning and position information. The residual connections and layer normalization are applied after the self-attention mechanism and feed-forward network. Residual connection avoids vanishing gradients and ensures a smooth flow of gradients, while layer normalization is applied to normalize the token representations and stabilize training. Apart from the embedding layer and stack of decoder layers, the decoder also includes an output layer. The output layer is nothing but a softmax layer that assigns probabilities to each token in the vocabulary, indicating the likelihood of each token being the next word in the generated sequence."
    },
    {
      "title": "_Transfer Learning_",
      "text": ""
    },
    {
      "title": "2.2.1 Why Transfer Learning?",
      "text": "Although machine learning models tasted some success, these models require feature engineering, which is a laborious and expensive process involving human intervention in the form of domain experts [1]. Deep learning models, essentially a subset of machine learning, don't require feature engineering as deep learning models learn features during training. Over the years, deep learning witnessed the evolution of various models like multi-layer perceptron (MLP), convolution neural networks (CNN), recurrent neural networks (RNN), long short-term memory networks (LSTM), gated recurrent unit networks (GRU), encoder-decoder networks, encoder-decoder with attention networks and recently transformers [55, 59]. Even though deep learning models eliminated the requirement of manual feature engineering and achieved significant progress, the main drawback with these models is the requirement of a large amount of labelled data to achieve good results. Along with developing various deep learning models, the research community also focused on developing high-quality datasets for various tasks [60]. However, manual data annotation is a time-consuming, expensive and laborious process. Additionally, when there is a change in the data distribution, it is essential to re-train deep learning models with new labelled data to maintain good performances [61]. To reduce the costs, the research community focused on how to effectively train deep learning models with limited labelled data. Transfer learning evolved as one of the effective solutions to train deep learning models with limited labelled data [58, 61]."
    },
    {
      "title": "2.2.2 What Is Transfer Learning?",
      "text": "Transfer Learning in the context of artificial intelligence involves existing knowledge transfer from one task (or domain) to another different but related task (or domain) [58, 61]. Transfer learning avoids training a model from scratch and helps improve the model's performance on the target task (or domain) by leveraging already existing knowledge. Transfer learning is largely based on the idea that when two tasks (or domains) are similar, the knowledge from the source task (or domain) with sufficient data can be used to enhance the performance of the target task (or domain) with limited data. For example, consider the task of sentiment analysis of reviews of different products. It is highly expensive to annotate large data separately for each product. In such cases, transfer learning helps to adapt the model trained on one product reviews to perform well on other product reviews without requiring large labelled data [62]. Transfer learning draws inspiration from human beings, i.e., human beings can do new tasks without or with few examples just by reusing previously gained knowledge [60]. Figure 2 illustrates real-life examples of knowledge transfer (transfer learning). For example, a person who can cycle can learn to ride a bike quickly with less effort. This is because riding a cycle and a bike involves a lot of common things like handling the balance, etc. Similarly, a person familiar with C programming language can learn Python programming language easily. This is because both C and Python are programming languages and share many common concepts. So, due to the ability to reuse the existing knowledge and train the target models with limited data, transfer learning evolved as a promising learning paradigm and eventually played a crucial role in the evolution of advanced deep learning models like pretrained language models [1, 3] and the recent large language models. Overall, the advantages of transfer learning are * Transfer learning helps to reduce the requirement of labelled data. (Data efficiency) * Transfer learning avoids training models from scratch by providing a good initialization from existing related models. (Faster training and development) * Transfer learning helps to enhance the performance on the target task (or domain) by reusing existing knowledge. (Enhance target task performance) * Transfer learning is explored across AI areas like computer vision, natural language processing, and speech processing. (Versatile) In conclusion, transfer learning is a powerful learning paradigm in artificial intelligence that has benefits regarding data efficiency, speed, performance, adaptability, and real-world practicality."
    },
    {
      "title": "2.2.3 Transfer Learning Vs Other Learning Paradigms",
      "text": "Along with transfer learning, the other learning paradigms that evolved to address large labelled data requirements are semi-supervised learning [63] and multi-task learning [64]. Semi-supervised learning is a learning paradigm in artificial intelligence that uses labelled and unlabelled data to train models [63]. As semi-supervised learning uses labeled and unlabelled data, it lies between unsupervised and supervised learning paradigms. As semi-supervised learning uses only a small amount of labelled data, it reduces the amount of labelled data required, like transfer learning. However, unlike transfer learning, where the distribution of source and target tasks can be different, in semi-supervised, the distribution of labelled and unlabelled data should be the same [58]. Multi-task learning is a learning paradigm which focuses on enhancing the performance of a group of tasks by leveraging the interconnections between the tasks and learning them simultaneously [63]. Unlike multi-task learning, which simultaneously learns all the tasks, transfer learning first learns the source task and then transfers the knowledge to the target task. In multi-task learning, the focus is generally on all the tasks, while transfer learning focuses more on the target task [61]."
    },
    {
      "title": "_Self-Supervised Learning_",
      "text": ""
    },
    {
      "title": "2.3.1 Why Self-Supervised Learning?",
      "text": "The main drawback with traditional deep learning models like CNN is the requirement of training from scratch. Training from scratch requires a large amount of labelled data. Data labelling is not only expensive but also a time-consuming and laborious process, which eventually makes the model development expensive. To reduce the requirement of labelled data and make the model development process less expensive, the computer vision research community focused on developing models like VGGNet [17], AlexNet [16] and GoogleNet [18] on top of large CNNs, transfer learning and supervised learning. These models are pretrained on a large number of labelled images from ImageNet dataset [65] using supervised learning, and then adapted to downstream Fig. 2: Real-life examples of knowledge transfer (transfer learning). Examples are inspired from [58]tasks. These pretrained models avoid training downstream models from scratch by providing a good initialization. Moreover, downstream models initialized from pretrained models converge faster and achieve good results even with limited labelled data [60]. Inspired by the huge success of pretrained image models, the NLP research community focused on developing pretrained language models [1, 3, 60]. However, the main challenge here is the use of supervised learning at scale to pretrain language models. This is because supervised learning at scale requires huge volumes of labelled data, which is almost impossible to obtain in many cases because of highly expensive annotation costs. Besides high annotation costs, supervised learning also suffers from generalization errors and spurious correlations [1, 22]. Self-supervised learning with the ability to automatically generate the labels and make use of unlabelled data evolved as an effective alternative to supervised learning to pretrain language models at scale [1, 21, 22]."
    },
    {
      "title": "2.3.2 What Is Self-Supervised Learning?",
      "text": "Self-supervised learning, a promising learning paradigm in artificial intelligence, helps models from different modalities like language, speech or image to learn background knowledge from large volumes of unlabeled data [21, 22]. Unlike supervised learning, which relies on large volumes of labelled data, self-supervised learning pretrains the models at scale based on the pseudo supervision offered by one or more pretraining tasks. Here, the pseudo supervision stems from the labels, which are automatically generated without human intervention based on the description of the pretraining task. In general, self-supervised learning involves one or more pretraining tasks [1, 3]. Moreover, the efficiency of self-supervised learning is heavily influenced by the choice of pretraining task [1, 24, 26]. Figure 3 presents the self-supervised learning paradigm. In the pretraining phase, the labels are automatically generated based on the description of pretraining tasks, and the models learn universal knowledge using the pseudo supervision offered by one or more pretraining tasks. Pretraining helps the models to gain strong background knowledge, which allows the models to provide a good initialization to downstream models. The initialization from pretrained models enhances the downstream models in terms of generalization, performance, and robustness and makes them data efficient. After pretraining, pretrained language models can be easily adapted to downstream tasks with limited labelled data, and large language models can be used to solve downstream tasks using in-context learning without any task-specific fine-tuning."
    },
    {
      "title": "2.3.3 Evolution Of Self-Supervised Learning",
      "text": "Figure 4 shows the evolution of self-supervised learning in natural language processing from embedding models to the recent large language models. The evolution of self-supervised learning in natural language processing happened in three stages, namely embedding models, pretrained language models and large language models. Initially, self-supervised learning is explored to develop non-contextual embedding models (e.g. Word2Vec [5], FastText [7]), followed by sentence embedding (e.g. Sent2Vec [66]) and contextual embedding models (e.g. ELMo [67]). The quest to develop pretrained models motivated NLP researchers to explore self-supervised learning to develop pretrained language models [1, 3, 60]. As pretrained language models cannot generalize to NLP tasks without fine-tuning, the NLP research community focused on developing large language models using self-supervised learning at a large scale [4, 40, 41, 42, 68]. To summarize, self-supervised is undergoing a rapid evolution and is also treated as a significant element in achieving near human-level intelligence [22]."
    },
    {
      "title": "2.3.4 Self-Supervised Learning Vs Other Learning Paradigms",
      "text": "Self-supervised learning, with its exceptional ability to make use of unlabelled data at scale, evolved as an alternative to supervised learning to pretrain models. However, self-supervised learning has similarities and dissimilarities with supervised learning [1]. Both self-supervised and supervised provide supervision. How Fig. 3: Illustration of self-supervised learning paradigm. ever, unlike supervised learning, which offers supervision based on human-labelled data, self-supervised learning offers supervision based on automatically generated data. Supervised learning is mostly used to train downstream models with task-specific data, while self-supervised learning is used to train pretrained models to offer good initialization to downstream models. Similarly, self-supervised learning has similarities and dissimilarities with unsupervised learning [1]. Both self-supervised learning and unsupervised learning make use of unlabelled data without requiring any labelled data. However, unlike self-supervised learning, which focuses on learning rich data representations using pseudo supervision, the main focus of unsupervised learning is to identify the hidden patterns in the data without any supervision."
    },
    {
      "title": "_Pretrained Language Models_",
      "text": ""
    },
    {
      "title": "2.4.1 Overview",
      "text": "Deep learning witnessed the evolution of several models, from convolution neural networks to the latest transformers [55, 59]. Transformer addressed drawbacks of traditional deep learning models like convolutional neural network, recurrent neural network and its variants and achieved significant progress [15, 69]. However, transformer and traditional deep learning models suffer from one major drawback: training from scratch, which requires large volumes of labelled data and makes model development expensive. Inspired by the success of pretrained image models like VGGNet [17], AlexNet [16] and GoogleNet [18] in computer vision, NLP researchers focused on developing pretrained models for natural language processing based on transformers and self-supervised learning [1, 3, 60, 70]. Pretrained language models are advanced deep learning models essentially transformer-based, pretrained on large volumes of text data and can be adapted to downstream tasks with limited labelled data. Along with transformer model, self-supervised learning and transfer learning are key concepts which make pretrained language models possible [1] (refer Figure 5). The era of pretrained language models started with GPT-1 [20] and BERT [19] models. The massive success of BERT and GPT-1 models triggered the development of other pretrained language models like RoBERTa [71], XLNet [23], ELECTRA [24], ALBERT [25], DeBERTa [26, 27], GPT-2 [28], T5 [29], BART [30], PEGASUS [72] etc."
    },
    {
      "title": "2.4.2 Evolution Of Pretrained Language Models",
      "text": "The evolution of pretrained language models happened along three dimensions: encoder-based models, decoder-based models and encoder-decoder based models [1]. Encoder-based models consist of an embedding layer and stack of encoder layers, with each encoder layer having self-attention and feed-forward networks. Encoder-based models are primarily used for natural language understanding tasks like text classification, entity extraction, relation extraction, etc. Some of the popular encoder-based pretrained language models are BERT, RoBERTa, XLNet, ALBERT, ELECTRA, DeBERTa, etc. Decoder-based models consist of an embedding layer and a stack of decoder layers, with each decoder layer having self-attention, masked self-attention and feed-forward networks. Decoder-based models are used for both natural language understanding and generation tasks. Some of the popular decoder-based pretrained language models are GPT-1, GPT-2 etc. Encoder-decoder based models consist of both encoder and decoder modules. In general, encoder-decoder based models are used for natural language generation tasks like machine translation, text summarization, etc., while some are explored for both natural language understanding and generation tasks. Some of the popular encoder-decoder based models are T5, BART, PEGASUS, M2M100, NLLB, etc. After the massive success of pretrained language models in the English language, the research community started to develop multilingual pretrained language models [73] and pretrained language models for non-English languages [1]. Some of the popular multilingual pretrained language models are mBERT [19], mT5 [74], mBART [75], IndicBERT [76], XLM [77], XLM-R [78], mDeBERTa [26] etc. As the performance of general domain pretrained language models is limited in domain Fig. 4: Evolution of self-supervised learning in natural language processing. specific tasks [1, 3], the research community focused on developing pretrained language models for specific domains like social media [79, 80], finance [81, 82, 83], legal [84, 85], coding [86, 87, 88], healthcare [89, 90, 91] etc., As pretrained language models have millions of parameters which make model fine-tuning and deployment expensive, compact pretrained language models like DistilBERT [92], TinyBERT [93], MobileBERT [94], MiniLM [95]etc., are developed. As pretrained language models have a limited context length which limits the performance on long sequences, long-sequence pretrained language models like LongFormer [96], BigBird [97] etc., are developed. Pretrained language models encode only the universal language knowledge available in the pre-training corpus and lack valuable knowledge available in ontologies. So, the research community developed ontology-enriched models like SapBERT [98], UmlsBERT [99], etc."
    },
    {
      "title": "_Large Language Models_",
      "text": ""
    },
    {
      "title": "2.5.1 Overview",
      "text": "The pretrained language models, starting from GPT-1 [20], BERT [19] models to the latest DeBERTa [26, 27], achieved significant progress and also reduced the amount of labelled data required to train the task-specific models [1, 3]. Pretrained language models follow the paradigm \"pretrain then fine-tune\", i.e., the model is pretrained first and then adapted to downstream tasks by fine-tuning. As task-specific fine-tuning is mandatory to adapt the pretrained language model to downstream tasks, pretrained language models cannot generalize to unseen downstream tasks without task-specific fine-tuning. Moreover, task-specific fine-tuning requires labelled data and creates a separate copy of the pretrained language model for each downstream NLP task, increasing the model development and deployment costs [1]. Pretrained language models are treated as narrow AI systems as they are adapted through fine-tuning and then used for specific downstream tasks. However, the main focus of the research community is to develop artificial general intelligence systems [43, 100] which are not narrowly focused on specific tasks but have the ability for general problem-solving and can handle even the unseen tasks by utilizing the existing knowledge like human beings. The NLP researchers observed that the performance of pretrained language models can be enhanced further through scaling along three dimensions: pretraining computation, pretraining data and model size [28, 29, 71]. Large size allows the models to capture more nuanced language patterns, which in turn enhances their ability to understand and generate text, while large pretraining data helps the model to learn from a wider range of text. The promising results from scaling and the quest to build artificial general intelligence systems motivated NLP researchers to build much bigger and bigger models, which eventually resulted in the evolution of GPT-3 and its successor models [31, 32, 4, 33]. Learning paradigms like transfer learning and self-supervised learning make large language models possible, but scaling makes these models powerful. The research community coined a new phrase, \"large language models\", to refer to GPT-3 and its successor large models to differentiate these models from small pretrained language models [44]. Large language models (LLMs) are a special class of pretrained language models obtained by scaling model size, pretraining corpus and computation as shown in Figure 6. Large language models (LLMs) are essentially deep learning models, specifically transformer-based, pretrained on large volumes of text data and aligned to human preferences using meta-training. Pretraining provides universal language knowledge to the model [1], while meta-training aligns the model to act based on the user's intentions. Here, the user's intention includes explicit intentions, like following instructions, and implicit intentions, like maintaining truthfulness and avoiding bias, toxicity, or harmful behaviour [2]. Because of their large size and pretraining on large volumes of text data, LLMs exhibit special abilities referred to as emerging abilities [101, 102], allowing them to achieve remarkable performances without any task-specific training in many natural language processing tasks. For downstream task usage, pretrained language models leverage supervised learning paradigm, which involves task-specific fine-tuning and hundreds or thousands of labelled instances [1, 3]. LLMs leverage in-context learning (ICL), a new learning paradigm that doesn't require task-specific fine-tuning and many la Fig. 5: Key ingredients in the evolution and success of pretrained language models. -belled instances [4, 45]. LLMs treat any NLP task as a conditional text generation problem and generate the desired text output by conditioning on the input prompt, including task description, test input and optionally, a few examples."
    },
    {
      "title": "2.5.2 Evolution Of Large Language Models",
      "text": "The evolution of large language models happened along two dimensions: closed-source LLMs and open-source LLMs. The era of LLMs roughly started with GPT-3. Following the success of GPT-3, Open AI developed successor models like InstructGPT [2], Codex [103], Chat-GPT and GPT-4 [42]. Google introduced models like GLaM [33], PaLM [31], PaLM2 [68], LaMDA [34] and Bard. DeepMind developed models like Gopher [35], Chinchilla [32], AlphaCode [104] and Sparrow [105]. Companies like Baidu, AI21 labs and Amazon developed the models Ernie 3.0 Titan [106], Jurassic-1 [107] and AlexaTM [108], respectively. Although the performances of closed-source LLMs are impressive, the main drawback with these models is that they are behind the paywalls, i.e., their weights are not publicly available, only some of them are accessible only through the APIs offered by the respective companies, and the model usage is charged based on the tokens processed and generated. To address this issue, the research community focused on developing open-source LLMs with publicly available weights. Some of the popular open-source LLMs are OPT [39], OPT-IML [109], Galactica [38], LLaMA [40], LLaMA2 [41] and Falcon. The performances of these open-source LLMs are on par with closed-source LLMs. Moreover, in some cases, open-source LLMs outperform closed-source LLMs. For example, Galactica beats closed-source LLMs like GPT-3, Chinchilla and PaLM. Inspired by the success of open-source LLMs in the English language, the research community focused on developing multilingual and bilingual LLMs. BLOOM [37] and BLOOMZ [110] are examples of multilingual LLMs, JAIS [111] (English and Arabic), GLM [112] (English and Chinese) and FLM-101B [113] (English and Chinese) are examples of bilingual LLMs. The success of closed and open-source LLMs in the general domain triggered the development of domain-specific LLMs like FinGPT [114] and BloombergGPT [115] in the finance domain, MedPaLM [116] and MedPaLM2 [117] in the healthcare domain and StarCoder [118], CodeLiaMa [119], CodeGen [120] and CodeGen2 [121] in the coding domains. For example, Bloomberg developed BloombergGPT, an exclusive LLM for the finance domain. Similarly, Google developed MedPaLM and MedPaLM2 LLMs exclusively for the healthcare domain based on PaLM and PaLM2 models respectively. Similarly, HuggingFace developed StarCoder, MetaAI developed Code LlaMA, and SalesForce developed CodeGen and CodeGen2 LLMs exclusively for coding tasks."
    },
    {
      "title": "3 Gpt-3 Family Large Language Models",
      "text": ""
    },
    {
      "title": "_Overview_",
      "text": "Open AI, an AI company established in 2015, focused on building generative models. The Open AI researchers initially explored RNNs for developing generative language models [122]. Inspired by the huge success of the transformer model and its ability to capture long-term dependencies, Open AI researchers leveraged the transformer decoder to build GPT-1 (117M parameters), the first-ever transformer-based pretrained language model [20]. GPT-1 introduced a new paradigm, \"pretrain and fine-tune\", to develop downstream task models effectively. Originally, the \"pretrain and fine-tune\" paradigm was introduced by Dai et al. [123] and then explored by Howard and Ruder [124] to build language models for text classification. However, unlike Radford et al. [20] work, these research works build language models based on LSTM, which lacks parallelization ability and has difficulties in capturing long-term dependencies. Radford et al. [20] used casual language modeling as a pretraining task to pretrain the GPT-1 model. The casual language modeling pretraining task involves generating the next token based on the previous tokens. GPT-1 achieved SOTA results in 9 out of 12 NLP tasks [20]. Fig. 6: Key ingredients in the evolution and success of large language models. Inspired by the success of GPT-1, Open AI researchers introduced the GPT-2 model to push the results further [28]. The GPT-2 model is pretrained on the WebText corpus (40B text), which is much larger than the Books corpus used to pretrain the GPT-1 model. The authors developed four versions of the GPT-2 model with varying parameters: 117M, 345M, 762M and 1.5B. The authors observed that the perplexity decreases with an increase in the model's size, and even for the largest version of 1.5B, the decrease in perplexity did not exhibit saturation. This revealed that GPT-2 underfitted the pretraining dataset, and extending the training duration could have further reduced perplexity. This observation triggered the insight that \"developing even larger language models will decrease the perplexity further and enhance natural language understanding and generation capabilities\". The insights gained from the GPT-1 and GPT-2 models laid a strong foundation for the evolution of the GPT-3 family large language models, including the latest models like ChatGPT and GPT-4. Figure 7 shows the journey of Open AI starting from GPT-1 to the latest GPT-4 and Figure 8 shows the GPT-3 family large language models starting from GPT-3 series to the latest GPT-4."
    },
    {
      "title": "_Gpt-3 Models_",
      "text": "The experiment results of GPT-2 showed that increasing the model size further reduces the perplexity, and the model with more parameters achieves better results than the models with fewer parameters. This observation motivated Open AI researchers to train much bigger GPT models, which eventually resulted in the introduction of the GPT-3 model [4]. GPT-3 model contains 175B Fig. 8: GPT-3 family large language models (GLLMs) starting from GPT-3 series to the latest GPT-4. Here, SFT stands for supervised fine-tuning, and RLHF stands for reinforcement learning from human feedback. Here, raw represents that the model is just pretrained and is not aligned using SFT or RLHF. Here, RLHF-Chat represents that the model is aligned using RLHF and optimized for chat. Fig. 7: Open AI journey starting from GPT-1 to the latest GPT-4. parameters and is 100 times bigger than its predecessor model, GPT-2. Moreover, the GPT-3 model is trained over a corpus with the text from multiple sources like webpages, Wikipedia and books, unlike GPT-1 and GPT-2 models, which are pretrained over corpora with the text from books and webpages, respectively. Scaling in three dimensions: pretraining data, model size, and pretraining computation allows the GPT-3 model to learn more from large volumes of texts from different sources, which eventually empowers the model to handle unseen tasks without any task-specific training. Unlike GPT-1 and GPT-2 models, which leverage supervised learning to do downstream tasks, GPT-3 leverages training-free in-context learning. In-context learning is a new learning paradigm that is training-free and solves the downstream tasks by using knowledge encoded in the model parameters [45]. In-context learning accepts prompts as input where the input prompt consists of task descriptions, optimally few examples and other instructions."
    },
    {
      "title": "_Gpt-3.5 Models_",
      "text": "Two main drawbacks of the GPT-3 model are (i) GPT-3 is not trained over code data, and hence, it lacks complex reasoning abilities like solving math problems [44], and (ii) GPT-3 model struggles to follow user instructions and sometimes generate harmful text [2]. These two drawbacks are addressed by GPT-3.5 models. Brown et al. [4] observed that GPT-3 can generate simple programs, although it is not specifically trained for generating code. The Open AI researchers triggered by this observation introduced Codex [103], an exclusive GLLM for coding tasks. Codex is developed by fine-tuning a GPT model with 12B parameters over publicly available Github code. Moreover, it is observed that GPT models explicitly trained over code data exhibit better reasoning capabilities. During pretraining, the GPT-3 model is optimized based on the casual language modeling objective, which involves predicting the next word based on the previous words. In-context learning during inference can be viewed as conditional text generation, where the model generates the output by conditioning on the given prompt. The model performs text generation during pretraining and inference, but it does vanilla text generation during pretraining and conditional text generation during inference. During pretraining, the model conditions on the previous words and generates the next word, i.e., vanilla text generation. However, during in-context learning, the model conditions on the prompt and generates the answer rather than generating the next words, i.e., conditional text generation. So, there is a gap between pretraining and in-context learning at inference. Due to this, in many cases during inference, the GPT-3 model fails to understand the given prompt and tends to generate the next words. The pretraining corpus of the GPT-3 model includes some amount of text with undesired qualities like misinformation, abuse, hate, sexism, etc., due to which the model sometimes generates harmful text. To enhance complex reasoning ability, the instruction following ability and reduce the harmful text generation, GPT-3.5 models are developed by fine-tuning GPT-3 models over code data and then aligned using supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF) [2]. For example, the text-davinci-002 model is developed by fine-tuning the GPT-3 model (text-davinci) over code data to get code-davinci-002, which is further aligned using SFT."
    },
    {
      "title": "_Chatgpt And Gpt-4_",
      "text": "GPT-3 models are capable of understanding and generating natural language, while GPT-3.5 models are capable of understanding and generating both natural language and code. However, both GPT-3 and GPT-3.5 models are not chat optimized. This drawback is addressed by ChatGPT (GPT-3.5-turbo) and GPT-4 [42] models. Open AI introduced ChatGPT in November 2022. With extraordinary conversational abilities, ChatGPT, ChatGPT has garnered millions of users within a few weeks of its launch. Following ChatGPT, Open AI released the GPT-4 model in March 2023, which can handle both text and image inputs. Apart from generating text with human-like fluency, these models further pushed the results in many natural language processing tasks. The performance of these models in downstream tasks and specific domains is discussed in detail in Sections 4 and 5."
    },
    {
      "title": "4 Performance Of Gllms In Downstream Tasks",
      "text": ""
    },
    {
      "title": "_Text Classification_",
      "text": "**Overview.** Text Classification is one of the fundamental tasks in natural language processing [145]. It involves assigning label(s) from a predefined set of labels to a given piece of text. Here, the piece of text can be a phrase, sentence, paragraph or even a document. Many of the natural language processing problems, like offensive language identification, stance detection, sentiment analysis, hate speech detection, etc., are approached as text classification. Text Classification can be binary, multi-class or multi-label. In the case of text classification, the large language model is prompted with a task description, a predefined set of labels, examples (optional) and the test input. Here, task description, a predefined set of labels and examples constitute the context. The model understands what actually the task is from the context and then assigns the most appropriate label(s) to the given test input. The additional inputs, like examples in the context, enrich the prompt with more information which allows the model to understand the task better and then perform better. **Research works exploring GLLMs for text classification.** The recent works explored GLLMs like GPT-3, GPT-3.5 ChatGPT and GPT-4 for various text classification problems like sentiment analysis [128, 129, 132, 134, 136, 142, 144], stance detection [125], intent classification [143], mental health analysis [126, 127], hate speech detection [139, 140], misinformation detection [132], paraphrase detection [134], news classification [136], natural language inference [134, 137, 138]etc. The evaluation is done in zero and few-shot settings using different prompting strategies like chain-of-thought (CoT) [125, 134, 137, 138, 141, 144], self-question prompting (SQP) [138], clue and reasoning prompting (CARP) [144] etc. Most of the research works focused on English datasets, except a few research works focused on other languages like Chinese [128], Slovenian [131], Indonesian [132], Javanese [132], and Buginese [132]. A brief summary of research works exploring GLLMs for various text classification problems is presented in Table 1. Most of the research works showed that compared to direct prompting, advanced prompting strategies help the model to achieve better results. This is because advanced prompting involves generating intermediate outputs, which in turn guide the model in generating the correct final output. Zhang et al. [125] explored the ChatGPT model with direct and chain-of-thought prompting for stance detection in tweets in zero and few-shot settings. Experiment results on three datasets showed that one-shot chain of thought prompting outperforms zero-shot direct prompting and also achieves near state-of-the-art results. Yang et al. [127] designed emotion-enhanced CoT prompting to combine emotion information with the power of CoT prompting for mental health analysis tasks. Experiments on five different mental health analysis tasks showed that ChatGPT with emotion-enhanced CoT outperforms other prompting strategies. Overall, ChatGPT outperforms traditional deep learning models like CNN and RNN but still lags behind task-specific fine-tuned models. Wu et al. [137] explored models like GPT-4 and ChatGPT for radiology natural language inference task. The authors reported \\begin{table} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline **Paper** & **Task(s)** & **GLLMs Explored** & **Prompt Settings** & **Domain(s)** & **Language(s)** & **SOTA Results** \\\\ \\hline [125] & Stance Detection & ChatGPT & ZS, FS & Social Media & English & No \\\\ \\hline [126] & Stress Detection, Depression Detection, Suicidal Detection & ChatGPT & ZS & Social Media & English & No \\\\ \\hline [127] & Mental Health Analysis Tasks & ChatGPT & ZS & Social Media & English & No \\\\ \\hline [128] & Sentiment Analysis & ChatGPT & ZS, FS & Social Media & English, Chinese & No \\\\ \\hline [129] & Stock Prediction based on Sentiment Analysis & ChatGPT & ZS & Finance & English & No \\\\ \\hline [130] & Computational Social Science Tasks & GPT-3, ChatGPT & ZS & Social Media & English & No \\\\ \\hline [131] & Genre Identification & ChatGPT & ZS & General & English, & No \\\\ \\hline [132] & Sentiment Analysis, Misinformation Detection & ChatGPT & ZS & Social Media & English, & No \\\\ & & & & & Indonesian, & \\\\ & & & & & Japanese, & \\\\ \\hline [133] & Nine NLU tasks including Sentiment Analysis and Natural Language Inference & ChatGPT & ZS & General, & English & No \\\\ \\hline [134] & Paraphrase Detection, Sentiment Analysis, Natural Language Inference & ChatGPT & ZS,FS & General & English & No \\\\ \\hline [135] & Sentiment Analysis, Natural Language Inference & GPT-3, GPT-3.5, ChatGPT & ZS, FS & General, & English & No \\\\ \\hline [136] & Financial News Classification, Sentiment Analysis & ChatGPT, GPT-4 & ZS & Finance & English & No \\\\ \\hline [137] & Natural Language Inference & ChatGPT, GPT4 & ZS,FS & Healthcare & English & No \\\\ \\hline [138] & Natural Language Inference, Document Classification & GPT3.5, GPT4, Bard & ZS, FS & Healthcare & English & No \\\\ \\hline [139] & Hate Speech Detection & GPT-3 & ZS, FS & Social Media & English & No \\\\ \\hline [140] & Implicit Hate Speech Detection & ChatGPT & ZS & Social Media & English & No \\\\ \\hline [141] & Clinical Text Classification & GPT-3, ChatGPT, GPT-4 & ZS, FS & Healthcare & English & No \\\\ \\hline [142] & Sentiment Analysis, Suicide Tendency Detection, Personality Prediction & ChatGPT & ZS & Social Media & English & No \\\\ \\hline [143] & Intent Classification & GPT-3 & ZS & Social Media & English & No \\\\ \\hline [144] & News Classification, Sentiment Analysis & InstructGPT & ZS, FS & General, & English & Yes \\\\ & & & & Social Media & \\\\ \\hline \\end{tabular} \\end{table} TABLE 1: Summary of research works exploring GLLMs for various text classification problems. Here ZS represents zero-shot, and FS represents few-shot. that GPT-4 with IRSA prompting strategy outperforms ChatGPT in both zero and few-shot settings. IRSA stands for Instruction Response Semantic Alignment. IRSA prompting strategy is almost the same as direct prompting except that in the case of IRSA prompting, the model is instructed to give the labels \"contain\" and \"not contain\" instead of \"entainment\" and \"not entailment\", just to reduce the complexity. Wang et al. [138] evaluated the performances of the latest LLMs like GPT-3.5, GPT-4, and Bard models on text classification tasks like natural language inference and document classification in the healthcare domain. The GPT-4 model with the newly designed self-question prompting (SQP) outperforms other models in both zero and few-shot settings. The SQP strategy involves identifying the key elements of input, generating questions and answers related to the key elements, and then using them to generate the final output. Parikh et al. [143] showed that the performance of the GPT-3 model for intent classification in zero-shot settings can be enhanced by including intent class descriptions in the prompt. Some of the research works demonstrated that GPT-3 family large language models can outperform task-specific fine-tuned models [131, 134] and domain-specific LLMs [136]. Kuzman et al. [131] showed that ChatGPT outperforms fine-tuned XLM-R model in the task of automatic genre identification in the English language. Zhong et al. [134] compared the performances of ChatGPT and fine-tuned models based on base and large versions of BERT and RoBERTa models on tasks like natural language inference, sentiment analysis and paraphrase identification. The results showed that ChatGPT outperforms both base and large fine-tuned models by a large margin in the case of natural language inference task. Li et al. [136] evaluated the performances of general LLMs like ChatGPT and GPT-4 and domain-specific LLMs like BloombergGPT on tasks like finance news classification and sentiment analysis. In the case of finance news classification, GPT-4 outperforms all other LLMs, including the domain-specific BloombergGPT model. In all the above discussed research works, the performance of GLLMs is impressive but still lags behind SOTA results. Sun et al. [144] showed that it is possible to achieve SOTA results in text classification tasks with the newly designed clue And reasoning prompting (CARP) prompting strategy. CARP involves a progressive reasoning approach for handling complex linguistic phenomena, and it involves three steps: finding clues based on input, generating reasoning steps based on the input and the generated clues, and then arriving at the final output based on the input, generated clues and reasoning steps. Experiment results showed that the results are impressive as InstructGPT with CARP prompting strategy using just 16 examples achieves SOTA results on four text classification datasets."
    },
    {
      "title": "_Information Extraction_",
      "text": "**Overview.** Information Extraction (IE) in natural language processing involves extracting structured data like entities, relationships and events from unstructured text data [164]. Transforming unstructured text data into structured data enables efficient data processing, knowledge discovery, decision making and enhances information retrieval and search. Information extraction involves a number of tasks like entity typing, entity extraction, relation classification, relation extraction, event detection, event argument extraction and event extraction [153]. Entity typing (ET) involves classifying identified named entity mentions into one of the predefined entity types [165]. Named Entity Recognition (NER) or Entity Extraction (EE) involves identifying entity mentions and then assigning them to appropriate entity types [166]. Relation classification (RC) involves identifying the semantic relationship between the given two target entities in a sentence [167]. Relation Extraction (RE) involves extracting the entities and then classifying the semantic relationship between the two target entities, i.e., involves entity extraction followed by relation classification [168]. Event Detection (ED) aims to identify and categorize words or phrases that trigger events [169]. Event Argument Extraction (EAE) involves identifying event arguments, i.e., entities involved in the event and then classifying their roles [170]. Event Extraction (EE) aims to extract both the events and the involved entities, i.e., it involves event detection followed by event argument extraction [171]. **Research works exploring GLLMs for information extraction tasks** The recent works explored GPT-3 family large language models for various information extraction tasks like entity typing [153], entity extraction [136, 146, 147, 148, 149, 153, 158, 159, 160, 162], relation classification [148, 149, 153, 154, 155, 156, 151, 152, 153], relation extraction [151, 152, 153, 154, 155, 158, 161], event classification [153], event argument extraction [153] and event extraction [158, 150, 148, 150]. The evaluation is done in zero and few-shot settings using different prompting strategies like chain-of-thought (CoT) [138, 152, 156, 152], self-verification [159], self-question prompting (SQP) [138], event ranking (ER) [152] etc. Most of the research works focused on English datasets, except a few research works focused on other languages like Chinese [148]. A brief summary of research works exploring GLLMs for various information extraction tasks is presented in Table II. Hu et al. [147] demonstrated the performance of ChatGPT in extracting clinical entities like problem, treatment, and test can be enhanced by including additional information about entity types like synonyms and subtypes in the prompt. Wei et al. [148] proposed ChatIE, a two-stage framework for information extraction, with each stage implemented as a multi-turn question answering. This two-stage framework helps the model break complex IE tasks into sub-tasks which allows the model to perform better. Results showed that ChatGPT used with the ChatIE framework outperforms vanilla ChatGPT by a large margin of more than 18 points. Gutierrez et al. [149] enhanced the performance of the GPT-3 model for entity extraction and relation classification by using techniques like contextual calibration [172] to reduce bias and kNN-based demonstration selection. Gao et al. [150] examined the performance of ChatGPT for event extraction in few-shot settings. The model is prompted with task descriptions, definitions of event types, positive and negative examples, and test input. The authors reported that including negative examples decreases the performance of the model, which is in line with other existing works [173]. The possible reason for this is that the model misunderstands negative examples as positive examples. Rehana et al. [151] explored GPT-3 family models like GPT-3, ChatGPT and GPT-4 for protein-protein interaction extraction. It is reported that including normalized protein names in the prompt enhances the performance of the model. However, fine-tuned PubMedBERT model outperforms GPT-4 model with an F1-score of 86.47. Yuan et al. [152] demonstrated that advanced prompting strategies like event ranking and chain-of-thought improve the performance of ChatGPT compared to vanilla prompting in temporal relation extraction. However, ChatGPT lags behind traditional neural networks like LSTM and fine-tuned pre-trained language models, which indicates the toughness of the temporal relation extraction task. Wang et al. [138] evaluated the performances of the latest LLMs like GPT-3.5, GPT-4, and Bard models on entity extraction and relation classification in the clinical domain. Experiment results showed that GPT-4 with self-question prompting outperforms other LLMs on most of the datasets. Li et al. [162] compared the performances of both natural language and code \\begin{table} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline **Paper** & **Task(s)** & **GLMs Explored Settings** & **Prompt** & **Domain(s)** & **Langage(s)** & **SOTA Results** \\\\ \\hline [146] & Entity Extraction & ChatGPT & ZS & General & English & No \\\\ \\hline [147] & Entity Extraction & GPT-3, ChatGPT & ZS & Healthcare & English & No \\\\ \\hline [148] & Entity Extraction, Event Extraction, Relation Classification & ChatGPT & ZS & General & English, Chinese & No \\\\ \\hline [149] & Entity Extraction, Relation Classification & GPT-3 & FS & Healthcare & English & No \\\\ \\hline [150] & Event Extraction & ChatGPT & FS & General & English & No \\\\ \\hline [151] & Protein-Protein Interaction Extraction & GPT-3, ChatGPT and GPT-4 & - & ZS & Healthcare & English & No \\\\ \\hline [152] & Temporal Relation Extraction & ChatGPT & ZS & General & English & No \\\\ \\hline [153] & Entity Typing, Entity Extraction, Relation Classification, Reaction Extraction, Event Detection, Event Extraction & ChatGPT & ZS & General & English & No \\\\ \\hline [154] & Temporal Relation Classification, Causal Relation Classification, Discourse Relation Classification & ChatGPT & ZS, FS & General & English & No \\\\ \\hline [155] & Relation Classification & GPT-3.5 & FS & General, Scientific Literature & English & Yes \\\\ \\hline [156] & Relation Classification & GPT-3.5 & FS & General, Scientific Literature & English & Yes \\\\ \\hline [157] & Entity Extraction & GPT-3.5, ChatGPT & ZS & General & English & No \\\\ \\hline [135] & Entity Extraction, Relation Extraction and Event Detection & GPT-3, GPT-3, ChatGPT & ZS, FS & General, Social Eedia & English & No \\\\ \\hline [158] & Entity Extraction, Relation Extraction and Event Detection & InstructGPT & FS & General & English & Yes \\\\ \\hline [159] & Entity Extraction & GPT-3 & FS & General & English & No \\\\ \\hline [138] & Entity Extraction, Relation Classification & GPT-3.5, GPT-4 & ZS, FS & Healthcare & English & No \\\\ \\hline [160] & Entity Extraction & GPT-3 & ZS & General & English & No \\\\ \\hline [161] & Relation Extraction & GPT-3 & FS & General, Healthcare & English & No \\\\ \\hline [136] & Entity extraction & ChatGPT, GPT-4 & FS & Finance & English & No \\\\ \\hline [162] & Entity Extraction, Relation Extraction & GPT-3, Codex & FS & General, Scientific Literature & English & No \\\\ \\hline [163] & Relation Classification & GPT-3.5, ChatGPT & ZS & General & English & No \\\\ \\hline \\end{tabular} \\end{table} TABLE II: Summary of research works exploring GLLMs for information extraction tasks. Here ZS represents zero-shot, and FS represents few-shot. LIMs like GPT-3 and Codex using natural language and code style prompts. Experiment results showed that (i) Codex outperforms GPT-3 model and moderately sized fine-tuned models and (ii) Codex model with natural language or code style prompt outperforms GPT-3 model (iii) Code style prompts achieves better results in case of both Codex and GPT-3 models. The possible explanation for this is Codex which is pretrained over large volumes of code, encode structured code information which is useful for IE tasks as IE tasks involve structured outputs. Zhang et al. [163] proposed the QA4RE framework, which frames relation extraction as a question-answering problem. In the QA4RE framework, the sentence serves as context, and the relation types serve as options from which the LLMs choose. Experiment results showed that the proposed approach improves the performance of ChatGPT and GPT-3.5 models by a good margin in relation extraction. Some of the research works [155, 156, 158] demonstrated that GPT-3 family models can achieve SOTA results in information extraction tasks. Wan et al. [156] achieved SOTA results in relation extraction with the GPT-RE framework. GPT-RE framework overcomes the drawbacks in existing works using entity-aware demonstration retrieval based on fine-tuned model and gold label-induced reasoning. The use of representations from fine-tuned relation model for demonstration selection is more effective as they naturally include entity and relation information. Ma et al. [158] proposed a \"filter then rerank\" approach to use both fine-tuned models and LLMs to take advantage of the strengths of both models for few-shot information extraction. Here fine-tuned model acts as a filter while LLM acts as a re-ranker. The proposed approach achieves SOTA results with an average improvement of over 2 points in the F1 score."
    },
    {
      "title": "_Question Answering_",
      "text": "**Overview.** Question Answering (QA) is an important natural language processing task which deals with the development of algorithms to understand and interpret user queries in natural language and then deliver accurate responses [174, 175]. The main aim of question answering systems is to enhance human-computer interaction, i.e., QA systems avoid the use of complex commands and allow the user to interact with machines in a more natural way through natural language queries. For example, popular AI assistants like Amazon Alexa1, Google Assistant2 and Apple Siri3 rely on QA to provide accurate answers to user queries. The option of interaction through natural language queries enhances the reach of technology to a broader audience. QA can be treated as a fine-grained version of information retrieval [176], and the demand for QA systems is increasing day by day because of the ability to generate answers which are accurate, relevant and short. Footnote 1: [https://alexa.amazon.com](https://alexa.amazon.com) Footnote 2: [https://assistant.google.com](https://assistant.google.com) Footnote 3: [https://www.apple.com/in/siri/](https://www.apple.com/in/siri/) **Research works exploring GLLMs for question answering tasks.** The NLP research community explored GLLMs for question answering in various domains like education [177, 184], news [180], healthcare [182, 183, 185, 186, 190, 191, 193, 195], social media [135], coding [187], legal [188, 194], finance [136] and scientific literature [189]. Most of the research works focused on the English language, except a few research works focusing on languages like Portuguese [177], Japanese [191, 195] and Chinese [193]. As advanced prompting methods allow GLLMs to perform well, some of the research works investigated the effectiveness of advanced prompting strategies like chain-of-thought [183, 177, 189, 183, 195], self-question prompting [138, 193] and holistically thought [193] for question answering. Table III presents a summary of research works exploring GLLMs for question answering across various domains and languages. Zheng et al. [181] studied the shortcomings of ChatGPT in answering complex open-domain questions and found errors related to understanding, factual accuracy, specificity, and logical reasoning. They also analyzed the importance of knowledge memorization, recall, and reasoning abilities in addressing these failures. The authors demonstrated that providing the model with external knowledge, cues for knowledge recall, and guidance for logical reasoning can enhance its ability to provide more accurate answers. Samaan et al. [182] examined the accuracy of ChatGPT in answering questions related to Bariatric surgery. The authors reported that ChatGPT correctly answered 131 questions from 151 questions, i.e., ChatGPT achieves an accuracy of 86.8%. The impressive performance of ChatGPT shows that it can serve as an additional information resource in addition to healthcare professionals and reduce their burden in answering patient questions. Holmes et al. [183] compared the performances of GLLMs like ChatGPT, GPT-4 with other LLMs like Bard, BLOOMZ and medical physicists in answering related questions to Radiation Oncology Physics. The performance of GPT-4 is very impressive as the model outperforms medical physicists and other LLMs like ChatGPT, Bard and BLOOMZ. The performance of GPT-4 is further enhanced using CoT prompting, i.e., the model is prompted to arrive at the answer after step-by-step reasoning. Nori et al. [185] performed a comprehensive evaluation of the GPT-4 model on medical question answering in zero and few-shot settings. For evaluation, the authors used six datasets: two related to the United States Medical License Examination (USMLE) exam and four from the MultiMedQA benchmark [116]. The performance of GPT-4 is very impressive as it outperforms not only general LLM like GPT-3.5 but also medical domain-specific LLM like Med-PaLM [116]. Moreover, on USMLE exam datasets, GPT-4 model score is 20 points more than the passing score. Hamidi et al. [186] evaluated ChatGPT and Claude in answering patient-specific medical questions from MIMIC-III clinical notes. Experiment results demonstrated that the performances of both models are promising as these models display significant levels of coherence, accuracy, coverage and relevance in their answers. Li et al. [136] demonstrated that GPT4 achieves the best results for question answering in the finance domain and outperforms ChatGPT, domain-specific models like BloombergGPT, FinQANet and general LLMs like OPT (66B), and BLOOM (176B). Although the performance of GLLMs is impressive in zero and few-shot settings in multiple choice question answering, these models still lag behind SOTA results. The main reason for this is the use of cloze prompts. In cloze prompts, the model is prompted with only question without answer options, so the model generates the answers just by conditioning on the question. Robinson et al. [192] proposed a new prompting strategy called multiple choice prompt which prompts the model with question and answer options so that the model generates the answer by conditioning on both question and answer options. Evaluation on 20 datasets showed that multiple-choice prompt helps GLLMs to achieve near SOTA results. Some of the research works explored the effectiveness of GLLMs in answering exam questions from various domains. Nunes et al. [177] investigated the performances of GLLMs like GPT-3.5, ChatGPT and GPT-4 in answering questions from the Brazilian university admission exam. Here all the questions are in Brazilian Portuguese language. The authors explored different prompting strategies like vanilla (zero-shot and few-shot) and CoT (few-shot). The authors observed that GPT-4 outperforms all other models by a large margin \\begin{table} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline **Paper** & **Task(s)** & **GLMs Explored Settings** & **Prompt Settings** & **Domain(s)** & **Language(s)** & **SOTA Results** \\\\ \\hline [177] & Admission Exam Question Answering & GPT-3.5, ChatGPT, GPT-4 & ZS, FS & Education & Brazilian Portuguese & No \\\\ \\hline [178] & Knowledge-based Complex Question Answering & GPT-3, GPT-3.5, ChatGPT & ZS & General & Multiple languages & No \\\\ \\hline [179] & Knowledge-based Visual Question Answering & GPT-3 & ZS & General & English & Yes \\\\ \\hline [180] & Tabular Question Answering & GPT-3 & ZS, FS & News & English & No \\\\ \\hline [181] & Open Domain Question Answering & ChatGPT & ZS & General & English & No \\\\ \\hline [182] & Bariatric Surgery Question Answering & ChatGPT & ZS & Healthcare & English & No \\\\ \\hline [183] & Radiation Oncology Physics Question Answering & ChatGPT, GPT-4 & ZS & Healthcare & English & No \\\\ \\hline [184] & Computer Science Question Answering & ChatGPT & ZS & Education & English & No \\\\ \\hline [185] & Medical Question Answering & GPT-3.5, GPT-4 & ZS, FS & Healthcare & English & No \\\\ \\hline [186] & Patient-specific Question Answering & ChatGPT & ZS & Healthcare & English & No \\\\ \\hline [132] & Question Answering & ChatGPT & ZS & General & English & Yes \\\\ \\hline [157] & Boolean Question Answering & ChatGPT & ZS & General & English & No \\\\ \\hline [133] & Multiple Choice Question Answering & ChatGPT & ZS & General, Social Media & English & No \\\\ \\hline [135] & Question Answering & GPT-3, GPT-3.5, ChatGPT & ZS, FS & General & English & No \\\\ \\hline [187] & Multiple Choice Code Question Answering & GPT-3.5 & ZS & Coding & English & No \\\\ \\hline [188] & Bar Exam Question Answering & GPT-3.5 & ZS & Legal & English & No \\\\ \\hline [189] & Multi-Document Question Answering & GPT-3.5 & FS & General, Scientific Literature & English & No \\\\ \\hline [190] & Plastic Survey Exam Question Answering & ChatGPT & ZS & Healthcare & English & No \\\\ \\hline [191] & Japanese Medical Exam Question Answering & GPT-3.5, GPT-4 & FS & Healthcare & Japanese & No \\\\ \\hline [136] & Financial Question Answering & ChatGPT, GPT-4 & ZS & Finance & English & No \\\\ \\hline [138] & Medical Question Answering & GPT-3.5, GPT4 & ZS, FS & Healthcare & English & No \\\\ \\hline [192] & Multiple Choice Question Answering & GPT-3, Codex, InstructGPT & ZS & General & English & No \\\\ \\hline [193] & Medical Conversational Question Answering & GPT-3, InstructGPT & ZS & Healthcare & English, Chinese & No \\\\ \\hline [194] & Question Answering & GPT-3 & ZS & Multiple domains & English & No \\\\ & & & & \\begin{tabular}{} \\end{tabular} & & \\\\ \\hline \\hline [195] & Japanese Medical Exam Question Answering & GPT-3, ChatGPT, GPT-4 & FS & Healthcare & Japanese & No \\\\ \\hline \\end{tabular} \\end{table} TABLE III: Summary of research works exploring GLLMs for question answering tasks. Here ZS represents zero-shot, and FS represents few-shot. of over 11 points and achieves the best results with CoT prompting in few-shot settings. Joshi et al. [184] evaluated ChatGPT in answering undergraduate-level computer science exam questions. For the evaluation, the authors gathered (i) questions from various computer science subjects like data structures, operating systems, machine learning and database management systems, (ii) questions from the GATE exam and (iii) programming questions from the Leetcode website. The results showed that ChatGPT is inconsistent in answering the questions, so students are not advised to rely on ChatGPT completely for their assignments and exams. Bommarito et al. [188] examined the ability of OpenAI's text-davinci-003 (GPT-3.5) model in answering multiple choice questions from the Bar Exam. Interestingly, human participants with extensive education and specialized training achieved a 68% accuracy rate, while the GPT-3.5 model achieved a lower accuracy rate of 50.3%. Gupta et al. [190] evaluated how effective ChatGPT is in answering questions from plastic surgery inservice training examination. The authors reported that ChatGPT achieves an accuracy of 54.96% by correctly answering 242 questions. Tanaka et al. [191] evaluated the performances of GLLMs like GPT-3.5 and GPT-4 in answering questions from the Japanese National Medical Licensing Examination (NMAE). Here the input includes sample examples, instructions to translate the question into English, and then summarizing the question before answering. The authors reported that GPT-4 achieves a score better than the minimum passing score, and further analysis showed that the incorrect answers are due to insufficient medical knowledge and insufficient information about the Japanese-specific medical system. Kasai et al. [195] reported that GPT-4 outperforms other models and passes the Japanese national medical licensing exam in the last six years. Moreover, ChatGPT with English-translated prompts achieves better results than ChatGPT with Japanese prompts. This is because ChatGPT is predominantly trained over the English text corpus. Some of the research works explored GLLMs for more challenging tasks in question answering like tabular question answering [180], knowledge-based complex question answering [178], multiple choice code question answering [187], multi-document question answering [189] and conversational question answering [193]. Srivastava et al. [180] evaluated the effectiveness of GPT-3 for question answering on tabular data in zero and few-shot settings. Here the model is prompted with unstructured passage text, tabular data in JSON format, examples (in the case of few-shot) and the question. The authors reported that GPT-3 displayed its ability to successfully locate the table, comprehend its structure, and accurately access the relevant cells or passages of text in order to provide answers to the given questions. Savelka et al. [187] evaluated the effectiveness of GPT-3.5 models in answering multiple-choice questions (MCQs), particularly those involving code snippets from programming courses. Experiment results showed that MCQs with code snippets have lower success rates compared to those without code, indicating a challenge in answering multiple-choice questions with code snippets. Pereira et al.[189] presented Visconde, a novel framework based on the GPT-3.5 model to tackle multi-document question answering. Visconde follows a three-step process involving decomposition, retrieval, and aggregation. The decomposition phase uses the GPT-3.5 model in few-shot settings for question simplification, the retrieval stage uses the SOTA model to select the relevant text chunks, and the final aggregation phase uses the GPT-3.5 with few-shot CoT prompting to get the answer. The authors observed that CoT prompting, i.e., generating reasoning steps before generating the final answer, enhances the performance. Weng et al. [193] enhanced the performance of GLLMs in answering medical conversational questions in English and Chinese using a novel prompt strategy called Holistically Thought (HoT). The HoT prompting strategy involves diffused thinking and focused thinking strategies to generate high-quality responses. Diffused thinking helps to generate various responses through diversified decoding, focused thinking generates a concise medical summary based on the dialogues and the final response is generated based on the dialogues, outputs of diffused thinking and focused thinking. Unlike all the above discussed research works where the performances of GLLMs are just satisfactory but not SOTA, some of the research works [179, 132] demonstrated that it is possible to achieve SOTA results for question answering task using GLLMs. For example, Yang et al. [179] explored GPT-3 model for knowledge-based visual question answering. Knowledge-based visual question answering involves answering questions which require information which is not available in the input images. The authors propose a novel approach which uses GPT-3 as a knowledge source which is implicit and unstructured. Experiment results showed that the proposed approach achieves new SOTA results by outperforming existing approaches with a large margin of over 8 points."
    },
    {
      "title": "_Machine Translation_",
      "text": "**Overview.** Machine Translation (MT), an important task of natural language processing, deals with the development of models which can translate input text from the source language to the target language [209, 210, 211]. MT models receive the input text in the source language, understand the syntax and semantics of the input text and then generate the translation in the target language. So, a good machine translation model should possess strong natural language understanding and generation skills to generate quality translations. The main objective of MT systems is to enhance cross-lingual communication by reducing the gap between individuals from different linguistic communities. The evolution of MT systems started with rule-based models followed by statistical and neural models [211]. Rule-based MT systems are built on top of manually crafted syntactic and grammatical rules. As manually framing rules is heavily laborious and expensive, these systems are later replaced by statistical MT systems. Statistical MT systems use statistical models trained on bilingual data. With the evolution of deep learning models, the research community started to build neural machine translation (NMT) systems with the help of neural models [12, 14, 212]. These neural models are essentially based on the encoder-decoder architecture, where the encoder understands the input sequence and encodes it into a vector, and the decoder, based on the encoder output, generates the output sequence auto-regressively. Some of the recent neural models used for translation are mBART-50 [213], M2M100 [214], NLLB200 [215] etc. **Research works exploring GLLMs for machine translation.** In recent times, GLLMs like ChatGPT and GPT-4 demonstrated remarkable performances in both natural language understanding and generation tasks. A good machine translation system requires strong natural language understanding and generation skills. As ChatGPT and GPT-4 possess strong natural language understanding and generation skills, the research community investigated the effectiveness of these models for machine translation across various domains like news [197, 198, 199, 200, 201], healthcare [197, 198], social media [199, 200, 201], dialogue [199, 200, 201] and e-commerce [199, 200]. Most of the research works focused on sentence-level machine translation [202, 203, 204, 205, 206, 207, 208, 196, 209, 200, 202, 204, 205, 206, 207, 208], except a few research works focused on paragraph-level machine translation [203, 204] and document-level machine translation [199, 201]. As advanced prompting methods allow GLLMs to perform well, some of the research works investigated the effectiveness of advanced prompting strategies like pivot [198], chain-of-thought [207] and multi-aspect prompting and selection [206]. Table 4 presents a summary of research works exploring GLLMs for machine translation across various domains and languages. Gu et al. [196] proposed a novel approach based on ChatGPT to enhance the quality of translation from Japanese to Chinese by effectively handling attribute clauses using a pre-edit scheme. The proposed approach, which integrates the pre-edit scheme with a novel two-step prompting strategy, enhances the translation quality by more than 35%. Peng et al. [197] explored the impact of temperature, task and domain information on the translation performance of ChatGPT. The authors showed that (i) ChatGPT performance degrades with an increase in temperature, and hence it is recommended to use a lower temperature (recommended is 0). and (ii) including task and domain information in the prompt \\begin{table} \\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \\hline **Paper** & **GLLMs Explored** & **Prompt Settings** & **Domain(s)** & **Language(s)** & **Granularity** & **Outperforms Commercial** \\\\ \\hline [196] & ChatGPT & ZS & General & Japanese, Chinese & Sentence & No \\\\ \\hline [197] & ChatGPT & ZS & General, News, Healthcare Romanian & English, Chinese, German, Romanian & Sentence & No \\\\ \\hline [198] & ChatGPT, GPT-4 & ZS & General, Healthcare \\&Social Media & English, Chinese, German, Romanian & Sentence & Yes \\\\ \\hline [199] & InstructGPT, ChatGPT, GPT-4 & ZS,FS & News, Social Media, E-Commerce, Dialogue & English, German, Chinese Document & Sentence, Document & Yes \\\\ \\hline [200] & ChatGPT & ZS, FS & General, News, Social Media, Dialogue, E-Commerce & English, French, Spanish & Sentence & Yes \\\\ \\hline [201] & ChatGPT, GPT-4 & ZS & General, Social Media, News, Dialogue & English, German, Russian & Document & Yes \\\\ \\hline [202] & ChatGPT & ZS, FS & General & 102 Languages in 202 directions & Sentence & No \\\\ \\hline [203] & ChatGPT & ZS & General & English, Chinese, French & Paragraph & No \\\\ \\hline [132] & ChatGPT & ZS & General & Twelve languages, including four low-resource languages & Sentence & No \\\\ \\hline [204] & GPT-3.5 & ZS & General & 18 language Pairs, including Japanese, English and Polish & Sentence, Paragraph & Yes \\\\ \\hline [205] & GPT-3.5 & ZS, FS & General & English, Arabic, Chinese, German, Spanish & Sentence & Yes \\\\ \\hline [206] & GPT-3.5 & ZS, FS & General & English, Chinese, Japanese, German, French & Sentence & No \\\\ \\hline [207] & GPT-3.5, GPT-4 & ZS & General & English, German, Chinese & Sentence & Yes \\\\ \\hline [208] & GPT-3.5 & ZS & General & English, German, Russian & Sentence & Yes \\\\ \\hline \\end{tabular} \\end{table} TABLE 4: Summary of research works exploring GLLMs for machine translation. Here ZS represents zero-shot, and FS represents few-shot. enhances the performance of ChatGPT consistently for both high and low language translations. Zhu et al. [202] evaluated the performance of ChatGPT and other LLMs like OPT, BLOOM and XGLM on 102 languages in 202 translation directions. The authors reported that ChatGPT comprehensively outperforms other LLMs but still lags behind neural machine translation models like NLLB in the majority of the translation directions. Further analysis showed three errors, namely hallucination, monotonic translation and off-target translation. Lyu et al. [203] presented some interesting research directions with respect to using LLMs for machine translation. The presented interesting research directions include stylized machine translation, interactive machine translation and translation memory-based machine translation. Neural machine translation systems just focus on source-target text mapping, which results in a lot of errors. Unlike neural machine translation systems, the human translation process involves intermediate steps to ensure high translation quality. Inspired by the human translation process, He et al. [206] proposed MAPS, which involves three steps: knowledge mining, knowledge integration and knowledge selection to generate quality translations. Extension evaluation of the WMT22 test set shows that MAPS improves the performance of models like GPT-3.5 and Alpaca and also addresses the hallucination issue by resolving 59% of hallucination errors. In all the above discussed research works, the performances of GLLMs are just satisfactory but not on par or beyond the performances of commercial machine translation systems. Some of the research works [198, 199, 201, 204, 205, 207, 208] showed that it is possible to outperform commercial machine translation systems using GLLMs. For example, Jiao et al. [198] investigated the translation capabilities of GLLMs like ChatGPT and GPT-4 and compared the performance with commercial systems like Google Translate, DeepL Translate and Tencent TranSmart. Extensive evaluation of multiple datasets showed that (i) the performance of GLLMs is on par with commercial systems in the case of high resources languages only, and (ii) the translation quality of low-resource languages can be enhanced using a novel pivot prompting strategy, which involves translating into high resource language before translating into the target low resource language. The naive prompts are unable to elicit the translation ability of ChatGPT fully. So, Gao et al. [200] focused on developing advanced prompting strategies by including additional information like task information, domain information and syntactic information like PoS (parts of speech) tags. The authors showed that ChatGPT, with the proposed advanced prompting strategy, achieves promising results and even outperforms commercial systems like Google Translate and DeepL Translate. Wang et al. [201] examined the performances of ChatGPT and GPT-4 for document-level machine translation and also compared the results with commercial systems from Google, DeepL and Tencent. The authors reported that GLLMs do well when the sentences in the document are combined and given at once to the model. Moreover, with this prompting strategy, both the GLLMs exhibit better performances than commercial machine translation systems according to human evaluation and also outperform most document-level neural machine translation methods in terms of d-BLEU scores. Karpinska et al. [204] explored the GPT-3.5 model for paragraph-level machine translation. The authors experimented with three different prompting strategies, namely translating sentence by sentence in isolation, translating sentence by sentence in the presence of the rest of the paragraph and translating the entire paragraph at once. After extensive evaluation of 18 language pairs, including English and Japanese, the authors report that translating the entire paragraph at once outperforms other strategies and commercial systems like Google Translate. Raunak et al. [208] examined the differences between the translations generated by GLLMs like GPT-3.5 and NMT systems like Microsoft Translator. The authors reported that GLLM generated translations are less literal, with better scores."
    },
    {
      "title": "_Keyphrase Generation_",
      "text": "**Overview.** Keyphrase generation (KPG) involves generating a set of phrases that capture the main ideas of a document [225]. The primary advantage of KPG over keyphrase extraction is the ability to generate both extractive and abstractive keyphrases. Keyphrase generation is approached as a sequence-to-sequence generation task [226, 227, 12] in the existing works. The current state-of-the-art model for keyphrase generation is, KeyBART [227], which is based on BART and trained using the text-to-text generation paradigm. Table V presents a summary of research works exploring GLLMs for keyphrase generation. **Research works exploring GLLMs for keyphrase generation.** Martinez et al. [216] performed a comprehensive evaluation of ChatGPT as a keyphrase generator by evaluating its performance on six datasets using six candidate prompts. The authors reported that the results are promising, but ChatGPT struggles in the case of generating absent keyphrases. Song et al. [217] evaluated ChatGPT on multiple datasets from news and scientific literature domains having both short and long documents. Experiment results showed that ChatGPT outperforms KeyBART [227], the SOTA model, on all the datasets."
    },
    {
      "title": "_Dialogue Tasks_",
      "text": "**Overview.** Dialogue tasks in natural language processing (NLP) deal with understanding and generating human-like conversations between machines and users [228]. The main objective of these tasks is to enable machines to have conversations with humans in a natural way. These dialogue tasks are essential components of building effective conversational agents, which have a wide range of applications, including customer support [228, 229]. **Research works exploring GLLMs for dialogue tasks.** The research community explored GLLMs like GPT-3, GPT-3.5 and ChatGPT for various dialogue tasks like dialogue summarization [157, 220, 221], dialogue question answering [224], emotion dialogue understanding and generation [219], dialogue state tracking [218], dialogue generation [132], and dialogue discourse analysis [223]. Some of the research works explored LLMs for the evaluation of dialogue tasks [222]. Most of the research works focused on general domain and English language datasets, except a few research works which focused on the medical domain [220] and languages like Chinese [223, 224]. Table VI presents a summary of research works exploring GLLMs for various dialogue tasks. Pan et al. [218] reported that ChatGPT exhibits better performance in dialogue state tracking compared to spoken language understanding. Further, the authors showed that the performance of ChatGPT can be enhanced by (i) using a multi-turn interactive prompt for dialogue state tracking and (ii) providing additional details like slot names, examples and descriptions for slot filling in spoken language understanding. Zhao et al. [219] explored the emotion dialogue capabilities of ChatGPT by evaluating the model on five different tasks, namely emotion recognition, emotion cause recognition, dialogue act classification (emotion dialogue understanding), empathetic response generation and emotion support generation. It is reported that ChatGPT exhibits better performances in emotion dialogue generation compared to emotion dialogue understanding. Chintagunta et al. [220] showed that the in-house model trained on GPT-3 generated summaries achieves performances comparable to when trained on human-generated summaries. Further, the in-house model trained on mixed summaries (human-generated and GPT-3 generated) achieves better performances than those trained on either one of the summaries. Prodan et al. [221] proposed a scoring system to choose the best examples for dialogue summarizing using few-shot GPT-3. The proposed scoring system enhances the quality of generated summaries with an 11% reduction in failures. Huynh et al. [222] studied the impact of various aspects influencing the performance of LLMs as Dialog evaluators. The authors reported that the performance as a dialogue evaluator largely depends on the diversity and relevance of the datasets used for instruction tuning. Fan et al. [223] investigated the effectiveness of ChatGPT for dialogue discourse analysis by evaluating its performance on three tasks, namely topic segmentation, discourse parsing and discourse relation recognition. ChatGPT's performance is promising in the case of topic segmentation, and CoT prompting enhances the performance. Wang et al. [224] proposed a novel approach based on explicit CoT prompting and demonstration selection to answer dialogue questions in few-shot settings. \\begin{table} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline **Paper** & **Tasks(s)** & **GLLMs Explored Settings** & **Prompt Settings** & **Domain(s)** & **Language(s)** \\\\ \\hline [218] & \\begin{tabular}{c} Spoken Language Understanding and Dialogue \\\\ State Tracking \\\\ \\end{tabular} & \\begin{tabular}{c} GPT-3.5, ChatGPT \\\\ \\end{tabular} & \\begin{tabular}{c} ZS \\\\ \\end{tabular} & General & English \\\\ \\hline [219] & \\begin{tabular}{c} Emotion Dialogue Understanding and Generation \\\\ Tasks \\\\ \\end{tabular} & \\begin{tabular}{c} ChatGPT \\\\ \\end{tabular} & \\begin{tabular}{c} ZS, FS \\\\ \\end{tabular} & General & English \\\\ \\hline [220] & \\begin{tabular}{c} Dialogue Summarization \\\\ \\end{tabular} & \\begin{tabular}{c} GPT-3 \\\\ \\end{tabular} & \\begin{tabular}{c} ZS \\\\ \\end{tabular} & Healthcare & English \\\\ \\hline [132] & \\begin{tabular}{c} Dialogue Generation \\\\ \\end{tabular} & \\begin{tabular}{c} ChatGPT \\\\ \\end{tabular} & \\begin{tabular}{c} ZS \\\\ \\end{tabular} & General & English \\\\ \\hline [157] & \\begin{tabular}{c} Dialogue Summarization \\\\ \\end{tabular} & \\begin{tabular}{c} ChatGPT \\\\ \\end{tabular} & \\begin{tabular}{c} ZS \\\\ \\end{tabular} & General & English \\\\ \\hline [221] & \\begin{tabular}{c} Dialog Evaluation \\\\ \\end{tabular} & \\begin{tabular}{c} GPT-3 \\\\ \\end{tabular} & FS & General & English \\\\ \\hline [222] & \\begin{tabular}{c} Dialogue Discourse Analysis \\\\ \\end{tabular} & \\begin{tabular}{c} ChatGPT \\\\ \\end{tabular} & \\begin{tabular}{c} ZS, FS \\\\ \\end{tabular} & General & \\begin{tabular}{c} English, \\\\ Chinese \\\\ \\end{tabular} \\\\ \\hline [224] & \\begin{tabular}{c} Dialogue Question Answering \\\\ \\end{tabular} & \\begin{tabular}{c} ChatGPT \\\\ \\end{tabular} & \\begin{tabular}{c} ZS, FS \\\\ \\end{tabular} & General & \\begin{tabular}{c} English, \\\\ Chinese \\\\ \\end{tabular} \\\\ \\hline \\end{tabular} \\end{table} TABLE VI: Summary of research works exploring GLLMs for various dialogue tasks. Here ZS represents zero-shot, and FS represents few-shot. \\begin{table} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline **Paper** & **GLLMs Explored Settings** & **Prompt Settings** & **Domain(s)** & **SOTA Results** \\\\ \\hline [216] & \\begin{tabular}{c} ChatGPT \\\\ \\end{tabular} & ZS & News, Scientific Literature & English & Yes \\\\ \\hline [217] & \\begin{tabular}{c} ChatGPT \\\\ \\end{tabular} & ZS & Scientific Literature & English & No \\\\ \\hline \\end{tabular} \\end{table} TABLE V: Summary of research works exploring GLLMs for keyphrase generation task. Here ZS represents zero-shot, and FS represents few-shot."
    },
    {
      "title": "_Information Retrieval_",
      "text": "Information retrieval (IR) involves accessing and retrieving relevant information from large volumes of data. Here, the main objective is to provide users with the most relevant information by matching their queries to the content of documents and ranking them based on relevance [232]. The process includes indexing, query formulation, search and retrieval, ranking, and presentation. Information retrieval is utilized in a wide range of fields, such as web search engines, digital libraries, e-commerce, healthcare, and scientific research [232]. It plays a vital role in facilitating efficient and effective access to information in the modern digital era. Table VII presents a summary of research works exploring GLLMs for information retrieval. Sun et al. [230] explored the effectiveness of GPT-3 family models like GPT-3, GPT-3.5, ChatGPT and GPT-4 for passage re-ranking in information retrieval. The results are promising as GPT-4 outperforms SOTA models like monoT5-3B [233] on multiple benchmarks. Moreover, the compact model trained on ChatGPT-generated data demonstrates superior performance compared to the monoT5-3B model when evaluated on the MS MARCO dataset in BEIR [234] benchmark. The existing approaches for document retrieval employ dual dense encoders, which encode query and document independently, resulting in shallow interaction between query and document [235]. To overcome this drawback, Ziems et al. [231] proposed a novel approach which involves generating URLs using LLMs for document retrieval. The authors reported that document retrieval by generating URLs outperforms existing approaches."
    },
    {
      "title": "_Recommendation Systems_",
      "text": "**Overview.** Recommendation systems aim to reduce information overload and enhance the user experience by making relevant recommendations related to products or content based on user preferences and behaviour [236]. In recent times, recommendation systems have gained immense popularity and are extensively utilized across a range of fields, such as entertainment, e-commerce, social media etc. For example, popular platforms like YouTube and Netflix use recommendation systems to suggest relevant videos and platforms like Amazon use recommendation systems to suggest relevant products to the user [237]. The commonly used approaches for recommendation systems are based on collaborative filtering [238], content-based [239] and knowledge-based [240]. The performance of traditional recommendation systems is limited by a number of issues like cold-start problem, poor generalization across domains and lack of explainability [241, 242]. To overcome these drawbacks in traditional recommendation systems, recent works explored GPT-3 family large language models for various tasks in recommendation systems like next item prediction [243], rating prediction [241, 244], top-k predictions [241], direct recommendation [245], sequence recommendation [245] and generating explanations [245]. The evaluation is done in a variety of domains like movies [241, 243, 246, 247, 248, 249], news [246], books [244, 246, 247], music [248, 249], social media [250], beauty [245], and games [249]. Table VIII presents a summary of research works exploring GLLMs for recommendation systems. **Research works exploring GLLMs for recommendation systems.** Wang et al. [243] proposed a novel prompting strategy called \"Next-Item Recommendation (NIR)\" to recommend movies using GLLMs. The proposed prompting strategy involves a three-step process to capture the user's preferences, choose representative movies they have watched in the past, and provide a ranked list of ten recommended movies. Dai et al. [246] reported that ChatGPT outperforms other GLLMs and is more effective with pair-wise and list-wise ranking compared to point-wise ranking. When it comes to balancing cost and performance, ChatGPT with list-wise ranking outperforms both point-wise and pair-wise ranking approaches. ChatGPT demonstrates the potential for providing explanations for recommendations and addressing the challenges of the cold start problem. Gao et al. [241] proposed Chat-REC, which leverages GLLMs to build conversational recommendation systems. The authors reported that Chat-REC performs well in tasks like top-k recommendations and zero-shot rating prediction. Moreover, Chat-REC enhances the conversational recommendation systems by making them more interactive and providing clear explanations. Mysore et al. [250] explored GLLMs like InstructGPT to generate synthetic data, and the experiment results showed that narrative-driven recommendation models trained on augmented datasets outperform LLM baselines and other approaches. Kang et al. [247] evaluated GLLMs like GPT-3.5 and ChatGPT on user rating prediction in zero and few-shot settings. Based on the experimental findings on datasets from movies and book domains, the authors reported that traditional models that have access to user interaction data perform better \\begin{table} \\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \\hline **Paper** & **Task(s)** & **GLLMs Explored Settings** & **Prompt** & **Domain(s)** & **Language(s)** & **SOTA Results** \\\\ \\hline [230] & Passage Re-ranking ChatGPT, GPT-4 & \\begin{tabular}{c} GPT-3, GPT-3, \\\\ ChatGPT, GPT-4 \\\\ \\end{tabular} & \\begin{tabular}{c} ZS, FS \\\\ file Literature \\\\ \\end{tabular} & \\begin{tabular}{c} General, News, Healthcare, Scientific \\\\ Language \\\\ \\end{tabular} & Yes \\\\ \\hline [231] & Document Retrieval & GPT-3.5 & ZS, FS & General & English & Yes \\\\ \\hline \\end{tabular} \\end{table} TABLE VII: Summary of research works exploring GLLMs for information retrieval tasks. Here ZS represents zero-shot, and FS represents few-shot. than GLLMs. Zhang et al. [248] introduced FaiRLLM, a new benchmark having eight sensitive attributes from domains like movies and music, to investigate the fairness of GLLM recommendations. The authors reported that GLLM-based recommendation systems are not fair to certain sensitive attributes. Liu et al. [245] evaluated the performance of ChatGPT in five recommendation tasks, which include predicting ratings, direct recommendation, sequence recommendation, generating explanations, and summarizing reviews. Based on the evaluation of Amazon beauty datasets, the authors reported that (i) ChatGPT is much better in rating prediction compared to other tasks like direct and sequence recommendation. and (ii) ChatGPT achieves new SOTA results in generating explanations based on human evaluation. Hou et al. [249] demonstrated that GLLMs possess strong potential for zero-shot ranking tasks, showcasing performance that is comparable to or even superior to traditional recommendation models. Here, the authors designed the prompts in a way that important information like candidate items, sequential interaction history and ranking instruction is included. Zhiyuli [244] proposed BookGPT, a novel framework which leverages GLLMs like ChatGPT for book recommendation. Specifically, the performance of BookGPT is evaluated on three sub-tasks, namely the book rating task, book summary recommendation task and user rating recommendation task. The performance of BookGPT is promising in all three sub-tasks, and the performance increases with an increase in prompt examples."
    },
    {
      "title": "_Coding Tasks_",
      "text": "**Overview.** Software engineering is a discipline which deals with designing, developing, testing, and maintaining software systems [272]. To create software systems, software engineers use a variety of programming languages, development tools, and technologies. To aid software engineers and enhance their productivity, the research community focused on automating a number of coding tasks like code generation from natural language descriptions, code repair, code explanation generation, code hints generation, code completion, code document generation, test cases generation, code vulnerability detection, code refactoring, etc. The evolution of pre-trained source code models has paved the way for achieving cutting-edge results across coding tasks [455]. Some of the popular pretrained source code models are CodeBERT [86], CodeGPT [273], CoTexT [274], GraphCodeBERT [275], CodeT5 [87], CodeT5+ [88], PLBART [276], PyCodeGPT [277] etc. Inspired by the success of GLLMs in NLP tasks, the research community focused on assessing the performances of these models in coding tasks also. **Research works exploring GLLMs for various coding tasks.** The research community explored GLLMs for coding tasks across various languages like Java [251, 252, 260, 263, 264, 265, 266, 267, 269, 270], Python [253, 254, 255, 256, 257, 260, 262, 263, 265, 267, 268, 271], PHP [260], GO [260], Ruby [260], JavaScript [260], C [261, 268], C++ [259, 268], Julia [268], and MATLAB [268]. Most of the research works focused on Python and Java languages, while a few research works focused on other languages like GO, PHP, GO, Ruby, JavaScript, C, C++, Julia and MATLAB. The assessment is done in zero and few-shot settings using mostly direct prompts. Table IX presents a summary of research works exploring GLLMs for various coding tasks. Some of the research works [253, 257, 268, 269] explored GLLMs for code generation task. Yeticstiren et al. [253] compared various AI-assisted code generation tools like ChatGPT, Amazon's Code Whisperer and Github's Copilot on the Human Eval [103] dataset. ChatGPT outperforms other tools by generating correct code 65.2% of the time, while the other tools generate correct code for a maximum of 46.3% of the time only. The test cases in existing datasets for code generation evaluation are limited in terms of quality and quantity. So, Liu et al. [257] proposed EvaPlus, a new framework for automatic test case generation using ChatGPT and the traditional mutation approach. The authors use EvaPlus to develop HumanEvalPlus on the top of the HumanEval [103] dataset. The au \\begin{table} \\begin{tabular}{|c|l|l|l|l|l|} \\hline **Paper** & **GLLMs Explored** & **Prompt Settings** & **Domain(s)** & **Language(s)** & **SOTA** \\\\ & & & & & **Results** \\\\ \\hline [243] & GPT-3-5 & ZS & Movies & English & No \\\\ \\hline [246] & GPT-3-5, ChatGPT & ZS, FS & News, Books, Movies, Music & English & No \\\\ \\hline [241] & GPT-3-5, ChatGPT & ZS & Movies & English & No \\\\ \\hline [250] & InstructGPT & FS & Social Media & English & No \\\\ \\hline [247] & GPT-3-5, ChatGPT & ZS, FS & Movies, Books & English & No \\\\ \\hline [248] & ChatGPT & ZS & Music, Movies & English & No \\\\ \\hline [245] & ChatGPT & ZS, FS & Beauty & English & Yes \\\\ \\hline [249] & ChatGPT & ZS & Movies, Games & English & No \\\\ \\hline [244] & ChatGPT & ZS, FS & Books & English & No \\\\ \\hline \\end{tabular} \\end{table} TABLE VIII: Summary of research works exploring GLLMs for recommendation systems. Here ZS represents zero-shot, and FS represents few-shot. thors reported that HumanEvalPlus can detect a lot of incorrectly generated code that was previously undetected. Nascimento et al. [259] compared the quality of code generated by ChatGPT and software developers for competitive coding problems on the LeetCode platform using various evaluation metrics. The authors reported that ChatGPT exhibits better performance compared to novice programmers but is outperformed by experienced programmers. Kashefi et al. [268] explored how effective ChatGPT is for generating code for numerical methods in five different programming languages: C, C++, Python, MATLAB and Julia. The authors observed that the results are promising but have some limitations which require further investigation. Destefains et al. [269] assessed the code generation ability of LLMs like Bard and GPT-3.5 by evaluating their performances in generating Java language code given the natural language descriptions. The authors observed that GPT-3.5 outperforms the Bard model by a large margin of more than 37%. Some of the research works [263, 265, 267, 271] explored GLLMs for code repair task. Prenner et al. [263] explored the Codex model for automatic program repair in Python and Java programming languages. The authors observed that the performance of Codex is comparable to state-of-the-art methods. Moreover, the Codex model is slightly better at fixing errors in Python language compared to Java language. Kang et al. [267] developed AutoSD, a novel framework for automatic program repair using GLLMs. The authors reported that the evaluation on three standard datasets showed that the proposed framework is on par with the baselines. Unit tests generated using traditional approaches suffer from low readability [270]. To address this drawback, some of the research works [264, 270] explored GLLMs for test case generation. Siddiq et al. [264] evaluated models like Codex and ChatGPT for unit test generation for Java code. Experiment results showed that Codex performs better with 80% coverage for the HumanEval dataset. However, both models perform poorly in the case of the SF110 benchmark, with less than 2% coverage. Yuan et al. [270] designed a ChatGPT-based unit test generation framework called \"Chat-Tester\". The iterative test refiner helps Chat-Tester to generate better unit tests compared to vanilla ChatGPT. In all the above discussed research works, the performance of GLLMs in various coding tasks is promising but still lags behind SOTA results. Some of the research \\begin{table} \\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \\hline **Paper** & **GLLMs Explored** & **Task(s)** & **Prompt Settings** & **Language(s)** & **SOTA Results** \\\\ \\hline [251] & ChatGPT & Code Repair & ZS, FS & Java & Yes \\\\ \\hline [252] & GPT-3, ChatGPT & Code Vulnerability Detection & ZS & Java & No \\\\ \\hline [253] & ChatGPT & Code Generation & ZS & Python & No \\\\ \\hline [254] & ChatGPT & Finding Failure-Inducing Test Cases & ZS & Python & Yes \\\\ \\hline [255] & ChatGPT & Code Generation & ZS & Java, C\\# & No \\\\ \\hline [256] & GPT-4 & Code Generation, Code Refactoring, Test Case Generation & ZS & Python & No \\\\ \\hline [257] & ChatGPT, GPT-4 & Code Generation & ZS & Python & No \\\\ \\hline [258] & ChatGPT & Code Explanation Generation & ZS & Python & No \\\\ \\hline [259] & ChatGPT & Code Generation & ZS & C++ & No \\\\ \\hline [260] & Codex & Code Documentation Generation & ZS, FS & Java, Python, G0, Ruby, JS & PHP, & Yes \\\\ \\hline [261] & GPT-3 & Code Explanation Generation & ZS & C & No \\\\ \\hline [262] & ChatGPT & Code Generation & ZS & Python & Yes \\\\ \\hline [263] & Codex & Automatic Code Repair & ZS, FS & Python, Java & No \\\\ \\hline [264] & Codex, ChatGPT & Unit Test Generation & ZS & Java & No \\\\ \\hline [265] & ChatGPT & Code Generation, APR, Code Explanation Generation & ZS & Python & No \\\\ \\hline [266] & Codex & Code Documentation Generation & ZS, FS & Java & Yes \\\\ \\hline [267] & Codex, ChatGPT & Automate Program Repair & ZS & Python, Java & No \\\\ \\hline [268] & ChatGPT & Code Generation & ZS & C\\_++, Python, Julia, MATLAB & No \\\\ \\hline [269] & GPT-3.5 & Code Generation & ZS & Java & No \\\\ \\hline [270] & ChatGPT & Unit Test Generation & ZS & Java & No \\\\ \\hline [271] & ChatGPT, GPT-4 & Code Repair, Code Completion, Code Explanation Generation, Coding Hints Generation & ZS & Python & No \\\\ \\hline \\end{tabular} \\end{table} TABLE IX: Summary of research works exploring GLLMs for various coding tasks. Here ZS represents zero-shot, and FS represents few-shot. works [254, 260, 262, 256] demonstrated that GLLMs can achieve SOTA results in coding tasks. Xia et al. [251] proposed ChatRepair, an automatic program repair tool based on ChatGPT. ChatRepair achieves remarkable performance, surpassing all the existing methods. It successfully resolves 114 and 48 bugs on Defects4] 1.2 and 2.0 [278], respectively, outperforming the previous best by 15 and 17 bugs, respectively. Khan et al. [260] explored Codex, GPT-3 family model pretrained on natural and programming languages to automate code documentation generation. The evaluation results on six programming languages showed that Codex, with just one example, outperforms existing approaches by a large margin of 11.2%. Geng et al. [266] explored Codex for code document generation and demonstrated that few-shot in-context learning with systematic demonstration selection helps the GPT-3 model to achieve new SOTA results on two standard datasets related to Java language. Some of the research works [254, 255, 262] explored advanced prompting like CoT, brainstorming, differential prompting, etc., for coding tasks. Liu et al. [255] evaluated the code generation capabilities of ChatGPT by evaluating its performances on text-to-code and code-to-code generation tasks on CodeXGLUE [273] datasets. The authors observed that advanced prompting strategies like CoT enhance the code generation capabilities of models like ChatGPT. Li et al. [262] proposed Brainstorm, a new framework for code generation. Brainstorm involves three steps: brainstorm to generate diverse thoughts, thoughts selection to select the best thought using a ranking model and writing code to generate the code based on the problem statement and the best thought. The authors reported that the proposed framework helps ChatGPT to increase its performance by more than 50% and achieve new SOTA results on the CodeContests [104] benchmark. Li et al. [254] showed that directly using ChatGPT to find failure-inducing test cases results in poor performances. So, the authors proposed a new prompting strategy called \"Differential Prompting\", which enables ChatGPT to achieve new SOTA results on the Quixbugs dataset [279]. Differential Prompting involves program intention inference followed by two more steps: program generation and differential testing."
    },
    {
      "title": "_Multimodal Ai Tasks_",
      "text": "**Overview.** Traditional AI systems are designed to handle data from a single modality such as text, image, audio or video. As real-world data is often multi-modal, researchers focused on developing multi-modal AI systems which can leverage input data from multiple modalities to generate more accurate results. Multi-modal AI systems leverage techniques from different areas of AI, like natural language processing, computer vision, speech processing etc., to process multi-modal input data effectively [280, 281]. Multi-Modal AI systems can perform a variety of understanding and generation tasks like visual question answering [282, 283, 284, 179], text-to-image generation [285, 286, 287], text-to-video generation [288], text-to-speech synthesis [289], speech-to-text synthesis [289], image captioning [290] etc. **Research works exploring GLLMs for Multimodal AI tasks.** After the huge success of LLMs in natural language generation and understanding tasks, the research community recently explored GPT-3 family models in multi-modal understanding and generation tasks in various combinations like image+language [282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298], video+language [288, 299], audio+language [300, 301]. Most of the research works focused on general domain datasets, which some of the research works focused on specific domains like healthcare [290, 298]. Table X presents a brief summary of research works exploring GLLMs for various multimodal AI tasks. Some of the research works developed multi-model AI systems for a specific task like action generation [291], knowledge-based visual question answering [282, 283, 284, 179], x-ray report generation [290], named entity recognition [294], text-to-video generation [288], layout generation [296], text-to-image generation [287]. Kalakonda et al. [291] proposed GPT-3 based plug-and-play framework called Action-GPT for text-based action generation. Here, the authors generated multiple detailed body movement descriptions from the action phrases and then used them to generate actions. Shao et al. [282] proposed Prophet, which avoids using an external knowledge base by using GPT-3 as an implicit knowledge base and includes vanilla visual question answering to provide answer heuristics to GPT-3. The answer heuristics, along with caption and question information, provide rich task-specific information to the GPT-3 model, which results in much better performances. Ranjit et al. [290] proposed automatic x-ray report generation based on contrastively pretrained vision-language encoder and GPT-3 family models like GPT-3.5, ChatGPT and GPT-4. The contrastively pretrained encoder is used to encode input x-ray image into image vector embedding based on which the most similar sentences from the radiology report corpus are retrieved. The retrieved similar sentences form the context and allow LLM to generate a quality X-Ray report. Li et al. [294] proposed PGIM, a two-stage approach which utilizes ChatGPT as an implicit knowledge base for multi-modal NER task. In the first stage, ChatGPT, when prompted with text descriptions of the image, generates the auxiliary knowledge. In the second stage, the downstream model receives the raw text and ChatGPT-generated auxiliary knowledge as input. The authors reported that the proposed approach outperforms existing SOTA approaches based on text-text and text-image paradigms. Hong et al. [288] proposed DircC12V for text-to-video generation, which leverages GPT-4 model as a frame-level director. Here, the GPT-4 model generates descriptions for each frame based on a single prompt, and then the Text-to-Image model is used to generate frames based on these descriptions. Feng et al. [296] developedLayoutGPT, which leverages LLM and Layout-to-Image models to generate 2D and 3D planning layouts from text descriptions. Zhang et al. [287] proposed \"Control-GPT\" based on LLMs and diffusion models for controllable text-to-image generation. Here, GPT-4 generates sketches based on Tikz code based on the text instructions, and then diffusion model generates realistic images with generated sketches and the text instructions as input. Here, the generated sketches help diffusion models to get a better idea about spatial relationships. Some of the research works focused on developing multi-model AI systems which can handle multiple tasks [289, 292, 293, 295, 299, 302]. As ChatGPT is trained on one data modality i.e., text data, ChatGPT can only handle text inputs and training models from scratch for vision-language tasks, is not a feasible option as it involves huge computation. So, Wu et al. [292] developed Visual ChatGPT based on ChatGPT and various visual foundation models to handle 22 vision language tasks. Bhattacharya et al. [299] proposed a novel three-stage approach to handle five video understanding tasks. The proposed approach involves transforming video into text stories and then using this text content for video understanding tasks. Hakimov et al. [295] explored GPT-3 model for five vision language tasks, including four classifications and one question answering. Here the model is prompted with text description of the input image along with other elements like task instruction and similar examples. Huang et al. [289] proposed AudioGPT, which allows ChatGPT to handle multiple audio understanding and generation tasks with the help of audio foundation models. Some of the research works explored GPT-3 family models for other tasks like data labelling [300], generating instructions [301], data generation [297], prompt editing [286] and evaluation [285] while developing multimodal AI systems. Mei et al. [300] used ChatGPT to rewrite those noisy audio captions and developed WaveCaps, an audio captions dataset of 400k instances. The authors reported that the models trained on WaveCaps \\begin{table} \\begin{tabular}{|p{42.7pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline **Paper** & **GLIMs Explored** & **Task(s)** & **Prompt Settings** & **Multimodality** & **Domain** \\\\ \\hline [291] & GPT-3 & Text-based Action Generation & ZS & Image + Language & General \\\\ \\hline [292] & ChatGPT & Twenty Two Vision Language Tasks & ZS & Image + Language & General \\\\ \\hline [282] & GPT-3 & Knowledge-based Visual Question Answering & FS & Image + Language & General \\\\ \\hline [300] & ChatGPT & Audio Labelling & ZS & Audio + Language & General \\\\ \\hline [293] & ChatGPT & Multi-Image Reasoning, Multi-hop Document Understanding. Open-World Concept Understanding. Video Summarization & ZS & Image + Language & General \\\\ \\hline [290] & GPT-3.5, ChatGPT, GPT-4 & Chest X-Ray Report Generation & ZS & Image + Language & Healthcare \\\\ \\hline [283] & GPT-3 & Knowledge-based Visual Question Answering & FS & Image + Language & General \\\\ \\hline [299] & GPT-3.5 & Five Video Understanding Tasks & ZS & Video + Language & General \\\\ \\hline [301] & GPT-4 & Generate Instructions & ZS & Audio + Language & General \\\\ \\hline [285] & GPT-3.5, GPT-4 & Evaluator for Text-to-Image Generation & ZS & Image + Language & General \\\\ \\hline [286] & GPT-3, GPT-3.5 & Editing in Text-to-Image Generation & FS & Image + Language & General \\\\ \\hline [294] & ChatGPT & Multimodal Named Entity Recognition & FS & Image + Language & General \\\\ \\hline [295] & GPT-3 & Five vision language tasks (four classification tasks and one question answering task) & FS & Image + Language & General \\\\ \\hline [288] & GPT-4 & Text-to-Video Generation & ZS & Video + Language & General \\\\ \\hline [179] & GPT-3 & Knowledge-based Visual Question Answering & FS & Image + Language & General \\\\ \\hline [296] & GPT-3.5, ChatGPT, GPT-4 & Layout Generation & FS & Image + Language & General \\\\ \\hline [302] & ChatGPT, GPT-4 & Multimodal tasks covering text, video, audio and images & ZS & Multimodal covering text, video, audio and images & General \\\\ \\hline [287] & GPT-3.5, ChatGPT, GPT-4 & Controlled Text-to-Image Generation & ZS & Image + Language & General \\\\ \\hline [297] & ChatGPT & Paraphrasing & ZS & Image + Language & General \\\\ \\hline [289] & ChatGPT & Audio Understanding and Generation Tasks & ZS & Multimodal covering text, audio and images & General \\\\ \\hline [298] & GPT-4 & Generate Instruction Tuning Dataset & FS & Image + Language & Healthcare \\\\ \\hline [284] & GPT-3 & Knowledge-based Visual Question Answering & FS & Image + Language & General \\\\ \\hline \\end{tabular} \\end{table} TABLE 10: Summary of research works exploring GLIMs for various multimodal AI tasks. Here ZS represents zero-shot, and FS represents few-shot. datasets achieve new SOTA results. Zhang et al. [301] developed SpeechGPT and then do cross-modal instruction tuning to enhance its multi-model instruction following ability. Here, the authors use GPT-4 to generate the instructions for diverse tasks. Fan et al. [297] proposed LaCLIP (Language augmented Contrastive Language-Image Pretraining), an extended version of CLIP which applies data augmentation to both text and image data to ensure that the model gets exposed to diversified texts during training. Here the data augmentation is performed using the open-source LLaMA model in few-shot settings, and the examples for LLaMA ICL are generated using ChatGPT. Zhu et al. [286] explored GPT-3 and GPT-3.5 models for prompt editing in text-to-image generation. The authors observed a potential reduction of 20-30% in the remaining edits required by implementing the prompt edits suggested by GPT-3 family models. Lu et al. [285] proposed LLMScore, a new metric which can effectively capture both image and object-level compositionality for text-to-image generation evaluation."
    },
    {
      "title": "_Machine Learning Tasks_",
      "text": "**Overview.** Machine learning (ML) is an area of artificial intelligence (AI) that deals with the development of algorithms that can learn from data and make decisions [305]. Even though machine learning algorithms are successfully used in various real-world applications, creating an effective ML solution for a new task can be difficult due to the numerous design choices involved. In recent times, AutoML has evolved as a solution to reduce the human effort involved in designing ML solutions [307]. However, AutoML algorithms suffer from various drawbacks [305], like (i) the requirement of multiple rounds of trial-and-error, resulting in significant time consumption, (ii) starting the search for a new task from scratch, ignoring past experience gained from the previous tasks and (iii) many AutoML methods lack interpretability because of their black-box nature. **Research works exploring GLLMs to automate machine learning tasks.** Inspired by the success of GLLMs in other tasks, the research community explored GLLMs as an alternative to AutoML to automate machine learning tasks [303, 304, 305, 306]. Table XI presents a summary of research works exploring GLLMs to automate machine learning tasks. Zheng et al. [303] explored how effective is GPT-4 for neural architecture search, i.e., designing optimal neural network configurations. The proposed approach involves two steps, namely (i) GPT-4 generates the optimal neural architecture based on the given problem statement, (ii) the generated configuration is evaluated, and for further refinement, the evaluation results along with the problem statement are passed to the model. This two-step process is repeated for a certain number of iterations to achieve the optimal configuration. Shen et al. [304] proposed HuggingGPT to solve AI tasks with the help of GLLMs like ChatGPT and models in AI communities like Hugging Face. HuggingGPT involves four steps, namely task planning, model selection, task execution and response generation. The authors reported that HuggingGPT achieves promising results in solving AI tasks in language, vision and speech. Zhang et al. [305] proposed MLCopilot, which leverages the power of GLLMs to solve machine learning tasks. MLCopilot works in two stages, namely offline and online. The offline stage involves creating an experience pool from which GLLM is used to retrieve relevant knowledge. The online stage involves retrieving relevant examples from the experience pool, and then GLLM generates results based on the task description, relevant examples and knowledge. Zhang et al. [306] proposed AutoML-GPT, which leverages the advanced GPT-4 GLLM to automatic machine learning tasks and reduces human efforts in building machine learning models. AutoML-GPT involves two stages. The first stage involves composing a prompt paragraph based on the model and data cards. The second stage involves performing the four crucial steps from data processing to training log prediction."
    },
    {
      "title": "_Planning_",
      "text": "**Overview.** Many important industries, like finance and banking, often involve repetitive sequential tasks. These workflows, despite their significance, are typically not fully automated or formally defined. Recently, due to strong reasoning capabilities, the research community explored GLLMs for planning. Some of the research works [309, 311] directly used LLMs for planning, while some of them [308, 310] explored LLMs for planning extraction, which can then be used by automated systems. **Research works exploring GLLMs for planning.** Table XII presents a summary of research works exploring GLLMs for planning. Human models are crucial in facilitating human-robot interaction (HRI), as they empower robots to plan their behaviour based on the impact of their actions on individuals. As it is difficult to craft good human labels, Zhang et al. [309] used the GPT-3.5 model (i) as zero-shot human models and also (ii) for planning in trust-related scenarios. Hu et al. [311] proposed a novel prompting strategy called \"Chain of Symbol\" prompting to elicit better the planning abilities of large language models like InstructGPT and ChatGPT. Unlike CoT prompting, which uses natural language descriptions to represent complex environments, CoS prompting uses condensed symbols to represent them in intermediate reasoning steps. The authors reported that CoS prompting outperforms CoT prompting in both performance and efficiency. There are usually natural language documents that describe the procedures for the company's employees. Plan extraction methods offer the opportunity to extract structured plans from these natural language descriptions of workflows [93, 95]. These extracted plans can then be used by automated systems. Olmo et al. [308] explored the GPT-3 model for plan extraction in few-shot settings from the natural language descriptions of workflows and showed that GPT-3 model outperforms existing SOTA models in some cases. Xie et al. [310] explored GPT-3.5 models to extract plans from natural language descriptions. The authors reported that the models are poor planners on their own, which is in line with the existing works [312, 313, 314] and are better at extracting plans from natural language. However, these models are sensitive to prompts and also struggle in the case of tasks involving spatial or numerical reasoning."
    },
    {
      "title": "5 Performance Of Gllms In Specific Domains",
      "text": "Apart from the general domain, natural language processing is also explored in specific domains like healthcare, finance, legal, social media, etc. Analyzing domain-specific texts is more challenging because of domain-specific terminology and abbreviations, complex language structures, etc. In domains like healthcare, finance and legal, domain experts use many words and abbreviations that are specific to the domain and not commonly found in general domain texts. In domains like social media, the texts are mostly authored by the general public using informal language and slang words. Moreover, social media texts are noisy, with many misspelt words, emojis, irregular grammar and abbreviations [315, 316]. Inspired by the success of pretrained language models like BERT, RoBERTa, ELECTRA, DeBERTa and T5 in the general domain, these models are also explored for domain-specific NLP tasks [1]. However, the performance of general domain models is limited as these models are pretrained on general domain texts [81, 89], and fine-tuning alone cannot provide enough domain knowledge [1]. So, the research community focused on developing domain-specific pretrained language models either by continual pretraining or pretraining from scratch [1, 3]. Currently, domain-specific pretrained language models achieve state-of-the-art results in most tasks in specific domains like healthcare, finance, legal, social media, etc. GPT-3 family large language models achieve impressive performances in most NLP tasks in zero and few-shot settings in the general domain. Surprisingly, these models outperform fine-tuned pretrained language models in some tasks and achieve state-of-the-art results [155, 156, 158, 155]. Inspired by the massive success of GLLMs in the general domain, the research community explored GLLMs in specific domains to assess how good these models are in domain-specific NLP tasks. Moreover, an extensive evaluation of these models in domain-specific tasks helps to arrive at valuable insights that will guide the research community to improve the performance further and increase the usage of these models in domain-specific NLP tasks."
    },
    {
      "title": "_Healthcare Domain_",
      "text": "The recent works explored GLLMs for a variety of clinical NLP tasks like question answering [117, 195, 320, 322, 323, 326, 333, 335, 337, 338, 341, 342], text de-identification [318], dialogue summarization [319, 328, 330], named entity recognition [321, 149], relation extraction [321], text classification [322, 326, 327, 335], semantic similarity [321, 326], text simplification [324, 327, 343], relation classification [149, 326], text summarization [325, 331], natural language inference [137, 335, 326, 335], word sense disambiguation [329], biomedical evidence extraction [329], coreference resolution [329], medical status extraction [329], medical attribute extraction [329], synonym generation [334], clinical decision support [336, 340] and diagnostic lists generation [339]. Most of the research \\begin{table} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline **Paper** & **Task(s)** & **GLLMs Explored** & **Prompt Settings** & **Language(s)** & **SOTA Results** \\\\ \\hline [308] & Plan Extraction & GPT-3 & FS & English & Yes \\\\ \\hline [309] & Planning in Human-Robot Interaction & GPT-3.5 & ZS & English & No \\\\ \\hline [310] & Plan Extraction & GPT-3.5 & FS & English & No \\\\ \\hline [311] & Planning & InstructGPT, ChatGPT & FS & English & No \\\\ \\hline \\end{tabular} \\end{table} TABLE XII: Summary of research works exploring GLLMs for planning. Here ZS represents zero-shot, and FS represents few-shot. \\begin{table} \\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline **Paper** & **Task(s)** & **GLLMs Explored** & **Prompt Settings** & **Language(s)** \\\\ \\hline [303] & Neural Architecture Search & GPT-4 & ZS & English \\\\ \\hline [304] & Multiple AI tasks in language, speech and vision areas & GPT-3.5, GPT-4 & ChatGPT, FS & English \\\\ \\hline [305] & Machine Learning Tasks & GPT-3.5 & FS & English \\\\ \\hline [306] & Machine Learning Tasks & GPT-4 & FS & English \\\\ \\hline \\end{tabular} \\end{table} TABLE XII: Summary of research works exploring GLLMs to automate machine learning tasks. Here ZS represents zero-shot, and FS represents few-shot. focused on English datasets, except a few focused on other languages like Japanese [195, 322] and Chinese [332, 333, 333]. Table 13 presents a summary of research works exploring GLLMs for various NLP tasks in the healthcare domain. Lyu et al. [343] investigated the performance of ChatGPT and GPT-4 models in the healthcare domain, specifically the radiology area, by evaluating their ability to simplify the content in radiology reports. Experiment results showed that (i) GPT-4 performs better than ChatGPT. and (ii) optimized prompt with detailed instructions improves the performance for both models by a good margin. Antaki et al. [342] evaluated the effectiveness of ChatGPT in answering Opthalmology questions. \\begin{table} \\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \\hline **Paper** & **GLLMs Explored** & **Task(s)** & **Prompt Settings** & **Language(s)** & **Outperforms Domain-Specific Models** \\\\ \\hline [317] & ChatGPT, GPT-4 & Question Answering & ZS & English & - \\\\ \\hline [318] & ChatGPT, GPT-4 & Text De-identification & ZS & English & Yes \\\\ \\hline [319] & GPT-4 & Dialogue Summarization & FS & English & Yes \\\\ \\hline [320] & GPT-3.5, ChatGPT, GPT-4 & Question Answering & ZS, FS & English & Yes \\\\ \\hline [321] & GPT-3.5, GPT-4 & Named Entity Recognition, Relation Extraction, Document Classification and Semantic Similarity & ZS, FS & English & Yes \\\\ \\hline [322] & GPT-3.5, ChatGPT & Question Answering & ZS & Japanese & - \\\\ \\hline [323] & GPT-3.5, GPT-4 & Question Answering, Reasoning & ZS & Chinese & Yes \\\\ \\hline [324] & GPT-3 & Text Simplification & FS & English & - \\\\ \\hline [149] & GPT-3 & Entity Extraction, Relation Classification & FS & English & No \\\\ \\hline [137] & ChatGPT, GPT-4 & Natural Language Inference & ZS, FS & English & - \\\\ \\hline [325] & ChatGPT & Text Summarization & FS & English & Yes \\\\ \\hline [138] & GPT3.5, GPT4 & Natural Language Inference, Document Classification & ZS, FS & English & - \\\\ \\hline [195] & GPT-3, ChatGPT, GPT-4 & Question Answering & FS & Japanese & - \\\\ \\hline [326] & GPT-3 & Natural Language Inference, Relation Classification, Semantic Similarity, Question Answering, Text Classification & FS & English & No \\\\ \\hline [327] & ChatGPT & Text Simplification & ZS & English & - \\\\ \\hline [328] & GPT-3, GPT-4 & Dialogue Summarization & FS & English & - \\\\ \\hline [329] & GPT-3 & Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, Medication Attribute Extraction & ZS, FS & English & - \\\\ \\hline [330] & GPT-3 & Dialogue Summarization & ZS, FS & English & - \\\\ \\hline [331] & GPT-3 & Text Summarization & ZS,FS & English & - \\\\ \\hline [332] & ChatGPT & Multi-Turn Medical Dialogue & ZS & Chinese & No \\\\ \\hline [117] & GPT-4 & Question Answering & FS & English & No \\\\ \\hline [333] & ChatGPT & Question Answering & ZS & Chinese & - \\\\ \\hline [334] & GPT-3 & Synonym Generation & ZS & English & - \\\\ \\hline [335] & GPT-3 & Natural Language Inference, Question Answering, Text Classification & ZS & English & No \\\\ \\hline [336] & ChatGPT & Clinical Decision Support & ZS & English & - \\\\ \\hline [337] & ChatGPT & Question Answering & ZS & English & - \\\\ \\hline [338] & ChatGPT & Question Answering & ZS & English & - \\\\ \\hline [339] & ChatGPT & Diagnosis Lists Generation & ZS & English & - \\\\ \\hline [340] & ChatGPT & Clinical Decision Support & ZS & English & - \\\\ \\hline [341] & GPT-3, GPT-3.5, ChatGPT & Question Answering & ZS & English & - \\\\ \\hline [342] & ChatGPT & Question Answering & ZS & English & - \\\\ \\hline [343] & ChatGPT, GPT-4 & Text Simplification & ZS & English & - \\\\ \\hline \\end{tabular} \\end{table} Table 13: Summary of research works exploring GLLMs for various NLP tasks in the healthcare domain. Here ZS represents zero-shot, and FS represents few-shot. Here ’-’ represents there is no comparison between GLLMs and domain-specific pretrained language models in the paper. The test set consists of both easy and moderate-level questions. Experiment results showed that ChatGPT achieves an average accuracy of 49.25%. Specifically, ChatGPT is able to answer the questions with good accuracy in general medicine. However, its performance in specific sub-areas of Opthalmology is worst. Gilson et al. [341] evaluated GLLMs like GPT-3, GPT-3.5, and ChatGPT model in answering the medical questions in Step 1 and Step 2 exams of USMLE. Experiment results showed that ChatGPT outperforms the other two models by a good margin. Rao et al. [336] demonstrated that ChatGPT performs better in the final diagnosis than in the initial diagnosis. This is because ChatGPT has access to more clinical data during the final diagnosis than the initial one. Carpenter et al. [334] demonstrated that GPT-3 can be used for the synonym generation for drugs of abuse. The authors query GPT-3 repeatedly for each drug to generate multiple synonyms, which are later filtered. The generated synonyms are then used to build a lexicon that is helpful for pharmacovigilance on social media platforms. Inspired by the success of the GPT-3 model for text summarization in the general domain, Shaib et al. [331] explored the GPT-3 model for summarizing biomedical documents. Experiment results revealed that (i) GPT-3 performance is promising in the case of single document summarization and (ii) GPT-3 struggles to summarize the content from multiple biomedical documents. Nair et al. [330] proposed a novel approach called \"MEDSUM-ENT\", a multi-stage framework for clinical dialogue summarization. The proposed method leverages the GPT-3 model through multiple intermediate calls to extract medical entities from the conversations. In the final step of summarization, the extracted entities, task instructions and in-context examples help the GPT-3 model to generate high-quality summaries. Based on the evaluation of radiology reports simplified by ChatGPT, Jeblick et al. [327] reported that ChatGPT-generated simplified radiology reports are not potentially harmful, complete and factually correct. However, further analysis reveals that some simplified reports contain factually incorrect sentences, potentially harmful paragraphs and a lack of essential medical findings. Hirosawa et al. [339] investigated the effectiveness of ChatGPT for clinical diagnosis by evaluating its ability to generate accurate diagnosis lists for clinical vignettes with common chief complaints. Experimental results showed that ChatGPT can generate diagnosis lists with good accuracy. However, the accuracy rate of ChatGPT is still less than the accuracy rate of physicians. Wang et al. [333] evaluated the performance of the ChatGPT model in answering medical questions in the Chinese language. Here, ChatGPT is prompted with questions in both English and Chinese to avoid language barriers. Experimental results show that the performance of ChatGPT is much lower than the average performance of the medical students. For example, ChatGPT correctly answers 45.8% of questions, while the average answering rate of medical students is 67.9% in 2021. Some of the research works demonstrated that domain-specific pretrained language models outperform GLLMs. Hernandez et al. [335] compared the performance of the GPT-3 model with the performances of general and domain-specific pretrained language models on three healthcare NLP tasks: natural language inference, question answering and text classification. Experiment results showed that domain-specific pretrained language models achieve better results even though they are much smaller than GPT-3. Xu et al. [332] introduced MedGPTEval, a benchmark to assess large language models in the healthcare domain. An extensive evaluation showed that domain-specific Chinese LLM outperforms general-purpose models like ChatGPT and ERNINE Bot. Singhal et al. [117] introduced MedPaLM2, a healthcare domain-specific LLM obtained by domain-specific finetuning of the PaLM2 [68] model. Experiment results showed that MedPaLM2 outperforms few-shot GPT-4 and achieves new state-of-the-art results on the MultiMedQA benchmark. Moradi et al. [326] investigated the performances of BioBERT and GPT-3 in few-shot settings on five biomedical NLP tasks: text classification, natural language inference, question answering, relation extraction and semantic similarity. The authors observed that BioBERT and GPT-3 models underperform the model fine-tuned using full training data. Moreover, the BioBERT model outperforms GPT-3 in few-shot settings even though the BioBERT model is 514 times smaller than GPT-3. Some research works showed that GLLMs can outperform domain-specific pretrained language models. Ma et al. [325] proposed ImpressionGPT, a novel approach for summarizing radiology reports using ChatGPT. The proposed method involves dynamic prompt construction and iterative optimization to enhance the performance of ChatGPT further. Evaluation on two standard datasets showed that the proposed framework achieves new SOTA results outperforming fine-tuned models like ChestXrayBERT [348]. Liu et al. [323] introduced CMExam, a dataset with 60k+ multiple-choice medical questions in the Chinese language and evaluated GLLMs like GPT-3.5 and GPT-4 on answer prediction and answer reasoning tasks. The authors observed that GPT-4 achieves the best results for both tasks, outperforming GPT-3.5 and medical domain-specific Chinese LLMs like Huatuo [352] and DoctorGLM [349]. Chen et al. [321] explored GLLMs like GPT-3.5 and GPT-4 on eight datasets spanning four tasks in zero and few-shot settings. The authors observed that fine-tuned PubMedBERT outperforms both the GLLMs in all the biomedical tasks except question answering. In the case of biomedical question answering, GPT-4 outperforms the fine-tuned PubMedBERT model by a large margin of 17 Giorgi et al. [319] explored models like Longformer Encoder-Decoder (LED) [96] based on supervised fine-tuning and GLLMs like GPT-4 based on few-shot ICL for clinical dialogue summarization as a part of MEDIQA-Chat 2023 [350] shared task. Here, the authors used Instructor [351] to select the most similar examples for few-shot ICL. Experiment results based on automatic metrics like BERTScore and ROUGE demonstrated that GPT-4 not only outperforms the LED model but also achieves first rank in the shared task. For medical text de-identification, Liu et al. [318] proposed a novel approach called \"DeID-GPT\", a two-step approach based on GLLMs. In the first step, HIPAA identifiers are included in the prompt. In the second step, GLLM receives the prompt and the medical record based on which the model generates the de-identified medical record having the personal information masked. The authors observed that GPT-4 outperforms not only ChatGPT but also fine-tuned models based on BERT, RoBERTa and ClinicalBERT."
    },
    {
      "title": "_Legal Domain_",
      "text": "The recent works explored GLLMs for a variety of legal NLP tasks like natural language inference [344], question answering [347, 352, 188], text generation [345, 347] and text classification [346]. Table XIV presents a summary of research works exploring GLLMs for various NLP tasks in the legal domain. Bommarito et al. [188] evaluated the performance of the GPT3.5 model in the legal domain by evaluating its ability to answer bar exam questions. The model answers the questions correctly at a rate of 50%, which is 25% more than the random guess baseline. However, the model performance is almost 18% less than the human performance, and overall model performance is below the passing threshold. Nguyen et al. [345] presented LawGPT 1.0, the first-ever chatbot model based on GPT-3 for the legal domain. The GPT-3 model is pretrained on mostly generic corpus, so it lacks domain-specific knowledge. To add domain-specific knowledge, LawGPT is developed by fine-tuning the GPT-3 model on the law corpus. Experimental results showed that LawGPT 1.0 performs on par with existing legal assistants. Chalkidis et al. [346] investigated how effective ChatGPT is for legal text classification by evaluating the model performance on the LexGLUE [360] benchmark, which consists of seven legal text classification datasets. The evaluation is performed in both zero and few-shot settings. Experiment results showed that ChatGPT performs poorly on legal text classification datasets. Choi et al. [347] demonstrated that the performance of ChatGPT is just above the passing threshold, i.e., equivalent to a C+ grade student. The authors found that advanced prompts like CoT [361] and Ranking prompts performed worse or the same as simple prompts for multiple-choice questions. For essay writing, the authors used carefully crafted simple prompts by including specific instructions at the end of the prompt."
    },
    {
      "title": "_Finance Domain_",
      "text": "The recent works explored GLLMs for a variety of finance NLP tasks like text classification [359, 353, 136, 354], named entity recognition [356, 136], question answering [357, 358], pairwise ranking [355], claim detection [356] and relation extraction [358]. Table XV presents a summary of research works exploring GLLMs for various NLP tasks in the finance domain. Li et al. [136] compared the performances of general LLMs like ChatGPT and GPT-4 in the finance domain with domain-specific models like BloombergGPT [115] and small fine-tuned models like FinBERT [82] and FinQANet [362]. The evaluation is done on five different datasets related to four financial NLP tasks: news headlines classification, sentiment analysis, entity extraction, and question answering. The ChatGPT and GPT4 models do well in question-answering task but lag behind in tasks requiring domain-specific knowledge like entity extraction and sentiment analysis. Fatouros et al. [353] evaluated the effectiveness of ChatGPT for financial sentiment analysis by assessing its performance on the fore-related news headlines dataset. Experiment results showed that ChatGPT outperforms the domain-specific FinBERT [83] model by a large margin of 35% and also exhibits a high correlation with market returns. Leippold et al. [354] explored GPT-3 for financial sentiment analysis and to generate adversarial attacks. Experiment results showed that FinBERT outperforms keyword-based approaches and the few-shot GPT-3 model in financial sentiment analysis. To study the \\begin{table} \\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \\hline **Paper** & **GLLMs Explored** & **Task(s)** & **Prompt Settings** & **Language(s)** & **Outperforms Domain-Specific Models** \\\\ \\hline [344] & GPT-3 & Natural Language Inference & ZS, FS & English & - \\\\ \\hline [188] & GPT-3.5 & Question Answering & ZS & English & - \\\\ \\hline [345] & GPT-3 & Question Answering, Text Generation & ZS & English & - \\\\ \\hline [346] & ChatGPT & Text Classification & ZS, FS & English & No \\\\ \\hline [347] & ChatGPT & Question Answering, Text Generation & ZS & English & - \\\\ \\hline \\end{tabular} \\end{table} TABLE XIV: Summary of research works exploring GLLMs for various NLP tasks in the legal domain. Here ZS represents zero-shot, and FS represents few-shot. Here ’-’ represents there is no comparison between GLLMs and domain-specific pretrained language models in the paper. robustness of FinBERT-based and keyword-based approaches, the authors explored GPT-3 to generate adversarial attacks. The main advantage of GPT-3 over existing adversarial attack-generating methods is that the model makes more subtle changes to the instances such that they are not noticeable to humans but still can fool the models. Wiriyathammabhum et al. [355] explored instruction fine-tuned T5 and GPT-3.5 models to evaluate investments-related social media posts in Chinese. The task involves two subtasks, namely pairwise ranking and unsupervised ranking. Experiment results showed that the few-shot prompted GPT-3.5 model outperforms the instruction fine-tuned T5 model and the few-shot prompted GPT-3.5 model with English-translated social media posts. Shah et al. [356] compared the performance of ChatGPT with the performance of fine-tuned pretrained language models for three different financial NLP tasks: claim detection, sentiment analysis and named entity recognition. The authors observed that fine-tuned models outperform ChatGPT, but ChatGPT performs much better than some open-source LLMs. Zhang et al. [357] introduced FinEval, a new benchmark to evaluate the financial domain of knowledge of LLMs in the Chinese language. FinEval includes 4,661 multiple-choice questions in Chinese language from four different categories spanning 34 academic subjects. Experiment results showed that GPT-4 achieves around 70% accuracy and outperforms all other LLMs, including ChatGPT and Chinese LLMs. Rajpoot et al. [358] assessed the effectiveness of ChatGPT and GPT-4 for financial relation extraction in few-shot settings. As the choice of examples is crucial in few-shot ICL, the authors explored learning free and learning-based retriever for example selection. The authors observed that GPT-4 outperforms ChatGPT by a decent margin, and the learning-based retriever performs better than the learning-free retriever."
    },
    {
      "title": "6 Multilingual Performance Of Gllms",
      "text": "**Overview.** GLLMs are pretrained over large volumes of text data from multiple languages. For example, the corpus used to pretrain the GPT-3 model includes text from around 90 languages, and the percentage of English text is more than 90% [366, 4]. In the beginning, most of the research focused on assessing the performance of GLLMs on English datasets only. However, it is essential to evaluate these models on datasets from non-English languages, especially low-resource languages, to know how effective GLLMs are for non-English languages, and the insights gained from the comprehensive evaluation help to further improve these models towards non-English languages. **Research works exploring GLLMs in multilingual settings.** Recently, some of the research works focused on evaluating GLLMs across various non-English languages. The evaluation is done on various tasks like parts of speech tagging [363, 366], named entity recognition [363, 370], relation extraction [363], natural language inference [363, 366, 370, 371], question answering [363, 365, 366, 367, 368], text summarization [363, 365, 366, 370], commonsense reasoning [363, 366], grammar error correction [364], text generation [365, 369], paraphrase identification [366], sentiment analysis [366, 370], language identification [132], machine translation [370, 132], genre identification [131], hate speech detection [368] and toxicity detection [370]. Most of the research focused on general domain datasets, except a few focused on other domains like social media [368, 370] and news [370]. Table X presents a summary of research works exploring GLLMs for NLP tasks in multilingual settings. Bang et al. [132] presented an extensive multilingual evaluation of ChatGPT across three tasks: sentiment analysis, language identification and machine translation. When compared to English, the performance of \\begin{table} \\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \\hline **Paper** & **GLLMs Explored** & **Task(s)** & **Prompt Settings** & **Language(s)** & **Outperforms Domain-Specific Models** \\\\ \\hline [136] & ChatGPT, GPT-4 & News Headlines Classification, Financial Sentiment Analysis, Named Entity Recognition, Question Answering & ZS & English & Yes \\\\ \\hline [353] & ChatGPT & Sentiment Analysis & ZS & English & Yes \\\\ \\hline [354] & GPT-3 & Sentiment Analysis & ZS & English & No \\\\ \\hline [355] & GPT-3.5 & Pairwise Ranking & FS & Chinese & - \\\\ \\hline [356] & ChatGPT & Sentiment Analysis, Claim Detection, Named Entity Recognition & ZS & English & No \\\\ \\hline [357] & ChatGPT, GPT-4 & Question Answering & ZS, FS & Chinese & - \\\\ \\hline [358] & ChatGPT, GPT-4 & Relation Extraction & FS & English & - \\\\ \\hline [352] & ChatGPT & Sentiment Analysis & ZS & Chinese & - \\\\ \\hline [399] & GPT-3.5, GPT-4 & Text Classification & ZS, FS & English & - \\\\ \\hline \\end{tabular} \\end{table} TABLE X: Summary of research works exploring GLLMs for various NLP tasks in the finance domain. Here ZS represents zero-shot, and FS represents few-shot. Here ‘-’ represents there is no comparison between GLLMs and domain-specific pretrained language models in the paper. ChatGPT degrades in the case of low-resource languages, particularly in the case of languages with non-Latin scripts. Das et al. [368] assessed the effectiveness of ChatGPT for emoji-based hate speech detection in multilingual settings. The authors reported that ChatGPT exhibits good performance but tends to misclassify abusive content as hate speech for non-English languages in the case of non-protected groups. Moreover, Armengol et al. [365] reported that the performance of GPT-3 can be improved in the case of low-resource languages with optimized tokenization. The focus of existing benchmarks like HELM [371] and BIG-Bench [372] is on the English language. So, some of the research works focused on introducing new benchmarks to facilitate a systematic and comprehensive evaluation of the multilingual performance of GLLMs [366, 370]. For example, Ahuja et al. [366] presented MEGA, a comprehensive evaluation benchmarking having 16 datasets covering 70 languages. Based on the evaluation of GLLMs like GPT-3.5, ChatGPT and GPT-4, the authors reported that GLLMs perform well in the case of languages with Latin scripts, and the performance is worst in the case of low-resource languages with non-Latin scripts across tasks. One of the possible reasons for this is the quality of tokenization. Similarly, Leong et al. [370] introduced BHASA, a benchmark to evaluate the performance of LLMs in four Southeast Asian languages. The benchmark consists of 20 datasets covering eight NLP tasks. The authors reported that (i) GPT-4 achieves better results compared to ChatGPT, and (ii) overall, the performance on some of the tasks is promising, with a lot of room for improvement in other tasks. Some of the existing works demonstrated that using prompts in English improves the performance of GLLMs in the case of non-English languages [131, 363]. For example, Lai et al. [363] performed a comprehensive evaluation of the multilingual abilities of ChatGPT on seven tasks covering more than 30 languages ranging from high-resource to extremely low-resource languages. The experiment results confirmed the bias of ChatGPT towards the English language, i.e., the performance is better for English compared to other languages and prompts in the English language can enhance the performance for non-English languages. The possible reason for the bias of GLLMs towards the English language is that GLLMs are trained mostly on English text corpus; hence, these models can better understand the prompt if it is in English [131]. Some of the research works investigated how GLLMs exhibit multilingual capabilities [367] and how effective GLLM-based evaluators are in scaling up evaluation in multilingual settings [369]. Zhang et al. [367] proposed a novel back translation prompting approach to systematically study how ChatGPT exhibit multilingual capabilities, although these models are largely pretrained on the English text corpus. The authors demonstrated that ChatGPT does translation in multilingual settings. Moreover, the multilingual performance of GLLMs is good only in the case of tasks which can translated. Hada et al. [369] assessed the effectiveness of GPT-4 as \\begin{table} \\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \\hline **Paper** & **GLLMs explored** & **Tasks)** & **Prompt Settings** & **Language(s)** & **Domain(s)** \\\\ \\hline [363] & ChatGPT & Pos Tagging, Entity Extraction, Relation Extraction, Natural Language Inference, Question Answering, Text Summarization, Common Sense Reasoning & ZS & 37 Languages & General \\\\ \\hline [364] & ChatGPT & Grammar Error Correction & ZS, FS & English, German, Chinese & General \\\\ \\hline [365] & GPT-3 & Question Answering, Natural Language Generation, Text Summarization & ZS & German, Spanish, Russian, Turkish, Catalan & General \\\\ \\hline [366] & GPT-3.5, ChatGPT, GPT-4 & Natural Language Inference, Paraphrase Identification, Commonsense Reasoning, Question Answering, Parts of Speech Tagging, Sentiment Analysis, Text Summarization & ZS & 70 languages & General \\\\ \\hline [132] & ChatGPT & Sentiment Analysis, Language Identification, Machine Translation & ZS & Multiple language including low resource languages like Sudanese, Javavese etc. & General \\\\ \\hline [131] & ChatGPT & Genne Identification & ZS & English, Slovenian & General \\\\ \\hline [367] & ChatGPT & Question Answering, Reasoning & ZS & Six languages including Chinese, German and French & General \\\\ \\hline [368] & ChatGPT & Hate Speech Detection & ZS & Eleven languages including Hindi, Arabic and Italian & Social Media \\\\ \\hline [369] & GPT-4 & Three Text Generation Tasks & ZS & Ten languages including Chinese and Japanese. & General \\\\ \\hline [370] & ChatGPT-4 & Question Answering, Sentiment Analysis, Text Summarization, Named Entity Recognition, Toxicity Detection, Machine Translation, Natural Language Inference, Casual Reasoning & ZS, FS & Indonesian, Vietnamese, Thai, Tamil & General, Social Media, News \\\\ \\hline \\end{tabular} \\end{table} Table 16: Summary of research works exploring GLLMs for NLP tasks in multilingual settings. Here, ZS represents zero-shot, and FS represents few-shot. an evaluator for natural language generation tasks in multilingual settings. The authors reported that GPT-4 tends to favour high scores and should be used carefully."
    },
    {
      "title": "7 Data Labelling And Data Augmentation Abilities Of Gllms",
      "text": ""
    },
    {
      "title": "_Data Labelling_",
      "text": "**Overview.** Large language models, specifically GLLMs, have achieved impressive performances in most of the NLP tasks, highlighting the huge potential of these models. However, large model size, high latency, high inference costs, proprietary access (in the case of GLLMs) and confidentiality concerns (in the case of sensitive domains like medical [381]) have become bottlenecks for the practical use of these models. Because of these bottlenecks, in environments with constrained resources or confidentiality constraints, pretrained language models are preferred over GLLMs as these models are much smaller in size and also more efficient compared to GLLMs [384]. For example, BERT base and large models contain just 110M and 340M parameters, while the GPT-3 model contains 175B parameters. Moreover, it is reported that GLLMs are trailing the SOTA models, with 4% to 70% lower performance when evaluated across a set of 25 diverse natural language processing tasks [133]. The performance of fine-tuned pretrained language models is largely determined by the quality as well as the quantity of labelled data. Human-annotated data is considered the gold standard [385, 386], and we have two strategies for this [373, 375]. The first one is using trained expert coders like students and research assistants, and the second one is using crowd workers from online platforms like Amazon Mechanical Turk. Although human-labelled data is considered the gold standard, the human annotation process is expensive, laborious and time-consuming. The second strategy, i.e., using crowd workers, is comparatively less expensive, but there is a growing concern regarding the degrading annotation quality of crowd workers [387]. Moreover, the annotation quality varies with annotators, and hence it is consistent. To address the challenges associated with the human annotation process, there is a growing interest in the NLP research community to leverage the extraordinary generative abilities of GLLMs to make the data annotation process less expensive, faster and consistent. Similar to the human annotation process, GLLMs are provided with detailed instructions along with some labelled examples to label the data. **Research exploring GLLMs for data labelling.** The research community explored GLLMs for data labelling in a variety of NLP tasks like stance detection [373, 376], political tweets classification [375], sentiment analysis [376, 379, 380], hate speech detection [376, 377], bot detection [376], toxic comments detection [377], offensive comments detection [377], adverse drug reaction extraction [378], text entailment [379], topic classification [379], text generation [379], answer type classification \\begin{table} \\begin{tabular}{|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|} \\hline **Paper** & **GLLMs Explored** & **Task(s)** & **Prompt Settings** & **Domain(s)** & **Language(s)** & **Outperforms Human Annotators** \\\\ \\hline [373] & ChatGPT & Stance, Relevance, Frame and Topics Detection & ZS & Social Media, News & English & Yes \\\\ \\hline [374] & GPT-3-5 & Three Binary Text Classification Tasks & ZS, FS & General & English & Yes \\\\ \\hline [375] & GPT-4 & Political Tweets Classification & ZS & Social Media & English & Yes \\\\ \\hline [376] & ChatGPT & Stance Detection, Sentiment Analysis, Hate Speech Detection, Bot Detection & ZS & Social Media & English & No \\\\ \\hline [377] & ChatGPT & Detection of Hateful, Toxic and Offensive Comments & ZS & Social Media & English & No \\\\ \\hline [378] & GPT-3-5, GPT-4 & Adverse Drug Reaction Extraction & ZS, FS & Healthcare & English & - \\\\ \\hline [379] & GPT-3 & Text Entailment, Topic Classification, Sentiment Analysis, Answer Type Classification, Question Generation, Text Generation & ZS & General & English & - \\\\ \\hline [380] & GPT-3 & Sentiment Analysis, Relation Extraction, Named Entity Recognition & FS & General & English & - \\\\ \\hline [381] & GPT-3-5 & Named Entity Recognition & ZS & Healthcare & English, French, Spanish, Italian, Basque & - \\\\ \\hline [382] & GPT-3-5 & Text Summarization & ZS, FS & General & English & - \\\\ \\hline [383] & ChatGPT & Detection of Stance, Topics, Relevance, General Frame and Policy Frame & ZS,FS & Social Media, News & English & Yes \\\\ \\hline [324] & GPT-3 & Radiology Text Simplification & FS & Healthcare & English & - \\\\ \\hline \\end{tabular} \\end{table} TABLE IV: Summary of research works exploring GLLMs for data labelling. Here, ‘-’ represents that the paper doesn’t include a comparison between GLLMs and human annotators. [379], question generation [379], relation extraction [380], named entity recognition [381, 380], text summarization [382], radiology text simplification [324] etc. Most of the research works focused on English datasets, except a few research works focused on other languages like French [381], Spanish [381], Italian [381] and Basque [381]. Table X presents a summary of research works exploring GLLMs for data labelling. Gu et al. [378] labelled sentences from PubMed abstracts using the GPT-3.5 model and then fine-tuned the PubMedBERT model for adverse drug reaction extraction. Experiment results showed that (i) PubMedBERT achieves results comparable to the SOTA model and (ii) PubMedBERT outperforms the GPT-3.5 and GPT-4 models by large margins of 6 and 5 points in F1 score, respectively. Based on the evaluation of multiple NLU and NLG tasks, Wang et al. [379] demonstrated that GPT-3 labelled data can result in a 50 to 96% reduction in labelling expenses. Moreover, pretrained language models fine-tuned on GPT-3 labelled data outperform the few-shot GPT-3 model in both NLU and NLG tasks. Further, the authors proposed an approach based on active learning to make use of both human and GPT-3 labels, which further enhances the performance of the fine-tuned models. Meoni et al. [381] investigated the effectiveness of GPT-3.5 labelled data and dictionary-based labelled data in fine-tuning pretrained language models to extract clinical entities in multiple languages like English, Spanish, Basque, Italian and French. The authors reported that (i) the performance of GPT-3.5 labelled data is on par with dictionary-based labelled data, and (ii) combining annotations from both approaches further enhances the results. Xu et al. [382] proposed InhertiSumm, a novel approach for training small text summarization models like ZCode++ [388] using GPT-3.5 generated summaries. The authors showed that the ZCode++ model with just 390M parameters trained using GPT-3.5 generated summaries performs on par with GPT-3.5 in zero and few-shot settings. Zhu et al. [376] investigated how effective ChatGPT is for labelling data for social computing tasks. Based on the evaluation of five datasets spanning over tasks like stance detection, hate speech detection, bot detection and sentiment analysis, the authors reported that ChatGPT achieves an average accuracy of 60.9. Li et al. [377] investigated the ability of ChatGPT to label hateful, offensive and toxic comments and compared the performances with MTurk annotations. The authors observed that ChatGPT performance is promising as it is able to label 80% of comments correctly. Moreover, the performance of ChatGPT is more consistent for non-harmful comments than harmful comments. Some of the research works [373, 374, 375, 376, 383] showed that GLLMs as data annotators can outperform human annotators. Gilardi et al. [373] investigated the effectiveness of ChatGPT as an annotator in zero-shot settings for four text classification tasks involving tweets and news articles. The authors reported that ChatGPT is more effective than MTurk crowd-workers as (i) ChatGPT achieves 25 points more than crowd-workers in terms of accuracy, (ii) ChatGPT is approximately 30 times cheaper, and (iii) intercoder agreement of ChatGPT is more than crowd-workers. He et al. [374] proposed a novel approach called \"explain then annotate\" to enhance the performance of GLLMs as text data annotators. The proposed approach involves two steps: (i) GLLM generates explanations for the demonstrations and then (ii) annotates the data by leveraging annotation guidelines, demonstrations and explanations through CoT prompting. Evaluation on three binary text classification tasks revealed that GPT-3.5 outperforms crowd-workers on one task and matches the performance of crowd-workers on the other two tasks. Tornberg et al. [375] demonstrated that zero-shot GPT-4 outperforms human annotators in labelling political English tweets. Further analysis demonstrated that GPT-4 possesses the ability to accurately label tweets that involve logical reasoning from contextual information. Alizadeh et al. [383] compared the performances of GLLMs like ChatGPT, open-source LLMs like FLAN [389] and MTurk annotators in labelling data (tweets and news articles) for five text classification tasks. The authors reported that ChatGPT achieves the best results, outperforming both open-source LLMs and MTurk annotators. One promising observation here is that open-source LLMs outperform MTurk annotators, and the performance is comparable to ChatGPT."
    },
    {
      "title": "_Data Augmentation_",
      "text": "**Overview.** The performance of downstream task-specific models is determined by the quality as well as the quantity of labelled data. Fine-tuning the pretrained language models on a small amount of labelled data will result in overfitting [1] and, subsequently, poor performances. However, it is not feasible all the time to label a large number of instances as the annotation process is expensive. So, the research community focused on alternative approaches like data augmentation to increase the size of training sets in a relatively inexpensive way [398, 399, 400, 401, 402]. The data augmentation approaches focus on generating additional training instances either by making small changes to the existing instances or creating new instances with a distribution similar to the existing instances. Data augmentation is initially explored in the area of computer vision [398] and then explored in natural language processing [399, 400, 401, 402]. When compared to computer vision, text data augmentation is more challenging because of the discrete nature of text. Data augmentation can be done at character, word and sentence levels. Character-level data augmentation approaches involve random deletion, addition, exchange or insertion of characters [403, 404]. For example, in the case of keyboard augmentation, a random character is replaced with its neighbour based on the QWERTY layout [403]. Similar [MISSING_PAGE_FAIL:36]"
    },
    {
      "title": "7.2.2 Data Generation",
      "text": "**Research works exploring GLLMs for data generation-based data augmentation.** The research community explored GLLMs for data generation-based data augmentation in various NLP tasks like dialogue generation [410], training smaller LLMs [411, 416], common sense reasoning [412], hate speech detection [413], undesired content detection [414], question answering [415, 425], intent classification [143], relation extraction [155, 422], instruction tuning [417, 418], paraphrase detection [420], tweet intimacy prediction [421], named entity recognition [422], machine translation [424] etc. GLLM-based data generation for data augmentation is explored in multiple domains like general [143, 155, 412, 413, 414, 415, 416, 417, 418, 420, 421, 422, 423, 424, 425, 426], social media [409, 401, 413, 421, 422, 423], news [423], scientific literature [155, 420], healthcare [410, 415, 422], dialogue [419], programming [411] etc. Table IX presents a summary of research works exploring GLLMs for data generation-based data augmentation. Some of the research works explored GLLMs for data generation-based data augmentation in various text classification tasks [409, 413, 421, 423, 422, 423]. For example, Hartvigsen et al. [413] used GPT-3 with demonstration-based prompting to create a large-scale synthetic dataset for the detection of implicit hate speech. Here, the authors explored a variant of constrained beam search to ensure subtle toxicity in the generated examples. Michail et al. [421] investigated the effectiveness of ChatGPT-generated synthetic data to fine-tune multilingual models for tweet intimacy prediction in the case of languages with no labelled instances. Here, ChatGPT is prompted with instructions and examples from a high-resource language and asked to generate new examples in the target language. Most of the existing research works use simple prompts for data generation, limiting the diversity of the generated synthetic data. To address this, Yu et al. [423] proposed a novel approach that leverages attributed prompts for data generation to increase the diversity in the generated data. Based on the evaluation on four topic classification datasets, the authors observed that (i) the proposed approach enhances the model performance and (ii) reduces the querying cost of ChatGPT by a large margin. Some of the research works explored GLLMs for data generation-based data augmentation in various information extraction tasks like relation extraction [155], relation classification [422] and named entity recognition [422]. Xu et al. [155] evaluated how effective is the GPT-3.5 model for relation classification. To address the \\begin{table} \\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \\hline **Paper** & **GLLMs Explored** & **Task(s)** & **Prompt Settings** & **Domain(s)** & **Language(s)** \\\\ \\hline [409] & ChatGPT & Text Classification & ZS & Social Media & Chinese \\\\ \\hline [410] & ChatGPT & Note2Dalogue Generation & ZS & Healthcare & English \\\\ \\hline [411] & GPT-3.5 & Training Phi-1 LLM & ZS & Programming & English \\\\ \\hline [412] & ChatGPT, GPT-4 & Cross-lingual Common Sense Reasoning & FS & General & Multiple Languages \\\\ \\hline [413] & GPT-3 & Hate Speech Detection & FS & Social Media & English \\\\ \\hline [414] & GPT-3 & Undesired Context Detection & ZS, FS & Social Media & English \\\\ \\hline [415] & ChatGPT, GPT-4 & Question Answering & ZS & Healthcare & English \\\\ \\hline [143] & GPT-3 & Intent Classification & ZS & General & English \\\\ \\hline [416] & GPT-3.5, GPT-4 & Training Smaller LLMs & ZS & General & English \\\\ \\hline [155] & GPT-3.5 & Relation Extraction & FS & General, Scientific Literature & English \\\\ \\hline [417] & GPT-4 & CoT Instruction Tuning & FS & General & English \\\\ \\hline [418] & GPT-4 & Instruction Tuning & ZS & General & English, Chinese \\\\ \\hline [419] & GPT-3 & Call segmentation, Topic extraction & ZS & Dialogue & English \\\\ \\hline [420] & GPT-3 & Paraphrase Detection & ZS & General, Scientific Literature & English \\\\ \\hline [421] & ChatGPT & Tweet Intimacy Prediction & FS & Social Media & Multiple Languages \\\\ \\hline [422] & ChatGPT & Named Entity Recognition, Relation Classification & ZS & Healthcare & English \\\\ \\hline [423] & ChatGPT & Topic Classification & ZS & News, Social Media & English \\\\ \\hline [424] & ChatGPT & Neural Machine Translation & ZS & General & Multiple Languages \\\\ \\hline [425] & GPT-3, Codex & Table Question Answering & ZS & General & English \\\\ \\hline [426] & GPT-4 & Text Generation Evaluation & ZS & General & Multiple Languages \\\\ \\hline \\end{tabular} \\end{table} TABLE IX: Summary of research works exploring GLLMs for data generation-based data augmentation. Here ZS represents zero-shot and FS represents few-shot. data scarcity problem in few-shot settings, the authors used the GPT-3.5 model to generate additional data. The prompt used for data generation consists of instance descriptions along with some example instances. Tang et al. [422] used ChatGPT in zero-shot settings to generate synthetic data for tasks like named entity recognition and relation classification in the healthcare domain. The authors showed that the model fine-tuned on this synthetic data outperforms zero-shot ChatGPT by a large margin in both tasks. Some of the research works explored GLLMs for data generation in LLM development stages, like LLM pre-training [411, 416] and instruction tuning [417, 418]. Gunasekar et al. [411] trained Phi-1, a code LLM using GPT-3.5 generated synthetic textbook and code data. Here, the training corpus includes 1B tokens of GPT-3.5 generated Python textbook and code data along with 6B tokens of code data from the web. Eldan et al. [416] explored GLLMs like GPT-3.5 and GPT-4 models to generate TinyStories, a synthetic dataset of stories with only the words understood by typical 3 to 4-year-old kids. The authors demonstrated that the GLLM generated dataset can be used to train smaller LLMs, which can generate coherent and consistent stories with near-perfect grammar. Instruction tuning requires large human-annotated datasets, which are often difficult to obtain. Stanford Alpaca 4 and Vicuna 5 showed the effectiveness of synthetic instruction tuning datasets generated using GPT-3.5 and ChatGPT, respectively. Inspired by the success of these models, Peng et al. [418] explored advanced models like GPT-4 to generate instruction-tuning datasets in English and Chinese languages. The experiment results showed that GPT-4 generated instruction tuning datasets further enhance the zero-shot performance of LLaMA models. Liu et al. [417] used GPT-4 to generate LogiCoT, a synthetic dataset of CoT rationales. This dataset can be used for instruction tuning the LLMs to enhance their logical reasoning abilities. Footnote 4: [https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html) Footnote 5: [https://lmsysys.org/blog/2023-03-30-vicuna/](https://lmsysys.org/blog/2023-03-30-vicuna/)"
    },
    {
      "title": "8 Detecting Gllm Generated Text",
      "text": "**Overview.** GLLMs demonstrated extraordinary human-like capabilities to understand user queries, follow the instructions and then answer the user queries with high-quality content. Apart from responding to user queries, these models can also generate news articles, research papers, code and essays with human-like fluency. With the ability to generate text with human-like fluency, these models are widely adopted in a variety of real-world applications like writing assistants, coding assistants, chatbots, etc [428]. Although there is a lot of excitement about GLLMs and their applications in recent times, there are also growing concerns regarding the potential misuse of these models for illegal activities [429], such as fake news on social media platforms [430, 431], fake reviews on e-commerce websites [432], fake research papers [433], academic fraud [434], etc. For example, these models can be easily used by malicious users to create fake news [430, 431] and propagate on social platforms at a large scale to exaggerate or manipulate the facts to get an undue advantage, especially during political campaigns. Similarly, students can use these models to write their assignments or generate code for their projects [434], and GLLM generated fake research papers [433] can have a serious impact on the scientific community as these papers are written without conducting any experiments. There is a strong need for the development of approaches to detect GLLM generated text, as there are growing concerns regarding the misuse of GLLMs. Such approaches help to distinguish the GLLM generated text from human-generated text and verify the source as well as the authenticity of the information. However, detecting GLLM generated text is more challenging as models like ChatGPT and GPT-4 can generate content with human-like fluency. **Research exploring the detection of GLLM generated text.** To avoid misuse and ensure the safe use of these models, the research community focused on developing approaches to identify the GLLM generated text accurately. The recent research works explored the detection of GLLM generated text in multiple domains like scientific literature [435, 436, 437, 438], academic [439, 440], healthcare [441, 442, 443], news [443], legal [442, 442], social media [432, 438], Finance [429] etc. Most of the research works focused on the English language, while a few research works focused on other languages like Japanese [436], German [438] and Spanish [440]. Table 20 presents a summary of research works exploring the detection of GLLM generated text. Some of the research works focused on assessing the effectiveness of the existing machine-generated text detection tools to detect GLLM generated text. A number of online tools are available, ranging from simple classifiers based on logistic regression to advanced classifiers based on pretrained language models to detect ChatGPT-generated text. To assess the effectiveness of these tools, Pegoraro et al. [444] introduced a dataset having ChatGPT-generated responses for questions from various domains like finance, medicine, etc., and user-generated responses from social media platforms. The comprehensive evaluation showed that the maximum success rate of these tools is less than 50% only, which leaves a lot of room for improvement. Orenstrakh et al. [440] evaluated the effectiveness of eight popular detectors using three metrics, namely resilience, false positives and accuracy. The authors observed that CopyLeaks, GPTKit and GLTR achieve the best results for the metrics accuracy, false positives and resilience. However, all these detectors struggle with non-English languages and paraphrased LLM-generated text. There is a lack of comprehensive evaluation benchmark to detect machine-generated text as the existing approaches use different models, datasets and settings. To address this, He et al. [447] proposed MGTBench, the first machine-generated text detection benchmark. Evaluation on this benchmark showed that, except for the ChatGPT detector [429] and LM detector [453], the performance of other detectors is not satisfactory. Guo et al. [429] introduced the HC3 dataset, having human-authored and ChatGPT-generated responses to questions from multiple domains like legal, healthcare, finance, psychology, etc. The performance of existing detection approaches on the HC3 dataset is just satisfactory, and linguistic analysis showed that human-authored answers are short in length but use a large vocabulary compared to ChatGPT-generated \\begin{table} \\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \\hline **Paper** & **Detect** & **Approach** & **Satisfactory Performance** & **Training Free** & **Domain(s)** & **Language(s)** \\\\ \\hline [444] & ChatGPT generated text & Evaluate multiple online tools & No & - & Multiple domains & English \\\\ \\hline [435] & GPT-3 generated text & Classifiers based on machine learning models like LR, SVM and deep learning models like LSTM and BEKI & Yes & No & Scientific Literature & English \\\\ \\hline [436] & ChatGPT and GPT-4 generated text & Classifier based on random forest and stylometric features & Yes & No & Scientific Literature & Japanese \\\\ \\hline [439] & GPT-3 and ChatGPT generated text & Classifier based on models like SVM and RoBERTa & Yes & No & Academic & English \\\\ \\hline [437] & ChatGPT generated text & Classifier based on models like RoBERTa & No & No & Scientific Literature & English \\\\ \\hline [441] & ChatGPT generated text & Classifier based on models like BERT & Yes & No & Healthcare & English \\\\ \\hline [440] & ChatGPT generated text & Evaluate multiple online tools & Yes & - & Academic & English, Spanish \\\\ \\hline [443] & GPT-3 generated text & Evaluate human evaluators & No & - & \\begin{tabular}{} \\end{tabular} & English \\\\ \\hline [442] & ChatGPT and GPT-4 generated text & Classifier based on models like BERT and RoBERTa & Yes & No & \\begin{tabular}{} \\end{tabular} & English \\\\ \\hline [438] & GPT-3, S, ChatGPT and GPT-4 generated text & Training free divergent N-gram Analysis & Yes & Yes & \\begin{tabular}{} \\end{tabular} & English, German \\\\ \\hline [445] & ChatGPT generated text & Evaluate the robustness of existing detectors & No & - & General & English \\\\ \\hline [446] & ChatGPT generated text & Evaluate existing plagiarism tools & No & - & General & English \\\\ \\hline [447] & ChatGPT generated text & Propose benchmark and evaluate existing detectors & Yes & - & General & English \\\\ \\hline [432] & ChatGPT generated text & Propose novel approach based on DistilBERT and SHAP to detect and explain & Yes & No & Social Media & English \\\\ \\hline [429] & ChatGPT generated text & Introduce new dataset and evaluate multiple existing detection models & Yes & - & \\begin{tabular}{} \\end{tabular} & English \\\\ \\hline [448] & GPT-3 and ChatGPT-based books & Propose FLAIR to detect online GPT-3 and ChatGPT-based bots & Yes & Yes & General & English \\\\ \\hline [449] & ChatGPT generated text & Classifiers based on models like RoBERTa and T5 & Yes & No & General & English \\\\ \\hline [428] & ChatGPT generated text & Propose a zero-shot approach based on local optimality & Yes & Yes & General & English \\\\ \\hline [450] & ChatGPT generated text & Propose an approach based on Siamese Network and binary classifier & Yes & No & General & English \\\\ \\hline [451] & ChatGPT & Trains classifier and polish ratio models to detect and explain & Yes & No & General & English \\\\ \\hline [452] & GPT-3.5 generated text & Evaluate robustness using paraphrase attacks & No & - & General & English \\\\ \\hline \\end{tabular} \\end{table} TABLE II: Summary of research works exploring the detection of GLLM generated text. answers. Some of the research works focused on developing approaches based on trained classifier models to detect GLLM generated text. Theocharopoulos et al. [435] evaluated the effectiveness of classifiers based on models like logistic regression, support vector machine, LSTM, and BERT to identify GPT-3 generated scientific abstracts. The LSTM-based classifier with word2vec embeddings achieves an accuracy of more than 98% and outperforms other classifiers. Zaitsu et al. [436] observed that LLM-generated texts differ significantly from human-written texts in terms of stylometric features. The authors demonstrated that random forest trained with different stylometric features can identify the LLM-generated Japanese text with 100% accuracy. Liu et al. [439] reported that fine-tuned RoBERTa model achieves an accuracy of more than 90% on the AruGPT dataset of human-written and GLLM generated argumentative essays. Moreover, linguistic analysis revealed that GLLM generated texts tend to be more complex syntactically, while human-generated texts are lexically more complex. To facilitate the development of a ChatGPT-written abstract detector, Yu et al. [437] introduced CHEAT, a large dataset of ChatGPT and human-written abstracts. Based on the evaluation of multiple existing approaches like ZeroGPT, OpenAI detector, ChatGPT-detector-roberta [429] and ChatGPT-qa-detector-roberta [429], the authors reported that performance is away from satisfactory and the human involvement further increases the detection difficulty. Zhan et al. [442] treated the detection of LLM generated as a binary classification problem and proposed a novel approach based on fine-tuned RoBERTa model. The authors reported that the proposed approach exhibits good performance and also has the ability to detect the text generated using a detection evasion technique. Mitrovic et al. [432] proposed a novel approach based on DistilBERT [92] and SHAP [454] to detect the machine-generated text and explain the reasoning. The proposed approach achieves an accuracy of 79%, and based on the explanations, the authors observed that ChatGPT-generated text maintains a polite tone, lacks specific details and generally refrains from expressing emotions. Chen et al. [449] introduced OpenGPText, which includes ChatGPT-generated paraphrased text. The authors reported that fine-tuned classifiers based on models like RoBERTa and T5 can achieve impressive results in detecting ChatGPT-generated text with an accuracy of more than 97%. Yu et al. [450] introduced GPT-Pat, a novel approach based on ChatGPT, a Siamese network and binary classifier, to detect machine-generated text effectively. The proposed approach enhances the SOTA accuracy by more than 12% and also exhibits better robustness to attacks like re-translation and text polishing. Yang et al. [451] focused on detecting GLLM-polished text, which is more challenging and useful in real-world applications. The proposed approach involves training a classification model to identify the machine-generated text and a polish ratio (regression) model to explain the ChatGPT involvement. A Polish ratio of 0.2 indicates ChatGPT involvement and a value of more than 0.6 represents the text is entirely ChatGPT generated. Training-based approaches to detect LLM-generated text have limited flexibility, especially when used for new domains [438]. To overcome this drawback, some of the research works focused on developing training-free approaches to detect GLLM generated text. Yang et al. [438] proposed DNA-GPT, a training-free approach based on divergent n-gram analysis. With the proposed approach, the authors achieved SOTA results on both English and German datasets. Wang et al. [448] proposed a novel framework called FLAIR to detect LLM-based bots with a single question in an effective way. The results showed that the proposed approach is effective and a good alternative to existing CAPTCHA-based approaches. Mireshghallah et al. [428] investigated whether models other than the generator can be used to identify machine-generated text. In general, smaller models serve as more effective universal text detectors. These models exhibit better accuracy in identifying text produced by both small and larger models. For example, OPT-125M achieves better results compared to the GPT-J 6B model in detecting ChatGPT-generated text. Some of the research works focused on assessing the robustness of machine-generated text detectors towards different attacks. Shi et al. [445] evaluated the robustness of existing detectors using attacks like synonym word replacement and writing style modification. The authors implemented both attacks using LLMs. The results showed that the existing detectors are not robust to the attacks, which emphasizes the need for more robust and reliable detectors to detect and avoid the misuse of LLMs. Krishna et al. [452] showed that existing detectors like OpenAI detector, GPTZero and DetectGPT [463] are not robust to paraphrase attacks. For example, paraphrase attacks result in a drop of more than 65% accuracy in the case of DetectGPT. Some of the research works focused on assessing the effectiveness of humans in identifying GLLM generated text. For example, Clark et al. [443] observed that non-expert evaluators are unable to differentiate GPT-3 generated text from human-authored text in three different domains, namely news, recipes and stories. The reason for this is the evaluators arrived at their decisions based on surface-level features without considering the advanced text generation capabilities of the GPT-3 model."
    },
    {
      "title": "9 Robustness Of Gllms",
      "text": "**Overview.** GPT-3 family large language models achieve impressive performances in zero and few-shot settings in many NLP tasks. In some tasks like text classification [144], relation extraction [156], etc. GLLMs without any explicit fine-tuning outperform state-of-the-art fine-tuned models. For example, Sun et al. [144] demonstrated that InstructGPT, with the advanced promptingstrategy, achieves SOTA results using just 16 examples on four text classification datasets. Similarly, Wan et al. [156] achieved SOTA results in relation extraction with the GPT-RE framework. However, to increase the reliability of these models in real-world applications, especially in critical domains like medicine, it is essential to systematically study the robustness of these models in various scenarios. Adversarial robustness refers to the model's ability to maintain good performance even in the case of deliberately crafted instances [464, 465]. These instances are called adversarial instances and are carefully designed by making subtle changes in the original inputs to deceive the model. Out-of-distribution (OOD) instances refer to examples that differ significantly from the data distribution used to train the model [466]. These instances fall outside the range of the model's training data and present challenges to the model's performance and generalization ability. Some of the recent research works focused on evaluating the robustness of GLLMs to out-of-distribution instances [461, 456], adversarial prompts [458, 459, 460] and adversarial inputs [455, 457, 462, 455] in one or more natural language processing tasks. Table II presents a summary of research works assessing GLLMs robustness to out-of-distribution instances, adversarial prompts and adversarial inputs. **Research works exploring GLLMs robustness.** Some of the research works evaluated the robustness of GLLMs in specific tasks like semantic parsing [457], code generation [459], table question answering [425], multi-choice question answering [461] and text-to-SQL generation [462]. Zhuo et al. [457] reported that Codex-based semantic parsers are not robust to adversarial examples, and the robustness can be enhanced using few-shot in-context learning. Shirafuji et al. [459] studied the robustness of GPT-3 family models like Codex, InstructGPT, and ChatGPT to adversarial prompts in code generation task. The authors observed that InstructGPT and ChatGPT exhibit better robustness compared to Codex. However, there is much room for improvement, indicating that quality code generation requires well-designed prompts. Zhao et al. [425] proposed RobuT, a benchmark to systematically study the robustness of large language models to adversarial inputs in table question answering. The authors reported that GLLMs like GPT-3 and Codex exhibit better robustness than fine-tuned models. Moreover, the authors demonstrated that GLLM generated adversarial inputs can enhance the adversarial robustness of fine-tuned models. Liu et al. [461] reported that ChatGPT and GPT-4 perform well in multiple choice question answering but struggle to answer out-of-distribution questions. Liu et al. [462] showed that ChatGPT exhibits impressive zero-shot performance in Text-to-SQL generation. Moreover, ChatGPT demonstrates better robustness to adversarial inputs than SOTA models in text-to-SQL generation. Some of the research works evaluated the GLLM robustness in multiple natural language understanding and generation tasks [455, 456, 458, 460]. Chen et al. [455] assessed the robustness of GPT-3 and GPT-3.5 models on 21 datasets covering nine natural language understanding tasks. Here the authors used adversarial text transformations from TextFlint [467]. The authors observed that the models are robust in tasks like machine reading comprehension and exhibit performance degradation of more than 35% in tasks like sentiment analysis and natural language inference. Wang et al. [456] evaluated the robustness of GPT-3.5 and ChatGPT models on adversarial and out-of-distribution (OOD) samples on nine datasets covering four NLU tasks and machine translation. The authors observed that ChatGPT exhibits good performances on adversarial and OOD samples, but still, there is much room for improvement. Zhu et al. [458] developed PromptBench, a benchmark with more than 4k adversarial prompts to evaluate the robustness of large language models to adversarial prompts. The benchmark covers 13 datasets spanning eight tasks, including four NLU tasks. The \\begin{table} \\begin{tabular}{|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|} \\hline **Paper** & **GLLMs Explored** & **Task(s)** & **Prompt Settings** & **Robustness** & **Domain(s)** & **Language(s)** \\\\ \\hline [455] & GPT-3, GPT-3.5 & Nine NLU Tasks & ZS, FS & Adversarial Input & General & English \\\\ \\hline [456] & GPT-3.5, ChatGPT & Four NLU Tasks, Machine Translation & ZS & Out of Distribution & General, Medical & English \\\\ \\hline [457] & Codex & Semantic Parsing & ZS, FS & Adversarial Input & Programming & English \\\\ \\hline [458] & ChatGPT & Eight Tasks including Four NLU tasks & ZS, FS & Adversarial Prompt & General & English \\\\ \\hline [459] & Codex, InstructGPT, ChatGPT & Code Generation & ZS & Adversarial Prompt & Programming & English \\\\ \\hline [425] & GPT-3, Codex & Table Question Answering & FS & Adversarial Input & General & English \\\\ \\hline [460] & ChatGPT & Fourteen IE Tasks & ZS, FS & Adversarial Prompt & General & English \\\\ \\hline [461] & ChatGPT, GPT-4 & Question Answering & ZS, FS & Out-of-Distribution & General & English \\\\ \\hline [462] & ChatGPT & Text-to-SQL Generation & ZS & Adversarial Input & General & English \\\\ \\hline \\end{tabular} \\end{table} TABLE II: Summary of research works exploring GLLMs robustness to out-of-distribution instances, adversarial prompts and adversarial inputs. Here ZS represents zero-shot, and FS represents few-shot. authors observed that GLLMs are not robust to adversarial prompts. Moreover, word-level attacks are the most effective, which results in a performance drop of more than 30%. Based on the evaluation of ChatGPT on fourteen information extraction sub-tasks, Han et al. [460] showed that ChatGPT is vulnerable to adversarial prompts, i.e., the performance is greatly affected by including irrelevant context in the prompt."
    },
    {
      "title": "10 Gllms As Evaluators",
      "text": "**Overview.** Natural language processing tasks can be broadly classified into natural language understanding (NLU) and natural language generation (NLG). NLU involves the interpretation of text, while NLG involves generating human-like text. The evaluation of NLU outputs is pretty straightforward, while the evaluation of NLG outputs is challenging because of the diversity and inherent complexity of the text [468]. Moreover, the NLG evaluation involves assessing the generated text outputs in multiple dimensions, such as coherence, fluency, naturalness and semantic consistency. Human evaluation and automatic evaluation are two existing approaches for NLG evaluation. The human evaluation depends on competent annotators for an accurate and reliable assessment [469]. **Human Evaluation vs Automatic Evaluation.** Human evaluation is treated as the gold standard, but it is time-consuming, expensive, difficult to scale, inconsistent, and not reproducible [468, 481]. To address the issues with human evaluation, automatic evaluation metrics are developed, which fall broadly into two categories: n-gram-based and embedding-based. N-gram-based metrics assess the quality based on the lexical overlap between the generated and reference texts. Some of the commonly used n-gram-based metrics are BLEU [487], ROUGE [488] and METEOR [489]. However, these metrics have a poor correlation with human scores because of their inability to capture semantic meaning [490]. Later, with the evolution of transformers and pretrained language models, the researchers developed embedding-based metrics like BERTScore [491], MoverScore [492], BARTScore [493], CodeBERTScore [494] etc. These metrics leverage the pretrained language models and assess the quality based on the semantic similarity between the generated and reference text. The main drawback of the existing automatic evaluation metrics is the requirement for references, which are difficult to obtain, especially in low-resource domains. Moreover, with just a few references, it is not possible to get an accurate and reliable assessment as few references cannot account for all the semantic variations [468]. So, there is a strong need for automatic evaluation metrics which are reference-free. **GLLM-based Evaluation.** Recently, with the huge success of GLLMs in most of the NLP tasks, the research community focused on developing automatic evaluation metrics based on these models. These models possess the ability of in-context learning, while instruction tuning enables these models to align themselves with human evaluation [2]. These two abilities enable these models to imitate the behaviour of human evaluators, who typically evaluate natural language generation task outputs by understanding instructions and the given examples. The GLLM-based evaluation metrics demonstrate a strong correlation with human scores even in the absence of reference outputs [472, 477]. Table XII presents a summary of research works exploring GLLM-based evaluation for various natural language generation tasks. **Research works exploring GLLM-based evaluation.** The NLP researchers proposed various GLLM-based evaluation frameworks to evaluate the outputs of various NLG tasks like code generation [470], text style transfer [471], text summarization [468, 472, 475, 476, 477, 478, 480, 483], dialogue generation [472, 477, 473, 474, 478, 485], machine translation [468, 467, 468, 469, 468], story generation [468, 468], paraphrase generation [468], text-to-image synthesis [285], data-to-text generation [477, 483], image captioning [480], text generation [481], open-ended question answering [484, 486]. Most of the research works proposed evaluation frameworks using direct prompting, while some of the research works introduced evaluation frameworks based on advanced prompting strategies like chain-of-thoughts [470, 472] and error analysis prompting [474]. Some of the proposed evaluation frameworks work with and without references [470, 473, 483], while some of them require references [471, 474, 475, 476, 477, 478, 479, 481, 482, 486]. Lai et al. [471] investigated how effective ChatGPT is to evaluate text style transfer task along three dimensions: fluency, content and style. The model achieves good correlations with human judgements, and the best results are obtained by using separate prompts for each dimension evaluation. Kocmi et al. [473] proposed GEMBA, a GPT-based metric to assess translation output quality, with references being optional. The authors reported that GPT-3.5 and higher models are only useful for the assessment, and GPT-4 achieves the best results. Based on the evaluation of four natural language generation tasks, paraphrase generation, text summarization, story generation and dialogue response generation, Chen et al. [468] showed that explicit score with greedy decoding strategy is the best way to assess NLG outputs using GLLMs like ChatGPT. Luo et al. [475] evaluated ChatGPT's ability as a factual inconsistency evaluator for text summarization task. Experiment results showed that ChatGPT outperforms existing metrics on most of the datasets. Shen et al. [476] explored how effective ChatGPT can be as a zero-shot evaluator for abstractive summarization systems using different evaluation methods like likert scaling [495] and head-to-head comparisons [496]. Extensive analysis showed that likert scaling implemented as a multiple-choice question gives the best and most stable results. Liu et al. [478] designed a novel approach which uses BRIO [497], a contrastive learning-based method, to train smaller models like BART for text summarization and metrics like GPTScore [477] or GPTRank for evaluation. The contrastive learning training method helps the model to effectively utilize the supervision signal offered by the reference LLMs. The evaluation showed that the proposed approach helps the smaller model to outperform LLMs like GPT-3 and ChatGPT. Gao et al. [479] evaluated ChatGPT for text summarization using various human evaluation methods and reported that (i) ChatGPT-based evaluation is both cost-effective and reproducible, unlike human evaluation, (ii) the performance of ChatGPT-based evaluation is highly dependent on the prompt design, and (iii) ChatGPT generated explanations correlates with its scores. Jain et al. [482] explored the effectiveness of the GPT-3.5 model as a multi-dimensional evaluator of text summarization. The authors reported that using in-context learning, GPT-3.5-based evaluation achieves SOTA performances on factual consistency and relevance dimensions. Based on the evaluation of five datasets covering text summarization, story generation and data-to-text generation, Wang et al. [483] reported that ChatGPT as an evaluator (i) exhibits good correlations with human scores, especially in the case of story generation task and (ii) is prompt sensitive. Bai et al. [484] introduced a novel evaluation framework called Language-Model-as-an-Examiner to evaluate open-ended questions. In this framework, GLLM acts as a knowledgeable examiner, generates questions using its own knowledge and then does the reference-free evaluation. Yang et al. [485] developed the BigTrans model (based on LLaMA -13B model) with a multilingual translation capacity of more than 100 languages. GPT-4 based assessment showed that BigTrans performance is on par with ChatGPT and Google translate. Zheng et al. [486] explored GPT-4 as a judge to evaluate open-ended question answering using two newly introduced benchmarks MT-Bench and Chatbot Arena. The experiment results showed that GPT-4 achieves more than 80 Unlike the above-discussed research works, which used direct prompting, some of the works explored advanced prompting to offer better guidance and context for the GLLM evaluator. Zhuo et al. [470] developed a \\begin{table} \\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \\hline **Paper** & **GLMs Explored** & **Task(s)** & **Prompt Settings** & **References Required** & **Domain(s)** & **Language(s)** \\\\ \\hline [470] & ChatGPT & Code Generation & ZS & Optional & Programming & Five Programming Languages \\\\ \\hline [471] & ChatGPT & Text Style Transfer & ZS & Yes & General & English \\\\ \\hline [472] & ChatGPT, GPT-4 & Text Summarization, Dialogue Generation & ZS & No & General & English \\\\ \\hline [473] & GPT, GPT-3.5, ChatGPT, GPT-4 & Machine Translation & ZS & Optional & General & English, German, Chinese, Russian \\\\ \\hline [468] & GPT-3.5, ChatGPT & Text Summarization, Dialogue Generation, Story Generation & ZS & No & General & English \\\\ \\hline [474] & GPT-3.5, ChatGPT & Machine Translation & ZS, FS & Yes & General & English, Chinese, German \\\\ \\hline [475] & ChatGPT & Text Summarization & ZS & No & General & English \\\\ \\hline [476] & ChatGPT & Text Summarization & ZS & No & General & English \\\\ \\hline [285] & GPT-4 & Text-to-Image Synthesis & ZS & N/A & General & English \\\\ \\hline [426] & GPT-4 & Machine Translation & ZS & Yes & General & English, German, Russian \\\\ \\hline [477] & GPT-3, GPT-3.5 & Dialogue Generation, Machine Translation, Text Summarization, Data-to-Text Generation & ZS, FS & No & General & English, Chinese \\\\ \\hline [478] & GPT-3, ChatGPT & Text Summarization & ZS & No & General & English \\\\ \\hline [479] & ChatGPT & Text Summarization & ZS & No & General & English \\\\ \\hline [480] & GPT-3.5 & Machine Translation, Text Summarization, Image Caption & ZS & Yes & General & English \\\\ \\hline [481] & ChatGPT, GPT-4 & Text Generation & ZS & No & General & English \\\\ \\hline [482] & GPT-3.5 & Text Summarization & ZS & No & General & English \\\\ \\hline [483] & ChatGPT & Text Summarization, Story Generation, Data-to-Text Generation & ZS & Optional & General & English \\\\ \\hline [484] & GPT-4 & Open-ended Question Answering & ZS & No & General & English \\\\ \\hline [485] & GPT-4 & Machine Translation & ZS & Yes & General & Multiple Languages \\\\ \\hline [486] & GPT-4 & Open-ended Question Answering & ZS & No & General & English \\\\ \\hline \\end{tabular} \\end{table} TABLE II: Summary of research works exploring GLLM-based evaluation for natural language generation tasks. Here ZS represents zero-shot, and FS represents few-shot. code generation evaluation framework based on ChatGPT and demonstrated that the proposed framework outperforms CodeBERTScore [494] consistently across multiple programming languages. Moreover, the performance of the evaluation framework can be enhanced using references and zero-shot CoT prompting. Liu et al. [472] proposed G-EVAL, a novel framework based on GPT-4 for the assessment of natural language generation tasks. The proposed framework uses CoT prompting and a form-filling paradigm. Here, CoT prompting enhances the performance of G-EVAL by offering more guidance and context. The performance of ChatGPT-based evaluation in segment-level machine translation is poor. To overcome this, Lu et al. [474] proposed a novel prompting called Error Analysis (EA) prompting, which combines error analysis [498] and CoT prompting. The authors showed that with EA prompting, ChatGPT can assess translations at the segment level much better. Some of the research works explored GLLMs for the evaluation of multi-modal AI tasks [285], fine-tuning open-source LLM evaluators [426], and paraphrasing references to enhance existing metrics based on pre-trained language models [480]. For example, Lu et al. [285] introduced LLMScore (based on GPT-4), a new metric which can effectively capture both image and object-level compositionality for text-to-image synthesis evaluation. Some of the research works explored these models to fine-tune open-source LLMs so that they can be used as evaluators, which makes the evaluation less expensive. For example, Xu et al. [426] introduced InstructScore, a novel and explainable metric based on fine-tuned LLaMA model for text generation evaluation. Here the authors use GPT-4 generated synthetic data to fine-tune the LLaMA model. InstructScore can generate an error diagnostic report having error details along with an explanation. Natural language generation evaluation using few references results in poor correlation with human judgements. To overcome this drawback, Tang et al. [480] introduced Para-Ref, which leverages LLMs to increase the number of references by paraphrasing. The evaluation on three NLG tasks, text summarization, machine translation and image caption, showed that the proposed approach enhances the correlation of sixteen automatic evaluation metrics with human judgements by a good margin. Some of the research works focused on addressing the limitations of using GLLMs as evaluators. For example, Wang et al. [481] demonstrated positional bias in GLLM-based evaluation, i.e., the order of candidate responses can significantly influence the results. The authors demonstrated that the two proposed strategies, namely multiple evidence calibration and balanced position calibration, can reduce the bias and enhance the correlation with human judgements."
    },
    {
      "title": "11 Future Research Directions",
      "text": ""
    },
    {
      "title": "_Enhance Robustness Of Gllms_",
      "text": "GLLMs achieved promising results across various NLP tasks in zero and few-shot settings across various NLP tasks. In some of the tasks like data labelling [373, 374, 375], [383], text classification [144], relation extraction [156], question answering [132, 179], keyphrase generation [217], etc., these models achieved even SOTA results. However, some of the recent research works exposed the brittleness of these models towards out-of-distribution inputs [461, 462], adversarial prompts [458, 459, 460] and inputs [455, 457, 462, 463]. For example, Liu et al. [461] reported that ChatGPT and GPT-4 perform well in multiple choice question answering but struggle to answer out-of-distribution questions. Similarly, Chen et al. [455] observed more than 35% performance degradation for GPT-3 and GPT-3.5 models in tasks like sentiment analysis and natural language inference for adversarial inputs. The brittleness towards out-of-distribution and adversarial inputs makes these models unreliable and limits their practical utility, especially in sensitive domains. So, it is necessary for the research community to focus more on this research direction to make GLLMs more robust and enhance their reliability and usage."
    },
    {
      "title": "_Red Teaming_",
      "text": "Red teaming involves an assessment to expose undesirable model behaviours like generating harmful text [499, 500, 501, 502]. GLLMs trained over large volumes of text data with a simple next-word prediction objective are surprisingly good at generating text with human-like fluency. However, the other side is that these models sometimes generate harmful text. For example, Risabh et al. [499] observed that GLLMs like ChatGPT and GPT-4 generate answers to more than 60% of harmful queries. One of the possible reasons for this undesirable behaviour of GLLMs is that data used for pretraining these models includes toxic, biased and noisy text to some extent [499]. This unwanted behaviour of generating harmful text raises concerns and limits the scalable deployment of these models for public use. We can expect more research in future to expose such undesirable behaviour in various scenarios and eventually enhance the safety alignment as well as the safe use of GLLMs."
    },
    {
      "title": "_State-Of-The-Art Results Across Nlp Tasks_",
      "text": "In the beginning, GLLMs like GPT-3 achieved impressive performances in zero and few-shot settings across NLP tasks. Advanced GLLMs like ChatGPT and GPT-4 further pushed the results but still lag behind SOTA results achieved by pretrained language models fine-tuned based on supervised learning. Later, with the evolution of advanced prompting strategies and novel approaches, GLLMs are able to achieve SOTA results in some of the NLP tasks. For example, InstructGPT with CARP prompting strategy using just 16 examplesachieves SOTA results on four text classification datasets [144]. Similarly, Wan et al. [156] achieved SOTA results in relation extraction with the novel GPT-RE framework. Yang et al. [179] proposed a novel approach which uses GPT-3 as an implicit knowledge source and achieves SOTA results in knowledge-based visual question answering. In future, we can expect more focus from the research community to achieve SOTA results using GLLMs in as many NLP tasks as possible, which will be treated as a further push towards artificial general intelligence. Moreover, this eliminates the painful process of labelling large amounts of data and then fine-tuning pretrained language models separately for each downstream task."
    },
    {
      "title": "_Robust Approaches To Detect Gllm Generated Text_",
      "text": "The ability to generate text with human-like fluency resulted in the wide adoption of GLLMs in various real-world applications like writing assistants, coding assistants, and chatbots [428]. There is a growing concern regarding the misuse of these models for various illegal activities [429], like fake news on social media platforms [430, 431], fake reviews on e-commerce websites [432], fake research papers [433], academic fraud [434], etc. The performance of existing approaches like DetectGPT, ZeroGPT, OpenAI detector, ChatGPT-detector-roberta and ChatGPT-qa-detector-roberta is not satisfactory [437, 444]. Moreover, the existing approaches are not robust to various attacks like paraphrasing, synonym word replacement and writing style modification [445, 452]. So, there is a great need for better approaches which can reliably detect GLLM generated text and also robust to various attacks, including paraphrasing. With reliable and robust detection approaches, the misuse of GLLMs for various illegal activities can be reduced to a great extent."
    },
    {
      "title": "_Reduce Inference Costs_",
      "text": "GLLMs achieve impressive performances across NLP tasks, with SOTA results in some tasks. However, the downside of using GLLMs is the high inference costs [503, 504]. For example, a small business is required to spend more than $21,000 monthly to use GPT-4 for better customer support 6. Such high inference costs have become a burden to small and medium-sized companies. Recently, Chen et al. [503] proposed FrugalGPT, a novel framework involving multiple strategies like prompt adaptation and LLM approximation to reduce the inference costs of GLLMs. The inference costs of GLLMs increase with the prompt size as the inference cost is computed based on the number of tokens processed. Prompt adaptation focuses on reducing the size of the prompt by using fewer but effective examples or querying the GLLMs as a batch. LLM approximation uses cache to avoid querying GLLM for similar queries, which eventually reduces overall inference costs. Similarly, Cheng et al. [504] proposed batch prompting, which involves GLLM inference in batches rather than processing one sample individually. The authors demonstrated that the proposed prompting strategy reduces Codex model inference cost across ten datasets with little or no degradation in the performance. Future research in this direction will result in much better approaches which will further reduce the GLLM inference costs and make GLLM usage more affordable for companies. Footnote 6: [https://neoteric.eu/blog/how-much-does-it-cost-to-use-gpt-models-gpt-3-pricing-explained](https://neoteric.eu/blog/how-much-does-it-cost-to-use-gpt-models-gpt-3-pricing-explained)"
    },
    {
      "title": "_Enhance Performance In Domain-Specific Nlp Tasks_",
      "text": "Inspired by the success of GLLMs in general domain NLP tasks, the research community explored GLLMs for NLP tasks in specific domains like healthcare, legal, finance, etc. However, the performances of GLLMs in domain-specific NLP tasks are not as impressive as those achieved in general domain NLP tasks [336, 335, 346, 347, 356]. For example, Moradi et al. [326] reported that the BioBERT model outperforms GPT-3 in few-shot settings even though the BioBERT model is 514 times smaller than GPT-3. Chalkidis et al. [346] evaluated ChatGPT on the LexGLUE benchmark and reported that ChatGPT performs poorly on legal text classification datasets. Analyzing domain-specific texts is more challenging because of domain-specific terminology and abbreviations, complex language structures, etc. In domains like healthcare, finance and legal, domain experts use many words and abbreviations that are specific to the domain and not commonly found in general domain texts. There is a lot of scope to improve the performance of GLLMs in domain-specific NLP tasks, which reduces the bottleneck for the widespread adoption of these models in specific domains."
    },
    {
      "title": "_Handle Limited Context Length_",
      "text": "One of the major drawbacks of GLLMs is their limited context length [505, 506, 52]. The maximum context length of GLLMs lies in the range of 2049 tokens to 32,768 tokens7. This limited context length poses a challenge and becomes a bottleneck for GLLMs to handle long documents or maintain long conservations in which the number of tokens falls beyond the maximum context length. Recently, Li [505] proposed selective context, a novel approach to effectively utilize the limited context length by filtering out the less useful content in the input text. The authors demonstrated the effectiveness of the proposed approach using the ChatGPT model for question-answering and text summarization tasks across datasets having lengthy input instances. Future research in this direction will help in the evolution of more efficient approaches which will effectively utilize the limited context length and eliminate the bottlenecks for the application of GLLMs in tasks that require processing long inputs."
    },
    {
      "title": "_Ensure Fair Evaluation Of Gllms_",
      "text": "GLLMs achieved impressive performances across NLP tasks and have received much attention recently. However, one concern regarding the evaluation of GLLMs is data contamination, which refers to the presence of test data instances of downstream tasks in the training corpus of GLLMs [507, 46, 508]. The problem of data contamination is more relevant in the case of GLLMs because of their proprietary nature and non-disclosure of training corpus details. Recent research works have reported the problem of data contamination in GLLMs like ChatGPT [508] and GPT-4 [507]. For example, Golchin et al. [507] demonstrated that GPT-4 is contaminated with instances from text classification, natural language inference and text summarization datasets like WNLI [509], AG News [510] and XSUM [511]. Recently, golchin et al. [507] proposed a novel approach to detect data contamination for LLMs. Future research must focus on developing simple and effective approaches to identify data contamination and ensure fair evaluation, enhancing the reliability of impressive performances of GLLMs."
    },
    {
      "title": "_Reduce Hallucinations_",
      "text": "Despite the remarkable performances of GLLMs, there is a growing concern regarding their tendency to generate factually incorrect information [512, 513]. This tendency to generate text that doesn't align with existing world knowledge, deviates from the user's input or contradicts the context generated earlier is referred to as hallucination [512]. Hallucination is a serious problem yet to be addressed fully [514], and it reduces the reliability of GLLMs, which becomes a bottleneck for the adoption of GLLMs, especially in sensitive domains like healthcare [515]. Recently, some of the research works focused on evaluating hallucination in GLLMs [515], assessing the ability of GLLMs to identify hallucinations [516] and developing approaches to reduce hallucinations [517]. For example, Li et al. [516] proposed HaluEval, a novel benchmark to assess the ability of GLLMs to identify hallucinations. Peng et al. [517] introduced LLM-AUGMENTER, a novel approach that reduces hallucinations in ChatGPT without impacting the quality of generated responses. Considering the seriousness of the hallucination problem, we can expect more future research to identify and reduce hallucinations in GLLMs, which enhance their reliability and adoption across domains, including sensitive domains like healthcare."
    },
    {
      "title": "_Enhance The Performance Of Gllms For Non-English Languages_",
      "text": "The performance of GLLMs is not impressive in the case of non-English languages, especially in the case of languages with non-Latin scripts [366, 363, 131, 366]. This is because GLLMs are mostly pretrained on English text. For example, more than 90% of text in the pretraining corpus of the GPT-3 model is from the English language [366, 4]. Some of the possible options to enhance the performance of GLLMs for non-English languages are the use of English prompts [363, 131] and optimized tokenization [365]. There is a great need for better approaches to greatly enhance the performance of GLLMs for non-English languages, which increase their adoption across the globe and benefit users from non-English communities."
    },
    {
      "title": "12 Conclusion",
      "text": "In this survey paper, we provide a comprehensive review of GPT-3 family large language models in multiple dimensions covering more than 350 recent research papers. Here, we present foundation concepts, GPT-3 family large language models and discuss the performances of these models in various downstream tasks, specific domains and multiple languages. We also discuss data labelling, data augmentation and data generation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. Overall, this comprehensive survey paper on GPT-3 family large language models will serve as a good resource for both academic and industry people to stay updated with the latest research."
    },
    {
      "title": "Acknowledgments",
      "text": "The author would like to thank Ajit Rajasekharan for his encouragement and support."
    },
    {
      "title": "References",
      "text": "* [1] K. S. Kalyan, A. Rajasekharan, and S. Sangeeta, \"Ammu: A survey of transformer-based pretrained models in natural language processing,\" _arXiv preprint arXiv:2108.05542_, 2021. * [2] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray _et al._, \"Training language models to follow instructions with human feedback,\" _Advances in Neural Information Processing Systems_, vol. 35, pp. 27730-27744, 2022. * [3] K. S. Kalyan, A. Rajasekharan, and S. Sangeeta, \"Ammu: a survey of transformer-based biomedical pretrained language models,\" _Journal of biomedical informatics_, vol. 126, p. 103982, 2022. * [4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell _et al._, \"Language models are few-shot learners,\" _Advances in neural information processing systems_, vol. 33, pp. 1877-1901, 2020. * [5] T. Mikolov, K. Chen, G. Corrado, and J. Dean, \"Efficient estimation of word representations in vector space,\" _arXiv preprint arXiv:1301.3781_, 2013. * [6] J. Pennington, R. Socher, and C. D. Manning, \"Glove: Global vectors for word representation,\" in _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_, 2014, pp. 1532-1543. * [7] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, \"Enriching word vectors with subword information,\" _Transactions of the association for computational linguistics_, vol. 5, pp. 135-146, 2017. * [8] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, \"A convolutional neural network for modelling sentences,\" in _Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2014, pp. 655-665. H. Salehinejad, S. Sankar, J. Barfett, E. Colak, and S. Valaee (2017) Recent advances in recurrent neural networks. arXiv preprint arXiv:1801.01078. Cited by: SSII-A. * [19]S. Hochreiter and J. Schmidhuber (1997) Long short-term memory. Neural computation9 (8), pp. 1735-1780. Cited by: SSI. [MISSING_PAGE_POST] * [51] J. Huang and K. C.-C. Chang, \"Towards reasoning in large language models: A survey,\" _arXiv preprint arXiv:2212.10403_, 2022. * [52] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Railenau, and R. McHardy, \"Challenges and applications of large language models,\" _arXiv preprint arXiv:2307.10169_, 2023. * [53] X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang, \"A survey on model compression for large language models,\" _arXiv preprint arXiv:2308.07633_, 2023. * [54] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen, \"A survey on multimodal large language models,\" _arXiv preprint arXiv:2306.13549_, 2023. * [55] T. Young, D. Hazarika, S. Poria, and E. Cambria, \"Recent trends in deep learning based natural language processing,\" _iex Computational intelligentCe magazine_, vol. 13, no. 3, pp. 55-75, 2018. * [56] Y. Kim, \"Convolutional neural networks for sentence classification,\" in _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_. Association for Computational Linguistics, 2014. * [57] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \"Learning phrase representations using rnn encoder-decoder for statistical machine translation,\" in _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_. Association for Computational Linguistics, 2014, p. 1724. * [58] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He, \"A comprehensive survey on transfer learning,\" _Proceedings of the IEEE_, vol. 109, no. 1, pp. 43-76, 2020. * [59] D. W. Otter, J. R. Medina, and J. K. Kalita, \"A survey of the usages of deep learning for natural language processing,\" _IEEE transactions on neural networks and learning systems_, vol. 32, no. 2, pp. 604-624, 2020. * [60] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao, A. Zhang, L. Zhang _et al._, \"Pre-trained models: Past, present and future,\" _AI Open_, vol. 2, pp. 225-250, 2021. * [61] S. J. Pan and Q. Yang, \"A survey on transfer learning,\" _IEEE Transactions on Knowledge and data engineering_, vol. 22, no. 10, pp. 1345-1359, 2009. * [62] J. Blitzer, M. Dredze, and F. Pereira, \"Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification,\" in _Proceedings of the 45th annual meeting of the association of computational linguistics_, 2007, pp. 440-447. * [63] J. E. Van Engelen and H. H. Hoos, \"A survey on semi-supervised learning,\" _Machine learning_, vol. 109, no. 2, pp. 373-440, 2020. * [64] Y. Zhang and Q. Yang, \"A survey on multi-task learning,\" _IEEE Transactions on Knowledge and Data Engineering_, vol. 34, no. 12, pp. 5586-5609, 2021. * [65] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A large-scale hierarchical image database,\" in _2009 IEEE conference on computer vision and pattern recognition_. Ieee, 2009, pp. 248-255. * [66] M. Pagilardini, P. Gupta, and M. Jaggi, \"Unsupervised learning of sentence embeddings using compositional n-gram features,\" in _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, 2018, pp. 528-540. * [67] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, \"Deep contextualized word representations,\" in _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_. New Orleans, Louisiana: Association for Computational Linguistics, Jun. 2018, pp. 2227-2237. [Online]. Available: [https://cathathology.org/N18-1202](https://cathathology.org/N18-1202) * [68] R. Anil, A. M. Dai, O. Pirat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen _et al._, \"Palm 2 technical report,\" _arXiv preprint arXiv:2305.10403_, 2023. * [69] T. Lin, Y. Wang, X. Liu, and X. Qiu, \"A survey of transformers,\" _AI Open_, 2022. * [70] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, \"Pre-trained models for natural language processing: A survey,\" _Science China Technological Sciences_, vol. 63, no. 10, pp. 1872-1897, 2020. * [71] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \"Roberta: A robustly optimized bert pretraining approach,\" _arXiv preprint arXiv:1907.11692_, 2019. * [72] J. Zhang, Y. Zhao, M. Saleh, and P. Liu, \"Pegasus: Pre-training with extracted gap-sentences for abstractive summarization,\" in _International Conference on Machine Learning_. PMLR, 2020, pp. 11 328-11 339. * [73] S. Doddapaneni, G. Ramesh, M. M. Khapra, A. Kunchukuttan, and P. Kumar, \"A primer on pretrained multilingual language models,\" _arXiv preprint arXiv:2107.00676_, 2021. * [74] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, \"mts: A massively multilingual pre-trained text-to-text transformer,\" in _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 2021, pp. 483-498. * [75] Y. Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad, M. Lewis, and L. Zettlemoyer, \"Multilingual denoising pre-training for neural machine translation,\" _Transactions of the Association for Computational Linguistics_, vol. 8, p. 726-742, 2020. * [76] D. Kakwani, A. Kunchukuttan, S. Golla, N. Gokul, A. Bhattacharyya, M. M. Khapra, and P. Kumar, \"Indicnlpsuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for indian languages,\" in _Findings of the Association for Computational Linguistics: EMNLP 2020, 2020_, pp. 4948-4961. * [77] A. Conneau and G. Lample, \"Cross-lingual language model pretraining,\" _Advances in neural information processing systems_, vol. 32, 2019. * [78] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzman, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, \"Unsupervised cross-lingual representation learning at scale,\" in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, 2020, pp. 8440-8451. * [79] D. Q. Nguyen, T. Vu, and A. T. Nguyen, \"Bertweet: A pre-trained language model for english tweets,\" in _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, 2020, pp. 9-14. * [80] F. Barbieri, J. Camacho-Collados, L. E. Anke, and L. Neves, \"Tweeteval: Unified benchmark and comparative evaluation for tweet classification,\" in _Findings of the Association for Computational Linguistics: EMNLP 2020_, 2020, pp. 1644-1650. * [81] Y. Yang, M. C. S. Uy, and A. Huang, \"Finbert: A pretrained language model for financial communications,\" _arXiv preprint arXiv:2006.08097_, 2020. * [82] D. Araci, \"Finbert: Financial sentiment analysis with pre-trained language models,\" _arXiv preprint arXiv:1908.10063_, 2019. * [83] Z. Liu, D. Huang, K. Huang, Z. Li, and J. Zhao, \"Finbert: A pre-trained financial language representation model for financial text mining,\" in _Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence_, 2021, pp. 4513-4519. * [84] I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Aletras, and I. Androutsopoulos, \"Legal-bert: The muppets straight out of law school,\" in _Findings of the Association for Computational Linguistics: EMNLP 2020_, 2020, pp. 2898-2904. * [85] S. Leivaditi, J. Rossi, and E. Kanoulas, \"A benchmark for lease contract review,\" _arXiv preprint arXiv:2010.10386_, 2020. * [86] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Chou, B. Qin, T. Liu, D. Jiang _et al._, \"Codebert: A pre-trained model for programming and natural languages,\" in _Findings of the Association for Computational Linguistics: EMNLP 2020, 2020_, pp. 1536-1547. * [87] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, \"Codeet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation,\" in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, 2021, pp. 8696-8708. * [88] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi, \"Codeet5+: Open code large language models for code understanding and generation,\" _arXiv preprint arXiv:2305.07922_, 2023. * [89] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, \"Biobert: a pre-trained biomedical language representation model for biomedical text mining,\" _Bioinformatics_, vol. 36, no. 4, pp. 1234-1240, 2020. * [90] Y. Gu, R. Tim, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon, \"Domain-specific language model pretraining for biomedical natural language processing,\" _arXiv preprint arXiv:2007.15779_, 2020. * [91] K. raj Kanakarajan, B. Kundumani, and M. Sankarasubu, \"Bio-electrics: pretrained biomedical text encoder using discriminators,\" in _Proceedings of the 20th Workshop on Biomedical Language Processing_, 2021, pp. 143-154. * [92] V. Sanh, L. Debutt, I. Chaumond, and T. Wolf, \"Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter,\" _arXiv preprint arXiv:2101.01108_, 2019. * [93] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu, \"Thysbert: Distilling bert for natural language understanding,\" in _Findings of the Association for Computational Linguistics EMNLP 2020_, 2020, p. 4163-4174. * [94] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou, \"Mobilebert: a compact task-agnostic bert for resource-limited devices,\" in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, 2020, pp. 2158-2170. * [95] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, \"Minlim: Deep self-attention distillation for task-agnostic compression of pre-trained transformers,\" _arXiv preprint arXiv:2002.10957_, 2020. * [96] I. Beltagy, M. E. Peters, and A. Cohan, \"Longformer: The long-document transformer,\" _arXiv preprint arXiv:2004.05150_, 2020. * [97] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang _et al._, \"Big bird: Transformers for longer sequences.\" in _NeurIPS_, 2020. * [98] F. Liu, E. Shareghi, Z. Meng, M. Basaldella, and N. Collier, \"Self-alignment pretraining for biomedical entity representations,\" in _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 2021, pp. 4228-4238. * [99] G. Michalopoulos, Y. Wang, H. Kaka, H. Chen, and A. Wong, \"Umlbsbert: Clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathesaurus,\" _arXiv preprint arXiv:2010.10391_, 2020. * [100] B. Goertzel, \"Artificial general intelligence: concept, state of the art, and future prospects,\" _Journal of Artificial General Intelligence_, vol. 5, no. 1, p. 1, 2014. * [101] J. Wei, Y. Tay, R. Rommassani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler _et al._, \"Emergent abilities of large language models,\" _Transactions on Machine Learning Research_, 2022. * [102] R. Schaefer, B. Miranda, and S. Koyejo, \"Are emergent abilities of large language models a mirage?\" _arXiv preprint arXiv:2304.15004_, 2023. * [103] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman _et al._, \"Evaluating large language models trained on code,\" _arXiv preprint arXiv:2107.03374_, 2021. * [104] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago _et al._, \"Competition-level code generation with alphacode,\" _Science_, vol. 378, no. 6624, pp. 1092-1097, 2022. * [105] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Raul, L. Weidinger, M. Chadwick, P. Thacker _et al._, \"Improving alignment of dialogue agents via targeted human judgements,\" _arXiv preprint arXiv:2209.14375_, 2022. * [106] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang, Y. Zhao, C. Pang _et al._, \"Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation,\" _arXiv preprint arXiv:2112.12731_, 2021. * [107] O. Lieber, O. Sharir, B. Lenz, and Y. Shoham, \"Jurassic-1: Technical details and evaluation,\" _White Paper. AI21 Labs_, vol. 1, 2021. * [108] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky _et al._, \"Alexattn 20b: Few-shot learning using a large-scale multilingual seq2seq model,\" _arXiv preprint arXiv:2208.01448_, 2022. * [109] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura _et al._, \"Opt-iml: Scaling language model instruction meta learning through the lens of generalization,\" _arXiv preprint arXiv:2212.12017_, 2022. * [110] N. Muenighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf _et al._, \"Crosslingual generalization through multitask finetuning,\" _arXiv preprint arXiv:2211.01786_, 2022. * [111] N. Sengupta, S. K. Sahu, B. Jia, S. Katipomu, H. Li, F. Koto, O. M. Afzal, S. Kambolo, O. Pandit, R. Pal _et al._, \"Jais and jais-hat: Arabic-centric foundation and instruction-tuned open generative large language models,\" _arXiv preprint arXiv:2308.16149_, 2023. * [112] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia _et al._, \"Glim-1300: An open bilingual pre-trained model,\" in _The Eleventh International Conference on Learning Representations_, 2022. * [113] X. Li, Y. Yao, X. Jiang, X. Fang, X. Meng, S. Fan, P. Han, J. Li, L. Du, B. Qin _et al._, \"Flim-10lb: An open llm and how to train it with 100 k budget,\" _arXiv preprint arXiv:2309.03852_, 2023. * [114] H. Yang, X.-Y. Liu, and C. D. Wang, \"Fingpt: Open-source financial large language models,\" _arXiv preprint arXiv:2306.06031_, 2023. * [115] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambaldur, D. Rosenberg, and G. Mann, \"Bloomberggpt: A large language model for finance,\" _arXiv preprint arXiv:2303.17564_, 2023. * [116] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl _et al._, \"Large language models encode clinical knowledge,\" _Nature_, pp. 1-9, 2023. * [117] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal _et al._, \"Towards expert-level medical question answering with large language models,\" _arXiv preprint arXiv:2305.09617_, 2023. * [118] R. Li, L. B. Allal, Y. Zi, N. Muenighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim _et al._, \"Starcoder: may the source be with you!\" _arXiv preprint arXiv:2305.06161_, 2023. * [119] B. Roziere, J. Gehring, F. Glecdeck, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin _et al._, \"Code llama: Open foundation models for code,\" _arXiv preprint arXiv:2308.12950_, 2023. * [120] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, \"Codegen: An open large language model for code with multi-turn program synthesis,\" in _The Eleventh International Conference on Learning Representations_, 2022. * [121] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, \"Codegen: Lessons for training l1ms on programming and natural languages,\" _arXiv preprint arXiv:2305.02309_, 2023. * [122] A. Radford, R. Jozefowicz, and I. Sutskever, \"Learning to generate reviews and discovering sentiment,\" _arXiv preprint arXiv:1704.01444_, 2017. * [123] A. M. Dai and Q. V. Le, \"Semi-supervised sequence learning,\" _Advances in neural information processing systems_, vol. 28, 2015. * [124] J. Howard and S. Ruder, \"Universal language model fine-tuning for text classification,\" in _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2018, pp. 328-339. * [125] B. Zhang, X. Fu, D. Ding, H. Huang, Y. Li, and L. Jing, \"Investigating chain-of-thought with chatgpt for stance detection on social media,\" _arXiv preprint arXiv:2304.03087_, 2023. * [126] B. Lamichane, \"Evaluation of chatgpt for rlp-based mental health applications,\" _arXiv preprint arXiv:2303.15727_, 2023. * [127] K. Yang, S. Ji, T. Zhang, Q. Xie, and S. Ananiadou, \"On the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis,\" _arXiv preprint arXiv:2304.03347_, 2023. * [128] Z. Wang, Q. Xie, Z. Ding, Y. Feng, and R. Xia, \"Is chatgpt a good sentiment analyzer? a preliminary study,\" _arXiv preprint arXiv:2304.04339_, 2023. * [129] A. Lopez-Lira and Y. Tang, \"Can chatgpt forecast stock price movements? return predictability and large language models,\" _arXiv preprint arXiv:2304.07619_, 2023. * [130] C. Ziems, W. Held, O. Shaikh, J. Chen, Z. Zhang, and D. Yang, \"Can large language models transform computational social science?\" _arXiv preprint arXiv:2305.03514_, 2023. * [131] T. Kuzeman, N. Libbesic, and I. Mozetic, \"Chat* [134] Q. Zhong, L. Ding, J. Liu, B. Du, and D. Tao, \"Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert,\" _arXiv preprint arXiv:2302.10198_, 2023. * [135] J. Ye, X. Chen, N. Xu, C. Zu, Z. Shao, S. Liu, Y. Cui, Z. Zhou, C. Gong, Y. Shen _et al._, \"A comprehensive capability analysis of gpt-3 and gpt-3.5 series models,\" _arXiv preprint arXiv:2303.10420_, 2023. * [136] X. Li, X. Zhu, Z. Ma, X. Liu, and S. Shah, \"Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? an examination on several typical tasks,\" _arXiv preprint arXiv:2305.05862_, 2023. * [137] Z. Wu, L. Zhang, C. Cao, X. Yu, H. Dai, C. Ma, Z. Liu, L. Zhao, G. Li, W. Liu _et al._, \"Exploring the trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology nli task,\" _arXiv preprint arXiv:2304.09138_, 2023. * [138] Y. Wang, Y. Zhao, and L. Petzold, \"Are large language models ready for healthcare? a comparative study on clinical language understanding,\" _arXiv preprint arXiv:2304.05368_, 2023. * [139] K.-L. Chiu, A. Collins, and H. Alexander, \"Detecting hate speech with gpt-3,\" _arXiv preprint arXiv:2103.12407_, 2021. * [140] F. Huang, H. Kwak, and J. An, \"Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech,\" _arXiv preprint arXiv:2302.07736_, 2023. * [141] S. Chen, Y. Li, S. Lu, H. Van, H. J. Aerts, G. K. Savova, and D. S. Bitterman, \"Evaluation of chatgpt family of models for biomedical reasoning and classification,\" _arXiv preprint arXiv:2304.02496_, 2023. * [142] M. M. Amin, E. Cambria, and B. W. Schuller, \"Will affective computing emerge from foundation models and general ai? a first evaluation on chatgpt,\" _IEEE Intelligent Systems_, vol. 38, p. 2, 2017. * [143] S. Parikh, Q. Vohra, P. Tumbade, and M. Tiwari, \"Exploring zero and few-shot techniques for intent classification,\" _arXiv preprint arXiv:2305.07157_, 2023. * [144] X. Sun, X. Li, J. Li, F. Wu, S. Guo, T. Zhang, and G. Wang, \"Text classification via large language models,\" _arXiv preprint arXiv:2305.08377_, 2023. * [145] Q. Li, H. Peng, J. Li, C. Xia, R. Yang, L. Sun, P. S. Yu, and L. He, \"A survey on text classification: From traditional to deep learning,\" _ACM Transactions on Intelligent Systems and Technology (TIST)_, vol. 13, no. 2, pp. 1-41, 2022. * [146] C.-E. Gonzalez-Gallardo, E. Boros, N. Girdhar, A. Hamdi, J. G. Moreno, and A. Doucet, \"Yes but.. can chatgpt identify entities in historical documents?\" _arXiv preprint arXiv:2303.17322_, 2023. * [147] Y. Hu, I. Ameer, X. Zuo, X. Peng, Y. Zhou, Z. Li, Y. Li, J. Li, X. Jiang, and H. Xu, \"Zero-shot clinical entity recognition using chatgpt,\" _arXiv preprint arXiv:2303.16416_, 2023. * [148] X. Wei, X. Cui, N. Cheng, X. Wang, X. Zhang, S. Huang, P. Xie, J. Xu, Y. Chen, M. Zhang _et al._, \"Zero-shot information extraction via chatting with chatgpt,\" _arXiv preprint arXiv:2302.10205_, 2023. * [149] B. J. Gutierrez, N. McNeal, C. Washington, Y. Chen, L. Li, H. Sun, and Y. Su, \"Thinking about gpt-3 in-context learning for biomedical lef think again,\" in _Findings of the Association for Computational Linguistics: EMNLP 2022_, 2022, pp. 4497-4512. * [150] J. Gao, H. Zhao, C. Yu, and R. Xu, \"Exploring the feasibility of chatgpt for event extraction,\" _arXiv preprint arXiv:2303.03836_, 2023. * [151] H. Rehana, N. B. Cam, M. Bsamaci, Y. He, A. Ozggin, and J. Hur, \"Evaluation of gpt and bert-based models on identifying protein-protein interactions in biomedical text,\" _arXiv preprint arXiv:2303.17728_, 2023. * [152] C. Yuan, Q. Xie, and S. Ananiadou, \"Zero-shot temporal relation extraction with chatgpt,\" _arXiv preprint arXiv:2304.05454_, 2023. * [153] B. Li, G. Fang, Y. Yang, Q. Wang, W. Ye, W. Zhao, and S. Zhang, \"Evaluating chatgpts information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness,\" _arXiv preprint arXiv:2304.11633_, 2023. * [154] C. Chan, J. Cheng, W. Wang, Y. Jiang, T. Fang, X. Liu, and Y. Song, \"Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations,\" _arXiv preprint arXiv:2304.14827_, 2023. * [155] X. Xu, Y. Zhu, X. Wang, and N. Zhang, \"How to unlash the power of large language models for few-shot relation extraction?\" _arXiv preprint arXiv:2305.01555_, 2023. * [156] Z. Wan, F. Cheng, Z. Mao, Q. Liu, H. Song, J. Li, and S. Kurohashi, \"Gpt-re: In-context learning for relation extraction using language models,\" _arXiv preprint arXiv:2305.02105_, 2023. * [157] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang, \"Is chatgpt a general-purpose natural language processing task solver?\" _arXiv preprint arXiv:2302.06476_, 2023. * [158] Y. Ma, Y. Cao, Y. Hong, and A. Sun, \"Large language model is not a good few-shot information extractor, but a good reranker for hard samples?\" _arXiv preprint arXiv:2303.08559_, 2023. * [159] S. Wang, X. Sun, X. Li, R. Quyang, F. Wu, T. Zhang, J. Li, and G. Wang, \"Gpt-ner: Named entity recognition via large language models,\" _arXiv preprint arXiv:2304.10428_, 2023. * [160] D. Stammbach, M. Antoniak, and E. Ash, \"Heroes, villains, and victims, and gpt-3: Automated extraction of character roles without training data,\" in _Proceedings of the 4th Workshop of Narrative Understanding (WNII2022)_, 2022, pp. 47-56. * [161] S. Wadhwa, S. Amir, and B. C. Wallace, \"Revisiting relation extraction in the era of large language models,\" _arXiv preprint arXiv:2305.05003_, 2023. * [162] P. Li, T. Sun, Q. Tang, H. Yan, Y. Wu, X. Huang, and X. Qiu, \"Codelet: Large code generation models are better few-shot information extractors,\" _arXiv preprint arXiv:2305.05711_, 2023. * [163] K. Zhang, B. J. Gutierrez, and Y. Su, \"Aligning instruction tasks unless large language models as zero-shot relation extractors,\" _arXiv preprint arXiv:2305.11159_, 2023. * [164] Y. Lu, Q. Liu, D. Dai, X. Xiao, H. Lin, X. Han, L. Sun, and H. Wu, \"Unified structure generation for universal information extraction,\" in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 5755-5772. * [165] Y. Chen, J. Cheng, H. Jiang, L. Liu, H. Zhang, S. Shi, and R. Xu, \"Learning from sibling mentions with scalable graph inference in fine-grained entity typing,\" in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 2076-2087. * [166] S. S. Das, A. Katiyar, R. J. Passonneau, and R. Zhang, \"Container: Few-shot named entity recognition via contrastive learning,\" in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 6338-6353. * [167] S. Wu and Y. He, \"Enriching pre-trained language model with entity information for relation classification,\" in _Proceedings of the 28th ACM international conference on information and knowledge management_, 2019, pp. 2361-2364. * [168] D. Ye, Y. Lin, P. Li, and M. Sun, \"Packed levitated marker for entity and relation extraction,\" in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 4904-4917. * [169] K. Zhao, X. Jin, L. Bai, J. Guo, and X. Cheng, \"Knowledge-enhanced self-supervised prototypical network for few-shot event detection,\" in _Findings of the Association for Computational Linguistics: EMNLP 2022_, 2022, pp. 6266-6275. * [170] Y. Ma, Z. Wang, Y. Cao, M. Li, M. Chen, K. Wang, and J. Shao, \"Prompt for extraction?\" page: Prompt for extraction? enie: Prompting argument interaction for event argument extraction,\" in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 6759-6774. * [171] X. Du and C. Cardie, \"Event extraction by answering (almost) natural questions,\" in _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2020, pp. 671-683. * [172] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh, \"Calibrate before use: Improving few-shot performance of language models,\" in _International Conference on Machine Learning_. PMLR, 2021, pp. 12 697-12 706. * [173] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap _et al._, \"Super-naturalistructures: Generalization via declarative instructions on 1600-nlp tasks,\" in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, 2022, pp. 5085-5109. * [174] M. Zaib, W. E. Zhang, Q. Z. Sheng, A. Mahmood, and Y. Zhang, \"Conversational question answering: A survey,\" _Knowledge and Information Systems_, vol. 64, no. 12, pp. 3151-3159, 2022. * [175] Y. Chali, S. A. Hasan, and S. R. Joly, \"Improving graph-based random walks for complex question answering using syntactic, shallow semantic and extended string subsequence kernels* [176] A. Torfi, R. A. Shirvani, Y. Keneshloo, N. Tavaf, and E. A. Fox, \"Natural language processing advancements by deep learning: A survey,\" _arXiv preprint arXiv:2003.01200_, 2020. * [177] D. Nunes, R. Primi, R. Pires, R. Lottou, and R. Nogueira, \"Evaluating gpt-3.5 and gpt+4 models on brazilian university admission exams,\" _arXiv preprint arXiv:2303.17003_, 2023. * [178] Y. Tan, D. Min, Y. Li, W. Li, N. Hu, Y. Chen, and G. Qi, \"Evaluation of chatpgrt as a question answering system for answering complex questions,\" _arXiv preprint arXiv:2303.07927_, 2023. * [179] Z. Yang, Z. Gan, J. Wang, X. Hu, Y. Lu, Z. Liu, and L. Wang, \"An empirical study of gpt-3 for few-shot knowledge-based vqa,\" in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 36, no. 3, 2022, pp. 3081-3089. * [180] P. Srivastava, T. Gau, and S. Guha, \"Towards zero-shot and few-shot table question answering using gpt-3,\" _arXiv preprint arXiv:2210.17284_, 2022. * [181] S. Zheng, J. Huang, and K. C.-C. Chang, \"Why does chatgpt fall short in answering questions faithfully?\" _arXiv preprint arXiv:2304.10513_, 2023. * [182] J. S. Samanan, Y. H. Yeo, N. Rajeev, L. Hawley, S. Abel, W. H. Ng, N. Srinivasan, J. Park, M. Burch, R. Watson _et al._, \"Assessing the accuracy of responses by the language model chatgpt to questions regarding bariatric surgery,\" _Obesity surgery_, pp. 1-7, 2023. * [183] J. Holmes, Z. Liu, L. Zhang, Y. Ding, T. T. Sio, L. A. McGee, J. B. Ashman, X. Li, T. Liu, J. Shen _et al._, \"Evaluating large language models on a highly-specialized topic, radiation oncology physics,\" _Frontiers in Oncology_, vol. 13, p. 1219326. * [184] I. Joshi, R. Budhiraja, H. DeV, J. Kadia, M. O. Atallah, S. Mitra, D. Kumar, and H. D. Abidekar, \"Chapt-a bessing of a curse for undergraduate computer science students and instructors?\" _arXiv preprint arXiv:2304.14993_, 2023. * [185] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, \"Capabilities of gpt-4 on medical challenge problems,\" _arXiv preprint arXiv:2303.13375_, 2023. * [186] A. Hamidi and K. Roberts, \"Evaluation of ai chatbots for patient-specific chat questions,\" _arXiv preprint arXiv:2306.02549_, 2023. * [187] J. Savelka, A. Agarwal, C. Bogart, and M. Sakr, \"Large language models (gpt) struggle to answer multiple-choice questions about code,\" _arXiv preprint arXiv:2303.0083_, 2023. * [188] M. Bommarito II and D. M. Katz, \"Gpt takes the bar exam,\" _arXiv preprint arXiv:2212.14402_, 2022. * [189] J. Pereira, R. Fidalgo, R. Lottou, and R. Nogueira, \"Visconde: Multi-document qa with gpt-3 and neural reranking,\" in _European Conference on Information Retrieval_. Springer, 2023, pp. 534-543. * [190] R. Gupta, I. Herzog, J. B. Park, J. Weisberger, P. Firouzbakht, V. Oocon, J. Chao, E. S. Lee, and B. A. Mailye, \"Performance of chatgpt on the plastic surgery inservice training examination,\" _Aesthetic surgery journal_, p. siad128, 2023. * [191] Y. Tanaka, T. Nakata, K. Aiga, T. Eiani, R. Muramatsu, S. Katagiri, H. Kawai, F. Higashino, M. Enomoto, M. Noda _et al._, \"Performance of generative pretrained transformer on the national medical licensing examination in japan,\" _arXiv_, pp. 2023-04, 2023. * [192] J. Robinson and D. Wingate, \"Leveraging large language models for multiple choice question answering,\" in _The Eleventh International Conference on Learning Representations_, 2022. * [193] Y. Weng, B. Li, F. Xia, M. Zhu, B. Sun, S. He, K. Liu, and J. Zhao, \"Large language models need holistically thought in medical conversational qa,\" _arXiv preprint arXiv:2305.05410_, 2023. * [194] S. Lin, J. Hilton, and O. Evans, \"Truthfulqa: Measuring how models mimic human falsehoods,\" in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 3214-3252. * [195] J. Kasai, Y. Kasai, K. Sakaguchi, Y. Yamada, and D. Radev, \"Evaluating gpt-4 and chatgpt on japanese medical licensing examinations,\" _arXiv preprint arXiv:2303.18027_, 2023. * [196] W. Gu, \"Linguistically informed chatgpt prompts to enhance japanese-chinese machine translation: A case study on attribute clauses,\" _arXiv preprint arXiv:2303.15587_, 2023. * [197] K. Peng, L. Ding, Q. Zhong, L. Shen, X. Liu, M. Zhang, Y. Ouyang, and D. Tao, \"Towards making the most of chatgpt for machine translation,\" _arXiv preprint arXiv:2303.13780_, 2023. * [198] W. Jiao, W. Wang, J. Huang, X. Wang, and Z. Tu, \"Is chatgpt a good transistor? ws with gpt-4 as the engine,\" _arXiv preprint arXiv:2301.08745_, 2023. * [199] A. Hendy, M. Abdelrehim, A. Sharaf, V. Raunak, M. Gabr, H. Matsushita, Y. J. Kim, M. Afify, and H. H. Awadalla, \"How good are gpt models at machine translation? a comprehensive evaluation,\" _arXiv preprint arXiv:2302.09210_, 2023. * [200] Y. Gao, R. Wang, and F. Hou, \"How to design translation prompts for chatgpt: An empirical study,\" _arXiv e-prints_, pp. arXiv:2304.2032. * [201] L. Wang, C. Lyu, T. Ji, Z. Zhang, D. Yu, S. Shi, and Z. Tu, \"Document-level machine translation with large language models,\" _arXiv preprint arXiv:2304.02210_, 2023. * [202] W. Zhu, H. Liu, Q. Dong, J. Xu, L. Kong, J. Chen, L. Li, and S. Huang, \"Multilingual machine translation with large language models: Empirical results and analysis,\" _arXiv preprint arXiv:2304.04675_, 2023. * [203] C. Lyu, J. Xu, and L. Wang, \"New trends in machine translation using large language models: Case examples with chatgpt,\" _arXiv preprint arXiv:2305.01181_, 2023. * [204] M. Karpinska and M. Iyyer, \"Large language models effectively leverage document-level context for literary translation, but critical errors persist,\" _arXiv preprint arXiv:2304.03254_, 2023. * [205] Y. Moslem, R. Haque, and A. Way, \"Adaptive machine translation with large language models,\" _arXiv preprint arXiv:2301.13294_, 2023. * [206] Z. He, T. Liang, W. Jiao, Z. Zhang, Y. Yang, R. Wang, Z. Tu, S. Shi, and X. Wang, \"Exploring human-like translation strategy with large language models,\" _arXiv preprint arXiv:2305.04118_, 2023. * [207] V. Raunak, A. Sharaf, H. H. Awadallah, and A. Menezes, \"Leveraging gpt-4 for automatic translation post-editing,\" _arXiv preprint arXiv:2305.14878_, 2023. * [208] V. Raunak, A. Menezes, M. Post, and H. H. Awadallah, \"Do gpts produce less literal translations?\" _arXiv preprint arXiv:2305.16806_, 2023. * [209] F. Stahlberg, \"Neural machine translation: A review,\" _Journal of Artificial Intelligence Research_, vol. 69, pp. 343-418, 2020. * [210] S. Yang, Y. Wang, and X. Chu, \"A survey of deep learning techniques for neural machine translation,\" _ArXiv_, vol. abs/2002.07526, 2020. * [211] Z. Tan, S. Wang, Z. Yang, G. Chen, X. Huang, M. Sun, and Y. Liu, \"Neural machine translation: A review of methods, resources, and tools,\" _AI Open_, vol. 1, pp. 5-21, 2020. * [212] D. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine translation by jointly learning to align and translate,\" _CoRR_, vol. abs/1409.0473, 2014. * [213] Y. Tang, C. Tran, X. Li, P.-J. Chen, N. Goyal, V. Chaudhary, J. Gu, and A. Fan, \"Multilingual translation with extensible multilingual pretraining and finetuning,\" _arXiv preprint arXiv:2008.00401_, 2020. * [214] A. Fan, S. Bhosale, H. Schwenk, Z. Ma, A. El-Kishky, S. Goyal, M. Baines, O. Celebi, G. Wenzek, V. Chaudhary, N. Goyal, T. Birch, V. Liptchinsky, S. Edunov, E. Grave, M. Auli, and A. Joulin, \"Beyond english-centric multilingual machine translation,\" _ArXiv_, vol. abs/2010.11115, 2020. * [215] M. R. Costa-Jussa, J. Cross, O. Celebi, M. Elbayad, K. Heafield, K. Hefferman, E. Kalbassi, J. Lam, D. Licht, J. Maillard _et al._, \"No language left behind: Scaling human-centered machine translation,\" _arXiv preprint arXiv:2207.04672_, 2022. * [216] R. Martinez-Cruz, A. J. Lopez-Lopez, and J. Portela, \"Chatgpt vs state-of-the-art models: A benchmarking study in keyphrase generation task,\" _arXiv preprint arXiv:2304.14177_, 2023. * [217] M. Song, H. Jiang, S. Shi, S. Yao, S. Lu, Y. Feng, H. Liu, and L. Jing, \"Is chatgpt a good keyphrase generator? a preliminary study,\" _arXiv preprint arXiv:2303.13001_, 2023. * [218] W. Pan, Q. Chen, X. Xu, W. Che, and L. Qin, \"A preliminary evaluation of chatgpt for zero-shot dialogue understanding,\" _arXiv preprint arXiv:2304.04256_, 2023. * [219] W. Zhao, Y. Zhao, X. Lu, S. Wang, Y. Tong, and B. Qin, \"Is chatgpt equipped with emotional dialogue capabilities?\" _arXiv preprint arXiv:2304.09582_, 2023. * [220] B. Chintagunta, N. Katariya, X. Amatri* [222] J. Huynh, C. Jiao, P. Gupta, S. Mehri, P. Bajaj, V. Chaudhary, and M. Eskenazi, \"Understanding the effectiveness of very large language models on dialog evaluation,\" _arXiv preprint arXiv:2301.12004_, 2023. * [223] Y. Fan and F. Jiang, \"Uncovering the potential of chatgpt for discourse analysis in dialogue: An empirical study,\" _arXiv preprint arXiv:2305.08391_, 2023. * [224] H. Wang, R. Wang, F. Mi, Z. Wang, R. Xu, and K.-F. Wong, \"Chain-of-thought prompting for responding to in-depth dialogue questions with llm,\" _arXiv preprint arXiv:2305.11792_, 2023. * [225] R. Meng, X. Yuan, T. Wang, S. Zhao, A. Trischler, and D. He, \"An empirical study on neural keyphrase generation,\" in _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 2021, pp. 4985-5007. * [226] X. Yuan, T. Wang, R. Meng, K. Thaker, P. Brusilovsky, D. He, and A. Trischler, \"One size does not fit all: Generating and evaluating variable number of keyphrases,\" in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, 2020, pp. 7961-7975. * [227] M. Kulkarni, D. Mahata, R. Arora, and R. Bhowmik, \"Learning rich representation of keyphrases from text,\" in _Findings of the Association for Computational Linguistics: NAACL 2022_. Seattle, United States: Association for Computational Linguistics, Jul. 2022, pp. 891-906. [Online]. Available: [https://aclanthology.org/2022.findings-nacl67](https://aclanthology.org/2022.findings-nacl67) * [228] I. V. Serban, R. Lowe, P. Henderson, L. Charlin, and J. Pineau, \"A survey of available corpora for building data-driven dialogue systems: The journal version,\" _Dialogue & Discourse_, vol. 9, no. 1, pp. 1-49, 2018. * [229] S. Larson and K. Leach, \"A survey of intent classification and slot-filling datasets for task-oriented dialog,\" _arXiv preprint arXiv:2207.13211_, 2022. * [230] W. Sun, L. Yan, X. Ma, P. Ren, D. Yin, and Z. Ren, \"Is chatgpt good at search? investigating large language models as re-ranking agent,\" _arXiv preprint arXiv:2304.09542_, 2023. * [231] N. Ziems, W. Yu, Z. Zhang, and M. Jiang, \"Large language models are built-in autoregressive search engines,\" _arXiv preprint arXiv:2305.09612_, 2023. * [232] A. Anand, L. Lyu, M. Idahl, Y. Wang, J. Wallat, and Z. Zhang, \"Explainable information retrieval: A survey,\" _arXiv preprint arXiv:2211.02405_, 2022. * [233] R. Nogueira, Z. Jiang, R. Pradeep, and J. Lin, \"Document ranking with a pretrained sequence-to-sequence model,\" in _Findings of the Association for Computational Linguistics: EMNLP 2020_, 2020, pp. 708-718. * [234] N. Thakur, N. Reimers, A. Ruckle, A. Srivastava, and I. Gurevych, \"Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models,\" in _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021. * [235] W. X. Zhao, J. Liu, R. Ren, and J.-R. Wen, \"Dense text retrieval based on pretrained language models: A survey,\" _arXiv preprint arXiv:2211.14876_, 2022. * [236] G. Adomavicius and A. Tuzhilin, \"Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions,\" _IEEE transactions on knowledge and data engineering_, vol. 17, no. 6, pp. 734-749, 2005. * [237] Y. Peng, \"A survey on modern recommendation system based on big data,\" _arXiv preprint arXiv:2206.02631_, 2022. * [238] F. Rezazimehr and C. Dadkhah, \"A survey of attack detection approaches in collaborative filtering recommender systems,\" _Artificial Intelligence Review_, vol. 54, pp. 2011-2066, 2021. * [239] Y. Xie, J. Gao, P. Zhou, Q. Ye, Y. Hua, J. Kim, F. Wu, and S. Kim, \"Rethinking multi-interest learning for candidate matching in recommender systems,\" _arXiv preprint arXiv:2302.14532_, 2023. * [240] M. Dong, X. Zeng, L. Koehl, and J. Zhang, \"An interactive knowledge-based recommender system for fashion product design in the big data environment,\" _Information Sciences_, vol. 540, pp. 469-488, 2020. * [241] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, \"Chat-rec: Towards interactive and explainable lllms-augmented recommender system,\" _arXiv preprint arXiv:2303.14524_, 2023. * [242] F. Zhu, Y. Wang, C. Chen, J. Zhou, L. Li, and G. Liu, \"Cross-domain recommendation: challenges, progress, and prospects,\" _arXiv preprint arXiv:2103.01696_, 2021. * [243] L. Wang and E.-P. Lim, \"Zero-shot next-item recommendation using large pretrained language models,\" _arXiv preprint arXiv:2304.03153_, 2023. * [244] A. Zhiyuli, Y. Chen, X. Zhang, and X. Liang, \"Bookgpt: A general framework for book recommendation empowered by large language model,\" _arXiv preprint arXiv:2305.15673_, 2023. * [245] J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, \"Is chatgpt a good recommender? a preliminary study,\" _arXiv preprint arXiv:2304.10149_, 2023. * [246] S. Dai, N. Shao, H. Zhao, W. Yu, Z. Si, C. Xu, Z. Sun, X. Zhang, and J. Xu, \"Uncovering chatgpt's capabilities in recommender systems,\" _arXiv preprint arXiv:2305.02182_, 2023. * [247] W.-C. Kang, J. Ni, N. Mehta, M. Sathiamoorthy, L. Hong, E. Chi, and D. Z. Cheng, \"Do lms understand user preferences? evaluating lms on user rating prediction,\" _arXiv preprint arXiv:2305.06474_, 2023. * [248] J. Zhang, K. Bao, Y. Zhang, W. Wang, F. Feng, and X. He, \"Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation,\" _arXiv preprint arXiv:2305.07609_, 2023. * [249] Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. McAuley, and W. X. Zhao, \"Large language models are zero-shot rankers for recommender systems,\" _arXiv preprint arXiv:2305.08845_, 2023. * [250] S. Mysore, A. McCallum, and H. Zamani, \"Large language model augmented narrative driven recommendations,\" _arXiv preprint arXiv:2306.02250_, 2023. * [251] C. S. Xia and L. Zhang, \"Keep the conversation going: Fixing 162 out of 337 bugs for 0.42 each using chatgpt,\" _arXiv preprint arXiv:2304.00385_, 2023. * [252] A. Cheshkov, P. Zadorozhny, and R. Levichev, \"Evaluation of chatgpt model for vulnerability detection,\" _arXiv preprint arXiv:2304.07232_, 2023. * [253] B. Yetistiren, I. Ozsoy, M. Ayerdem, and E. Tuzun, \"Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhispierer, and chatgpt,\" _arXiv preprint arXiv:2304.11078_, 2023. * [254] T.-O. Li, W. Zong, Y. Wang, H. Tian, Y. Wang, and S.-C. Cheung, \"Finding failure-inducing test cases with chatgpt,\" _arXiv preprint arXiv:2304.11686_, 2023. * [255] C. Liu, X. Bao, H. Zhang, N. Zhang, H. Hu, X. Zhang, and M. Yan, \"Improving chatgpt prompt for code generation,\" _arXiv preprint arXiv:2305.08360_, 2023. * [256] R. A. Pollack, T. Lu, and G. Begus, \"Ai-assisted coding: Experiments with gpt-4,\" _arXiv preprint arXiv:2304.13187_, 2023. * [257] J. Liu, C. S. Xia, Y. Wang, and L. Zhang, \"Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation,\" _arXiv preprint arXiv:2305.01210_, 2023. * [258] E. Chen, R. Huang, H.-S. Chen, Y.-H. Tseng, and L.-Y. Li, \"Gpttor: a chatgpt-powered programming tool for code explanation,\" _arXiv preprint arXiv:2305.01863_, 2023. * [259] N. Nascimento, P. Alencar, and D. Cowan, \"Comparing software developers with chatgpt: An empirical investigation,\" _arXiv preprint arXiv:2305.11837_, 2023. * [260] J. Y. Khan and G. Uddin, \"Automatic code documentation generation using gpt-3,\" in _Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering_, 2022, pp. 1-6. * [261] J. Leinonen, P. Denny, S. MacNeil, S. Sarsa, S. Bernstein, J. Kim, A. Tran, and A. Hellas, \"Comparing code explanations created by students and large language models,\" _arXiv preprint arXiv:2304.03938_, 2023. * [262] X.-Y. Li, J.-T. Xue, Z. Xie, and M. Li, \"Think outside the code: Brainstorming boosts large language models in code generation,\" _arXiv preprint arXiv:2305.10679_, 2023. * [263] J. A. Prenner and R. Robbes, \"Automatic program repair with openai's codec: Evaluating quikbugs,\" _arXiv preprint arXiv:2111.03922_, 2021. * [264] M. L. Siddiq, J. C. S. Santos, R. H. Tanvir, N. Ulfat, F. A. Rifat, and V. C. Lopes, \"Exploring the effectiveness of large language models in generating unit tests,\" _ArXiv_, vol. abs/2305.00418, 2023. * [265] H. Tian, W. Lu, T. O. Li, X. Tang, S.-C. Cheung, J. Klein, and T. F. Bissyande, \"Is chatgpt the ultimate programming assistant-how far is it?\" _arXiv preprint arXiv:2304.11938_, 2023. * [266] M. Geng, S. Wang, D. Dong, H. Wang, G. Li, Z. Jin, X. Mao, and X. Liao, \"An empirical study on using large language models for multi-intent comment generation,\" _ArXiv_, vol. abs/2304.11384, 2023. * [267] S. Kang, B. Chen, S. Yoo, and J.-G. Lou, \"Explainable automated debugging via large language model-driven scientific debugging,\" _arXiv preprint arXiv:2304.02195_, 2023. * [268] A. Kashefi and T. Mukerji, \"Chatgpft for programming numerical methods,\" _ArXiv_, vol. abs/2303.12093, 2023. * [269] G. Desterfanis, S. Bartouci, and M. Ortu, \"A preliminary analysis on the code generation capabilities of gpt+3.5 and bard ai models for java functions,\" _arXiv preprint arXiv:2305.09402_, 2023. * [270] Z. Yuan, Y. Lou, M. Liu, S. Ding, K. Wang, Y. Chen, and X. Peng, \"No more manual tests? evaluating and improving chatpft in unit test generation,\" _ArXiv_, vol. abs/2305.04207, 2023. * [271] T. Phung, V.-A. Padurnea, J. P. Cambronero, S. Gulwani, T. Kohn, R. Majumdar, A. K. Singla, and G. Soares, \"Generative ai for programming education: Benchmarking chatgpft, gpt+4, and human tutors,\" _ArXiv_, vol. abs/2306.17156, 2023. * [272] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo, J. Grundy, and H. Wang, \"Large language models for software engineering: A systematic literature review,\" _arXiv preprint arXiv:2308.10620_, 2023. * [273] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang _et al._, \"Codeszblue: A machine learning benchmark dataset for code understanding and generation,\" in _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)_, 2021. * [274] L. Phan, H. Tran, D. Le, H. Nguyen, J. Annibal, A. Pellekian, and Y. Ye, \"Cotex: Multi-task learning with code-text transformer,\" in _Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Png 2021)_, 2021, pp. 40-47. * [275] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, L. Shujie, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu _et al._, \"Graphcodebert: Pre-training code representations with data flow,\" in _International Conference on Learning Representations_, 2020. * [276] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, \"Unified pre-training for program understanding and generation,\" in _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 2021, pp. 2655-2668. * [277] D. Zan, B. Chen, D. Yang, Z. Lin, M. Kim, B. Guan, Y. Wang, W. Chen, and J.-G. Lou, \"Cerf: Continual pre-training on sketches for library-oriented code generation,\" _arXiv preprint arXiv:2206.06888_, 2022. * [278] R. Just, D. Jalali, and M. D. Ernst, \"Defect4f: A database of existing adults to enable controlled testing studies for java programs,\" in _Proceedings of the 2014 international symposium on software testing and analysis_, 2014, pp. 437-440. * [279] D. Lin, J. Koppel, A. Chen, and A. Solar-Lezama, \"Quixbugs: A multi-lingual program repair benchmark set based on the quixey challenge,\" in _Proceedings of Computation of the 2017 ACM SIGPLAN international conference on systems, programming, languages, and applications: software for humanity_, 2017, pp. 55-56. * [280] A. Sundar and L. Heck, \"Multimodal conversational ai: A survey of datasets and approaches,\" in _Proceedings of the 4th Workshop on NLP for Conversational AI_, 2022, pp. 131-147. * [281] P. Xu, X. Zhu, and D. A. Clifton, \"Multimodal learning with transformers: A survey,\" _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023. * [282] Z. Shao, Z. Yu, M. Wang, and J. Yu, \"Prompting large language models with answer heuristics for knowledge-based visual question answering,\" in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023, pp. 14974-14983. * [283] Y. Lin, Y. Xie, D. Chen, Y. Xu, C. Zhu, and L. Yuan, \"Review: Regional visual representation matters in knowledge-based visual question answering,\" _arXiv preprint arXiv:2206.01201_, 2022. * [284] L. Gui, B. Wang, Q. Huang, A. G. Hauptmann, Y. Bisk, and J. Gao, \"Kat: A knowledge augmented transformer for vision-and-language,\" in _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 2022, pp. 956-968. * [285] Y. Lu, X. Yang, X. Li, X. E. Wang, and V. Y. Wang, \"Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation,\" _arXiv preprint arXiv:2305.11116_, 2023. * [286] W. Zhu, X. Wang, Y. Lu, T.-J. Fu, X. E. Wang, M. Eckstein, and W. Y. Wang, \"Collaborative generative ai: Integrating gpt+k for efficient editing in text-to-image generation,\" _arXiv preprint arXiv:2305.11317_, 2023. * [287] T. Zhang, Y. Zhang, V. Vineet, N. Joshi, and X. Wang, \"Controllable text-to-image generation with gpt+4,\" _arXiv preprint arXiv:2305.18583_, 2023. * [288] S. Hong, J. Seo, S. Hong, H. Shin, and S. Kim, \"Large language models are frame-level directors for zero-shot text-to-video generation,\" _arXiv preprint arXiv:2305.14330_, 2023. * [289] R. Huang, M. Li, D. Yang, J. Shi, X. Chang, Z. Ye, Y. Wu, Z. Hong, J. Huang, J. Liu _et al._, \"Audiogpt: Understanding and generating speech, music, sound, and talking head,\" _arXiv preprint arXiv:2304.12995_, 2023. * [290] M. Ranitt, G. Ganapathy, R. Manuel, and T. Ganu, \"Retrieval augmented chest x-ray report generation using openai gpt models,\" _arXiv preprint arXiv:2305.03660_, 2023. * [291] S. S. Kalakonda, S. Maheshwari, and R. K. Sarvadevabhatla, \"Action-gpt: Leveraging large-scale language models for improved and generalized zero shot action generation,\" _arXiv preprint arXiv:2211.15603_, 2022. * [292] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan, \"Visual chatgpft: Talking, drawing and editing with visual foundation models,\" _arXiv preprint arXiv:2303.04671_, 2023. * [293] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarrasab, F. Ahmed, Z. Liu, C. Liu, M. Zeng, and L. Wang, \"Mm-react: Prompting chatgpft for multimodal reasoning and action,\" _arXiv preprint arXiv:2303.11381_, 2023. * [294] J. Li, H. Li, Z. Pan, and G. Pan, \"Prompt chatgpft in mmer: Improved multimodal named entity recognition method based on auxiliary refining knowledge from chatgpft,\" _arXiv preprint arXiv:2305.12212_, 2023. * [295] S. Hakimov and D. Schlangen, \"Images in language space: Exploring the suitability of large language models for vision & language tasks,\" _arXiv preprint arXiv:2305.13782_, 2023. * [296] W. Feng, W. Zhu, T.-j. Fu, V. Jampani, A. Akula, X. He, S. Basu, X. E. Wang, and W. Y. Wang, \"Layoutgpt: Compositional visual planning and generation with large language models,\" _arXiv preprint arXiv:2305.15393_, 2023. * [297] L. Fan, D. Krishnan, P. Isola, D. Katabi, and Y. Tian, \"Improving clip training with language rewrites,\" _arXiv preprint arXiv:2305.20088_, 2023. * [298] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao, \"Llava-med: Training a large language-and-vision assistant for biomedicine in one day,\" _arXiv preprint arXiv:2306.00890_, 2023. * [299] A. Bhattacharya, Y. K. Singla, B. Krishnamurthy, R. R. Shah, and C. Chen, \"A video is worth40g6 tokens: Verbalative story videos to understand them in zero shot,\" _arXiv preprint arXiv:2305.09758_, 2023. * [300] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley, Y. Zou, and W. Wang, \"Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\" _arXiv preprint arXiv:2303.17395_, 2023. * [301] D. Zhang, S. Li, X. Zhang, J. Zhan, P. Wang, Y. Zhou, and X. Qiu, \"Speechep: Empowering large language models with intrinsic cross-modal conversational abilities,\" _arXiv preprint arXiv:2305.11000_, 2023. * [302] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, and J. Liu, \"Chatbridge: Bridging modalities with large language model as a language catalyst,\" _arXiv preprint arXiv:2305.16103_, 2023. * [303] M. Zheng, X. Su, S. You, F. Wang, C. Qian, C. Xu, and S. Albanie, \"Can gpt+4 perform neural architecture search?\" _arXiv preprint arXiv:2304.10970_, 2023. * [304] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, \"Huggggpt: Solving ai tasks with chatgpft and its friends in hugging,\" _arXiv preprint arXiv:2303.17580_, 2023. * [305] L. Zhang, Y. Zhang, K. Ren, D. Li, and Y. Yang, \"Mlcopilot: Unleashing the power of large language models in solving machine learning tasks,\" _arXiv preprint arXiv:2304.14979_, 2023. * [306] S. Zhang, C. Gong, L. Wu, X. Liu, and M. Zhou, \"Automl-gpt: Automatic machine learning with gpt,\" _arXiv preprint arXiv:2305.02499_, 2023. * [307] F. Hutter, L. Kotthoff, and J. Vanschoren, _Automated machine learning: methods, systems, challenges_. Springer Nature, 2019. * [308] A. Olmo, S. Sreedharan,* [309] B. Zhang and H. Soh, \"Large language models as zero-shot human models for human-robot interaction,\" _arXiv preprint arXiv:2303.03548_, 2023. * [310] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh, \"Translating natural language to planning goals with large-language models,\" _arXiv preprint arXiv:2302.05128_, 2023. * [311] H. Hu, H. Lu, H. Zhang, W. Lam, and Y. Zhang, \"Chain-of-symbol prompting elicits planning in large language models,\" _arXiv preprint arXiv:2305.10276_, 2023. * [312] K. Valmeekam, A. Olmo, S. Sreedharan, and S. Kambhampati, \"Large language models still can't plan (a benchmark for lms on planning and reasoning about change),\" in _NeurIPS 2022 Foundation Models for Decision Making Workshop_, 2022. * [313] K. M. Collins, C. Wong, J. Feng, M. Wei, and J. B. Tenenbaum, \"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks,\" _arXiv preprint arXiv:2205.05718_, 2022. * [314] K. Mahowald, A. A. Ivanova, I. A. Blank, N. Kanwisher, J. B. Tenenbaum, and E. Fedorenko, \"Dissociating language and thought in large language models: a cognitive perspective,\" _arXiv preprint arXiv:2301.06627_, 2023. * [315] K. S. Kalyan and S. Sangerella, \"Medical concept normalization in user-generated texts by learning target concept embeddings,\" in _Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis_, 2020, pp. 18-23. * [316] ----, \"Target concept guided medical concept normalization in noisy user-generated texts,\" in _Proceedings of Deep Learning Inside Out (DeLLO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures_, 2020, p. 64-73. * [317] J. Holmes, Z. Liu, L. Zhang, Y. Ding, T. T. Sio, L. A. McGee, J. B. Ashman, X. Li, T. Liu, J. Shen _et al._, \"Evaluating large language models on a highly-specialized topic, radiation oncology physics,\" _arXiv preprint arXiv:2304.01938_, 2023. * [318] Z. Liu, X. Yu, L. Zhang, Z. Wu, C. Gao, H. Dai, L. Zhao, W. Liu, D. Shen, Q. Li _et al._, \"Deal-gpt: Zero-shot medical text de-identification by gpt-4,\" _arXiv preprint arXiv:2303.11032_, 2023. * [319] J. Giorgi, A. Toma, R. Xie, S. Chen, K. An, G. Zheng, and B. Wang, \"Wanglab at media-qacht 2023: Clinical note generation from doctor-patient conversations using large language models,\" in _Proceedings of the 5th Clinical Natural Language Processing Workshop_, 2023, pp. 323-334. * [320] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, \"Capabilities of gpt-4 on medical challenge problems,\" _ArXiv_, vol. abs/2303.13357, 2023. * [321] Q. Chen, J. Du, Y. Hu, V. K. Keloth, X. Peng, K. Raja, R. Zhang, Z. Lu, and H. Xu, \"Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations,\" _arXiv preprint arXiv:2305.16326_, 2023. * [322] Y. Tanaka, T. Nakata, K. Aiga, T. Ettani, R. Muramatsu, S. Katagiri, H. Kawai, F. Higashino, M. Enomoto, M. Noda, M. Kometani, M. Takamura, T. Yoneda, H. Kakizaki, and A. Nomura, \"Performance of generative pretrained transformer on the national medical licensing examination in iapan,\" in _medRxiv_, 2023. * [323] J. Liu, P. Zhou, Y. Hua, D. Chong, Z. Tian, A. Liu, H. Wang, C. You, Z. Guo, L. Zhu _et al._, \"Benchmarking large language models on emexam-a comprehensive chinese medical exam dataset,\" _arXiv preprint arXiv:2306.03030_, 2023. * [324] Z. Yang, S. Cherian, and S. Vucetic, \"Data augmentation for radiology report simplification,\" in _Findings of the Association for Computational Linguistics: EACL 2023_, 2023, pp. 187-1887. * [325] C. Ma, Z. Wu, J. Wang, S. Xu, Y. Wei, Z. Liu, L. Guo, X. Cai, S. Zhang, T. Zhang _et al._, \"Impressiongpt an iterative optimizing framework for radiology report summarization with chatpef,\" _arXiv preprint arXiv:2304.08448_, 2023. * [326] M. Moradi, K. Bage, F. Haberl, and M. Samwald, \"Gpt-3 models are poor few-shot learners in the biomedical domain,\" _arXiv preprint arXiv:2109.02555_, 2021. * [327] K. Jeblick, B. Schachtner, J. Dexl, A. Mittermeier, A. T. Stuber, J. Topalis, T. Weber, P. Wesp, B. Sabel, J. Ricke _et al._, \"Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports,\" _arXiv preprint arXiv:2212.14882_, 2022. * [328] X. Tang, A. Tran, J. Tan, and M. Gerstein, \"Gersteinlab at media-chat 2023: Clinical note summarization from doctor-patient conversations through fine-tuning and in-context learning,\" _arXiv preprint arXiv:2305.05001_, 2023. * [329] M. Agrawal, S. Hegselmann, H. Lang, Y. Kim, and D. Sontag, \"Large language models are few-shot clinical information extractors,\" in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, 2022, pp. 1998-2022. * [330] V. Nair, E. Schumacher, and A. Kannan, \"Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models,\" _arXiv preprint arXiv:2305.05982_, 2023. * [331] C. Shaib, M. L. Li, S. Joseph, I. J. Marshall, J. J. Li, and B. C. Wallace, \"Summarizing, simplifying, and synthesizing medical evidence using gpt-3 (with varying success),\" _arXiv preprint arXiv:2305.06299_, 2023. * [332] J. Xu, L. Lu, S. Yang, B. Liang, X. Peng, J. Pang, J. Ding, X. Shi, L. Yang, H. Song _et al._, \"Medgpteval: A dataset and benchmark to evaluate responses of large language models in medicine,\" _arXiv preprint arXiv:2305.07340_, 2023. * [333] X. Wang, Z. Gong, G. Wang, J. Jia, Y. Xu, J. Zhao, Q. Fan, S. Wu, W. Hu, and X. Li, \"Chatgpt performs on the chinese national medical licensing examination,\" 2023. * [334] K. A. Carpenter and R. B. Altman, \"Using gpt-3 to build a lexicon of drugs of abuse synonyms for social media pharmacovigilance,\" _Biomlocules_, vol. 13, no. 2, p. 387, 2023. * [335] E. Hernandez, D. Mahajan, J. Wulff, M. J. Smith, Z. Ziegler, D. Nadler, P. Szolovits, A. Johnson, E. Alsentzer _et al._, \"Do we still need clinical language models?\" in _Conference on Health, Inference, and Learning_. PMLR, 2023, pp. 578-597. * [336] A. S. Rao, M. Pang, J. Kim, M. Kamineni, W. Lie, A. K. Prasad, A. Landman, K. Dryer, and M. D. Succi, \"Assessing the utility of chatgpt through the entire clinical workflow,\" _medRxiv_, pp. 2023-02, 2023. * [337] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon, C. Elepano, M. Madriaga, R. Aggabao, G. Diaz-Candido, J. Maningo _et al._, \"Performance of chatgpt on usmple: Potential for ai-assisted medical education using large language models,\" _PLoS digital health_, vol. 2, no. 2, p. e00001898, 2023. * [338] A. Hulman, O. L. Dollerup, J. F. Mortensen, M. Fenech, K. Norman, H. Stoewring, and T. K. Hansen, \"Chatgpt-versus human-generated answers to frequently asked questions about diabetes: a turing test-inspired survey among employees of a danish diabetes center,\" _medRxiv_, pp. 2023-02, 2023. * [339] T. Hirosawa, Y. Harada, M. Yokose, T. Sakamoto, R. Kawamura, and T. Shimizu, \"Diagnostic accuracy of differential-diagnosis lists generated by generative pretrained transformer 3 chatbot for clinical vignettes with common chief complaints: A pilot study,\" _International journal of environmental research and public health_, vol. 20, no. 4, p. 3378, 2023. * [340] S. Liu, A. P. Wright, B. L. Patterson, J. P. Wanderer, R. W. Turer, S. D. Nelson, A. B. McCoy, D. F. Sittig, and A. Wright, \"Assessing the value of chatgpt for clinical decision support optimization,\" _MedRxiv_, pp. 2023-02, 2023. * [341] A. Gilson, C. W. Safranek, T. Huang, V. Socrates, L. Chi, R. A. Taylor, D. Chatrash _et al._, \"How does chatgpt perform on the unlited states medical licensing examination? the implications of large language models for medical education and knowledge assessment,\" _JMIR Medical Education_, vol. 9, no. 1, p. e45312, 2023. * [342] F. Antaki, S. Touma, D. Milad, J. El-Khoury, and R. Duval, \"Evaluating the performance of chatgpt in ophthalmology: An analysis of its successes and shortcomings,\" _Ophthalmology Science_, p. 100324, 2023. * [343] Q. Lyu, J. Tan, M. E. Zapadka, J. Pomatapura, C. Niu, K. J. Myers, G. Wang, and C. T. Whitlow, \"Translating radiology reports into plain language using chatgpt and gpt-4 with prompt learning: results, limitations, and potential,\" _Visual Computing for Industry, Biomedicine, and Art_, vol. 6, no. 1, p. 9, 2023. * [344] F. Yu, L. Quatery, and F. Schilder, \"Legal prompting: Teaching a language model to think like a lawyer,\" _arXiv preprint arXiv:2212.01326_, 2022. * [345] H.-T. Nguyen, \"A brief report on lawgpt 1.0: A virtual legal assistant based on gpt-3,\" _arXiv preprint arXiv:2302.05729_, 2023. * [346] I. Chalkidis, \"Chatgpt may pass the bar exam soon, but has a long way to go for the leglague benchmark,\" _arXiv preprint arXiv:2304.12202_, 2023. * [347] J. H. Choi, K. E. Hickman, A. Monahan, and D. Schwarcz, \"Chatgpt goes to law school,\" _Available at SSRN_, 2023. * [348] X. Cai, S. Liu, J. Han, L. Yang, Z. Liu, and T. Liu, \"Chestray-bert: A pretrained language model for chest radiology report summarization,\" _IEEE Transactions on Multimedia_, 2021. * [349] H. Xiong, S. Wang, Y. Zhu, Z. Zhao, Y. Liu, Q. Wang, and D. Shen, \"Doctorglm: Fine-tuning your chinese doctor is not a herchuen task,\" _arXiv preprint arXiv:2304.01097_, 2023. * [350] A. B. Abacha, W.-w. Yim, G. Adams, N. Snider, and M. Yetisgen-Yildiz, \"Overview of the mediqa-chat 2023 shared tasks on the summarization & generation of doctor-patient conversations,\" in _Proceedings of the 5th Clinical Natural Language Processing Workshop_, 2023, pp. 503-513. * [351] H. Su, J. Kasai, Y. Wang, Y. Hu, M. Ostendorf, W.-t. Yih, N. A. Smith, L. Zettlemoyer, T. Yu _et al._, \"One embedder, any task: Instruction-finetuned text embeddings,\" _arXiv preprint arXiv:2212.09741_, 2022. * [352] Y. Lan, Y. Wu, W. Xu, W. Feng, and Y. Zhang, \"Chinese fine-grained financial sentiment analysis with large language models,\" _arXiv preprint arXiv:2306.14096_, 2023. * [353] G. Fatouros, J. Soldatos, K. Kouroumani, G. Makridis, and D. Kyriazis, \"Transforming sentiment analysis in the financial domain with chatgtr,\" _arXiv preprint arXiv:2308.07935_, 2023. * [354] M. Leippold, \"Sentiment spin: Attacking financial sentiment with gpt-3,\" _Finance Research Letters_, p. 103957, 2023. * [355] P. Wirjanthamabhum, \"Promptshots at the finlp-2022 erai task: Pairwise comparison and unsupervised ranking,\" in _Proceedings of the Fourth Workshop on Financial Technology and Natural Language Processing (FinNLP)_, 2022, pp. 104-110. * [356] A. Shah and S. Chava, \"Zero is not hero yet: Benchmarking zero-shot performance of l1ms for financial tasks,\" _arXiv preprint arXiv:2305.16633_, 2023. * [357] L. Zhang, W. Cai, Z. Liu, Z. Yang, W. Dai, Y. Liao, Q. Qin, Y. Li, X. Liu, Z. Liu _et al._, \"Fineval: A chinese financial domain knowledge evaluation benchmark for large language models,\" _arXiv preprint arXiv:2308.09957_, 2023. * [358] P. K. Rajpoot and A. Parikh, \"Gpt-fnre: In-context learning for financial relation extraction using large language models,\" _arXiv preprint arXiv:2306.17519_, 2023. * [359] L. Loukas, I. Stogiannidis, P. Malakasiotis, and S. Vassos, \"Breaking the bank with chatgpt: Few-shot text classification for finance,\" _arXiv preprint arXiv:2308.14634_, 2023. * [360] I. Chalkidis, J. Lana, D. Hartung, M. Bommarito, I. Androutsopoulos, D. Katz, and N. Aletras, \"Lesglue: A benchmark dataset for legal language understanding in english,\" in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 4310-4330. * [361] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou _et al._, \"Chainat-of-thought prompting elicits reasoning in large language models,\" _Advances in Neural Information Processing Systems_, vol. 35, pp. 24 824-24 837, 2022. * [362] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. Moussa, M. Beane, T.-H. Huang, B. R. Routledge _et al._, \"Finga: A dataset of numerical reasoning over financial data,\" in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, 2021, pp. 3697-3711. * [363] V. D. Lai, N. T. Ngo, A. P. B. Vysesh, H. Man, F. Dernoncourt, T. Bui, and T. H. Nguyen, \"Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning,\" _arXiv preprint arXiv:2304.05613_, 2023. * [364] T. Fang, S. Yang, K. Lan, D. F. Wong, J. Hu, L. S. Chao, and Y. Zhang, \"Is chatgpt a highly fluent grammatical error correction system? a comprehensive evaluation,\" _arXiv preprint arXiv:2304.01746_, 2023. * [365] J. Armengol-Estape, O. de Gibert Bonet, and M. Melero, \"On the multilingual capabilities of very large-scale english language models,\" in _Proceedings of the Thirteenth Language Resources and Evaluation Conference_, 2022, pp. 3056-3068. * [366] K. Ahuja, R. Hada, M. Ochieng, P. Jain, H. Diddee, S. Maina, T. Ganu, S. Segal, M. Axmed, K. Bali _et al._, \"Mega: Multilingual evaluation of generative ai,\" _arXiv preprint arXiv:2303.12528_, 2023. * [367] X. Zhang, S. Li, B. Hauer, N. Shi, and G. Kondrak, \"Don't trust gpt when your question is not in english,\" _arXiv preprint arXiv:2305.16339_, 2023. * [368] M. Das, S. K. Pandey, and A. Mukherjee, \"Evaluating chatgpt's performance for multilingual and emoj-based hate speech detection,\" _arXiv preprint arXiv:2305.13276_, 2023. * [369] R. Hada, V. Gumma, A. de Wynter, H. Diddee, M. Ahmed, M. Choudhury, K. Bali, and S. Sitaram, \"Are large language model-based evaluators the solution to scaling up multilingual evaluation?\" _arXiv preprint arXiv:2309.07462_, 2023. * [370] W. Q. Leong, J. G. Ngui, Y. Susanto, H. Rengarajan, K. Sarveswaran, and W. C. Tjlu, \"Bhasa: A holistic southeast asian linguistic and cultural evaluation suite for large language models,\" _arXiv preprint arXiv:2309.06085_, 2023. * [371] R. Bommasani, P. Liang, and T. Lee, \"Holistic evaluation of language models,\" _Annals of the New York Academy of Sciences_, 2023. * [372] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Carriga-Alonso _et al._, \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,\" _Transactions on Machine Learning Research_, 2023. * [373] F. Gilardi, M. Alizadeh, and M. Kubli, \"Chatgpt outperforms crowd-workers for text-annotation tasks,\" _arXiv preprint arXiv:2303.15056_, 2023. * [374] X. He, Z. Lin, Y. Gong, A. Jin, H. Zhang, C. Lin, J. Jiao, S. M. Yiu, N. Duan, W. Chen _et al._, \"Annollm: Making large language models to be better crowdsourced annotators,\" _arXiv preprint arXiv:2303.16854_, 2023. * [375] P. Tornberg, \"Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning,\" _arXiv preprint arXiv:2304.06588_, 2023. * [376] Y. Zhu, P. Zhang, E.-L. Haq, P. Hui, and G. Tyson, \"Can chatgpt reproduce human-generated labels? a study of social computing tasks,\" _arXiv preprint arXiv:2304.10145_, 2023. * [377] L. Li, L. Fan, S. Atreja, and L. Hemphall, \"\" hot\" chatgpt: The promise of chatgpt in detecting and discriminating hateful, offensive, and toxic comments on social media,\" _arXiv preprint arXiv:2304.10619_, 2023. * [378] Y. Gu, S. Zhang, N. Uusuyama, Y. Woldesenbet, C. Wong, P. Sanapathi, M. Wei, N. Valluri, E. Strandberg, T. Naumann _et al._, \"Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events,\" _arXiv preprint arXiv:2307.06439_, 2023. * [379] S. Wang, Y. Liu, Y. Xu, C. Zhu, and M. Zeng, \"Want to reduce labeling cost gpt-3-cnn help,\" in _Findings of the Association for Computational Linguistics:KmLD 2021_, 2021, pp. 4195-4205. * [380] B. Ding, C. Qin, L. Liu, L. Bing, S. Joty, and B. Li, \"Is gpt-3 a good data annotator?\" _arXiv preprint arXiv:2212.10450_, 2022. * [381] S. Menoni, E. De la Clergerie, and T. Ryftel, \"Large language models as instructors: A study on multilingual cnnti extraction,\" in _The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks_, 2023, pp. 178-190. * [382] Y. Xu, R. Xu, D. Iter, Y. Liu, S. Wang, C. Zhu, and M. Zeng, \"Inheritsumm: A general, versatile and compact summarizer by distilling from gpt,\" _arXiv preprint arXiv:2305.13083_, 2023. * [383] M. Alizadeh, M. Kubli, Z. Samei, S. Dehghani, J. D. Bermeo, M. Korobeynikova, and F. Gilardi, \"Open-source large language models outperform crowd workers and approach chatgpt in text-annotation tasks,\" _arXiv preprint arXiv:2307.02179_, 2023. * [384] S. Thapa, U. Nasemen, and M. Nasim, \"From humans to machines: can chatgpt-like l1ms effectively replace human annotators in mtasks,\" in _Workshop Proceedings of the 17th International AAAI Conference on Web and Social Media_, 2023. * [385] J. S. Murthy, G. Siddesh, and K. Srinivasa, \"Twitsenti: a real-time twitter sentiment analysis and visualization framework,\" _Journal of Information & Knowledge Management_, vol. 18, no. 02, p. 1950013, 2019. * [386] W. Van Attveeldt, M. A. Van der Velden, and M. Boukes, \"The validity of sentiment analysis: Comparing manual annotation, crowd-coding, dictionary approaches, and machine learning algorithms,\" _Communication Methods and Measures_, vol. 15, no. 2, pp. 121-140, 2021. * [387] M. Chmilewski and S. C. Kucker, \"An mturk crisis? shifts in data quality and the impact on study results,\" _Social Psychological and Personality Science_, vol. 11, no. 4, pp. 464-473, 2020. * [388] P. He, B. Peng, L. Lu, S. Wang, J. Mei, Y. Liu, R. Xu, H. H. Awadalla, Y. Shi, C. Zhu _et al._, \"Z-code++: A pre-trained language model optimized for abstractive summarization,\" _arXiv preprint arXiv:2208.087ing instruction-finetuned language models,\" _arXiv preprint arXiv:2210.11416_, 2022. * [30] J. Cegin, J. Simko, and P. Brusilovsky, \"Chatgpt to replace crowdsourcing of paraphrases for intent classification: Higher diversity and comparable model robustness,\" _arXiv preprint arXiv:2305.12947_, 2023. * [31] S. Oh, W. Jung _et al._, \"Data augmentation for neural machine translation using generative language model,\" _arXiv preprint arXiv:2307.16833_, 2023. * [32] S. Sharma, A. Joshi, N. Mukhija, Y. Zhao, H. Bhathena, P. Singh, S. Santhanam, and P. Biswas, \"Systematic review of effect of data augmentation using paraphrasing on named entity recognition,\" in _NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research_, 2022. * [33] Z. Guo, P. Wang, Y. Wang, and S. Yu, \"Dr. llama: Improving small language models in domain-specific o via generative data augmentation,\" _arXiv preprint arXiv:2305.07804_, 2023. * [34] A. Abaskohi, S. Rothe, and Y. Yaghoobzadeh, \"Lm-cppf: Paraphrasing-guided data augmentation for contrastive prompt-based few-shot fine-tuning,\" _arXiv preprint arXiv:2305.18169_, 2023. * [35] S. Sarker, L. Qian, and X. Dong, \"Medical data augmentation via chatgpt: A case study and medication identification and medication event classification,\" _arXiv preprint arXiv:2306.07297_, 2023. * [36] H. Dai, Z. Liu, W. Liao, X. Huang, Y. Cao, Z. Wu, L. Zhao, S. Xu, W. Liu, N. Liu _et al._, \"Augspt: Leveraging chapft for text data augmentation,\" _arXiv preprint arXiv:2302.13007_, 2023. * [37] Y. Fang, X. Li, S. W. Thomas, and X. Zhu, \"Chatgpt as data augmentation for compositional generalization: A case study in open intent detection,\" _arXiv preprint arXiv:2308.13517_, 2023. * [38] C. Shorten and T. M. Khoshgoftaar, \"A survey on image data augmentation for deep learning,\" _Journal of big data_, vol. 6, no. 1, pp. 1-48, 2019. * [39] B. Li, Y. Hou, and W. Che, \"Data augmentation approaches in natural language processing: A survey,\" _Ai Open_, vol. 3, pp. 71-90, 2022. * [40] P. Liu, X. Wang, C. Xiang, and W. Meng, \"A survey of text data augmentation,\" in _2020 International Conference on Computer Communication and Network Security (CCNS)_. IEEE, 2020, pp. 191-195. * [41] S. Y. Feng, V. Gangal, J. Wei, S. Chandar, S. Vosoughi, T. Mitamura, and E. Hovy, \"A survey of data augmentation approaches for nlp,\" in _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, 2021, pp. 968-988. * [42] M. Bayer, M.-A. Kaufhold, and C. Reuter, \"A survey on data augmentation for text classification,\" _ACM Computing Surveys_, vol. 55, no. 7, pp. 1-39, 2022. * [43] Y. Belinkov and Y. Bisk, \"Synthetic and natural noise both break neural machine translation,\" in _International Conference on Learning Representations_, 2018. * [44] C. Coulombe, \"Text data augmentation made simple by leveraging nlp cloud apis,\" _arXiv preprint arXiv:1812.04718_, 2018. * [45] J. Wei and K. Zou, \"Eda: Easy data augmentation techniques for boosting performance on text classification tasks,\" in _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP-IJCNLP)_, 2019, pp. 6382-6388. * [46] W. Y. Wang and D. Yang, \"That's so annoying!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using pet-peeve tweets,\" in _Proceedings of the 2015 conference on empirical methods in natural language processing_, 2015, pp. 2587-2563. * [47] R. Sennrich, B. Haddow, and A. Birch, \"Improving neural machine translation models with monolingual data,\" in _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2016, pp. 86-96. * [48] C. Mallikarjuna and S. Sivanesan, \"Question classification using limited labelled data,\" _Information Processing & Management_, vol. 59, no. 6, p. 103094, 2022. * [49] H. Zhan, Z. Li, Y. Wang, L. Luo, T. Feng, X. Kang, Y. Hua, L. Qu, L.-K. Soon, S. Sharma _et al._, \"Socialial: A benchmark for socially-aware dialogue systems,\" _arXiv preprint arXiv:2304.12026_, 2023. * [50] J. Wang, Z. Yao, A. Mitra, S. Osebe, Z. Yang, and H. Yu, \"Umass_biompt at mediag-chat 2023: Can llms generate high-quality synthetic note-oriented doctor-patient conversations?\" _arXiv preprint arXiv:2306.16931_, 2023. * [51] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno, S. Gopi, M. Javaherij, P. C. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah, H. S. Behl, X. Wang, S. Bubeck, R. Eldan, A. T. Kalai, Y. T. Lee, and J.-F. Li, \"Textbooks are all you need,\" _ArXiv_, vol. abs/2306.11644, 2023. * [52] C. Whitehouse, M. Choudhury, and A. F. Aji, \"Llm-powered data augmentation for enhanced crosslingual performance,\" _ArXiv_, vol. abs/2305.14288, 2023. * [53] T. Hartvigsen, S. Gabriel, H. Palangi, M. Sap, D. Ray, and E. Kamar, \"Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection,\" in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 3309-3326. * [54] T. Markov, C. Zhang, S. Agarwal, F. E. Nekou, T. Lee, S. Adler, A. Jiang, and L. Weng, \"A holistic approach to undesired content detection in the real world,\" in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 37, no. 12, 2023, pp. 15 009-15 018. * [55] Z. Guo, P. Wang, Y. Wang, and S. Yu, \"Dr. llama: Improving small language models on pubmeda via generative data augmentation,\" _ArXiv_, vol. abs/2305.07804, 2023. * [56] R. Eldan and Y. Li, \"Tinystories: How small can language models be and still speak coherent english?\" _arXiv preprint arXiv:2305.07759_, 2023. * [57] H. Liu, Z. Teng, L. Cui, C. Zhang, Q. Zhou, and Y. Zhang, \"Logicot: Logical chain-of-thought instruction-tuning data collection with gpt+4,\" _arXiv preprint arXiv:2305.12147_, 2023. * [58] B. Peng, C. Li, P. He, M. Galley, and J. Gao, \"Instruction tuning with gpt+4,\" _arXiv preprint arXiv:2304.03277_, 2023. * [59] I. Mallieul, U. Alam, Y. Yehuda, S. Keren, O. Barkan, R. Ronen, and N. Koenigstein, \"Gpt-cells: Enhancing call segmentation and tagging by generating synthetic conversations via large language models,\" _arXiv preprint arXiv:2306.07941_, 2023. * [60] J. P. Wahle, T. Ruas, F. Kirstein, and B. Gipp, \"How large language models are transforming machine-paraphrase plagiarism,\" in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, 2022, pp. 952-963. * [61] A. Michail, S. Konstantinou, and S. Clematide, \"Uzh_clyp at semeval-2023 tasky: Head-first fine-tuning and chatgpt data generation for cross-lingual learning in tweet intimacy prediction,\" _arXiv preprint arXiv:2303.01194_, 2023. * [62] R. Tang, X. Han, X. Jiang, and X. Hu, \"Does synthetic data generation of lms help clinical text mining?\" _arXiv preprint arXiv:2303.04360_, 2023. * [63] Y. Yu, Y. Zhuang, J. Zhang, Y. Meng, A. Ratner, R. Krishna, J. Shen, and C. Zhang, \"Large language model as attributed training data generator: A tale of diversity and bias,\" _arXiv preprint arXiv:2306.15895_, 2023. * [64] W. Yang and G. Nicolai, \"Neural machine translation data generation and augmentation using chatgpt,\" _arXiv preprint arXiv:2307.05779_, 2023. * [65] Y. Zhao, C. Zhao, L. Nan, Z. Qi, W. Zhang, X. Tang, B. Mi, and D. Radev, \"Robut: A systematic study of table qa robustness against human-annotated adversarial perturbations,\" _arXiv preprint arXiv:2306.14321_, 2023. * [66] W. Xu, D. Wang, L. Pan, Z. Song, M. Freitag, W. Y. Wang, and L. Li, \"Instrucduce: Towards explainable text generation evaluation with automatic feedback,\" _arXiv preprint arXiv:2305.14282_, 2023. * [67] A. Sugiyama and N. Yoshinaga, \"Data augmentation using back-translation for context-aware neural machine translation,\" in _Proceedings of the fourth workshop on discourse in machine translation (DiscoMT 2019)_, 2019, pp. 35-44. * [68] F. Mireshghallah, J. Mattern, S. Gao, R. Shokri, and T. Berg-Kirkpatrick, \"Smaller language models are better black-box machine-generated text detectors,\" _ArXiv_, vol. abs/2305.09859, 2023. * [69] B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding, J. Yue, and Y. Wu, \"How close is chatgpt to human experts? comparison corpus, evaluation, and detection,\" _ArXiv_, vol. abs/2301.07597, 2023. * [70] P. Hacker, A. Engel, and M. Mauer, \"Regulating chatgpt and other large generative ai models,\" in _Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency_, 2023, pp. 1112-1123. * [431] L. De Angelis, F. Baglivo, G. Arzilli, G. P. Privitera, P. Ferragina, A. E. Tozzi, and C. Rizzo, \"Chatgpt and the rise of large language models: the new ai-driven infocmider threat in public health,\" _Frontiers in Public Health_, vol. 11, p. 1166120, 2023. * [432] S. Mitrovi'c, D. Andreoletti, and O. Ayoub, \"Chatgpt for human? detect and explain. explaining decisions of machine learning model for detecting short chaptapt-generated text,\" _ArXiv_, vol. abs/2301.13852, 2023. * [433] C. A. Gao, F. M. Howard, N. S. Markov, E. C. Dyer, S. Ramesh, Y. Luo, and A. T. Pearson, \"Comparing scientific abstracts generated by chatgpt to real abstracts with detectors and blinded human reviewers,\" _NPJ Digital Medicine_, vol. 6, no. 1, p. 75, 2023. * [434] D. R. Cotton, P. A. Cotton, and J. R. Shipway, \"Chatting and cheating: Ensuring academic integrity in the era of chatgpt,\" _Innovations in Education and Teaching International_, pp. 1-12, 2023. * [435] P. C. Theodoropoulos, P. Anagnostou, A. Isoukala, S. V. Georgakopoulos, S. K. Tasoulis, and V. P. Plagianakos, \"Detection of fake generated scientific abstracts,\" _arXiv preprint arXiv:2304.06148_, 2023. * [436] W. Zaiitsu and M. Jin, \"Distinguishing chatgpt (-3.5,-4)-generated and human-written papers through japanese stylometric analysis,\" _arXiv preprint arXiv:2304.05534_, 2023. * [437] P. Yu, J. Chen, X. Feng, and Z. Xia, \"Cheat: A large-scale dataset for detecting chatgprint-written abstracts,\" _arXiv preprint arXiv:2304.12008_, 2023. * [438] X. Yang, W. Cheng, L. Petzold, W. Y. Wang, and H. Chen, \"Dna-ggpt: Divergent n-gram analysis for training-free detection of gpt-generated text,\" _arXiv preprint arXiv:2305.17359_, 2023. * [439] Y. Liu, Z. Zhang, W. Zhang, S. Yue, X. Zhao, X. Cheng, Y. Zhang, and H. Hu, \"Argupgt: evaluating, understanding and identifying argumentative assy generated by gpt models,\" _arXiv preprint arXiv:2304.07666_, 2023. * [440] M. S. Orenstrakh, O. Karnalim, C. A. Suarez, and M. Litt, \"Detecting llm-generated text in computing education: A comparative study for chatgpt cases,\" _arXiv preprint arXiv:2307.07411_, 2023. * [441] W. Liao, Z. Liu, H. Dai, S. Xu, Z. Wu, Y. Zhang, X. Huang, D. Zhu, H. Cai, T. Liu _et al._, \"Differentiate chatgpt-generated and human-written medical texts,\" _arXiv preprint arXiv:2304.11567_, 2023. * [442] H. Zhan, X. He, Q. Xu, Y. Wu, and P. Stenetorp, \"G3detector: General gpt-generated text detector,\" _arXiv preprint arXiv:2305.12680_, 2023. * [443] E. Clark, T. August, S. Serrano, N. Haduong, S. Gururangan, and N. A. Smith, \"All hats'human's not gold: Evaluating human evaluation of generated text,\" in _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, 2021, pp. 7282-7296. * [444] A. Pegoraro, K. Kumari, H. Fereidooni, and A.-R. Sadeghi, \"To chatgpt, or not to chatgpt: That is the question!\" _arXiv preprint arXiv:2304.01487_, 2023. * [445] Z. Shi, Y. Wang, F. Yin, X. Chen, K.-W. Chang, and C.-J. Hsieh, \"Red teaming language model detectors with language models,\" _arXiv preprint arXiv:2305.19713_, 2023. * [446] M. Khalil and E. Er, \"Will chatgpt get you caught? rethinking of plagiarism detection,\" _arXiv preprint arXiv:2302.04335_, 2023. * [447] X. He, X. Shen, Z. Chen, M. Backes, and Y. Zhang, \"Mgtbench: Benchmarking machine-generated text detection,\" _arXiv preprint arXiv:2303.14822_, 2023. * [448] H. Wang, X. Luo, W. Wang, and X. Yan, \"Bot or human? detecting chatgpt imposters with a single question,\" _ArXiv_, vol. abs/2305.06424, 2023. * [449] Y. Chen, H. Kang, V. Zhai, L. Li, R. Singh, and B. Ramakrishnan, \"Gpt-sentinel: Distinguishing human and chatgpt generated content,\" _ArXiv_, vol. abs/2305.07969, 2023. * [450] X. Yu, Y. Qi, K. Chen, G. Chen, X. Yang, P. Zhu, W. Zhang, and N. H. Yu, \"Gpt paternity test: Gpt generated text detection with gpt genetic inheritance,\" _ArXiv_, vol. abs/2305.12519, 2023. * [451] L. Yang, F. Jiang, and H. Li, \"Ls chatgpt involved in texts? measure the polish ratio to detect chatgpt-generated text,\" _ArXiv_, vol. abs/2307.11380, 2023. * [452] K. Krishna, Y. Song, M. Karpinska, J. Wieting, and M. Iyyer, \"Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense,\" _arXiv preprint arXiv:2303.13408_, 2023. * [453] D. Ippolito, D. Duckworth, C. Callison-Burch, and D. Eck, \"Automatic detection of generated text is easiest when humans are fooled,\" in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, 2020, pp. 1808-1822. * [454] S. M. Lundberg and S.-I. Lee, \"A unified approach to interpreting model predictions,\" _Advances in neural information processing systems_, vol. 30, 2017. * [455] X. Chen, J. Ye, C. Zu, N. Xu, R. Zheng, M. Peng, J. Zhou, T. Gui, Q. Zhang, and X. Huang, \"How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks,\" _arXiv preprint arXiv:2303.00293_, 2023. * [456] J. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y. Wang, L. Yang, H. Huang, W. Ye, X. Geng _et al._, \"On the robustness of chatgpt: An adversarial and out-of-distribution perspective,\" _arXiv preprint arXiv:2302.12095_, 2023. * [457] T. Y. Zhuo, Z. Li, Y. Huang, Y.-F. Li, W. Wang, G. Haffari, and F. Shiri, \"On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codec,\" _arXiv preprint arXiv:2301.12868_, 2023. * [458] K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y. Wang, L. Yang, W. Ye, N. Z. Gong, Y. Zhang _et al._, \"Promptbench: Towards evaluating the robustness of large language models on adversarial prompts,\" _arXiv preprint arXiv:2306.04528_, 2023. * [459] A. Shirafuji, Y. Watanabe, T. Ito, M. Morishita, Y. Nakamura, Y. Oda, and J. Suzuki, \"Exploring the robustness of large language models for solving programming problems,\" _arXiv preprint arXiv:2306.14583_, 2023. * [460] R. Han, T. Peng, C. Yang, B. Wang, L. Liu, and X. Wan, \"Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors,\" _arXiv preprint arXiv:2305.14450_, 2023. * [461] H. Liu, R. Ning, Z. Teng, J. Liu, Q. Zhou, and Y. Zhang, \"Evaluating the logical reasoning ability of chatgpt and gpt-4,\" _arXiv preprint arXiv:2304.03439_, 2023. * [462] A. Liu, X. Hu, L. Wen, and P. S. Yu, \"A comprehensive evaluation of chatgpt's zero-shot text-to-sql capability,\" _arXiv preprint arXiv:2303.13547_, 2023. * [463] E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning, and C. Finn, \"Detectgpt: Zero-shot machine-generated text detection using probability curvature,\" _arXiv preprint arXiv:2301.11305_, 2023. * [464] S. Goyal, S. Doddapaneni, M. M. Khapra, and B. Ravindran, \"A survey of adversarial defences and robustness in nlp,\" _ACM Computing Surveys_, 2022. * [465] S. Qiu, Q. Liu, S. Zhou, and W. Huang, \"Adversarial attack and defense technologies in natural language processing: A survey,\" _Neurocomputing_, vol. 492, pp. 278-307, 2022. * [466] Z. Shen, J. Liu, Y. He, X. Zhang, R. Xu, H. Yu, and P. Cui, \"Towards out-of-distribution generalization: A survey,\" _arXiv preprint arXiv:2108.13642_, 2021. * [467] X. Wang, Q. Liu, T. Gui, Q. Zhang, Y. Zou, X. Zhou, J. Ye, Y. Zhang, R. Zheng, Z. Fang _et al._, \"Textflint: Unified multilingual robustness evaluation toolkit for natural language processing,\" in _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations_, 2021, pp. 347-355. * [468] Y. Chen, R. Wang, H. Jiang, S. Shi, and R. Xu, \"Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study,\" _arXiv preprint arXiv:2304.00723_, 2023. * [469] A. B. Sai, A. K. Mohankumar, and M. M. Khapra, \"A survey of evaluation metrics used for nlg systems,\" _ACM Computing Surveys (CSUR)_, vol. 55, no. 2, pp. 1-39, 2022. * [470] T. Y. Zhuo, \"Large language models are state-of-the-art evaluators of code generation,\" _arXiv preprint arXiv:2304.14317_, 2023. * [471] H. Lai, A. Toral, and M. Nissim, \"Multidimensional evaluation for text style transfer using chatgpt,\" _arXiv preprint arXiv:2304.13462_, 2023. * [472] Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu, \"Gpteval: Nlg evaluation using gpt-4 with better human alignment,\" _arXiv preprint arXiv:2303.16634_, 2023. * [473] T. Kocmi and C. Federmann, \"Large language models are state-of-the-art evaluators of translation quality,\" _arXiv preprint arXiv:2302.14520_, 20* [475] Z. Luo, Q. Xie, and S. Ananiadou, \"Chatgpt as a factual inconsistency evaluator for text summarization,\" 2023. * [476] C. Shen, L. Cheng, Y. You, and L. Bing, \"Are large language models good evaluators for abstractive summarization?\" _arXiv preprint arXiv:2305.13091_, 2023. * [477] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu, \"Gptscore: Evaluate as you desire,\" _arXiv preprint arXiv:2302.04166_, 2023. * [478] Y. Liu, A. R. Fabbri, P. Liu, D. Radev, and A. Cohan, \"On learning to summarize with large language models as references,\" _arXiv preprint arXiv:2305.14239_, 2023. * [479] M. Gao, J. Ruan, R. Sun, X. Yin, S. Yang, and X. Wan, \"Human-like summarization evaluation with chatgpt,\" _arXiv preprint arXiv:2304.02554_, 2023. * [480] T. Tang, H. Lu, Y. E. Jiang, H. Huang, D. Zhang, W. X. Zhao, and F. Wei, \"Not all metrics are guilty: Improving nlg evaluation with llm paraphrasing,\" _arXiv preprint arXiv:2305.15067_, 2023. * [481] P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, D. Liu, T. Liu, and Z. Sui, \"Large language models are not fair evaluators,\" _arXiv preprint arXiv:2305.17926_, 2023. * [482] S. Jain, V. Keshava, S. M. Sathyendra, P. Fernandes, P. Liu, G. Neubig, and C. Zhou, \"Multi-dimensional evaluation of text summarization with in-context learning,\" _arXiv preprint arXiv:2306.01200_, 2023. * [483] J. Wang, Y. Liang, F. Meng, H. Shi, Z. Li, J. Xu, J. Qu, and J. Zhou, \"Is chatpat a good llg evaluator? a preliminary study,\" _arXiv preprint arXiv:2303.04048_, 2023. * [484] Y. Bai, J. Ying, Y. Cao, X. Lv, Y. He, X. Wang, J. Yu, K. Zeng, Y. Xiao, H. Lyu _et al._, \"Benchmarking foundation models with language-model-as-an-examiner,\" _arXiv preprint arXiv:2306.04181_, 2023. * [485] W. Yang, C. Li, J. Zhang, and C. Zong, \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages,\" _arXiv preprint arXiv:2305.18098_, 2023. * [486] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing _et al._, \"Judging llm-as-a-judge with mt-bench and chatbot arena,\" _arXiv preprint arXiv:2306.05685_, 2023. * [487] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \"Bleu: a method for automatic evaluation of machine translation,\" in _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, 2002, pp. 311-318. * [488] C.-Y. Lin, \"Rouge: A package for automatic evaluation of summaries,\" in _Text summarization branches out_, 2004, pp. 74-81. * [489] S. Banerjee and A. Lavie, \"Meteor: An automatic metric for mt evaluation with improved correlation with human judgments,\" in _Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization_, 2005, pp. 65-72. * [490] T. Kocmi, C. Federmann, R. Grundkiewicz, M. Junczysch-Dowmunt, H. Matsushita, and A. Menezes, \"To ship or not to ship: An extensive evaluation of automatic metrics for machine translation,\" in _Proceedings of the Sixth Conference on Machine Translation_, 2021, pp. 478-494. * [491] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, \"Perscore: Evaluating text generation with bert,\" in _International Conference on Learning Representations_, 2019. * [492] W. Zhao, M. Peyrard, F. Liu, Y. Gao, C. M. Meyer, and S. Eger, \"Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance,\" in _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, 2019, pp. 563-578. * [493] W. Yuan, G. Neubig, and P. Liu, \"Bartscore: Evaluating generated text as text generation,\" _Advances in Neural Information Processing Systems_, vol. 34, pp. 27 263-27 277, 2021. * [494] S. Zhou, U. Alon, S. Agarwal, and G. Neubig, \"Codebertscore: Evaluating code generation with pretrained models of code,\" _arXiv preprint arXiv:2302.05527_, 2023. * [495] J. He, W. Krytstsik, B. McCann, N. Rajani, and C. Xiong, \"Crltsum: Towards generic controllable text summarization,\" in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, 2022, pp. 5879-5915. * [496] C. Shen, L. Cheng, L. Bing, Y. You, and L. Si, \"Sentbs: Sentence-level beam search for controllable summarization,\" in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, 2022, pp. 10 256-10 265. * [497] Y. Liu, P. Liu, D. Radev, and G. Neubig, \"Brio: Bringing order to abstractive summarization,\" in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 2890-2903. * [498] Q. Lu, L. Ding, L. Xie, K. Zhang, D. F. Wong, and D. Tao, \"Toward human-like evaluation for natural language generation with error analysis,\" _arXiv preprint arXiv:2212.10179_, 2022. * [499] R. Bhardwaj and S. Poria, \"Red-teaming large language models using chain of utterances for safety-alignment,\" _arXiv preprint arXiv:2308.09662_, 2023. * [500] D. Ganguli, L. Lovitt, J. Kermion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Nousse _et al._, \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned,\" _arXiv preprint arXiv:2209.07858_, 2022. * [501] N. Mehrabi, P. Goyal, C. Dupuy, Q. Hu, S. Ghosh, R. Zemel, K.-W. Chang, A. Galstyan, and R. Gupta, \"Fifet: Feedback look in-context red teaming,\" _arXiv preprint arXiv:2308.04265_, 2023. * [502] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving, \"Red teaming language models with language models,\" in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, 2022, pp. 3419-3448. * [503] L. Chen, M. Zaharia, and J. Zou, \"Frugalgpt: How to use large language models while reducing cost and improving performance,\" _arXiv preprint arXiv:2305.05176_, 2023. * [504] Z. Cheng, J. Kasai, and T. Yu, \"Batch prompting: Efficient inference with large language model apis,\" _arXiv preprint arXiv:2301.08721_, 2023. * [505] Y. Li, \"Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering,\" _arXiv preprint arXiv:2304.12102_, 2023. * [506] M. A. Arefeen, B. Debnath, and S. Chakradhar, \"Leancontext: Cost-efficient domain-specific question answering using llms,\" _arXiv preprint arXiv:2309.00841_, 2023. * [507] S. Golchin and M. Surdeanu, \"Time travel in llms: Tracing data contamination in large language models,\" _arXiv preprint arXiv:2308.08493_, 2023. * [508] R. Aiyappa, J. An, H. Kwak, and Y.-Y. Ahn, \"Can we trust the evaluation on chatpst?\" _arXiv preprint arXiv:2303.12767_, 2023. * [509] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, \"Glue: A multi-task benchmark and analysis platform for natural language understanding,\" in _International Conference on Learning Representations_, 2018. * [510] X. Zhang, J. Zhao, and Y. LeCun, \"Character-level convolutional networks for text classification,\" _Advances in neural information processing systems_, vol. 28, 2015. * [511] S. Narayan, S. B. Cohen, and M. Lapata, \"Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization,\" in _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, 2018, pp. 1797-1807. * [512] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen _et al._, \"Sitem's song in the ai ocean: A survey on hallucination in large language models,\" _arXiv preprint arXiv:2309.01219_, 2023. * [513] V. Rawte, A. Sheth, and A. Das, \"A survey of hallucination in large foundation models,\" _arXiv preprint arXiv:2309.05922_, 2023. * [514] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and J. Weston, \"Chain-of-verification reduces hallucination in large language models,\" 2023. * [515] L. K. Umapathi, A. Pal, and M. Sankarasubbu, \"Med-halt: Medical domain hallucination test for large language models,\" _arXiv preprint arXiv:2307.15343_, 2023. * [516] J. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, \"Halueval: A large-scale hallucination evaluation benchmark for large language models,\" _arXiv e-prints_, pp. arXiv-2305, 2023. * [517] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen _et al._, \"Check your facts and try again: Improving large language models with external knowledge and automated feedback,\" _arXiv preprint arXiv:2302.12813_, 2023."
    }
  ]
}