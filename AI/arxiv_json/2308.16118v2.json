{
  "title": "Response: Emergent analogical reasoning in large language models",
  "authors": [
    "Damian Hodel",
    "Jevin West"
  ],
  "abstract": "\n In their recent Nature Human Behaviour paper, \"Emergent analogical reasoning in large language models,\"  (Webb, Holyoak, and Lu, 2023)  the authors argue that \"GPT-3 exhibits a very general capacity to identify and generalize-in zero-shot fashion-relational patterns found within both formal problems and meaningful texts.\" This conclusion arises from their comparison of GPT-3 with human performance across four analogical reasoning domains, where they find comparable results. In this response, we argue that this approach is unsuitable for evaluating general, zero-shot reasoning in large language models (LLMs). Two primary reasons underlie our objection. First, the term \"zeroshot\" implies problem sets entirely novel to GPT-3. However, the chosen approach cannot conclusively eliminate the possibility of these problems residing in the LLM's training data, as acknowledged by the authors themselves in the review file 1 . Second, the assumption underlying this approach is that tests designed for humans can accurately measure LLM capabilities. This assumption is prevalent, but remains unverified. We also provide empirical results to support our claims, see appendix (Section 7.1). Our counterexamples show that GPT-3 fails to solve simplest variations of the original tasks, whereas human performance remains consistently high across all modified versions. \n",
  "references": [
    {
      "id": null,
      "title": "Response: Emergent analogical reasoning in large language models",
      "authors": [
        "Damian Hodel",
        "Jevin West"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "On analogy-making in large language models",
      "authors": [
        "Melanie Mitchell"
      ],
      "year": "2023",
      "venue": "On analogy-making in large language models",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Emergent analogical reasoning in large language models",
      "authors": [
        "Taylor Webb",
        "Keith J Holyoak",
        "Hongjing Lu"
      ],
      "year": "2023",
      "venue": "Nature Human Behaviour",
      "doi": "10.1038/s41562-023-01659-w.url"
    },
    {
      "id": "b2",
      "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
      "authors": [
        "Zhaofeng Wu"
      ],
      "year": "2023",
      "venue": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "3 Conclusion",
      "text": "Based on their analysis, Webb, Holyoak, and Lu (2023) argue that LLMs have acquired a general ability for zero-shot reasoning. With full respect to the authors and their work, we disagree with this interpretation. As we show and argue in our response, the methods are insufficient to evaluate a capacity for true, zero-shot reasoning. Given the current hype surrounding LLMs, we hope this can be used to spur further tests and evaluations of what LLMs can and cannot do."
    },
    {
      "title": "4 Code And Data Availability",
      "text": "Code and data can be downloaded from: [https://github.com/hodeld/emergent_analogies_LLM_fork](https://github.com/hodeld/emergent_analogies_LLM_fork)"
    },
    {
      "title": "References",
      "text": "* (1) * Mitchell (Jan. 2023) Mitchell, Melanie (Jan. 2023). _On analogy-making in large language models_. URL: [https://aiguide.substack.com/p/on-analogy-making-in-large-language](https://aiguide.substack.com/p/on-analogy-making-in-large-language) (visited on 08/09/2023). Webb, Taylor, Keith J. Holyoak, and Hongjing Lu (July 2023). \"Emergent analogical reasoning in large language models\". en. In: _Nature Human Behaviour_. issn: 2397-3374. doi: 10.1038/s41562-023-01659-w. url: [https://www.nature.com/articles/s41562-023-01659-w](https://www.nature.com/articles/s41562-023-01659-w). * Wu et al. (Aug. 2023) Wu, Zhaofeng et al. (Aug. 2023). _Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks_. en. arXiv:2307.02477 [cs]. url: [http://arxiv.org/abs/2307.02477](http://arxiv.org/abs/2307.02477) (visited on 03/14/2024)."
    },
    {
      "title": "5 Author Contributions",
      "text": "D.H. conducted the experiments. D.H. and J.W. drafted the manuscript."
    },
    {
      "title": "6 Competing Interests",
      "text": "The authors declare no competing interests. Appendix"
    },
    {
      "title": "Counterexamples",
      "text": "To investigate whether the problems presented in the original paper truly assess analogical reasoning in GPT-3 or primarily its capability to recite training data, we create non-standard variants of the original tasks that are less likely to be found in training data. Our focus is on the letter string analogies, a subset of the four problem domains examined, and we conduct tests with both human subjects and GPT-3. In our experiments, GPT-3 performance significantly declines when presented with these additional counterexamples, while human performance remains consistently high across all tests (2). This suggests that the claims made in the original paper regarding GPT-3's zero-shot reasoning may not be substantiated."
    },
    {
      "title": "7.1.1 Methods",
      "text": "In order to test GPT-3's generality in zero-shot analogical reasoning, we extend the letter string analogies with two modifications and compare GPT-3's and humans' performance analogous to the original approach. The modifications involve using a synthetic alphabet and increasing the size of the interval from one to two letters, see Figure 1. If the claim regarding GPT-3's zero-shot reasoning capability is true, we can expect similar performance across modifications, in particular, independent of the alphabet. Unlike the original study, we view the comparison with human performance not as evidence for or against GPT-3's analogical reasoning abilities, but rather as a confirmation of the validity of our set of problems. We create the synthetic alphabet by randomly changing the order of the letters in the real alphabet. For both humans and GPT-3, we incorporate the synthetic alphabet in the tasks by preceding the original prompt with the sentence \"Use this fictional alphabet: [x y l k w b f z t n j r q a h v g m u o p d i c s e].\" The increase in the size of the interval from one to two letters aims to rule out the possibility that GPT-3 merely replicates the fed sequence of letters. We achieve this in two ways. For the problem types 'extend sequence','successor', and 'predecessor', we increase the interval size for the _letter to change_ from one to two. For the problem types'remove redundant letter', 'fix alphabetic sequence', and'sort', we increase the interval size of the _complete letter sequence_ from one to two 9. Footnote 9: It is worth noting that we apply this modification to both the source (the first row for each example in Figure 1) and the target (the second row for each example in Figure 1), minimizing the difficulty of the modified problems and allowing us to compare our tests to the zero-generalization problems given in the original paper. We compare GPT-3's and human performance for the following three settings: the original tasks as reported in (Webb, Holyoak, and Lu, 2023), counterexamples that involve the interval size modification, and counterexamplesthat include both the interval size modification and the synthetic alphabet. To ensure that GPT-3 is capable of processing the introduced modifications (Wu et al., 2023, \"counterfacial comprehension check\"), we additionally include tests on GPT-3 for two additional settings: original examples on the real alphabet but including the modified prompt, i.e. \"Use this fictional alphabet: [a b c d e f g h i j k l m n o p q r s t u v w x y z ]. \"), and counterexamples involving the synthetic alphabet but without increasing the interval size. GPT-3 evaluationOur code for reproducing Figure 2 is available on Github10. For each problem type, we create 50 instances to mirror the original paper. The settings are as follows: model variant=text-davinci-003, temperature=0, maximum length=20. Using the original code, we mirror the evaluation and analysis Figure 1: Letter string analogies along their transformations of both the original paper and our counterexamples. We introduce a synthetic alphabet into the task and apply two types of letter sequence modifications, both based on increasing the interval from one to two letters. For the transformation types ’extend sequence’, ’successor’, and ’predecessor’, the modification only affects the _letter to change_ (last or first letter). For ’remove redundant letter’, ’fix alphabetic sequence’, and ’sort’, the interval is increased for the complete letter sequence. We apply the same modifications to the problems generated with the synthetic alphabet. approach of the original paper. The prompt pattern including the synthetic alphabet illustrates the following example. Use this fictional alphabet: [x y l k w b f z t n j r q a h v g m u o p d i c s e]. Let's try to complete the pattern: [x y l k] [x y l k b] [t n j r] [ Human behavioral experiment.We conducted human behavior experiments through an online study with University of Washington (UW) undergraduates analogous to the experiments of the original paper. All participants provided their informed consent prior to the study, and the data collection process was approved by the UW Institutional Review Board (IRB ID STUDY00019080, approved on 6 November 2023). 121 participants completed the study. They were compensated with extra course credits for their participation. The first author of the original study generously provided participant instructions, which we adapted for our experiments. In particular, we presented the participants an additional example problem to introduce the synthetic alphabet. Use this fictional alphabet: [x y l k w b f z t n j r q a h v g m u o p d i c s e]. [x x x] [y y y] [l l l ] [?] Each participant completed a total of 18 zero-generalization tasks, consisting of six problems for each setting (one problem for each transformation type)."
    },
    {
      "title": "7.1.2 Results",
      "text": "In our experiments, human achieved consistently higher accuracy than GPT-3, in particular on modified letter string tasks involving both the synthetic alphabet and increased letter interval size, see Figure 2. Human performance remains at a level similar across modifications (Figure 4) while GPT-3 performance declines significantly for modified problem types (Figure 4). The generative accuracy of GPT-3 for the synthetic alphabet is close to zero (\\(<0.1\\)) when performing the modified tasks 'extend sequence','successor' or 'predecessor', and 'fix alphabetic sequence'. Only for'remove redundant letter' and'sort' does GPT-3 achieve accuracy in a range similar to that reported in the original paper (Webb, Holyoak, and Lu, 2023). Figure 5 shows the accuracy of GPT-3 in the two counterfactual comprehension checks (Wu et al., 2023). For all but on the 'precessor' task on the synthetic alphabet, we obtain a GPT-3 accuracy of at least 30% of the original level, demonstrating GPT-3's ability to process the introduced modifications. Lastly, Figure 6 illustrates the comparison of human performance in the original tasks between the participants of the original study and those in our study. Although the subjects in our study marginally outperform those in the previous study, the similarity in performances is evidence that our experimental setup and execution align with the original study at UCLA."
    },
    {
      "title": "7.1.3 Discussion",
      "text": "The recent paper, \"Emergent analogical reasoning in large language models\" (Webb, Holyoak, and Lu, 2023), and subsequent news articles argue that LLMs may have acquired the emergent ability for zero-shot analogical reasoning. We are less certain of these conclusions, given our own follow-up experiments. Our results show low success of GPT-3 in solving letter string problems with simple modifications and with a synthetic alphabet, while human performance remains high. Only in two out of six problem types ('remove redundant letter' and'sort'), GPT-3 achieves similar generative accuracy on our counterexamples compared to the original problems involving the real alphabet, as well as in comparison to human performance on the same modified problems. For these two problem Figure 2: Comparison between GPT-3’s (blue) and human (orange) performances on modified letter string problems involving a synthetic alphabet and a larger interval size. The transformation types and their order correspond to Figure (b)b in the original paper. Humans demonstrate significantly higher accuracy compared to GPT-3. Human results represent the average performance of 121 participants (UW undergraduates). Each participant received one randomly selected instance of each problem subtype. GPT-3 results reflect the average performance across all 50 instances. Gray error bars indicate 95% binomial confidence intervals for the average performance across multiple problems. subtypes, GPT-3 does not need to generate a letter from the full alphabet, but only to remove the duplicate letter or to rearrange given letters, which may explain the higher performance. The results of these two tasks also serve as an additional counterfactual comprehension check (Wu et al., 2023) in addition to the accuracy of GPT-3 under the only marginally modified conditions, shown in Figure 5. The results demonstrate that GPT-3 is capable of processing synthetic alphabets, which validates our approach. So what explains the high success of GPT-3 in solving the problems on the real alphabet (as used in the original paper) but failure with the synthetic alphabet and with the modified interval size for most of the letter string problems while human performance remains consistently high? Our results suggest that the answer resides in the training data confirming the analysis of the methods in Section 2. Unlike humans, GPT-3 performs well only for simple analogy problems with the standard English alphabet, which are likely to be present in the training data. These findings contradict two of the main claims in the original paper (Webb, Holyoak, and Lu, 2023) regarding GPT-3's capacity for general, zero-shot reasoning and its human-like characteristics in analogical reasoning. Consequently, we reject the proposition made in the original paper that GPT-3 may have developed mechanisms similar to those underlying human intelligence. The GPT-3 failure to solve simple variations of the original problems demon Figure 3: **GPT-3** performance for zero-generalization letter string problems for the original experiment (blue) and with the larger interval size (green), and larger interval size with synthetic alphabet (orange). Except for ’remove redundant letter,’ GPT-3’s accuracy declines significantly for the modified problems. The results reflect an average performance for N=50 instances. strates the brittleness of the presented approach when assessing human-like reasoning in language models."
    },
    {
      "title": "Chatgpt'S Answer To Our Question: \"Could You Give An Example Of A Copycat Problem?\"",
      "text": "Figure 4: **Human** performance for zero-generalization letter string problems for the original experiment (blue) and with the larger interval size (green), and larger interval size with synthetic alphabet (orange). Human accuracy in the modified problems is comparable to that in the original problems (blue). The results reflect the average performance of N = 121 participants (UW undergraduates). Figure 5: Counterfactual comprehension check. Comparison of GPT-3 performance on zero-generalization letter string problems between original tasks (blue) and the only marginally modified tasks involving a synthetic alphabet without modification of the interval size (green) and a modified prompt without modified string sequence (orange). The accuracy on modified tasks is lower than on the original ones but, greater than 0.2 except for ‘remove redundant letter’ and ‘sort’ involving the synthetic alphabet. The figure and the order of the transformation types correspond to Figure 6b in the original paper. These results reflect an average performance for N=50 instances. Figure 6: Comparison of human performance on the original letter string tasks between the outcomes reported in the original study (blue) and the findings presented in this paper (green). UW undergraduate students exhibit marginally higher accuracies. The transformation types and their order correspond to Figure 6b in the original paper. [MISSING_PAGE_EMPTY:14]"
    }
  ]
}