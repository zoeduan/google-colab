{
  "title": "Garbage in, garbage out: Zero-shot detection of crime using Large Language Models",
  "authors": [
    "Anj Simmons",
    "Rajesh Vasa"
  ],
  "abstract": "\n This paper proposes exploiting the common sense knowledge learned by large language models to perform zero-shot reasoning about crimes given textual descriptions of surveillance videos. We show that when video is (manually) converted to high quality textual descriptions, large language models are capable of detecting and classifying crimes with state-of-the-art performance using only zero-shot reasoning. However, existing automated video-to-text approaches are unable to generate video descriptions of sufficient quality to support reasoning (garbage video descriptions into the large language model, garbage out). \n",
  "references": [
    {
      "id": null,
      "title": "Garbage in, garbage out: Zero-shot detection of crime using Large Language Models",
      "authors": [
        "Anj Simmons",
        "Rajesh Vasa"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Detection of rare events with uncertain outcomes",
      "authors": [
        "R Ilin"
      ],
      "year": "2021",
      "venue": "International Journal of Approximate Reasoning",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Large language models are zero-shot reasoners",
      "authors": [
        "T Kojima",
        "S S Gu",
        "M Reid",
        "Y Matsuo",
        "Y Iwasawa"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Evaluating commonsense in pre-trained language models",
      "authors": [
        "X Zhou",
        "Y Zhang",
        "L Cui",
        "D Huang"
      ],
      "year": "2020",
      "venue": "AAAI 2020 -34th AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "GPT-4 Technical Report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "GPT-4 Technical Report",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Vatex: A large-scale, high-quality multilingual dataset for video-and-language research",
      "authors": [
        "X Wang",
        "J Wu",
        "J Chen",
        "L Li",
        "Y.-F Wang",
        "W Y Wang"
      ],
      "year": "2019",
      "venue": "The IEEE International Conference on Computer Vision (ICCV)",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Real-world anomaly detection in surveillance videos",
      "authors": [
        "W Sultani",
        "C Chen",
        "M Shah"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y J Lee"
      ],
      "year": "2023",
      "venue": "Visual instruction tuning",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
      "authors": [
        "M Turpin",
        "J Michael",
        "E Perez",
        "S R Bowman"
      ],
      "year": "2023",
      "venue": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "GIT: A Generative Image-to-text Transformer for Vision and Language",
      "authors": [
        "J Wang",
        "Z Yang",
        "X Hu",
        "L Li",
        "K Lin",
        "Z Gan",
        "Z Liu",
        "C Liu",
        "L Wang"
      ],
      "year": "2022",
      "venue": "Transactions on Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Bytetrack: Multi-object tracking by associating every detection box",
      "authors": [
        "Y Zhang",
        "P Sun",
        "Y Jiang",
        "D Yu",
        "F Weng",
        "Z Yuan",
        "P Luo",
        "W Liu",
        "X Wang"
      ],
      "year": "",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Garbage In, Garbage Out: Zero-Shot Detection Of Crime Using Large Language Models",
      "text": "Anj Simmons, Rajesh Vasa Applied Artificial Intelligence Institute, Deakin University, Geelong, Australia Email: {a.simmons, rajesh.vasa}@deakin.edu.au"
    },
    {
      "title": "Abstract",
      "text": "This paper proposes exploiting the common sense knowledge learned by large language models to perform zero-shot reasoning about crimes given textual descriptions of surveillance videos. We show that when video is (manually) converted to high quality textual descriptions, large language models are capable of detecting and classifying crimes with state-of-the-art performance using only zero-shot reasoning. However, existing automated video-to-text approaches are unable to generate video descriptions of sufficient quality to support reasoning (garbage video descriptions into the large language model, garbage out). large language models, chain of thought"
    },
    {
      "title": "I Introduction",
      "text": "Intelligence and law enforcement agencies are tasked with detecting threats and preventing crime. Such agencies have access to increasing volumes of data; however, as the amount of information available greatly exceeds the capacity of humans available to monitor it, it is impossible to fully monitor this deluge of information and respond in a timely manner. Therefore, there is a need for more sophisticated techniques to ingest information and surface up just the cases that indicate a potential crime of a category relevant to the agency. Automatically detecting crime poses a challenge, as while there are known categories of crime (abuse, arson, assault, burglary, stealing, vandalism, etc.) the ways in which each category of crime can be committed are diverse. Furthermore, the losses caused by crime follow a power law distribution [1] in which rare events (e.g., terrorist incidents) cause disproportionate losses. Contrast this to the assumption of supervised learning approaches, which perform well on categories with sufficient training data, but perform poorly on categories with limited training data. Although supervised learning approaches may still have a role to play in extracting specific features relevant to crime, e.g., the presence of a weapon, it is necessary to consider the broader context of the scenario to determine if a crime is occurring, which has traditionally required a human. In this paper, we propose exploiting the zero-shot reasoning capabilities [2] of Large Language Models (LLMs) for the task of detecting and reasoning about crime. Zero-shot reasoning allows prompting the LLM to reason about whether a description of events suggests evidence of a crime, without the need to provide training examples (other than the corpus the LLM has been pre-trained on). LLMs capture common sense knowledge (albeit at a surface level) [3] which is important to support this reasoning process and avoid false positives, for example, it is not a crime to take an item from a shop if one pays for it before leaving. An example of the proposed approach is shown in Figure 1. To test the approach, we evaluate the ability of a state-of-the-art LLM (GPT-4 [4]) to detect crimes given textual descriptions of events in real-world surveillance videos. For the purposes of this paper, we manually created descriptions of 168 surveillance videos. We also explored approaches for automatically generating descriptions from video, which would allow for a fully automated approach to detecting crime in surveillance video, but found that the quality of the Fig. 1: Example application of proposed method for zero-shot detection of crime: 1) Video is converted to a prompt consisting of a textual description of the video along with a fixed task description. The surveillance video description in this example was driven by a human, but we also explore approaches to automate this step. 2) GPT-4 is used to reason (i.e., produce a Chain-of-Though) about possible explanations and determine the most likely category. automatically generated descriptions was insufficient for the LLM to accurately detect and reason about crimes (garbage in, garbage out). The key contributions of this paper are: 1. A dataset of textual descriptions derived from real-world surveillance videos which can be used to benchmark the ability of LLMs to reason about crime, and 2. Identification of obstacles in existing video-to-text approaches that prevent a fully-automated approach. Data and code for this paper are publicly available online1. Footnote 1: [https://github.com/anjsimmo/zero-shot-crime-detection](https://github.com/anjsimmo/zero-shot-crime-detection)"
    },
    {
      "title": "Ii Background And Related Work",
      "text": "Previous work has considered the task of activity recognition in videos. However, this requires large training datasets, which are not available in the case of rare types of crime. Previous work has also considered training multimodal models to support zero-shot reasoning about images and other input modalities. However, such models have limited transparency about which information in the images are used to inform the decision, which is essential to know and control in the case of crime detection."
    },
    {
      "title": "_Activity Recognition_",
      "text": "Large-scale datasets have been collected for the task of describing activities in video, such as VaTeX [5] (consisting of 41,250 videos with annotations). For the task of crime detection, the UCF-Crime dataset [6] consists of 128 hours of surveillance video obtained from YouTube and LiveLeak. However, existing activity recognition methods perform poorly on the UCF-Crime dataset, only achieving 28.4% classification accuracy [6]. Furthermore, rare crime categories such as terrorism are not included. In contrast to previous crime detection approaches, this paper explores a zero-shot approach to circumvent the need for large training datasets."
    },
    {
      "title": "_Multimodal Models_",
      "text": "Multimodal models are trained on two or more input modalities, such as both text and images. Such models can accept image inputs directly, allowing reasoning about content in images without the need for an intermediate step to first convert video to text. Prior work has also explored integrating LLMs with pre-trained image encoders to allow reasoning about the content of images [7]. There are three reasons why we focus on reasoning about textual descriptions of videos in this paper rather than training a multimodal model to operate on the video directly. Firstly, using textual descriptions allows the use of existing state of the art LLMs, such as GPT-4 and derivatives of LLaMA, without the need to retrain or fine-tune them to accept new input modalities. Although the GPT-4 model itself is multimodal [4], OpenAI do not yet provide a way for the public to input images to GPT-4 via the API. Secondly, we may wish to incorporate information other than images in future, for example, descriptions of sounds detected or new kinds of sensors. When all information sources are represented in textual form, it is trivial to integrate new information. Thirdly, textual descriptions provide a way to restrict which information the LLM has access to by censoring details the LLM should not use in its decision, such as race and gender. Inspecting the chain-of-thought produced by the LLM for bias is insufficient, as LLMs may produce unfaithful explanations that do not reveal the underlying factors that influenced the decision [8]. Censoring these details is more difficult when the input includes images/videos."
    },
    {
      "title": "Iii Method",
      "text": "This section explains the method by which we converted surveillance videos into textual descriptions and evaluated the ability of GPT-4 to detect and reason about crime. An overview of the process is shown in Figure 2."
    },
    {
      "title": "_Dataset_",
      "text": "We test our approach on the UCF-Crime dataset of surveillance videos [6]. The authors of the UCF-Crime [6] propose two tasks. The first task is detection of anomalous events in surveillance videos, on which the original paper scores 75.41 AUC, and current state of the art scores 86.98 AUC2. However, this first task only involves identifying which frames relate to an anomalous event, not reasoning about the type of anomalous event. The second task is anomalous activity Fig. 2: We provide descriptions of people/objects and interactions within the UCF-Crime dataset, then evaluate the ability of GPT-4 chain-of-thought reasoning to infer the category of crime. recognition. This involves determining the category of crime (if any) that occurs in a surveillance video. The authors of the UCF-Crime dataset explain that state of the art activity recognition methods perform poorly, only achieving 28.4% accuracy. This second task is the focus of this paper."
    },
    {
      "title": "_Human Captions_",
      "text": "The first author manually wrote descriptions of people, objects, and their interactions in a sample of 12 surveillance videos from 14 categories (13 crime categories + normal), resulting in a dataset of 168 surveillance video descriptions. While the author was aware of the ground truth crime category for videos in the UCF-Crime dataset, the descriptions were written to be as objective as possible rather than suggesting a particular interpretation of the events taking place. The manually written descriptions do not include race or gender. An example of a manually created description is: \"A white car approaches a silver car. Two people come out. One of them goes near the front wheel of the car. The other then opens the bonnet. After 30 seconds, the two people return to the white car.\""
    },
    {
      "title": "_Gpt-4 Prompt_",
      "text": "We prompt GPT-4, a state of the art LLM, to reason about the most likely category of crime given the textual description of the surveillance video. We include the phrase \"let's think step by step\" in the prompt to trigger zero-shot reasoning [2]. The full prompt is: ``` SURVEILANCEVIDEODESCRIPTION: Awhitecarapproachesa silvercar... TASK: Listpossibleexplanations,andcategorisethemasoneone of [Abuse,Arrest,Arson,Assault,Burgary,Explosion,Fighting,RoadAccident,Robbery,Shooting,Shoplifting,Stealing,Vandalism,Normal].Thendeterminewhichcategoryismostlikely.Let'sthinkstepbystep.Finally,outputonelinecontainingasinglecategoryinquotes.Donotincludeanythingotherthancategoryonthefinalline. ``` GPT-4 reasons about possible explanations for the observed events, followed by what it determines to be the most likely category. An example response from GPT-4 is: Possible explanations: 1. The two people are helping the driver of the silver car with a mechanical issue [Normal] 2. The two people are attempting to steal parts from the silver car [Stealing] 3. The two people are tampering with the silver car to cause harm [Vandalism] 4. The two people are involved in a car theft [Robbery] Most likely category: \"Normal\" We evaluate how accurately GPT-4 is able to recover the ground truth label for the video."
    },
    {
      "title": "_Automated Captions_",
      "text": "We also explore the accuracy with which GPT-4 can recover the ground truth on automatically generated captions of videos. To automatically generate a description of the video, we sample a frame every 10 seconds, pass it through a image-to-text model, then concatenate the results along with time stamps. The choice to sample a frame every 10 seconds was to balance the need to capture key changes in the video while avoiding long and repetitive descriptions that may exceed the input token limit of GPT-4. The method for converting these descriptions to prompts and evaluating the classification accuracy is identical to the process for human captions."
    },
    {
      "title": "Iv-D1 Git Captions",
      "text": "To generate image captions, we used a pre-trained Generative Image-to-text Transformer (GIT) [9], specifically, git-large-coco3 which has been fine-tuned by Microsoft on the COCO dataset. Footnote 3: [https://huggingface.co/microsoft/git-large-coco](https://huggingface.co/microsoft/git-large-coco) An example of the final description generated for a video by this process is: \"10 sec: a car is seen parked on the street. 20 sec: a car is seen parked on the street. 30 sec: a car is seen passing another car. 40 sec: a man is sitting on the ground next to a car. 50 sec: a car is parked on the street and another car is parked behind it. 60 sec: a man is opening the trunk of a car. 70 sec: a man is trying to get a car out of the back of a car. 80 sec: a car is parked in front of a house. 90 sec: a car is parked on the street. 100 sec: a car is parked on the street. 110 sec: the car is parked on the street. 120 sec: a car is seen driving down a street.\""
    },
    {
      "title": "Iv-D2 Llava Descriptions",
      "text": "Large Language and Vision Assistant (LLaVA) [7] is a visual instruction tuned version of LLaMa. Specifically, we use the 13 billion parameter version, 13b-v0, which was the largest and most recent release of the LLaVA model at the time of conducting the experiment. To generate descriptions, we prompt LLaVA with an image from the video and the question \"What is it?\" An example of the final description generated for a video by this process is: \"10 sec: In the image, there is a black car parked on a street next to a building. The car appears to be parked in a parking space, and there are potted plants nearby. The car is facing a house with a fence and a gate. There is also a person standing on the sidewalk, possibly observing the car or the surrounding area. To assist the user further, more context or specific questions about the scene would be needed. 20 sec: In the image, there is a gray car parked on the side of the street, and a white van is driving down the street. The scene takes place on a residential street with a fence and a house in the background. To assist you better, I would need more information about the situation or a specific question related to the image. 30 sec:...\""
    },
    {
      "title": "Iv-D3 Yolo-V8 + Bytetrack",
      "text": "We make use of a YOLO-based object tracking library4 to track people and objectsin surveillance videos. Specifically, we use yolov8x5 (the largest and most accurate version of the model) combined with the ByteTrack [10] tracking method. Footnote 5: [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics) The tracks are updated every frame (30 frames per second) to support keeping track of identities; however, we only output the current state every 10 seconds. To convert this to text, for each tracked object, we state the object class (person, car, etc.), identity of the tracked object (e.g. \"car 2\") and position (e.g. \"bottom-left\"). An example of the final description generated for a video by this process is: \"0 sec: car 1 is at the bottom-left of the image. 10 sec: car 1 is at the bottom-left of the image. car 2 is at the top-middle of the image. 20 sec: car 1 is at the bottom-left of the image. car 2 is at the middle of the image. 30 sec: car 1 is at the bottom-left of the image. car 2 is at the middle of the image. person 3 is at the bottom-middle of the image. person 5 is at the middle of the image. 40 sec: car 1 is at the bottom-left of the image. car 2 is at the middle of the image. person 5 is at the middle of the image. person 6 is at the bottom-middle of the image. 50 sec:...\""
    },
    {
      "title": "Iv Results",
      "text": "We tested each method on 12 videos from 14 categories (168 videos total) and report the classification accuracy in Table I. For comparison, we also include a random baseline (1/14), and the Tube Convolutional Neural Network (TCNN) baseline reported by Sultani et al. [6] in the UCF-Crime paper. In cases where GPT-4 was unable to process the input (e.g. exceeded token length) or did not output a valid response (i.e. the final line of output was not a valid category in the expected format) we exclude these from the accuracy calculation. There was 1 case of invalid output for GPT-4 + Human captions, 1 case of invalid output for GPT-4 + GIT Captions, and 2 cases of input that could not be processed due to exceeding token length for GPT-4 + LLaVA Captions. There were no cases of invalid input or output for GPT-4 + YOLO-v8 + ByteTrack."
    },
    {
      "title": "V Discussion",
      "text": "Our results show that while GPT-4 was able to determine the crime category with state of the art performance when provided a human generated caption of the video, it performed poorly when provided with automatically generated captions. In the rest of this section, we elaboration on the limitations of automatic caption generation approaches that need to be overcome to support a fully automated approach to crime detection."
    },
    {
      "title": "_Image Captions Lack Detail_",
      "text": "Reasoning about crime requires details of who did what. However, image captioning models only describe the scene at a high level. For example, consider the caption generated for Figure 3. The image captioning model correctly identifies that the image contains a man sitting on the ground next to a car. However, it provides no details about which man and which car. Without this detail, there is insufficient information to reason about whether the man is sitting on the ground to repair their own car, or is sitting on the ground to steal something from someone else's car."
    },
    {
      "title": "_Llm Based Vision Models Hallucinate_",
      "text": "In contrast to image captioning models, LLM based vision models are capable of generating detailed descriptions of images, but may hallucinate about objects present and actions Fig. 4: LLaVA Description: “In the image, a group of young men is playing pool together in a room. There are at least seven people present, with some standing and others sitting around the pool table. The players are engaged in a game of pool, with the cue ball visible on the table. The room appears to be a casual gathering space, as there are chairs and a bench placed around the area. Additionally, there is a microwave in the room, suggesting that it might be a part of a larger living space, such as a dormitory or a recreational area. The overall atmosphere seems to be one of enjoyment and camaraderie among the players.” Fig. 3: GIT Caption: “a man is sitting on the ground next to a car.”being performed. Furthermore, the descriptions they generate are biased towards a particular interpretation of the scenario, an example of which is shown in Figure 4. Reasoning about crime requires objective descriptions."
    },
    {
      "title": "_Object Tracking Algorithms Cannot Maintain Identity Of Objects Over Long Time Periods_",
      "text": "Reasoning about crime requires linking the identity of actors across time. For example, it is not a crime for an actor to take an item from a store if they pay for it before leaving. However, in our experiments, we observed that object tracking was unable to maintain a constant identity for people and objects over long time periods. For example, in a video where there were only two people, the description generated by applying object tracking refers to \"person 3\", \"person 5\", \"person 6\" and \"person 8\", making it difficult to link the actions of people across time."
    },
    {
      "title": "_Curated Object Detection Datasets Do Not Include Weapons_",
      "text": "Large benchmark datasets for object detection, such as COCO, do not include weapons. While it is understandable that technology companies that curate datasets for training machine learning algorithms may want to distance themselves from undesirable uses of AI, if we wish for AI systems to be able to help prevent violence, then it is important for datasets curators to include depictions of weapons and violence."
    },
    {
      "title": "Vi Threats To Validity",
      "text": "GPT-4 may have already have seen images from UCF-Crime videos in its training data. However, we test on textual descriptions of the videos rather than the videos themselves, and these textual descriptions have not been released before. As such, it is unlikely that the performance of GPT-4 reported in this paper is a result of overfitting to training data. The author was aware of the ground-truth category when creating the human captions for videos, which may have biased the descriptions. Furthermore, in busy scenes, it was not practical to describe every action taking place, hence the descriptions may be biased towards just describing the actions of relevance to the crime. As such, the performance of GPT-4 on human captions should be taken only as an indicator of what is possible, and may not be possible to fully automate even if the obstacles raised in this paper can be overcome."
    },
    {
      "title": "Vii Conclusion",
      "text": "This paper demonstrated that with high quality textual descriptions, large language models are capable of detecting and classifying crimes with state-of-the-art performance using only zero-shot reasoning. Unfortunately, existing automated video-to-text approaches were unable to generate video descriptions of sufficient quality to support reasoning, thus fully automated detection of crime is not yet possible. The failure of these approaches to generate descriptions suitable for reasoning about crime indicates that such models are not as general purpose as widely perceived, and that these models require domain adaptation for downstream tasks. Future research is needed to overcome the loss of objective detail that occurs during the video-to-text conversion process."
    },
    {
      "title": "Acknowledgment",
      "text": "This paper was supported by research funding from the National Intelligence Postdoctoral Grant program (NIPG-2021-006)."
    },
    {
      "title": "References",
      "text": "* [1]R. Ilin (2021) Detection of rare events with uncertain outcomes. International Journal of Approximate Reasoning131, pp. 252-267. Cited by: SSI, SSII-A, SSII-"
    }
  ]
}