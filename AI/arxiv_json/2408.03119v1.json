{
  "title": "Evaluating the Translation Performance of Large Language Models Based on Euas-20",
  "authors": [
    "Yan Huang",
    "Wei Liu"
  ],
  "abstract": "\n In recent years, with the rapid development of deep learning technology, large language models (LLMs) such as BERT and GPT have achieved breakthrough results in natural language processing tasks. Machine translation (MT), as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap. Despite the significant progress in translation performance achieved by large language models, machine translation still faces many challenges. Therefore, in this paper, we construct the dataset Euas-20 to evaluate the performance of large language models on translation tasks, the translation ability on different languages, and the effect of pre-training data on the translation ability of LLMs for researchers and developers. \n",
  "references": [
    {
      "id": null,
      "title": "Evaluating the Translation Performance of Large Language Models Based on Euas-20",
      "authors": [
        "Yan Huang",
        "Wei Liu"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Is chatgpt a good translator? a preliminary study",
      "authors": [
        "W Jiao",
        "W Wang",
        "J T Huang",
        "X Wang",
        "Z Tu"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good translator? a preliminary study",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Chatgpt mt: Competitive for high-(but not low-) resource languages",
      "authors": [
        "N R Robinson",
        "P Ogayo",
        "D R Mortensen",
        "G Neubig"
      ],
      "year": "2023",
      "venue": "Chatgpt mt: Competitive for high-(but not low-) resource languages",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Adaptive machine translation with large language models",
      "authors": [
        "Y Moslem",
        "R Haque",
        "J D Kelleher",
        "A Way"
      ],
      "year": "2023",
      "venue": "Adaptive machine translation with large language models",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Towards robust in-context learning for machine translation with large language models",
      "authors": [
        "S Zhu",
        "M Cui",
        "D Xiong"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Unsupervised parallel sentences of machine translation for asian language pairs",
      "authors": [
        "S Zhu",
        "C Mi",
        "T Li",
        "Y Yang",
        "C Xu"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Mining parallel sentences from internet with multi-view knowledge distillation for low-resource language pairs",
      "authors": [
        "S Zhu",
        "S Gu",
        "S Li",
        "L Xu",
        "D Xiong"
      ],
      "year": "2024",
      "venue": "Knowledge and Information Systems",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "FEDS-ICL: enhancing translation ability and efficiency of large language model by optimizing demonstration selection",
      "authors": [
        "S Zhu",
        "L Pan",
        "D Xiong"
      ],
      "year": "",
      "venue": "Inf. Process. Manag",
      "doi": "10.1016/j.ipm.2024.103825"
    },
    {
      "id": "b7",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "N Bashlykov",
        "S Batra",
        "P Bhargava",
        "S Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Falcon-40b: an open large language model with state-of-the-art performance",
      "authors": [
        "E Almazrouei",
        "H Alobeidli",
        "A Alshamsi",
        "A Cappelli",
        "R Cojocaru",
        "M Debbah",
        "E Goffinet",
        "D Heslow",
        "J Launay",
        "Q Malartic"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "authors": [
        "L Zheng",
        "W L Chiang",
        "Y Sheng",
        "S Zhuang",
        "Z Wu",
        "Y Zhuang",
        "Z Lin",
        "Z Li",
        "D Li",
        "E Xing"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Mistral 7b",
      "authors": [
        "A Q Jiang",
        "A Sablayrolles",
        "A Mensch",
        "C Bamford",
        "D S Chaplot",
        "D D Casas",
        "F Bressand",
        "G Lengyel",
        "G Lample",
        "L Saulnier"
      ],
      "year": "2023",
      "venue": "Mistral 7b",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Bloom: A 176b-parameter open-access multilingual language model",
      "authors": [
        "Le Scao",
        "T Fan",
        "A Akiki",
        "C Pavlick",
        "E Ilić",
        "S Hesslow",
        "D Castagné",
        "R Luccioni",
        "A S Yvon",
        "F Gallé",
        "M"
      ],
      "year": "2023",
      "venue": "Bloom: A 176b-parameter open-access multilingual language model",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Gemma: Open models based on gemini research and technology",
      "authors": [
        "G Team",
        "T Mesnard",
        "C Hardin",
        "R Dadashi",
        "S Bhupatiraju",
        "S Pathak",
        "L Sifre",
        "M Rivière",
        "M S Kale",
        "J Love"
      ],
      "year": "2024",
      "venue": "Gemma: Open models based on gemini research and technology",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Attention is all you need. Advances in neural information processing systems",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A N Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need. Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Efficiently exploring large language models for document-level machine translation with in-context learning",
      "authors": [
        "M Cui",
        "J Du",
        "S Zhu",
        "D Xiong"
      ],
      "year": "2024",
      "venue": "Efficiently exploring large language models for document-level machine translation with in-context learning",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Tjunlp: System description for the wmt23 literary task in chinese to english translation direction",
      "authors": [
        "S Zhu",
        "D Xiong"
      ],
      "year": "2023",
      "venue": "Proceedings of the Eighth Conference on Machine Translation",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Phrase-based statistical machine translation",
      "authors": [
        "R Zens",
        "F J Och",
        "H Ney"
      ],
      "year": "2002",
      "venue": "KI 2002: Advances in Artificial Intelligence: 25th Annual German Conference on AI, KI 2002 Aachen",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Moses: Open source toolkit for statistical machine translation",
      "authors": [
        "P Koehn",
        "H Hoang",
        "A Birch",
        "C Callison-Burch",
        "M Federico",
        "N Bertoldi",
        "B Cowan",
        "W Shen",
        "C Moran",
        "R Zens"
      ],
      "year": "2007",
      "venue": "Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Six challenges for neural machine translation",
      "authors": [
        "P Koehn",
        "R Knowles"
      ],
      "year": "2017",
      "venue": "Six challenges for neural machine translation",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "The flores-101 evaluation benchmark for lowresource and multilingual machine translation",
      "authors": [
        "N Goyal",
        "C Gao",
        "V Chaudhary",
        "P J Chen",
        "G Wenzek",
        "D Ju",
        "S Krishnan",
        "M Ranzato",
        "F Guzmán",
        "A Fan"
      ],
      "year": "2022",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "K Papineni",
        "S Roukos",
        "T Ward",
        "W J Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Comet: A neural framework for mt evaluation",
      "authors": [
        "R Rei",
        "C Stewart",
        "A C Farinha",
        "A Lavie"
      ],
      "year": "2020",
      "venue": "Comet: A neural framework for mt evaluation",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Detecting hallucinated content in conditional neural sequence generation",
      "authors": [
        "C Zhou",
        "G Neubig",
        "J Gu",
        "M Diab",
        "P Guzman",
        "L Zettlemoyer",
        "M Ghazvininejad"
      ],
      "year": "2021",
      "venue": "Detecting hallucinated content in conditional neural sequence generation",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "authors": [
        "Y Bang",
        "S Cahyawijaya",
        "N Lee",
        "W Dai",
        "D Su",
        "B Wilie",
        "H Lovenia",
        "Z Ji",
        "T Yu",
        "W Chung"
      ],
      "year": "2023",
      "venue": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "How good are gpt models at machine translation? a comprehensive evaluation",
      "authors": [
        "A Hendy",
        "M Abdelrehim",
        "A Sharaf",
        "V Raunak",
        "M Gabr",
        "H Matsushita",
        "Y J Kim",
        "M Afify",
        "H H Awadalla"
      ],
      "year": "2023",
      "venue": "How good are gpt models at machine translation? a comprehensive evaluation",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Multilingual machine translation with large language models: Empirical results and analysis",
      "authors": [
        "W Zhu",
        "H Liu",
        "Q Dong",
        "J Xu",
        "S Huang",
        "L Kong",
        "J Chen",
        "L Li"
      ],
      "year": "2023",
      "venue": "Multilingual machine translation with large language models: Empirical results and analysis",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Evaluating The Translation Performance Of Large Language Models Based On Euas-20",
      "text": "Yan Huang 1College of Software, Zhengzhou University of Light Industry, 1 Wei Liu 1College of Software, Zhengzhou University of Light Industry, 1 Footnote 1: email: lwei230215@gmail.com"
    },
    {
      "title": "Abstract",
      "text": "In recent years, with the rapid development of deep learning technology, large language models (LLMs) such as BERT and GPT have achieved breakthrough results in natural language processing tasks. Machine translation (MT), as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap. Despite the significant progress in translation performance achieved by large language models, machine translation still faces many challenges. Therefore, in this paper, we construct the dataset Euas-20 to evaluate the performance of large language models on translation tasks, the translation ability on different languages, and the effect of pre-training data on the translation ability of LLMs for researchers and developers. Keywords:large language models, machine translation, data set"
    },
    {
      "title": "1 Introduction",
      "text": "The application and performance of Large Language Models (LLMs) on translation performance has become an important research direction and practical achievement in the field of modern natural language processing. In the recent emergence of large language models (LLMs), e.g., GPT-3 and GPT-4, their translation performance on Zero-shot can be compared to that of powerful fully supervised machine translation systems[1, 2, 3, 4]. However, the massive corpus used for training big language models is usually dominated by monolingual data, in which the English corpus is dominant, while the proportion of corpus in other languages is relatively small [5, 6]. Under this data distribution, whether the big language models can effectively model the correspondence between different languages and learn reliable translation knowledge is a great concern for researchers [7]. Models may face challenges in handling translation tasks between these languages. Therefore, we evaluate the popular large language models currently available in the market to acquire a better perception of the translation performance of large language models. In this paper, the translation ability of Large Language Models (LLMs) is investigated by answering two questions, 1) What is the translation ability of LLMs?2) Factors affecting the translation ability of LLMs?3) What are the translation results of the LLMs?In response to the first question, we evaluate several popular LLMs: English language-centric LLMs including Llama2[8], Falcon [9], Vicuna[10], Mistral [11] and multilingual LLMs including Bloom and Bloomz[12], Gemma[13]. In order to prevent data leakage and get more accurate results, we constructed a dataset Euas-20(A representative selection of 20 languages). LLMs translate other languages into Chinese and English respectively. The results show a significant improvement in the multilingual translation ability of LLMs. This improvement is not only reflected in the increase of the model's parameters, but also due to the model's improvement in training data and methods. We compare the translation results of LLMs on different languages. We find that there is a significant difference in the translation performance of LLMs on different languages, and LLMs perform better than Chinese when translating into English. For languages similar to English, LLMs also demonstrate better translation performance. Meanwhile, we find that LLMs also have translation ability on zero-resource languages. This suggests that the large language models have some generalisation ability and are able to establish correspondences between different languages in the absence of direct training data. To address the second question, we analysed the LLMs by collecting information from their corpora. We find that a high-quality and diverse corpus can significantly improve the translation performance of LLMs. Multi-language and multi-domain training data can not only enhance the generalisation ability of the model, but also improve its effectiveness in different languages and different domains. To address the third question, we have analysed the translation results of LLMs in various aspects.The translation results of LLMs are subject to some illusory problems, and their fluent output may mislead the users and make it difficult for them to identify the errors in the translation. In addition, LLMs tend to choose the most appropriate translation words in translation tasks by analysing a variety of factors such as semantics, fluency and culture. We also found that when LLMs met words that they had not encountered during model training, the models could not understand or process them accurately due to the lack of training on these words. The purpose of this paper is to review and analyse the performance of the current large language model on translation tasks, to explore whether the large language model can effectively model the correspondence between different languages and the factors affecting translation, for the reference of researchers and developers."
    },
    {
      "title": "2 Background",
      "text": ""
    },
    {
      "title": "Large Language Models",
      "text": "Large Language Models (LLMs) have made significant progress in translation performance. Based on deep learning, especially the Transformer architecture[14, 15, 16], these Large Language Models have learned rich linguistic knowledge by pretraining on a large amount of textual data, thus achieving excellent performance in various downstream tasks. The training process of a large language model is divided into two main phases. The first is the pre-training phase, in which the model learns unsupervised on large-scale textual data to master the basic structure and lexical usage of the language. The goal of this phase is for the model to learn a generic language representation. Next is the fine-tuning phase, in which the pre-trained model is subjected to supervised learning on a specific translation task using a bilingual parallel corpus to equip it with the ability to translate specific language pairs. The big language models support multiple languages, demonstrating the ability to generalize across languages. However, the training data of the big language models are dominated by the English corpus, with a smaller proportion of data in other languages, and this unbalanced data distribution poses a severe test for the performance of the models in a multilingual environment. Researchers are actively exploring ways to address these issues in order to further improve the performance of large language models in translation tasks."
    },
    {
      "title": "Machine Translation",
      "text": "Machine Translation (MT) is a technology that uses computers to automatically translate text from one language to another. In recent years, with the rapid development of artificial intelligence and natural language processing technology, especially the emergence of large language models (e.g., OpenAI's GPT series and Google's BERT), the ability of machine translation has been significantly improved. Modern machine translation systems mainly rely on Neural Machine Translation (NMT) technology[17, 18]. NMT utilises deep learning and neural network models, and is able to efficiently capture and process complex relationships between source and target languages through encoder-decoder architectures and self-attention mechanisms. Compared with traditional rule-based methods and statistical machine translation (SMT)[19, 20], NMT performs better in terms of translation accuracy, fluency, and context understanding. Machine translation, as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap. However, machine translation still faces challenges, including translation of low-resource languages and maintaining coherence and fluency of translation in long texts [21]."
    },
    {
      "title": "3 Experimental Setup",
      "text": ""
    },
    {
      "title": "Dataset",
      "text": "In order to evaluate the real translation capabilities of large language models, we constructed a dataset called Euas-20. This dataset contains twenty representative languages (Table 1), covering a large part of the global population, while [MISSING_PAGE_EMPTY:4] dataset is diverse and representative, and we are able to more comprehensively evaluate the translation capabilities of large language models across different domains and languages."
    },
    {
      "title": "Llms",
      "text": "We evaluated the translation capabilities of nine currently popular LLMSs: Falcon7b, mistral-7b, Llama-2-7b-hf, bloom-7b1, bloomz-7b1-mt, Meta-Llama-3-8B, mpt-7b, vicuna-7b, and gamma-7b."
    },
    {
      "title": "Evaluation Methods",
      "text": "Focusing on Chinese and English, through a prompt (Fig. 2),'source-sentence' stands for the original sentence and 'target-sentence' stands for the target sentence, and the original sentence is input to the LLMs by the command (Translate the following sentence from'source-language' to 'target-language' and The 'target- language' translation is), so that the LLMs can translate and output the target sentence under Zero-Shot learning. In this way, various languages in the dataset are translated into Chinese and English."
    },
    {
      "title": "Evaluation Indicators",
      "text": "Evaluation metrics are an important measure of translation quality. We adopt commonly used automatic evaluation metrics including BLEU[23], which calculates translation accuracy by comparing the n-gram overlap between candidate Figure 1: Prompt 1 and reference translations, which is the traditional method for assessing the quality of machine translation. In addition, we also consider the emerging metric COMET[24], which is designed to learn to predict human judgements of machine translation quality, and which better reflects subjective human assessments of translation quality. By combining these evaluation metrics, we are able to assess the translation performance of large-scale language models in a more comprehensive way, ensuring the accuracy and reliability of the assessment results."
    },
    {
      "title": "4 Testing Of Machine Translation For Llms",
      "text": "In this section, we report the results of the translation of LLMs (Fig.3) and analyse the translation performance of LLMS."
    },
    {
      "title": "Continuous Improvement Of Translation Ability Of Llms",
      "text": "In recent years, the multilingual translation capability of Large Language Models (LLMs) has been significantly improved. Even under Zero-Sample Learning (Zero-Shot) conditions, LLMs still exhibit good translation performance in most translation directions, as shown in Fig. 4. Based on the scores of LLMs on different languages, we can find that the translation ability of LLMs has gradually improved, especially the recent LLMs have reached new heights in terms of translation performance. For example, Llama-3-8B significantly outperforms the previous Llama-2-7B, and vicuna-7B outperforms Llama-2-7B. Overall, Llama-3-8B performs the best among all the LLMs evaluated, and it obtains the highest Figure 2: Prompt 2 BLEU and COMET scores in most translation directions, showing its superior translation capabilities. This progress is not only reflected in the increase of the model's parameters, but also due to the model's improvement in training data and methodology. Llama-3-8B uses larger and higher quality multilingual datasets during training, and adopts more advanced training algorithms, which enable it to maintain a high level of translation quality even when dealing with complex and rare language pairs. At the same time, the model's architectural optimisation and inference speed have also been improved, making Llama-3-8B not only more accurate but also more efficient in practical applications. In addition, Llama-3-8B and other advanced LLMs demonstrate a high degree of flexibility and adaptability in coping with multilingual text comprehension and generation tasks. These models can play an important role in cross-cultural and cross-linguistic communication."
    },
    {
      "title": "Translation Performance Of Llmss Across Languages",
      "text": "The translation performance of large-scale language models (LLMs) varies significantly across languages. Typically, LLMs translate well on high-resource languages, but have relatively poor translation performance on low- and medium-resource languages. We find that LLMs perform particularly well when translating into English and relatively poorly when translating into Chinese. For languages similar to English, LLMs also demonstrate better translation perfor Figure 3: BLEU and COMET scores for nine LLMs translations centred on English and Chinese. mance. For example, LLMs generally achieve better translation results in the Indo-European Romance and Germanic languages. For some languages that are more different from English, such as the Tai-Kadai languages, LLMs produce very poor translation results. This uneven translation performance is mainly due to the differences in the training data, where the high volume and quality of data for high-resource languages make the models perform better on these languages. On the other hand, low and medium resource languages are difficult for the model to learn enough linguistic features due to the scarcity of data, resulting in unsatisfactory translation results. Nevertheless, LLMs show some translation ability even on zero-resource languages. This suggests that the large language models possess some generalisation ability and are able to establish correspondences between different languages in the absence of direct training data. Behind this ability is the fact that the models have learnt common features and structures between languages through large-scale multilingual training, so that they can still translate reasonably well in the face of new language pairs."
    },
    {
      "title": "Effect Of Corpus On The Translation Performance Of Llms",
      "text": "By analysing pre-training data and corpora of large-scale language models (LLMs), we can investigate the relationship between translation performance and corpus Figure 4: Translation performance (BLEU) of LLMS on our evaluated languages, ‘xx-en’ and ‘xx-zh’ denote translation from other languages to English and Chinese, respectively. size and category. By collecting LLMs with training data sizes (Table 2), we find that the larger the pre-training data size, the better the translation performance of the LLMs.For example, Llama-3-8B and Gemma-7B outperform other models overall. This suggests that rich training data is one of the key factors to improve the translation ability of the models. Most of the pre-training corpora of the big language models are English-centric, which on the one hand makes the models perform extremely well in English language ability; on the other hand, it also leads to their weaker ability in non-English languages. This English-centric corpus configuration improves the efficiency of the model in handling English-related tasks, but the model's performance appears to be insufficient when dealing with other languages, especially low- and medium-resource languages. Models trained in multiple languages have achieved better results in translation than LLMs limited to one or a few languages. For example, the translation performance of Gemma-7B is significantly better than that of Falcon-7B.Meanwhile, when translating languages, multilingual models tend to have better translation ability for languages that have been pre-trained than for languages that have not been pre-trained. For example, the corpus share of bloom-7b1 and bloomz-7b1-mt (Fig. 5) has better translation ability for pre-trained languages. This suggests that multi-language training can effectively enhance \\begin{table} \\begin{tabular}{l l} \\hline LLM & Token \\\\ \\hline Gemma 7b & 6 trillion \\\\ Llama-2-7b-hf & 2 trillion \\\\ mpt-7b & 1 trillion \\\\ Meta-Llama-3-8B & 15 trillion \\\\ \\hline \\end{tabular} \\end{table} Table 2: Training volume of LLMs Figure 5: Corpus share of LLMs the model's translation capability, enabling it to better handle translation tasks between various language pairs. From these observations, it can be found that a high-quality and diverse corpus can significantly improve the translation performance of LLMs. Multi-language and multi-domain training data can not only enhance the generalisation ability of the model, but also improve its effectiveness in different languages and different domains. Therefore, in order to meet the translation needs of various languages, future LLMs should make full use of diverse corpora in the pre-training process and continuously increase the proportion of data from low- and medium-resource languages."
    },
    {
      "title": "Illusions In The Translation Of Llms",
      "text": "Neural Machine Translation (NMT) is a task that translates a source language into a target language through inference and relies on parallel data samples used for training. Compared to Statistical Machine Translation (SMT), the output of NMT is usually very fluent, with a quality close to the human level. However, this poses a potential problem: when NMT hallucinates (i.e., generates inaccurate or spurious translations), its smooth output may mislead users and make it difficult for them to identify errors in the translation. By analysing the translation results of LLMs, we classified the hallucinations in NMT as two categories [25], intrinsic and extrinsic hallucinations. Intrinsic Illusion: Incorrect information is included in the translation that does not match what is in the source text. An example of such an illusion is ', which negates'in the source text. Extrinsic illusions: the translation produces additional content that does not exist in the source text.'is an example of illusory content because it is added without any apparent connection to the input. The results show that as the pre-trained corpus continues to grow, the pre-trained model becomes more and more effective in generating faithful summaries of human assessments. By comparing the translation results of Gemma-7B and Falcon-7B, more intrinsic illusions are generated for monolingual models; while for multilingual models, this is less frequent. Also, we found that nouns are the most hallucinated words, and verbs also account for a certain number of hallucinations. In addition, LLMS tend to be more prone to intrinsic hallucinations when confronted with untrained language. \\begin{table} \\begin{tabular}{l l} \\hline Category Source & Correct Translation Hallucinatory Translation \\\\ \\hline Intrinsic & Excuse me, how much do you know about this technology? & \\\\ Extrinsic & Excuse me, how much do you know about this technology? & \\\\ \\hline \\end{tabular}. \\end{table} Table 3: Illusions in the translation of LLMsTherefore, it is crucial to improve the accuracy and reliability of machine translation. By continuously improving our models, enhancing the diversity and quality of our datasets, and using more advanced evaluation metrics to detect and reduce illusions in translation, we can mitigate these risks and provide more secure and reliable translation services."
    },
    {
      "title": "Translation Words That Llms Tend To Choose In Translation Tasks",
      "text": "This section explores the translation words that Large Language Models tend to choose in translation tasks and the reasons behind them. Through previous analyses of the translation results of Gemma-7B and Falcon-7B, we found that LLMs tend to choose common word collocations in the target language during the translation process. This not only improves the naturalness of the translation, but also makes it more in line with the usage habits of the target language. For example,'make a decision' in English is often translated as'instead of'in Chinese because the former is a common collocation in Chinese and is more in line with the language convention. In addition, we also found that the model selects those words that are closest in meaning to be translated by deep understanding of the original and the translated text. For example, when translating the English word 'computer' into Chinese, the model chooses'instead of'because'is a common collocation in modern Chinese. 'is more commonly used and semantically accurate in modern Chinese. LLMs tend to choose the most appropriate translation words in translation tasks by comprehensively analysing various factors such as semantics, fluency and culture. This approach not only improves the accuracy and naturalness of the translation, but also makes the translation result more in line with the usage habits of the target language."
    },
    {
      "title": "Phenomenon Of Unregistered Words",
      "text": "Out-of-vocabulary words (OOV words), refer to words that have not been encountered during model training. These words may be new terms, technical terms, foreign language vocabulary, or recently emerged buzzwords. We found that due to the lack of training on these words, the model cannot understand or process them accurately. We choose the translation results of Gemma-7B and Falcon-7B as representative. For monolingual models, when the model is confronted with words that have not been trained across languages, such as'madilim na bagay' (dark matter) in Filipino, the model will ignore or mistranslate them to other nouns. For multilingual models, even if the model has been trained cross-linguistically, when the model encounters a new word like'schadenfreude' (a German word that refers to the emotion of taking pleasure in someone else's misfortunes), it may not be able to correctly understand the meaning because the word has never appeared in its training data. ever appeared in its training data. As a result, the model will choose to ignore it, not translate it or translate it incorrectly. In the future, LLMs need to increase their training data to cover a wider range of vocabulary, as well as dynamically expand the model's vocabulary by using external resources such as vocabularies or online data sources; to reduce this phenomenon and to improve the translation ability of LLMs."
    },
    {
      "title": "5 Related Work",
      "text": "In the field of large language model translation capability evaluation, there have been a large number of related studies devoted to exploring the translation performance of different models on multiple language pairs and text types.Bang et al. (2023) [26]and Hendy et al. (2023)[27] evaluated ChatGPT on 13 and 18 languages, respectively; Zhu et al. (2023)[28] evaluated the translation capability of four popular large language models, XGLM, BLOOMZ, OPT, and ChatGPT, on 102 languages, on 202 directions. 202 directions The multilingual translation capabilities of four popular large language models, XGLM, BLOOMZ, OPT and ChatGPT, were evaluated. In this paper, 20 representative languages are selected to evaluate nine current mainstream large-scale language models. The evaluation focuses on Chinese and English, but covers a wide range of other languages as well. Multilingual translations are performed with these models and the results are compared with a state-of-the-art translation engine (Google Translate) in order to comprehensively evaluate the translation capabilities and performance of these large language models. The aim of the study is to determine the performance of these models in different linguistic contexts, as well as their usability and accuracy in real translation tasks. Despite the significant progress made by large-scale language models on translation tasks, a number of challenges remain. For example, there is still room for improvement in the model's ability to handle low-resource languages and diverse texts. Future research directions include improving the evaluation metrics, optimising the model structure and enhancing the training methods to further improve the performance and generalisation of large language models on translation tasks. These improvements will not only help to enhance the model's translation accuracy, but also enhance its adaptability in dealing with complex and diverse language environments."
    },
    {
      "title": "6 Conclusion",
      "text": "In this paper, a dataset called Euas-20 is constructed and nine popular large language models (LLMs) are evaluated using this dataset. The evaluation process focuses on Chinese and English, and compares the translation performance of these models and their translation capabilities on various languages through translations in 20 languages. Also, the paper analyses the impact of pre-trainingdata and corpus on the translation performance of large language models.The translation results of the LLMs were analysed in various ways. The results show that although the translation performance of LLMs is improving, with Llama-3 performing particularly well, far exceeding other models, the translation ability of these models on different languages is still very unbalanced. Especially when dealing with low-resource languages, they still face great challenges. In addition, a high-quality and diverse corpus plays a significant role in improving the translation performance of large language models. Future research needs to continue to explore how to enhance the translation capabilities of LLMs on more languages to achieve more balanced and comprehensive translation performance. This includes improving the model structure, optimising training methods, and extending and enhancing the quality and diversity of the corpus."
    },
    {
      "title": "References",
      "text": "* [1] Jiao, W., Wang, W., Huang, J.t., Wang, X., Tu, Z.: Is chatgpt a good translator? a preliminary study. arXiv preprint arXiv:2301.08745 1(10) (2023) * [2] Robinson, N.R., Ogayo, P., Mortensen, D.R., Neubig, G.: Chatgpt mt: Competitive for high-(but not low-) resource languages. arXiv preprint arXiv:2309.07423 (2023) * [3] Moslem, Y., Haque, R., Kelleher, J.D., Way, A.: Adaptive machine translation with large language models. arXiv preprint arXiv:2301.13294 (2023) * [4] Zhu, S., Cui, M., Xiong, D.: Towards robust in-context learning for machine translation with large language models. In: Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). pp. 16619-16629 (2024) * [5] Zhu, S., Mi, C., Li, T., Yang, Y., Xu, C.: Unsupervised parallel sentences of machine translation for asian language pairs. ACM Transactions on Asian and Low-Resource Language Information Processing 22(3), 1-14 (2023) * [6] Zhu, S., Gu, S., Li, S., Xu, L., Xiong, D.: Mining parallel sentences from internet with multi-view knowledge distillation for low-resource language pairs. Knowledge and Information Systems 66(1), 187-209 (2024) * [7] Zhu, S., Pan, L., Xiong, D.: FEDS-ICL: enhancing translation ability and efficiency of large language model by optimizing demonstration selection. Inf. Process. Manag. 61(5), 103825 (2024), [https://doi.org/10.1016/j.jpm.2024.103825](https://doi.org/10.1016/j.jpm.2024.103825) * [8] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) * [9] Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, E., Heslow, D., Launay, J., Malartic, Q., et al.: Falcon-40b: an open large language model with state-of-the-art performance. Findings of the Association for Computational Linguistics: ACL 2023, 10755-10773 (2023) * [10] Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al.: Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems 36 (2024) * [11] Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., Casas, D.d.l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al.: Mistral 7b. arXiv preprint arXiv:2310.06825 (2023)* [12] Le Scao, T., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A.S., Yvon, F., Galle, M., et al.: Bloom: A 176b-parameter open-access multilingual language model (2023) * [13] Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Riviere, M., Kale, M.S., Love, J., et al.: Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295 (2024) * [14] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017) * [15] Cui, M., Du, J., Zhu, S., Xiong, D.: Efficiently exploring large language models for document-level machine translation with in-context learning. arXiv preprint arXiv:2406.07081 (2024) * [16] Zhu, S., Xiong, D.: Tjunlp: System description for the wmt23 literary task in chinese to english translation direction. In: Proceedings of the Eighth Conference on Machine Translation. pp. 307-311 (2023) * [17] Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y.: Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014) * [18] Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014) * [19] Zens, R., Och, F.J., Ney, H.: Phrase-based statistical machine translation. In: KI 2002: Advances in Artificial Intelligence: 25th Annual German Conference on AI, KI 2002 Aachen, Germany, September 16-20, 2002 Proceedings 25. pp. 18-32. Springer (2002) * [20] Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., et al.: Moses: Open source toolkit for statistical machine translation. In: Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions. pp. 177-180. Association for Computational Linguistics (2007) * [21] Koehn, P., Knowles, R.: Six challenges for neural machine translation. arXiv preprint arXiv:1706.03872 (2017) * [22] Goyal, N., Gao, C., Chaudhary, V., Chen, P.J., Wenzek, G., Ju, D., Krishnan, S., Ranzato, M., Guzman, F., Fan, A.: The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics 10, 522-538 (2022) * [23] Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311-318 (2002) * [24] Rei, R., Stewart, C., Farinha, A.C., Lavie, A.: Comet: A neural framework for mt evaluation (2020), [https://arxiv.org/abs/2009.09025](https://arxiv.org/abs/2009.09025) * [25] Zhou, C., Neubig, G., Gu, J., Diab, M., Guzman, P., Zettlemoyer, L., Ghazvininejad, M.: Detecting hallucinated content in conditional neural sequence generation (2021), [https://arxiv.org/abs/2011.02593](https://arxiv.org/abs/2011.02593) * [26] Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., et al.: A multitask, multilingual, multimodal evaluation of chat-gpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023 (2023) * [27] Hendy, A., Abdelrehim, M., Sharaf, A., Raunak, V., Gabr, M., Matsushita, H., Kim, Y.J., Afify, M., Awadalla, H.H.: How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210 (2023)* [28] Zhu, W., Liu, H., Dong, Q., Xu, J., Huang, S., Kong, L., Chen, J., Li, L.: Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv:2304.04675 (2023)"
    }
  ]
}