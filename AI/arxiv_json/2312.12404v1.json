{
  "title": "Towards Automatic Support of Software Model Evolution with Large Language Models",
  "authors": [
    "Christof Tinnes",
    "Thomas Fuchss",
    "Hka Karlsruhe",
    "Germany"
  ],
  "abstract": "\n Modeling structure and behavior of software systems plays a crucial role, in various areas of software engineering. As with other software engineering artifacts, software models are subject to evolution. Supporting modelers in evolving models by model completion facilities and providing high-level edit operations such as frequently occurring editing patterns is still an open problem. Recently, large language models (i.e., generative neural networks) have garnered significant attention in various research areas, including software engineering. In this paper, we explore the potential of large language models in supporting the evolution of software models in software engineering. We propose an approach that utilizes large language models for model completion and discovering editing patterns in model histories of software systems. Through controlled experiments using simulated model repositories, we conduct an evaluation of the potential of large language models for these two tasks. We have found that large language models are indeed a promising technology for supporting software model evolution, and that it is worth investigating further in the area of software model evolution. \n",
  "references": [
    {
      "id": null,
      "title": "Towards Automatic Support of Software Model Evolution with Large Language Models",
      "authors": [
        "Christof Tinnes",
        "Thomas Fuchss",
        "Hka Karlsruhe",
        "Germany"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "VMTL: A language for end-user model transformation",
      "authors": [
        "Harald Vlad Acreţoaie",
        "Daniel Störrle",
        "Strüber"
      ],
      "year": "2018",
      "venue": "Software & Systems Modeling",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Recommender systems in model-driven engineering. Software and System Modelling",
      "authors": [
        "Lissette Almonte",
        "Esther Guerra",
        "Iván Cantador",
        "Juan De",
        "Lara"
      ],
      "year": "2022",
      "venue": "Recommender systems in model-driven engineering. Software and System Modelling",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Learning minimal and maximal rules from observations of graph transformations",
      "authors": [
        "Abdullah M Alshanqiti",
        "Reiko Heckel",
        "Tamim Ahmed Khan"
      ],
      "year": "2012",
      "venue": "Electronic Communication of the European Association of Software Science and Technology",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Henshin: Advanced concepts and tools for in-place EMF model transformations",
      "authors": [
        "Thorsten Arendt",
        "Enrico Biermann",
        "Stefan Jurack",
        "Christian Krause",
        "Gabriele Taentzer"
      ],
      "year": "2010",
      "venue": "Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Specifying model transformations by direct manipulation using concrete visual notations and interactive recommendations",
      "authors": [
        "Iman Avazpour",
        "John Grundy",
        "Lars Grunske"
      ],
      "year": "2015",
      "venue": "Journal of Visual Languages and Computing",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Molgpt: Molecular generation using a transformer-decoder model",
      "authors": [
        "Viraj Bagal",
        "Rishal Aggarwal",
        "Deva Vinod",
        "Priyakumar"
      ],
      "year": "2021",
      "venue": "Journal of Chemical Information and Modeling",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Advanced model transformation language constructs in the VIATRA2 framework",
      "authors": [
        "András Balogh",
        "Dániel Varró"
      ],
      "year": "2006",
      "venue": "Proceedings of the ACM Symposium on Applied Computing",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "A neural probabilistic language model",
      "authors": [
        "Yoshua Bengio",
        "Réjean Ducharme",
        "Pascal Vincent"
      ],
      "year": "2000",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Formal foundation of consistent EMF model transformations by algebraic graph transformation",
      "authors": [
        "Enrico Biermann",
        "Claudia Ermel",
        "Gabriele Taentzer"
      ],
      "year": "2012",
      "venue": "Software and Systems Modeling",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "An example is worth a thousend words: Composite operation modeling by-example",
      "authors": [
        "Petra Brosch",
        "Philip Langer",
        "Martina Seidl",
        "Konrad Wieland",
        "Manuel Wimmer",
        "Gerti Kappel",
        "Werner Retschitzegger",
        "Wieland Schwinger"
      ],
      "year": "2009",
      "venue": "Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Metamodel-based test generation for model transformations: an algorithm and a tool",
      "authors": [
        "Erwan Brottier",
        "Franck Fleurey",
        "Jim Steel",
        "Benoit Baudry",
        "Yves Le Traon"
      ],
      "year": "2006",
      "venue": "Proceedings of International Sympoisum on Software Reliability Engineering (ISSRE)",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Model differences in the eclipse modeling framework. UPGRADE",
      "authors": [
        "Cédric Brun",
        "Alfonso Pierantonio"
      ],
      "year": "2008",
      "venue": "The European Journal for the Informatics Professional",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "A generic LSTM neural network architecture to infer heterogeneous model transformations",
      "authors": [
        "Loli Burgueño",
        "Jordi Cabot",
        "Shuai Li",
        "Sébastien Gérard"
      ],
      "year": "2022",
      "venue": "Software and Systems Modelling",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "An nlp-based architecture for the autocompletion of partial domain models",
      "authors": [
        "Loli Burgueño",
        "Robert Clarisó",
        "Sébastien Gérard",
        "Shuai Li",
        "Jordi Cabot"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Advanced Information Systems Engineering",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "An lstm-based neural network architecture for model transformations",
      "authors": [
        "Loli Burgueño",
        "Jordi Cabot",
        "Sébastien Gérard"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Developing Mobile Applications on the Android Platform",
      "authors": [
        "Guiran Chang",
        "Chunguang Tan",
        "Guanhua Li",
        "Chuan Zhu"
      ],
      "year": "2010",
      "venue": "Developing Mobile Applications on the Android Platform",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Evaluating large language models trained on code",
      "authors": [
        "Mark Chen",
        "Jerry Tworek",
        "Heewoo Jun",
        "Qiming Yuan",
        "Henrique Ponde De Oliveira Pinto",
        "Jared Kaplan",
        "Harri Edwards",
        "Yuri Burda",
        "Nicholas Joseph",
        "Greg Brockman"
      ],
      "year": "2021",
      "venue": "Evaluating large language models trained on code",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Recommendation and weaving of reusable mashup model patterns for assisted development",
      "authors": [
        "Roy Soudip",
        "Florian Chowdhury",
        "Fabio Daniel",
        "Casati"
      ],
      "year": "2014",
      "venue": "ACM Transactions on Internet Technology",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "An empirical study on the usage of transformer models for code completion",
      "authors": [
        "Matteo Ciniselli",
        "Nathan Cooper",
        "Luca Pascarella",
        "Antonio Mastropaolo",
        "Emad Aghajani",
        "Denys Poshyvanyk",
        "Massimiliano Di Penta",
        "Gabriele Bavota"
      ],
      "year": "2022",
      "venue": "Transactions on Software Engineering",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Mining graph data",
      "authors": [
        "Diane J Cook",
        "Lawrence B Holder"
      ],
      "year": "2006",
      "venue": "Mining graph data",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Feature-based survey of model transformation approaches",
      "authors": [
        "Krzysztof Czarnecki",
        "Simon Helsen"
      ],
      "year": "2006",
      "venue": "IBM systems journal",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Mastering simulink",
      "authors": [
        "B James",
        "Thomas L Dabney",
        "Harman"
      ],
      "year": "2004",
      "venue": "Mastering simulink",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Fundamental theory for typed attributed graph transformation",
      "authors": [
        "Hartmut Ehrig",
        "Ulrike Prange",
        "Gabriele Taentzer"
      ],
      "year": "2004",
      "venue": "International Conference on Graph Transformation (ICGT)",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Generation of visual editors as Eclipse plug-ins",
      "authors": [
        "Karsten Ehrig",
        "Claudia Ermel",
        "Stefan Hänsgen",
        "Gabriele Taentzer"
      ],
      "year": "2005",
      "venue": "Proceedings of the International Conference on Automated Software Engineering (ASE)",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Design patterns: elements of reusable object-oriented software",
      "authors": [
        "Erich Gamma",
        "Ralph Johnson",
        "Richard Helm",
        "Ralph E Johnson",
        "John Vlissides"
      ],
      "year": "1995",
      "venue": "Design patterns: elements of reusable object-oriented software",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Can language models capture graph semantics? from graphs to language model and vice-versa",
      "authors": [
        "Tarun Garg",
        "Kaushik Roy",
        "A Sheth"
      ],
      "year": "2022",
      "venue": "Can language models capture graph semantics? from graphs to language model and vice-versa",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Program synthesis",
      "authors": [
        "Sumit Gulwani",
        "Oleksandr Polozov",
        "Rishabh Singh"
      ],
      "year": "2017",
      "venue": "Foundations and Trends® in Programming Languages",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Learning distributed representations of concepts",
      "authors": [
        "Geoffrey E Hinton"
      ],
      "year": "1986",
      "venue": "Proceedings of the Annual Conference of the Cognitive Science Society",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Systematically deriving domain-specific transformation languages",
      "authors": [
        "Katrin Hölldobler",
        "Bernhard Rumpe",
        "Ingo Weisemöller"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "A short tutorial on the weisfeiler-lehman test and its variants",
      "authors": [
        "Teresa Ningyuan",
        "Soledad Huang",
        "Villar"
      ],
      "year": "2021",
      "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Programmable controllers -part 3: Programming languages",
      "authors": [
        "Iec"
      ],
      "year": "2014",
      "venue": "Programmable controllers -part 3: Programming languages",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Interpolated estimation of markov source parameters from sparse data",
      "authors": [
        "F Jelinek",
        "R L Mercer"
      ],
      "year": "1980",
      "venue": "Proceedings of the Workshop on Pattern Recognition in Practice",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Model transformation by-example: A survey of the first wave",
      "authors": [
        "Gerti Kappel",
        "Philip Langer",
        "Werner Retschitzegger",
        "Wieland Schwinger",
        "Manuel Wimmer"
      ],
      "year": "2012",
      "venue": "Conceptual Modelling and Its Theoretical Foundations -Essays Dedicated to Bernhard Thalheim on the Occasion of His 60th Birthday",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Calculation and Propagation of Model Changes based on User-Level Edit Operations: A Foundation for Version and Variant Management in Model-Driven Engineering",
      "authors": [
        "Timo Kehrer"
      ],
      "year": "2015",
      "venue": "Calculation and Propagation of Model Changes based on User-Level Edit Operations: A Foundation for Version and Variant Management in Model-Driven Engineering",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Automatic inference of rule-based specifications of complex in-place model transformations",
      "authors": [
        "Timo Kehrer",
        "Abdullah M Alshanqiti",
        "Reiko Heckel"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Conference on Model Transformations (ICMT)",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Consistency-preserving edit scripts in model versioning",
      "authors": [
        "Timo Kehrer",
        "Udo Kelter",
        "Gabriele Taentzer"
      ],
      "year": "2013",
      "venue": "Proceedings of the International Conference on Automated Software Engineering (ASE)",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Generating edit operations for profiled uml models",
      "authors": [
        "Timo Kehrer",
        "Michaela Rindt"
      ],
      "year": "2013",
      "venue": "MoDELS Workshop on Models and Evolution (ME@MoDELS)",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Automatically deriving the specification of model editing operations from meta-models",
      "authors": [
        "Timo Kehrer",
        "Gabriele Taentzer",
        "Michaela Rindt",
        "Udo Kelter"
      ],
      "year": "2016",
      "venue": "Proceedings of the International Conference on Model Transformations (ICMT)",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Subdue: Compression-based frequent pattern discovery in graph data",
      "authors": [
        "S Nikhil",
        "Lawrence B Ketkar",
        "Diane J Holder",
        "Cook"
      ],
      "year": "2005",
      "venue": "Proceedings of the 1st International Workshop on Open Source Data Mining: Frequent Pattern Mining Implementations",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "The epsilon book",
      "authors": [
        "Dimitris Kolovos",
        "Louis Rose",
        "Antonio García-Domínguez",
        "Richard Paige"
      ],
      "year": "2012",
      "venue": "The epsilon book",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "The 4+ 1 view model of architecture",
      "authors": [
        "Philippe B Kruchten"
      ],
      "year": "1995",
      "venue": "IEEE software",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Applying codebert for automated program repair of java simple bugs",
      "authors": [
        "Ehsan Mashhadi",
        "Hadi Hemmati"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Mining Software Repositories (MSR)",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Generating correctness-preserving editing operations for diagram editors",
      "authors": [
        "Steffen Mazanek",
        "Mark Minas"
      ],
      "year": "2009",
      "venue": "Electronic Communication of the European Association of Software Science and Technology",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Practical graph isomorphism, ii",
      "authors": [
        "Brendan D Mckay",
        "Adolfo Piperno"
      ],
      "year": "2014",
      "venue": "Journal of Symbolic Computation",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "A taxonomy of model transformation",
      "authors": [
        "Tom Mens",
        "Pieter Van Gorp"
      ],
      "year": "2006",
      "venue": "Electronic Notes in Theoretical Computer Science",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Recommending model refactoring rules from refactoring examples",
      "authors": [
        "Chihab Mokaddem",
        "Houari Sahraoui",
        "Eugene Syriani"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "ReVision: A tool for history-based model repair recommendations",
      "authors": [
        "Manuel Ohrndorf",
        "Christopher Pietsch",
        "Udo Kelter",
        "Timo Kehrer"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Software Engineering (ICSE): Companion Proceedings",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Meta Object Facility (MOF) 2.0 Query/View/Transformation Specification, Version 1.3",
      "authors": [
        "Omg"
      ],
      "year": "2016",
      "venue": "Meta Object Facility (MOF) 2.0 Query/View/Transformation Specification, Version 1.3",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Unified modeling language (UML) version 2.5.1. Standard, Object Management Group",
      "authors": [
        "Omg"
      ],
      "year": "2017",
      "venue": "Unified modeling language (UML) version 2.5.1. Standard, Object Management Group",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Omg sysml v. 1.6. Standard, Object Management Group",
      "authors": [
        "Omg"
      ],
      "year": "2019",
      "venue": "Omg sysml v. 1.6. Standard, Object Management Group",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "Personal Knowledge: Towards a Post Critical Philosophy",
      "authors": [
        "Michael Polanyi"
      ],
      "year": "1958",
      "venue": "Personal Knowledge: Towards a Post Critical Philosophy",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Flashmeta: A framework for inductive program synthesis",
      "authors": [
        "Oleksandr Polozov",
        "Sumit Gulwani"
      ],
      "year": "2015",
      "venue": "SIGPLAN Not",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "Model-driven engineering: A survey supported by the unified conceptual model",
      "authors": [
        "Alberto Rodrigues",
        "Da Silva"
      ],
      "year": "2015",
      "venue": "Computer Languages, Systems and Structures",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "A systematic mapping study of source code representation for deep learning in software engineering",
      "authors": [
        "Hazem Peter Samoaa",
        "Firas Bayram",
        "Pasquale Salza",
        "Philipp Leitner"
      ],
      "year": "2022",
      "venue": "IET Software",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Constructing difference tools for models using the SiDiff framework",
      "authors": [
        "Maik Schmidt",
        "Tilman Gloetzner"
      ],
      "year": "2008",
      "venue": "Proceedings of the International Conference on Software Engineering (ICSE): Companion Proceedings",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Setting goals and choosing metrics for recommender system evaluations",
      "authors": [
        "Gunnar Schröder",
        "Maik Thiele",
        "Wolfgang Lehner"
      ],
      "year": "2011",
      "venue": "Proceedings of the Conference on Recommender Systems (RecSys)",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Towards domain-specific model editors with automatic model completion",
      "authors": [
        "Sagar Sen",
        "Benoit Baudry",
        "Hans Vangheluwe"
      ],
      "year": "2010",
      "venue": "Simulation",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Slgpt: using transfer learning to directly generate simulink model files and find bugs in the simulink toolchain",
      "authors": [
        "Lal Sohil",
        "Christoph Shrestha",
        "Csallner"
      ],
      "year": "2021",
      "venue": "Proceedings of the Conference on Evaluation and Assessment in Software Engineering (EASE)",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Views on internal and external validity in empirical software engineering",
      "authors": [
        "Janet Siegmund",
        "Norbert Siegmund",
        "Sven Apel"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Conference on Software Engineering (ICSE)",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Choose your programming copilot: A comparison of the program synthesis performance of github copilot and genetic programming",
      "authors": [
        "Dominik Sobania",
        "Martin Briesch",
        "Franz Rothlauf"
      ],
      "year": "2021",
      "venue": "Choose your programming copilot: A comparison of the program synthesis performance of github copilot and genetic programming",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "Generic model assist",
      "authors": [
        "Friedrich Steimann",
        "Bastian Ulke"
      ],
      "year": "2013",
      "venue": "Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Towards a cognizant virtual software modeling assistant using model clones",
      "authors": [
        "Matthew Stephan"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Conference on Software Engineering (ICSE) (NIER)",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "A survey of model comparison approaches and applications",
      "authors": [
        "Matthew Stephan",
        "James R Cordy"
      ],
      "year": "2013",
      "venue": "Proceedings of the International Conference on Model-Driven Engineering and Software Development (MODELSWARD)",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "MT-Scribe: An end-user approach to automate software model evolution",
      "authors": [
        "Yu Sun",
        "Jeff Gray",
        "Jules White"
      ],
      "year": "2011",
      "venue": "Proceedings of the International Conference on Engineering (ICSE)",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Interpreting language models through knowledge graph extraction",
      "authors": [
        "Vinitra Swamy",
        "Angelika Romanou",
        "Martin Jaggi"
      ],
      "year": "2021",
      "venue": "Interpreting language models through knowledge graph extraction",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "Change-preserving model repair",
      "authors": [
        "Gabriele Taentzer",
        "Manuel Ohrndorf",
        "Yngve Lamo",
        "Adrian Rutle"
      ],
      "year": "2017",
      "venue": "Fundamental Approaches to Software Engineering",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "Learning domain-specific edit operations from model repositories with frequent subgraph mining",
      "authors": [
        "Christof Tinnes",
        "Timo Kehrer",
        "Joblin Mitchell",
        "Uwe Hohenstein",
        "Andreas Biesdorf",
        "Sven Apel"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Automated Software Engineering (ASE)",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "Sometimes you have to treat the symptoms: tackling model drift in an industrial clone-and-own software product line",
      "authors": [
        "Christof Tinnes",
        "Wolfgang Rössler",
        "Uwe Hohenstein",
        "Torsten Kühn",
        "Andreas Biesdorf",
        "Sven Apel"
      ],
      "year": "2022",
      "venue": "Proceedings of the Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE)",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "Model-driven software evolution: A research agenda",
      "authors": [
        "Arie Van Deursen",
        "Eelco Visser",
        "Jos Warmer"
      ],
      "year": "2007",
      "venue": "Model-driven software evolution: A research agenda",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "Model transformation by example",
      "authors": [
        "Dániel Varró"
      ],
      "year": "2006",
      "venue": "Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "What do they capture? -A structural analysis of pre-trained language models for source code",
      "authors": [
        "Wei Yao Wan",
        "Hongyu Zhao",
        "Yulei Zhang",
        "Guandong Sui",
        "Hai Xu",
        "Jin"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Software Engineering (ICSE)",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "Recommending metamodel concepts during modeling activities with pre-trained language models",
      "authors": [
        "Martin Weyssow",
        "Houari Sahraoui",
        "Eugene Syriani"
      ],
      "year": "2022",
      "venue": "Software and Systems Modeling",
      "doi": ""
    },
    {
      "id": "b73",
      "title": "A systematic evaluation of large language models of code",
      "authors": [
        "F Frank",
        "Uri Xu",
        "Graham Alon",
        "Vincent Neubig",
        "Josua Hellendoorn"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Symposium on Machine Programming",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "gSpan: graph-based substructure pattern mining",
      "authors": [
        "Xifeng Yan",
        "Jiawei Han"
      ],
      "year": "2002",
      "venue": "gSpan: graph-based substructure pattern mining",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "Specifying and detecting meaningful changes in programs",
      "authors": [
        "Yijun Yu",
        "Thein Than Tun",
        "Bashar Nuseibeh"
      ],
      "year": "2011",
      "venue": "Proceedings of the International Conference on Automated Software Engineering (ASE)",
      "doi": ""
    },
    {
      "id": "b76",
      "title": "Natural language processing for requirements engineering: a systematic mapping study",
      "authors": [
        "Liping Zhao",
        "Waad Alhoshan",
        "Alessio Ferrari",
        "J Keletso",
        "Muideen A Letsholo",
        "Erol-Valeriu Ajagbe",
        "Riza T Chioasca",
        "Batista-Navarro"
      ],
      "year": "2021",
      "venue": "ACM Computing Surveys (CSUR)",
      "doi": ""
    },
    {
      "id": "b77",
      "title": "Transmol: repurposing a language model for molecular generation",
      "authors": [
        "Rustam Zhumagambetov",
        "Ferdinand Molnár",
        "A Vsevolod",
        "Siamac Peshkov",
        "Fazli"
      ],
      "year": "2021",
      "venue": "RSC advances",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Towards Automatic Support Of Software Model Evolution With Large Language Models",
      "text": "Christof Tinns Thomas Fuchs HKA Karlsruhe Centre for Research in Mathematics and Statistics, University of California, Berkeley, CA 94706"
    },
    {
      "title": "Abstract",
      "text": "Modeling structure and behavior of software systems plays a crucial role, in various areas of software engineering. As with other software engineering artifacts, software models are subject to evolution. Supporting modelers in evolving models by model completion facilities and providing high-level edit operations such as frequently occurring editing patterns is still an open problem. Recently, large language models (i.e., generative neural networks) have garnered significant attention in various research areas, including software engineering. In this paper, we explore the potential of large language models in supporting the evolution of software models in software engineering. We propose an approach that utilizes large language models for model completion and discovering editing patterns in model histories of software systems. Through controlled experiments using simulated model repositories, we conduct an evaluation of the potential of large language models for these two tasks. We have found that large language models are indeed a promising technology for supporting software model evolution, and that it is worth investigating further in the area of software model evolution."
    },
    {
      "title": "1 Introduction",
      "text": "Models play an important role in modern software and system development [53], software documentation [41, 49], system architecture [50], simulation [22], industry automation [31], and user interface layout design [16]. For disambiguation, we refer to these models as _software models_. All artifacts in software and system development are typically subject to evolution, which also applies to software models [69]: Software models have to evolve because of changing requirements, but they can also be subject to bugfixes or refactorings. For example, a shopfloor automation system has to adapt to changed production processes (i.e., changing requirements), or the handling of an exceptional situation has to be corrected (i.e., a bugfix). From the perspective of a user of a modelling tool, we can understand the evolution of software models as a sequence of _edit operations_: To change or evolve the model, the tool user executes edit operations (e.g., using mouse clicks and keyboard strokes) provided by the tool. For example, the user could modify the velocity of a shopfloor conveyor belt, or add another sensor input to the warning mechanism of a large drive. Edit operations can be specified by in-place model transformations [21, 45], which in turn can be formalized as graph transformations [9, 23, 24]. For the evolution of software models, modelling tools typically provide an initial set of edit operations (e.g., adding an attribute to an UML class). Nevertheless, since the usage of a (domain-specific) language is also subject to evolution and since (project-specific) usage patterns might emerge, this initial set of edit operations can rarely be exhaustive. For example, in object-oriented design, design patterns [25] are widely used but are not part of the UML language specification [49]. Likewise, in electronic circuit design, patterns such as a low-pass filter will often be used and might not be part of the tool's standard component library (e.g., consisting of capacitors, resistors, diodes). Consequently, approaches for the specification of new user-specific model transformations--and edit operations, in particular--have been developed [4, 7, 40, 48]. Using a specification language for defining edit operations poses two challenges: First, specifying new edit operations requires the knowledge of the specification language and of the concrete domain-specific language (or metamodel) for which the edit operations have to be specified. Second, domain-specific edit operations are often not explicitly known, that is, they are a form of tacit knowledge [51]. Externalizing the knowledge can therefore be hard or even impossible for domain experts. While it might even be necessary to define project-specific edit operations [67, 68] the overhead for their manual specification is often not economically feasible for many projects and tool providers [33, 35, 46]. Therefore, researchers have sought to support or even automate the specification of model transformations and edit operations, in particular. These approaches range from a visual or concrete syntax-based support [1, 5, 29], over supervised approaches that turn single examples [10, 33, 64] or a set of examples [35, 46] into model transformation specifications (so-called model transformation by-example (MTBE)), to unsupervised approaches such as the generation of model transformations out of a given meta-model [37, 38, 43]. More recently, mining model transformations from the software model history using frequent subgraph mining has been proposed [67]. Mining approaches are especially appealing since they do not require any manual specification, no hand-crafting of examples (as in MTBE), and they are not limited to simple well-formedness rules that can be derived out-of-the metamodel. Unfortunately, frequent subgraph mining and related techniques are not scalable; in particular large differences between two model revisions might lead to extremely long execution times or large memory consumption [67]. Furthermore, mining approaches lack abstraction capabilities. For example, two edit operations that are identical but for one type--even with common parent type in the metamodel--are seen as two different edit operations and could even be missed by the approach. From the perspective of software model evolution it would also be desirable to have context-dependent _completions_, instead of learning a fixed set of edit operations. That is, the user would benefit from the recommendation of a _completion operation_ that complements the edit operations that have been applied to the model. Motivated by recent breakthroughs of generative neural network architectures for various tasks such as code generation and completion [17], program repair [42], and even to the generation of graph like structures--most notably molecular generation [6]--we investigate feasibility of generative language models for edit pattern mining and software model completion. The rationale is that, if we fine-tune the language model to generate completions for software models, it should have learned the underlying patterns of the (modelling) language in question. We propose and study an approach to learn edit and completion patterns from the software model history and to extract the corresponding operations from a generative language model. Specifically, we define and use an encoding for serializations of model difference graphs that we will use to fine-tune the language models. Using a synthetic dataset with control over the actually performed edit operations, we then evaluate whether we can generate serializations of edit operations using the fine-tuned language models and whether we can complete software model changes correctly. We find that, regarding completion, the approach performs surprisingly well and generates the correct completion in 87.60% of our samples. The approach is also able to generate edit operations, although, in some cases, not all applied edit operations were generated. In a summary, we make the following contributions: * We formalize the concepts of edit operations, completion operations, and edit patterns such that they are suitable for the mining perspective, and show that this formalization is consistent with earlier constructions. * We propose an approach to use large language models for mining edit patterns and software model completion. * We evaluate the approach in a controlled experiment. We find that the approach can provide correct software model completions and also generate edit operations that have been applied in the simulation of the software model repositories."
    },
    {
      "title": "2. Foundations And State-Of-The-Art",
      "text": ""
    },
    {
      "title": "Software Models And Edit Operations",
      "text": "In this section we will provide background about software models and we present a new formalization of edit operations that is well suited for studying mining approaches. We will then see that this formalization is compatible with earlier formalizations. In model-driven engineering the language for a software model (i.e., its abstract syntax and static semantics) is typically defined by a metamodel \\(\\mathcal{TM}\\). A model is an instance of its metamodel. We denote by \\(\\mathcal{M}\\) the set of all valid models (according to some metamodel). This can be formalized using typed attributed graphs [9; 23]. **Definition 2.1** (Abstract Syntax Graph).: An _abstract syntax graph_\\(G_{m}\\) of a model \\(m\\in\\mathcal{M}\\) is a typed attributed graph, typed over an attributed type graph \\(TG\\) that is given by the metamodel \\(\\mathcal{TM}\\). The idea of typed graphs is to define a graph homomorphism (i.e., a function from the typed graph \\(G\\) to the type graph \\(TG\\)). Details of this formalization are given in the work by Biermann et al. [9]. The abstract syntax graph of a model (together with the type graph) contains all information that a model contains. The _concrete_ (_visual_) _syntax_ of a model is only an external representation on the model that provides visual cues for the end-user of a model. It is meant to allow for easier handling and understanding of the models and therefore sometimes even hides information of the model. In the present work, we are concerned with model repositories. We assume that the modelling tool already takes care of checking the correct typing of the models, and we therefore expect that the models are correctly typed. We therefore work with a simplified graph representation of the models in which the abstract syntax graph is just a _labeled directed graph_ with node labels equal to the node type names and edge labels equal to the edge type names of the abstract syntax graph from Definition 2.1. **Definition 2.2** (Labeled Directed Graph).: Given a label alphabet \\(L\\), a _labeled directed graph_\\(G\\) is a tuple \\((V,E,\\lambda)\\), where \\(V\\) is a finite set of nodes, \\(E\\) is a subset of \\(V\\times V\\), called the edge set, and \\(\\lambda:V\\cup E\\to L\\) is the labeling function, which assigns a label to nodes and edges. We refer to the set of all directed labeled graphs by \\(\\mathcal{G}\\). In the following, we will use the labeled graphs corresponding to the model instead of the graphs from Definition 2.1. Rather than working directly on the abstract syntax graph of the models, we will mostly be working with model differences. **Definition 2.3** (Structural Model Difference).: A _structural model difference_\\(\\Delta_{mn}\\) of a pair of model versions \\(m\\) and \\(n\\) is obtained by matching corresponding model elements in the model graphs \\(G_{m}\\) and \\(G_{n}\\) (using a model matcher [63], e.g., EMFComparc [12] or SiDiff [55]). Then, there are added elements (the ones present in \\(G_{n}\\) but not in \\(G_{m}\\)), removed element (the ones present in \\(G_{m}\\) but not in \\(G_{n}\\)), and preserved elements which are present in \\(G_{m}\\) and \\(G_{n}\\). We assume here that this matching is deterministic, that is, given two models \\(m,n\\in\\mathcal{M}\\), we get a unique structural model difference \\(\\Delta_{mn}\\). The structural model difference can be represented as a _difference graph_[47]\\(G_{\\Delta mn}\\), where the nodes carry some prefix label \"Add\", \"Preserve\", or \"Remove\", and matching elements (i.e., the preserved ones) from \\(G_{m}\\) and \\(G_{n}\\) are unified with each other (i.e., the will be present only once). We define a _simple change graph_ to be the smallest subgraph comprising all changes in the difference graph \\(G_{\\Delta_{mn}}\\). **Definition 2.4** (Simple Change Graph).: Given a difference graph \\(G_{\\Delta_{mn}}\\), a _simple change graph_\\(SCG_{\\Delta_{mn}}\\subseteq G_{\\Delta_{mn}}\\) is derived from \\(G_{\\Delta_{mn}}\\) by first selecting all the elements in \\(G_{\\Delta_{mn}}\\) representing a change (i.e., added, removed nodes and edges) and, second, adding preserved nodes that are adjacent to a changed edge. The simple change graph is the smallest subgraph of \\(G_{\\Delta_{mn}}\\) containing all changed nodes and edges. **Definition 2.5** (Endogenous model transformation).: An _endogenous model transformation_ is a pair \\(t=(m,n)\\in\\mathcal{M}\\times\\mathcal{M}\\). We call \\(m\\) the _source model_ and \\(n\\) the _target model_ of the transformation and \\(\\mathcal{T}\\stackrel{{\\text{def}}}{{=}}\\mathcal{M}\\times \\mathcal{M}\\) the space of endogenous model transformations. We can then also define a function \\(SCG\\colon\\mathcal{T}\\to\\mathcal{G}\\) that takes a model transformation (i.e., a pair of models) as input and returns the simple change graph for the corresponding model difference. We can use this map \\(SCG\\) to define an equivalence relation on \\(\\mathcal{T}\\) by \\[t_{1}=(m,n) \\sim t_{2}=(k,l)\\,,\\qquad\\text{if and only if}\\] \\[SCG_{\\Delta_{mm}} =SCG_{\\Delta_{kl}}.\\] It is straightforward to see that this relation indeed defines an equivalence relation (i.e., the relation is reflexive, symmetric, and transitive). We can therefore define the quotient set \\(\\mathcal{T}/\\!\\sim\\), which\\(-\\)by construction\\(-\\)is set isomorphic to the set of simple change graphs, that is, the range of the map \\(SCG\\). We can use this construction to formally define the concept of an _edit operation_. **Definition 2.6**.: An _edit operation_ is an equivalence class in the set \\(\\mathcal{E}\\stackrel{{\\text{def}}}{{=}}\\mathcal{T}/\\!\\sim\\). An edit operation is therefore a set of model transformations that have the same simple change graph. We can also interpret an edit operation as a template for a rule to transform a model \\(m\\) into a model \\(n\\): For a simple change graph, we call the subgraphs of \"Remove\" and \"Preserve\" nodes the left-hand side graph \\(L\\), and the \"Add\" and \"Preserve\" nodes the right-hand side graph \\(R\\). The embedding of \\(L\\) and \\(R\\) along the preserved nodes \\(K\\) (i.e., \\(L\\gets K\\gets R\\), where \\(\\leftarrow\\) denotes an injective homomorphism, i.e., an embedding) in the simple change graph defines how to remove the \"Remove\" nodes from \\(m\\) and glue the \"Add\" nodes along \\(K\\). Given an edit operation \\(\\varepsilon\\) and a concrete model \\(m\\), one can define a matching match \\(:L\\;\\;\\hookrightarrow\\;G_{m}\\), and perform the removal of \"Remove\" nodes and the gluing of \"Add\" nodes as defined by the simple change graph corresponding to \\(\\varepsilon\\), and then set concrete attributes. This yields the corresponding model \\(n\\) with \\((m,n)\\in\\varepsilon\\), and this way an edit operation \\(\\varepsilon\\in\\mathcal{E}\\) can be interpreted as a template for a model transformation in agreement with previous constructions [9, 34, 67]. We therefore also write \\(m\\stackrel{{\\varepsilon}}{{\\to}}n\\) to denote a concrete element (i.e., a model transformation) in the equivalence class \\(\\varepsilon\\in\\mathcal{E}\\). The set of edit operations obtained by this construction is huge\\(-\\)infinite to be more precise\\(-\\)and it contains also \"operations\" such as constructing a large model from scratch (i.e., taking \\(m\\coloneqq\\bot\\), the empty model and \\(n\\in\\mathcal{M}\\) a large model). This does not coincide with more pragmatic definitions, for example, as the set of operations provided by the modelling tool, which typically is finite. A transition from \\(m\\stackrel{{\\varepsilon}}{{\\to}}n\\) can usually also be obtained by a sequence (aka. _edit script_[36]) \\[m\\stackrel{{\\varepsilon_{1}}}{{\\to}}m_{1}\\stackrel{{ \\varepsilon_{2}}}{{\\to}}\\ldots\\stackrel{{\\varepsilon_{k-1}}}{{ \\to}}m_{k-1}\\stackrel{{\\varepsilon_{k}}}{{\\to}}n.\\] **Definition 2.7**.: A set \\(S\\subset\\mathcal{E}\\) is called a _generator_ for \\(\\mathcal{E}\\), if every model can be reached by a sequence of edit operations in \\(S\\), that is, \\[\\forall\\,m\\in\\mathcal{M}. \\exists\\,\\varepsilon_{1},\\ldots,\\varepsilon_{k_{m}}\\in S,\\,m_{1}, \\ldots,m_{k_{m}-1}\\in\\mathcal{M}\\] such that \\[\\bot\\stackrel{{\\varepsilon_{1}}}{{\\rightarrow}}m_{1} \\stackrel{{\\varepsilon_{2}}}{{\\rightarrow}}\\ldots\\stackrel{{ \\varepsilon_{k_{m}-1}}}{{\\rightarrow}}m_{k_{m}-1}\\stackrel{{ \\varepsilon_{k}}}{{\\rightarrow}}m.\\] An example for a generator \\(S\\) is the set of elementary edit operations that can be derived from a metamodel, as given in the work of Kehrer et al.[38]. This set is finite and contains rather fine-grained edit operations. Any subset \\(E\\subset\\mathcal{E}\\) can be completed to a generator by joining it with an existing generator (e.g., the set of elementary edit operations), therefore ensuring that all valid models can be reached. In this work, we explore the continuum between elementary edit operations and the set \\(\\mathcal{E}\\) of all edit operations. Our goal is to identify those higher-level edit operations whose effect is actually observable in model histories, providing empirical evidence that these are meaningful edit operations from a modeler's point of view. We call these higher-level edit operations _edit patterns_. Furthermore, we are interested in completing software models, that is, for an observed evolution \\(m\\stackrel{{\\varepsilon}}{{\\rightarrow}}n\\), we want to find a completion \\(\\gamma\\in\\mathcal{E}\\), such that \\(m\\stackrel{{\\varepsilon}}{{\\rightarrow}}n\\stackrel{{ \\gamma}}{{\\rightarrow}}c\\) is a meaningful (i.e., observable) completion. We call this \\(\\gamma\\) a _completion operation_. To give a concrete example for the difference between \\(\\mathcal{E}\\), a generator \\(S\\), and edit patterns, consider the modelling language SysML, which is used in system engineering [50]. The set \\(\\mathcal{E}\\) would then include very fine-grained elementary edit operations from \\(S\\), for example, adding a port to a component or adding a connector between two ports, but also very large edit operations, for example for setting up the entire system architecture of a manufacturing execution system. An edit pattern could be given, for example, by \"adding an interface\", that is, adding source port, target port, and connector in one step. After a port has been added to a component, a completion operation could be the completion of adding a target port and connecting the ports via a connector."
    },
    {
      "title": "Automatic Inference Of Edit Operations",
      "text": "Since simple operations on the abstract syntax graph of a software model are very fine-grained and may lead to inconsistent models, and since structural model differences are too specific, there has been a long history of attempting to infer _relevant_ edit operations (and model transformations in general) from available information (e.g., the metamodel, given examples, or a model history). **Definition 2.8** (Automatic Inference of Edit Operations).: Given a metamodel \\(\\mathcal{TM}\\) with models \\(\\mathcal{M}\\), _automatic inference of edit operations_ is a computable function \\(I\\): \\(2^{\\mathcal{M}}\\to 2^{\\mathcal{E}}\\) that, given a set of models (and their metamodel) \\(\\mathcal{M}^{\\prime}\\subset\\mathcal{M}\\), computes a set of edit operations \\(\\mathcal{E}^{\\prime}\\subset\\mathcal{E}\\). There are _supervised_ and _unsupervised_ approaches to the inference of edit operations. One branch of supervised approaches are demonstration approaches, were a tool user presents the transformation steps to an operation recorder [10, 76]. Typically, these approaches require some manual post-processing, for example, edit conditions have to be refined manually [10]. Another branch of supervised approaches include by-example approaches, with which the tool user specifies a set of examples (and sometimes also counter examples) and the tool automatically infers the model transformations [3, 35]. These approaches have been motivated by the seminal work of Varro [70], who proposed by-example learning for exogenous model transformations. Furthermore, heuristic approaches [46] applying search-based techniques to finding a set of refactorings have been proposed. Unsupervised approaches include generative approaches, deriving model transformations from the metamodel, and mining approaches. Generative approaches have been proposed in the area of model transformation testing [11]. Also, more recently, generative fuzzing approaches based on language models have been proposed that try to generate models with similar properties to real-world models [58]. Edit operations are only indirectly addressed within these generative approaches. It has been shown that a complete set of consistency preserving edit operations can also be derived from the metamodel [38; 43]. These operations capture static constraints that are already present in the metamodel and are typically very simple operations. More recent MTBE approaches also use neural networks to learn exogenous model transformations, for example, Burgueno et al. [13; 15] investigate learning exogenous model transformations from examples using long-short-term memory neural networks. However, these approaches need concrete input-output model pairs and have not been evaluated for endogenous model transformations. Recently, mining model transformations from the modelling history using graph mining approaches has been proposed [67]. An advantage of mining approaches over by-example approaches is that they do not require to handcraft examples. A disadvantage is their computational complexity and that negative application conditions and multi-object patterns (e.g., creating a variable count of elements in a model transformation) can not inferred. Anyway, a post-processing (e.g., using by-example approaches) is conceivable, and it this sense, by-example approaches and mining approaches are orthogonal."
    },
    {
      "title": "Software Model Completion",
      "text": "_(Software) model completion_ is the task of further evolving a software model based on a given (partial) model. More formally: **Definition 2.9** (Model Completion).: Given a set of model transformations \\(\\mathcal{T}\\), _model completion_ is a computable function \\(C\\colon\\mathcal{T}\\to\\mathcal{T}\\) that, given a model transformation \\(m\\xrightarrow{\\varepsilon}n\\) from a source model \\(m\\) to a (partial) target model \\(n\\), computes a model transformation \\(C(m\\xrightarrow{\\varepsilon}n)=n\\xrightarrow{\\gamma}c\\). The problem of model completion is to find a meaningful completion \\(m\\xrightarrow{\\varepsilon}n\\xrightarrow{\\gamma}c\\). We call the edit operation \\(\\gamma\\) a completion operation. To implement model completion, some authors proposed to automatically complete a partial model to a model that conforms to the meta-model and possibly other constraints(e.g., via rule based approaches or constraint solving [47; 57; 61; 66]). However, these approaches are only able to complete a partial model to a model that conforms to the metamodel or satisfies additional explicitly given constraints. Other works take existing model repositories or pattern databases into account and employ model clone detection to discover similar (parts of) existing models to make completion recommendations [18; 62]. Fine-tuned large language models would come in here very handy, because they encode the software model history in their parameters and can provide context-specific completions. The would not require hand-crafted pattern databases or expensive clone detection in the model repository and are able to generalize to unseen context information. Nevertheless, we are not aware of existing work investigating language models for software model completion. An overview of model completion approaches is given in the secondary study by Almonte et al. [2]."
    },
    {
      "title": "Language Models",
      "text": "**Definition 2.10** (Language Model).: A _language model_ is a conditional probability distribution \\(\\mathbb{P}(\\omega|c)\\) for a (sequence of) token(s) \\(\\omega\\), given a sequence of context tokens \\(c\\). [MISSING_PAGE_FAIL:7] models, meaningful edit operations could be the ones helping to understand the model evolution. There is evidence that edit operations that compress the modelling history the most are meaningful to domain experts (Krishnan et al., 2017). In this spirit, we are interested in identifying the most compressing (pattern) subgraphs in observed simple change graphs (i.e., the ones derived from to successive software model revisions). Approaches to identify frequent or compressing subgraphs (Krishnan et al., 2017; Krishnan et al., 2017; Krishnan et al., 2017) of a database of graphs often grow the patterns (also called motifs) edge-wise. That is, they start with a single node and extend it edge by edge. Given a context graph \\(G\\), two possible extensions edges \\(e\\) and \\(e^{\\prime}\\), and some metric \\(M\\) (e.g., frequency or compression), the search then favours the extension \\(e\\) with better metric \\(M(e\\cap g)>M(e^{\\prime}\\cap g)\\) (e.g., in the form of a beam search as in Subdue (Sudue, 2017)). For a frequency and a compression measure, this condition can then be reformulated to yield a probabilistic formulation: \\[M(e\\cap G)> M(e^{\\prime}\\cap G) \\tag{1}\\] \\[\\Leftrightarrow \\mathbb{P}(e\\cap G)> \\mathbb{P}(e^{\\prime}\\cap G)\\] (2) \\[\\Leftrightarrow \\frac{\\mathbb{P}(e\\cap G)}{\\mathbb{P}(G)}> \\frac{\\mathbb{P}(e^{\\prime}\\cap G)}{\\mathbb{P}(G)}\\] (3) \\[\\Leftrightarrow \\mathbb{P}(e\\mid G)> \\mathbb{P}(e^{\\prime}\\mid G) \\tag{4}\\] In (2), we have normalized over the whole dataset to yield a probabilistic formulation and in (4), we apply the definition of the conditional probability. In this formulation, the extension criteria reminds a lot on the language models from Section 2.4, with the major difference that language models are probability distributions on sequences of tokens, while the formulation above is for a probability distribution on sets of graphs. This suggests that language models can be used to generate serializations of patterns in simple change graphs in an edge-wise fashion."
    },
    {
      "title": "Concrete Approach",
      "text": "The motivation of Section 3.1 suggests the following high-level procedure to employ language models for the mining of edit patterns: (1) We serialize simple change graphs of (successive) pairs of software models edge-wise. (2) We then generate pairs of partial simple change graphs and their completion to the full simple change graph serialization. (3) These pairs are then used to fine-tune a pre-trained language model. (4) The fine-tuned language model can then be used to generate serializations of simple change graphs. When a context is already given, we are in a completion setting. (5) In a last step, we rank the generated serializations from the previous step. We can divide the procedure into two phases: training phase (Step 1, 2, and 3) and the generation phase (i.e., Step 4 and 5). In what follows we will describe the steps in more detail."
    },
    {
      "title": "3.2.1. Training Phase",
      "text": "The _input_ to the training phase is a set of simple change graphs computed from pair-wise (successive) differences of software models in a model repository. The _output_ of the training phase is a fine-tuned language model. Step (1) - Serialize the graph componentsWe have to serialize the graph as an edge list. One reason for this is that we want to sample the simple change graphs edge-wise, as suggested in Section 3.1. Common formats such as the GraphML1 are not suitable, because they start with a list of vertices before they list the edges. It is intuitive from a language model perspective why this is not a suitable format: Suppose our initial simple change graphs are the results of the application of more than one edit pattern. In this case, the initial simple change graphs will be larger than the simple change graphs of the edit patterns that we want to discover. Anyway, the language model will learn to generate serializations statistically similar to the input. This implies that the number of nodes would probably always be too large for a serialization of an edit pattern. We therefore use a simple graph serialization called EdgeList for directed labeled graphs: The serialization of every edge has the format ``` e-src_id*-tgt_id*-edge_label*-src_label*-tgt_label* ``` where <src_id> and <tgt_id> are identifiers for the source and target vertices of the edge, respectively. The serialization of a graph starts with a header line in the format ``` t*-graph_id> ``` and then all edges of the graph serialized line by line. An example is given in Listing 1. ``` t*1 e01Add_portAdd_ComponentAdd_Port e02Add_requirementAdd_ComponentAdd_Requirement ``` Listing 1: An example SCG in the EdgeList format. Another degree of freedom that arises in the serialization step is the order of the edges and the identifiers of the vertices in this serialization. After the order of the edges is chosen, we can enumerate the vertices (e.g., increasing integers in the order they appear when following the edges). This still leaves us with \\(|E|!\\) choices (up to automorphism), which can quickly become impractically large. There are _canonical graph serializations_[(44)], but they do have exponential worst-case time complexity and do not help for our task, because these serializations do not ensure that pattern subgraphs appear with their own canonical ordering2. Therefore, choosing _some_ of the possible \\(|E|!\\) edge orderings will probably lead to better results. In this work we chose one ordering (obtained through a depth-first search) that worked well for our data, and we leave the investigation of the influence of further edge orderings for future research. Footnote 2: Otherwise graph isomorphism problem and subgraph isomorphism problem would be in the same complexity class, which is not thought to be the case. _Step (2) - Randomly split serializations in prompt and completion:_ As input for the fine-tuning of language model, we provide a set of context and completion pairs. From every EdgeList serialization from the previous step, we generate three training samples: We compute three cut points. One cut point is randomly chosen in the first 10% of the edges, the second randomly between the first and the last 10% of the edges, and the third cut point randomly among the last 10% of the edges. For every cut point, we then obtain one training sample by taking the edges before the cut point as context and the edges after the cut point as the completion. _Step (3) - Fine-tune a language model:_ In this step, we use the dataset obtained above to fine-tune a pre-trained language model. Specifically, we use so-called autoregressive language models (the GPT-3 model family), although other types of language models are also conceivable. In autoregressive language models, only the probabilities for the next token in a sequence is predicted based on the context (i.e., the previous tokens). This way, we obtain a language model that is fine-tuned to the simple change graph serializations computed for a specific model repository."
    },
    {
      "title": "3.2.2. Generation Phase",
      "text": "The fine-tuned language model can now be applied to generate edit patterns, on the one hand, and completion operations, on the other. _Step (4) - Generate simple change graphs for edit patterns and completion operations:_ The main difference between the generation of edit pattern candidates and completion operation candidates is the context. For the generation of completion operation candidates, we use the simple change graph serialization of an observed evolution \\(m_{1}\\xrightarrow{e}m_{2}\\) as the context in the generation of candidates. For the generation of edit patterns, we use an \"empty edge\" context (i.e., the token \\(\\mathsf{e}\\) to be more precise). The candidate generation works as follows (see pseudo code in Listing 1): The algorithm takes a set of _incomplete_ edit operation candidates (in the form of serialized simple change graphs) and uses the fine-tuned language model to sample new edge candidates and appends them to the incomplete edit operation candidate (Line 12). The sampling generates all possible extensions above a certain probability threshold. Since we cannot guarantee that the extensions lead to a correct EdgeList serialization, we check the syntactical correctness and reject incorrect extensions (Line 13). Furthermore, even syntactically valid extensions could be invalid according to the metamodel and have to be rejected likewise (Line 14). After that, the corresponding simple change graph represents a valid edit operation by definition. Based on a graph isomorphism test, we then filter out duplicates (Line 15). Although graph isomorphism is theoretically expensive from a computational perspective, in our setting, it is acceptable since we have only a few medium size graphs, and employ Weisfeiler-Lehman hashes [30] to speed up the comparison. We add complete candidates to the output list (Line 19) and repeat this process until all candidates are complete (Line 9). Whether a candidate is complete is checked using several conditions such as the total probability of the candidate, a drop in the probability of a generated edge, or a generated stop token. _Step (5) - Ranking of generated candidates_: In the last step, we rank the generated candidates from Step 4. Several ranking metrics are conceivable, for example, the probability for a candidate given by the token probabilities of the language model, the compression metric as used in the work of Tinnes et al. [67], or also scaled variants of the token probabilities such as scaling with the number of edges or nodes. We will evaluate different ranking metrics. Figure 1. Pseudocode for the candidate generation."
    },
    {
      "title": "4. Evaluation",
      "text": "In this section we evaluate to what extent language models, as exemplified by our approach (Section 3), can help to derive edit patterns and completion operations from the software model history."
    },
    {
      "title": "Research Questions",
      "text": "To better understand the merits of language models for edit operation and completion operation mining, we set out to answer the following research questions: **RQ 1:**_Using our approach, are language models capable of generating correct simple change graph graph serializations?_ Since a language model is not aware of the meta-model and definition of a graph _per se_, the generated edit patterns or completion operations might not be correct simple change graph serializations. That is, they might be invalid according to the metamodel (e.g., invalid combination of edge, source, and target node labels) or could even be invalid directed labeled graph serializations (i.e., not adhere to the EdgeList format). We are particularly interested how this depends on the properties of the dataset and the properties of the language model used for the approach. **RQ 2:**_Using our approach, are language models capable of providing completion operations for software models?_ The approach uses a language model that is trained to complete the simple change graph serializations. It optimizes the _token probability_, given a context. This does not ensure _per se_ that the provided completions represent simple change graphs that are isomorphic to the _correct_ simple change graphs. Therefore, we are interested in to which extent the completed and the original simple change graph coincide and how this depends on the properties of the dataset and of the language model. **RQ 3:**_Using our approach, can edit operations be reconstructed from the language model?_ The main idea of our approach is that the language model leverages patterns in the training data while generating text. Therefore, it should be possible to read out the patterns from the language model. The approach presented here is just one idea how the patterns can be retrieved from the language model, and we have to evaluate this approach empirically. We further evaluate how the generation of edit pattern candidates depends on the properties of the dataset and the properties of the language model used for the approach. **RQ 4:**_Which ranking metric for the edit operation candidates performs best?_ There are several possibilities to rank the list of generated edit operation candidates, including language model probability, the probability scaled by the factorial of the number of edges, or scaled by the number of edges, or a more computationally expensive compression-like metric as used in a previous approach [(67)]. The idea behind the scaled metrics is that - as discussed already above - there are several possible simple change graph serialization (up to \\(|E|\\)!). At least, the probability will inevitably decrease with the number of edges and we have to account for this to avoid favouring smaller edit operation candidates."
    },
    {
      "title": "Dataset",
      "text": "As we will discuss in Section 4.6, the goal of this work is to study the merits of language models for edit pattern and completion operation mining with a high internal validity. To answer the research questions, we simulated the evolution of a software model similar to previous work [67]. This gives us control over the edit operations that have been applied to yield the model history. For this simulation, we used a metamodel from Timnes et al. [67] that resembles a simple component model (as used in modelling system architecture) with components, implementations, ports, connectors, and requirements. We then randomly apply edit operations. Specifically, we applied three different kinds of edit operations (i.e., adding a component, adding an interface, and adding a new package including a component). We controlled for the number of edit operations that are applied per model revision (i.e., 11, 31, 51, 81) and the number of model revisions in one dataset (i.e., 10 or 20). We furthermore randomly applied perturbations, that is, with a certain probability (i.e., 0%, 50%, 100%), we slightly modified the edit operation by a successive application of an additional edit operation that overlaps with the original edit operation. This way, we obtain 24 simulations of a software model evolution (see Figure 2) and we know which edit operations had been applied between two model revisions of the repositories."
    },
    {
      "title": "Operationalization",
      "text": "For every simulated repository in the dataset from Section 4.2, we applied the steps of the training phase, described in Section 3. We controlled for the number of epochs (i.e., 4 and 6) and the base language model used for the fine-tuning (i.e., text-ada-001, text-curie-001, and text-davinci-003 from the GPT-3 family of language models). Since fine-tuning the text-davinci-003 model is quite expensive (i.e., 3 Cents per thousand tokens at the time of writing), we fine-tuned this model only for the model repositories where perturbation probability equals 100% (the ones which are typically the harder ones). We therefore have to report separately if we also include the text-davinci-003 model in the analysis. We split the datasets (for each of the repositories from Section 4.2) in a training and a test set (90% of the samples in the Figure 2. Simulation of the model repositories. train set and 10% in the test set), so that we can report on the performance of the (textual) completion task the models were trained on. The total cost for the training via the OpenAI API was 347$. **Experiment 1:** To answer the first research question, we use the fine-tuned language models and apply the edit pattern candidate generation from Section 3. During the edge extension in the generation phase, we checked whether the completion actually corresponds to a correct graph serialization. We also checked whether the edge extension corresponds to a correct extension according to the meta-model, that is, whether the generated graph actually corresponds to a simple change graph. We then report how many of the extensions during the generation were incorrect and how the number of incorrectly generated serializations depends on the base language model, the number of training epochs, the perturbation parameter, and the number of training tokens in the dataset. **Experiment 2:** We fine-tune the language models based on a model completion task. Therefore, the procedure from Section 3 for deriving fine-tuning data from a model repository together with the training test split described above yield data that can be used to evaluate whether language models can be used for model completion, answering the second research question. We investigate the simple change graph completion from two perspectives: First, we investigate the average token accuracy on the test set during the fine-tuning of the language model. The _average token accuracy_ gives us the relative number of correctly retrieved tokens. The metric is not aware of any specifics of the dataset. For example, even a single wrong token in a serialization can produce a syntactically wrong serialization while the token accuracy for this can still be high. We therefore also analyze the completion operation candidates from a graph matching perspective. Since generating all completion candidates for all test samples of all fined-tuned language models will be quite expensive, we select two fine-tuned language models and perform the analysis for them. From the set of generated completion operation candidates, we especially look at two candidates: the one providing \"the best completion\" and the \"top-ranked completion\" using the edge-scaled ranking metric (see Experiment 4). In some sense, these two selected candidates give us an upper and a lower bound for the performance on the software model completion task. To make completions comparable, we assign a numeric value to them. This is possible, since we define completion candidates isomorphic to the correct graph to be better than completion candidates that are too large (i.e., the ground truth is a subgraph of the completion candidate), which themselves are better than completion candidates that are too small, which again are better than incorrect completion candidates (i.e., some edges missing and some additional edges). **Experiment 3:** To answer RQ 3, we evaluate the generation of edit pattern candidates from the fine-tuned language models. To this end, we apply the approach from Section 3 to the synthetic model repositories. Since we know the edit operations that have been applied, we can directly look for the applied edit operations in the list of edit pattern recommendations. We count the number of correct edit operation candidates that have been generated. There are three correct edit operations in total, and each of them has been applied in every software model to our synthetic dataset (see Figure 2). Additionally, we investigate how the number of correctly retrieved edit operations depends on repository as well as language model parameters. We will also investigate the costs for the generation of edit pattern candidates. **Experiment 4:** As in experiment 3, we apply the approach to the synthetic model repositories. We compute the different ranking metrics (i.e., language model probability, the probability scaled by the factorial of the number of edges, probability scaled by the number of edges, and compression-like metric) for all generated edit pattern candidates. We then compare the different rankings based on the given ground truth, that is, the rank of the known applied edit operation. To compare the different ranking metrics, we use the _mean average precision at k_ (MAP@k), which is commonly used in the evaluation of recommender systems [56]: \\[\\mathrm{MAP@k}:=\\frac{1}{|D|}\\sum_{\\mathrm{D}}\\mathrm{AP@k}\\,\\] where \\(D\\) is the family of all datasets (one dataset represents one repository) and \\(\\mathrm{AP@k}\\) is defined by \\[\\mathrm{AP@k}:=\\frac{\\sum_{i=1}^{k}\\mathrm{P}(i)\\cdot\\mathrm{rel}(i)}{|\\text{ all correct simple change graphs}|}\\,\\] where \\(\\mathrm{P}(i)\\) is the precision at \\(i\\), and \\(\\mathrm{rel}(i)\\) indicates if the candidate at rank \\(i\\) is relevant."
    },
    {
      "title": "Results",
      "text": "**Experiment 1:** For 51.8% of the simulated repositories, the generation procedure produced exclusively valid graphs, for 48.2% it produced also only correct simple change graph (i.e., also correct with respect to the metamodel). On average, 2.26 invalid graphs are generated in the generation phase, with a minimum of 0 and a maximum of 30. A constraint in the given EdgeList serialization is that a node with a given id has to appear always with the same label (i.e., the node labels are redundantly encoded in EdgeList). The only type of violation against a valid EdgeList encoding we have encountered in this experiment was that this correspondence of node id and node label has been violated. A manual inspection of the data for the model repositories with a large amount of invalid graphs (\\(>5\\)) reveals that these are the \"smaller\" datasets with mostly a high perturbation. For example, the repository with only 10 revisions, 11 applied edit operations, and a perturbation of 100% is the one with the maximum of 30 invalid graphs. In Table 1 we report the correlation coefficients (Spearman3) between invalid graphs and invalid simple change graphs and the base language model (BM), the number of training epochs, the perturbation probability (P), and the number of training tokens of the dataset (T). For the correlation with the base language model, we sort the base models according to their size (i.e., ada: 0, curie: 1, davinci: 2). Footnote 3: We use Spearman correlation instead of Pearson correlation, since we can not assume that there are linear dependencies. We observe significant positive Spearman correlation between the number of invalid generated graph serializations and the perturbation parameter. Furthermore, there is a significant negative Spearman correlation between the number of invalid generated graph serializations and the number of tokens in the training set. **Experiment 2:** At the token level, we find an average token accuracy of 0.969, with a minimum of 0.921, and a maximum of 0.990 on our test data set. Only 2.71 completion candidates are generated, on average. For a large number of samples, only one completion operation candidate has been generated. In all of theses cases, the only candidate has also been the correct one. On \\begin{table} \\begin{tabular}{r c c c c} \\hline \\hline & BM & Epochs & P & T \\\\ \\hline \\#Invalid & \\(-0.08\\) & \\(-0.10\\) & \\(0.35\\)\\({}^{**}\\) & \\(-0.33\\)\\({}^{*}\\) \\\\ \\#Invalid metamodel & \\(-0.22\\) & \\(-0.05\\) & \\(0.31\\)\\({}^{*}\\) & \\(-0.22\\) \\\\ \\#Invalid (+ davinci) & \\(-0.28\\) & \\(-0.11\\) & – & \\(-0.56\\)\\({}^{**}\\) \\\\ \\#Invalid metamodel (+ davinci) & \\(-0.24\\) & \\(-0.13\\) & – & \\(-0.56\\)\\({}^{**}\\) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1. Correlation (Spearman) between invalid graphs generation with other parameters. (\\({}^{**}\\): \\(p<.001\\), \\({}^{*}\\): \\(p<.01\\))average, in 87.60% of the samples, the correct completion is among the candidates, with an average rank of 1.55 (w.r.t. the edge-scaled ranking). In Table 2 we report on the Spearman correlation coefficients of the score of the completion candidates with the number of omitted edges (i.e., the ones that have to be computed), the total number of edges of the ground truth simple change graph, and the number of completions that have been generated. We see significant negative correlations between the score (of the best generated candidate and the best ranked candidate) and the number of edges that have to be completed, the number of edges from the full original simple change graph, and the number of completions candidates that have been generated. **Experiment 3:** On average, out of the three applied edit operations, we could retrieve 2.17 for the text-ada-001 model, 2.13 for the text-curie-001 model, and 1.00 for the text-davinci-003 model. The text-davinci-003 model has only been trained to the datasets that appeared to be difficult for text-ada-001 and text-curie-001 (i.e., perturbation probability of 100%). In Table 3, we list the Spearman correlations of the correctly retrieved edit operations with the base model (BM), the number of training epochs, the perturbation probability (P), and the number of applied edit operations between two model revisions (E). The average generation cost for text-ada-001 was 0.45 Cent, for text-curie-001 3.68 Cent, and for text-davinci-003 51.84 Cent. **Experiment 4:** We list the MAP scores for the 4 different ranking metrics in Table 4. Furthermore, we compare the average precisions obtained through the different ranking. We can observe a high significant (\\(p<.001\\)) Spearman correlations coefficients \\(>0.65\\) among all of them, the largest one between compression metric and node factorial scaled probability metric (0.90). \\begin{table} \\begin{tabular}{c c c c} \\hline \\hline & \\#Omitted Edges & \\#Total Edges & \\#Completions \\\\ \\hline Score (best rank) & \\(-0.69\\)* & \\(-0.38\\)* & \\(-0.75\\)* \\\\ Score (best candidate) & \\(-0.29\\)* & \\(-0.33\\)* & \\(-0.21\\)* \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Correlation coefficients (Spearman) of the completion candidate score with the number of omitted edges in the sample, the number of total edges of the correct simple change graph, and the number of completion candidates that have been generated. (*: \\(p<.001\\)) \\begin{table} \\begin{tabular}{c c c c c} \\hline \\hline & BM & Epochs & P & E \\\\ \\hline \\#Correct & \\(-0.33\\)* & \\(-0.03\\) & \\(-0.80\\)* & \\(0.10\\) \\\\ \\#Correct (+ davinci) & \\(-0.22\\) & \\(0.08\\) & \\(-\\) & \\(0.28\\) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Correlations between the correctly retrieved edit operations and repository as well as language model parameters. (*: \\(p<.001\\))"
    },
    {
      "title": "Discussion",
      "text": "**RQ 1:** From our results, we can conclude that with our current approach we generate mostly valid graph serializations. Indeed, even for a majority of the simulated repositories, we do not generate invalid graph serializations, at all, and, on average, 2.26 invalid candidates per generation phase. If we consider correct simple change graphs, only 0.901 illegal simple change graphs (i.e., they do not conform to the metamodel or the definition of simple change graphs) are generated, on average. The analysis also shows that the repositories for which we also get invalid graph serializations are the smaller ones (in the number of training tokens), with a high perturbation. The dependency on the size of the repository and the perturbation is also significant (as can be seen in Table 1). This suggests, that one should use larger repositories or even pre-train the language model with simple change graph serializations from other repositories. We also observe a small negative but insignificant correlation w.r.t. the size of the base language model that has been used for the fine-tuning. This suggests that larger base language models might perform better in the generation of simple change graph. Given that text-davinci-003 is 50 times as expensive as text-ada-001, and this correlation is not significant, we can conclude that text-ada-001 is an acceptable choice for the base language model for the generation of simple change graphs. **Summary:** Overall, invalid graphs are generated only rarely. Smaller repositories are more likely to lead to invalid graphs than larger repositories. **RQ 2:** From the results, we can clearly conclude that, in a majority of the samples, the correct completions have been generated. Also, the number of completion operation candidates is typically low and the correct completion operation (if among the candidates) is typically top ranked. The larger the simple change graph and the more edges we omit for the completion, the worse the score of the completion candidates. Anyway, for the larger graphs, a subgraph of the correct completion was among the candidates in most cases. **Summary:** A manageable amount of completion operation candidates are generated and in a majority of the cases the correct completion has been generated by the language model. Larger simple change graphs (in number of edges) and a larger number of omitted edges are more challenging than smaller ones, which is rather intuitive. **RQ 3:** Overall, using the approach from Section 3, we were able to retrieve two and, in some cases, even all of the applied edit operations. The perturbation seems to be the major influencing factor that increases the difficulty. Using larger language models (i.e., text-davinci-003) did not improve the results. We can even see a decrease of the number of \\begin{table} \\begin{tabular}{l c c c c} \\hline \\hline & Compression & Factorial & Probability & Edges-Scaled \\\\ \\hline MAP(\\(\\varnothing\\)3 & 0.32 & 0.20 & 0.13 & 0.33 \\\\ MAP(\\(\\varnothing\\)5 & 0.38 & 0.29 & 0.17 & 0.36 \\\\ MAP(\\(\\varnothing\\)10 & 0.41 & 0.32 & 0.24 & 0.40 \\\\ MAP(\\(\\varnothing\\)\\(\\infty\\) & 0.42 & 0.33 & 0.25 & 0.41 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4. MAP for the different ranking metrics. Grey background indicates the best MAP among all metrics. retrieved edit operations with increasing language model size. The costs for using the language models are definitely acceptable (0.45 Cent to 51.84 Cent), especially given that the more expensive language models do not perform any better. **Summary:** We were able to retrieve edit patterns from a fine-tuned language model. However, with increasing perturbation probability, the approach yields worse results. **RQ 4:** From the results of Experiment 4, we can see that, for \\(k>3\\) (where \\(k\\) is the number of considered generation candidates), the compression metric outperforms the other metrics. Since the compression metric is expensive to calculate, we also tried to recompute it from the probability given by the language model. Although we can observe high correlations among the average precisions, none of the three metrics yields the same ranking as the compression-based ranking. For practical purposes, the edges-scaled probability metric is most feasible, since it gives results close to the compression metric and does not require any expensive calculations. **Summary:** We can confirm earlier results (Wang et al., 2017) that the compression metric seems to be a good metric to select edit patterns. There seems to be a - yet unknown - relationship between the probability given by the language model and the compression metric from earlier work. **Frequent subgraph mining vs. language models:** In a recent study (Wang et al., 2017), frequent subgraph mining has been proposed for edit operation mining. First, this approach cannot be applied to model completion out-of-the-box, because it does not yield a conditional distribution that can be used for completion. Second, regarding edit operation mining, frequent subgraph mining is challenging from a computational/memory complexity point of view. This sometimes requires omitting large simple change graphs in the mining. The reason for this is that subgraph matching requires to try many possible combinations that vertices from the subgraph can be matched to the super graph. Using language models, we circumvent this issue and train on sequential - instead of graph-like - data. The training resources (and therefore also cost) scale linearly with the number of tokens. A graph would be \"equivalent\" to all of its possible serializations (i.e., approximately \\(|E|!\\)). Training a language model on all of theses serializations would then be infeasible. Anyway, using a language model as described in Section 3, we get control over the number of serializations and we have seen that considering only one serialization per graph yields promising results."
    },
    {
      "title": "Threats To Validity",
      "text": "We have evaluated the proposed approach in a controlled experiment setting, this way, maximizing internal validity. However, further experimentation and evaluation is required before it can be considered for implementation in real-world software engineering projects. Our research is currently in the simulation phase. This stage of research is similar to the early stages of drug development in the pharmaceutical industry, where candidates for a treatment are first evaluated through simulations and small-scale experiments before moving on to larger studies. One of the main reasons to take such a staged approach is that the application of language models to large-scale industrial experiments requires training on large model repositories. Training on all possible simple change graphs and serializations is infeasible from a cost perspective. We therefore have to better understand large language models in the domain of model generation to use a suitable sampling in real-world applications. Furthermore, without cost-intensive legal considerations, real-world data can not be uploaded to the GPT-3 API and therefore we would have to rely on much smaller, non-state-of-the-art language models. So, we made the explicit decision to trade-off external validity for internal validity (Zhou et al., 2017) before moving on to the next stage, that is, an application to real-world models. The purpose of the present study is to understand the merits of language models for edit pattern and completion operation mining. With respect to internal validity, we have chosen those properties and parameters that intuitively have the highest impact on fine-tuning language models, although we cannot control for any arbitrary property of the modelling language or the model repository (because of combinatorial explosion). Several design decisions (e.g., which parameters to fix and which to vary) have to be made before language models can be applied to the domain of software model completion (e.g., serialization strategy, graph encoding, choice of the base model, etc.). Other design decisions could have led to other conclusions and there is still room for improvement and optimization of language models for edit pattern mining and model completion. Furthermore, the base models (i.e., GPT-3) that were fine-tuned in this work are only available through an API. We wanted to conduct this study with state-of-the-art models, and at the time the experiments were conducted, GPT-3 was the state-of-the-art without significant competition. However, it would also be interesting to compare a larger set of language models on the above tasks, although this is well beyond the scope of the present study."
    },
    {
      "title": "5. Conclusion",
      "text": "In this work, we have evaluated the principle feasibility of using language models (i.e., GPT-3) for extracting edit patterns from software model repositories and for model completion. A key advantage is that a language model has to be fine-tuned only once and can then be used for edit pattern mining and model completion. We presented a formalization of both tasks that makes the synergy between them more obvious. To evaluate the use of language models for edit pattern mining and model completion, we conducted a controlled experiment with a synthetic software model repository. We found promising results for both tasks (edit pattern mining and model completion), even without elaborate optimizations. Our approach fine-tunes language models on a graph representation of model differences. It was able to correctly complete a majority of serialized difference graphs from a synthetic test dataset. Furthermore, some of the edit operations that have been applied to generate the synthetic difference graphs could be generated via the fine-tuned language model, although we were not able to always generate all applied edit operations. For future work, it is necessary to optimize the approach, for example, investigating the influence of adding more simple change graph serializations during the fine-tuning of the language model. Another way of optimization is to further fine-tune the approach in a supervised manner, on a set of correct and incorrect completions or a set of known edit patterns. Also, in a productive environment, user-feedback may be involved in the generations by wrapping the language model in a reinforcement learning approach. Optimized versions of the approach have to be evaluated in more realistic, real-world settings. Also, it would be interesting to investigate whether language models are able to learn attribute relationships (e.g., functional relationships between attributes) and learn to abstract (e.g., automatically discover \"is-a\" relationships)."
    },
    {
      "title": "6. Data Availability",
      "text": "We have provided data as well as Python scripts for our approach--to replicate the results of this paper--as a replication package. We furthermore provide the R scripts to replicate our statistical evaluation. The replication package will be made public (e.g., GitHub) in case of acceptance."
    },
    {
      "title": "References",
      "text": "* [1] Vlad Acretoaie, Harald Storrle, and Daniel Struber. VMTL: A language for end-user model transformation. _Software & Systems Modeling_, 17(4):1139-1167, 2018. * [2] Lissette Almonte, Esther Guerra, Ivan Cantador, and Juan de Lara. Recommender systems in model-driven engineering. _Software and System Modelling_, 21(1):249-280, 2022. * [3] Abdullah M. Alshanqiti, Reiko Heckel, and Tamim Ahmed Khan. Learning minimal and maximal rules from observations of graph transformations. _Electronic Communication of the European Association of Software Science and Technology_, 47, 2012. * [4] Thorsten Arendl, Enrico Biermann, Stefan Jurack, Christian Krause, and Gabriele Taentzer. Henshin: Advanced concepts and tools for in-place EMF model transformations. In _Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)_, pages 121-135. Springer, 2010. * [5] Iman Avarpour, John Grundy, and Lars Grunke. Specifying model transformations by direct manipulation using concrete visual notations and interactive recommendations. _Journal of Visual Languages and Computing_, 28:195-211, 2015. * [6] Viraj Bagal, Rishal Aggarwal, PK Vinod, and U Deva Priyakumar. Molgpt: Molecular generation using a transformer-decoder model. _Journal of Chemical Information and Modeling_, 62(9):2064-2076, 2021. * [7] Andras Balogh and Daniel Varro. Advanced model transformation language constructs in the VIATRA2 framework. _Proceedings of the ACM Symposium on Applied Computing_, 2:1280-1287, 2006. * [8] Yoshua Bengio, Rejean Ducharme, and Pascal Vincent. A neural probabilistic language model. In _Advances in Neural Information Processing Systems_, pages 932-938. MIT Press, 2000. * [9] Enrico Biermann, Claudia Ermel, and Gabriele Taentzer. Formal foundation of consistent EMF model transformations by algebraic graph transformation. _Software and Systems Modeling_, 11(2):227-250, 2012. * [10] Petra Brosch, Philip Langer, Martina Seidl, Konrad Wieland, Manuel Wimmer, Gerti Kappel, Werner Retschitzegger, and Wieland Schwinger. An example is worth a thousand words: Composite operation modeling by-example. In _Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)_, pages 271-285. ACM, 2009. * [11] Erwan Brottier, Franck Fleurey, Jim Steel, Benoit Baudry, and Yves Le Traon. Metamodel-based test generation for model transformations: an algorithm and a tool. In _Proceedings of International Symposium on Software Reliability Engineering (ISSEE)_, pages 85-94, 2006. * [12] Cedric Brun and Alfonso Perantonio. Model differences in the eclipse modeling framework. _UPGRADE, The European Journal for the Informatics Professional_, 9(2):29-34, 2008. * [13] Loli Burgueno, Jordi Cabot, Shuai Li, and Sebastien Gerard. A generic LSTM neural network architecture to infer heterogeneous model transformations. _Software and Systems Modelling_, 21(1):139-156, 2022. * [14] Loli Burgueno, Robert Clariso, Sebastien Gerard, Shuai Li, and Jordi Cabot. An nlp-based architecture for the autocompletion of partial domain models. In _Proceedings of the International Conference on Advanced Information Systems Engineering_, pages 91-106. Springer, 2021. * [15] Loli Burgueno, Jordi Cabot, and Sebastien Gerard. An lstm-based neural network architecture for model transformations. In _Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)_, pages 294-299. IEEE, 2019. * [16] Guiran Chang, Chunguang Tan, Guanhua Li, and Chuan Zhu. _Developing Mobile Applications on the Android Platform_. Springer Berlin Heidelberg, 2010. * [17] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _arXiv preprint_, 2021. * [18] Soudip Roy Chowdhury, Florian Daniel, and Fabio Casati. Recommendation and weaving of reusable mashup model patterns for assisted development. _ACM Transactions on Internet Technology_, 14(2-3), 2014. * [19] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Antonio Mastropaolo, Emad Aghajani, Denys Poshyvanyk, Massimiliano Di Penta, and Gabriele Bavota. An empirical study on the usage of transformer models for code completion. _Transactions on Software Engineering_, 48(12):4818-4837, 2022. * [20] Diane J Cook and Lawrence B Holder. _Mining graph data_. John Wiley & Sons, 2006. * [21] Krxysztof Czarnecki and Simon Helsen. Feature-based survey of model transformation approaches. _IBM systems journal_, 45(3):621-645, 2006. * [22] James B Dabney and Thomas L Harman. _Mastering simulink_, volume 230. Pearson/Prentice Hall Upper Saddle River, 2004. * [23] Hartmut Ehrig, Ulrik Prange, and Gabriele Taentzer. Fundamental theory for typed attributed graph transformation. In _International Conference on Graph Transformation (ICGT)_, pages 161-177. Springer, 2004. * [24] Karsten Ehrig, Claudia Ermel, Stefan Hansgen, and Gabriele Taentzer. Generation of visual editors as Eclipse plug-ins. In _Proceedings of the International Conference on Automated Software Engineering (ASE)_, pages 134-143. IEEE, 2005. * [25] Erich Gamma, Ralph Johnson, Richard Helm, Ralph E Johnson, and John Vlissides. _Design patterns: elements of reusable object-oriented software_. Prentice Hall, 1995. * [26] Tarun Garg, Kaushik Roy, and A. Sheth. Can language models capture graph semantics? from graphs to language model and vice-versa. _ArXiv_, abs/2206.09259, 2022. * [27] Sumit Gulwani, Oleksandr Polozov, Rishahi Singh, et al. Program synthesis. _Foundations and Trends(r) in Programming Languages_, 4(1-2):1-119, 2017. * [28]* [28] Geoffrey E. Hinton. Learning distributed representations of concepts. In _Proceedings of the Annual Conference of the Cognitive Science Society_, volume 1. Amherst, MA, 1986. * [29] Katrin Holldobler, Bernhard Rumpe, and Ingo Weissmoller. Systematically deriving domain-specific transformation languages. In _Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)_, pages 136-145. ACM/IEEE, 2015. * [30] Ningyuan Teresa Huang and Soledad Villar. A short tutorial on the weisfeiler-lehman test and its variants. In _International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 8533-8537. IEEE, 2021. * part 3: Programming languages. Technical report, DIN/EN/IEC 61131, 2014. * [32] F. Jelinek and R.L. Mercer. Interpolated estimation of markov source parameters from sparse data. In _Proceedings of the Workshop on Pattern Recognition in Practice_, 1980. * Essays Dedicated to Bernhard Thalheim on the Occasion of His 60th Birthday_, pages 197-215. Springer, 2012. * [34] Timo Kehrer. _Calculation and Propagation of Model Changes based on User-Level Edit Operations: A Foundation for Version and Variant Management in Model-Driven Engineering_. PhD thesis, University of Siegen, 2015. * [35] Timo Kehrer, Abdullah M Alshanqiti, and Reiko Heckel. Automatic inference of rule-based specifications of complex in-place model transformations. In _Proceedings of the International Conference on Model Transformations (ICMT)_, pages 92-107. Springer, 2017. * [36] Timo Kehrer, Udo Kelter, and Gabriele Taentner. Consistency-preserving edit scripts in model versioning. In _Proceedings of the International Conference on Automated Software Engineering (ASE)_, pages 191-201, 2013. * [37] Timo Kehrer, Michaela Rindt, Pit Piesch, and Udo Kelter. Generating edit operations for profiled uml models. In _MoDELS Workshop on Models and Evolution (ME@MoDELS)_, pages 30-39. Citeseer, 2013. * [38] Timo Kehrer, Gabriele Taentzer, Michaela Rindt, and Udo Kelter. Automatically deriving the specification of model editing operations from meta-models. In _Proceedings of the International Conference on Model Transformations (ICMT)_, volume 9765, pages 173-188, 2016. * [39] Nikhil S. Ketkar, Lawrence B. Holder, and Diane J. Cook. Subdue: Compression-based frequent pattern discovery in graph data. In _Proceedings of the 1st International Workshop on Open Source Data Mining: Frequent Pattern Mining Implementations_, page 71-76. ACM, 2005. * [40] Dimitris Kolovos, Louis Rose, Antonio Garcia-Dominguez, and Richard Paige. The epsilon book (2013). _Google Scholar_, 2012. * [41] Philippe B Kruchten. The 4+ 1 view model of architecture. _IEEE software_, 12(6):42-50, 1995. * [42] Ehsan Mashhadi and Hadi Hemmati. Applying codebert for automated program repair of java simple bugs. In _Proceedings of the International Conference on Mining Software Repositories (MSR)_, pages 505-509. IEEE, 2021. * [43] Steffen Mazanek and Mark Minas. Generating correctness-preserving editing operations for diagram editors. _Electronic Communication of the European Association of Software Science and Technology_, 18, 2009. * [44] Brendan D. McKay and Adolfo Piperno. Practical graph isomorphism, ii. _Journal of Symbolic Computation_, 60:94-112, 2014. * [45] Tom Mens and Pieter Van Gop. A taxonomy of model transformation. _Electronic Notes in Theoretical Computer Science_, 152(1-2):125-142, 2006. * [46] Chihu Mokaddan, Houari Sahraoui, and Eugene Syriani. Recommending model refactoring rules from refactoring examples. In _Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)_, pages 257-266. ACM, 2018. * [47] Manuel Ohrndorf, Christopher Pietsch, Udo Kelter, and Timo Kehrer. ReVision: A tool for history-based model repair recommendations. In _Proceedings of the International Conference on Software Engineering (ICSE): Companion Proceedings_, pages 105-108. ACM, 2018. * [48] OMG. Meta Object Facility (MOF) 2.0 Query/View/Transformation Specification, Version 1.3. Technical report, Object Management Group, June 2016. * [49] OMG. Unified modeling language (UML) version 2.5.1. Standard, Object Management Group, December 2017. * [50] OMG. Omg sysml v. 1.6. Standard, Object Management Group, December 2019. * [51] Michael Polanyi. _Personal Knowledge: Towards a Post Critical Philosophy_. University of Chicago Press, 1958. * [52] Oleksandr Polozov and Sumit Gulwani. Flashmeta: A framework for inductive program synthesis. _SIGPLAN Not._, 50(10):107-126, 2015. * [53] Alberto Rodrigues Da Silva. Model-driven engineering: A survey supported by the unified conceptual model. _Computer Languages, Systems and Structures_, 43:139-155, 2015. * [54] Hazem Peter Samoaa, Firas Bayram, Pasquale Salza, and Philipp Leitner. A systematic mapping study of source code representation for deep learning in software engineering. _IET Software_, 2022. * [55] Maik Schmidt and Tilman Gloetzner. Constructing difference tools for models using the SiDiff framework. In _Proceedings of the International Conference on Software Engineering (ICSE): Companion Proceedings_, pages 947-948. ACM/IEEE, 2008. * [56] Gunnar Schroder, Maik Thiele, and Wolfgang Lehner. Setting goals and choosing metrics for recommender system evaluations. In _Proceedings of the Conference on Recommender Systems (RecSys)_, page 53. ACM, 2011. * [57] Sagar Sen, Benoit Baudry, and Hans Vangheluwe. Towards domain-specific model editors with automatic model completion. _Simulation_, 86(2):109-126, 2010. * [58] Sohil Lal Shrestha and Christoph Csallner. Slgpt: using transfer learning to directly generate simulink model files and find bugs in the simulink toolchain. In _Proceedings of the Conference on Evaluation and Assessment in Software Engineering (EASE)_. ACM, 2021. * [59] Janet Siegmund, Norbert Siegmund, and Sven Apel. Views on internal and external validity in empirical software engineering. In _Proceedings of the International Conference on Software Engineering (ICSE)_, pages 9-19. IEEE, 2015. * [60] Dominik Sobania, Martin Briesch, and Franz Rothlauf. Choose your programming copilot: A comparison of the program synthesis performance of github copilot and genetic programming. _CoRR_, abs/2111.07875, 2021. * [61] Friedrich Steimann and Bastian Ulke. Generic model assist. In Ana Moreira, Bernhard Schatz, Jeff Gray, Antonio Vallecillo, and Peter Clarke, editors, _Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)_, pages 18-34. Springer Berlin Heidelberg, 2013. * [62] Matthew Stephan. Towards a cognizant virtual software modeling assistant using model clones. In Anita Sarma and Leonardo Murta, editors, _Proceedings of the International Conference on Software Engineering (ICSE) (NIER)_, pages 21-24. IEEE / ACM, 2019. * [63] Matthew Stephan and James R Cordy. A survey of model comparison approaches and applications. In _Proceedings of the International Conference on Model-Driven Engineering and Software Development (MODELSWARD)_, pages 265-277, 2013. * [64] Yu Sun, Jeff Gray, and Jules White. MT-Scribe: An end-user approach to automate software model evolution. In _Proceedings of the International Conference on Software Engineering (ICSE)_, pages 980-982. ACM/IEEE, 2011. * [65] Vinitra Swamy, Angelika Romanou, and Martin Jaggi. Interpreting language models through knowledge graph extraction. _CoRR_, abs/2111.08546, 2021. * [66] Gabriele Taentzer, Manuel Ohrndorf, Yngve Lamo, and Adrian Rutle. Change-preserving model repair. In Marieke Huisman and Julia Rubin, editors, _Fundamental Approaches to Software Engineering_, pages 283-299, Berlin, Heidelberg, 2017. Springer Berlin Heidelberg. * [67] Christof Tinnes, Timo Kehrer, Joblin. Mitchell, Uwe Hohenstein, Andreas Biesdorf, and Sven Apel. Learning domain-specific edit operations from model repositories with frequent subgraph mining. In _Proceedings of the International Conference on Automated Software Engineering (ASE)_. ACM/IEEE, 2021. * [68] Christof Tinnes, Wolfgang Rossler, Uwe Hohenstein, Torsten Kuhn, Andreas Biesdorf, and Sven Apel. Sometimes you have to treat the symptoms: tackling model drift in an industrial clone-and-own software product line. In _Proceedings of the Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/TSE)_, pages 1355-1366. ACM, 2022. * [69] Arie Van Deursen, Eelco Visser, and Jos Warmer. Model-driven software evolution: A research agenda. _Technical Report Series TUD-SERG-2007-006_, 2007. * [70] Daniel Varro. Model transformation by example. In _Proceedings of the International Conference on Model Driven Engineering Languages and Systems (MODELS)_, pages 410-424. Springer, 2006. * [71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems_, 30, 2017. * A structural analysis of pre-trained language models for source code. In _Proceedings of the International Conference on Software Engineering (ICSE)_, pages 2377-2388. ACM, 2022. * [73] Martin Weyssow, Houari Sahraoui, and Eugene Syriani. Recommending metamodel concepts during modeling activities with pre-trained language models. _Software and Systems Modeling_, 21(3):1071-1089, 2022. * [74] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josta Hellendoorn. A systematic evaluation of large language models of code. In _Proceedings of the International Symposium on Machine Programming_, page 1-10. ACM, 2022. * [75] Xifeng Yan and Jiawei Ham. \\(\\epsilon\\)Spam: graph-based substructure pattern mining. 3, 2002. * [76] Yijun Yu, Thein Than Tun, and Bashar Nuseibeh. Specifying and detecting meaningful changes in programs. In _Proceedings of the International Conference on Automated Software Engineering (ASE)_, pages 273-282. IEEE, 2011. * [77] Liping Zhao, Waad Alhoshan, Alessio Ferrari, Keletso J Letsholo, Muhdeen A Ajaghe, Erol-Valeriu Chioasca, and Riza T Batista-Navarro. Natural language processing for requirements engineering: a systematic mapping study. _ACM Computing Surveys (CSUR)_, 54(3):1-41, 2021. * [78] Rustam Zhumagambetov, Ferdinand Molnar, Svexold A Peshkov, and Siamac Fazli. Transmol: repurposing a language model for molecular generation. _RSC advances_, 11(42):25921-25932, 2021."
    }
  ]
}