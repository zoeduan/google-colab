{
  "title": "MaaSDB: Spatial Databases in the Era of Large Language Models (Vision Paper)",
  "authors": [
    "Jianzhong Qi",
    "Zuqing Li",
    "Egemen Tanin"
  ],
  "abstract": "\n Large language models (LLMs) are advancing rapidly. Such models have demonstrated strong capabilities in learning from large-scale (unstructured) text data and answering user queries. Users do not need to be experts in structured query languages to interact with systems built upon such models. This provides great opportunities to reduce the barrier of information retrieval for the general public. By introducing LLMs into spatial data management, we envisage an LLM-based spatial database system to learn from both structured and unstructured spatial data. Such a system will offer seamless access to spatial knowledge for the users, thus benefiting individuals, business, and government policy makers alike. \n CCS CONCEPTS • Information systems → Spatial-temporal systems; Database query processing. \n",
  "references": [
    {
      "id": null,
      "title": "MaaSDB: Spatial Databases in the Era of Large Language Models (Vision Paper)",
      "authors": [
        "Jianzhong Qi",
        "Zuqing Li",
        "Egemen Tanin"
      ],
      "year": "2023",
      "venue": "",
      "doi": "10.1145/3589132.3625597"
    },
    {
      "id": "b0",
      "title": "Spatial Embedding: A Generic Machine Learning Model for Spatial Query Optimization",
      "authors": [
        "Alberto Belussi",
        "Sara Migliorini",
        "Ahmed Eldawy"
      ],
      "year": "2022",
      "venue": "SIGSPATIAL",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Spatial Structure-Aware Road Network Embedding via Graph Contrastive Learning",
      "authors": [
        "Yanchuan Chang",
        "Egemen Tanin",
        "Xin Cao",
        "Jianzhong Qi"
      ],
      "year": "2023",
      "venue": "EDBT",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Faith and Fate: Limits of Transformers on Compositionality",
      "authors": [
        "Nouha Dziri",
        "Ximing Lu",
        "Melanie Sclar",
        "Lorraine Xiang",
        "Liwei Li",
        "Bill Jiang",
        "Peter Yuchen Lin",
        "Chandra West",
        "Bhagavatula",
        "Le Ronan",
        "Jena D Bras",
        "Soumya Hwang",
        "Sean Sanyal",
        "Xiang Welleck",
        "Allyson Ren",
        "Zaid Ettinger",
        "Yejin Harchaoui",
        "Choi"
      ],
      "year": "2023",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "The RLR-Tree: A Reinforcement Learning Based R-Tree for Spatial Data",
      "authors": [
        "Tu Gu",
        "Kaiyu Feng",
        "Gao Cong",
        "Cheng Long",
        "Zheng Wang",
        "Sheng Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM on Management of Data",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "A Survey on Deep Learning Approaches for Text-to-SQL",
      "authors": [
        "George Katsogiannis",
        "-Meimarakis",
        "Georgia Koutrika"
      ],
      "year": "2023",
      "venue": "VLDB Journal",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "AI Meets Database: AI4DB and DB4AI",
      "authors": [
        "Guoliang Li",
        "Xuanhe Zhou",
        "Lei Cao"
      ],
      "year": "2021",
      "venue": "AI Meets Database: AI4DB and DB4AI",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Human Language Understanding & Reasoning",
      "authors": [
        "D Christopher",
        "Manning"
      ],
      "year": "2022",
      "venue": "Daedalus",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Preventing Bad Plans by Bounding the Impact of Cardinality Estimation Errors",
      "authors": [
        "Guido Moerkotte",
        "Thomas Neumann",
        "Gabriele Steidl"
      ],
      "year": "2009",
      "venue": "PVLDB",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Let's Speak Trajectories",
      "authors": [
        "Mashaal Musleh",
        "Mohamed F Mokbel",
        "Sofiane Abbar"
      ],
      "year": "2022",
      "venue": "SIGSPATIAL",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "DTT: An Example-Driven Tabular Transformer by Leveraging Large Language Models",
      "authors": [
        "Arash Dargahi",
        "Nobari",
        "Davood Rafiei"
      ],
      "year": "2023",
      "venue": "DTT: An Example-Driven Tabular Transformer by Leveraging Large Language Models",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Towards an Analysis of Range Query Performance in Spatial Data Structures",
      "authors": [
        "Bernd-Uwe Pagel",
        "Hans-Werner Six",
        "Heinrich Toben",
        "Peter Widmayer"
      ],
      "year": "1993",
      "venue": "PODS",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "",
      "authors": [
        "Noseong Park",
        "Mahmoud Mohammadi",
        "Kshitij Gorde",
        "Sushil Jajodia",
        "Hongkyu Park",
        "Youngmin Kim"
      ],
      "year": "2018",
      "venue": "Data Synthesis Based on Generative Adversarial Networks. PVLDB",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "",
      "authors": [
        "Jianzhong Qi",
        "Guanli Liu",
        "Christian S Jensen",
        "Lars Kulik"
      ],
      "year": "2020",
      "venue": "Effectively Learning Spatial Indices. PVLDB",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "A Learning Based Approach to Predict Shortest-Path Distances",
      "authors": [
        "Jianzhong Qi",
        "Wei Wang",
        "Rui Zhang",
        "Zhuowei Zhao"
      ],
      "year": "2020",
      "venue": "EDBT",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Querying Large Language Models with SQL",
      "authors": [
        "Mohammed Saeed",
        "Nicola De Cao",
        "Paolo Papotti"
      ],
      "year": "2023",
      "venue": "Querying Large Language Models with SQL",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Unstructured and Structured Data: Can We Have the Best of Both Worlds with Large Language Models?",
      "authors": [
        "Wang-Chiew Tan"
      ],
      "year": "2023",
      "venue": "Unstructured and Structured Data: Can We Have the Best of Both Worlds with Large Language Models?",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "From Natural Language Processing to Neural Databases",
      "authors": [
        "James Thorne",
        "Majid Yazdani",
        "Marzieh Saeidi",
        "Fabrizio Silvestri",
        "Sebastian Riedel",
        "Alon Halevy"
      ],
      "year": "2021",
      "venue": "From Natural Language Processing to Neural Databases",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Multimodal Neural Databases",
      "authors": [
        "Giovanni Trappolini",
        "Andrea Santilli",
        "Emanuele Rodolà",
        "Alon Halevy",
        "Fabrizio Silvestri"
      ],
      "year": "2023",
      "venue": "Multimodal Neural Databases",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables",
      "authors": [
        "Matthias Urban",
        "Carsten Binnig"
      ],
      "year": "2023",
      "venue": "Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Continuous Visible K Nearest Neighbor Query on Moving Objects",
      "authors": [
        "Yanqiu Wang",
        "Rui Zhang",
        "Chuanfei Xu",
        "Jianzhong Qi",
        "Yu Gu",
        "Ge Yu"
      ],
      "year": "2014",
      "venue": "Information Systems",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Real-time Continuous Intersection Joins over Large Sets of Moving Objects Using Graphic Processing Units",
      "authors": [
        "G D Phillip",
        "Zhen Ward",
        "Rui He",
        "Jianzhong Zhang",
        "Qi"
      ],
      "year": "2014",
      "venue": "VLDB Journal",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Translating Human Mobility Forecasting through Natural Language Generation",
      "authors": [
        "Flora D Hao Xue",
        "Yongli Salim",
        "Charles L A Ren",
        "Clarke"
      ],
      "year": "2022",
      "venue": "WSDM",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Leveraging Language Foundation Models for Human Mobility Forecasting",
      "authors": [
        "Bhanu Hao Xue",
        "Flora D Prakash Voutharoja",
        "Salim"
      ],
      "year": "2022",
      "venue": "SIGSPATIAL",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "T3S: Effective Representation Learning for Trajectory Similarity Computation",
      "authors": [
        "Peilun Yang",
        "Hanchen Wang",
        "Ying Zhang",
        "Lu Qin",
        "Wenjie Zhang",
        "Xuemin Lin"
      ],
      "year": "2021",
      "venue": "ICDE",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "A Neural Database for Differentially Private Spatial Range Queries",
      "authors": [
        "Sepanta Zeighami",
        "Ritesh Ahuja",
        "Gabriel Ghinita",
        "Cyrus Shahabi"
      ],
      "year": "2022",
      "venue": "PVLDB",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Towards a Painless Index for Spatial Objects",
      "authors": [
        "Rui Zhang",
        "Jianzhong Qi",
        "Martin Stradling",
        "Jin Huang"
      ],
      "year": "2014",
      "venue": "ACM Transactions on Database Systems",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "2. Related Work",
      "text": "We review studies on ML-based (spatial) database optimization. Existing works focus on using ML techniques to optimize the effectiveness and efficiency of different modules of spatial database systems, e.g., using ML models to replace (e.g., RSMI (Kang et al., 2017)) or to optimize (e.g., RLR-tree (Beng et al., 2017)) the structure of traditional spatial indices (Kang et al., 2018). A study (Beng et al., 2017) trains autoencoder models to compute _spatial embeddings_, i.e., vectors encoding dataset characteristics such as distribution to help predict range query selectivity for spatial query optimization. Other studies compute embeddings for spatial objects (e.g., road segments (Beng et al., 2017) or trajectories (Kang et al., 2018)) to encode their spatial features for spatial query processing. In these studies, the ML models are second-class citizens - they help retrieve query results but do not change the classic retrieval-based query paradigm. A few other studies use ML models to answer spatial queries directly. For example, Qi et al. (Qi et al., 2018) train a feedforward neural network (FFN) to predict the shortest-path distance given two points on a road network. Zeighami et al. (Zeighami et al., 2019) train FFNs to predict the answer for range count queries. While these studies show that ML models can memorize facts from spatial data, they focus on aggregate queries. Their models output scalar values and not data records. They do not have a natural language-based user interface. In a broader context of database research, there are text-to-SQL studies (Beng et al., 2017) that train ML models to translate textual queries into SQL queries, thus providing a natural language-based user interface. These models may exploit meta data such as column names of the data tables. However, they do not generate query results directly and typically do not access the actual data records at training. Motivated by the strong performance of LLMs, several vision papers (Kang et al., 2018; Kang et al., 2018; Kang et al., 2018) use pre-trained LLMs or _transformer_ (the building block of LLMs)-based models trained on unstructured data to answer database queries. These papers share similar visions with ours in that they also envisage ML models to become first-class citizens in a database system. They differ from our vision in that they do not consider structured spatial data and the challenges. Tan (Tan, 2018) presents several challenges on query processing over structured data with LLMs without envisaging a solution. A couple of other studies apply LLMs with structured data. Urban and Binnig (Urban and Binnig, 2018) extract tables from a document using LLMs, while Nobari and Rafiei (Nobari and Rafiei, 2018) transform tables into a desired representation for better joinability. They do not use the learned models to generate query answers directly. _Overall, none of these studies consider the specific challenges and opportunities brought by LLMs to spatial databases. Our paper fills this gap_. Musleh et al. (Musleh et al., 2018) envisage a BERT-based system for trajectory analysis, while Xue et al. (Xue et al., 2019; Zhang et al., 2019) use language models for time series forecasting, exploiting the analogy between trajectories/time series and sentences. Our study complements the studies by considering spatial data and queries beyond trajectories."
    },
    {
      "title": "3. Pilot Study",
      "text": ""
    },
    {
      "title": "The Vision Of The Future System",
      "text": "We envisage a next-generation spatial database system as shown in Fig. 1. This system consists of a _query analyzer and query plan generator_, a set of _query result generators_, and a _result synthesizer_, which are all formed by ML models and are connected together to generate answers for user queries. The system provides a natural language-based interface for users to query the spatial knowledge learned by the ML models from spatial data stored in the system. Users can interact with the system (e.g., via a computer or a smartphone) to submit queries in natural language. Upon receiving a query (e.g., _generate a half-day trip in Hamburg within walking distance from the conference venue of SIGSPATIAL'23_), the query analyzer and query plan generator (which may be an LLM) will analyze the query intent, generate sub-queries, and assign the sub-queries to the relevant subset of the query result generators (e.g., a sub-query to find the conference venue of SIGSPATIAL'23 and a sub-query to find POIs within walking distance from the conference venue). The invoked query result generators will generate an answer for each sub-query. Different types of query result generators will be built by training on different types of data. For example, a (transformer-based) query result generator trained on unstructured conference web pages will be able to answer the sub-query on the conference venue, while a (GAN-based, detailed in Section 3.2) query result generator trained on a table of POIs in Hamburg will be able to answer the sub-query about the POIs. When the results of all sub-queries have been generated, the result synthesizer (which may be another LLM) will combine them based on the user query and generate the final query answer to be returned to the user. Multiple challenges and research opportunities arise from the envisaged system, which will be discussed in Section 4."
    },
    {
      "title": "Preliminary Experimental Study",
      "text": "We verify the feasibility of the envisaged spatial database system through a preliminary experimental study. Figure 1. Overview of the future spatial database system **Settings.** We focus on structured data, since the aforementioned recent vision papers have shown the feasibility of using pre-trained LLMs or transformer-based models to answer certain types of database queries. We train an ML model on a data table and study how well the model can remember the (key) characteristics of the data. We use a multi-dimensional dataset (instead of a table of just spatial coordinates, for generality) named CensusIncome.2 The dataset has 48,842 records, each with 8 categorical (e.g., occupation and marital status) and 6 numeric (e.g., age and capital gain) attributes. Since the dataset does not come with a query workload, we follow a previous study (Krishnan et al., 2017) and generate 20,000 range queries with randomly selected numeric attributes and ranges. Footnote 2: [https://archive.ics.uci.edu/dataset/20/census-income](https://archive.ics.uci.edu/dataset/20/census-income) We use a generative adversarial network (GAN)-based model which has been shown to be able to generate tabular data (Krishnan et al., 2017). We train a GAN model with the dataset and test how well it can generate data records that preserve the data distribution and answer range count queries, i.e., given a query range, we return the number of records in the range. A GAN model has a _generator_ module \\(G\\) and a _discriminator_ module \\(D\\). The generator generates a record given a random noise vector, \\(\\mathbf{z}\\), as its input, while the discriminator classifies whether the generated record (i.e., \\(G(\\mathbf{z})\\)) resembles a real record from the training dataset. The model is trained with a loss function that aims to generate records that cannot be distinguished from the real records. We adapt the loss function of the generator to add a _Q-Error_(Krishnan et al., 2017) loss term, which measures how well a generated table preserves the selectivity of given range queries, as follows: \\[\\min_{G}\\mathcal{L}(G)=\\mathbb{E}_{\\mathbf{z}\\sim\\mathbf{p}_{\\mathbf{z}}(\\mathbf{z})}[\\log(1 -D(G(\\mathbf{z})))]+\\frac{1}{N}\\sum_{t}\\max\\left(1,\\frac{sel(q_{i})}{sel(q_{i})}, \\frac{sel(q_{i})}{sel(q_{i})}\\right)\\] The second term here is the Q-Error loss, where \\(N\\) denotes the number of range queries, \\(q_{i}\\) denotes the \\(i\\)th range query, \\(sel(q_{i})\\) denotes the ground truth selectivity of \\(q_{i}\\) on the training dataset, and \\(\\hat{sel}(q_{i})\\) denotes the selectivity of \\(q_{i}\\) on the generated table. We name the adapted GAN model **RC-GAN**. We omit the detailed model structure and hyperparameter values due to space limit. The experiments are run on a desktop computer with a 16-core CPU, 32 GB memory, and 24 GB GPU memory. **Results.** We train RC-GAN (implemented with PyTorch 1.13.1) on the CensusIncome dataset in 10 epochs (which take about an hour) and use the trained model to generate a table of the same size of the dataset. We report the Q-Error of the generated data in Table 1, where \"Training queries\" refers to computing the Q-Error with the 20,000 range queries as described above, which have been used in model training, while \"Testing queries\" refers to computing the Q-Error with another set of 5,000 range queries that are generated separately (with the same procedure) and have not been seen at training. We can see that the median Q-Errors are very close to 1 under both settings, i.e., the generated table has almost the same query selectivity as the original dataset for half of the queries. The Q-Errors at the 75th percentile are still within 2, while they only deteriorate to larger values at the 90th percentile. Importantly, the Q-Errors for the testing queries are close to those for the training queries. These results demonstrate the potential of ML models to \"memorize\" the key characteristics of structured data records. We further train two Gradient Boosting classifiers with 15% of the CensusIncome dataset and with 5% of the CensusIncome dataset plus 10% of data generated by RC-GAN, respectively. The classifiers predict if the income attribute of a record is greater than 50,000 given the other attributes. We test the classifiers on 1,000 randomly selected records of CensusIncome not seen at training. Table 2 reports the results. We see that the classifiers trained under both settings have very close performance, confirming the capability of RC-GAN to \"memorize\" the data distribution characteristics. Our results above are obtained with an ML model where the number of parameters is at the thousand scale. When larger models with more parameters are available, even better results are expected. **Learning spatial knowledge with LLMs.** To provide further evidence on LLMs' potential to learn spatial knowledge, we query ChatGPT with prompts: _the geo-coordinates of the top 50 cities in Australia are_ and _can you give me more cities_, until 50 cities were returned. The returned geo-coordinates were correct for 49 cities, with only the geo-coordinates of Hervey Bay (a small city in Queensland) being off by 10 km. We further randomly pair up the cities to form 50 pairs. For every pair of cities \\(A\\) and \\(B\\), we query ChatGPT with prompt: \\(A\\)_is to which side of \\(B\\)_. The returned position results were correct for 44 pairs, with another 5 pairs obtained correct results after the geo-coordinates are further included in the prompt. Only one pair (Canberra and Orange) retained a wrong result (_south_west was returned while the answer should be _south_). These demonstrate the potential of LLMs to learn spatial knowledge and answer queries, and the research opportunities to train such models to answer more complex queries faithfully."
    },
    {
      "title": "4. Conclusions And Challenges",
      "text": "We presented a next-generation spatial database system. This system treats ML models as first-class citizens and trains such models to \"memorize\" data stored in a spatial database and to generate query answers. It enables a new generation-based query paradigm that replaces the traditional retrieval-based paradigm. The system will significantly enhance the accessibility of spatial database systems, as the ML models can offer an inbuilt natural language-based user interface and well understand users' query needs. It will bring huge benefits in spatial analytics and query processing, encouraging a new generation of location-based services and allowing better-informed location-based decision making. To realize such a system, there are various challenges, a subset of which are summarized below. Simply fine-tuning an open-sourced LLM such as llama 2 directly cannot realize the system. \\begin{table} \\begin{tabular}{l c c c} \\hline \\hline Query set & Median & 75th & 90th \\\\ \\hline Training queries & 1.23 & 1.71 & 3.24 \\\\ Testing queries & 1.25 & 1.86 & 4.08 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1. Q-Error of RC-GAN \\begin{table} \\begin{tabular}{l c c} \\hline \\hline Training data & Precision & Recall & F1 \\\\ \\hline 5\\% of CensusIncome + 10\\% of RC-GAN & 0.79 & 0.97 & 0.87 \\\\ 15\\% of CensusIncome & 0.82 & 0.96 & 0.88 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2. Classification Accuracy with Generated Data(1) _Faithful query result generation_. Being able to generate query results directly without an extra data retrieval process offers great opportunities to answer complex spatial (analytical) queries. This, however, also brings significant challenges to ensure the faithfulness of the results generated. ML models are known to return inaccurate results. In terms of LLMs, _hallucination_ is a known problem that impinges LLMs' wider applicability. When LLMs are applied to form the query engine of spatial databases, it is important to address the hallucination problem, e.g., to build a system that returns faithfulness scores together with the generated answers. A unique opportunity arises when building such a system for spatial databases, as traditional retrieval-based query procedures can be applied in parallel to compute query answers that serve as the ground truth for training the faithfulness scoring module. (2) _Large model training with structured spatial data_. There are two major issues that prevent training LLMs on structured data records directly (which are probably the reason why the other vision papers [15; 17; 18]) did not take this approach): (i) There is limited availability of structured spatial data. Comparing with the volume of free texts (e.g., web documents), the number of spatial data tables available is much smaller. The number of data tables in a spatial database is even smaller. How to train an LLM generalizable to different queries with data in such smaller scale is challenging. (ii) There is incompatibility between structured spatial data and the training procedure of LLMs. LLMs are trained via the task of predicting the next word in a sentence. Simply treating every spatial data record as a sentence and every data field as a word to train an LLM is ineffective. This is because words in a sentence have a strong correlation, and the context of a word implies the semantics of the word. In contrast, different fields of a data record may be much less relevant, and the nearby fields of a value do not necessarily imply the semantics of the value. Further, values in a data record may be numeric and continuous, and the same value may have completely different meanings in different fields, while words are discrete and each word has much fewer different meanings. Novel model design and training procedures are needed for structured spatial data. (3) _Versatile query processing_. A problem related to the difficulty in model training given limited structured spatial data is how to answer different types of spatial queries using a model trained with limited data. While data of limited scale may be easier to be \"memorized\" by an ML model, it does not help train a model that is generalizable to different types of queries. Also due to the limited scale of data, the trained models may not have seen too many different prompts that imply different types of queries. The generalizability of the trained models would most likely need to come from unstructured spatial data, e.g., the Wikipedia article of a POI. Algorithms to fine-tune such models and incorporate knowledge from structured spatial data await exploration. Further, the models need to have multi-step reasoning capabilities to answer complex spatial queries. For example, to \"generate a half-day trip in Hamburg within walking distance from the conference venue of SIGSPATIAL'23\" would require (i) producing the location of the conference venue, (ii) producing POIs within walking distance around it, and (iii) selecting and ordering the POIs to form a trip. While prompting, training, and fine-tuning strategies have been proposed for this issue, achieving such advanced reasoning capabilities remains an open challenge [3]. (4) _Challenges in managing ML models for data management_. There are inherent problems in data management with ML models, such as how to update the trained ML models when the underlying data have changed (e.g., moving objects [20; 21]). Such challenges have been discussed in the literature [15; 18] and are not reiterated."
    },
    {
      "title": "Acknowledgements.",
      "text": "This work is partially supported by Australian Research Council (ARC) Discovery Project DP230101534."
    },
    {
      "title": "References",
      "text": "* (1) * Belussi et al. (2022) Alberto Belussi, Sara Migliorini, and Ahmed Eldawy. 2022. Spatial Embedding: A Generic Machine Learning Model for Spatial Query Optimization. In _SIGSPATIAL_. * Chang et al. (2012) Yanchuan Chang, Eggeman Tanin, Xin Cao, and Jianzhong Qi. 2012. Spatial Structure-Aware Road Network Embedding via Graph Contrastive Learning. In _EDBT_. * Dziri et al. (2023) Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Roman Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ben, Allyson Etinger, Zaid Harchaoui, and Yeip Choi. 2023. Faihi and Face: Limits of Transformers on Compositionality. In _NeurIPS_. * Gu et al. (2023) Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, and Sheng Wang. 2023. The RLR-Tree: A Reinforcement Learning Based R-Tree for Spatial Data. _Proceedings of the ACM on Management of Data_ (2023). * Katogiannis-Meinars and Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koer Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koita Koerita Koerita Koerita Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koita Koerita Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koita Koerita Koerita Koerita Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koitaerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koita Koerita Koerita Koerita Koerita Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koer Koita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koer Koita Koer Koita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koerita Koerita Koerita Koer Koerita Koerita Koerita Koer Koita Koer Koita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koer Koita Koerita Koer Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koer Koerita Koer Koita Koerita Koer Koita Koerita Koerita Koerita Koerita Koerita Koer Koita Koerita Koer Koerita Koer Koerita Koerita Koerita Koer Koita Koerita Koer Koita Koerita Koerita Koer Koerita Koerita Koer Koerita Koerita Koer Koerita Koer Koita Koer Koerita Koer Koita Koer Koita Koer Koerita Koerita Koerita Koer Koer Koerita Koer Koerita Koer Koita Koer Koerita Koer Koerita Koer Koita Koer Koer Koita Koerita Koer Koerita Koer Koita Koer Koerita Koer Koerita Koer Koita Koer Koer Koita Koer Koerita Koerita Koer Koer Koita Koer Koerita Koer Koerita Koer Koita Koer Koerita Koer Koer Koita Koer Koerita Koerita Koer Koerita Koer Koerita Koer Koerita Koer Koerita Koer Koerita Koer Koer Koita Koer Koerita Koer Koer Koita Koer Koer Koita Koer Koerita Koer Koer Koita Koer Koerita Koer Koer Koer Koita Koer Koer Koita Koer Koer Koita Ko"
    }
  ]
}