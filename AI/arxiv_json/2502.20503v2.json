{
  "title": "Protecting multimodal large language models against misleading visualizations",
  "authors": [
    "Jonathan Tonglet",
    "Tinne Tuytelaars",
    "Marie-Francine Moens",
    "Iryna Gurevych"
  ],
  "abstract": "\n We assess the vulnerability of multimodal large language models to misleading visualizations -charts that distort the underlying data using techniques such as truncated or inverted axes, leading readers to draw inaccurate conclusions that may support misinformation or conspiracy theories. Our analysis shows that these distortions severely harm multimodal large language models, reducing their question-answering accuracy to the level of the random baseline. To mitigate this vulnerability, we introduce six inference-time methods to improve performance of MLLMs on misleading visualizations while preserving their accuracy on non-misleading ones. The most effective approach involves (1) extracting the underlying data table and (2) using a text-only large language model to answer questions based on the table. This method improves performance on misleading visualizations by 15.4 to 19.6 percentage points. \n",
  "references": [
    {
      "id": null,
      "title": "Protecting multimodal large language models against misleading visualizations",
      "authors": [
        "Jonathan Tonglet",
        "Tinne Tuytelaars",
        "Marie-Francine Moens",
        "Iryna Gurevych"
      ],
      "year": "2025",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Misinformed by visualization: What do we learn from misinformative visualizations?",
      "authors": [
        "L Y Lo",
        "-H Gupta",
        "A Shigyo",
        "K Wu",
        "A Bertini",
        "E Qu",
        "H"
      ],
      "year": "2022",
      "venue": "Computer Graphics Forum",
      "doi": "10.1111/cgf.14559"
    },
    {
      "id": "b1",
      "title": "How deceptive are deceptive visualizations? an empirical analysis of common distortion techniques",
      "authors": [
        "A V Pandey",
        "K Rall",
        "M L Satterthwaite",
        "O Nov",
        "E Bertini"
      ],
      "year": "2015",
      "venue": "Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. CHI '15",
      "doi": "10.1145/2702123.2702608"
    },
    {
      "id": "b2",
      "title": "Misleading beyond visual tricks: How people actually lie with charts",
      "authors": [
        "M Lisnic",
        "C Polychronis",
        "A Lex",
        "M Kogan"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. CHI '23. Association for Computing Machinery",
      "doi": "10.1145/3544548.3580910"
    },
    {
      "id": "b3",
      "title": "i came across a junk\": Understanding design flaws of data visualization from the public's perspective",
      "authors": [
        "X Lan",
        "Y Liu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "doi": "10.1109/TVCG.2024.3456341"
    },
    {
      "id": "b4",
      "title": "Surfacing visualization mirages",
      "authors": [
        "A Mcnutt",
        "G Kindlmann",
        "M Correll"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. CHI '20",
      "doi": "10.1145/3313831.3376420"
    },
    {
      "id": "b5",
      "title": "The deceptive potential of common design tactics used in data visualizations",
      "authors": [
        "C Lauer",
        "S O'brien"
      ],
      "year": "2020",
      "venue": "Proceedings of the 38th ACM International Conference on Design of Communication. SIGDOC '20",
      "doi": "10.1145/3380851.3416762"
    },
    {
      "id": "b6",
      "title": "Truncating bar graphs persistently misleads viewers",
      "authors": [
        "B W Yang",
        "C Vargas Restrepo",
        "M L Stanley",
        "E J Marsh"
      ],
      "year": "2021",
      "venue": "Journal of Applied Research in Memory and Cognition",
      "doi": "10.1016/j.jarmac.2020.10.002"
    },
    {
      "id": "b7",
      "title": "Calvi: Critical thinking assessment for literacy in visualizations",
      "authors": [
        "L W Ge",
        "Y Cui",
        "M Kay"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. CHI '23",
      "doi": "10.1145/3544548.3581406"
    },
    {
      "id": "b8",
      "title": "ChartQA: A benchmark for question answering about charts with visual and logical reasoning",
      "authors": [
        "A Masry",
        "X L Do",
        "J Q Tan",
        "S Joty",
        "E Hoque"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
      "doi": "10.18653/v1/2022.findings-acl.177"
    },
    {
      "id": "b9",
      "title": "OpenAI: Gpt-4 technical report",
      "authors": [],
      "year": "2023",
      "venue": "OpenAI: Gpt-4 technical report",
      "doi": "10.48550/arXiv.2303.08774"
    },
    {
      "id": "b10",
      "title": "An empirical evaluation of the gpt-4 multimodal language model on visualization literacy tasks",
      "authors": [
        "A Bendeck",
        "J Stasko"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "doi": "10.1109/TVCG.2024.3456155"
    },
    {
      "id": "b11",
      "title": "Chartom: A visual theoryof-mind benchmark for multimodal large language models",
      "authors": [
        "S Bharti",
        "S Cheng",
        "J Rho",
        "M Rao",
        "X Zhu"
      ],
      "year": "2024",
      "venue": "Chartom: A visual theoryof-mind benchmark for multimodal large language models",
      "doi": "10.48550/arXiv.2408.14419"
    },
    {
      "id": "b12",
      "title": "Vlat: Development of a visualization literacy assessment test",
      "authors": [
        "S Lee",
        "S.-H Kim",
        "B C Kwon"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "doi": "10.1109/TVCG.2016.2598920"
    },
    {
      "id": "b13",
      "title": "Qwen2.5 technical report",
      "authors": [
        "Q Team"
      ],
      "year": "2024",
      "venue": "Qwen2.5 technical report",
      "doi": "10.48550/arXiv.2412.15115"
    },
    {
      "id": "b14",
      "title": "How good (or bad) are llms at detecting misleading visualizations?",
      "authors": [
        "L Y Lo",
        "-H Qu",
        "H"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "doi": "10.1109/TVCG.2024.3456333"
    },
    {
      "id": "b15",
      "title": "Can gpt-4 models detect misleading visualizations?",
      "authors": [
        "J Alexander",
        "P Nanda",
        "K.-C Yang",
        "A Sarvghad"
      ],
      "year": "2024",
      "venue": "2024 IEEE Visualization and Visual Analytics (VIS)",
      "doi": "10.1109/VIS55277.2024.00029"
    },
    {
      "id": "b16",
      "title": "Advancing multimodal large language models in chart question answering with visualization-referenced instruction tuning",
      "authors": [
        "X Zeng",
        "H Lin",
        "Y Ye",
        "W Zeng"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "doi": "10.1109/TVCG.2024.3456159"
    },
    {
      "id": "b17",
      "title": "TinyChart: Efficient chart understanding with program-of-thoughts learning and visual token merging",
      "authors": [
        "L Zhang",
        "A Hu",
        "H Xu",
        "M Yan",
        "Y Xu",
        "Q Jin",
        "J Zhang",
        "F Huang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2024.emnlp-main.112"
    },
    {
      "id": "b18",
      "title": "ChartGemma: Visual instruction-tuning for chart reasoning in the wild",
      "authors": [
        "A Masry",
        "M Thakkar",
        "A Bajaj",
        "A Kartha",
        "E Hoque",
        "S Joty",
        "O Rambow",
        "L Wanner",
        "M Apidianaki",
        "H Al-Khalifa",
        "B D Eugenio",
        "S Schockaert",
        "K Darwish"
      ],
      "year": "2025",
      "venue": "Proceedings of the 31st International Conference on Computational Linguistics: Industry Track",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "DePlot: One-shot visual language reasoning by plot-to-table translation",
      "authors": [
        "F Liu",
        "J Eisenschlos",
        "F Piccinno",
        "S Krichene",
        "C Pang",
        "K Lee",
        "M Joshi",
        "W Chen",
        "N Collier",
        "Y Altun"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": "10.18653/v1/2023.findings-acl.660"
    },
    {
      "id": "b20",
      "title": "Improved baselines with visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Y Li",
        "Y J Lee"
      ],
      "year": "2024",
      "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR52733.2024.02484"
    },
    {
      "id": "b21",
      "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "authors": [
        "P Wang",
        "S Bai",
        "S Tan",
        "S Wang",
        "Z Fan",
        "J Bai",
        "K Chen",
        "X Liu",
        "J Wang",
        "W Ge",
        "Y Fan",
        "K Dang",
        "M Du",
        "X Ren",
        "R Men",
        "D Liu",
        "C Zhou",
        "J Zhou",
        "J Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "doi": "10.48550/arXiv.2409.12191"
    },
    {
      "id": "b22",
      "title": "Ovis: Structural embedding alignment for multimodal large language model",
      "authors": [
        "S Lu",
        "Y Li",
        "Q.-G Chen",
        "Z Xu",
        "W Luo",
        "K Zhang",
        "H.-J Ye"
      ],
      "year": "2024",
      "venue": "Ovis: Structural embedding alignment for multimodal large language model",
      "doi": "10.48550/arXiv.2405.20797"
    },
    {
      "id": "b23",
      "title": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
      "authors": [
        "Z Chen",
        "W Wang",
        "Y Cao",
        "Y Liu",
        "Z Gao",
        "E Cui",
        "J Zhu",
        "S Ye",
        "H Tian",
        "Z Liu",
        "L Gu",
        "X Wang",
        "Q Li",
        "Y Ren",
        "Z Chen",
        "J Luo",
        "J Wang",
        "T Jiang",
        "B Wang",
        "C He",
        "B Shi",
        "X Zhang",
        "H Lv",
        "Y Wang",
        "W Shao",
        "P Chu",
        "Z Tu",
        "T He",
        "Z Wu",
        "H Deng",
        "J Ge",
        "K Chen",
        "K Zhang",
        "L Wang",
        "M Dou",
        "L Lu",
        "X Zhu",
        "T Lu",
        "D Lin",
        "Y Qiao",
        "J Dai",
        "W Wang"
      ],
      "year": "2024",
      "venue": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
      "doi": "10.48550/arXiv.2412.05271"
    },
    {
      "id": "b24",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz",
        "J Davison",
        "S Shleifer",
        "P Platen",
        "C Ma",
        "Y Jernite",
        "J Plu",
        "C Xu",
        "T Le Scao",
        "S Gugger",
        "M Drame",
        "Q Lhoest",
        "A Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/2020.emnlp-demos.6"
    },
    {
      "id": "b25",
      "title": "Note on the sampling error of the difference between correlated proportions or percentages",
      "authors": [
        "Q Mcnemar"
      ],
      "year": "1947",
      "venue": "Psychometrika",
      "doi": "10.1007/BF02295996"
    }
  ],
  "sections": [
    {
      "title": "Protecting Multimodal Large Language Models",
      "text": "against misleading visualizations Jonathan Tonglet Tinne Tuytelaars Department of Electrical Engineering, KU Leuven. Marie-Francine Moens Department of Computer Science, KU Leuven. Iryna Gurevych Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science and Hessian Center for AI (hessian.AI), TU Darmstadt."
    },
    {
      "title": "Abstract",
      "text": "We assess the vulnerability of multimodal large language models to misleading visualizations - charts that distort the underlying data using techniques such as truncated or inverted axes, leading readers to draw inaccurate conclusions that may support misinformation or conspiracy theories. Our analysis shows that these distortions severely harm multimodal large language models, reducing their question-answering accuracy to the level of the random baseline. To mitigate this vulnerability, we introduce six inference-time methods to improve performance of MLLMs on misleading visualizations while preserving their accuracy on non-misleading ones. The most effective approach involves (1) extracting the underlying data table and (2) using a text-only large language model to answer questions based on the table. This method improves performance on misleading visualizations by 15.4 to 19.6 percentage points.1 Footnote 1: Code and data available at: github.com/UKPLab/arxiv2025-misleading-visualizations **Keywords: large language models, question answering, visualization** Visualizations are widely used to convey data insights efficiently. However, design flaws - whether intentional or not - can distort the correct interpretation of the underlying data [1, 2, 3, 4, 5, 6, 7]. These design flaws, or misleaders, include truncated, inverted, and dual axes, 3D effects, or inconsistent tick intervals. Misleading visualizations pose a serious threat to our society, as they are used to support misinformation and conspiracytheories [3]. Figure 1 shows real-world examples of misleading visualizations used to misinform readers on sensitive topics such as access to safe drinking water, politics, COVID-19, or abortion statistics. The CALVI test [8] demonstrated the impact of misleading visualizations in decreasing human readers' ability to interpret the underlying data accurately. The visualization reasoning abilities of multimodal large language models (MLLMs) have advanced rapidly, as demonstrated on the reference benchmark ChartQA [9], suggesting their potential as chart reasoning assistants. However, it remains uncertain whether they are sensitive to misleaders, as humans are. If vulnerable, MLLMs used as chart reasoning assistants risk amplifying both the spread of misinformation and human belief in it. Recent work has reported a sensitivity of GPT4 [10] to misleaders, but their analysis was limited to a single MLLM evaluated on six misleading visualizations [11]. In this study, we compare the question-answering (QA) performance of 16 MLLMs of varying sizes across three datasets: (a) a misleading visualization dataset containing \\(n=143\\) instances and featuring 17 distinct misleaders, defined in Table A1. It combines the existing synthetic CALVI (\\(n=45\\)) [8] and CHARTOM (\\(n=56\\)) [12] datasets with Figure 1: Four examples of real-world misleading visualizations [1] with QA pairs. The correct answer is colored in green, while the wrong answer supported by the misleader is colored in purple. [MISSING_PAGE_FAIL:3] and (6) using a text-only LLM to generate code for a new visualization based on the extracted table, replacing the original one as input."
    },
    {
      "title": "1 Results",
      "text": ""
    },
    {
      "title": "Assessing The Vulnerability To Misleading Visualizations",
      "text": "Figure 3 presents the QA accuracy of 16 MLLMs across the three datasets. The results reveal three key trends. First, MLLMs perform substantially worse on misleading visualizations than on non-misleading ones, with accuracy dropping by up to 34.8 percentage points. The decline is even more pronounced compared to ChartQA, reaching up to 65.5 percentage points. Furthermore, the mean MLLM accuracy on misleading visualizations (24.8%) is close to the random baseline (25.6%). This suggests that MLLMs struggle to correctly interpret the distorted data, making them highly vulnerable to misleaders. This aligns with findings from the CALVI study [8], where human accuracy averaged 80% on non-misleading visualizations but dropped to 39% on misleading ones. Second, accuracy on misleading visualizations does not follow the upward performance trend observed in ChartQA, and to a lower extent, non-misleading visualizations. From these results, we conclude that reducing MLLM vulnerability to misleading visualizations will not happen naturally as a by-product of improving performance on standard benchmarks like ChartQA. This makes dedicated mitigation methods all the more necessary. Third, the best-performing MLLMs on misleading visualizations, GPT-4o and InternVL2.5-38B, primarily outperform other MLLMs on the real-world subset. We assume this is due to their large parametric knowledge, which spans beyond the end date of the real-world subset (2022). This knowledge about real-world events and statistics allows them to answer some questions without leveraging the visualization. However, this advantage does not extend to synthetic data in CALVI and CHARTOM."
    },
    {
      "title": "Mitigating The Impact Of Misleading Visualizations",
      "text": "We evaluate the impact of the correction methods with three open-weight, mid-sized MLLMs which all perform below the random baseline on misleading visualizations: Qwen2VL-7B, Ovis1.6-9B, and InternVL2.5-8B. Figure 4 presents the change in accuracy across the six correction methods: (1) inclusion of a warning message, inclusion of the (2) axes, (3) table, or (4) both, (5) table-based QA, and (6) redrawing the visualization. Qwen2.5-7B [14] serves as the text-only LLM for table-based QA and redrawing the visualization. The most effective approach by far is table-based QA, yielding significant improvements of 15.4 to 19.6 percentage points. While incorrect or incomplete table extractions decrease performance on non-misleading visualizations -- particularly for line charts, scatter plots, and maps -- these losses are not significant. Another promising correction method is redrawing the visualization. It achieves significant but more modest improvements for two MLLMs. This approach is effective only when the generated code compiles on a Python interpreter; otherwise, it defaultsto using the original visualization. Across both datasets, the lowest redrawing success rates are observed for scatter plots (79%), and stacked bar charts (80%), which we explain by their high number of visual items and complex layouts, while the success rates for all other chart types are above 90%. By either removing or modifying the visualization, these two methods effectively neutralize misleaders that exploit visual perception errors, such as inverted axes and inconsistent tick intervals. In Figure 2, redrawing the visualization corrects the misleading trend line, eliminating its deceptive elements and ensuring a more accurate representation of the data. Other correction methods do not yield significant improvements. Including the extracted table in the prompt even has a significant negative impact on non-misleading visualizations for one MLLM. Figure 3: Top: Accuracy (%) of various MLLMs on misleading visualization [1; 8; 12], non-misleading visualization [8; 12; 13], and ChartQA datasets [9]. The horizontal dashed line indicates the accuracy of the random baseline on misleading visualizations. Models are sorted by increasing accuracy on ChartQA. Bottom: Accuracy (%) of various MLLMs on subsets of the misleading visualizations. Adding a warning message provides non-significant changes of at most 4.9 percentage points, with most improvements observed on the real-world subset. This method assumes prior knowledge of the misleader present in the visualization, making the reported results an upper bound on its effectiveness. In practice, a classifier needs to detect first the presence of misleaders, which remains a challenging task [15, 16], as MLLMs struggle not only to distinguish non-misleading visualizations from misleading ones but also to correctly identify the specific misleader in a given visualization. Given the already low results obtained with ground-truth misleader labels, this correction method appears unpromising overall. However, training a highly accurate misleader detection model could enable the selective application of other correction methods, eliminating their negative impact on non-misleading visualizations. We examine further the quality of the intermediate table extraction step using CHARTOM [12], which pairs each question with two visualizations of the same data -- one misleading and one non-misleading. Ideally, the extracted tables should be identical for both. However, across all MLLMs, the extracted tables have a perfect match in only 4 out of 56 pairs (7.1%), and a partial match for 13 to 14 other pairs (23 to 25%). Notably, 3D effects achieve their deceptive purpose, as MLLMs systematically produce different tables when the visualization is presented with or without 3D effects. Since table-based QA and redrawing the visualization highly depend on this intermediate table extraction step, improving it is an important direction for future work. Figure 4: Change (\\(\\Delta\\)) in accuracy (percentage points) on misleading and non-misleading visualization datasets using different inference-time correction methods to mitigate misleading visualizations. Statistically significant changes (p\\(\\leq\\)0.05) are hashed."
    },
    {
      "title": "2 Discussion",
      "text": "Our findings highlight the vulnerability of MLLMs to misleading visualizations, reducing their QA performance compared to non-misleading ones, down to the level of the random baseline. To address this vulnerability, we explored six inference-time correction methods. The most effective approach involves extracting the underlying table and performing the QA task using only the table, improving accuracy by up to 19.6 percentage points. While recent work in visualization comprehension with MLLMs primarily treats visualizations as images [17, 18, 19], our findings challenge this direction and underscore the merits of earlier approaches, such as DePlot [20], which use table extraction as an intermediate step, followed by table-based QA with a text-only model. By making MLLMs more robust to misleading visualizations, correction methods help ensure that AI-assisted chart reasoning does not yield inaccurate interpretations of the underlying data, reducing the risk that human users believe in and propagate misinformation. We identify two limitations to our work. First, the visualization redrawing method, which uses the package Matplotlib, does not support maps, as Matplotlib lacks sufficient functionality for rendering high-quality maps. Second, we assume prior knowledge of the chart type (bar, line,...) for generating the prompts for axes extraction and visualization redrawing. This is a reasonable assumption, as the chart type can either be provided by a human user or accurately predicted by a classifier as a preprocessing step."
    },
    {
      "title": "3 Methods",
      "text": ""
    },
    {
      "title": "Datasets",
      "text": "The misleading and non-misleading visualization datasets combine three existing resources and one introduced in this work. First, CALVI [8] includes 45 misleading and 15 non-misleading visualizations based on synthetic data, each paired with a multiple-choice question (MCQ) with three to four choices. Second, CHARTOM [12] contains 56 samples, including 28 MCQs, 20 free-text questions, and 8 ranking questions. The MCQs provide two to four choices. CHARTOM is the only dataset where each question is linked to two visualizations -- one misleading and one non-misleading. Like CALVI, the underlying data is synthetic. Third, VLAT [13], the reference dataset to evaluate the human comprehension of visualizations, provides 12 non-misleading visualizations, each paired with three to seven MCQs, for a total of 53 instances. The visualizations are based on real-world data. MCQs have two to four choices. Fourth, we introduce a dataset of 42 real-world misleading visualizations, each annotated with a MCQ with three to four choices. The real-world visualizations come from a collection annotated with misleader labels [1], which inspired the synthetic examples in CALVI. We manually create MCQs, using those from CALVI and CHARTOM as inspiration. The motivation for creating this additional resource is that CALVI and CHARTOM rely both on synthetic data. By incorporating questions about real-world data, we introduce direct conflicts with MLLMs' parametric knowledge, allowing us to assess their vulnerability in real-world scenarios. We also report the performance of MLLMs on the test set of ChartQA [9], which contains 2500 real-world visualizations paired with free-text questions, of which half are human-written and the others are AI-generated. The datasets cover together 11 chart types: line charts, area charts, stacked area charts, bar charts, stacked bar charts, histograms, pie charts, scatter plots, bubble charts, maps, treemaps. For free-text questions where the expected answer is a number, we adopt the relaxed accuracy metric of ChartQA [9]. Specifically, a prediction is considered correct if it falls within a \\(\\pm 5\\%\\) interval of the ground truth. For MCQs, we present the choices in their default order. In Table B2, we report the mean accuracy of the MLLMs over three different choice orders. The choices are shuffled using three randomly generated numbers as seeds (654, 114, 25). Standard deviations for the accuracy range from 0.0 to 4.06 percentage points."
    },
    {
      "title": "Inference With (M)Llms",
      "text": "We evaluate 11 open-weight MLLMs from the Llava-Next [21], Qwen2VL [22], Ovis-1.6 [23], and InternVL2.5 [24] families, ranging from 2 to 38 billion parameters. Additionally, we include two commercial models with undisclosed number of parameters, GPT-4 and GPT-4o [10], as well as three open-weight MLLMs specifically trained for visualization reasoning: ChartInstruction [17], TinyChart [18], and ChartGamma [19] ranging from 3 to 13 billion parameters. We use the Hugging Face transformers [25] implementation for all open-weight (M)LLMs and access GPT models via the Azure OpenAI API. To ensure deterministic outputs, we fix the random seed at 42, set the temperature to 0, and use a top-p value of 1. Following the standard ChartQA evaluation setup, all (M)LLMs are prompted in a zero-shot manner. For TinyChart, we report results using the Direct approach [18]."
    },
    {
      "title": "Implementation Of Correction Methods",
      "text": "All prompts are in the code, which is provided as supplementary material. The significance of correction methods is assessed using the McNemar test (p \\(\\leq\\) 0.05) [26]. The p-values are reported in Table C3. **Misleader warning**: we insert in the prompt a short warning message based on the definitions of misleaders. The message is the same for all instances with the same type of misleader. There are no messages for the five types of misleaders where the visualization is deceiving only in the context of a specific question, making a standardized warning message impossible: cherry picking, misleading annotations, concealed uncertainty, missing normalization, and missing data. **Axes and table extraction**: we prompt the MLLM to extract the axes or underlying table in a zero-shot setting. The axes and tables are formatted as text strings. We do not impose constraints on the delimiters used to indicate new rows and columns. **Table-based QA**: the extracted table is provided as input with the question and an instruction to answer it based on the table. **Redrawn visualization**: the text-only LLM receives the extracted table and the chart type as text input and generates Python code to create a visualization usingMatplotlib. The code is then executed to produce a new visualization. If the code compiles successfully, the newly generated visualization replaces the original one in the QA prompt; otherwise, the original visualization remains in use. Acknowledgements.This work has been funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY center (Grant Number: LOEWE/1/12/519/03/05.001(0016)/72) and by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE. We gratefully acknowledge the support of Microsoft with a grant for access to OpenAI GPT models via the Azure cloud (Accelerate Foundation Model Academic Research). We want to express our gratitude to Niklas Traser for conducting an initial exploration of the real-world data, to Jan Zimny for our insightful discussions on the topic of misleading visualizations, and to German Ortiz, Manisha Venkat, and Max Glockner for their feedback on a draft of this work. [MISSING_PAGE_EMPTY:10] [MISSING_PAGE_EMPTY:11]"
    },
    {
      "title": "References",
      "text": "* [1] Lo, L.Y.-H., Gupta, A., Shigyo, K., Wu, A., Bertini, E., Qu, H.: Misinformed by visualization: What do we learn from misinformative visualizations? In: Computer Graphics Forum, vol. 41, pp. 515-525 (2022). [https://doi.org/10.1111/cgf.14559](https://doi.org/10.1111/cgf.14559). Wiley Online Library * [2] Pandey, A.V., Rall, K., Satterthwaite, M.L., Nov, O., Bertini, E.: How deceptive are deceptive visualizations? an empirical analysis of common distortion techniques. In: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. CHI '15, pp. 1469-1478. Association for Computing Machinery, New York, NY, USA (2015). [https://doi.org/10.1145/2702123.2702608](https://doi.org/10.1145/2702123.2702608) * [3] Lisnic, M., Polychronis, C., Lex, A., Kogan, M.: Misleading beyond visual tricks: How people actually lie with charts. In: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. CHI '23. Association for Computing Machinery, New York, NY, USA (2023). [https://doi.org/10.1145/3544548.3580910](https://doi.org/10.1145/3544548.3580910) * [4] Lan, X., Liu, Y.: \"i came across a junk\": Understanding design flaws of data visualization from the public's perspective. IEEE Transactions on Visualization and Computer Graphics **31**(1), 393-403 (2025) [https://doi.org/10.1109/TVCG.2024.3456341](https://doi.org/10.1109/TVCG.2024.3456341) * [5] McNutt, A., Kindlmann, G., Correll, M.: Surfacing visualization mirages. In: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. CHI '20, pp. 1-16. Association for Computing Machinery, New York, NY, USA (2020). [https://doi.org/10.1145/3313831.3376420](https://doi.org/10.1145/3313831.3376420) * [6] Lauer, C., O'Brien, S.: The deceptive potential of common design tactics used in data visualizations. In: Proceedings of the 38th ACM International Conference on Design of Communication. SIGDOC '20. Association for Computing Machinery, New York, NY, USA (2020). [https://doi.org/10.1145/3380851.3416762](https://doi.org/10.1145/3380851.3416762) * [7] Yang, B.W., Vargas Restrepo, C., Stanley, M.L., Marsh, E.J.: Truncating bar graphs persistently misleads viewers. Journal of Applied Research in Memory and Cognition **10**(2), 298-311 (2021) [https://doi.org/10.1016/j.jarmac.2020.10.002](https://doi.org/10.1016/j.jarmac.2020.10.002) * [8] Ge, L.W., Cui, Y., Kay, M.: Calvi: Critical thinking assessment for literacy in visualizations. In: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. CHI '23. Association for Computing Machinery, New York, NY, USA (2023). [https://doi.org/10.1145/3544548.3581406](https://doi.org/10.1145/3544548.3581406) * [9] Masry, A., Do, X.L., Tan, J.Q., Joty, S., Hoque, E.: ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In: Muresan, S., Nakov, P., Villavicencio, A. (eds.) Findings of the Association forComputational Linguistics: ACL 2022, pp. 2263-2279. Association for Computational Linguistics, Dublin, Ireland (2022). [https://doi.org/10.18653/v1/2022.findings-acl.177](https://doi.org/10.18653/v1/2022.findings-acl.177) * [10] OpenAI: Gpt-4 technical report. Technical report, OpenAI (2023). [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774) * [11] Bendeck, A., Stasko, J.: An empirical evaluation of the gpt-4 multimodal language model on visualization literacy tasks. IEEE Transactions on Visualization and Computer Graphics **31**(1), 1105-1115 (2025) [https://doi.org/10.1109/TVCG.2024.3456155](https://doi.org/10.1109/TVCG.2024.3456155) * [12] Bharti, S., Cheng, S., Rho, J., Rao, M., Zhu, X.: Chartom: A visual theory-of-mind benchmark for multimodal large language models. arXiv preprint arXiv:2408.14419 **abs/2408.14419** (2024) [https://doi.org/10.48550/arXiv.2408.14419](https://doi.org/10.48550/arXiv.2408.14419) * [13] Lee, S., Kim, S.-H., Kwon, B.C.: Vlat: Development of a visualization literacy assessment test. IEEE Transactions on Visualization and Computer Graphics **23**(1), 551-560 (2017) [https://doi.org/10.1109/TVCG.2016.2598920](https://doi.org/10.1109/TVCG.2016.2598920) * [14] Team, Q.: Qwen2.5 technical report. Technical report, Alibaba Cloud (2024). [https://doi.org/10.48550/arXiv.2412.15115](https://doi.org/10.48550/arXiv.2412.15115) * [15] Lo, L.Y.-H., Qu, H.: How good (or bad) are llms at detecting misleading visualizations? IEEE Transactions on Visualization and Computer Graphics **31**(1), 1116-1125 (2025) [https://doi.org/10.1109/TVCG.2024.3456333](https://doi.org/10.1109/TVCG.2024.3456333) * [16] Alexander, J., Nanda, P., Yang, K.-C., Sarvghad, A.: Can gpt-4 models detect misleading visualizations? In: 2024 IEEE Visualization and Visual Analytics (VIS), pp. 106-110 (2024). [https://doi.org/10.1109/VIS55277.2024.00029](https://doi.org/10.1109/VIS55277.2024.00029) * [17] Zeng, X., Lin, H., Ye, Y., Zeng, W.: Advancing multimodal large language models in chart question answering with visualization-referenced instruction tuning. IEEE Transactions on Visualization and Computer Graphics **31**(1), 525-535 (2025) [https://doi.org/10.1109/TVCG.2024.3456159](https://doi.org/10.1109/TVCG.2024.3456159) * [18] Zhang, L., Hu, A., Xu, H., Yan, M., Xu, Y., Jin, Q., Zhang, J., Huang, F.: TinyChart: Efficient chart understanding with program-of-thoughts learning and visual token merging. In: Al-Onaizan, Y., Bansal, M., Chen, Y.-N. (eds.) Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1882-1898. Association for Computational Linguistics, Miami, Florida, USA (2024). [https://doi.org/10.18653/v1/2024.emnlp-main.112](https://doi.org/10.18653/v1/2024.emnlp-main.112) * [19] Masry, A., Thakkar, M., Bajaj, A., Kartha, A., Hoque, E., Joty, S.: ChartGemma: Visual instruction-tuning for chart reasoning in the wild. In: Rambow, O., Wanner, L., Apidianaki, M., Al-Khalifa, H., Eugenio, B.D., Schockaert, S., Darwish,K., Agarwal, A. (eds.) Proceedings of the 31st International Conference on Computational Linguistics: Industry Track, pp. 625-643. Association for Computational Linguistics, Abu Dhabi, UAE (2025). [https://aclanthology.org/2025.coling-industry.54](https://aclanthology.org/2025.coling-industry.54) * [20] Liu, F., Eisenschlos, J., Piccinno, F., Krichene, S., Pang, C., Lee, K., Joshi, M., Chen, W., Collier, N., Altun, Y.: DePlot: One-shot visual language reasoning by plot-to-table translation. In: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Findings of the Association for Computational Linguistics: ACL 2023, pp. 10381-10399. Association for Computational Linguistics, Toronto, Canada (2023). [https://doi.org/10.18653/v1/2023.findings-acl.660](https://doi.org/10.18653/v1/2023.findings-acl.660) * [21] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. In: 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 26286-26296 (2024). [https://doi.org/10.1109/CVPR52733.2024.02484](https://doi.org/10.1109/CVPR52733.2024.02484) * [22] Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., Lin, J.: Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. arXiv preprint arXiv:2409.12191 **abs/2409.12191** (2024) [https://doi.org/10.48550/arXiv.2409.12191](https://doi.org/10.48550/arXiv.2409.12191) * [23] Lu, S., Li, Y., Chen, Q.-G., Xu, Z., Luo, W., Zhang, K., Ye, H.-J.: Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797 **abs/2405.20797** (2024) [https://doi.org/10.48550/arXiv.2405.20797](https://doi.org/10.48550/arXiv.2405.20797) * [24] Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., Gu, L., Wang, X., Li, Q., Ren, Y., Chen, Z., Luo, J., Wang, J., Jiang, T., Wang, B., He, C., Shi, B., Zhang, X., Lv, H., Wang, Y., Shao, W., Chu, P., Tu, Z., He, T., Wu, Z., Deng, H., Ge, J., Chen, K., Zhang, K., Wang, L., Dou, M., Lu, L., Zhu, X., Lu, T., Lin, D., Qiao, Y., Dai, J., Wang, W.: Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271 **abs/2412.05271** (2024) [https://doi.org/10.48550/arXiv.2412.05271](https://doi.org/10.48550/arXiv.2412.05271) * [25] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., Rush, A.: Transformers: State-of-the-art natural language processing. In: Liu, Q., Schlangen, D. (eds.) Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38-45. Association for Computational Linguistics, Online (2020). [https://doi.org/10.18653/v1/2020.emnlp-demos.6](https://doi.org/10.18653/v1/2020.emnlp-demos.6) * [26] McNemar, Q.: Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika **12**(2), 153-157 (1947) [https://doi.org/10.1007/BF02295996](https://doi.org/10.1007/BF02295996)"
    }
  ]
}