{
  "title": "Bactrainus: Optimizing Large Language Models for Multihop Complex Question Answering Tasks",
  "authors": [
    "Iman Barati",
    "Arash Ghafouri",
    "Behrouz Minaei-Bidgoli"
  ],
  "abstract": "\n In recent years, the use of large language models (LLMs) has significantly increased, and these models have demonstrated remarkable performance in a variety of general language tasks. However, the evaluation of their performance in domain-specific tasks, particularly those requiring deep natural language understanding, has received less attention. In this research, we evaluate the ability of large language models in performing domain-specific tasks, focusing on the multi-hop question answering (MHQA) problem using the HotpotQA dataset. This task, due to its requirement for reasoning and combining information from multiple textual sources, serves as a challenging benchmark for assessing the language comprehension capabilities of these models. To tackle this problem, we have designed a two-stage selector-reader architecture, where each stage utilizes an independent LLM. In addition, methods such as Chain of Thought (CoT) and question decomposition have been employed to investigate their impact on improving the model's performance. The results of the study show that the integration of large language models with these techniques can lead to up to a 4% improvement in F1 score for finding answers, providing evidence of the models' ability to handle domain-specific tasks and their understanding of complex language. \n",
  "references": [
    {
      "id": null,
      "title": "Bactrainus: Optimizing Large Language Models for Multihop Complex Question Answering Tasks",
      "authors": [
        "Iman Barati",
        "Arash Ghafouri",
        "Behrouz Minaei-Bidgoli"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
      "authors": [
        "P Rajpurkar",
        "J Zhang",
        "K Lopyrev",
        "P Liang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
      "authors": [
        "Z Yang",
        "P Qi",
        "S Zhang",
        "Y Bengio",
        "W Cohen",
        "R Salakhutdinov",
        "C D Manning"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Teaching Machines to Read and Comprehend",
      "authors": [
        "K M Hermann",
        "T Kocisky",
        "E Grefenstette",
        "L Espeholt",
        "W Kay",
        "M Suleyman",
        "P Blunsom"
      ],
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Know What You Don't Know: Unanswerable Questions for SQuAD",
      "authors": [
        "P Rajpurkar",
        "R Jia",
        "P Liang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Reading Wikipedia to Answer Open-Domain Questions",
      "authors": [
        "D Chen",
        "A Fisch",
        "J Weston",
        "A Bordes"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Constructing a Multihop QA Dataset for Comprehensive Evaluation of Reasoning Steps",
      "authors": [
        "X Ho",
        "A.-K D Nguyen",
        "S Sugawara",
        "A Aizawa"
      ],
      "year": "2020",
      "venue": "Constructing a Multihop QA Dataset for Comprehensive Evaluation of Reasoning Steps",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "MuSiQue: Multi-hop Questions via Single-hop Question Composition",
      "authors": [
        "H Trivedi",
        "N Balasubramanian",
        "T Khot",
        "A Sabharwal"
      ],
      "year": "2021",
      "venue": "MuSiQue: Multi-hop Questions via Single-hop Question Composition",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "HybridQA: A Dataset of Multi-hop Question Answering over Tabular and Textual Data. Findings of the Association for Computational Linguistics: EMNLP 2020",
      "authors": [
        "W Chen",
        "H Zha",
        "Z Chen",
        "W Xiong",
        "H Wang",
        "W Y Wang"
      ],
      "year": "2020",
      "venue": "HybridQA: A Dataset of Multi-hop Question Answering over Tabular and Textual Data. Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
      "authors": [
        "T Mihaylov",
        "P Clark",
        "T Khot",
        "A Sabharwal"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "QASC: A Dataset for Question Answering via Sentence Composition",
      "authors": [
        "T Khot",
        "P Clark",
        "M Guerquin",
        "P Jansen",
        "A Sabharwal"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences",
      "authors": [
        "D Khashabi",
        "S Chaturvedi",
        "M Roth",
        "S Upadhyay",
        "D Roth"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-hop Queries",
      "authors": [
        "Y Tang",
        "Y Yang"
      ],
      "year": "2024",
      "venue": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-hop Queries",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Reading Wikipedia to Answer Open-Domain Questions",
      "authors": [
        "D Chen",
        "A Fisch",
        "J Weston",
        "A Bordes"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
      "authors": [
        "K Clark",
        "M.-T Luong",
        "Q V Le",
        "C D Manning"
      ],
      "year": "2020",
      "venue": "Proceedings of the 8th International Conference on Learning Representations (ICLR)",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Adaptive Document Retrieval for Deep Question Answering",
      "authors": [
        "B Kratzwald",
        "S Feuerriegel"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Dense Passage Retrieval for Open-Domain Question Answering",
      "authors": [
        "V Karpukhin",
        "B Oguz",
        "S Min",
        "P Lewis",
        "L Wu",
        "S Edunov",
        "D Chen",
        "W T Yih"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Multi-step Retriever-Reader Interaction for Scalable Opendomain Question Answering",
      "authors": [
        "R Das",
        "M Zaheer",
        "C Dyer",
        "A Mccallum"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering",
      "authors": [
        "K Lee",
        "M.-W Chang",
        "K Toutanova",
        "W T Yih"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Retrospective Reader for Machine Reading Comprehension",
      "authors": [
        "S Zhang",
        "J Yang",
        "P Qi",
        "C D Manning"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering",
      "authors": [
        "I Nair",
        "S Somasundaram",
        "A Saxena",
        "K Goswami"
      ],
      "year": "2023",
      "venue": "Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "FOLIO: Natural Language Reasoning with First-Order Logic",
      "authors": [
        "S Han",
        "H Schoelkopf",
        "Y Zhao",
        "Z Qi",
        "M Riddell",
        "L Benson",
        "L Sun"
      ],
      "year": "2022",
      "venue": "FOLIO: Natural Language Reasoning with First-Order Logic",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "STaR: Bootstrapping Reasoning with Reasoning",
      "authors": [
        "E Zelikman",
        "Y Wu",
        "J Mu",
        "N D Goodman"
      ],
      "year": "2022",
      "venue": "STaR: Bootstrapping Reasoning with Reasoning",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Do Multi-hop Readers Dream of Reasoning Chains?",
      "authors": [
        "H Wang",
        "M Yu",
        "X Guo",
        "R Das",
        "W Xiong",
        "T Gao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2nd Workshop on Machine Reading for Question Answering",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
      "authors": [
        "A Saparov",
        "H He"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations (ICLR)",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Modeling Multi-hop Question Answering as Single Sequence Prediction",
      "authors": [
        "S Yavuz",
        "K Hashimoto",
        "Y Zhou",
        "N S Keskar",
        "C Xiong"
      ],
      "year": "2022",
      "venue": "Modeling Multi-hop Question Answering as Single Sequence Prediction",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Interleaving Retrieval with Chain-of",
      "authors": [
        "H Trivedi",
        "N Balasubramanian",
        "T Khot",
        "A Sabharwal"
      ],
      "year": "2023",
      "venue": "Interleaving Retrieval with Chain-of",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
      "authors": [
        "T Khot",
        "H Trivedi",
        "M Finlayson",
        "Y Fu",
        "K Richardson",
        "P Clark",
        "A Sabharwal"
      ],
      "year": "2023",
      "venue": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
      "authors": [
        "D Zhou",
        "N Schärli",
        "L Hou",
        "J Wei",
        "N Scales",
        "X Wang",
        "D Schuurmans"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations (ICLR)",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts",
      "authors": [
        "B Zhou",
        "K Richardson",
        "X Yu",
        "D Roth"
      ],
      "year": "2022",
      "venue": "Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering",
      "authors": [
        "Z Deng",
        "Y Zhu",
        "Y Chen",
        "M Witbrock",
        "P Riddle"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "GenDec: A Robust Generative Questiondecomposition Method for Multi-hop Reasoning",
      "authors": [
        "J Wu",
        "L Yang",
        "Y Ji",
        "W Huang",
        "B F Karlsson",
        "M Okumura"
      ],
      "year": "2024",
      "venue": "GenDec: A Robust Generative Questiondecomposition Method for Multi-hop Reasoning",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Beam retrieval: General end-to-end retrieval for multihop question answering",
      "authors": [
        "J Zhang",
        "H Zhang",
        "D Zhang",
        "Y Liu",
        "S Huang"
      ],
      "year": "2023",
      "venue": "Beam retrieval: General end-to-end retrieval for multihop question answering",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Rethinking label smoothing on multi-hop question answering",
      "authors": [
        "Z Yin",
        "Y Wang",
        "X Hu",
        "Y Wu",
        "H Yan",
        "X Zhang",
        "Z Cao",
        "X Huang",
        "X Qiu"
      ],
      "year": "2023",
      "venue": "China National Conference on Chinese Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "From easy to hard: Two-stage selector and reader for multi-hop question answering",
      "authors": [
        "X.-Y Li",
        "W.-J Lei",
        "Y.-B Yang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Backrainus: Optimizing Large Language Models For Multi-Hop Complex Question Answering Tasks",
      "text": "Iman Barati Iran University of Science & Technology, Tehran, Iran, iman_barati@comp.iust.ac.ir Arash Ghafouri Iran University of Science & Technology, Tehran, Iran, aghafuri@comp.iust.ac.ir Behrouz Minaei-Bidgoli Iran University of Science & Technology, Tehran, Iran, iman_barati@comp.iust.ac.ir"
    },
    {
      "title": "Abstract",
      "text": "In recent years, the use of large language models (LLMs) has significantly increased, and these models have demonstrated remarkable performance in a variety of general language tasks. However, the evaluation of their performance in domain-specific tasks, particularly those requiring deep natural language understanding, has received less attention. In this research, we evaluate the ability of large language models in performing domain-specific tasks, focusing on the multi-hop question answering (MHQA) problem using the HotpotQA dataset. This task, due to its requirement for reasoning and combining information from multiple textual sources, serves as a challenging benchmark for assessing the language comprehension capabilities of these models. To tackle this problem, we have designed a two-stage selector-reader architecture, where each stage utilizes an independent LLM. In addition, methods such as Chain of Thought (CoT) and question decomposition have been employed to investigate their impact on improving the model's performance. The results of the study show that the integration of large language models with these techniques can lead to up to a 4% improvement in F1 score for finding answers, providing evidence of the models' ability to handle domain-specific tasks and their understanding of complex language. Large Language Models, Multi-hop Question Answering, Task Decomposition, Knowledge Distillation, HotpotQAIntroduction In recent years, LLMs have become one of the most significant achievements in natural language processing, demonstrating exceptional performance in a wide range of general language tasks, such as translation, summarization, and text generation. These models leverage attention-based architectures, an enormous number of parameters, and training on diverse and extensive datasets, offering capabilities beyond traditional natural language processing methods, such as instruction following and in-context learning. However, existing evaluations are often based on general benchmarks and employ zero-shot or few-shot approaches. While these methods showcase the overall capabilities of the models, they do not provide a thorough and in-depth analysis of their performance on specific tasks. In other words, these evaluations do not compare the models' capabilities to traditional methods or models optimized for a particular task. Therefore, investigating how to improve the performance of language models on domain-specific tasks and identifying their limitations remains an important and underexplored research gap. The goal of this paper is to evaluate the performance of large language models on a specific task. In this context, we have chosen to examine MHQA, which is one of the most complex tasks in natural language understanding. This task requires the extraction and reasoning of information from multiple sources and, due to its complexity, provides a comprehensive evaluation of a model's capabilities. To address this challenge, we have employed a two-stage selector-reader architecture called Bactrainus. In the selector stage, the model's ability to retrieve and extract relevant information is evaluated, while in the reader stage, the model's ability to reason and perform in-context learning is assessed. Additionally, we explore whether dividing the task into smaller sub-tasks and assigning each to a separate language model can help improve performance. Furthermore, we investigate the effects of knowledge distillation. To this end, we employ question decomposition in the selector stage and chain of thought in the reader stage to assess how these techniques can impact the reasoning and answering process."
    },
    {
      "title": "2 Related Works",
      "text": "Given the focus of this research--multi-hop question answering (QA) using LLMs--a wide range of previous studies could be deemed relevant. However, to maintain clarity and concentrate on essential aspects, this section covers only those works that are methodologically close to our approach or offer a deeper understanding of the topic. We begin with a review of QA datasets, especially those emphasizing multi-hop scenarios, then move on to the main components of open-domain QA systems, and finally discuss LLM-based methods for multi-hop QA."
    },
    {
      "title": "**Qa Datasets**",
      "text": ""
    },
    {
      "title": "2.1.1 Single-Hop Qa Datasets",
      "text": "In the early stages of machine reading comprehension MRC, most work featured single-hop QA data in which the answer was located within a short paragraph. Examples include: CNN/Daily Mail (Hermann et al., 2015), one of the first large-scale datasets, which overcame the data-scarcity challenge by automatically generating multiple questions from news articles. SQuAD (Rajpurkar et al., 2016) introduced over 100,000 human-written questions based on Wikipedia articles and later strengthened supervised learning by adding unanswerable questions (Rajpurkar et al., 2018). SQuADopen (Chen et al., 2017) expanded SQuAD1.1 for evaluation in an open-domain QA setting--omitting an explicit paragraph so that a model must locate the relevant document."
    },
    {
      "title": "2.1.2 Multi-Hop Qa Datasets",
      "text": "With the emergence of multi-hop QA, the challenge of chaining information from multiple documents or paragraphs became central. Notable datasets include: HotpotQA (Yang et al., 2018), which not only provides multi-step questions (bridge and comparison) but also introduces a distractor configuration that mixes irrelevant paragraphs, requiring more rigorous reasoning. 2WikiMultihopQA (Ho et al., 2020) merges structured and unstructured data to create inferential and comparative questions, while similarly adding distractor paragraphs. MuSiQue (Trivedi et al., 2021), which forms more complex multi-hop questions (2-4 hops) and increases the diversity of reasoning graphs, making simple single-step approaches insufficient. HybridQA (Chen et al., 2020) and OpenBookQA (Mihaylov et al., 2018) integrate table-based or scientific knowledge with text to enable multi-step reasoning. QASC (Khot et al., 2020) focuses on scientific facts, demanding high-quality retrieval and fact composition. Additionally, works such as MultiRC (Khashabi et al., 2018) and MultiHop-RAG (Tang and Yang, 2024) strive to develop datasets that require true multi-step inference in domains like news or other specialized corpora."
    },
    {
      "title": "Main Components Of Open-Domain Qa Systems",
      "text": "Open-domain QA systems must retrieve relevant documents from a large repository rather than relying on a single, explicit paragraph. The pivotal architecture came from Chen et al. (2017), who introduced two modules--a retriever and a reader--later adopted by systems like DrQA (Chen et al., 2017). In DrQA, a classic retrieval technique (e.g., TF-IDF) narrows down the candidate documents, and a neural reading model (initially a convolutional network) pinpoints the precise answer. With the rise of pre-trained language models such as BERT (Devlin et al., 2019) and ELECTRA (Clark et al., 2020), both the retriever and reader modules evolved. Dense retrievers (Karpukhin et al., 2020; Das et al., 2019; Lee et al., 2019) achieve deeper semantic alignment between question and text. Adaptive retrieval (Kratzwald and Feuerriegel, 2018) aims to select an optimal number of documents based on each query, while answer verification (Zhang et al., 2020) handles invalid or unanswerable questions."
    },
    {
      "title": "Large Language Models In Multi-Hop Qa",
      "text": "Recent developments in large language models (LLMs)--such as GPT variants, T5, or BART--have introduced novel methods beyond the traditional retriever-reader paradigm: * **LLM-Enhanced Retrieval**: Sometimes LLMs summarize lengthy passages and select relevant segments (Nair et al., 2023). A primary constraint is the limited context window size. * **Chain of Thought:** Studies (Han et al., 2022; Zelikman et al., 2022; Wang et al., 2019; Saparov and He, 2023) show that prompting an LLM to generate step-by-step reasoning can improve multi-hop QA. Projects like PathFiD (Yavuz et al., 2022) and IRCoT (Trivedi et al., 2023) rely on iterative evidence construction. * **Question Decomposition:** Several approaches (Khot et al., 2023; Zhou et al., 2023; Zhou et al., 2022; Deng et al., 2022; Wu et al., 2024) break down a complex question into simpler sub-questions, often with LLM-driven generation or refinement, facilitating multi-step inference. * **Retrieval-Augmented Generation (RAG):** In some frameworks (Tang and Yang, 2024), a generative model (usually an LLM) composes multi-step answers with the help of retrieved evidence, iterating between retrieval and reasoning. From multi-hop datasets and classic retriever-reader systems to modern LLM-based strategies--like chain-of-thought prompting, RAG, and question decomposition--the field has continued to refine multi-step QA. Our proposed method leverages these developments to address current limitations and aims to present an efficient, optimized solution for multi-hop question answering."
    },
    {
      "title": "3 Experimental Setup",
      "text": "This section details the dataset and its preparation, followed by model configurations and implementation specifics. Finally, the hardware specifications and utilization strategy are presented to ensure reproducibility."
    },
    {
      "title": "**Dataset**",
      "text": "We utilize the HotpotQA dataset under the Distractor setting to evaluate model performance in MHQA. In this configuration, each question is accompanied by 10 candidate paragraphs, 2 of which are gold paragraphs that directly contribute to the answer. The key challenge is to accurately identify the supporting facts from the irrelevant information. To align this dataset with LLMs, the training samples are converted into the Alpaca format (designed for Supervised Fine-tuning)."
    },
    {
      "title": "**Implementation Details**",
      "text": "There are two primary approaches for examining the impact of different settings on model performance. In the first approach, each model is individually tuned with specialized hyperparameters and a system prompt, and the final results are compared. In the second approach, all models operate under a single, uniform configuration to enable a fair comparison under consistent conditions. In this study, we adopt the second approach and only introduce limited modifications to the system prompt in certain experiments. Since the primary objective in multi-hop QA is to obtain precise and coherent answers, the following configurations are applied to generate model outputs: * Temperature = 0.01 * Top-p = 0.8A low temperature biases the model toward more probable tokens, while a high Top-p prevents the exclusion of potentially important tokens. Consequently, the model yields focused, accurate answers that align with the goals of this research."
    },
    {
      "title": "Hardware",
      "text": "All experiments were conducted on a server equipped with 4 A100 GPUs (80 GB memory each). In some cases requiring larger models or extensive parallelization, all four GPUs were utilized; however, for most experiments, no more than two GPUs were necessary. This computational infrastructure significantly reduced training and inference time and enabled experimentation with larger-scale models."
    },
    {
      "title": "4 Methodology",
      "text": "This section describes the main architecture adopted for multi-hop question answering, followed by the proposed knowledge distillation techniques aimed at enhancing model performance. The primary goal is to improve accuracy and reliability by dividing the complex task into manageable sub-modules and leveraging high-level knowledge transfer."
    },
    {
      "title": "Selector-Reader Architecture",
      "text": "To systematically investigate the model's performance, We utilize a two-stage selector-reader architecture, as illustrated in Figure 1. This design comprises two main components: * **Selector**: Identifies and extracts the supporting facts from among multiple candidate paragraphs (including the \"gold paragraphs\" and distractors). * **Reader**: Utilizes the selector's output (i.e., the chosen paragraphs or sentences) to answer the main question. Figure 1: provides an overview of the two-stage selector-reader architecture. In other words, the selector focuses on retrieving relevant text from extensive contexts, requiring the ability to understand inter-document relationships and the chaining of information for multi-step queries. Subsequently, the reader component leverages multi-hop reasoning and in-context learning techniques to synthesize the extracted evidence and derive the final, correct answer. We hypothesize that separating the complex QA task into two independent sub-tasks can lead to improved performance. This separation allows each component to concentrate on its respective objective without entangling both tasks. The effectiveness of this assumption is evaluated in the Results and Evaluation section."
    },
    {
      "title": "Knowledge Distillation",
      "text": "A key strategy for improving multi-hop QA performance is knowledge distillation, which posits that high-level reasoning or step-by-step logic can be transferred from one large language model (Teacher) to another (Student). In this study, we explore two main approaches for knowledge distillation: * **Distilling Knowledge During Fine-tuning:** In this approach, additional outputs containing step-by-step reasoning or auxiliary information (e.g., a chain of thought) are produced by a more capable model and presented as targets during training. As the student model attempts to reproduce this detailed reasoning, it internalizes higher-level knowledge. * **Distilling Knowledge at Inference Time:** Here, supplementary hints or step-by-step explanations are directly appended to the model's input during inference, providing immediate guidance on how to approach the solution. This additional context aids the student model in generating more accurate answers. In our implementation, the \"chain of thought\" in the reader component and \"question decomposition\" in the selector component play pivotal roles: **Reader Component:** The chain of thought--representing the stepwise logic linking relevant evidence is appended to the model's output during fine-tuning. This enables the model to learn the structure of reasoning and produce more precise answers. **Selector Component:** Complex, multi-step questions are decomposed into simpler sub-questions by a separate language model. These sub-questions are then fed into the selector, enhancing its ability to pinpoint and retrieve supporting paragraphs more efficiently. Consequently, the workload on the reader is reduced, leading to improved final accuracy."
    },
    {
      "title": "5 Experiments And Results",
      "text": "In this section, each component of the proposed Selector-Reader architecture is evaluated independently to demonstrate its contribution to multi-hop question answering. Subsequently, these components are integrated to address the primary objective of the HotpotQA dataset--namely, determining both the correct answer and its associated supporting facts for complex multi-hop questions."
    },
    {
      "title": "Evaluating The Reader Component",
      "text": ""
    },
    {
      "title": "5.1.1 Zero-Shot Prompt",
      "text": "To assess the performance of a LLM acting as the \"reader,\" we first assume that a perfect \"selector\" has already provided the necessary supporting facts with no errors. In other words, we have a guaranteed set of correct supporting evidence for each question. This setup allows us to isolate and measure the LLM's intrinsic ability to perform multi-hop inference and extract an answer from the given text. Initially, we evaluate several prominent models in a zero-shot configuration. In this scenario, the model does not receive any additional in-context examples or fine-tuning instructions specifically designed for the multi-hop task. By comparing multiple closed-source and open-source LLMs, we identify the most promising candidates for further experiments. Table 1 summarizes the Exact Match (EM) and F1 scores of the models tested in the reader component under zero-shot prompting. Models are grouped into categories by parameter count and whether they are closed- or open-source. Analysis of Results: * Among all models tested, GPT-4o yields the highest Exact Match (67.54) and F1 (83.44). However, due to its closed-source nature, limited access for fine-tuning, and high associated costs, it was excluded from subsequent experiments. * Among models with fewer than 10 billion parameters, Llama3.1 8B Instruct achieves the best performance (EM = 60.11, F1 = 74.52). * For models exceeding 10 billion parameters, Llama3.1 70B Instruct produces notably strong results (EM = 65.60, F1 = 80.04). \\begin{table} \\begin{tabular}{|c|c|c|} \\hline \\multirow{2}{*}{**Models**} & \\multicolumn{2}{c|}{**Measure**} \\\\ \\cline{2-3} & **Exact Match** & **F1 Score** \\\\ \\hline \\multicolumn{3}{|c|}{**Close-Source LLMs**} \\\\ \\hline **GPT-4o** & **67.54** & **83.44** \\\\ \\hline **Claude 3.5 sonnet** & 67.12 & 83.07 \\\\ \\hline **gemini-1.5-pro** & 65.52 & 80.91 \\\\ \\hline **GPT-4 Turbo** & 66.98 & 82.62 \\\\ \\hline **GPT-4o-mini** & 62.71 & 78.65 \\\\ \\hline **gemini-1.5-flash** & 62.53 & 77.96 \\\\ \\hline **GPT-3.5 Turbo** & 47.28 & 63.17 \\\\ \\hline \\multicolumn{3}{|c|}{**Open-Source LLMs [¡108 Params]**} \\\\ \\hline **Llama3.1 8B Instruct** & **60.11** & **74.52** \\\\ \\hline **Llama3 8B Instruct** & 58.09 & 73.27 \\\\ \\hline **Aya23 7B* & 57.53 & 71.91 \\\\ \\hline **Gemma2 9B** & 57.12 & 71.04 \\\\ \\hline **Qwen2-7B-Instruct** & 51.63 & 63.02 \\\\ \\hline **phi3-mini-4k-instruct** & 46.82 & 62.18 \\\\ \\hline **phi3-medium-4k-instruct** & 50.34 & 63.92 \\\\ \\hline **Mistral 7b v2** & 22.19 & 52.01 \\\\ \\hline **Mistral 7b v3** & 24.23 & 54.86 \\\\ \\hline \\multicolumn{3}{|c|}{**Open-Source LLMs (Between 10B and 70B Params)**} \\\\ \\hline **Aya23 35B* & **64.97** & **80.09** \\\\ \\hline **Gemma2 27B** & 58.63 & 75.13 \\\\ \\hline **Command r** & 55.32 & 71.43 \\\\ \\hline **Mistral 7x8** & 41.39 & 61.8 \\\\ \\hline \\multicolumn{3}{|c|}{**Open-Source LLMs (-70B Params)**} \\\\ \\hline **Qwen 2 72B** & 65.14 & 79.96 \\\\ \\hline **Llama 3 70B Instruct** & 64.81 & 79.1 \\\\ \\hline **Llama 3.1 70B Instruct** & **65.6** & **80.04** \\\\ \\hline **Open-Source LLMs [¿708 Params]** & & \\\\ \\hline **Mixral 7x22** & 50.12 & 67.23 \\\\ \\hline **Command r plus** & 59.64 & 74.81 \\\\ \\hline **Llama 3.1 405b Instruct** & **67.46** & **82.56** \\\\ \\hline \\end{tabular} \\end{table} Table 1: Zero-shot performance of various large language models on the reader component Based on these observations, Llama3.1 8B Instruct (representing a smaller model category) and Llama3.1 70B Instruct (representing a larger model category) were selected for further investigation. Subsequent experiments will explore various fine-tuning and optimization strategies to enhance their performance in a multi-hop QA setting."
    },
    {
      "title": "5.1.2 Effect Of The Number Of Supporting Facts",
      "text": "To gain deeper insights into how the reader performance of LLMs might be influenced by the complexity of questions, we examine the number of supporting facts associated with each sample in HotpotQA. Intuitively, questions with more supporting facts are often presumed to require more multi-hop reasoning, suggesting they could be more challenging. Here, we test that assumption by analyzing model accuracy across varying numbers of supporting facts. Figure 2 illustrates the distribution of the number of supporting facts in both the training and evaluation splits of HotpotQA. This visualization reveals how many questions require two, three, or four (and above) pieces of evidence to reach the correct answer. Figure 2: **Number of supporting facts in the training and evaluation datasets of the HotpotQA dataset** To further investigate this phenomenon, we selected five representative models of varying sizes and types. Table 2 reports their EM and F1 scores when questions are grouped by the number of supporting facts. We also summarize the results in Figure 3, showing how each model's performance changes as the required number of facts increases. Figure 3 visualizes these results. Contrary to the initial assumption, a higher number of supporting facts does not necessarily yield lower performance. Some models achieve better results when faced with four or more supporting facts. Notably, however, the performance gap between smaller and larger models tends to widen at higher numbers of supporting facts, suggesting that larger models have a stronger capacity for complex multi-hop reasoning. \\begin{table} \\begin{tabular}{|c|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{**Models**} & \\multicolumn{6}{c|}{**Number of Supporting Facts**} \\\\ \\cline{2-7} & \\multicolumn{2}{c|}{Tow} & \\multicolumn{2}{c|}{Three} & \\multicolumn{2}{c|}{Four or More} \\\\ \\cline{2-7} & EM & F1 & EM & F1 & EM & F1 \\\\ \\hline GPT-4o & 67.66 & 83.50 & 66.57 & 83.05 & 69.28 & 84.06 \\\\ \\hline Llama 3.1 405B Instruct & 67.87 & 82.47 & 65.63 & 82.41 & 69.34 & 83.67 \\\\ \\hline Llama 3.1 70B Instruct & 66.01 & 79.92 & 63.19 & 79.20 & 69.11 & 83.28 \\\\ \\hline Aya23 35B* & 65.65 & 80.43 & 62.85 & 79.08 & 65.52 & 80.22 \\\\ \\hline Llama 3.1 8B Instruct & 60.82 & 75.09 & 58.11 & 73.29 & 60.06 & 73.46 \\\\ \\hline \\end{tabular} \\end{table} Table 2: **Analyzing the impact of the number of supporting facts on the performance of the reader** Figure 3: **The diagram illustrates the impact of the number of supporting facts on the performance of large language models in the reader component** For instance, when there are two supporting facts, the difference in EM between Llama 3.1 8B Instruct and Llama 3.1 405B Instruct is only about 7%, but for four or more supporting facts, this gap increases to over 9%, indicating that larger models can aggregate and reason over multiple pieces of evidence more effectively. These findings suggest that while having more supporting facts does not automatically make a question harder, it may amplify the advantage of larger LLMs in handling multi-hop reasoning. This trend could inform future model development and dataset curation, where model size and complexity of evidence are both critical variables."
    },
    {
      "title": "5.1.3 Investigating Model Dependence On Supporting Facts And Input Size",
      "text": "Having selected Llama 3.1 8B Instruct and Llama 3.1 70B Instruct as our primary reader models, we explore two key questions: 1. Does the language model inherently \"know\" the answer without any supporting facts, or does it genuinely rely on these facts? 2. In other words, can the model answer HotpotQA questions with only the question text, or must it derive the solution from the supporting facts? How does the amount of provided text (beyond supporting facts) influence reader performance? Specifically, if we supply additional paragraphs--some of which may be irrelevant--to the model instead of only the minimal supporting facts, will accuracy be affected?"
    },
    {
      "title": "Performance In The \"Question Only\" Setting",
      "text": "To address the first question, we designed an experiment where the model is given only the question with no supporting context. Table 3 reports the outcomes for these input configurations. As shown, the \"Question Only\" scenario yields a notable drop in performance. The model can handle only a few cases correctly--often yes/no questions or binary choices--potentially solvable by random guessing or general knowledge. When even partial relevant content (such as gold paragraphs) is provided, performance improves significantly. This outcome underscores that neither model possesses adequate internal knowledge to answer most HotpotQA questions. Instead, they rely substantially on the provided supporting evidence."
    },
    {
      "title": "Effect Of Input Size On In-Context Learning",
      "text": "Next, to address the second question regarding input size, we conducted four experiments differing only in how much text is fed to the model: 1. Supporting facts only (baseline) 2. Gold paragraphs 3. Gold paragraphs + 2 distractors 4. All paragraphs in HotpotQA To select the two distractor paragraphs, we employed the sentence transformer gte-large-en-v1.5 calculating similarity between the question and all non-gold paragraphs, then choosing the two most semantically similar paragraphs as distractors (Figure 4). The final results, shown in Table 3 (above) and Figure 5, indicate that transitioning from \"Supporting Facts Only\" to \"Gold Paragraphs\" does not drastically alter the scores, implying these paragraphs are not substantially different or misleading. However, adding two distractor paragraphs leads to a noticeable performance decline, and using all \\begin{table} \\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \\hline **Model** & \\multicolumn{2}{c|}{**Question**} & \\multicolumn{2}{c|}{**Supporting Facts**} & \\multicolumn{2}{c|}{**Gold Only**} & \\multicolumn{2}{c|}{**Gold + 2**} & \\multicolumn{2}{c|}{**All Paragraphs**} \\\\ \\cline{2-11} & \\multicolumn{2}{c|}{} & \\multicolumn{2}{c|}{} & \\multicolumn{2}{c|}{**F1**} & \\multicolumn{2}{c|}{**EM**} & \\multicolumn{2}{c|}{**F1**} & \\multicolumn{2}{c|}{**EM**} & \\multicolumn{2}{c|}{**F1**} & \\multicolumn{2}{c|}{**EM**} & \\multicolumn{2}{c|}{**F1**} \\\\ \\hline **Llama 3.1 8B Instruct** & 21.66 & 29.76 & 60.11 & 74.52 & 58.29 & 72.44 & 52.48 & 65.53 & 45.21 & 57.20 \\\\ \\hline **Llama 3.1 70B** & 31.02 & 41.85 & 65.60 & 80.04 & 64.74 & 79.23 & 57.31 & 70.96 & 46.50 & 58.61 \\\\ **Instruct** & & & & & & & & & & \\\\ \\hline \\end{tabular} \\end{table} Table 3: Effect of varying input conditions on the reader performance of two Llama 3.1 modelsparagraphs yields the most significant drop. Hence, the model cannot effectively isolate the necessary evidence when large amounts of irrelevant text are present, and the lengthy input confuses the in-context learning process. Necessity of Targeted Retrieval: These experiments affirm that delivering concise, high-fidelity input (supporting facts or gold paragraphs) to the reader is crucial for success. Validating the Initial Hypothesis: Splitting the QA task into independent sub-tasks (selector and reader) and allocating each to a separate LLM appears beneficial, at least in zero-shot settings. Figure 4: The process of selecting two distractor paragraphs for the reader’s input Figure 5: Bar chart illustrating the impact of various input conditions on reader performance Model Capacity Constraints: Providing excessively large or misleading inputs significantly reduces accuracy, highlighting the importance of accurate document selection to prevent confusion in multi-hop reasoning."
    },
    {
      "title": "5.1.4 Impact Of Few-Shot Prompting And Chain Of Thought",
      "text": "To further explore how multi-hop question answering might be enhanced by large language models (LLMs), we investigate two additional factors: * **Few-shot prompting:** Does providing multiple examples (e.g., one, two, four, or eight shots) help the model better understand and respond to complex questions? * **Chain of Thought:** Can explicitly showing step-by-step reasoning in the provided examples boost the model's multi-hop inference capabilities? We first examine the effect of varying the number of examples in the prompt--ranging from zero-shot to one-shot, two-shot, four-shot, and eight-shot--for two Llama 3.1 models with 8B and 70B parameters. These examples were selected from high-difficulty questions in HotpotQA, ensuring coverage of both \"bridge\" and \"comparison\" types. Table 4 presents the results. Observing Table 4, the one-shot configuration yields the best performance in both models. Providing a single example helps orient the model towards the task requirements; however, adding more examples does not consistently improve accuracy. In some cases (e.g., two-shot), performance even dips, possibly due to overfitting to the examples or confusion arising from multiple demonstrations. After two-shot, performance recovers slightly with four and eight examples but remains near or below the one-shot peak (Figure 6). In the second experiment, we repeated the above few-shot variations but appended chain-of-thought explanations to the prompt examples. These step-by-step rationales were generated by Llama 3.1 70B itself. Specifically, the model \\begin{table} \\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{2}{c|}{**Zero-shot**} & \\multicolumn{2}{c|}{**1-shot**} & \\multicolumn{2}{c|}{**2-shot**} & \\multicolumn{2}{c|}{**4-shot**} & \\multicolumn{2}{c|}{**8-shot**} \\\\ \\cline{2-11} & **EM** & **F1** & **EM** & **F1** & **EM** & **F1** & **EM** & **F1** & **EM** & **F1** \\\\ \\hline Llama-3.1-8B-instruct & 60.11 & 74.52 & 63.24 & 77.50 & 61.42 & 75.55 & 62.56 & 76.65 & 62.78 & 76.93 \\\\ \\hline Llama-3.1-70B-instruct & 65.60 & 80.04 & **68.18** & **82.66** & 65.32 & 79.83 & 66.44 & 80.99 & 66.93 & 81.41 \\\\ \\hline \\end{tabular} \\end{table} Table 4: **Few-shot results (no chain of thought) for Llama 3.1 reader models**was given a question, supporting facts, and the final answer, then asked to describe how it arrived at that answer. Table 5 summarizes the results. For the 8B model, incorporating a chain of thought slightly degrades performance, suggesting that walking through the reasoning step-by-step introduces complexity for a smaller model. Meanwhile, the 70B model shows a modest benefit from chain-of-thought prompts at higher shot counts (two or more). Nonetheless, the best overall performance remains one-shot without a chain of thought, as depicted in Figure 6. **Optimal Shot Count**: One-shot prompting tends to yield the highest performance, whereas introducing additional examples can sometimes cause confusion. **Chain of Thought**: For the smaller (8B) model, chain-of-thought prompts slightly impair performance, but for the larger (70B) model at higher shot counts, they yield a minor improvement. \\begin{table} \\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \\hline **Model** & \\multicolumn{2}{c|}{**Zero-shot**} & \\multicolumn{2}{c|}{**1-shot**} & \\multicolumn{2}{c|}{**2-shot**} & \\multicolumn{2}{c|}{**4-shot**} & \\multicolumn{2}{c|}{**8-shot**} \\\\ \\cline{2-11} & **EM** & **F1** & **EM** & **F1** & **EM** & **F1** & **EM** & **F1** & **EM** & **F1** \\\\ \\hline Llama-3.1-8B-instruct & 60.11 & 74.52 & 63.24 & 77.50 & 61.42 & 75.55 & 62.56 & 76.65 & 62.78 & 76.93 \\\\ \\hline Llama-3.1-70B-instruct & 65.60 & 80.04 & 68.18 & 82.66 & 65.32 & 79.83 & 66.44 & 80.99 & **66.93** & **81.41** \\\\ \\hline \\end{tabular} \\end{table} Table 5: Few-shot results with chain of thought Figure 6: The effect of the number of examples on few-shot prompting, without and with chain of thought **Overall Insight**: While few-shot prompting and step-by-step reasoning can help in certain scenarios, they may also introduce noise or complexity. The type, number, and quality of examples need careful tuning to achieve consistent gains."
    },
    {
      "title": "5.1.5 Fine-Tuning",
      "text": "Following the zero-shot and few-shot experiments, we now aim to fully leverage the potential of Llama 3.1 Instruct 8B and Llama 3.1 Instruct 70B for multi-hop question answering by performing fine-tuning on the HotpotQA dataset. To do this, we convert the training data into an instructional format and apply the LoRA approach (due to limited computational resources) to efficiently fine-tune the models. \\begin{table} \\begin{tabular}{|c|c|c|c|c|} \\hline \\multirow{2}{*}{**Setting**} & \\multicolumn{4}{c|}{**Reader**} \\\\ \\cline{2-5} & \\multicolumn{2}{c|}{Bactrainus 8B} & \\multicolumn{2}{c|}{Bactrainus 8B + CoT 8B} & \\multicolumn{2}{c|}{Bactrainus 8B + CoT 70B} & \\multicolumn{2}{c|}{Bactrainus 70B} \\\\ \\hline **Base Model** & Llama 3.1 Instruct 8B & Llama 3.1 Instruct 8B & Bactrainus 8B (1 epoch) & Llama 3.1 Instruct 70B \\\\ \\hline **Number of Training** & 90,564 & 90,564 & 15,661 & 90,564 \\\\ **Data Points** & & & & \\\\ \\hline **Training Steps** & 2 & 2 & 1 & 1 \\\\ \\hline **Batch Size** & 8 & 4 & 4 & 1 \\\\ \\hline **Gradient** & 32 & 16 & 16 & 8 \\\\ **Accumulation Steps** & & & & \\\\ \\hline **Maximum Learning** & 1.00E-04 & 1.00E-04 & 1.00E-04 & 1.00E-04 \\\\ **Rate** & & & & \\\\ \\hline **Learning Rate** & Cosine & Cosine & Cosine & Cosine \\\\ **Scheduler Type** & & & & \\\\ \\hline **Warm-Up Ratio** & 0.03 & 0.03 & 0.1 & 0.03 \\\\ \\hline **Maximum Sequence** & 512 & 1024 & 1024 & 512 \\\\ **Length** & & & & \\\\ \\hline **LoRA Rank** & 64 & 64 & 64 & 16 \\\\ \\hline **LoRA Alpha** & 128 & 128 & 32 & 16 \\\\ \\hline **Trainable LoRA** & QKVO, MLP & QKVO, MLP & QKVO, MLP & QKVO, MLP \\\\ **Weights** & & & & \\\\ \\hline **Fully Trainable Layer** & lm-head & lm-head & lm-head & - \\\\ \\hline **LoRA Dropout** & 0.05 & 0.05 & 0.05 & 0.05 \\\\ \\hline \\end{tabular} \\end{table} Table 6: **Hyperparameters for fine-tuning the reader component**We created an instruction-based format where the question and supporting facts serve as input, and the answer is the target output. In some configurations, a chain of thought is also included in the output. The key hyperparameters of this procedure are listed in Table 6. Upon completion, each fine-tuned model is referred to as Bactrainus. As suggested in Section 4-2, generating auxiliary reasoning traces (chain of thought) via another large language model can facilitate knowledge transfer. We explore two main scenarios: * **Chain of Thought from an 8B Model** Here, Llama 3.1 8B is provided with the question, supporting facts, and final answer, then asked to outline the step-by-step reasoning process. The reader model (also 8B) is fine-tuned to reproduce both the final answer and the chain of thought (Figure 7). * **Chain of Thought from a 70B Model** In this scenario, Llama 3.1 70B generates reasoning traces for the more challenging samples of the training set. The 8B reader--previously fine-tuned only on direct answers--undergoes continual fine-tuning with these newly generated traces from the 70B model. Figure 7: **Generating a chain of thought with the LLama model and using it in reader fine-tuning** Table 7 presents the outcomes of the Bactrainus reader models under the assumption that supporting facts are fully available. These models are fine-tuned for multi-hop reasoning tasks. As shown, the best results come from Bactrainus Reader 70B, achieving 75.73 EM and 90.01 F1. Knowledge Transfer via Chain of Thought built by the 70B model slightly improves the 8B reader (Bactrainus Reader 8B + CoT 70B), whereas using an 8B-generated chain of thought leads to a minor performance drop. This trend supports the notion that larger models can produce higher-quality reasoning traces for knowledge transfer."
    },
    {
      "title": "The Selector Component",
      "text": "In MHQA, having a capable reader alone is insufficient. The system must also identify which paragraphs or sentences contain the supporting facts needed to answer the query. This stage is referred to as the selector. Its primary goal is to determine which subset of the documents are relevant and, within those, which sentences constitute the supporting evidence. Most previous studies have employed traditional information retrieval (IR) methods or encoder-only architectures for the selector. In contrast, we leverage LLMs to handle the selection of supporting facts, given that the HotpotQA dataset contains a limited number of candidate documents for each question. The central hypothesis is that the deep textual understanding and in-context learning capabilities of an LLM can potentially outperform standard IR solutions in identifying relationships between complex questions and relevant documents."
    },
    {
      "title": "5.2.1 Fine-Tuning The Selector",
      "text": "Because LLMs are generally not trained for retrieval, and because the selector's output must conform to highly specific and rigid evaluation metrics, standard zero-shot or few-shot prompts alone are inadequate to enforce the strict output \\begin{table} \\begin{tabular}{|c|c|c|} \\hline \\multicolumn{1}{|c|}{**Model**} & \\multicolumn{2}{c|}{**Measure**} \\\\ \\cline{2-3} & **EM** & **F1 Score** \\\\ \\hline Bactrainus Reader 8B & 74.02 & 86.46 \\\\ \\hline Bactrainus Reader 8B + Cot 8B & 72.97 & 85.62 \\\\ \\hline Bactrainus Reader 8B + Cot 70B & 74.19 & 86.91 \\\\ \\hline Bactrainus Reader 70B & **75.73** & **90.01** \\\\ \\hline \\end{tabular} \\end{table} Table 7: **Results of fine-tuned reader models (with supporting facts)**format required. Consequently, we adopt a fine-tuning (supervised) approach. We convert the HotpotQA dataset into an instruction-based format, wherein the model takes a multi-hop question plus all associated documents as input and is trained to produce the supporting facts or gold paragraphs. As with the reader module, we refer to each fine-tuned model in the selector stage as Bactrainus. Due to hardware constraints, we employed only Llama 3.1 Instruct 8B in this component. Like the reader component, we use LoRA to fine-tune LLMs, allowing selective training of certain parameters. Table 8 outlines the hyperparameters for four key scenarios: Single-stage Selector: The model directly receives the multi-hop question and all candidate paragraphs, and must identify the supporting facts in a single pass. \\begin{table} \\begin{tabular}{|c|c|c|c|c|} \\hline \\multirow{2}{*}{**Setting**} & \\multicolumn{4}{c|}{**Model**} \\\\ \\cline{2-5} & **Single-stage** & **Paragraph** & **Sentence Selector** & **Question** \\\\ & **Selector** & **Selector** & & **Decomposer** \\\\ \\hline Base Model & Llama 3.1 Instruct 8B & Llama 3.1 Instruct 8B & Llama 3.1 Instruct 8B & Llama 3.1 Instruct 8B \\\\ \\hline Number of Training Data Points & 90564 & 90564 & 90564 & 90564 \\\\ \\hline Training Steps & 2 & 2 & 2 & 1 \\\\ \\hline Batch Size & 2 & 2 & 4 & 8 \\\\ \\hline Gradient Accumulation Steps & 8 & 8 & 16 & 32 \\\\ \\hline Maximum Learning Rate & 1.00E-04 & 1.00E-04 & 2.00E-05 & 2.00E-05 \\\\ \\hline Learning Rate & cosine & cosine & cosine & cosine \\\\ \\cline{2-5} Scheduler Type & & & & \\\\ \\hline Warm-Up Ratio & 0.03 & 0.03 & 0.03 & 0.03 \\\\ \\hline Maximum Sequence Length & 4096 & 4096 & 1024 & 2048 \\\\ \\hline LoRA Rank & 64 & 64 & 64 & 64 \\\\ \\hline LoRA Alpha & 128 & 128 & 128 & 32 \\\\ \\hline Trainable LoRA Weights & QKVO, MLP & QKVO, MLP & QKVO, MLP & QKVO, MLP \\\\ \\hline Fully Trainable Layer & lm-head & lm-head & lm-head & lm-head \\\\ \\hline LoRA Dropout & 0.05 & 0.05 & 0.05 & 0.05 \\\\ \\hline \\end{tabular} \\end{table} Table 8: **Hyperparameters for fine-tuning the selector models**Paragraph Selector: The model focuses on pinpointing only the gold paragraphs among all candidates. Sentence Selector: Assuming the gold paragraphs are already known, the model filters the key sentences (supporting facts) within them. Question Decomposer: The model splits the multi-hop query into simpler sub-questions to facilitate subsequent fact selection. We evaluate each fine-tuned selector model on two types of outputs: Gold Paragraphs: Accuracy in identifying which paragraphs are indeed the gold paragraphs required for answering the question. Supporting Facts: Accuracy in isolating the key sentences (supporting facts) from the identified paragraphs. Table 9 shows the results, EM and F1 scores. Single-stage Selector achieves strong paragraph-level performance (EM=96.83, F1=98.37) but is less accurate at pinpointing individual sentences (EM=65.74, F1=89.27). Paragraph Selector excels at identifying gold paragraphs but does not isolate supporting sentences. Sentence Selector (with paragraphs known) attains (EM=66.01, F1=89.93), indicating that extracting key sentences remains a more challenging problem. Two-stage Selector (paragraph \\(+\\) sentence) does not significantly outperform the single-stage approach. This lack of improvement might stem from the high interdependence between paragraph- and sentence-level selection. \\begin{table} \\begin{tabular}{|c|c|c|c|c|} \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{2}{c|}{**Gold Paragraphs**} & \\multicolumn{2}{c|}{**Supporting Facts**} \\\\ \\cline{2-5} & **EM** & **F1** & **EM** & **F1** \\\\ \\hline **Single-stage Selector** & **96.83** & **98.37** & 65.74 & 89.27 \\\\ \\hline **Paragraph Selector** & 96.64 & 98.24 & – & – \\\\ \\hline **Sentence Selector** & – & – & 66.01 & 89.93 \\\\ **(assuming gold paragraphs)** & \\multirow{2}{*}{96.64} & \\multirow{2}{*}{98.24} & \\multirow{2}{*}{65.55} & \\multirow{2}{*}{89.21} \\\\ \\cline{1-1} \\cline{5-5} **Two-stage Selector** & & & & \\\\ **(Paragraph + Sentence)** & & & & \\\\ Adding sub-questions (see the last row) offers a slight improvement in identifying supporting facts (EM=65.93, F1=89.63), though it does not yield a marked breakthrough."
    },
    {
      "title": "5.2.2 Two-Stage Architecture",
      "text": "Given that the single-stage model performs quite well for paragraph-level selection but leaves room for improvement in sentence-level retrieval, we explore a two-stage approach: * Paragraph Selector: Identifies which paragraphs are gold. * Sentence Selector: Extracts supporting facts from among the selected paragraphs. Figure 8 illustrates the overall two-stage design, and Figure 9 depicts the fine-tuning procedure for training two separate models. Contrary to initial expectations, the results in Table 9 show no major gains over the single-stage method--likely because splitting the task removes some valuable cross-information that exists between the paragraph and sentence levels. Figure 8: Two-stage selector architecture"
    },
    {
      "title": "5.2.3 Question Decomposer",
      "text": "To enhance sentence-level accuracy, we introduce auxiliary sub-questions: After identifying gold paragraphs, sub-questions are generated (especially for more difficult queries) by a larger LLM (e.g., Llama 70B). These sub-questions, along with the original question, are fed into an 8B question-decomposer model fine-tuned to interpret them, thereby helping the sentence selector isolate the supporting facts more effectively. Finally, as shown in Figure 10, these sub-questions are supplied to the sentence selector. Table 9 indicates a moderate improvement in identifying supporting facts (EM=65.93, F1=89.63), but not a major leap. The single-stage approach using Llama 3.1 Instruct 8B demonstrates near state-of-the-art results in paragraph selection, suggesting that LLMs can be effectively used for retrieval in limited-scale datasets. Figure 9: **Fine-tuning workflow for the two-stage selector** Extracting fine-grained supporting facts remains more difficult, and splitting the task into paragraph- and sentence-level selection does not necessarily help--likely due to tight interdependence between these two sub-tasks. Supplying sub-questions offers a small performance boost but is not transformative. Future research could explore more sophisticated methods for generating sub-questions or employing even larger models to guide the sentence selector."
    },
    {
      "title": "Integrating The Selector And Reader",
      "text": "Having examined the selector and reader components separately, we now combine them to solve the distractor setting of the HotpotQA dataset end-to-end. The goal is to identify both the supporting facts and the final answer for multi-hop questions, then compare the results to existing methods. Figure 11 illustrates six possible ways to integrate the selector and reader, each differing in how the outputs of one component feed into the other and in the detailed configuration of these modules. Additionally, in each scenario (except for the first), the reader can be any of the fine-tuned Backtrainus models described in Section 5-1."
    },
    {
      "title": "5.1.1 Single-Model Fine-Tuning (All-In-One)",
      "text": "We fine-tune an 8B Llama model to simultaneously predict supporting facts and the final answer. This approach tests our initial hypothesis that dividing a complex task into smaller sub-tasks might improve overall performance. Previously, the selector experiments indicated that splitting the selection process into two sub-tasks (paragraph- and sentence-level) did not yield a substantial improvement--sometimes even reducing performance. Figure 10: **Using sub-questions in the two-stage selector architecture** Single-stage Selector The second scenario employs the single-stage selector, which identifies both gold paragraphs and supporting facts in one pass. We compare it to the two-stage approach to see if there are gains from separating paragraph and sentence selection. 3) Two-stage Selector (Paragraph \\(+\\) Sentence), feeding supporting facts to the reader Here, paragraph selection happens first; the selected paragraphs are passed to a sentence selector, which filters out the supporting facts. The reader receives these supporting facts as input. 4) Two-stage Selector (Paragraph \\(+\\) Sentence), feeding gold paragraphs to the reader This scenario also uses a two-stage approach, but the reader is given the entire gold paragraphs (instead of just supporting facts). Our zero-shot experiments suggested that providing paragraphs vs. supporting facts did not heavily impact certain performance metrics; however, we wanted to observe if this strategy might reduce the impact of selector errors on the reader. 5) Two-stage Selector (Paragraph \\(+\\) Sentence) \\(+\\) Sub-questions, feeding supporting facts to the reader Same as scenario 3, but sub-questions (generated by a secondary large language model) are injected into the sentence selector. This aims to improve sentence-level retrieval. 6) Two-stage Selector (Paragraph \\(+\\) Sentence) \\(+\\) Sub-questions, feeding gold paragraphs to the reader Same as scenario 4, but again, sub-questions are introduced to aid the sentence selector. In every scenario except the first (the all-in-one approach), the reader can be one of three Backtrainus variants: * Backtrainus 8B * Backtrainus 8B \\(+\\) Chain of Thought (CoT) generated by a 70B model * Backtrainus 70BFigure 11: Illustration of six ways to connect the reader and selector components This allows us to compare the influence of model size and knowledge distillation via chain-of-thought prompts on the reader's performance. Table 11 presents the outcomes of all six integration methods, enumerating the performance on supporting-fact retrieval (Exact Match and F1), final-answer correctness, and the joint of both. Comparing scenario 1 (all-in-one fine-tuning) with others indicates that the single-model approach yields lower scores--by about two percentage points in both supporting-fact and answer retrieval. Hence, the hypothesis that dividing a complex QA task into simpler sub-tasks yields performance gains appears valid. The effect of chain-of-thought prompts is best observed by comparing rows where the reader has no CoT vs. those with CoT 70B. Although the improvement is modest, a Bactrainus 8B reader generally performs slightly better when guided by chain-of-thought data from a 70B model. \\begin{table} \\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{**Scenario**} & \\multirow{2}{*}{**Reader**} & \\multicolumn{2}{c|}{**Supporting Facts**} & \\multicolumn{2}{c|}{**Answer**} & \\multicolumn{2}{c|}{**joint**} \\\\ \\cline{3-8} & & EM & F1 & EM & F1 & EM & F1 \\\\ \\hline 1 & - & 63.42 & 88.50 & 71.24 & 83.31 & 47.93 & 75.96 \\\\ \\hline 2 & Bactrainus 8B & 65.74 & 89.27 & 73.24 & 85.41 & 50.84 & 77.90 \\\\ \\hline 2 & Bactrainus 8B + CoT 70B & 65.74 & 89.27 & 73.29 & 85.48 & 50.86 & 77.94 \\\\ \\hline 2 & Bactrainus 70B & 65.74 & 89.27 & 74.96 & 88.83 & 51.61 & 79.56 \\\\ \\hline 3 & Bactrainus 8B & 65.55 & 89.21 & 73.20 & 85.38 & 50.73 & 77.86 \\\\ \\hline 3 & Bactrainus 8B + CoT 70B & 65.55 & 89.21 & 73.22 & 85.41 & 50.74 & 77.88 \\\\ \\hline 3 & Bactrainus 70B & 65.55 & 89.21 & 74.92 & 88.80 & 51.54 & 79.53 \\\\ \\hline 4 & Bactrainus 8B & 65.55 & 89.21 & 71.18 & 82.92 & 48.28 & 76.41 \\\\ \\hline 4 & Bactrainus 8B + CoT 70B & 65.55 & 89.21 & 71.31 & 83.14 & 48.42 & 76.56 \\\\ \\hline 4 & Bactrainus 70B & 65.55 & 89.21 & 74.04 & 87.85 & 51.02 & 77.03 \\\\ \\hline 5 & Bactrainus 8B & **65.93** & **89.63** & 73.34 & 85.56 & 50.88 & 77.99 \\\\ \\hline 5 & Bactrainus 8B + CoT 70B & **65.93** & **89.63** & 73.36 & 85.60 & 50.91 & 78.02 \\\\ \\hline 5 & Bactrainus 70B & **65.93** & **89.63** & **75.07** & **89.01** & **51.73** & **79.70** \\\\ \\hline 6 & Bactrainus 8B & **65.93** & **89.63** & 71.18 & 82.92 & 48.28 & 76.41 \\\\ \\hline 6 & Bactrainus 8B + CoT 70B & **65.93** & **89.63** & 71.31 & 83.14 & 48.42 & 76.56 \\\\ \\hline 6 & Bactrainus 70B & **65.93** & **89.63** & 74.04 & 87.85 & 51.02 & 77.03 \\\\ \\hline \\end{tabular} \\end{table} Table 11: **Performance of different selector-reader integrations on HotpotQA (distractor setting)**Similarly, introducing sub-questions to the sentence selector (scenarios 5 and 6) yields better performance than the same configurations without sub-questions (scenarios 3 and 4), albeit with only a small gain. While zero-shot tests earlier suggested minimal differences between providing gold paragraphs or just supporting facts to the reader, in these fully fine-tuned scenarios, the difference becomes more pronounced. Readers fine-tuned specifically on supporting-fact inputs may underperform when given entire paragraphs. Evidently, the reader's sensitivity to input format can significantly impact results."
    },
    {
      "title": "5.3.1 Comparison To State-Of-The-Art",
      "text": "Finally, we benchmark the best of our proposed methods (scenario 5 with Bactrainus 70B, which provides the strongest results) against other leading approaches from the HotpotQA leaderboard. Table 12 shows that our method outperforms the existing baselines in terms of supporting facts, final answers, and their intersection. Across all metrics, Bactrainus 70B provides the strongest overall performance, confirming the utility of our multi-stage design (selector + reader), fine-tuned LLMs, and additional knowledge transfer steps. In short, these experiments validate both of our key hypotheses: Splitting the multi-hop QA task into smaller sub-tasks is beneficial. Distilling knowledge--via chain of thought or sub-question decomposition--can further improve answer quality, although the gains may be moderate. \\begin{table} \\begin{tabular}{|c|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{2}{c|}{**Supporting Facts**} & \\multicolumn{2}{c|}{**Answer**} & \\multicolumn{2}{c|}{**Joint**} \\\\ \\cline{2-7} & EM & F1 & EM & F1 & EM & F1 \\\\ \\hline **Bactrainus 70B** & 65.93 & 89.63 & **75.07** & **89.01** & **51.73** & **79.70** \\\\ \\hline **Bactrainus 8B + CoT 70B** & 65.93 & 89.63 & 73.36 & 85.60 & 50.91 & 78.02 \\\\ \\hline **Beam Retrieval (Zhang et al., 2023)** & **66.25** & **90.09** & 72.69 & 85.04 & 50.53 & 77.54 \\\\ \\hline **PipNet** & 63.71 & 89.41 & 72.26 & 84.86 & 48.76 & 76.69 \\\\ \\hline **Smoothing R3 (Yin et al., 2023)** & 65.44 & 89.55 & 72.07 & 84.34 & 49.73 & 76.69 \\\\ \\hline **FE2H on ALBERT (Li et al., 2023)** & 65.44 & 89.55 & 71.89 & 84.34 & 50.04 & 76.54 \\\\ \\hline \\end{tabular} \\end{table} Table 12: **Comparison of our best proposed method with previous state-of-the-art**Dividing the Task: Splitting the retrieval and reasoning processes generally improves performance by around 2% (supporting facts, final answers) compared to a single all-in-one approach. Larger Models & Knowledge Transfer: Employing a bigger model (70B) or chain-of-thought data from a 70B teacher model consistently yields slight but notable boosts."
    },
    {
      "title": "6 Conclusion",
      "text": "This study examined the capabilities of LLMs for MHQA and proposed a multi-component system comprising a selector and a reader. We conducted comprehensive experiments on the HotpotQA dataset under the distractor setting, leading to the following key insights: 1) Effectiveness of LLMs as a Selector Despite the conventional assumption that LLMs are not typically used for retrieval, our fine-tuned approaches demonstrated that they can effectively identify gold paragraphs and supporting sentences at near state-of-the-art levels. Even the single-stage selector performed competitively, highlighting the strong inherent understanding these models possess and the natural interdependence between paragraph and sentence selection in multi-hop data. 2) Improving Reader Performance via Model Scale and Chain of Thought Our experiments revealed that increasing model size (e.g., from 8B to 70B parameters) and adding CoT prompts--either directly or through guidance from a larger teacher model--enhanced multi-hop reasoning and final accuracy. While not every scenario showed dramatic gains, these findings confirm that knowledge transfer, in the form of structured step-by-step reasoning, can be crucial for elevating model performance on complex questions. 3) Advantages of a Modular Approach Over a Single-Model Setup Comparing a single all-in-one model (handling both selection and reading jointly) with a modular design (separating the selector and reader sub-tasks) indicated that breaking down the multi-hop QA process generally yields around a 2% improvement in both supporting-fact identification and final answers. This underscores the benefit of task decomposition and separate optimization for each component (selector-reader), as opposed to a monolithic end-to-end method. 4) Comparison with Prior Methods and Achieving Superior Results on HotpotQAOur best configuration--a two-stage selector coupled with a 70B reader--outperformed advanced existing methods on HotpotQA, surpassing them in metrics for supporting-fact detection and answer accuracy. These outcomes illustrate the value of combining large language models with targeted fine-tuning and auxiliary cues (such as chain-of-thought reasoning or question decomposition). Indeed, this strategy not only rivals but often exceeds traditional retrieval-based or encoder-only approaches. Overall, our findings suggest that a modular multi-hop solution--splitting the selector and reader while incorporating techniques like chain-of-thought and knowledge transfer--can significantly boost performance. Thus, leveraging large language models in multi-stage QA systems presents a promising avenue for tackling complex question-answering challenges."
    },
    {
      "title": "7 Future Works",
      "text": "Although our results highlight the high potential of large language models in multi-hop question answering, there remain significant research challenges and open questions for further advancements. Some promising directions include: 1. Scalability and Computational Optimization Scaling models beyond 70B parameters or using ensembles of multiple models could significantly improve reasoning capabilities. Nevertheless, computational costs and hardware limitations pose major constraints. Exploring model compression, distributed computing, and cost-effective techniques (e.g., LoRA, Adapters) remains crucial. 2. Generalization Across Languages and Domains Our study primarily focused on English data from the HotpotQA dataset. Extending and adapting the proposed methods to other languages and question types--especially in specialized fields (e.g., legal or medical)--is vital for assessing the broader applicability of our approach. 3. Interactive and Adaptive Learning Approaches In practical QA systems, users may pose follow-up questions in a conversational format. Designing multi-hop models capable of integrating immediate user feedback and refining their answers dynamically is a compelling topic for future research. 4. Enhanced Monitoring and Interpretation of AnswersWhile chain-of-though reasoning and question decomposition provide partial transparency into a model's internal processes, ambiguities in logic persist. Developing interpretability mechanisms and integrating automated or human-in-the-loop evaluations can increase system reliability and trustworthiness."
    },
    {
      "title": "5.1.5 Multi-Agent Approaches For Complex Multi-Hop Qa",
      "text": "In many complex scenarios, a single agent may not optimally handle all sub-tasks. Instead, multi-agent systems could be employed, wherein each specialized agent focuses on a particular sub-task--such as document retrieval, multi-hop inference, or user interaction--and coordinates with the others. This design can enhance both performance and flexibility, offering a more modular and scalable solution better suited to real-world applications."
    },
    {
      "title": "References",
      "text": "* Devlin et al. (2019) Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). * Rajpurkar et al. (2016) Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). SQuAD: 100,000+ Questions for Machine Comprehension of Text. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). * Yang et al. (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., & Manning, C. D. (2018). HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). * Hermann et al. (2015) Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., & Blunsom, P. (2015). Teaching Machines to Read and comprehend. Advances in Neural Information Processing Systems (NeurIPS). * Rajpurkar et al. (2018) Rajpurkar, P., Jia, R., & Liang, P. (2018). Know What You Don't Know: Unanswerable Questions for SQuAD. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL), Volume 2 (Short Papers). * Chen et al. (2017) Chen, D., Fisch, A., Weston, J., & Bordes, A. (2017). Reading Wikipedia to Answer Open-Domain Questions. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), Volume 1 (Long Papers). * Chen et al. (2018)Ho, X., Nguyen, A.-K. D., Sugawara, S., & Aizawa, A. (2020). Constructing a Multihop QA Dataset for Comprehensive Evaluation of Reasoning Steps. arXiv preprint arXiv:2011.01060. * Trivedi et al. (2021) Trivedi, H., Balasubramanian, N., Khot, T., & Sabharwal, A. (2021). MuSiQue: Multi-hop Questions via Single-hop Question Composition. arXiv preprint arXiv:2108.00573. * Chen et al. (2020) Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H., & Wang, W. Y. (2020). HybridQA: A Dataset of Multi-hop Question Answering over Tabular and Textual Data. Findings of the Association for Computational Linguistics: EMNLP 2020, 1026-1036. * Mihaylov et al. (2018) Mihaylov, T., Clark, P., Khot, T., & Sabharwal, A. (2018). Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). * Khot et al. (2020) Khot, T., Clark, P., Guerquin, M., Jansen, P., & Sabharwal, A. (2020). QASC: A Dataset for Question Answering via Sentence Composition. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 8082-8090. * Khashabi et al. (2018) Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S., & Roth, D. (2018). Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 252-262. * Tang & Yang (2024) Tang, Y., & Yang, Y. (2024). MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-hop Queries. arXiv preprint arXiv:2401.15391. * Chen et al. (2017) Chen, D., Fisch, A., Weston, J., & Bordes, A. (2017). Reading Wikipedia to Answer Open-Domain Questions. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), 1870-1879. * Clark et al. (2020) Clark, K., Luong, M.-T., Le, Q. V., & Manning, C. D. (2020). ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. Proceedings of the 8th International Conference on Learning Representations (ICLR). * Kratzwald & Feuerriegel (2018) Kratzwald, B., & Feuerriegel, S. (2018). Adaptive Document Retrieval for Deep Question Answering. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), 576-581. * Krizhevsky et al. (2017)Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., & Yih, W. T. (2020). Dense Passage Retrieval for Open-Domain Question Answering. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 6769-6781. * Das et al. (2019) Das, R., Zaheer, M., Dyer, C., & McCallum, A. (2019). Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2389-2399. * Lee et al. (2019) Lee, K., Chang, M.-W., Toutanova, K., & Yih, W. T. (2019). Latent Retrieval for Weakly Supervised Open Domain Question Answering. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), 6086-6096. * Zhang et al. (2020) Zhang, S., Yang, J., Qi, P., & Manning, C. D. (2020). Retrospective Reader for Machine Reading Comprehension. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 8625-8636. * Nair et al. (2023) Nair, I., Somasundaram, S., Saxena, A., & Goswami, K. (2023). Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering. arXiv preprint arXiv:2311.13565. * Han et al. (2022) Han, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Benson, L., Sun, L., et al. (2022). FOLIO: Natural Language Reasoning with First-Order Logic. arXiv preprint arXiv:2209.00840. * Zelikman et al. (2022) Zelikman, E., Wu, Y., Mu, J., & Goodman, N. D. (2022). STaR: Bootstraping Reasoning with Reasoning. arXiv preprint arXiv:2203.14465. * Wang et al. (2019) Wang, H., Yu, M., Guo, X., Das, R., Xiong, W., & Gao, T. (2019). Do Multi-hop Readers Dream of Reasoning Chains? In Proceedings of the 2nd Workshop on Machine Reading for Question Answering (pp. 91-97). * Saparov & He (2023) Saparov, A., & He, H. (2023). Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought. In The Eleventh International Conference on Learning Representations (ICLR). * Yavuz et al. (2022) Yavuz, S., Hashimoto, K., Zhou, Y., Keskar, N. S., & Xiong, C. (2022). Modeling Multi-hop Question Answering as Single Sequence Prediction. arXiv preprint arXiv:2205.09226. * Trivedi et al. (2023) Trivedi, H., Balasubramanian, N., Khot, T., & Sabharwal, A. (2023). Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. arXiv preprint arXiv:2212.10509. * Zhang et al. (2020)Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., & Sabharwal, A. (2023). Decomposed Prompting: A Modular Approach for Solving Complex Tasks. arXiv preprint arXiv:2210.02406. * Zhou et al. (2023) Zhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., et al. (2023). Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. In The Eleventh International Conference on Learning Representations (ICLR). * Zhou et al. (2022) Zhou, B., Richardson, K., Yu, X., & Roth, D. (2022). Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts. arXiv preprint arXiv:2210.16865. * Deng et al. (2022) Deng, Z., Zhu, Y., Chen, Y., Witbrock, M., & Riddle, P. (2022). Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering. Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence (IJCAI-22), 4093-4099. * Wu et al. (2024) Wu, J., Yang, L., Ji, Y., Huang, W., Karlsson, B. F., & Okumura, M. (2024). GenDec: A Robust Generative Question-decomposition Method for Multi-hop Reasoning. arXiv preprint arXiv:2402.11166. * Zhang et al. (2023) Zhang, J., Zhang, H., Zhang, D., Liu, Y., & Huang, S. (2023). Beam retrieval: General end-to-end retrieval for multi-hop question answering. arXiv Preprint arXiv:2308.08973. * Yin et al. (2023) Yin, Z., Wang, Y., Hu, X., Wu, Y., Yan, H., Zhang, X., Cao, Z., Huang, X., & Qiu, X. (2023). Rethinking label smoothing on multi-hop question answering. In China National Conference on Chinese Computational Linguistics (pp. 72-87). Singapore: Springer Nature. * Li et al. (2023) Li, X.-Y., Lei, W.-J., & Yang, Y.-B. (2023). From easy to hard: Two-stage selector and reader for multi-hop question answering. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE."
    }
  ]
}