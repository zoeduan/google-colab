{
  "title": "RoleEval: A Bilingual Role Evaluation Benchmark for Large Language Models",
  "authors": [
    "Tianhao Shen",
    "Sun Li",
    "Quan Tu",
    "Deyi Xiong",
    "Tianwen Wei",
    "Liang Zhao",
    "Lichang Zhang",
    "Bo Zhu",
    "Lijie Wang",
    "Haihua Yang",
    "Biye Li",
    "Cheng Cheng",
    "Weiwei Lü",
    "Rui Hu",
    "Chenxia Li",
    "Liu Yang",
    "Xilin Luo",
    "Xuejie Wu",
    "Lunan Liu",
    "Wenjun Cheng",
    "Peng Cheng",
    "Jianhao Zhang",
    "Xiaoyu Zhang",
    "Lei Lin",
    "Xi- Aokun Wang",
    "Yutuan Ma",
    "Chuanhai Dong",
    "Yanqi Sun",
    "Yifu Chen",
    "Yongyi Peng",
    "Xiaojuan Liang",
    "Shuicheng Yan",
    "Bigscience Workshop",
    "Teven Le Scao",
    "Angela Fan",
    "Christopher Akiki",
    "Ellie Pavlick",
    "Suzana Ilić",
    "Daniel Hesslow",
    "Roman Castagné",
    "Alexandra Sasha Luc- Cioni",
    "François Yvon",
    "Matthias Gallé",
    "Jonathan Tow",
    "Alexander M Rush",
    "Stella Biderman",
    "Albert Webson",
    "Sasanka Pawan",
    "Thomas Ammanamanchi",
    "Benoît Wang",
    "Niklas Sagot",
    "Albert Muennighoff",
    "Olatunji Vil- Lanova Del Moral",
    "Rachel Ruwase",
    "Stas Bawden",
    "Angelina Bekman",
    "Iz Mcmillan-Major",
    "Huu Belt- Agy",
    "Lucile Nguyen",
    "Samson Saulnier",
    "Pe- Dro Tan",
    "Victor Ortiz Suarez",
    "Hugo Sanh",
    "Yacine Laurençon",
    "Julien Jernite",
    "Margaret Launay",
    "Colin Mitchell",
    "Aaron Raffel",
    "Adi Gokaslan",
    "Aitor Simhi",
    "Alham Soroa",
    "Aji Fikri",
    "Amit Alfassy",
    "Anna Rogers",
    "Ariel Kreisberg Nitzav",
    "Canwen Xu",
    "Chenghao Mou",
    "Chris Emezue",
    "Christopher Klamm",
    "Colin Leong",
    "Daniel Van Strien",
    "David Ifeoluwa Adelani",
    "Dragomir Radev",
    "Eduardo González Ponferrada",
    "Efrat Lev- Kovizh",
    "Eyal Bar Natan Ethan Kim",
    "Francesco De Toni",
    "Gérard Dupont",
    "Germán Kruszewski",
    "Gi- Ada Pistilli",
    "Hady Elsahar",
    "Hamza Benyamina",
    "Hieu Tran",
    "Ian Yu",
    "Idris Abdulmumin",
    "Itziar Isaac Johnson",
    "Leon Weber",
    "Long Phan",
    "Loubna Ben",
    "Lu- Dovic Tanguy",
    "Manan Dey",
    "Manuel Romero Muñoz",
    "Maraim Masoud",
    "María Grandury",
    "Mario Šaško",
    "Maximin Max Huang",
    "Mike Mayank Singh",
    "Tian-Jian Jiang",
    "Minh Chien Vu",
    "Moham- Mad A Jauhar",
    "Mustafa Ghaleb",
    "Nishant Subramani",
    "Nora Kassner",
    "Nurulaqilla Khamis",
    "Olivier Nguyen",
    "Omar Espejel",
    "Ona De Gibert",
    "Paulo Villegas",
    "Pe- Ter Henderson",
    "Pierre Colombo",
    "Priscilla Amuok",
    "Quentin Lhoest",
    "Rheza Harliman",
    "Rishi Bommasani",
    "Roberto Luis López",
    "Rui Ribeiro",
    "Salomey Osei",
    "Sampo Pyysalo",
    "Sebastian Nagel",
    "Shamik Bose",
    "Shamsuddeen Hassan Muhammad",
    "Shanya Sharma",
    "Shayne Longpre",
    "Somaieh Nikpoor",
    "Stanislav Silber- Berg",
    "Suhas Pai",
    "Sydney Zink",
    "Tiago Timponi",
    "Timo Schick",
    "Tristan Thrush",
    "Valentin Danchev",
    "Vassilina Nikoulina",
    "Veronika Laippala",
    "Violette Lepercq",
    "Vrinda Prabhu",
    "Zaid Alyafeai",
    "Zeerak Ta- Lat",
    "Arun Raja",
    "Benjamin Heinzerling",
    "Chenglei Si",
    "Emre Davut",
    "Elizabeth Taşar",
    "Sabrina J Salesky",
    "Wilson Y Mielke",
    "Abheesht Lee",
    "Andrea Sharma",
    "Antoine Santilli",
    "Arnaud Chaffin",
    "Debajy- Oti Stiegler",
    "Eliza Datta",
    "Gunjan Szczechla",
    "Han Chhablani",
    "Harshit Wang",
    "Hendrik Pandey",
    "Jason Alan Strobelt",
    "Jos Fries",
    "Leo Rozen",
    "Lintang Gao",
    "M Sutawika",
    "Maged S Sai- Ful Bari",
    "Matteo Al-Shaibani",
    "Ni- Hal Manica",
    "Ryan Nayak",
    "Samuel Teehan",
    "Sheng Albanie",
    "Srulik Shen",
    "Stephen H Ben-David",
    "Taewoon Bach",
    "Tali Kim",
    "Thibault Bers",
    "Trishala Fevry",
    "Neeraj",
    "Deepak Narayanan",
    "Hatim Bourfoune",
    "Jared Casper",
    "Jeff Rasley",
    "Max Ryabinin",
    "Mayank Mishra",
    "Minjia Zhang",
    "Mohammad Shoeybi",
    "Myriam Peyrounette",
    "Nicolas Patry",
    "Nouamane Tazi",
    "Omar Sanseviero",
    "Patrick Von Platen",
    "Pierre Cornette",
    "Pierre François Lavallée",
    "Rémi Lacroix",
    "Samyam Rajbhandari",
    "San- Chit Gandhi",
    "Shaden Smith",
    "Stéphane Requena",
    "Suraj Patil",
    "Tim Dettmers",
    "Ahmed Baruwa",
    "Amanpreet Singh",
    "Anastasia Cheveleva",
    "Anne-Laure Ligozat",
    "Arjun Subramonian",
    "Aurélie Névéol",
    "Dan Garrette",
    "Deepak Tunuguntla",
    "Ehud Reiter",
    "Ekaterina Taktasheva",
    "Ekaterina Voloshina",
    "Eli Bog- Danov, Genta",
    "Indra Winata",
    "Hailey Schoelkopf",
    "Jan- Christoph Kalo",
    "Jekaterina Novikova",
    "Jessica Zosa Forde",
    "Jordan Clive",
    "Jungo Kasai",
    "Ken Kawamura",
    "Liam Hazan",
    "Marine Carpuat",
    "Miruna Clinciu",
    "Na- Joung Kim",
    "Newton Cheng",
    "Oleg Serikov",
    "Omer Antverg",
    "Oskar Van Der Wal",
    "Rui Zhang",
    "Ruochen Zhang",
    "Sebastian Gehrmann",
    "Shachar Mirkin",
    "Shani Pais",
    "Tatiana Shavrina",
    "Thomas Scialom",
    "Tian Yun",
    "Tomasz Limisiewicz",
    "Verena Rieser",
    "Vitaly Protasov",
    "Vladislav Mikhailov",
    "Yada Pruksachatkun",
    "Yonatan Belinkov",
    "Zachary Bamberger",
    "Zdeněk Kasner",
    "Al- Ice Rueda",
    "Amanda Pestana",
    "Amir Feizpour",
    "Ammar Khan",
    "Amy Faranak",
    "Ana Santos",
    "Anthony Hevia",
    "Antigona Unldreaj",
    "Arash Aghagol",
    "Arezoo Abdol- Lahi",
    "Aycha Tammour",
    "Azadeh Hajihosseini",
    "Bahareh Behroozi",
    "Benjamin Ajibade",
    "Car- los Bharat Saxena",
    "Muñoz Ferrandis",
    "Daniel Mcduff",
    "David Lansky",
    "Davis David",
    "Douwe Kiela",
    "Duong A Nguyen",
    "Edward Tan",
    "Emi Baylor",
    "Ez- Inwanne Ozoani",
    "Frankline Fatima Mirza",
    "Habib Rezanejad",
    "Hessie Jones",
    "Indrani Bhat- Tacharya",
    "Irene Solaiman",
    "Irina Sedenko",
    "Isar Ne- Jadgholi",
    "Jesse Passmore",
    "Josh Seltzer",
    "Julio Bonis Sanz",
    "Livia Dutra",
    "Mairon Samagaio",
    "Maraim El- Badri",
    "Margot Mieskes",
    "Marissa Gerchick",
    "Martha Akinlolu",
    "Michael Mckenna",
    "Mike Qiu",
    "Muhammed Ghauri",
    "Mykola Burynok",
    "Nafis Abrar",
    "Nazneen Ra- Jani",
    "Nour Elkott",
    "Nour Fahmy",
    "Olanrewaju Samuel",
    "Ran An",
    "Rasmus Kromann",
    "Ryan Hao",
    "Samira Al- Izadeh",
    "Sarmad Shubber",
    "Silas Wang",
    "Sourav Roy",
    "Sylvain Viguier",
    "Thanh Le",
    "Tobi Oyebade",
    "Trieu Le",
    "Yoyo Yang",
    "Abhinav Zach Nguyen",
    "Ramesh Kashyap",
    "Alfredo Palasciano",
    "Alison Callahan",
    "Anima Shukla",
    "Antonio Miranda-Escalada",
    "Ayush Singh",
    "Benjamin Beilharz",
    "Bo Wang",
    "Caio Brito",
    "Chenxi Zhou",
    "Chirag Jain",
    "Chuxin Xu",
    "Clémentine Fourrier",
    "Daniel León Periñán",
    "Daniel Molano",
    "Dian Yu",
    "Enrique Manjava- Cas",
    "Fabio Barth",
    "Florian Fuhrimann",
    "Gabriel Altay",
    "Giyaseddin Bayrak",
    "Gully Burns",
    "Helena U Vrabec",
    "Imane Bello",
    "Ishani Dash",
    "Jihyun Kang",
    "John Giorgi",
    "Jonas Golde",
    "Jose David Posada",
    "Ranga- Sai Karthik",
    "Lokesh Sivaraman",
    "Lu Bulchandani",
    "Luisa Liu",
    "Madeleine Shinzato",
    "Maiko Hahn De Bykhovetz",
    "Marc Takeuchi",
    "Maria A Pàmies",
    "Mari- Anna Castillo",
    "Mario Nezhurina",
    "Matthias Sänger",
    "Michael Samwald",
    "Michael Cullan",
    "Michiel Weinberg",
    "Mina De Wolf",
    "Minna Mihaljcic",
    "Moritz Liu",
    "Myung- Sun Freidank",
    "Natasha Kang",
    "Nathan Seelam",
    "Nicholas Dahlberg",
    "Nikolaus Michio Broad",
    "Pascale Muellner",
    "Patrick Fung",
    "Ramya Haller",
    "Renata Chandrasekhar",
    "Robert Eisenberg",
    "Rodrigo Martin",
    "Rosaline Canalli",
    "Ruisi Su",
    "Samuel Su",
    "Samuele Cahyawijaya",
    "Shlok S Garda",
    "Shubhanshu Deshmukh",
    "Sid Mishra",
    "Simon Ki- Blawi",
    "Sinee Ott",
    "Srishti Sang-Aroonsiri",
    "Stefan Ku- Mar",
    "Sushil Schweter",
    "Tanmay Bharati",
    "Théo Laud",
    "Tomoya Gigant",
    "Wojciech Kainuma",
    "Kusa",
    "Zhiheng Xi",
    "Wenxiang Chen",
    "Xin Guo",
    "Xinchao Xu",
    "Zhibin Gou",
    "Wenquan Wu",
    "Zheng-Yu Niu",
    "Aiyuan Yang",
    "Bin Xiao",
    "Bingning Wang",
    "Borong Zhang",
    "Ce Bian",
    "Chao Yin",
    "Chenxu Lv",
    "Da Pan",
    "Dian Wang",
    "Dong Yan",
    "Fan Yang",
    "Fei Deng",
    "Feng Wang",
    "Feng Liu",
    "Guangwei Ai",
    "Haizhou Dong",
    "Hang Zhao",
    "Haoze Xu",
    "Hongda Sun",
    "Hui Zhang",
    "Ji- Aming Liu",
    "Jian Ji",
    "Juntao Xie",
    "Kun Dai",
    "Lei Fang",
    "Liang Su",
    "Lifeng Song",
    "Liyun Liu",
    "Luyao Ru",
    "Mang Ma",
    "Mickel Wang",
    "Mingan Liu",
    "Nuolan Lin",
    "Pei- Dong Nie",
    "Ruiyang Guo",
    "Tao Sun",
    "Tianpeng Zhang",
    "Tianyu Li",
    "Wei Li",
    "Weipeng Cheng",
    "Xiangrong Chen",
    "Xiaochuan Zeng",
    "Xiaoxi Wang",
    "Xin Chen",
    "Xin Men",
    "Xuehai Yu",
    "Yanjun Pan",
    "Yiding Shen",
    "Yiyu Wang",
    "Youxin Li",
    "Yuchen Jiang",
    "Yupeng Gao",
    "Zhang"
  ],
  "abstract": "\n The rapid evolution of large language models necessitates effective benchmarks for evaluating their role knowledge, which is essential for establishing connections with the real world and providing more immersive interactions. This paper introduces RoleEval, a bilingual benchmark designed to assess the memorization, utilization, and reasoning capabilities of role knowledge. RoleEval comprises RoleEval-Global (including internationally recognized characters) and RoleEval-Chinese (including characters popular in China), with 6,000 Chinese-English parallel multiple-choice questions focusing on 300 influential people and fictional characters drawn from a variety of domains including celebrities, anime, comics, movies, TV series, games, and fictions. These questions cover basic knowledge and multi-hop reasoning abilities, aiming to systematically probe various aspects such as personal information, relationships, abilities, and experiences of the characters. To maintain high standards, we perform a hybrid quality check process combining both automatic and human verification, ensuring that the questions are diverse, challenging, and discriminative. Our extensive evaluations with RoleEval across various open-source and proprietary large language models, under both the zero-and fewshot settings, reveal insightful findings. Notably, while GPT-4 outperforms other models on RoleEval-Global, Chinese large language models excel on RoleEval-Chinese, highlighting significant knowledge distribution differences. We expect that RoleEval would highlight the significance of assessing role knowledge for large language models across various languages and cultural settings. 1    \n",
  "references": [
    {
      "id": null,
      "title": "RoleEval: A Bilingual Role Evaluation Benchmark for Large Language Models",
      "authors": [
        "Tianhao Shen",
        "Sun Li",
        "Quan Tu",
        "Deyi Xiong",
        "Tianwen Wei",
        "Liang Zhao",
        "Lichang Zhang",
        "Bo Zhu",
        "Lijie Wang",
        "Haihua Yang",
        "Biye Li",
        "Cheng Cheng",
        "Weiwei Lü",
        "Rui Hu",
        "Chenxia Li",
        "Liu Yang",
        "Xilin Luo",
        "Xuejie Wu",
        "Lunan Liu",
        "Wenjun Cheng",
        "Peng Cheng",
        "Jianhao Zhang",
        "Xiaoyu Zhang",
        "Lei Lin",
        "Xi- Aokun Wang",
        "Yutuan Ma",
        "Chuanhai Dong",
        "Yanqi Sun",
        "Yifu Chen",
        "Yongyi Peng",
        "Xiaojuan Liang",
        "Shuicheng Yan",
        "Bigscience Workshop",
        "Teven Le Scao",
        "Angela Fan",
        "Christopher Akiki",
        "Ellie Pavlick",
        "Suzana Ilić",
        "Daniel Hesslow",
        "Roman Castagné",
        "Alexandra Sasha Luc- Cioni",
        "François Yvon",
        "Matthias Gallé",
        "Jonathan Tow",
        "Alexander M Rush",
        "Stella Biderman",
        "Albert Webson",
        "Sasanka Pawan",
        "Thomas Ammanamanchi",
        "Benoît Wang",
        "Niklas Sagot",
        "Albert Muennighoff",
        "Olatunji Vil- Lanova Del Moral",
        "Rachel Ruwase",
        "Stas Bawden",
        "Angelina Bekman",
        "Iz Mcmillan-Major",
        "Huu Belt- Agy",
        "Lucile Nguyen",
        "Samson Saulnier",
        "Pe- Dro Tan",
        "Victor Ortiz Suarez",
        "Hugo Sanh",
        "Yacine Laurençon",
        "Julien Jernite",
        "Margaret Launay",
        "Colin Mitchell",
        "Aaron Raffel",
        "Adi Gokaslan",
        "Aitor Simhi",
        "Alham Soroa",
        "Aji Fikri",
        "Amit Alfassy",
        "Anna Rogers",
        "Ariel Kreisberg Nitzav",
        "Canwen Xu",
        "Chenghao Mou",
        "Chris Emezue",
        "Christopher Klamm",
        "Colin Leong",
        "Daniel Van Strien",
        "David Ifeoluwa Adelani",
        "Dragomir Radev",
        "Eduardo González Ponferrada",
        "Efrat Lev- Kovizh",
        "Ethan Kim",
        "Francesco De Toni",
        "Gérard Dupont",
        "Germán Kruszewski",
        "Gi- Ada Pistilli",
        "Hady Elsahar",
        "Hamza Benyamina",
        "Hieu Tran",
        "Ian Yu",
        "Idris Abdulmumin",
        "Isaac Johnson",
        "Leon Weber",
        "Long Phan",
        "Loubna Ben",
        "Lu- Dovic Tanguy",
        "Manan Dey",
        "Manuel Romero Muñoz",
        "Maraim Masoud",
        "María Grandury",
        "Mario Šaško",
        "Max Huang",
        "Mayank Singh",
        "Tian-Jian Jiang",
        "Minh Chien Vu",
        "Moham- Mad A Jauhar",
        "Mustafa Ghaleb",
        "Nishant Subramani",
        "Nora Kassner",
        "Nurulaqilla Khamis",
        "Olivier Nguyen",
        "Omar Espejel",
        "Ona De Gibert",
        "Paulo Villegas",
        "Pe- Ter Henderson",
        "Pierre Colombo",
        "Priscilla Amuok",
        "Quentin Lhoest",
        "Rheza Harliman",
        "Rishi Bommasani",
        "Roberto Luis López",
        "Rui Ribeiro",
        "Salomey Osei",
        "Sampo Pyysalo",
        "Sebastian Nagel",
        "Shamik Bose",
        "Shamsuddeen Hassan Muhammad",
        "Shanya Sharma",
        "Shayne Longpre",
        "Somaieh Nikpoor",
        "Stanislav Silber- Berg",
        "Suhas Pai",
        "Sydney Zink",
        "Tiago Timponi",
        "Timo Schick",
        "Tristan Thrush",
        "Valentin Danchev",
        "Vassilina Nikoulina",
        "Veronika Laippala",
        "Violette Lepercq",
        "Vrinda Prabhu",
        "Zaid Alyafeai",
        "Zeerak Ta- Lat",
        "Arun Raja",
        "Benjamin Heinzerling",
        "Chenglei Si",
        "Emre Davut",
        "Elizabeth Taşar",
        "Sabrina J Salesky",
        "Wilson Y Mielke",
        "Abheesht Lee",
        "Andrea Sharma",
        "Antoine Santilli",
        "Arnaud Chaffin",
        "Debajy- Oti Stiegler",
        "Eliza Datta",
        "Gunjan Szczechla",
        "Han Chhablani",
        "Harshit Wang",
        "Hendrik Pandey",
        "Jason Alan Strobelt",
        "Jos Fries",
        "Leo Rozen",
        "Lintang Gao",
        "M Sutawika",
        "Maged S Sai- Ful Bari",
        "Matteo Al-Shaibani",
        "Ni- Hal Manica",
        "Ryan Nayak",
        "Samuel Teehan",
        "Sheng Albanie",
        "Srulik Shen",
        "Stephen H Ben-David",
        "Taewoon Bach",
        "Tali Kim",
        "Thibault Bers",
        "Trishala Fevry",
        "Neeraj",
        "Deepak Narayanan",
        "Hatim Bourfoune",
        "Jared Casper",
        "Jeff Rasley",
        "Max Ryabinin",
        "Mayank Mishra",
        "Minjia Zhang",
        "Mohammad Shoeybi",
        "Myriam Peyrounette",
        "Nicolas Patry",
        "Nouamane Tazi",
        "Omar Sanseviero",
        "Patrick Von Platen",
        "Pierre Cornette",
        "Pierre François Lavallée",
        "Rémi Lacroix",
        "Samyam Rajbhandari",
        "San- Chit Gandhi",
        "Shaden Smith",
        "Stéphane Requena",
        "Suraj Patil",
        "Tim Dettmers",
        "Ahmed Baruwa",
        "Amanpreet Singh",
        "Anastasia Cheveleva",
        "Anne-Laure Ligozat",
        "Arjun Subramonian",
        "Aurélie Névéol",
        "Dan Garrette",
        "Deepak Tunuguntla",
        "Ehud Reiter",
        "Ekaterina Taktasheva",
        "Ekaterina Voloshina",
        "Eli Bog- Danov, Genta",
        "Indra Winata",
        "Hailey Schoelkopf",
        "Jan- Christoph Kalo",
        "Jekaterina Novikova",
        "Jessica Zosa Forde",
        "Jordan Clive",
        "Jungo Kasai",
        "Ken Kawamura",
        "Liam Hazan",
        "Marine Carpuat",
        "Miruna Clinciu",
        "Na- Joung Kim",
        "Newton Cheng",
        "Oleg Serikov",
        "Omer Antverg",
        "Oskar Van Der Wal",
        "Rui Zhang",
        "Ruochen Zhang",
        "Sebastian Gehrmann",
        "Shachar Mirkin",
        "Shani Pais",
        "Tatiana Shavrina",
        "Thomas Scialom",
        "Tian Yun",
        "Tomasz Limisiewicz",
        "Verena Rieser",
        "Vitaly Protasov",
        "Vladislav Mikhailov",
        "Yada Pruksachatkun",
        "Yonatan Belinkov",
        "Zachary Bamberger",
        "Zdeněk Kasner",
        "Al- Ice Rueda",
        "Amanda Pestana",
        "Amir Feizpour",
        "Ammar Khan",
        "Amy Faranak",
        "Ana Santos",
        "Anthony Hevia",
        "Antigona Unldreaj",
        "Arash Aghagol",
        "Arezoo Abdol- Lahi",
        "Aycha Tammour",
        "Azadeh Hajihosseini",
        "Bahareh Behroozi",
        "Benjamin Ajibade",
        "Bharat Saxena",
        "Muñoz Ferrandis",
        "Daniel Mcduff",
        "David Lansky",
        "Davis David",
        "Douwe Kiela",
        "Duong A Nguyen",
        "Edward Tan",
        "Emi Baylor",
        "Ez- Inwanne Ozoani",
        "Fatima Mirza",
        "Habib Rezanejad",
        "Hessie Jones",
        "Indrani Bhat- Tacharya",
        "Irene Solaiman",
        "Irina Sedenko",
        "Isar Ne- Jadgholi",
        "Jesse Passmore",
        "Josh Seltzer",
        "Julio Bonis Sanz",
        "Livia Dutra",
        "Mairon Samagaio",
        "Maraim El- Badri",
        "Margot Mieskes",
        "Marissa Gerchick",
        "Martha Akinlolu",
        "Michael Mckenna",
        "Mike Qiu",
        "Muhammed Ghauri",
        "Mykola Burynok",
        "Nafis Abrar",
        "Nazneen Ra- Jani",
        "Nour Elkott",
        "Nour Fahmy",
        "Olanrewaju Samuel",
        "Ran An",
        "Rasmus Kromann",
        "Ryan Hao",
        "Samira Al- Izadeh",
        "Sarmad Shubber",
        "Silas Wang",
        "Sourav Roy",
        "Sylvain Viguier",
        "Thanh Le",
        "Tobi Oyebade",
        "Trieu Le",
        "Yoyo Yang",
        "Zach Nguyen",
        "Ramesh Kashyap",
        "Alfredo Palasciano",
        "Alison Callahan",
        "Anima Shukla",
        "Antonio Miranda-Escalada",
        "Ayush Singh",
        "Benjamin Beilharz",
        "Bo Wang",
        "Caio Brito",
        "Chenxi Zhou",
        "Chirag Jain",
        "Chuxin Xu",
        "Clémentine Fourrier",
        "Daniel León Periñán",
        "Daniel Molano",
        "Dian Yu",
        "Enrique Manjava- Cas",
        "Fabio Barth",
        "Florian Fuhrimann",
        "Gabriel Altay",
        "Giyaseddin Bayrak",
        "Gully Burns",
        "Helena U Vrabec",
        "Imane Bello",
        "Ishani Dash",
        "Jihyun Kang",
        "John Giorgi",
        "Jonas Golde",
        "Jose David Posada",
        "Ranga- Sai Karthik",
        "Lokesh Sivaraman",
        "Lu Bulchandani",
        "Luisa Liu",
        "Madeleine Shinzato",
        "Maiko Hahn De Bykhovetz",
        "Marc Takeuchi",
        "Maria A Pàmies",
        "Mari- Anna Castillo",
        "Mario Nezhurina",
        "Matthias Sänger",
        "Michael Samwald",
        "Michael Cullan",
        "Michiel Weinberg",
        "Mina De Wolf",
        "Minna Mihaljcic",
        "Moritz Liu",
        "Myung- Sun Freidank",
        "Natasha Kang",
        "Nathan Seelam",
        "Nicholas Dahlberg",
        "Nikolaus Michio Broad",
        "Pascale Muellner",
        "Patrick Fung",
        "Ramya Haller",
        "Renata Chandrasekhar",
        "Robert Eisenberg",
        "Rodrigo Martin",
        "Rosaline Canalli",
        "Ruisi Su",
        "Samuel Su",
        "Samuele Cahyawijaya",
        "Shlok S Garda",
        "Shubhanshu Deshmukh",
        "Sid Mishra",
        "Simon Ki- Blawi",
        "Sinee Ott",
        "Srishti Sang-Aroonsiri",
        "Stefan Ku- Mar",
        "Sushil Schweter",
        "Tanmay Bharati",
        "Théo Laud",
        "Tomoya Gigant",
        "Wojciech Kainuma",
        "Kusa",
        "Zhiheng Xi",
        "Wenxiang Chen",
        "Xin Guo",
        "Xinchao Xu",
        "Zhibin Gou",
        "Wenquan Wu",
        "Zheng-Yu Niu",
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Ce Bian",
        "Chao Yin",
        "Chenxu Lv",
        "Da Pan",
        "Dian Wang",
        "Dong Yan",
        "Fan Yang",
        "Fei Deng",
        "Feng Wang",
        "Feng Liu",
        "Guangwei Ai",
        "Haizhou Dong",
        "Hang Zhao",
        "Haoze Xu",
        "Hongda Sun",
        "Hui Zhang",
        "Ji- Aming Liu",
        "Jian Ji",
        "Juntao Xie",
        "Kun Dai",
        "Lei Fang",
        "Liang Su",
        "Lifeng Song",
        "Liyun Liu",
        "Luyao Ru",
        "Mang Ma",
        "Mickel Wang",
        "Mingan Liu",
        "Nuolan Lin",
        "Pei- Dong Nie",
        "Ruiyang Guo",
        "Tao Sun",
        "Tianpeng Zhang",
        "Tianyu Li",
        "Wei Li",
        "Weipeng Cheng",
        "Xiangrong Chen",
        "Xiaochuan Zeng",
        "Xiaoxi Wang",
        "Xin Chen",
        "Xin Men",
        "Xuehai Yu",
        "Yanjun Pan",
        "Yiding Shen",
        "Yiyu Wang",
        "Youxin Li",
        "Yuchen Jiang",
        "Yupeng Gao",
        "Zhang"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "MPCHAT: Towards Multimodal Persona-Grounded Conversation",
      "authors": [
        "References Jaewoo Ahn",
        "Yeda Song",
        "Sangdoo Yun",
        "Gunhee Kim"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.189"
    },
    {
      "id": "b1",
      "title": "The falcon series of open language models",
      "authors": [
        "Ebtesam Almazrouei",
        "Hamza Alobeidli",
        "Abdulaziz Alshamsi",
        "Alessandro Cappelli",
        "Ruxandra Cojocaru",
        "Mérouane Debbah",
        "Étienne Goffinet",
        "Daniel Hesslow",
        "Julien Launay",
        "Quentin Malartic"
      ],
      "year": "2023",
      "venue": "The falcon series of open language models",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang",
        "Binyuan Hui",
        "Luo Ji",
        "Mei Li",
        "Junyang Lin",
        "Runji Lin",
        "Dayiheng Liu",
        "Gao Liu",
        "Chengqiang Lu",
        "Keming Lu",
        "Jianxin Ma",
        "Rui Men",
        "Xingzhang Ren",
        "Xuancheng Ren",
        "Chuanqi Tan",
        "Sinan Tan",
        "Jianhong Tu",
        "Peng Wang",
        "Shijie Wang",
        "Wei Wang",
        "Shengguang Wu",
        "Benfeng Xu",
        "Jin Xu",
        "An Yang",
        "Hao Yang",
        "Jian Yang",
        "Shusheng Yang",
        "Yang Yao",
        "Bowen Yu",
        "Hongyi Yuan",
        "Zheng Yuan",
        "Jianwei Zhang",
        "Xingxuan Zhang",
        "Yichang Zhang",
        "Zhenru Zhang",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
      "authors": [
        "Stella Biderman",
        "Hailey Schoelkopf",
        "Quentin Anthony",
        "Herbie Bradley",
        "O' Kyle",
        "Eric Brien",
        "Mohammad Hallahan",
        "Shivanshu Aflah Khan",
        "Purohit",
        "Edward Usvsn Sai Prashanth",
        "Raff"
      ],
      "year": "",
      "venue": "Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca",
      "authors": [
        "Yiming Cui",
        "Ziqing Yang",
        "Xin Yao"
      ],
      "year": "2023",
      "venue": "Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "GLM: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.26"
    },
    {
      "id": "b6",
      "title": "",
      "authors": [
        "Leo Gao",
        "Jonathan Tow",
        "Stella Baber Abbasi",
        "Sid Biderman",
        "Anthony Black",
        "Charles Dipofi",
        "Laurence Foster",
        "Jeffrey Golding",
        "Alain Hsu",
        "Haonan Le Noac'h",
        "Kyle Li",
        "Niklas Mcdonell",
        "Chris Muennighoff",
        "Jason Ociepa",
        "Laria Phang",
        "Hailey Reynolds",
        "Aviya Schoelkopf",
        "Lintang Skowron",
        "Eric Sutawika",
        "Tang"
      ],
      "year": "",
      "venue": "Anish Thite",
      "doi": "10.5281/zenodo.10256836"
    },
    {
      "id": "b7",
      "title": "Measuring massive multitask language understanding",
      "authors": [
        "Dan Hendrycks",
        "Collin Burns",
        "Steven Basart",
        "Andy Zou",
        "Mantas Mazeika",
        "Dawn Song",
        "Jacob Steinhardt"
      ],
      "year": "2021",
      "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Maosong Sun, and Junxian He. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
      "authors": [
        "Yuzhen Huang",
        "Yuzhuo Bai",
        "Zhihao Zhu",
        "Junlei Zhang",
        "Jinghan Zhang",
        "Tangjun Su",
        "Junteng Liu",
        "Chuancheng Lv",
        "Yikai Zhang",
        "Jiayi Lei",
        "Yao Fu"
      ],
      "year": "",
      "venue": "Maosong Sun, and Junxian He. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "",
      "authors": [
        "Albert Q Jiang",
        "Alexandre Sablayrolles",
        "Arthur Mensch",
        "Chris Bamford",
        "Devendra Singh Chaplot",
        "Diego De Las Casas",
        "Florian Bressand",
        "Gianna Lengyel",
        "Guillaume Lample",
        "Lucile Saulnier",
        "Renard Lélio",
        "Marie-Anne Lavaud",
        "Pierre Lachaux",
        "Teven Stock",
        "Thibaut Le Scao",
        "Thomas Lavril",
        "Timothée Wang",
        "William El Lacroix",
        "Sayed"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Pingyu Wu, and Haozhen Sun. 2023a. ChatHaruhi: Reviving Anime Character in Reality via Large Language Model",
      "authors": [
        "Cheng Li",
        "Ziang Leng",
        "Chenxi Yan",
        "Junyi Shen",
        "Hao Wang",
        "M I Weishi",
        "Yaying Fei",
        "Xiaoyang Feng",
        "Song Yan",
        "Haosheng Wang",
        "Linkang Zhan",
        "Yaokai Jia"
      ],
      "year": "",
      "venue": "Pingyu Wu, and Haozhen Sun. 2023a. ChatHaruhi: Reviving Anime Character in Reality via Large Language Model",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023b. CMMLU: Measuring massive multitask language understanding in Chinese",
      "authors": [
        "Haonan Li",
        "Yixuan Zhang",
        "Fajri Koto",
        "Yifei Yang",
        "Hai Zhao"
      ],
      "year": "",
      "venue": "Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023b. CMMLU: Measuring massive multitask language understanding in Chinese",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
      "authors": [
        "Abhilasha Bill Yuchen Lin",
        "Ximing Ravichander",
        "Nouha Lu",
        "Melanie Dziri",
        "Khyathi Sclar",
        "Chandra Chandu",
        "Yejin Bhagavatula",
        "Choi"
      ],
      "year": "2023",
      "venue": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Towards emotional support dialog systems",
      "authors": [
        "Siyang Liu",
        "Chujie Zheng",
        "Orianna Demasi",
        "Sahand Sabour",
        "Yu Li",
        "Zhou Yu",
        "Yong Jiang",
        "Minlie Huang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meetof the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.269"
    },
    {
      "id": "b14",
      "title": "",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll L Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Character-LLM: A Trainable Agent for Role-Playing",
      "authors": [
        "Yunfan Shao",
        "Linyang Li",
        "Junqi Dai",
        "Xipeng Qiu"
      ],
      "year": "2023",
      "venue": "Character-LLM: A Trainable Agent for Role-Playing",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Am I me or you? state-of-the-art dialogue models cannot maintain an identity",
      "authors": [
        "Kurt Shuster",
        "Jack Urbanek",
        "Arthur Szlam",
        "Jason Weston"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2022",
      "doi": "10.18653/v1/2022.findings-naacl.182"
    },
    {
      "id": "b18",
      "title": "Edouard Grave, and Guillaume Lample. 2023a. LLaMA: Open and Efficient Foundation Language Models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurelien Rodriguez",
        "Armand Joulin"
      ],
      "year": "",
      "venue": "Edouard Grave, and Guillaume Lample. 2023a. LLaMA: Open and Efficient Foundation Language Models",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "2023b. Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale",
        "Dan Bikel",
        "Lukas Blecher",
        "Cristian Canton Ferrer",
        "Moya Chen",
        "Guillem Cucurull",
        "David Esiobu",
        "Jude Fernandes",
        "Jeremy Fu",
        "Wenyin Fu",
        "Brian Fuller",
        "Cynthia Gao",
        "Vedanuj Goswami",
        "Naman Goyal",
        "Anthony Hartshorn",
        "Saghar Hosseini",
        "Rui Hou",
        "Hakan Inan",
        "Marcin Kardas",
        "Viktor Kerkez",
        "Madian Khabsa",
        "Isabel Kloumann",
        "Artem Korenev",
        "Punit Singh Koura",
        "Marie-Anne Lachaux",
        "Thibaut Lavril",
        "Jenya Lee",
        "Diana Liskovich",
        "Yinghai Lu",
        "Yuning Mao",
        "Xavier Martinet",
        "Todor Mihaylov",
        "Pushkar Mishra",
        "Igor Molybog",
        "Yixin Nie",
        "Andrew Poulton",
        "Jeremy Reizenstein",
        "Rashi Rungta",
        "Kalyan Saladi",
        "Alan Schelten",
        "Ruan Silva",
        "Eric Michael Smith",
        "Ranjan Subramanian",
        "Ellen Xiaoqing",
        "Binh Tan",
        "Ross Tang",
        "Adina Taylor",
        "Jian Williams",
        "Puxin Xiang Kuan",
        "Zheng Xu",
        "Iliyan Yan",
        "Yuchen Zarov",
        "Chen Zhang",
        "Xueyang Ma",
        "Zeyu Feng",
        "Hao Zhang",
        "Jingsen Yang",
        "Zhiyuan Zhang",
        "Jiakai Chen",
        "Xu Tang",
        "Yankai Chen",
        "Wayne Xin Lin",
        "Zhewei Zhao",
        "Wei"
      ],
      "year": "",
      "venue": "2023b. Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models",
      "authors": [
        "Zekun Moore",
        "Wang",
        "Zhongyuan Peng",
        "Haoran Que",
        "Jiaheng Liu",
        "Wangchunshu Zhou",
        "Yuhan Wu",
        "Hongcheng Guo",
        "Ruitong Gan",
        "Zehao Ni",
        "Man Zhang",
        "Zhaoxiang Zhang",
        "Wanli Ouyang",
        "Ke Xu",
        "Wenhu Chen",
        "Jie Fu",
        "Junran Peng"
      ],
      "year": "2023",
      "venue": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Jason Weston, Jack Urbanek, and Mojtaba Komeili. 2023a. Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models",
      "authors": [
        "Jimmy Wei",
        "Kurt Shuster",
        "Arthur Szlam"
      ],
      "year": "",
      "venue": "Jason Weston, Jack Urbanek, and Mojtaba Komeili. 2023a. Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "GLM-130B: An Open Bilingual Pre-trained Model",
      "authors": [
        "Aohan Zeng",
        "Xiao Liu",
        "Zhengxiao Du",
        "Zihan Wang",
        "Hanyu Lai",
        "Ming Ding",
        "Zhuoyi Yang",
        "Yifan Xu",
        "Wendi Zheng",
        "Xiao Xia",
        "Weng Lam Tam",
        "Zixuan Ma",
        "Yufei Xue",
        "Jidong Zhai",
        "Wenguang Chen",
        "Peng Zhang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "GLM-130B: An Open Bilingual Pre-trained Model",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Personalizing dialogue agents: I have a dog, do you have pets too?",
      "authors": [
        "Saizheng Zhang",
        "Emily Dinan",
        "Jack Urbanek",
        "Arthur Szlam",
        "Douwe Kiela",
        "Jason Weston"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1205"
    },
    {
      "id": "b24",
      "title": "Personalized Dialogue Generation with Diversified Traits",
      "authors": [
        "Yinhe Zheng",
        "Guanyi Chen",
        "Minlie Huang",
        "Song Liu",
        "Xuan Zhu"
      ],
      "year": "2019",
      "venue": "Personalized Dialogue Generation with Diversified Traits",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "2023a. LIMA: Less Is More for Alignment",
      "authors": [
        "Chunting Zhou",
        "Pengfei Liu",
        "Puxin Xu",
        "Srini Iyer",
        "Jiao Sun",
        "Yuning Mao",
        "Xuezhe Ma",
        "Avia Efrat",
        "Ping Yu",
        "Lili Yu",
        "Susan Zhang",
        "Gargi Ghosh",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Omer Levy"
      ],
      "year": "",
      "venue": "2023a. LIMA: Less Is More for Alignment",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Jie Tang, and Minlie Huang. 2023b. Character-GLM: Customizing Chinese Conversational AI Characters with Large Language Models",
      "authors": [
        "Jinfeng Zhou",
        "Zhuang Chen",
        "Dazhen Wan",
        "Bosi Wen",
        "Yi Song",
        "Jifan Yu",
        "Yongkang Huang",
        "Libiao Peng",
        "Jiaming Yang",
        "Xiyao Xiao",
        "Sahand Sabour",
        "Xiaohan Zhang",
        "Wenjing Hou",
        "Yijia Zhang",
        "Yuxiao Dong"
      ],
      "year": "",
      "venue": "Jie Tang, and Minlie Huang. 2023b. Character-GLM: Customizing Chinese Conversational AI Characters with Large Language Models",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Zero-shot results on RoleEval (en) in five categories: celebrities (CE), anime and comics (AC), movie and TV series (MT), games (GA), and fiction (FI)",
      "authors": [],
      "year": "",
      "venue": "Zero-shot results on RoleEval (en) in five categories: celebrities (CE), anime and comics (AC), movie and TV series (MT), games (GA), and fiction (FI)",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "",
      "authors": [
        "Chinese-Llama"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Table 7: Results by knowledge and reasoning questions in RoleEval",
      "authors": [],
      "year": "",
      "venue": "Table 7: Results by knowledge and reasoning questions in RoleEval",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "",
      "authors": [],
      "year": "",
      "venue": "Model RoleEval-Global",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Reasoning (600 questions) Knowledge (1,700 questions) Reasoning (300 questions) Zero-shot Few-shot Zero-shot Few-shot Zero-shot Few-shot Zero-shot Few-shot",
      "authors": [],
      "year": "",
      "venue": "Reasoning (600 questions) Knowledge (1,700 questions) Reasoning (300 questions) Zero-shot Few-shot Zero-shot Few-shot Zero-shot Few-shot Zero-shot Few-shot",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "",
      "authors": [
        "Chinese-Llama"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Roleeval: A Bilingual Role Evaluation Benchmark For Large Language Models",
      "text": "Tianhao Shen\\({}^{1}\\), Sun Li\\({}^{2}\\), Quan Tu\\({}^{3}\\), Deyi Xiong\\({}^{1}\\) \\({}^{1}\\) College of Intelligence and Computing, Tianjin University, Tianjin, China \\({}^{2}\\) China Academy of Information and Communications Technology, Beijing, China \\({}^{3}\\) Gaoling School of Artificial Intelligence, Renmin University of China {thshen,dyxiong}@tju.edu.cn lisun@caict.ac.cn quantu@ruc.edu.cn * Corresponding author."
    },
    {
      "title": "Abstract",
      "text": "The rapid evolution of large language models necessitates effective benchmarks for evaluating their role knowledge, which is essential for establishing connections with the real world and providing more immersive interactions. This paper introduces RoleEval, a bilingual benchmark designed to assess the memorization, utilization, and reasoning capabilities of role knowledge. RoleEval comprises _RoleEval-Global_ (including internationally recognized characters) and _RoleEval-Chinese_ (including characters popular in China), with 6,000 Chinese-English parallel multiple-choice questions focusing on 300 influential people and fictional characters drawn from a variety of domains including celebrities, anime, comics, movies, TV series, games, and fictions. These questions cover basic knowledge and multi-hop reasoning abilities, aiming to systematically probe various aspects such as personal information, relationships, abilities, and experiences of the characters. To maintain high standards, we perform a hybrid quality check process combining both automatic and human verification, ensuring that the questions are diverse, challenging, and discriminative. Our extensive evaluations with RoleEval across various open-source and proprietary large language models, under both the zero- and few-shot settings, reveal insightful findings. Notably, while GPT-4 outperforms other models on _RoleEval-Global_, Chinese large language models excel on _RoleEval-Chinese_, highlighting significant knowledge distribution differences. We expect that RoleEval would highlight the significance of assessing role knowledge for large language models across various languages and cultural settings.1 Footnote 1: Our dataset is available at [https://github.com/Magnetic2014/RoleEval](https://github.com/Magnetic2014/RoleEval)."
    },
    {
      "title": "1 Introduction",
      "text": "Recent years have witnessed the huge success of large language models (LLMs), and agents based on these models present immense potential for reshaping our engagement with machines with their expansive knowledge and remarkable predictive capabilities [23, 24]. The cornerstone of this transformation lies in the development of LLM agents with a keen perception of the real world, offering users a more immersive experience than ever before and laying a solid foundation for the emergence of applications such as Character AI2 and AI Dungeon.3 This necessitates the role-playing capabilities of these models, which aim to establish connections with real-world people or characters created by LLMs. Footnote 2: [https://beta.character.ai](https://beta.character.ai) Footnote 3: [https://aidungeon.com](https://aidungeon.com) Previous studies show that much of LLM knowledge is acquired during the pretraining phase [25, 26]. A comprehensive pretraining allows for better role-playing capabilities by covering a wide range of knowledge. This is crucial not just for representing existing characters but also for creating new, believable personas without prior history. Just as actors need a solid understanding of real-world facts to convincingly portray fictional characters, learning from real-world personas enables a model to authentically and consistently represent both real and imaginary characters. However, there is a shortage of systematic evaluation of role knowledge for these LLMs. Traditional persona-based evaluation benchmarks, such as PersonaChat [27] and Personal-Dialog [28], often rely on artificially constructed personas or occupations abstracted from a group of people, which lack the complexity and real-world connection of genuine personas. Thus it is hard for using such conventional benchmarks to assess the models' capabilityin handling intricate character knowledge. Recent role-playing benchmarks like RoleBench Wang et al. (2023) evaluate consistency in language style and knowledge extracted from scripts. However, the knowledge extracted from scripts is fragmented and lacks systematicness, making it challenging to accurately and comprehensively assess the breadth of knowledge captured by LLMs. In light of these issues, we introduce RoleEval, a bilingual benchmark designed to assess the capturing and understanding of role knowledge related to both real-world and fictional characters for LLMs. RoleEval is comprised of two sub-benchmarks: _RoleEval-Global_ and _RoleEval-Chinese_, with 300 characters and 6,000 Chinese-English parallel questions in total. Characters in RoleEval are from five categories: 1) celebrities, 2) anime and comics, 3) movies and TV series, 4) games, and 5) fictions. Each character is associated with 20 questions (17 focusing on basic knowledge and 3 on multi-hop reasoning), which evaluate the knowledge and capacity of LLMs in understanding personal information, relationships, abilities, and experiences, as illustrated in Figure 1. These questions are initially crafted in Chinese, then translated into English to construct bilingual parallel data. Specifically, _RoleEval-Global_ is structured around 200 internationally recognized characters with 4,000 questions, which is suitable for evaluating both English and Chinese LLMs. _RoleEval-Chinese_ is targeting another set of 100 influential characters in China with 2,000 questions, following the same question construction framework as _RoleEval-Global_, to specifically evaluate Chinese LLMs. The benchmark incorporates both automatic and manual quality controls to maintain high-quality and challenging questions. To the best of our knowledge, RoleEval is the first benchmark to systematically evaluate the role knowledge for LLMs. It aims to benchmark the current state of LLMs in capturing, understanding, utilizing, and reasoning over knowledge of a wide range of both public figures and fictional characters. By delving into the depths of how LLMs perceive and portray an extensive array of characters, we aim to uncover new avenues for their application, making them not only repositories of information but also active, context-aware participants in our digital narratives. In a nutshell, our contributions are as follows: * We propose RoleEval, a bilingual role evaluation benchmark with 6,000 Chinese-English parallel questions covering 300 diverse characters, to systematically examine the ability of capturing, understanding, and reasoning over role knowledge for LLMs, which is an important prerequisite for successful role-playing. * To ensure quality and boost efficiency, we propose a hybrid quality check process with the combination of both automatic and manual verification to ensure appropriate difficulty and discrimination ability control for questions. * We conduct extensive evaluations using RoleEval on a variety of large language models under both zero- and few-shot settings, encompassing models with varying parameter sizes and those mainly designed for English and Chinese, as well as both open-source and closed-source proprietary models."
    },
    {
      "title": "2 Related Work",
      "text": "Role-playing agents Shuster et al. (2022); Li et al. (2023); Shao et al. (2023); Wang et al. (2023), which can be even traced back to ELIZA Weizenbaum (1966), the first automated dialogue agent that conducts psychological consulting, have recently attracted growing interest in both industry and academia. However, their evaluation is predominantly conducted on models after supervised Figure 1: RoleEval includes different types of characters, knowledge, reasoning, question forms, influence, and languages. fine-tuning. This approach does not incorporate direct feedback from pretrained base models, which can offer critical insights into their intrinsic role-playing capabilities and limitations. On the other hand, existing evaluations largely rely on outputs from the ChatGPT (Ouyang et al., 2022) or humans. However, ChatGPT is not an infallible evaluator, and human evaluation lacks reproducibility. This leads to a lack of objective, accurate, and systematic knowledge assessments. In this case, our benchmark can serve as a robust standard for evaluating current role-playing agents, assessing whether the models possess sufficient role knowledge. Previous research in role-playing evaluation has largely focused on abstract personas (Zhang et al., 2018; Zheng et al., 2019; Xu et al., 2022; Wei et al., 2023; Ahn et al., 2023; Zhou et al., 2023) or specific professions like psychology consultants, chemists, or software engineers (Liu et al., 2021; Wang et al., 2023). However, these methods often oversimplify real-world personas, failing to capture the complex nature of real-world human personalities and behaviors in role-playing scenarios. Despite the value of these approaches, they fall short in exploring individual character knowledge, which is critical for authentic role-playing experiences. Therefore, there is a clear need for more sophisticated and realistic persona models in role-playing research. There are also some closely related works for character-based evaluation, intending to inspect the ability to mimic real-world characters (Shao et al., 2023; Wang et al., 2023; Zhou et al., 2023). However, they do not have a detailed and systematic framework to evaluate role knowledge, leading to fragmented and incomplete knowledge assessment. Moreover, their evaluation relies on humans or other powerful large language models such as ChatGPT. However, as we have stated above, this kind of evaluation suffers from the reproducibility and accuracy of judges. In contrast, RoleEval examines the knowledge required for role play in a detailed, objective and systematic approach for LLMs."
    },
    {
      "title": "3 Roleeval",
      "text": "We develop RoleEval to assess how well role knowledge is captured, utilized, and reasoned with. To that end, we gather a diverse range of characters and manually create questions that systematically evaluate various aspects of role knowledge. Such created questions usually demand a thorough understanding and adaptable use of role knowledge, as well as multi-level reasoning over role knowledge."
    },
    {
      "title": "Character Collection",
      "text": "RoleEval compiles its character set from diverse sources, including Wikipedia4, Baidu Baike5, Fandom6, and Moegirlpedia.7 While Wikipedia and Baidu Baike provide a wide range of information, Fandom and Moegirlpedia offer detailed insights into anime, comics, and games. For _RoleEval-Global_, we gather 200 characters known internationally across five categories: celebrities, anime/comics, movies/TV series, games, and fiction, ensuring diversity by selecting an equal number of characters from each category and different works for each fictional character. _RoleEval-Chinese_ extends this collection with an additional 100 characters popular primarily in China, totaling 300 characters for RoleEval. These characters are detailed in the appendix A.5. Footnote 4: [https://www.wikipedia.org](https://www.wikipedia.org) Footnote 5: [https://baike.baidu.com](https://baike.baidu.com) Footnote 6: [https://www.fandom.com](https://www.fandom.com) Footnote 7: [https://zh.moegirl.org.cn](https://zh.moegirl.org.cn) Before manually creating questions related to collected characters, we rigorously verify each character's encyclopedia information to ensure comprehensiveness. We prefer characters with abundant, collaboratively-contributed information for its reliability and objectivity, ensuring our dataset's diversity, accuracy, and true representation of characters. Additionally, we assess character popularity on social media (via follower counts and engagement) and search engine presence (by search result volume). During the manual question creation phase, we refer to multiple online encyclopedias, selecting only consistent knowledge points to avoid conflicts."
    },
    {
      "title": "Question Design",
      "text": "Our focus is primarily on factual knowledge and comprehensively assessing the ability of LLMs in \\begin{table} \\begin{tabular}{c|c c c} \\hline \\hline & **RoleEval-Global** & **RoleEval-Chinese** & **Total** \\\\ \\hline **\\# Characters** & 200 & 100 & 300 \\\\ **\\# Questions** & 4,000 & 2,000 & 6,000 \\\\ **Languages** & 2h, en & 2h, en & 2h, en \\\\ **Avg. Q. Tokens (zh)** & 24.6 & 26.1 & 25.1 \\\\ **Avg. Q. Tokens (en)** & 14.5 & 16.0 & 15.0 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Statistics of RoleEval. Here we use zh and en to indicate Chinese and English respectively. capturing and understanding various character roles and contexts. In addition to basic knowledge that is directly stated in encyclopedias, we also design multi-hop questions to examine the ability of LLMs to dynamically combine and reason with existing knowledge. Specifically, after collecting the 300 characters from online encyclopedias, we build RoleEval in the form of multiple-choice questions, with four options for each question, which is a common practice adopted by many existing benchmarks such as MMLU Hendrycks et al. (2021) and C-Eval Huang et al. (2023). Each character is associated with 17 and 3 unique questions for basic knowledge and multi-hop reasoning respectively, thus culminating in a total of 6,000 questions. The statistics of RoleEval are shown in Table 1. We also illustrate the length distribution of created questions in Figure 5. In RoleEval, we consider three types of fundamental knowledge required to depict a character: 1. **Inherent Attributes**: This type of knowledge includes the fundamental characteristics intrinsic to the character, such as gender, race, personality, skills, and abilities. These attributes are typically presented in a tabular format, or directly described in online encyclopedias. 2. **Social Relationships**: This type of knowledge pertains to the relationships of the character with other individuals, which could include parents, disciples, and other significant personal or professional relationships. 3. **Experiences**: This type of knowledge details the experiences or events that the character has undergone. For real-world individuals, this usually includes significant life events or experiences in which they were direct participants. For fictional characters, this involves extracting key plot points or story arcs described in online encyclopedias. To enhance the comprehensive assessment and diversify the scope of multiple-choice questions, our approach extends beyond merely querying knowledge from online encyclopedias. As a supplement to direct questions, we incorporate two additional question types, which are designed to be combined with the previously identified four types of knowledge for a more dynamic and comprehensive evaluation: 1. **Negation Question Type:** These questions, usually formatted as \"Which of the following is NOT...\", require a comprehensive understanding of a specific knowledge point. 2. **Non-occurrence Scenario Question Type:** These questions test for non-occurrences, with correct answers often framed in the negative (e.g., \"Did not happen...\"). This format examines whether the model generates illusions or false assumptions. We further add three reasoning questions for each character that need multi-hop reasoning over these types of fundamental knowledge. According to the knowledge required in the intermediate reasoning steps, we classify these reasoning questions into three categories, and assign one question for each available (role, reasoning category) pair: 1. **Character Relationship Reasoning:** When answering this type of questions, models need to reason about the relationship between characters. 2. **Event Participant Reasoning:** To answer a question in this category, models need to reason out the participants of an event, and then combine it with other information in the question to locate the answer. 3. **Timeline Reasoning:** Answering questions in this group requires models to understand the sequence of events, infer the time of occurrence of the event in question stems and options, and then select the correct option based on the timeline. These reasoning questions go beyond simply memorizing one-hop knowledge in text, intending to connect multiple related characters, events, and storylines. It requires the compositionality capability of models to intrinsically and dynamically combine multiple knowledge points and answer the given questions, thus making our RoleEval a challenging benchmark for LLMs. For more detailed information, we illustrate the examples of negation, non-occurrence and different types of reasoning in Appendix A.1 and A.2."
    },
    {
      "title": "Quality Check",
      "text": "To ensure quality and boost efficiency during benchmark construction, we propose a hybrid quality check process with the combination of both automatic and human verification. In the automatic checking stage, we use GPT-4 and GPT-3.5 API8 to control the degree of difficulty and discrimination ability. Assume the accuracy of GPT-4 and GPT-3.5 API for answering questions of character \\(c\\) is \\(x_{c}\\) and \\(y_{c}\\) respectively. We apply the below criteria as the preliminary and instant feedback for human annotators: Footnote 8: Unless otherwise specified, the term “GPT-4” and “GPT-3.5” shall henceforth be used to represent gpt-4-0613 and gpt-3.5-turbo-0613 respectively. \\[x_{l}\\leq x_{c}\\leq x_{u} \\tag{1}\\] \\[y_{c}\\leq y_{u}\\] (2) \\[x_{c}-y_{c}\\geq d \\tag{3}\\] where \\(x_{l}\\) and \\(x_{u}\\) indicate the predefined lower threshold and upper threshold for \\(x_{c}\\), and \\(y_{u}\\) indicates the predefined upper threshold for \\(y_{c}\\). These three hyperparameters control the overall degree of difficulty, making sure that questions are not too easy or too hard for LLMs, in terms of the answering accuracy of GPT-4/3.5. And \\(d\\) is the lower threshold of difference between \\(x_{c}\\) and \\(y_{c}\\). Since GPT-4 is a significantly more powerful model than GPT-3.5 in practice, we use this hyperparameter to ensure the discrimination ability of this benchmark for various models. Only questions that satisfy the conditions in Eq. (1)-(3) are kept. In our preliminary study, we find \\(x_{l}=0.3\\), \\(x_{u}=0.9\\), \\(y_{u}=0.8\\), and \\(d=0.15\\) achieve appropriate difficulty and discrimination ability control for questions. After this automatic scrutinization, we manually check the kept questions and options to ensure the quality of our benchmark. To easily check the factual correctness and prevent questions from overemphasis on peripheral aspects, we ask annotators to also provide links to the referenced text in the online encyclopedias along with each question to achieve effective oversight."
    },
    {
      "title": "Translation",
      "text": "To evaluate LLMs in different languages, we translate Chinese RoleEval questions to English using GPT-4.9 We have found that GPT-4 produce more adaptable and precise character-specific translations than traditional machine translation engines like Google Translate, due to its customizable prompts. To address potential ambiguities in entity translations, we hire human translators to review and adjust them based on the original Chinese content. Given RoleEval's focus on factual accuracy over stylistic elements, combining machine translation with human post-editing effectively minimizes Figure 2: Few-shot prompt examples for RoleEval. English translations are placed after the corresponding Chinese text for clarity. [MISSING_PAGE_FAIL:6] knowledge cutoff in gpt-4-1106, enhancing its performance in domains with rapidly evolving information. Nevertheless, the overall accuracy still indicates large room for improvement even for the state-of-the-art LLMs. On the _RoleEval-Chinese_ (**zh**) dataset, some Chinese LLMs, such as Qwen-72B and Yi-34B, show superior performance to GPT-4. This is likely due to their higher proportion of Chinese training data and abundant high-quality discussions about characters in _RoleEval-Chinese_ on Chinese online platforms. Notably, GPT-4 retains its edge in anime, comics, and games, where many characters are also popular in the English-speaking world. These results highlight the importance of choosing balanced training data and evaluating role knowledge across various languages and cultural settings. We also provide results and analysis grouped by knowledge and reasoning questions in Appendix A.4. Differences in LanguagesWe observe a significant improvement in GPT-4 and 3.5's performance on RoleEval (**en**) dataset compared to RoleEval (**zh**), even though these two parts of the dataset have identical semantic content. This observation is also evident in predominantly English-language open-source models, especially for models with little Chinese training data, such as LLaMA, Mistral, and Falcon. This suggests that even the most powerful LLMs still lack effective cross-lingual knowledge transfer, indicating that these models fail to build complete bi-directional mappings between entities in different languages. Conversely, Chinese models generally underperform on the English RoleEval datasets, highlighting a similar language-specific limitation, though the gap is narrower since Chinese LLMs still use a large amount of English pre-training data. Among all open-source models, Yi-34B achieves almost the same performance in both Chinese and English on _RoleEval-Global_, indicating its balanced training for global influential characters in both languages. \\begin{table} \\begin{tabular}{c|c c c c c|c c c c c|c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{6}{c|}{**RoleEval-Global (4,000 questions)**} & \\multicolumn{6}{c}{**RoleEval-Chinese (2,000 questions)**} \\\\ \\cline{2-13} & **CE** & **AC** & **MT** & **GA** & **FI** & **Avg.** & **CE** & **AC** & **MT** & **GA** & **FI** & **Avg.** \\\\ \\hline GPT-3.5-0613 & 46.62 & 48.38 & 51.75 & 49.50 & 47.38 & 48.73 & 42.25 & 43.50 & 39.75 & 43.75 & 39.00 & 41.65 \\\\ GPT-3.5-1106 & 48.75 & 51.88 & 51.25 & 49.88 & 48.38 & 50.02 & 47.50 & 46.75 & 41.75 & 44.75 & 38.75 & 43.90 \\\\ GPT-4-0613 & 73.38 & 72.12 & 74.25 & 72.25 & 69.62 & 72.32 & 57.75 & 60.25 & 57.75 & 60.00 & 58.00 & 58.75 \\\\ GPT-4-1106 & **74.75** & **73.62** & **74.38** & **72.50** & 71.62 & **73.38** & 62.50 & **63.25** & 63.00 & **62.00** & 63.00 & 62.75 \\\\ Falcon-7B & 23.88 & 28.12 & 24.50 & 28.00 & 28.12 & 26.52 & 24.75 & 30.50 & 31.50 & 29.75 & 25.25 & 28.35 \\\\ Falcon-40B & 39.62 & 32.25 & 32.38 & 30.00 & 45.00 & 35.85 & 28.25 & 33.00 & 30.25 & 29.25 & 38.50 & 31.85 \\\\ LLaMA-7B & 25.50 & 31.87 & 25.87 & 26.00 & 28.88 & 27.62 & 28.50 & 24.75 & 20.50 & 27.75 & 29.00 & 26.10 \\\\ LLaMA-13B & 28.50 & 28.50 & 28.25 & 26.50 & 27.75 & 27.90 & 27.25 & 29.75 & 27.25 & 26.00 & 29.00 & 27.85 \\\\ LLaMA-30B & 24.88 & 31.13 & 30.25 & 27.75 & 28.62 & 28.52 & 30.00 & 28.75 & 26.00 & 31.75 & 28.00 & 28.90 \\\\ LLaMA-65B & 32.12 & 31.87 & 32.75 & 31.00 & 34.88 & 32.52 & 30.00 & 32.25 & 29.00 & 35.50 & 29.00 & 31.15 \\\\ LLaMA-2-7B & 37.00 & 29.88 & 28.75 & 34.50 & 38.25 & 33.67 & 25.75 & 28.00 & 33.75 & 29.75 & 34.50 & 30.35 \\\\ LLaMA-2-13B & 36.50 & 34.00 & 33.00 & 31.87 & 31.75 & 33.42 & 28.75 & 30.50 & 25.25 & 29.75 & 28.25 & 28.50 \\\\ LLaMA-2-70B & 53.50 & 43.25 & 39.25 & 40.25 & 47.25 & 44.70 & 36.00 & 38.00 & 36.25 & 36.25 & 34.75 & 36.25 \\\\ Mistral-7B & 36.12 & 33.50 & 32.00 & 30.25 & 35.00 & 33.38 & 32.50 & 37.50 & 26.25 & 33.25 & 31.50 & 32.20 \\\\ \\hline MiniMax & 51.75 & 54.50 & 62.62 & 56.75 & 52.75 & 55.67 & 54.00 & 55.00 & 52.75 & 57.50 & 54.00 & 54.65 \\\\ Baichuan2-7B & 56.00 & 49.62 & 45.50 & 40.50 & 52.38 & 48.80 & 52.25 & 43.75 & 49.00 & 47.25 & 55.00 & 49.45 \\\\ Baichuan2-13B & 60.25 & 52.38 & 51.00 & 46.88 & 60.75 & 54.25 & 54.75 & 47.75 & 54.00 & 47.50 & 60.00 & 52.80 \\\\ ChatGLM3-6B & 56.50 & 47.62 & 48.38 & 41.88 & 54.50 & 49.78 & 50.00 & 44.50 & 48.00 & 44.25 & 58.00 & 48.95 \\\\ Chinese-LLaMA-2-7B & 35.62 & 36.75 & 35.62 & 35.38 & 34.38 & 35.55 & 34.50 & 29.00 & 33.00 & 30.25 & 36.25 & 32.60 \\\\ Chinese-LLaMA-2-13B & 45.38 & 38.25 & 39.88 & 31.87 & 42.12 & 39.50 & 36.50 & 36.50 & 34.00 & 34.00 & 40.50 & 36.30 \\\\ Qwen-7B & 54.75 & 44.38 & 44.62 & 42.75 & 53.00 & 47.90 & 49.00 & 42.00 & 47.50 & 44.75 & 51.25 & 46.90 \\\\ Qwen-14B & 62.50 & 52.38 & 55.00 & 45.50 & 58.00 & 54.67 & 56.25 & 45.50 & 54.75 & 51.50 & 56.75 & 52.95 \\\\ Qwen-72B & 72.88 & 63.88 & 70.38 & 56.75 & **73.50** & 67.47 & **70.00** & 59.75 & 66.00 & 61.25 & 74.00 & **66.20** \\\\ Skywork-13B & 59.13 & 51.75 & 51.88 & 44.50 & 58.75 & 53.20 & 55.25 & 45.75 & 56.00 & 48.50 & 57.50 & 52.60 \\\\ Yi-6B & 61.88 & 51.38 & 52.38 & 45.38 & 60.75 & 54.35 & 59.25 & 46.00 & 61.50 & 47.75 & 62.00 & 55.30 \\\\ Yi-34B & 72.38 & 60.62 & 69.75 & 53.25 & 73.12 & 65.83 & 65.50 & 54.50 & **70.00** & 56.00 & **77.00** & 64.60 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Five-shot results on RoleEval (**zh**) in five categories: celebrities (**CE**), anime and comics (**AC**), movie and TV series (**MT**), games (**GA**), and fiction (**FI**). **Comparative Analysis of Open-Source Models** LLaMA2-70B has emerged as the best open-source model primarily trained in English, closely matching GPT-3.5. While for Chinese LLMs, Qwen-72B and Yi-34B not only surpass GPT-3.5 but also exceed GPT-4 on the _RoleEval-Chinese_ (**zh**) dataset. However, these Chinese models still show noticeable gaps compared to GPT-4 in other scenarios. **Parameter Scaling Laws** We also analyzed the correlation between the accuracy of role knowledge and the size and the number of training tokens of LLMs on _RoleEval-Global_.12 As shown in Figure 3, accuracy generally improves with model size for LLaMA and Qwen, and the trend is consistent with previously established knowledge transfer patterns: For Chinese LLMs, the rate of improvement is greater for RoleEval (**zh**) than for RoleEval (**en**). In contrast, LLMs primarily trained on English corpora show opposite trends. Notably, BLOOM and Pythia do not show performance improvements across various settings. We speculate that this is due to their relatively lower token training volume (366B for BLOOM and 300B for Pythia), while most models with great performance have already been trained on more than 1TB tokens. \\begin{table} \\begin{tabular}{c|c c c c c|c c c c c|c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{6}{c|}{**RoleEval-Global (4,000 questions)**} & \\multicolumn{6}{c}{**RoleEval-Chinese (2,000 questions)**} \\\\ \\cline{2-13} & **CE** & **AC** & **MT** & **GA** & **FI** & **Avg.** & **CE** & **AC** & **MT** & **GA** & **FI** & **Avg.** \\\\ \\hline GPT-3.5-0613 & 57.38 & 59.62 & 58.13 & 59.50 & 57.50 & 58.43 & 42.00 & 47.75 & 42.50 & 42.25 & 45.50 & 44.00 \\\\ GPT-3.5-1106 & 58.75 & 56.62 & 55.75 & 58.00 & 55.00 & 56.82 & 38.25 & 45.50 & 44.00 & 44.50 & 46.00 & 43.65 \\\\ GPT-4-0613 & **77.62** & **79.50** & 73.12 & 74.88 & **75.00** & **76.02** & 54.25 & 61.75 & **63.00** & **63.00** & **63.00** & **61.00** \\\\ GPT-4-1106 & 75.12 & 78.75 & **75.00** & **76.12** & **75.00** & 76.00 & **57.50** & **63.50** & 60.00 & 62.50 & 58.00 & 60.30 \\\\ Falcon-7B & 26.25 & 27.75 & 28.50 & 29.38 & 31.00 & 28.58 & 27.25 & 27.75 & 27.75 & 29.75 & 28.50 & 28.20 \\\\ Falcon-40B & 47.38 & 45.00 & 49.62 & 43.12 & 50.00 & 47.02 & 34.00 & 38.25 & 30.75 & 38.75 & 35.25 & 35.40 \\\\ LLaMA-7B & 29.38 & 30.50 & 29.25 & 33.50 & 28.50 & 30.23 & 24.00 & 27.50 & 29.75 & 33.00 & 29.25 & 28.70 \\\\ LLaMA-13B & 39.38 & 40.25 & 39.88 & 40.62 & 43.00 & 40.63 & 32.75 & 31.75 & 30.75 & 38.50 & 32.00 & 33.15 \\\\ LLaMA-30B & 51.62 & 46.88 & 48.62 & 43.12 & 52.62 & 48.57 & 34.75 & 35.75 & 30.75 & 40.00 & 35.00 & 35.25 \\\\ LLaMA-65B & 58.13 & 50.50 & 54.37 & 47.62 & 54.50 & 53.02 & 41.50 & 38.50 & 33.50 & 43.25 & 37.50 & 38.85 \\\\ LLaMA-2-7B & 38.88 & 37.00 & 37.50 & 41.62 & 42.38 & 39.48 & 28.75 & 29.25 & 32.75 & 37.50 & 32.25 & 32.10 \\\\ LLaMA-2-13B & 49.38 & 43.50 & 46.50 & 44.25 & 48.25 & 46.38 & 30.50 & 36.50 & 33.25 & 36.50 & 33.25 & 34.00 \\\\ LLaMA-2-70B & 63.25 & 57.38 & 59.00 & 50.00 & 63.25 & 58.58 & 43.25 & 41.50 & 40.25 & 47.50 & 43.50 & 43.20 \\\\ Mistral-7B & 54.87 & 46.75 & 49.62 & 44.25 & 52.25 & 49.55 & 35.75 & 42.00 & 30.00 & 41.75 & 31.50 & 36.20 \\\\ \\hline MiniMax & 54.87 & 56.38 & 53.50 & 54.12 & 51.38 & 54.05 & 34.00 & 39.50 & 40.75 & 38.25 & 39.00 & 38.30 \\\\ Baichuan2-7B & 51.00 & 45.12 & 49.00 & 42.12 & 50.00 & 47.45 & 37.25 & 35.75 & 33.00 & 40.25 & 37.00 & 36.65 \\\\ Baichuan2-13B & 56.12 & 47.50 & 51.50 & 45.62 & 54.00 & 50.95 & 35.50 & 36.50 & 31.25 & 42.25 & 34.75 & 36.05 \\\\ ChatGLM3-6B & 55.12 & 46.62 & 49.25 & 43.25 & 52.62 & 49.37 & 36.25 & 36.25 & 35.25 & 42.25 & 43.50 & 38.70 \\\\ Chinese-LLaMA-2-13B & 47.75 & 46.00 & 46.88 & 45.00 & 48.38 & 46.80 & 34.00 & 38.50 & 27.75 & 37.50 & 34.00 & 34.35 \\\\ Chinese-LLaMA-2-7B & 36.50 & 30.75 & 31.75 & 36.25 & 39.50 & 34.95 & 30.50 & 27.75 & 33.00 & 30.50 & 27.75 & 29.90 \\\\ Qwen-7B & 53.87 & 46.12 & 48.12 & 40.00 & 51.12 & 47.85 & 36.25 & 36.00 & 36.25 & 42.25 & 40.00 & 38.15 \\\\ Qwen-14B & 61.12 & 49.00 & 53.87 & 45.38 & 56.12 & 53.10 & 41.00 & 38.75 & 38.25 & 43.25 & 41.00 & 40.45 \\\\ Qwen-72B & 70.12 & 62.00 & 69.00 & 55.75 & 69.50 & 65.27 & 52.75 & 47.50 & 46.50 & 54.25 & 50.50 & 50.30 \\\\ Skywork-13B & 56.25 & 46.75 & 51.62 & 44.38 & 53.62 & 50.52 & 39.25 & 34.50 & 38.25 & 41.75 & 38.50 & 38.45 \\\\ Yi-6B & 59.25 & 52.00 & 54.12 & 47.50 & 56.25 & 53.82 & 42.25 & 38.50 & 41.50 & 44.25 & 45.00 & 42.30 \\\\ Yi-34B & 73.12 & 61.75 & 67.88 & 57.12 & 67.25 & 65.42 & 56.00 & 52.00 & 47.50 & 55.00 & 57.00 & 53.50 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Five-shot results on RoleEval (**en**) in five categories: celebrities (**CE**), anime and comics (**AC**), movie and TV series (**MT**), games (**GA**), and fiction (**FI**). Figure 3: Parameter Scaling Laws on _RoleEval-Global_. Token Scaling LawsTo further explore the scaling law on the number of tokens, we conducted experiments on publicly available intermediate checkpoints of BLOOM-7B1, Pythia-6.9B, Baichuan-7B, and Skywork-13B. For BLOOM, Baichuan, and Skywork, we selected all available intermediate checkpoints, resulting in 8, 11, and 6 checkpoints respectively. For Pythia, we chose a checkpoint every 13,000 steps and obtained 12 intermediate checkpoints for evaluation. Results from Figure 4 indicate that while the performance of Baichuan and Skywork with their first checkpoint (trained with 220B and 500B respectively) is near random, which is similar to BLOOM and Pythia, their subsequent checkpoints show steady improvement after 500B tokens, which indicates the importance of sufficient training tokens with fixed model size. However, the bottleneck of cross-lingual knowledge transfer can still be observed with increasing training tokens, which means that simply increasing the number of parameters and adding training tokens may not be the best way to break down the barriers between languages. In future research, we intend to examine other LLMs with intermediate checkpoints trained on larger-scale datasets primarily in English, aiming to substantiate our hypotheses with more robust evidence."
    },
    {
      "title": "5 Conclusion",
      "text": "In this paper, we have presented RoleEval, a large-scale bilingual role evaluation benchmark, featuring 6,000 Chinese-English parallel questions across 300 diverse characters (200 for _RoleEval-Global_ and 100 for _RoleEval-Chinese_) from five different domains. RoleEval is specifically designed to scrutinize the capabilities of LLMs in capturing, understanding, and reasoning over role knowledge. Our hybrid quality check process, blending automatic and human verification, guarantees the questions' difficulty and discrimination level, setting a new standard in benchmark design. Extensive evaluations of RoleEval on various LLMs, including both zero- and few-shot settings, highlight significant differences in knowledge distribution, as evidenced by GPT-4's superior performance in _RoleEval-Global_ and the notable excellence of Chinese LLMs in _RoleEval-Chinese_. These findings not only demonstrate the disparities in LLMs' knowledge proficiency but also illuminate the path for future enhancements in bilingual and culture-specific LLMs. Through RoleEval, we aim to provide a robust framework for future advancements in LLM evaluation, particularly in role-playing scenarios, thereby enriching the landscape of language understanding and reasoning benchmarks."
    },
    {
      "title": "Limitations",
      "text": "In evaluating the effectiveness of real-world role evaluation benchmarks, there are two existing limitations. Firstly, the aspect of timeliness is crucial; the knowledge regarding real-world characters may change over time, making the benchmark outdated or irrelevant. To address this, we plan to explore methods for the automatic updating of benchmarks, ensuring that they remain current and reflective of ongoing changes. Secondly, the current format of benchmarks often restricts questions to having only one correct answer. This approach fails to adequately test scenarios where multiple answers could be correct, thus limiting the benchmark's ability to evaluate complex decision-making skills. A potential solution could be to incorporate a more dynamic question format that allows for the identification and acceptance of multiple correct answers, thereby enriching the assessment process by acknowledging the multi-faceted nature of real-world problems."
    },
    {
      "title": "Ethics Statement",
      "text": "Our benchmark is designed to enhance the model's understanding of role knowledge, which is crucial for improving persona consistency and factual accuracy while reducing hallucination. To achieve this, we have selected encyclopedic content that has been edited by multiple individuals. This approach helps to minimize factual errors and biases, particularly in comparison to other texts sourced from the Internet. Furthermore, all data collected Figure 4: Token Scaling Laws on _RoleEval-Global_. for this project originates from publicly available materials, ensuring no concerns regarding privacy infringement. We confirm that all materials employed were utilized for non-commercial purposes, in adherence to copyright regulations and privacy policies. Moreover, our goal is to promote a comprehensive and accurate understanding of role knowledge. Therefore, although most of our questions and options are positive, we have not entirely excluded potential negative aspects of the selected characters from our benchmark. Users should be aware of this when using this benchmark. Our commitment is to provide a comprehensive and balanced understanding, but users should remain critical and mindful of the context in which this information is used."
    },
    {
      "title": "References",
      "text": "* Ahn et al. (2023) Jaewoo Ahn, Yeda Song, Sangdoo Yun, and Gunhee Kim. 2023. MPCHAT: Towards Multimodal Persona-Grounded Conversation. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3354-3377, Toronto, Canada. Association for Computational Linguistics. * Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The Falcon series of open language models. _arXiv preprint arXiv:2311.16867_. * Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical Report. _ArXiv preprint_, abs/2309.16609. * Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbic Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. _ArXiv preprint_, abs/2304.01373. * Cui et al. (2023) Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca. _ArXiv preprint_, abs/2304.08177. * Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335, Dublin, Ireland. Association for Computational Linguistics. * Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. * Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net. * Huang et al. (2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. _ArXiv preprint_, abs/2305.08322. * Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. 2023. Mistral 7B. _ArXiv preprint_, abs/2310.06825. * Li et al. (2023) Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi MI, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, Linkang Zhan, Yaokai Jia, Pingyu Wu, and Haozhen Sun. 2023a. ChatHaruhi: Reviving Anime Character in Reality via Large Language Model. _ArXiv preprint_, abs/2308.09597. * Li et al. (2023) Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023b. CMMLU: Measuring massive multitask language understanding in Chinese. _ArXiv preprint_, abs/2306.09212. * Lin et al. (2023) Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2023. The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning. _ArXiv preprint_, abs/2312.01552. Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou Yu, Yong Jiang, and Minlie Huang. 2021. Towards emotional support dialog systems. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 3469-3483, Online. Association for Computational Linguistics. * OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. _ArXiv preprint_, abs/2303.08774. * Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. _ArXiv preprint_, abs/2203.02155. * Shao et al. (2023) Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-LLM: A Trainable Agent for Role-Playing. _ArXiv preprint_, abs/2310.10158. * Shuster et al. (2022) Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Am I me or you? state-of-the-art dialogue models cannot maintain an identity. In _Findings of the Association for Computational Linguistics: NAACL 2022_, pages 2367-2387, Seattle, United States. Association for Computational Linguistics. * Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. LLaMA: Open and Efficient Foundation Language Models. _ArXiv preprint_, abs/2302.13971. * Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurll, David Eisibou, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihalyov, Pushkar Mishra, Igor Molybg, Yixin Nie, Andrew Poutlon, Jeremy Reizenstein, Rashil Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open Foundation and Fine-Tuned Chat Models. _ArXiv preprint_, abs/2307.09288. * Wang et al. (2023a) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. 2023a. A Survey on Large Language Model based Autonomous Agents. _ArXiv preprint_, abs/2308.11432. * Wang et al. (2023b) Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhu Chen, Jie Fu, and Junran Peng. 2023b. RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models. _ArXiv preprint_, abs/2310.00746. * Wei et al. (2023a) Jimmy Wei, Kurt Shuster, Arthur Szlam, Jason Weston, Jack Urbanek, and Mojtaba Komeili. 2023a. Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. _ArXiv preprint_, abs/2304.13835. * Wei et al. (2023b) Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuhanai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, and Yahui Zhou. 2023b. Skywork: A More Open Bilingual Foundation Model. _ArXiv preprint_, abs/2310.19341. * Weizenbaum (1966) Joseph Weizenbaum. 1966. ELIZA--a computer program for the study of natural language communication between man and machine. _Communications of the ACM_, 9(1):36-45. * Workshop et al. (2017) BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammannamachi, Thomas Wang, Benotti Sagot, Niklas Muenninghoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurencon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonzalez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gerard Dupont, German Kruszewski, Giada Pitstilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chin, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra,Leon Weber, Long Phan, Loubna Ben all, Ludovic Tanguy, Manan Dey, Manuel Romero Munoz, Marium Masoud, Maria Grandury, Mario Sasko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chen Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommassani, Roberto Luis Lopez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaich Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Tasar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheshesth Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M. Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urnish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornete, Pierre Francois Lavallee, Remi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stephane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurelie Neveol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Haliev Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Munoz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononniwiu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irne Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagao, Mariam Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Eklott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clementine Fourrier, Daniel Leon Perinan, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrmann, Gabriel Alday, Giyasadin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishan Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pamies, Maria A. Castillo, Marianna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljclic, Minau Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrig Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S. Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Since Sang-aroonsiri, Sristh Kumar, Stefan Schweter, Sushil Bharati, Tammay Laud, Theo Gigant, Tomography Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkataraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. _ArXiv preprint_, abs/2211.05100. * Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. The Rise and Potential of Large Language Model Based Agents: A Survey. _ArXiv preprint_, abs/2309.07864. * Xu et al. (2022) Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. 2022. Long time no see! open-domain conversation with long-term persona memory. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 2639-2650, Dublin, Ireland. Association for Computational Linguistics. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. 2023. Baichuan 2: Open Large-scale Language Models. _ArXiv preprint_, abs/2309.10305. * Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. 2022. GLM-130B: An Open Bilingual Pre-trained Model. _ArXiv preprint_, abs/2210.02414. * Zhang et al. (2018) Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog, do you have pets too? In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2204-2213, Melbourne, Australia. Association for Computational Linguistics. * Zheng et al. (2019) Yinhe Zheng, Guanyi Chen, Minlie Huang, Song Liu, and Xuan Zhu. 2019. Personalized Dialogue Generation with Diversified Traits. _ArXiv preprint_, abs/1901.09672. * Zhou et al. (2023a) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023a. LIMA: Less Is More for Alignment. _ArXiv preprint_, abs/2305.11206. * Zhou et al. (2023b) Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, Sahand Sabour, Xiaohan Zhang, Wenjing Hou, Yijia Zhang, Yuxiao Dong, Jie Tang, and Minlie Huang. 2023b. Character-GLM: Customizing Chinese Conversational AI Characters with Large Language Models. _ArXiv preprint_, abs/2311.16832. [MISSING_PAGE_FAIL:14] indicates a significant advancement in reasoning, as smaller models approach the upper echelons of reasoning skills previously dominated by more advanced models. In the realm of Chinese LLMs, Qwen-72B and Yi-34B also stand out with their exceptional reasoning abilities, positioning themselves between the GPT-3.5 and GPT-4 models. This not only underscores the progress in developing LLMs in different languages but also indicates the potential for LLMs to improve both knowledge and reasoning by considering a wider range of languages when constructing training corpora."
    },
    {
      "title": "Detailed Character List",
      "text": "We list all the collected 300 characters in Table 9, 10, 11, 12, 13 and 14, with both the name and source of each character in Chinese and English. \\begin{table} \\begin{tabular}{c|c c c c|c c|c c c c c|c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{6}{c|}{**RoleEval-Global (4,000 questions)**} & \\multicolumn{6}{c}{**RoleEval-Chinese (2,000 questions)**} \\\\ \\cline{2-13} & **CE** & **AC** & **MT** & **GA** & **FI** & **Avg.** & **CE** & **AC** & **MT** & **GA** & **FI** & **Avg.** \\\\ \\hline GPT-3.5-0613 & 45.25 & 45.88 & 46.38 & 47.12 & 45.50 & 46.03 & 41.00 & 41.00 & 34.75 & 33.50 & 38.50 & 37.75 \\\\ GPT-3.5-1106 & 45.62 & 45.50 & 45.50 & 44.50 & 43.25 & 44.87 & 40.75 & 41.25 & 33.25 & 36.50 & 39.50 & 38.25 \\\\ GPT-4-0613 & 70.75 & 71.50 & 73.12 & 69.38 & 70.50 & 71.05 & 62.00 & 59.75 & 57.25 & 55.00 & 58.75 & 58.55 \\\\ GPT-4-1106 & **75.12** & **73.75** & **75.25** & **71.50** & 70.75 & **73.27** & **63.00** & 61.25 & 56.25 & 62.00 & 60.00 & 60.50 \\\\ Falcon-7B & 23.88 & 25.25 & 25.87 & 28.00 & 25.12 & 25.62 & 26.00 & 24.50 & 25.50 & 25.00 & 26.25 & 25.45 \\\\ Falcon-40B & 40.50 & 35.25 & 32.12 & 35.50 & 45.75 & 37.82 & 32.25 & 30.00 & 36.25 & 29.75 & 31.00 & 31.85 \\\\ LLaMA-7B & 27.12 & 29.50 & 28.38 & 30.88 & 25.12 & 28.20 & 31.50 & 26.25 & 26.50 & 30.50 & 28.50 & 28.65 \\\\ LLaMA-13B & 28.38 & 28.38 & 23.25 & 26.62 & 30.00 & 27.33 & 22.50 & 23.50 & 25.75 & 32.00 & 26.50 & 26.05 \\\\ LLaMA-30B & 27.12 & 28.25 & 28.25 & 27.38 & 23.62 & 26.92 & 25.00 & 30.50 & 23.00 & 22.75 & 32.00 & 26.65 \\\\ LLaMA-65B & 31.87 & 29.38 & 30.50 & 29.62 & 30.63 & 30.40 & 27.25 & 29.00 & 28.25 & 29.00 & 29.00 & 28.50 \\\\ LLaMA-2-7B & 28.38 & 30.75 & 29.25 & 29.88 & 28.50 & 29.35 & 27.50 & 26.00 & 31.25 & 27.50 & 26.75 & 27.80 \\\\ LLaMA-2-13B & 30.38 & 30.75 & 32.38 & 29.38 & 32.50 & 31.08 & 27.50 & 27.75 & 26.50 & 26.50 & 29.50 & 27.55 \\\\ LLaMA-2-70B & 46.12 & 40.25 & 37.62 & 40.12 & 42.50 & 41.32 & 36.50 & 37.00 & 34.50 & 31.00 & 34.00 & 34.60 \\\\ Mistral-7B & 37.00 & 31.37 & 29.00 & 32.12 & 30.63 & 32.02 & 32.25 & 27.50 & 29.00 & 26.50 & 32.00 & 29.45 \\\\ \\hline MiniMax & 53.12 & 56.62 & 60.00 & 56.12 & 55.00 & 56.17 & 56.00 & 52.25 & 53.00 & 53.50 & 54.25 & 53.80 \\\\ Baichuan2-7B & 54.37 & 47.25 & 42.50 & 38.38 & 56.12 & 47.72 & 40.25 & 53.25 & 53.50 & 49.25 & 41.25 & 47.50 \\\\ Baichuan2-13B & 60.50 & 51.50 & 46.88 & 45.25 & 57.12 & 52.25 & 45.75 & 51.25 & 55.25 & 54.50 & 44.25 & 50.20 \\\\ ChatGLM3-6B & 55.38 & 47.62 & 49.12 & 42.00 & 56.88 & 50.20 & 45.00 & 51.25 & 55.50 & 49.50 & 44.00 & 49.05 \\\\ Chinese-LLaMA-2-7B & 29.12 & 27.25 & 23.25 & 26.75 & 30.00 & 27.27 & 22.50 & 27.00 & 27.25 & 31.25 & 24.00 & 26.40 \\\\ Chinese-LLaMA-2-13B & 35.62 & 34.50 & 33.62 & 29.25 & 36.88 & 33.97 & 31.00 & 31.75 & 34.50 & 31.00 & 33.00 & 32.25 \\\\ Qwen-7B & 56.38 & 43.62 & 46.50 & 38.88 & 53.87 & 47.85 & 40.25 & 49.50 & 50.75 & 46.25 & 43.25 & 46.00 \\\\ Qwen-14B & 59.50 & 54.12 & 54.25 & 43.88 & 58.63 & 54.08 & 43.25 & 51.00 & 58.75 & 50.75 & 50.75 & 50.90 \\\\ Qwen-72B & 70.62 & 63.75 & 68.38 & 59.25 & **72.00** & 66.80 & 58.75 & 66.50 & **72.25** & **64.00** & **62.75** & **64.85** \\\\ Skywork-13B & 58.13 & 50.00 & 47.88 & 43.75 & 56.25 & 51.20 & 44.50 & 53.00 & 57.50 & 52.50 & 46.00 & 50.70 \\\\ Yi-6B & 62.75 & 54.25 & 53.00 & 45.12 & 58.13 & 54.65 & 46.00 & 57.50 & 62.00 & 58.50 & 45.25 & 53.85 \\\\ Yi-34B & 67.38 & 59.00 & 62.75 & 51.62 & 67.88 & 61.73 & 54.75 & **67.25** & 68.75 & 59.75 & 56.00 & 61.30 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: Zero-shot results on RoleEval (**zh**) in five categories: celebrities (**CE**), anime and comics (**AC**), movie and TV series (**MT**), games (**GA**), and fiction (**FI**). \\begin{table} \\begin{tabular}{c|c c c c c|c c c c c c|c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{8}{c|}{**RoleEval-Global (4,000 questions)**} & \\multicolumn{8}{c}{**RoleEval-Chinese (2,000 questions)**} \\\\ \\cline{2-13} & **CE** & **AC** & **MT** & **GA** & **FI** & **Avg.** & **CE** & **AC** & **MT** & **GA** & **FI** & **Avg.** \\\\ \\hline GPT-3.5-0613 & 57.00 & 59.00 & 56.12 & 59.13 & 57.50 & 57.75 & 37.75 & 44.50 & 43.75 & 43.25 & 45.00 & 42.85 \\\\ GPT-3.5-1106 & 55.50 & 53.62 & 53.50 & 56.00 & 53.50 & 54.42 & 39.25 & 40.50 & 42.00 & 39.00 & 42.00 & 40.55 \\\\ GPT-4-0613 & 74.38 & 74.12 & **73.88** & 72.75 & 71.25 & 73.28 & 48.50 & 56.00 & **60.50** & 58.75 & 55.25 & 55.80 \\\\ GPT-4-1106 & **74.38** & **78.62** & 72.38 & **74.38** & **73.00** & **74.55** & 51.00 & **56.75** & 59.75 & **60.25** & **57.75** & **57.10** \\\\ Falcon-7B & 28.38 & 26.75 & 23.12 & 26.12 & 31.37 & 27.15 & 26.50 & 24.50 & 30.75 & 28.75 & 24.00 & 26.90 \\\\ Falcon-40B & 47.12 & 43.88 & 47.12 & 42.38 & 48.88 & 45.88 & 37.50 & 36.75 & 30.25 & 38.25 & 36.75 & 35.90 \\\\ LLaMA-7B & 32.00 & 31.50 & 29.38 & 31.00 & 32.88 & 31.35 & 23.00 & 24.75 & 29.25 & 30.25 & 29.25 & 27.30 \\\\ LLaMA-13B & 35.88 & 36.50 & 37.00 & 32.25 & 42.25 & 36.78 & 33.50 & 28.25 & 29.25 & 34.75 & 30.75 & 31.30 \\\\ LLaMA-30B & 45.62 & 41.62 & 47.12 & 41.88 & 51.00 & 45.45 & 35.75 & 31.75 & 35.00 & 38.00 & 36.00 & 35.30 \\\\ LLaMA-65B & 53.62 & 49.75 & 50.88 & 41.75 & 54.75 & 50.15 & 38.00 & 35.25 & 35.00 & 40.00 & 37.25 & 37.10 \\\\ LLaMA-2-7B & 33.62 & 32.88 & 31.87 & 33.88 & 37.50 & 33.95 & 25.25 & 27.00 & 30.50 & 29.25 & 31.75 & 28.75 \\\\ LLaMA-2-13B & 48.88 & 45.62 & 48.75 & 42.50 & 47.50 & 46.65 & 33.25 & 38.25 & 35.50 & 39.75 & 29.75 & 35.30 \\\\ LLaMA-2-70B & 58.50 & 54.37 & 57.75 & 47.88 & 61.00 & 55.90 & 37.75 & 41.50 & 39.25 & 43.50 & 42.50 & 40.90 \\\\ Mistral-7B & 53.87 & 48.38 & 48.50 & 44.75 & 50.12 & 49.12 & 37.50 & 36.00 & 33.25 & 38.50 & 36.00 & 36.25 \\\\ \\hline MiniMax & 54.50 & 55.88 & 52.50 & 54.12 & 53.00 & 54.00 & 36.75 & 40.50 & 41.00 & 40.25 & 40.50 & 39.80 \\\\ Baichuan2-7B & 52.12 & 43.75 & 44.88 & 39.62 & 49.00 & 45.87 & 35.25 & 34.00 & 30.00 & 38.50 & 34.25 & 34.40 \\\\ Baichuan2-13B & 55.00 & 47.38 & 48.12 & 42.62 & 54.25 & 49.47 & 39.00 & 38.50 & 30.75 & 40.75 & 35.50 & 36.90 \\\\ ChatGLM3-6B & 52.12 & 46.75 & 51.62 & 43.75 & 52.62 & 49.37 & 34.75 & 37.50 & 35.00 & 41.00 & 39.75 & 37.60 \\\\ Chinese-LLaMA-2-7B & 29.00 & 26.62 & 22.38 & 25.50 & 30.25 & 26.75 & 23.75 & 22.75 & 31.75 & 26.75 & 26.75 & 26.35 \\\\ Chinese-LLaMA-2-13B & 41.00 & 43.38 & 43.75 & 40.25 & 45.38 & 42.75 & 31.50 & 33.00 & 33.00 & 35.00 & 33.25 & 33.15 \\\\ Qwen-7B & 53.87 & 43.88 & 49.00 & 40.50 & 50.88 & 47.63 & 35.25 & 31.75 & 36.25 & 39.75 & 35.75 & 35.75 \\\\ Qwen-14B & 59.62 & 50.88 & 54.37 & 44.62 & 55.88 & 53.07 & 42.25 & 42.25 & 37.50 & 44.50 & 41.75 & 41.65 \\\\ Qwen-72B & 69.00 & 62.25 & 67.88 & 57.12 & 68.12 & 64.87 & 48.00 & 47.00 & 46.25 & 51.25 & 55.00 & 49.50 \\\\ Skywork-13B & 53.62 & 46.25 & 50.25 & 41.38 & 53.62 & 49.02 & 36.75 & 36.50 & 33.25 & 42.00 & 37.50 & 37.20 \\\\ Yi-6B & 57.38 & 51.50 & 52.88 & 43.62 & 53.25 & 51.73 & 39.75 & 39.50 & 37.25 & 42.00 & 42.00 & 40.10 \\\\ Yi-34B & 67.00 & 61.62 & 63.00 & 54.00 & 62.50 & 61.62 & **52.25** & 50.00 & 39.50 & 51.75 & 49.25 & 48.55 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: Zero-shot results on RoleEval (**en**) in five categories: celebrities (**CE**), anime and comics (**AC**), movie and TV series (**MT**), games (**GA**), and fiction (**FI**). [MISSING_PAGE_FAIL:17] [MISSING_PAGE_FAIL:18] [MISSING_PAGE_EMPTY:19] [MISSING_PAGE_EMPTY:20]"
    }
  ]
}