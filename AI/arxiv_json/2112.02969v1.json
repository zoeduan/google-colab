{
  "title": "Jigsaw: Large Language Models meet Program Synthesis",
  "authors": [
    "Naman Jain",
    "Skanda Vaidyanath",
    "Arun Iyer",
    "Nagarajan Natarajan",
    "Suresh Parthasarathy",
    "Sriram Rajamani",
    "Rahul Sharma"
  ],
  "abstract": "\n Large pre-trained language models such as , Codex [11], and Google's language model  [7]  are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool Jigsaw, targeted at synthesizing code for using Python Pandas API using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, Jigsaw has an important role to play in improving the accuracy of the systems. \n",
  "references": [
    {
      "id": null,
      "title": "Jigsaw: Large Language Models meet Program Synthesis",
      "authors": [
        "Naman Jain",
        "Skanda Vaidyanath",
        "Arun Iyer",
        "Nagarajan Natarajan",
        "Suresh Parthasarathy",
        "Sriram Rajamani",
        "Rahul Sharma"
      ],
      "year": "2021",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "GitHub Copilot • Your AI pair programmer",
      "authors": [],
      "year": "",
      "venue": "GitHub Copilot • Your AI pair programmer",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "",
      "authors": [
        "Jupyter"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "",
      "authors": [
        "Parenthesis Blog"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "",
      "authors": [
        "Parenthesis Stackoverflow"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Spider 1.0: Yale Semantic Parsing and Text-to-SQL Challenge",
      "authors": [],
      "year": "",
      "venue": "Spider 1.0: Yale Semantic Parsing and Text-to-SQL Challenge",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Program Synthesis with Large Language Models",
      "authors": [
        "Jacob Austin",
        "Augustus Odena",
        "Maxwell Nye",
        "Maarten Bosma",
        "Henryk Michalewski",
        "David Dohan",
        "Ellen Jiang",
        "Carrie Cai",
        "Michael Terry",
        "Quoc Le",
        "Charles Sutton"
      ],
      "year": "2021",
      "venue": "Program Synthesis with Large Language Models",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "DeepCoder: Learning to Write Programs",
      "authors": [
        "Matej Balog",
        "Alexander L Gaunt",
        "Marc Brockschmidt",
        "Sebastian Nowozin",
        "Daniel Tarlow"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "AutoPandas: neural-backed generators for program synthesis",
      "authors": [
        "Rohan Bavishi",
        "Caroline Lemieux",
        "Roy Fox"
      ],
      "year": "2019",
      "venue": "Proc. ACM Program. Lang",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "B Tom",
        "Benjamin Brown",
        "Nick Mann",
        "Melanie Ryder",
        "Jared Subbiah",
        "Prafulla Kaplan",
        "Arvind Dhariwal",
        "Pranav Neelakantan",
        "Girish Shyam",
        "Amanda Sastry",
        "Sandhini Askell",
        "Ariel Agarwal",
        "Gretchen Herbert-Voss",
        "Tom Krueger",
        "Rewon Henighan",
        "Aditya Child",
        "Daniel M Ramesh",
        "Jeffrey Ziegler",
        "Clemens Wu",
        "Christopher Winter",
        "Mark Hesse",
        "Eric Chen",
        "Mateusz Sigler",
        "Scott Litwin",
        "Benjamin Gray",
        "Jack Chess",
        "Christopher Clark",
        "Sam Berner",
        "Alec Mccandlish",
        "Ilya Radford",
        "Dario Sutskever",
        "Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Evaluating Large Language Models Trained on Code",
      "authors": [
        "Mark Chen",
        "Jerry Tworek",
        "Heewoo Jun",
        "Qiming Yuan",
        "Henrique Ponde De Oliveira Pinto",
        "Jared Kaplan",
        "Harrison Edwards",
        "Yuri Burda",
        "Nicholas Joseph",
        "Greg Brockman",
        "Alex Ray",
        "Raul Puri",
        "Gretchen Krueger",
        "Michael Petrov",
        "Heidy Khlaaf",
        "Girish Sastry",
        "Pamela Mishkin",
        "Brooke Chan",
        "Scott Gray",
        "Nick Ryder",
        "Mikhail Pavlov",
        "Alethea Power",
        "Lukasz Kaiser",
        "Mohammad Bavarian",
        "Clemens Winter",
        "Philippe Tillet",
        "Felipe Petroski Such",
        "Dave Cummings",
        "Matthias Plappert",
        "Fotios Chantzis",
        "Elizabeth Barnes",
        "Ariel Herbert-Voss",
        "William Hebgen Guss",
        "Alex Nichol",
        "Alex Paino",
        "Nikolas Tezak",
        "Jie Tang",
        "Igor Babuschkin",
        "Suchir Balaji",
        "Shantanu Jain",
        "William Saunders",
        "Christopher Hesse",
        "Andrew N Carr",
        "Jan Leike"
      ],
      "year": "2021",
      "venue": "Evaluating Large Language Models Trained on Code",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Web question answering with neurosymbolic program synthesis",
      "authors": [
        "Qiaochu Chen",
        "Aaron Lamoreaux",
        "Xinyu Wang",
        "Greg Durrett",
        "Osbert Bastani",
        "Isil Dillig"
      ],
      "year": "2021",
      "venue": "PLDI '21: 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Multimodal synthesis of regular expressions",
      "authors": [
        "Qiaochu Chen",
        "Xinyu Wang",
        "Xi Ye"
      ],
      "year": "2020",
      "venue": "Proceedings of the 41st ACM SIGPLAN International Conference on Programming Language Design and Implementation",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Maximal multi-layer specification synthesis",
      "authors": [
        "Yanju Chen",
        "Ruben Martins",
        "Yu Feng"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "PyMT5: multi-mode translation of natural language and Python code with transformers",
      "authors": [
        "Colin Clement",
        "Dawn Drain",
        "Jonathan Timcheck",
        "Alexey Svyatkovskiy",
        "Neel Sundaresan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Learning Syntactic Program Transformations from Examples",
      "authors": [
        "Reudismam Rolim De Sousa",
        "Gustavo Soares",
        "Loris D 'antoni",
        "Oleksandr Polozov",
        "Sumit Gulwani",
        "Rohit Gheyi",
        "Ryo Suzuki",
        "Bjoern Hartmann"
      ],
      "year": "2016",
      "venue": "Learning Syntactic Program Transformations from Examples",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "RobustFill: Neural Program Learning under Noisy I/O",
      "authors": [
        "Jacob Devlin",
        "Jonathan Uesato",
        "Surya Bhupatiraju",
        "Rishabh Singh",
        "Abdel-Rahman Mohamed",
        "Pushmeet Kohli"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Program synthesis using conflict-driven learning",
      "authors": [
        "Yu Feng",
        "Ruben Martins",
        "Osbert Bastani",
        "Isil Dillig"
      ],
      "year": "2018",
      "venue": "Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Programming by examples",
      "authors": [
        "Sumit Gulwani"
      ],
      "year": "2016",
      "venue": "Dependable Software Systems Engineering",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Transform-Data-by-Example (TDE): An Extensible Search Engine for Data Transformations",
      "authors": [
        "Yeye He",
        "Xu Chu",
        "Kris Ganjam",
        "Yudian Zheng",
        "R Vivek",
        "Surajit Narasayya",
        "Chaudhuri"
      ],
      "year": "2018",
      "venue": "Proc. VLDB Endow",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples",
      "authors": [
        "Ashwin Kalyan",
        "Abhishek Mohta",
        "Oleksandr Polozov",
        "Dhruv Batra",
        "Prateek Jain",
        "Sumit Gulwani"
      ],
      "year": "2018",
      "venue": "6th International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Accelerating search-based program synthesis using learned probabilistic models",
      "authors": [
        "Woosuk Lee",
        "Kihong Heo",
        "Rajeev Alur",
        "Mayur Naik"
      ],
      "year": "2018",
      "venue": "Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu",
        "Zhengbao Jiang",
        "Hiroaki Hayashi",
        "Graham Neubig"
      ],
      "year": "2021",
      "venue": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Integrating Programming by Example and Natural Language Programming",
      "authors": [
        "Mehdi Hafezi Manshadi",
        "Daniel Gildea",
        "James F Allen"
      ],
      "year": "2013",
      "venue": "Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "A Machine Learning Framework for Programming by Example",
      "authors": [
        "Aditya Krishna Menon",
        "Omer Tamuz",
        "Sumit Gulwani",
        "Butler W Lampson",
        "Adam Kalai"
      ],
      "year": "2013",
      "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "On the fly synthesis of edit suggestions",
      "authors": [
        "Anders Miltner",
        "Sumit Gulwani",
        "Vu Le",
        "Alan Leung",
        "Arjun Radhakrishna",
        "Gustavo Soares",
        "Ashish Tiwari",
        "Abhishek Udupa"
      ],
      "year": "2019",
      "venue": "Object-Oriented Programming, Systems, Languages & Applications (OOPSLA)",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "The pandas development team",
      "authors": [],
      "year": "2020",
      "venue": "pandas-dev/pandas: Pandas",
      "doi": "10.5281/zenodo.3509134"
    },
    {
      "id": "b26",
      "title": "Neuro-Symbolic Program Synthesis",
      "authors": [
        "Emilio Parisotto",
        "Abdel-Rahman Mohamed",
        "Rishabh Singh",
        "Lihong Li",
        "Dengyong Zhou",
        "Pushmeet Kohli"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions",
      "authors": [
        "H Pearce",
        "Baleegh Ahmad",
        "Benjamin Tan",
        "Brendan Dolan-Gavitt",
        "R Karri"
      ],
      "year": "2021",
      "venue": "An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "True Few-Shot Learning with Language Models",
      "authors": [
        "Ethan Perez",
        "Douwe Kiela",
        "Kyunghyun Cho"
      ],
      "year": "2021",
      "venue": "True Few-Shot Learning with Language Models",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Flashmeta: A framework for inductive program synthesis",
      "authors": [
        "Oleksandr Polozov",
        "Sumit Gulwani"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Language Models are Unsupervised Multitask Learners",
      "authors": [
        "Alec Radford",
        "Jeff Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "Language Models are Unsupervised Multitask Learners",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Code completion with statistical language models",
      "authors": [
        "Veselin Raychev",
        "Martin T Vechev",
        "Eran Yahav"
      ],
      "year": "2014",
      "venue": "ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI '14",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Compositional Program Synthesis from Natural Language and Examples",
      "authors": [
        "Mohammad Raza",
        "Sumit Gulwani",
        "Natasa Milic-Frayling"
      ],
      "year": "2015",
      "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJ-CAI 2015",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "SmBoP: Semi-autoregressive Bottom-up Semantic Parsing",
      "authors": [
        "Ohad Rubin",
        "Jonathan Berant"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Introduction to modern information retrieval",
      "authors": [
        "Gerard Salton",
        "Michael J Mcgill"
      ],
      "year": "1986",
      "venue": "Introduction to modern information retrieval",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "TF-Coder: Program Synthesis for Tensor Manipulations",
      "authors": [
        "Kensen Shi",
        "David Bieber",
        "Rishabh Singh"
      ],
      "year": "2020",
      "venue": "TF-Coder: Program Synthesis for Tensor Manipulations",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Predicting a Correct Program in Programming by Example",
      "authors": [
        "Rishabh Singh",
        "Sumit Gulwani"
      ],
      "year": "2015",
      "venue": "Computer Aided Verification -27th International Conference, CAV 2015",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers",
      "authors": [
        "Bailin Wang",
        "Richard Shin",
        "Xiaodong Liu",
        "Oleksandr Polozov",
        "Matthew Richardson"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "B2: Bridging Code and Interactive Visualization in Computational Notebooks",
      "authors": [
        "Yifan Wu",
        "Joseph M Hellerstein",
        "Arvind Satyanarayan"
      ],
      "year": "2020",
      "venue": "UIST '20: The 33rd Annual ACM Symposium on User Interface Software and Technology, Virtual Event",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Sketch-Driven Regular Expression Generation from Natural Language and Examples",
      "authors": [
        "Xi Ye",
        "Qiaochu Chen",
        "Xinyu Wang",
        "Isil Dillig",
        "Greg Durrett"
      ],
      "year": "2019",
      "venue": "Sketch-Driven Regular Expression Generation from Natural Language and Examples",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task",
      "authors": [
        "Tao Yu",
        "Rui Zhang",
        "Kai Yang",
        "Michihiro Yasunaga",
        "Dongxu Wang",
        "Zifan Li",
        "James Ma",
        "Irene Li",
        "Qingning Yao",
        "Shanelle Roman",
        "Zilin Zhang",
        "Dragomir R Radev"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Calibrate Before Use: Improving Few-shot Performance of Language Models",
      "authors": [
        "Tony Z Zhao",
        "Eric Wallace",
        "Shi Feng",
        "Dan Klein",
        "Sameer Singh"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Jigsaw: Large Language Models Meet Program Synthesis",
      "text": "Naman Jain t-namanjain@microsoft.com Microsoft Research Bangalore, India Skanda Vaidyanath svaidyan@stanford.edu Arun Iyer ariy@microsoft.com Microsoft Research Bangalore, India Nagarajan Natarajan nagarajn@microsoft.com Microsoft Research Bangalore, India Suresh Parthasarathy supartha@microsoft.com Sriram Rajamani sriram@microsoft.com Microsoft Research Bangalore, India Rahul Sharma rahsha@microsoft.com Microsoft Research Bangalore, India"
    },
    {
      "title": "Abstract.",
      "text": "Large pre-trained language models such as GPT-3 (Gupta et al., 2018), Codex (Kumar et al., 2018), and Google's language model (Pedraza et al., 2018) are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool Jigsaw, targeted at synthesizing code for using Python Pandas API using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, Jigsaw has an important role to play in improving the accuracy of the systems. + Footnote †: Work done by author during internship at Microsoft Research, India + Footnote †: Work done by author during internship at Microsoft Research, India + Footnote †: Work done by author during internship at Microsoft Research, India + Footnote †: Work done by author during internship at Microsoft Research, India + Footnote †: Work done by author during internship at Microsoft Research, India + Footnote †: Work done by author during internship at Microsoft Research, India + Footnote †: Work done by author during internship at Microsoft Research, India"
    },
    {
      "title": "1. Introduction",
      "text": "Pre-trained large language models (PTLM) such as GPT-3 (Gupta et al., 2018) are finding pervasive applications in Natural Language Processing (NLP), as a general purpose platform to solve many NLP tasks. Recent efforts show that PTLMs can generate code from natural language prompts, by associating documentation text with code from a large training set (Pedraza et al., 2018; Gupta et al., 2018; Kumar et al., 2018). This presents a new avenue for program synthesis. However, PTLMs do not \"understand\" either the syntax or semantics of the code, and treat code as text (Pedraza et al., 2018). Consequently, the code produced by such models has no guarantees of correctness or quality. Hence, any system that uses such PTLMs to generate code will need to augment it with program analysis and program synthesis modules to ensure correctness. In this paper, we present the design and empirical evaluation of such a multi-modal program synthesis system called **Jigsaw**, which is targeted specifically at synthesizing code for using large and complex APIs. Jigsaw is multi-modal (as depicted in Figure 1) in the sense that it can ingest input as (1) a natural language string expressing intent and (2) a set of test cases, or input-output examples, and produces a code snippet as output. Future incarnations may be designed to accept other modes of input as well. The architecture of Jigsaw is shown in Figure 2. The pre-processing module converts the natural language intent into a customized query to send to the PTLM. The post-processing module performs syntactic and semantic checks, and performs transformations on the code produced by the PTLM, ensuring that the code passes the supplied test cases and other quality checks. The transformations are specifically designed to correct common and recurring errors made by PTLMs, such as referencing errors (where the code references variable names incorrectly), argument errors (where the code invokes the correct API, but with incorrect arguments), and a class of semantic errors (which can be corrected by learning AST-to-AST transformations). Section 2 shows concrete examples of such errors, and Section 3 shows how the transformations correct such errors. Jigsaw learns from usage by incorporating user feedback into both pre-processing and post-processing modules, and learns from user engagements to improve its overall quality. Our experiments show how Jigsaw is able to learn from past usage to improve future performance. The current version of Jigsaw is designed and evaluated to synthesize code for the Python Pandas API (Pedraza et al., 2018). However, the principles behind the design of Jigsaw are general, and the design can be extended to other libraries and programming languages as well. We create a user interface for Jigsaw using a Jupyter notebook (Pedraza et al., 2018) extension. The extension can be invoked using a magic command, and invocation of the command creates a sidebar window with a Jigsaw card for each invocation. The Jigsaw card allows users to Figure 1. Multi-modal problem specification in Jigsaw [MISSING_PAGE_FAIL:2] The user then issues the following query in the session: %jigsaw -q \"Remove substring 'Name:' from column 'country' of df\" Jigsaw produces the following snippets. ``` {df['country']=df['country'].str.replace('Name:',',') } ``` A key knob in PTLMs is setting the right context for a given user query. This context is passed as an input to the PTLM in addition to the user query. To this end, Jigsaw first prepares the input in the _pre-processing stage_ (details in Section 3.2). The preparation involves assembling a set of \"relevant\" question-answer pairs to inform the PTLM of the nature of the input task -- which is converting natural language text to Python code, specifically, Pandas code. With the context selection in the pre-processing stage, Jigsaw produces the desired code snippets shown above. In contrast, the GPT-3 model without context selection produces the following incorrect snippet for the above query: ``` df=df.country.str.remove('Name:') ``` Recent studies, both in the context of natural language understanding [30, 44] as well as in the programming [7] domains, have shown the influence and importance of context selection in the output of PTLMs. Our work provides further evidence that context selection can significantly impact the quality of the code generated for Pandas programming tasks, with two different PTLMs, demonstrated in Section 5.4."
    },
    {
      "title": "Learning To Fix Recurring Failure Modes Of Ptlms",
      "text": "The core aspect of Jigsaw system design is incorporating a post-processing phase that involves: (a) characterizing, (b) transforming the (syntactically and/or semantically) erroneous code snippets, and, more importantly, (c) endowing the system with the capability to improve (in terms of accuracy) from feedback as more users interact with it over time. Below, we highlight common classes of errors we observe over two different Pandas programming datasets (created by us, and described in Section 4) using two different PTLMs, namely GPT-3 and Codex. **1. Referencing errors:** We observe that, even with suitable context, PTLMs can produce incorrect referencing of variable names in otherwise accurate code snippets. **2. Incorrect arguments:** In some cases, PTLMs produce code with the right composition of API functions, but with incorrect arguments. For instance, consider the following invocation: %jigsaw -q \"remove all duplicate entries of column 'inputB\" ``` dfout=df.drop_duplicates(subset=['imgB'])#PTLM dfout=df.drop_duplicates(subset=['imgB'],keepFalse)#Correct ``` **3. Semantic errors:** A recurring failure mode for the PTLMs we have experimented with is that they produce code snippets that are _almost_ correct, but the semantics are wrong because of a minor error. We can quantify this via suitable edit distance between the ASTs of the produced and the correct (i.e., intended) code snippets. For instance, consider the following invocation: %jigsaw -q \"Get fourth value from column 'C' in dfin and assign to dfout\" ``` dfout=dfin.ix[3,'C']#PLLM dfout=dfin.loc[3,'C']#Correct ``` Jigsaw employs a post-processing phase that critically relies on the multi-modal specification (I/O examples, in particular) to overcome the aforementioned recurring failures. To this end, we pass the incorrect output code snippet from PTLM (which can be ascertained with the help of I/O examples in the specification) through a series of components driven by PL-based techniques (details in Section 3.3). The two key ideas are outlined below. 1. Using the API functions in the incorrect code snippets produced by PTLM, we seed the enumerative search for the right arguments. We perform this search efficiently adapting the AutoPandas framework [9] which is an enumerative-search based programming by examples framework built for Pandas API 2. The user interface of Jigsaw enables getting feedback which is then used by our system to learn a set of AST-to-AST transformations using the Prose synthesis framework [16, 31]. The challenge here lies in clustering errors that are _alike_ so that a small set of general transformations can be learnt."
    },
    {
      "title": "3. Jigsaw Architecture",
      "text": "The architecture of Jigsaw is depicted in Figure 2. In this section, we describe each module in detail. Figure 2. Architecture of Jigsaw"
    },
    {
      "title": "Pre-Trained Language Models",
      "text": "We describe Pre-trained Language Models (PTLMs) using GPT-3 as an example. GPT-3 stands for \"Generative Pre-trained Transformer 3\", which is the third version of a large transformer model developed by OpenALGPT-3 is a neural model with 175 billion parameters, trained on a very large corpus consisting of publicly available datasets such as CommonCrawl 1, WebRText dataset, two internet-based books corpora, and English Wikipedia. GPT-3 is a general-purpose model that can be customized to perform a variety of NLP tasks. Such customizations do not involve fine-tuning the ML model for the specific task at hand. Instead, the user of GPT-3 can describe the task using a few examples (on the order of 4-5 examples works usually), and GPT-3 is then able to produce answers for the specific task. A session with GPT-3 has the form: \\((Q_{1},A_{1})\\), \\((Q_{2},A_{2})\\), \\(\\ldots\\), \\((Q_{k},A_{k})\\), \\(Q\\), where \\(k\\) is a small number (typically 4 or 5), the pairs \\((Q_{i},A_{i})\\) are question-answer pairs to describe the task we want GPT-3 to perform, and \\(Q\\) is the question for which we seek an answer. Footnote 1: [https://commoncrawl.org/the-data](https://commoncrawl.org/the-data) For example, if \\((Q_{i},A_{i})\\) are such that \\(Q_{i}\\) are English statements and \\(A_{i}\\) are corresponding French translations, then GPT-3 becomes an English-French language translator. See session in Figure 3. Other recent PTLMs include Codex (Cowlin et al., 2017), which is OpenAI's recent language model trained specifically on code, and Google's large language model (Cowlin et al., 2017); these models translate natural language to program. Jigsaw uses PTLMs to produce Pandas code, given a natural language description of intent, and test cases. Specifically, Jigsaw session with GPT-3 has the form: \\((N_{1},P_{1})\\), \\((N_{2},P_{2})\\), \\(\\ldots\\), \\((N_{k},P_{k})\\), \\(N\\) where \\(N_{i}\\) is English description of intent, and \\(P_{i}\\) is the code snippet we want the PTLM to produce. We currently do not pass input-output examples to the PTLM. Instead, we use these test cases to check and filter the candidate codes produced by the PTLM during post-processing, or transform the code produced by the PTLM such that it passes the test cases."
    },
    {
      "title": "Pre-Processing",
      "text": "The goal of Jigsaw's pre-processing module is to convert the user intent into a suitable query for the PTLM. As mentioned above, PTLMs take a sequence of question-answer pairs \\((N_{1},P_{1})\\), \\((N_{2},P_{2})\\), \\(\\ldots\\), \\((N_{k},P_{k})\\) as a preamble before we supply the current query. We use the term _context_ to denote this preamble. Previous works in natural language processing (NLP) (Zhu et al., 2017; Wang et al., 2018; Wang et al., 2018) have shown that the performance of these PTLMs is heavily influenced by this context; and the performance improves if the question-answer pairs in context are similar to the current query \\(N\\). Hence, we maintain a _context bank_, of possible question-answer pairs, and then choose elements of the context bank that are similar to the current query, and add these to the context. Jigsaw creates a context bank offline by scraping and annotating examples from API documentation for Pandas, as well as other resources of examples (from tutorials, etc.) that are used to teach the API. When the user asks a question, the question is fed to a context selector that uses a text similarity metric to pick the most relevant prompts from the context bank (see Figure 4). We study two kinds of similarity metrics for the context selector: _a)_ tf-idf similarity (Yin et al., 2017) (TFIDF), and _b)_ transformer similarity (Yin et al., 2017) (TRANSFORMER). The context thus produced is appended with the current query and fed as input to the PTLM. In cases where Jigsaw is unable to produce the correct answer, we let users make changes to the incorrect Jigsaw code and use such a feedback to enhance the context bank (details in Section 3.4). PTLMs also take an input parameter called _temperature_. Lower values of temperature result in fewer accurate answers. Higher values result in a less accurate but more diverse set of answers. We report on how we pick temperature values in Section 5."
    },
    {
      "title": "Post-Processing",
      "text": "The code snippets produced by the PTLM vary in accuracy and quality, depending on the natural language sentences used to ask to encode the question, the context bank supplied, the context selection as well as the temperature parameter. The goal of Jigsaw's post-processing step is to filter and transform the output produced by the PTLM to produce a correct answer. Our measure of correctness is that the code produced should pass the I/O examples specified by the user. In many cases, the code does not parse or fails with an exception. We consider such cases as test failures. If a non-empty set of candidate solutions produced by the PTLM satisfies the test cases, then we merely show those code snippets to the user. Our experience is that for about 30%-60% of the cases (depending on the PTLM and the dataset), the PTLM produces correct outputs. In the remaining cases, Jigsaw uses the candidate solutions produced by PTLM as starting points and performs transformations on candidate code snippets using simple program analysis and synthesis techniques to produce correct solutions. We describe the correctness checks and transformations below: **Correctness checks:** In cases where we have I/O examples, we run the candidate code snippet starting with each of the specified Figure 4. Illustration of Pre-processing step of Jigsaw Figure 3. English to French translation using GPT-3inputs, and check if the output produced agrees with the corresponding specified output. This check can be expanded to include static analyses to check for security vulnerabilities and other errors, as motivated by recent work [29]. **Variable Name transformations**: In some cases, PTLMs produce accurate code snippets, but with incorrect variable names. This is often due to the model's bias towards common variable names like _df_ or Pandas dataframes and also because users assume variable referencing to be implicit. As an example, we find that GPT-3 produces the code snippet df1.merge(df2) when the correct answer is df2.merge(df1). Since users specify inputs and output variables in the natural language description or in test cases, this post-processing step uses such information from multi-modal inputs, as well as names of variables in scope, by systematically searching over potential variables, and trying possible permutations and combinations of variable names so as to pass the test cases. **Argument transformations**: In some cases, the PTLMs produce code snippets with correct method names and method sequences (in case multiple methods need to be invoked in a nested manner or one after another), but with incorrect arguments. As an example, in response to the query \"replace 'United States' in 'location' by 'US' and '3434' in 'zip' by '4343', Codex produces: ``` dfout=df.replace({'United States':'US', 3434:4343}) ``` This snippet invokes the correct method replace, but misses the detail in the question that 'United States' and 3434 must be replaced with 'US' and 4343 _only_ when these values are present in the columns 'location' and 'zip' respectively. The correct code synthesized by Jigsaw for this query is as shown below: ``` dfout=dfrin.replace({'location':{'United States':'US'}, 'zip':{'3434:4343}}) ``` Motivated by such cases, this post-processing step systematically searches through arguments from an inferred argument space for a given sequence of method/function names. In order to implement the systematic search over the space of arguments, we adapt the approach used by Autopandas tool [9], with the following modifications. Autopandas uses a Graph Neural Network, that takes I/O examples as input, to choose method names. However, we need a lot of domain-specific data to train such neural networks. In our case, we simply extract the method names from the output of PTLM given the natural language query (which readily scales to programming domains beyond Pandas). The argument space to perform the search is inferred using the natural language text input, the arguments present in the PTLM output, the column names from the dataframe schema as well as variables in scope. We extend the generators in Autopandas to consider complex data types such as lists and dictionaries, and we extend the set of APIs considered to include APIs that return Pandas Series types ( one-dimensional labeled arrays capable of holding data of any type) in addition to the ones that return Pandas dataframe types. With these modifications, we find that Jigsaw is able to transform several incorrect code snippets produced by the PTLM to correct code snippets (as shown in Section 5). **AST-to-AST transformations**: In some cases, we find that the PTLM produces code that is almost correct but has a minor error. We also find that such errors are _repeatedly_ made by the PLTM, and are fixable with suitable AST-to-AST transformations, learned from user interactions with Jigsaw. As a specific example, we find GPT-3 often misses the bitwise not operator, and produces the code: ``` train=datadata.index.isin(test.index)] ``` instead of the following correct code with the bitwise not operator: ``` train=data[~data.index.isin(test.index)] ``` As another example, we find that GPT-3 misses paranthesizations, which results in the generated code raising an exception. Specifically, the generated code is: ``` dfout=dfinf['bar':'>d8]dfinf['bar'>d8] ``` instead of the following code synthesized by Jigsaw which is parenthesized correctly: ``` dfout=dfmin[(dfinf['bar':'>d8])(dfinf['bar'>d8]) ``` Such errors cannot be fixed via variable name transformation or argument transformations. Jigsaw corrects such errors by learning re-writing rules as AST-to-AST transformations. These transformations are applications of production rules from grammar used in BluePencil [26] which is used for suggesting code re-factorings. However, it is not possible to learn these rules at the appropriate level of generality from a single example. This generality is necessary so that the missing negation or parenthesizing can be corrected by the learnt transformation, even if the same pattern is repeated with a different set of variables or constants. To achieve this, we collect data from user interactions, where the user edits the answer produced by Jigsaw to produce the correct code. We cluster the data points (i.e., code snippets) so that similar data points are grouped together and we learn a single AST-to-AST transformation that is able to handle all the data points in a cluster. Unlike the case of refactoring where users will implicitly hint at clustering of similar edits (by attempting them one after the other), we resort to a greedy heuristics-based clustering algorithm. This clustering is performed in an online fashion as we get more data points for learning AST-to-AST transformations. For each data point, we decide if the data point is grouped inside an existing cluster or instantiate a new cluster. In the former case, we check if the AST-to-AST transformations from the existing cluster can be re-learnt to be more general, and if so, re-learn the transformations. In addition, we perturb the data points in each cluster to change variable names and constants, in order to prevent learning transformations that over-fit. Together with the above-mentioned clustering and perturbation heuristics, we find that we are able to learn transformations at the appropriate level of generality (Section 5.2.1). Code has well-defined structure, usually represented as abstract syntax trees. We take advantage of this structure while learning these AST-to-AST tree transformations. We use the PROSE program synthesis system [31, 16] to learn the transformations from a cluster of incorrect-correct code snippets. While Jigsaw currently works only on Python code, the post-processing step works at the level of ASTs, and can be made to work across programming languages as well. For instance, in [26] the same tools are deployed to learn non-trivial code refactoring in C#, SQL, Markdown, and spreadsheets. We refer to Argument transformations and AST-to-AST transformations together as Semantic Repair in experiments (Section 5)."
    },
    {
      "title": "Learning From User Feedback",
      "text": "The user interface of Jigsaw (integrated into the Jupyter notebook) is designed to let users submit correct code in cases where Jigsaw is incorrect. Jigsaw can be improved by assimilating user feedback. Specifically, we design techniques for updating context-bank in the pre-processing module and AST-to-AST transformations in the post-processing steps, as more users interact with Jigsaw. **Updating context bank:** The procedure for updating context bank with user queries is given in Algorithm 1. We first check whether Jigsaw already found a correct solution for the given (new) query \\(N\\), thus giving us some confidence about its correctness. Otherwise, we check if any of the solutions generated by Jigsaw is \"close\" to some correct code (determined by the standard edit distance on strings \\(\\mathbf{d}_{\\text{EDIT}}\\) and a chosen threshold \\(\\epsilon_{\\text{CODE}}\\)). If either of the two conditions is satisfied, we add the new query to our context bank while additionally ensuring that a similar query already does not exist (via TFIDF based distance \\(\\mathbf{d}_{\\text{TFIDF}}\\) and a threshold \\(\\epsilon_{\\text{BANK}}\\)). ``` 0: Context Bank : C = {(\\(N_{1}\\), \\(P_{1}\\)), (\\(N_{2}\\), \\(P_{2}\\)), \\(\\ldots\\), (\\(N_{|\\text{C}|}\\), \\(P_{|\\text{C}|}\\))}, New query and feedback (code snippet): \\(N\\), \\(P\\) 0: Updated Context bank C 0: Let output = Jigsaw(\\(N\\), C) If \\(\\min_{i}\\mathbf{d}_{\\text{EDIT}}(\\text{output}_{i},P)>\\epsilon_{\\text{CODE}}\\) return C If \\(\\max_{i}\\mathbf{d}_{\\text{TFIDF}}(N,N_{i})<\\epsilon_{\\text{BANK}}\\) return C return C \\(\\cup\\{(N,P)\\}\\) ``` **Algorithm 1** Updating context bank **Updating transformations:** For every query paired with correct code snippet(s), we select all incorrect codes suggested by PTLM within some small edit distance of a correct code. The AST-to-AST transformations learning sub-module performs clustering (with perturbations) on the selected code snippets as discussed in the above subsection, and updates the set of transformations. While the above pre-processing and post-processing steps were designed in the context of the Python Pandas API, we believe that ideas such as context selection, correctness checking, and transformations are general and that it is possible to design pre-processing and post-processing steps in a generic manner that can work across languages and APIs. For each API, specific transformation rules can be learnt from usage data generated by users of that API."
    },
    {
      "title": "4. Datasets",
      "text": "We perform our experiments on two different datasets 2. Footnote 2: The datasets can be found at [https://github.com/microsoft/JigsawDataset](https://github.com/microsoft/JigsawDataset)"
    },
    {
      "title": "Pandaseval Dataset Pandaseval1",
      "text": "This dataset consists of 68 Python Pandas tasks. Each task can be solved using a single line of code by composing at most 2-3 Pandas functions; sometimes followed by assigning variables. This dataset was created by authors of this paper by going through queries in online forums like StackOverflow. An example task from this dataset is \"For every row in df1, update 'common' column to True if value in column 'A' of df1 also lies in column 'B' of df2'."
    },
    {
      "title": "Hackathon Dataset Pandaseval2",
      "text": "This dataset consists of 21 Pandas tasks; each task can be solved by composing at most 2-3 Pandas functions, possibly followed by assigning variables, as in the PandasEval1 dataset. We posed these tasks as illustrations, in a hackathon we conducted with 25 users over 2 different sessions. Table 1 presents self-reported proficiency of the users in Python and Pandas. An example illustration that shows the intent of a task is given in Figure 5. Users were asked to read such pictorial illustrations and come up with their own natural language (English) query constructions for each task. We then collected the queries written by the users, clustered, and annotated them to produce the PandasEval2 dataset comprising of a total 725 unique queries constructions. The task corresponding to the illustration in Figure 5, from the dataset PandasEval2, is shown below. Here dfin and dfout refer to the dataframes in Figure 5. We note that while users provided precise and clear natural language queries in many cases, they also came up with imprecise and incorrect formulations in some cases. For instance, in the spec shown above, the query provided by user1 is correct, whereas the one provided by user2 is incorrect because the word \"_France_\" is present in the \"_LATA_\" column as well; Figure 5 conveys that only the \"_country_\" column needs to change, and not the \"_LATA_\" column. Since such queries were created by users interacting with the system, and users tend to make mistakes, it is useful to have such variations in the dataset. While curating the dataset, we removed natural language queries that were clearly incorrect, and retained queries that were imprecise and partially correct. ``` 1\"task_8\":{ 2\"queries\":[ 3[\"replace 'France' with 'FR' in 'country' column and 'Paris' with 'PAR' in 'city' column\", \"user'], 4[\"In dataframe dfin, replace cells having'France' to 'FR' and cells having 'Paris' to 'PR', \"user2\"] 5], 6\"IO\":[[ 7\"inputs\": \"dfin\", \\begin{table} \\begin{tabular}{|c|c|c|c|} \\hline & Beginner & Intermediate & Advanced \\\\ \\hline Python & 1 & 21 & 3 \\\\ Pandas & 17 & 8 & 0 \\\\ \\hline \\end{tabular} \\end{table} Table 1. Proficiency of participants from PandasEval2 dataset Figure 5. Example task, part of the dataset PandasEval2, as presented to the user during the Hackathon session. As mentioned earlier, we conducted the hackathon over two sessions. We use PandasEval2_S1 to denote the dataset generated from user queries from the first session, and PandasEval2_S2 to denote the dataset generated from user queries from the second session. For each of the 21 tasks, we created semantic variations (e.g. changing constants, API arguments) of the same task. Consequently, users in the second session (PandasEval2_S2) saw different variants of the tasks when compared to users in the first session (PandasEval2_S1). Specifically, 3 tasks were exactly the same, 9 had differences in constants and 9 had changes in arguments. We introduced these variants in order to study if jigsaw can learn from usage in the first session to improve user experience in the second session (see Section 5.2). We use PandasEval2 to denote the union of PandasEval2_S1 and PandasEval2_S2."
    },
    {
      "title": "5. Experiments",
      "text": "We evaluate Jigsaw on the two datasets introduced in Section 4, with emphasis on the following questions: _a_) How accurate is the Jigsaw system compared to the black-box PTLMs and other code synthesis methods? _b_) What is the utility and applicability of the individual Jigsaw components (in the pre-processing and post-processing modules)? _c_) Can these components benefit from feedback over time as more users interact with the system? For the first question, we evaluate jigsaw in an offline setting, i.e., without learning from any feedback (in Sections 5.1); and present comparisons against the state-of-the-art AutoPandas framework, which generates Pandas snippets using only I/O example (in Section 5.3). For the second question, we perform a temporal study on the PandasEval2 dataset (in Section 5.2), where we leverage user feedback from the first hackathon session to update the system and measure the performance improvement in the second session. We also perform ablation studies (in Section 5.4) pertaining to our context selection sub-module. We end with a preliminary evaluation of Jigsaw on tasks pertaining to the TensorFlow API (in Section 5.5). We consider **accuracy** as our primary evaluation metric, i.e., fraction of specifications in the dataset for which a _correct_ program was synthesized. We define a program as correct if it satisfies the given I/O examples, and additionally passes a manual inspection of whether the synthesized code meets the intent of the natural language description. The manual inspection helps us reject programs that satisfy the I/O examples by overfitting on them and violate the general intent of the natural language descriptions. Note that there is inherent randomness in the output of the PTLMs, so we run every evaluation three times and report the mean accuracy (%) and standard deviation (over the runs). In some cases, we also present **task completion** metric which is the percentage of tasks correctly solved by a user (regardless of the number of queries used to solve a task) interacting with the system. Furthermore, in every case, we present the best accuracy obtained by varying the temperature parameter of \\(\\mathsf{PTLM}\\in\\{0,0.2,0.4,0.6\\}\\)."
    },
    {
      "title": "Offline Evaluation",
      "text": "In Table 2, we present the performance of Jigsaw on PandasEval1 and PandasEval2 datasets, with GPT-3 and Codex as black-box PTLMs. The second column of the table indicates the context selection strategy for the PTLM. For this study, we consider NO-CONTEXT (no tailored context provided for the user query; we use a default context: \"import pandas as pd\"), and TRANSFORMER (Transformer similarity based context selection, discussed in Section 3) with number of context prompts fixed as 4. Each cell in the table gives the accuracy metric with mean and standard deviation as defined above. For Jigsaw, the column titled Variable Name indicates the performance of the system using only this part of the post-processing module; and the column titled Semantic Repair indicates the performance of the system in its entirety, i.e., running Variable Name transformations followed by Semantic Repair (Argument transformations and AST-to-AST transformations). Comparing PTLM and Semantic Repair columns, it is evident that Jigsaw improves upon the black-box PTLMs, in terms of accuracy, by 15%-40% irrespective of the context selection strategy, on both the datasets as well as on both the PTLMs. These results underscore the utility of program analysis-based augmentation of large language models. Next, from Table 2, we find that providing useful context for the language model along with the query significantly improves upon not providing any context (comparing rows 1 vs 2, and rows 3 vs 4), across the datasets and PTLMs. It is clear that PTLM with TRANSFORMER context is better than NO-CONTEXT by a margin \\(\\sim 5\\%\\) (without post-processing) and up to 15% with post-processing for Codex on the two datasets. For GPT-3, TRANSFORMER context is significantly better than NO-CONTEXT on the PandasEval2 dataset and on PandasEval1, the numbers are statistically insignificant. PTLMs require some initial context in the form of examples to characterize the task to be solved, and these results underscore the importance of having a pre-processing module in Jigsaw. Finally, from Table 2, we also observe the effectiveness of the individual post-processing modules of Jigsaw, as discussed below. Note that, for these results, we seed our AST-to-AST transformations using a small dataset collected from StackOverflow questions. Later, in Section 5.2, we show that these numbers can be significantly improved by learning transformations from usage over time. **Variable Name transformations:** PTLMs make variable referencing errors (as noted in Section 2) because of its implicit bias towards common dataframe names such as df, df1, df2, dfout and also because users tend to not specify variables explicitly in their queries. We find that this simple post-processing module gives an improvement of 10%-30% for Codex and 10%-15% for GPT-3. **Semantic Repair:** We see that the semantic repair post-processing module improves absolute performance of Codex by \\(\\sim 5\\%\\) and of GPT-3 by 6%-11%. This underscores the significance of using program analysis techniques to augment language models that do not have inherent understanding of code semantics. Recall (from Section 3) that Semantic Repair consists of Argument transformations and AST-to-AST transformations sub-modules. We find that, using just the Argument transformations (without AST-to-AST transformations), improves absolute performance of the system by 5%-9% and 3%-5% for GPT-3 and Codex respectively (not shown in Table 2). [MISSING_PAGE_FAIL:8] and of the code snippets themselves, and we expect that more usage data positively influences the overall quality. ```"
    },
    {
      "title": "Task-L@Fout=Df.Loc[Df.Isnull(O.My(Axis=L),:]#Incorrect@Fout=Df.Loc[Df.Isnull(O.My(Axis=L))]#Correct@Fust-2X-Ldf.Isnull(O.My(Axis=L))]#Incorrect@Fust-2X-Ldf.Isnull(O.My(Axis=L))]#Incorrect@Fust-2X-Ldf.P=Df.P.Loc[Df.Per[\"Name\"].Str.Contains(\"Ch\")]#Incorrect@Fust-2X-Ldf.Per[\"Name\"].Str.Contains(\"Ch\")]#Correct",
      "text": "``` Listing 2Cluster of code snippets from two different tasks that yields the Bitwise-Not transformation in Table 4."
    },
    {
      "title": "Comparison To Autopandas",
      "text": "AutoPandas (AP) (Becker et al., 2017) is a Pandas program synthesis engine capable of generating programs with two or three Pandas functions. It uses _generators_ for enumerating over the Pandas API and guides the search with the help of Graph Neural Networks (GNNs) which operate on the input-output (I/O) dataframe(s) and returns the most likely function sequences and arguments. In contrast, we make use of multi-modal specification (both natural language query and I/O examples). Programming by examples is known to have ambiguous under-specifications (Krishna et al., 2017; Krishna et al., 2017). From our experience this issue is exacerbated for large APIs that provide multiple ways for achieving similar functionalities. For instance, consider the specification in Figure 1. If we only consider the I/O example for the given task, we can find many trivial solutions that just drop or select certain rows of dataframe. We evaluate AP on our PandasEval1 and PandasEval2 datasets. As discussed in Section 3, AP does not support series operations, column assignments and dictionary or list generators, many of which are necessary in Pandas workflows. So, out of 68 tasks in the PandasEval1 dataset and 21 tasks in the PandasEval2 dataset, only 7 and 20 are covered by the AutoPandas framework respectively. Hence, we compare Jigsaw (instantiated with the Codex PTLM) against AP only on these 27 tasks and use a timeout of 3 minutes. In the first row of Table 5, we see that Jigsaw clearly outperforms AutoPandas even in the restricted subset solvable by AP. This is because 16 of the 27 tasks are under-specified if only I/O examples are used and AP returns over-fitting solution on many of these tasks; this highlights the necessity of multi-modality. We also run Jigsaw on the AP dataset (Becker et al., 2017), where all tasks are supported by AutoPandas and I/O examples are sufficient. This dataset has been sourced from StackOverflow posts. Since Jigsaw uses text as the primary input, we add natural language descriptions in these posts for querying Codex. The results are in the second row of Table 5; while Codex alone is inferior to AP, Jigsaw (with Codex) performs better than AP."
    },
    {
      "title": "Ablation Study",
      "text": "In both the offline and temporal evaluations presented in the previous subsections, we fixed the number of context prompts to 4 and TRANSFORMER as the context selector in the pre-processing module. In this ablation study, we ask if the performance of Jigsaw is sensitive to these choices, and provide justification for the same. All experiments in this section are carried out with the same setting as that of Section 5.1. Table 6 compares the performance of Jigsaw with two different context selection strategies, namely, TFIDF and TRANSFORMER. We find that the transformer context selector is slightly better, but more importantly, that the performance of Jigsaw is not sensitive to the selection strategy. Table 7 compares the performance of Jigsaw with different number of context prompt examples, i.e., 1, 4, and 8. Our experiments show that while there isn't a significant difference between the performances of 4 prompts vs. 8 prompts, both perform better than using just 1 prompt. Again, Jigsaw is relatively robust to these choices. Finally, note that all variations of these choices, for the number of prompts as well as the selection strategy, outperform the NO-CONTEXT setting (see Table 2); this further underscores the utility of the pre-processing module. \\begin{table} \\begin{tabular}{l l l l} \\hline \\hline & Context & PandasEval1 & PandasEval2 \\\\ \\hline \\multirow{2}{*}{GPT-3} & TFIDF & \\(46.5\\pm 4.8\\) & \\(32.4\\pm 0.5\\) \\\\ & TRANSFORMER & \\(47.1\\pm 2.1\\) & \\(35.1\\pm 0.7\\) \\\\ \\hline \\multirow{2}{*}{Codex} & TFIDF & \\(69.1\\pm 2.4\\) & \\(70.1\\pm 0.1\\) \\\\ & TRANSFORMER & \\(66.7\\pm 0.7\\) & \\(72.2\\pm 0.5\\) \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6. Ablation study: Performance of jigsaw with two context selection strategies. \\begin{table} \\begin{tabular}{l l l l} \\hline \\hline Code Before & Code After & Semantic Explanation \\\\ \\hline out=data[data.index.isin(test.index)] & out=data[data.index.isin(test.index)] & Adding bitwise not inside subscript \\\\ ff=df[df[’foo’]>70][df[’foo’]<34] & ff=df[’foo’]>70] & Parenthesizing mistake3 \\\\ out=df.loc[’,’HP’] & out=df.loc[’,’HP’] & Changing iloc to loc \\\\ fout=df1.append(df2) & fout=df1.append(df2) & Dropping the last keyword argument \\\\ dfout=dfn.duplicated() & fout=dfn.duplicated() & Computing sum of series using.sum() \\\\ train=data.drop(test) & train=data.drop(test.index) & Adding.index in first argument (of drop) \\\\ dfn=dfn[’A’].rolling(window=3).mean() & dfn[’A’]=dfn[’A’].rolling(3).mean() & Reassign back to the column \\\\ fout=dfn[(x<40)](y>53) & dout=dfn[(x<40)](y>53) & d(z=z=4)] \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4. Applications (Code After) of learned transformations on code snippets produced by PTLM (Code Before). \\begin{table} \\begin{tabular}{l l l l} \\hline \\hline & AutoPandas (Becker et al., 2017) & PTLM & Jigsaw \\\\ \\hline Subset of Jigsaw datasets & 16/27 & 20/27 & 23/27 \\\\ AutoPandas dataset & 17/26 & 15/26 & 19/26 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5. Number of tasks solved by Jigsaw and AP on a subset of our dataset supported by AP and their dataset."
    },
    {
      "title": "Beyond Pandas",
      "text": "To test the generality of Jigsaw, we did a preliminary evaluation with 25 TensorFlow (Cai et al., 2017) tasks sourced from TF-coder (Wang et al., 2018) and online forums like StackOverflow. We setup the pre-processing module of Jigsaw similar to the offline evaluation, by creating a context bank of 25 prompts from documentation pages. We reuse the Variable Name module and do a _what-if_ analysis for argument and tree transformations manually. Table 8 shows the performance of Jigsaw on the TensorFlow dataset. As seen from the table, Codex alone is able to solve only 8 of the 25 tasks, variable transformation improves the performance to 15 tasks. We manually compare the code outputs to the expected output, to check if argument and tree transformations can be learnt. Based on this analysis, we find that Semantic Repair can potentially improve the performance to 19 tasks. We show some examples below. For the query \"Given a tensor in1, replace all instances of 1 with 0\", PTLM outputs the following: ``` tf.where(x=1,0,x) ``` The correct code for this query, synthesized by Jigsaw using variable transformation, is shown below: ``` tf.where(in1=1,0,in1) ``` For the query \"Given a tensor in1 and a tensor of indices ind, get the sum of elements present at indices in ind from tensor in1. \", the PTLM outputs the following incorrect code: ``` tf.gather(in1,ind) ``` The correct code, shown below, can be synthesized by Jigsaw with a learnt AST-to-AST transformation, if sufficient data points are collected from usage: ``` tf.reduce_sum(tf.gather(in1,ind)) ``` In summary, this shows that the proposed pre-processing and post-processing modules are useful, and can be generalized to other libraries and programming languages as well."
    },
    {
      "title": "6. Threats To Validity",
      "text": "Our data sets have been created by manually inspecting internet forums like StackOverflow. We tried to cover the common programming patterns in Pandas. However, they are not representative of all Pandas programs in the wild. We designed the PandasEval_S1 and PandasEval2_S2 datasets by collecting data from two sessions of a hackathon, as a proxy for the real-world setting, where large software teams are working on the same project with similar tasks, allowing Jigsaw to learn and improve over time. We varied the tasks between the two sessions, so as to simulate variants of tasks. However, the variations we introduced may not representative of variations of tasks in the real world. Our study had only 25 participants; evaluating whether the productivity of developers is enhanced in a statistically significant manner in a large scale deployment of Jigsaw is beyond the scope of this paper. When comparing Jigsaw to AutoPandas, Jigsaw takes as input both the natural language description in the StackOverflow posts and the I/O examples in the posts, while AutoPandas only takes the I/O examples as inputs. Hence, Jigsaw has more information about the tasks than AutoPandas. Jigsaw takes less than a minute per task and we use a timeout of three minutes for AutoPandas. Although higher timeouts might improve the performance of AutoPandas (10-15 minutes (Cai et al., 2017)), they are not compatible with the interactive user experience that we are aiming for. Whether AutoPandas solved a task correctly or not is determined by manual inspection and is susceptible to human errors."
    },
    {
      "title": "7. Related Work",
      "text": "The literature on using machine learning for program synthesis is vast (Wang et al., 2018; Wang et al., 2018; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019) and we restrict to works which are closest to Jigsaw (synthesizing code for large APIs using large models and multi-modal specifications). These works can be classified into the following categories: 1) designed for large APIs but do not use large models, 2) based purely on large models with no multimodal specification, and 3) multimodal synthesis for small APIs. Details follow: 1. The TDE (Wang et al., 2018) system for Java relies on rich type information (which is absent in Pandas) and fails to generate argument combinations that are absent from its corpus. AutoPandas (Cai et al., 2017) generates Pandas code exclusively from input-output (I/O) examples using a combination of GNNs, which predict function sequences, and enumerative search. TF-coder (Wang et al., 2019) uses both natural language descriptions and I/O examples to generate TensorFlow code. Both of \\begin{table} \\begin{tabular}{l c c c} \\hline \\hline & \\# Prompts & PandasEval1 & PandasEval2 \\\\ \\hline \\multirow{4}{*}{GPT-3} & 1 & 47.5 \\(\\pm\\) 1.8 & 34.9 \\(\\pm\\) 0.9 \\\\ & 4 & 47.1 \\(\\pm\\) 2.1 & 35.1 \\(\\pm\\) 0.7 \\\\ & 8 & 48.0 \\(\\pm\\) 2.5 & 32.9 \\(\\pm\\) 0.6 \\\\ \\hline \\multirow{4}{*}{Codex} & 1 & 62.3 \\(\\pm\\) 0.7 & 71.8 \\(\\pm\\) 0.5 \\\\ & 4 & 66.7 \\(\\pm\\) 0.7 & 72.2 \\(\\pm\\) 0.5 \\\\ \\cline{1-1} & 8 & 66.2 \\(\\pm\\) 1.2 & 72.4 \\(\\pm\\) 0.9 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 7. Ablation study: Performance of Jigsaw with different number of context prompts. \\begin{table} \\begin{tabular}{l c c} \\hline \\hline PTLM & Variable Name & Semantic Repair \\\\ \\hline 8/25 & 15/25 & 19/25 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 8. Preliminary results of Jigsaw with TensorFlow API. [MISSING_PAGE_FAIL:11] PA, USA, June 18-22, 2018_, Jeffrey S. Foster and Dan Grossman (Eds.). ACM, 420-435. * Gulwani (2016) Sumit Gulwani. 2016. Programming by examples. _Dependable Software Systems Engineering_ 45, 137 (2016), 3-15. * He et al. (2018) Yeye He, Xu Chu, Kris Ganjam, Yudian Zheng, Vivek R. Narasayya, and Surajit Chaudhuri. 2018. Transform-Data-by-Encoding (TIDE): An Extensible Search Engine for Data Transformations. _Proc. VLDB Endos_. 11, 110 (2018), 1165-1177. * May 3, 2018, Conference Track Proceedings_, OpenReview.net. * Lee et al. (2018) Woosuk Lee, Kihong Rodejev Alur, and Mayur Naik. 2018. Accelerating search-based program synthesis using learned probabilistic models. In _Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2018, Philadelphia, PA, USA, June 18-22, 2018_, Jeffrey S. Foster and Dan Grossman (Eds.). ACM, 436-449. * Liu et al. (2021) Pengfein Liu, Weixing Yuan, Jinlin Fu, Zhenghao Jiang, Hiroki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _arXiv preprint arXiv:2107.13586_ (2021). * Manshadi et al. (2013) Mehdi Hafezi Manshadi, Daniel Giddea, and James F. Allen. 2013. Integrating Programming by Example and Natural Language Programming. In _Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, July 14-18, 2013, Bellewe, Washington, USA, Marie desJardins and Michael L. Littman (Eds.). AAAI Press_. * Menon et al. (2013) Aditya Krishna Menon, Omer Tamur, Sumit Gulwani, Butler W. Lampson, and Adam Kalai. 2013. A Machine Learning Framework for Programming by Example. In _Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013 (JMLR Workshop and Conference Proceedings, Vol. 28)_. JMLR.org, 187-195. * Miltner et al. (2019) Anders Miltner, Sumit Gulwani, Vu Le, Alan Leung, Arjun Radhakrishna, Gustav Soares, Ashish Tiwari, and Abhishek Udupa. 2019. On the fly synthesis of edit suggestions. In _Object-Oriented Programming, Systems, Languages & Applications (OOPSLA)_. ACM. [https://www.microsoft.com/en-us/research/publication/on-the-fly](https://www.microsoft.com/en-us/research/publication/on-the-fly) synthesis-of-edit-suggestations/ * Paroce et al. (2021) The pandas development team. _pandas-dev/pandas-pandas-pandas_. [https://doi.org/10.5281/zenodo.3509134](https://doi.org/10.5281/zenodo.3509134) * Pastirovito et al. (2017) Emilio Pastirovito, Abdel-rahman Mohamed, Rishahah Singh, Lihong Li, Dengyong Zhou, and Pushner Joshi. 2017. Neuro-Symbolic Program Synthesis. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_, OpenReview.net. * Peace et al. (2021) H. Pearce, Balegh Ahmad, Benjamin Tam, Brendan Dolan-Gavitt, and R. Karri. 2021. An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions. _ArXiv_ abs/2108.09293 (2021). * Perez et al. (2021) Ethan Perez, Douve Kelch, and Yunghyunyu Cho. 2021. True Few-Shot Learning with Language Models. _ArXiv_ abs/2105.11447 (2021). * Polozov and Gulwani (2015) Oleksandr Polozov and Sumit Gulwani. 2015. Flashmeta: A framework for inductive program synthesis. In _Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications_. 170-126. * Radford et al. (2019) Ace Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019). * June 09 - 11, 2014_, Michael F. P. Ol'Shoyle and Keshav Pingali (Eds.). ACM, 419-428. * Raza et al. (2015) Mohammad Raza, Sumit Gulwani, and Natasa Mlic-Frayning. 2015. Compositional Program Synthesis from Natural Language and Examples. In _Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015_, Qing Yang and Michael J. Wooldridge (Eds.). AAAI Press, 792-800. * Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics. [https://arxiv.org/abs/1903.10084](https://arxiv.org/abs/1903.10084) * Rubin and Berant (2021) Ohad Rubin and Jonathan Berant. 2021. SmBoP: Semi-autoregressive Bottom-up Semantic Parsing. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021_, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Gotteel, Tammoy Chakraborty, and Ychao Zhou (Eds.). Association for Computational Linguistics, 311-324. * Salton and McGill (1986) Gerard Salton and Michael J McGill. 1986. Introduction to modern information retrieval (1986). * Shi et al. (2020) Kensen Shi, David Bieber, and Rishahah Singh. 2020. TF-Coder: Program Synthesis for Tensor Manipulations. _CoRR_ abs/2003.09040 (2020). * 27th International Conference, CAV 2015, San Francisco, CA, USA, July 18-24, 2015, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 2020)_, Daniel Kroening and Corina S. Pasareareanu (Eds.). Springer, 398-414. * Wang et al. (2020) Baihin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2020. SNt-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. In _Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, Dan Jurafsky, Joyce Chat, Natale Schluiter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 7567-7578. * Wu et al. (2020) Yifan Wu, Joseph M. Hellerstein, and Arvind Satyanyanyan. 2020. Bi- Bridging Code and Interactive Visualization in Computational Notbooks. In _USST '20: The 3rd Annual ACM Symposium on User Interface Software and Technology, Virtual Event, USA, October 20-23, 2020_, Shamsari T.I., Iokou, E. MacLean, Fanny Chevalier, and Stefanie Mueller (Eds.). ACM, 152-165. * Ye et al. (2019) Xi Ye, Qiaochu Chen, Xinyu Wang, Isil Dillig, and Greg Durrett. 2019. Sketch-Driven Regular Expression Generation from Natural Language and Examples. _CoRR_ abs/1908.05858 (2019). * November 4, 2018_, Ellen Riloff, David Chiang, Julia Hocheminer, and Jun'ichi Tsujii (Eds.). Association for Computational Linguistics, 3911-3921. * Zhao et al. (2021) Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate Before Use: Improving Few-shot Performance of Language Models. In _International Conference on Machine Learning_."
    }
  ]
}