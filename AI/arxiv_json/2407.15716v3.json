{
  "title": "Predicting System Crashes with Large Language Models",
  "authors": [
    "Priyanka Mudgal"
  ],
  "abstract": "\n As the dependence on computer systems expands across various domains, focusing on personal, industrial, and large-scale applications, there arises a compelling need to enhance their reliability to sustain business operations seamlessly and ensure optimal user satisfaction. Achieving enhanced reliability depends on predicting and preemptively addressing these failures presents a formidable challenge. However, the challenge of predicting failures and mitigate them in timely manner is a complex problem. Thus, it becomes important to adopt effective strategies for managing failures and minimizing their impact. System logs generated by these devices serve as valuable repositories of historical trends and past failures. The use of machine learning techniques for failure prediction has become commonplace, enabling the extraction of insights from past data to anticipate future behavior patterns. Recently, large language models have demonstrated remarkable capabilities in tasks including summarization, reasoning, and event prediction. Therefore, in this paper, we endeavor to investigate the potential of large language models in predicting system failures, leveraging insights learned from past failure behavior to inform reasoning and decision-making processes effectively. Our approach involves leveraging data from the Intel® Computing Improvement Program (ICIP) system crash logs to identify significant events and develop CrashEventLLM. This model, built upon a large language model framework, serves as our foundation for crash event prediction. Specifically, our model utilizes historical data to forecast future crash events, informed by expert annotations. Additionally, it goes beyond mere prediction, offering insights into potential causes for each crash event. This work provides the preliminary insights into prompt-based large language models for the log-based event prediction task. \n",
  "references": [
    {
      "id": null,
      "title": "Predicting System Crashes with Large Language Models",
      "authors": [
        "Priyanka Mudgal"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Windows internals, part 2",
      "authors": [
        "M Russinovich",
        "D Solomon",
        "A Ionescu"
      ],
      "year": "2002",
      "venue": "ser. Ch",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "2024 crowdstrike incident",
      "authors": [],
      "year": "",
      "venue": "2024 crowdstrike incident",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Automated it system failure prediction: A deep learning approach",
      "authors": [
        "K Zhang",
        "J Xu",
        "M R Min",
        "G Jiang",
        "K Pelechrinis",
        "H Zhang"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Big Data (Big Data",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Failure prediction in ibm bluegene/l event logs",
      "authors": [
        "Y Liang",
        "Y Zhang",
        "H Xiong",
        "R Sahoo"
      ],
      "year": "2007",
      "venue": "Seventh IEEE International Conference on Data Mining (ICDM 2007)",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Ensemble method for system failure detection using large-scale telemetry data",
      "authors": [
        "P Mudgal",
        "R H Wouhaybi"
      ],
      "year": "2024",
      "venue": "Ensemble method for system failure detection using large-scale telemetry data",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Language models are few-shot learners",
      "authors": [
        "T B Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D M Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "Language models are unsupervised multitask learners",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C L Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray",
        "J Schulman",
        "J Hilton",
        "F Kelton",
        "L Miller",
        "M Simens",
        "A Askell",
        "P Welinder",
        "P Christiano",
        "J Leike",
        "R Lowe"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Language models can improve event prediction by few-shot abductive reasoning",
      "authors": [
        "X Shi",
        "S Xue",
        "K Wang",
        "F Zhou",
        "J Y Zhang",
        "J Zhou",
        "C Tan",
        "H Mei"
      ],
      "year": "2023",
      "venue": "Language models can improve event prediction by few-shot abductive reasoning",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Few-shot training llms for projectspecific code-summarization",
      "authors": [
        "T Ahmed",
        "P Devanbu"
      ],
      "year": "2022",
      "venue": "Few-shot training llms for projectspecific code-summarization",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Fret: Functional reinforced transformer with bert for code summarization",
      "authors": [
        "R Wang",
        "H Zhang",
        "G Lu",
        "L Lyu",
        "C Lyu"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation",
      "authors": [
        "X Du",
        "M Liu",
        "K Wang",
        "H Wang",
        "J Liu",
        "Y Chen",
        "J Feng",
        "C Sha",
        "X Peng",
        "Y Lou"
      ],
      "year": "2023",
      "venue": "Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "An extensive study on pre-trained models for program understanding and generation",
      "authors": [
        "Z Zeng",
        "H Tan",
        "H Zhang",
        "J Li",
        "Y Zhang",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, ser. ISSTA 2022",
      "doi": "10.1145/3533767.3534390"
    },
    {
      "id": "b13",
      "title": "Synchromesh: Reliable code generation from pretrained language models",
      "authors": [
        "G Poesia",
        "O Polozov",
        "V Le",
        "A Tiwari",
        "G Soares",
        "C Meek",
        "S Gulwani"
      ],
      "year": "2022",
      "venue": "Synchromesh: Reliable code generation from pretrained language models",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Resolving crash bugs via large language models: An empirical study",
      "authors": [
        "X Du",
        "M Liu",
        "J Li",
        "H Wang",
        "X Peng",
        "Y Lou"
      ],
      "year": "2023",
      "venue": "Resolving crash bugs via large language models: An empirical study",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "An assessment of chatgpt on log data",
      "authors": [
        "P Mudgal",
        "R Wouhaybi"
      ],
      "year": "2024",
      "venue": "An assessment of chatgpt on log data",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Interpretable online log analysis using large language models with prompt strategies",
      "authors": [
        "Y Liu",
        "S Tao",
        "W Meng",
        "J Wang",
        "W Ma",
        "Y Zhao",
        "Y Chen",
        "H Yang",
        "Y Jiang",
        "X Chen"
      ],
      "year": "2024",
      "venue": "Interpretable online log analysis using large language models with prompt strategies",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Log parsing with prompt-based few-shot learning",
      "authors": [
        "V.-H Le",
        "H Zhang"
      ],
      "year": "2023",
      "venue": "Log parsing with prompt-based few-shot learning",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Lilac: Log parsing using llms with adaptive parsing cache",
      "authors": [
        "Z Jiang",
        "J Liu",
        "Z Chen",
        "Y Li",
        "J Huang",
        "Y Huo",
        "P He",
        "J Gu",
        "M R Lyu"
      ],
      "year": "2024",
      "venue": "Lilac: Log parsing using llms with adaptive parsing cache",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Raglog: Log anomaly detection using retrieval augmented generation",
      "authors": [
        "J Pan",
        "S L Wong",
        "Y Yuan"
      ],
      "year": "2023",
      "venue": "Raglog: Log anomaly detection using retrieval augmented generation",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Llmparser: An exploratory study on using large language models for log parsing",
      "authors": [
        "Z Ma",
        "A R Chen",
        "D J Kim",
        "T.-H Chen",
        "S Wang"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ser. ICSE '24",
      "doi": "10.1145/3597503.3639150"
    },
    {
      "id": "b21",
      "title": "Large language models for failure mode classification: An investigation",
      "authors": [
        "M Stewart",
        "M Hodkiewicz",
        "S Li"
      ],
      "year": "2023",
      "venue": "Large language models for failure mode classification: An investigation",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Telemetry for reliability",
      "authors": [
        "R Kwasnick",
        "P Karayacoubian",
        "P Polasam",
        "A Wang",
        "A Biswas",
        "J Tayeb",
        "R Mattani",
        "B Arbab"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Reliability Physics Symposium (IRPS)",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "B Ichter",
        "F Xia",
        "E Chi",
        "Q Le",
        "D Zhou"
      ],
      "year": "2023",
      "venue": "Chain-of-thought prompting elicits reasoning in large language models",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "",
      "authors": [
        "J Openai",
        "S Achiam",
        "S Adler",
        "L Agarwal",
        "I Ahmad",
        "F L Akkaya",
        "D Aleman",
        "J Almeida",
        "S Altenschmidt",
        "S Altman",
        "R Anadkat",
        "I Avila",
        "S Babuschkin",
        "V Balaji",
        "P Balcom",
        "H Baltescu",
        "M Bao",
        "J Bavarian",
        "I Belgum",
        "J Bello",
        "G Berdine",
        "C Bernadett-Shapiro",
        "L Berner",
        "O Bogdonoff",
        "M Boiko",
        "A.-L Boyd",
        "G Brakman",
        "T Brockman",
        "M Brooks",
        "K Brundage",
        "T Button",
        "R Cai",
        "A Campbell",
        "B Cann",
        "C Carey",
        "R Carlson",
        "B Carmichael",
        "C Chan",
        "F Chang",
        "D Chantzis",
        "S Chen",
        "R Chen",
        "J Chen",
        "M Chen",
        "B Chen",
        "C Chess",
        "C Cho",
        "H W Chu",
        "D Chung",
        "J Cummings",
        "Y Currier",
        "C Dai",
        "T Decareaux",
        "N Degry",
        "D Deutsch",
        "A Deville",
        "D Dhar",
        "S Dohan",
        "S Dowling",
        "A Dunning",
        "A Ecoffet",
        "T Eleti",
        "D Eloundou",
        "L Farhi",
        "N Fedus",
        "S P Felix",
        "J Fishman",
        "I Forte",
        "L Fulford",
        "E Gao",
        "C Georges",
        "V Gibson",
        "T Goel",
        "G Gogineni",
        "R Goh",
        "J Gontijo-Lopes",
        "M Gordon",
        "S Grafstein",
        "R Gray",
        "J Greene",
        "S S Gross",
        "Y Gu",
        "C Guo",
        "J Hallacy",
        "J Han",
        "Y Harris",
        "M He",
        "J Heaton",
        "C Heidecke",
        "A Hesse",
        "W Hickey",
        "P Hickey",
        "B Hoeschele",
        "K Houghton",
        "S Hsu",
        "X Hu",
        "J Hu",
        "S Huizinga",
        "S Jain",
        "J Jain",
        "A Jang",
        "R Jiang",
        "H Jiang",
        "D Jin",
        "S Jin",
        "B Jomoto",
        "H Jonn",
        "T Jun",
        "Łukasz Kaftan",
        "A Kaiser",
        "I Kamali",
        "N S Kanitscheider",
        "T Keskar",
        "L Khan",
        "J W Kilpatrick",
        "C Kim",
        "Y Kim",
        "J H Kim",
        "J Kirchner",
        "M Kiros",
        "D Knight",
        "Łukasz Kokotajlo",
        "A Kondraciuk",
        "A Kondrich",
        "K Konstantinidis",
        "G Kosic",
        "V Krueger",
        "M Kuo",
        "I Lampe",
        "T Lan",
        "J Lee",
        "J Leike",
        "D Leung",
        "C M Levy",
        "R Li",
        "M Lim",
        "S Lin",
        "M Lin",
        "T Litwin",
        "R Lopez",
        "P Lowe",
        "A Lue",
        "K Makanju",
        "S Malfacini",
        "T Manning",
        "Y Markov",
        "B Markovski",
        "K Martin",
        "A Mayer",
        "B Mayne",
        "S M Mcgrew",
        "C Mckinney",
        "P Mcleavey",
        "J Mcmillan",
        "D Mcneil",
        "A Medina",
        "J Mehta",
        "L Menick",
        "A Metz",
        "P Mishchenko",
        "V Mishkin",
        "E Monaco",
        "D Morikawa",
        "T Mossing",
        "M Mu",
        "O Murati",
        "D Murk",
        "A Mély",
        "R Nair",
        "R Nakano",
        "A Nayak",
        "R Neelakantan",
        "H Ngo",
        "L Noh",
        "C Ouyang",
        "J O'keefe",
        "A Pachocki",
        "J Paino",
        "A Palermo",
        "G Pantuliano",
        "J Parascandolo",
        "E Parish",
        "A Parparita",
        "M Passos",
        "A Pavlov",
        "A Peng",
        "F Perelman",
        "G Sastry",
        "H Schmidt",
        "D Schnurr",
        "J Schulman",
        "D Selsam",
        "K Sheppard",
        "T Sherbakov",
        "J Shieh",
        "S Shoker",
        "P Shyam",
        "S Sidor",
        "E Sigler",
        "M Simens",
        "J Sitkin",
        "K Slama",
        "I Sohl",
        "B Sokolowsky",
        "Y Song",
        "N Staudacher",
        "F P Such",
        "N Summers",
        "I Sutskever",
        "J Tang",
        "N Tezak",
        "M B Thompson",
        "P Tillet",
        "A Tootoonchian",
        "E Tseng",
        "P Tuggle",
        "N Turley",
        "J Tworek",
        "J F C Uribe",
        "A Vallone",
        "A Vijayvergiya",
        "C Voss",
        "C Wainwright",
        "J J Wang",
        "A Wang",
        "B Wang",
        "J Ward",
        "J Wei",
        "C Weinmann",
        "A Welihinda",
        "P Welinder",
        "J Weng",
        "L Weng",
        "M Wiethoff",
        "D Willner",
        "C Winter",
        "S Wolrich",
        "H Wong",
        "L Workman",
        "S Wu",
        "J Wu",
        "M Wu",
        "K Xiao",
        "T Xu",
        "S Yoo",
        "K Yu",
        "Q Yuan",
        "W Zaremba",
        "R Zellers",
        "C Zhang",
        "M Zhang",
        "S Zhao",
        "T Zheng",
        "J Zhuang",
        "W Zhuk",
        "B Zoph"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Large language models are zero-shot time series forecasters",
      "authors": [
        "N Gruver",
        "M Finzi",
        "S Qiu",
        "A G Wilson"
      ],
      "year": "2023",
      "venue": "Large language models are zero-shot time series forecasters",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "authors": [
        "P Lewis",
        "E Perez",
        "A Piktus",
        "F Petroni",
        "V Karpukhin",
        "N Goyal",
        "H Küttler",
        "M Lewis",
        "W Yih",
        "T Rocktäschel",
        "S Riedel",
        "D Kiela"
      ],
      "year": "2021",
      "venue": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Simulation of nonhomogeneous poisson processes by thinning",
      "authors": [
        "P A W Lewis",
        "G S Shedler"
      ],
      "year": "1979",
      "venue": "Naval Research Logistics Quarterly",
      "doi": "10.1002/nav.3800260304"
    },
    {
      "id": "b28",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "N Bashlykov",
        "S Batra",
        "P Bhargava",
        "S Bhosale",
        "D Bikel",
        "L Blecher",
        "C C Ferrer",
        "M Chen",
        "G Cucurull",
        "D Esiobu",
        "J Fernandes",
        "J Fu",
        "W Fu",
        "B Fuller",
        "C Gao",
        "V Goswami",
        "N Goyal",
        "A Hartshorn",
        "S Hosseini",
        "R Hou",
        "H Inan",
        "M Kardas",
        "V Kerkez",
        "M Khabsa",
        "I Kloumann",
        "A Korenev",
        "P S Koura",
        "M.-A Lachaux",
        "T Lavril",
        "J Lee",
        "D Liskovich",
        "Y Lu",
        "Y Mao",
        "X Martinet",
        "T Mihaylov",
        "P Mishra",
        "I Molybog",
        "Y Nie",
        "A Poulton",
        "J Reizenstein",
        "R Rungta",
        "K Saladi",
        "A Schelten",
        "R Silva",
        "E M Smith",
        "R Subramanian",
        "X E Tan",
        "B Tang",
        "R Taylor",
        "A Williams",
        "J X Kuan",
        "P Xu",
        "Z Yan",
        "I Zarov",
        "Y Zhang",
        "A Fan",
        "M Kambadur",
        "S Narang",
        "A Rodriguez",
        "R Stojnic",
        "S Edunov",
        "T Scialom"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "ROUGE: A package for automatic evaluation of summaries",
      "authors": [
        "C.-Y Lin"
      ],
      "year": "2004",
      "venue": "Text Summarization Branches Out",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Rouge 2.0: Updated and improved measures for evaluation of summarization tasks",
      "authors": [
        "K Ganesan"
      ],
      "year": "2018",
      "venue": "Rouge 2.0: Updated and improved measures for evaluation of summarization tasks",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Predicting System Crashes With Large Language Models",
      "text": "1st Priyanka Mudgal* _Intel Corporation, USA_ priyanka.mudgal@intel.com 2nd Bijan Arbab _Intel Corporation, USA_ bijan.arbab@intel.com 3rd Swaathi Sampath Kumar _Intel Corporation, USA_ swaathi.sampath.kumar@intel.com"
    },
    {
      "title": "Abstract",
      "text": "As the dependence on computer systems expands across various domains, focusing on personal, industrial, and large-scale applications, there arises a compelling need to enhance their reliability to sustain business operations seamlessly and ensure optimal user satisfaction. Achieving enhanced reliability depends on predicting and preemptively addressing these failures presents a formidable challenge. However, the challenge of predicting failures and mitigate them in timely manner is a complex problem. Thus, it becomes important to adopt effective strategies for managing failures and minimizing their impact. System logs generated by these devices serve as valuable repositories of historical trends and past failures. The use of machine learning techniques for failure prediction has become commonplace, enabling the extraction of insights from past data to anticipate future behavior patterns. Recently, large language models have demonstrated remarkable capabilities in tasks including summarization, reasoning, and event prediction. Therefore, in this paper, we endeavor to investigate the potential of large language models in predicting system failures, leveraging insights learned from past failure behavior to inform reasoning and decision-making processes effectively. Our approach involves leveraging data from the Intel(r) Computing Improvement Program (ICIP) system crash logs to identify significant events and develop CrashEventLLM. This model, built upon a large language model framework, serves as our foundation for crash event prediction. Specifically, our model utilizes historical data to forecast future crash events, informed by expert annotations. Additionally, it goes beyond mere prediction, offering insights into potential causes for each crash event. This work provides the preliminary insights into prompt-based large language models for the log-based event prediction task. log data, log analysis, large language model, crash prediction using LLM, deep learning, machine learning. *Previously at Intel while this work was conducted."
    },
    {
      "title": "I Introduction",
      "text": "Recent advancements in computing technology have caused a significant rise in our dependence on computing systems across a wide range of vital services. From banking operations, utility management, airlines, to information technology infrastructure, a plethora of everyday functions now rely on a diverse array of computing systems. The critical importance of minimizing system failures or downtime cannot be overstated, as it is often essential for societal function. System failures or crashes typically originate from system errors, where each critical error has the capacity to prompt system shutdowns or reboots in an effort to resolve the issues [1]. Such errors can lead to considerable financial losses and inflict significant damage on crucial information technology (IT) infrastructure. Within the Windows environment, users often encounter the disruptive \"blue screen of death\" (BSOD), hampering device functionality [1]. Recent such example is from crowdstrike incident [2], where roughly 8.5 million systems crashed and were unable to properly restart in leading to the largest outage in the history of information technology. Typically, in the event of a system crash in the Windows operating system (OS), a crash log is generated to aid in the analysis of the crash triggers. Critical crashes are identified and logged as event 41. This event signifies that some unforeseen activity hindered the proper subdown of Windows. Such shutdowns may result from power supply interruptions or stop errors. System crash logs serve as a comprehensive record of an IT system's operational activities and events, offering detailed insights. These logs are important in two folds: fault detection and failure prediction. Fault detection focuses on swiftly identifying the indicators of the critical failures as they arise, often employing anomaly detection methods. On the contrary, failure prediction adopts a proactive approach, aiming to issue early alerts regarding potential failures. Several research work addressed the failure prediction problem based on machine learning and deep learning techniques [3, 4]. A related study utilized telemetry data for system failure prediction through a concurrent work [5]. Recent growth of large language models (LLMs) [6, 7, 8] in the area of real world event-prediction and text-reasoning tasks have demonstrated interesting results [9]. In particular, due to being pre-trained on text, logs, and code corpora, LLMs have also shown promising effectiveness in software engineering tasks, such as software comprehension [10, 11], code generation and bug resolving [12, 13, 14, 15], log analysis [16, 17, 18, 19, 20, 21], and failure mode classification [22]. Therefore, it becomes interesting to investigate their capability in system crash prediction. In this paper, we investigate their capabilities in predicting and reasoning about the system crash events given the past. Specifically, we model system's sequences of crash timestamps and predict their future crashes along with the possible cause of failure. Our hypothesis is that LLMs are potentially useful for advancing solutions to this problem because event sequences are often accompanied with rich text information which LLMs can handle. This paper introduces CrashEventLLM, a novel framework designed to integrate a large language model (LLM) for event prediction. The framework, depicted in Fig. 1,is based on the utilization of crash log data obtained from the Intel(r) Computing Improvement Program (ICIP) [23], focusing on a diverse range of client devices and containing crucial crash information such as timestamps and bugcheckcodes. The guid in Fig. 1 have been altered to safeguard the privacy of the users of those systems. The framework organizes raw crash logs into sequences by leveraging a pretrained LLM, which are then utilized to fine-tune a second LLM for predicting future failure occurrences within a specified time window. This predictive model learns to perform abductive reasoning, informed by historical crash events within a defined time frame, to generate potential causes for each predicted crash. The generality of our prediction modeling framework allows for the incorporation of various large language models. Through our experiments with different model sizes, we demonstrate the competency of LLMs in crash event prediction, regardless of their scale, when fine-tuned for this specific task."
    },
    {
      "title": "Ii **Method**",
      "text": "In this section, we present an overview of both the dataset utilized and the models subjected to evaluation. Fig. 1 illustrates the framework of our architecture."
    },
    {
      "title": "**Dataset**",
      "text": "The dataset comprises information gathered from systems whose users have opted to participate in data collection and analytics (DCA) using ICIP telemetry software [23]. ICIP functions as a telemetry software tool for monitoring product health, provided to users when they visit www.intel.com for driver downloads. Primarily, this data includes client PCs featuring various generations of central processing units (CPUs). DCA retrieves data from machines exclusively during their operational phases, known as the S0 state, and collects it at regular intervals, typically every 5 seconds. On-device aggregation of data occurs every 24 hours, with the aggregated data uploaded to the datastore when the system is active and connected to the network. Data is accessible only for the days when the machine is active, specifically in the S0 state, for at least a few seconds."
    },
    {
      "title": "**Large Language Models**",
      "text": "Language models acquire knowledge through text comprehension. In recent years, LLMs trained on vast amounts of internet text have exhibited remarkable performance across a spectrum of challenging tasks, including arithmetic reasoning and multi-turn dialogue [24, 25]. Recent research [26] has further highlighted the exceptional Fig. 1: Our framework utilizes advanced large language models to forecast forthcoming system crashes and their causes through a two-stage process. In the initial stage, an event sequence generation model sifts through raw event logs, identifying system crashes while capturing essential details such as timestamp, bug check code, and parameters. Following this, custom prompts are formulated to refine another large language model. This model undergoes extensive fine-tuning, scrutinizing historical events to detect patterns. Subsequently, it leverages this acquired insight to predict both the occurrence of the next crash event and its underlying cause. performance of LLMs on time-series or sequential data, attributed to their inherent capacity to represent multi-modal distributions, which aligns well with salient features in many time series, such as recurring seasonal trends. Additionally, LLMs have demonstrated proficiency in log analytics tasks such as anomaly detection and log parsing [17, 19, 20, 21]. However, as noted by Mudgal et al. [16], event prediction using LLMs with zero-shot learning has exhibited suboptimal performance. Therefore, an intriguing area for exploration is the evaluation of large language models for system crash prediction employing few-shot learning techniques. Given that logs comprise complex and critical information often requiring preprocessing, we devise an event sequence generation model based on retrieval augmented generation (RAG) architecture [27] to extract crash events and generate event sequences featuring the timing and causes of previously recorded crashes. Subsequently, we construct prompts utilizing the generated crash sequence data to feed into the prediction model, which then forecasts future crashes. Both models are elucidated in Sections II-B1 and II-B2."
    },
    {
      "title": "Ii-B1 **Event Sequence Generation Model**",
      "text": "The objective of this module is to produce sequences of crash events in the form (\\(t_{1}\\), \\(k_{1}\\)), (\\(t_{2}\\), \\(k_{2}\\)), \\(\\ldots\\), where 0 \\(<\\) t1 \\(<\\) t2 \\(<\\)... denote the times of crash occurrences in sequential manner, and each \\(K_{i}\\)\\(\\in\\) K represents a discrete crash type. Our aim is to create linearized sequences encapsulating crash event concepts. Initially, we present the raw log files to a LLM and guide it through a series of prompts to identify unique systems within the logs and subsequently discern the crashes associated with them by using langchain. Formally, given a raw crash log file \\(L\\), we task the LLM with identifying \\(N\\) unique systems and generating a sequence of the past \\(M\\) crash events arranged in chronological order as a pair of time and reason of crash (\\(t_{i}\\), \\(k_{i}\\)). This process facilitates the extraction of structured information from the raw crash logs, encompassing system details, crash timestamps, and reasons for the crashes. Following extraction, the information is partitioned into distinct segments using a fixed-size time window, resulting in two types of sequences: a time sequence detailing the timestamps of past crash events while excluding irrelevant content, and a crash cause sequence providing a high-level abstraction of sequential crash patterns to support pattern-based event prediction."
    },
    {
      "title": "Ii-B2 **Prediction Model**",
      "text": "The goal of this module is to predict the next event for a given history of events \\(H_{i}\\) = (\\(t_{1}\\), \\(k_{1}\\)), (\\(t_{2}\\), \\(k_{2}\\)), \\(\\cdots\\), (\\(t_{i-1}\\), \\(k_{i-1}\\)). Precisely, it consists of two subtasks: the first is to predict the time \\(t_{i}\\) of the next event; the second is to predict the type \\(k_{i}\\) of the next event with the knowledge of its time \\(t_{i}\\). The standard approach is to build a probabilistic model over the sequences. Such models typically define an intensity function \\(\\lambda_{k}\\): the intensity value \\(\\lambda_{k}(t)\\) is the instantaneous rate that an event of type \\(k\\) occurs at time \\(t\\). Given the function \\(\\lambda_{k}\\), one could obtain the minimum Bayes risk (MBR) prediction of the next event given the history [28]. Specifically, we start with a set of past crash events \\(E_{t}\\) for a randomly selected system \\(X\\) in the form - \"When will the next crash happen on system \\(X\\)?\". Based on the model prediction, we append additional question to predict the cause of crash given the past crash causes \\(E_{c}\\) in the form - \"What will be the predicted crash cause? This LLM fundamentally utilizes the sequence of crash events that was produced by event sequence generation model to predict the occurrence of the next crash. Overall, as shown in the architecture diagram Fig. 1, our design serves dual purposes: (i) preprocesses the logs to extract the information of crash events (ii) leverages in-context learning of the past crashes and predicts the future crashes with the possible cause."
    },
    {
      "title": "**Experimental Setup**",
      "text": "We conduct experiments utilizing two LLMs: Llama-2, possessing 7 billion parameters, and another version of it with 13 billion parameters, as described by Touvron et al. [29], referred to as Llama2-7B and Llama2-13B, respectively. For our dataset, we employed 10-shot prompts, where each \"shot\" consists of a demonstration comprising an effect event followed by one or more expert-annotated cause events. Both models underwent fine-tuning on an annotated dataset comprising 100 (observation, label) pairs and were subsequently validated on a 40-pair validation set."
    },
    {
      "title": "Iii **Evaluation Results**",
      "text": ""
    },
    {
      "title": "**Results**",
      "text": "Our main results are demonstrated in Fig. 2. We evaluate our results in three categories: time prediction, cause prediction, and full prediction. As the output of our prediction model is in natural language as a sentence with the predicted time and cause of crash, we postprocess the output to extract those information pieces and compute the ROUGE scores [30, 31]. ROUGE compares n-gram overlap of words on a surface level. We chose ROUGE-1 and ROUGE-L as they are the most commonly annotated ROUGE variants used for evaluating text generation. In ROUGE-1 and ROUGE-L, we show the evaluation results for precision, recall, and F1 for all three categories. The fine-tuned model predicted the output in a defined format for full validation set."
    },
    {
      "title": "**Analysis**",
      "text": "The Fig. 2 displays the outcomes obtained from our framework's Llama-7B and Llama-13B versions. Notably, the performance of the Llama-7B version rivals with Llama-13B version in both time prediction and full prediction tasks. However, the Llama-7B model surpasses the Llama-13B model in crash cause prediction. It's noteworthy that despite the significant parameter difference between Llama-7B and Llama-13B, the former outperforms the latter. To get the deep insights into this observation, it would be valuable to assess our approach on a larger validation set, given the relatively small size of our current validation dataset. Furthermore, we acknowledge the potential for enhancing our result evaluation through improved postprocessing techniques. For instance, previous studies have indicated that removing stopwords from generated output can lead to improved ROUGE scores [31]. Therefore, we intend to explore such techniques for refining our evaluation processes. The formulation of prompts holds substantial impact over the generated text's quality. Despite this, our methodology involved crafting prompt templates without preliminary experimentation or customization tailored to our dataset. Instead, we conducted post-analysis by assessing various templates on a small subset of our validation data. Surprisingly, we noted minimal discrepancies in results, provided the task description was clearly outlined, which was also highlighted in previous research work [9]."
    },
    {
      "title": "Iv **Limitations And Future Directions**",
      "text": "Our framework utilizes LLMs like Llama-7B and Llama-13B for generating event sequences and predicting crash events. However, it inherits potential limitations of these models, such as issues related to hallucination and biased content. Consequently, there's a risk of generating irrelevant, inaccurate, or misleading crash events, thereby compromising our framework's overall performance. Moreover, our dataset comprises crash events from heterogeneous systems with diverse characteristics, potentially leading to distinct crash patterns. Due to the small size of our dataset, LLMs might struggle to learn effectively, resulting in poor ROUGE scores. For future research in system crash prediction, we propose three directions: First, fine-tuning more advanced LLMs with specific data and conducting large-scale studies could unlock new arenas for system crash prediction using crash logs and LLMs. Second, expanding the dataset size may enhance performance. Specifically, Mudgal et al. [5] utilized system telemetry data to detect system failures. Similarly, integrating telemetry data including system characteristics and software updates with system crash information could enhance the identification of correlations between telemetry data and system crashes, leading to more accurate predictions of system failures or crashes. Third, while we evaluated predicted outputs using ROUGE scores in this paper, advancements in LLMs warrant exploring additional performance metrics for evaluation purposes."
    },
    {
      "title": "V **Conclusion**",
      "text": "In this paper, we investigate the potential of LLMs in system crash prediction by using the system crash logs. We introduce the CrashEventLLM framework designed to leverage LLMs for crash event sequence generation and crash event prediction. Through experiments and model fine-tuning, we demonstrate the competency of LLMs in predicting crash events, offering insights into potential causes. The versatility of our prediction modeling framework allows for the integration of various LLMs, highlighting the adaptability of LLMs in addressing specific tasks like crash event prediction. The implications of our findings for future research in crash event sequence generation and crash event predictions are considerable, suggesting areas for potentially impactful innovations with further exploration in this field."
    },
    {
      "title": "Vi **Acknowledgement**",
      "text": "We gratefully acknowledge the contributions of our colleague Joshua Boelter."
    },
    {
      "title": "References",
      "text": "* [1] M. Russinovich, D. Solomon, and A. Ionescu, \"Windows internals, part 2,\" ser. Ch. 14. Redmond: Microsoft Press, 2002. * [2] \"2024 crowdstrike incident.\" [https://en.wikipedia.org/wiki/2024_Crowdstike_incident](https://en.wikipedia.org/wiki/2024_Crowdstike_incident). * [3] K. Zhang, J. Xu, M. R. Min, G. Jiang, K. Pelechrinis, and H. Zhang, \"Automated it system failure prediction: A deep learning approach,\" in _2016 IEEE International Conference on Big Data (Big Data)_, 2016, pp. 1291-1300. * [4] Y. Liang, Y. Zhang, H. Xiong, and R. Sahoo, \"Failure prediction in ibm bluegene/l event logs,\" in _Seventh IEEE International Conference on Data Mining (ICDM 2007)_, 2007, pp. 583-588. * [5] P. Mudgal and R. H. Wouhoughi, \"Ensemble method for system failure detection using large-scale telemetry data,\" 2024. [Online]. Available: [https://arxiv.org/abs/2407.00048](https://arxiv.org/abs/2407.00048) * [6] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henigham, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, \"Language models are few-shot learners,\" 2020. * [7] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \"Language models are unsupervised multitask learners,\" 2019. [Online]. Available: [https://api.semanticscholar.org/CorpusID:160025533](https://api.semanticscholar.org/CorpusID:160025533) Fig. 2: Prediction performance of Llama2-7B and Llama2-13B. The left figure shows the ROUGE scores for crash time prediction, the middle figure for crash cause prediction, and the right figure for full prediction. While both LLMs perform comparably in time and full prediction, surprisingly, Llama2-7B demonstrates superior performance in crash cause prediction. * [12] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe, \"Training language models to follow instructions with human feedback,\" 2022. * [13] X. Shi, S. Xue, K. Wang, P. Zhou, J. Y. Zhang, J. Zhou, C. Tan, and H. Mei, \"Language models can improve event prediction by few-shot abductive reasoning,\" 2023. * [14] T. Ahmed and P. Devanbu, \"Few-shot training llms for project-specific code-summarization,\" 2022. * [15] R. Wang, H. Zhang, G. Lu, L. Lyu, and C. Lyu, \"Fret: Functional reinforced transformer with Bert for code summarization,\" _IEEE Access_, vol. 8, pp. 135 591-135 604, 2020. * [16] X. Du, M. Liu, K. Wang, H. Wang, J. Liu, Y. Chen, J. Feng, C. Sha, X. Peng, and Y. Lou, \"Classesval: A manually-crafted benchmark for evaluating llms on class-level code generation,\" 2023. * [17] Z. Zeng, H. Tan, H. Zhang, J. Li, Y. Zhang, and L. Zhang, \"An extensive study on pre-trained models for program understanding and generation,\" in _Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis_, ser. ISSTA 2022. New York, NY, USA: Association for Computing Machinery, 2022, pp. 39-51. [Online]. Available: [https://doi.org/10.1145/3533767.3534399](https://doi.org/10.1145/3533767.3534399) * [18] G. Poesia, O. Polozov, V. Le, A. Tiwari, G. Soares, C. Meek, and S. Gulwani, \"Synchronous: Reliable code generation from pre-trained language models,\" 2022. * [19] X. Du, M. Liu, J. Li, H. Wang, X. Peng, and Y. Lou, \"Resolving crash bugs via large language models: An empirical study,\" 2023. * [20] P. Mudgal and R. Wouhsybi, \"An assessment of chapter on log data,\" in _AI-generated Content_, F. Zhao and D. Miao, Eds. Singapore: Springer Nature Singapore, 2024, pp. 148-169. * [21] Y. Liu, S. Tao, W. Meng, J. Wang, W. Ma, Y. Zhao, Y. Chen, H. Yang, Y. Jiang, and X. Chen, \"Interpretable online log analysis using large language models with prompt strategies,\" 2024. * [22] V.-H. Le and H. Zhang, \"Log parsing with prompt-based few-shot learning,\" 2023. * [23] Z. Jiang, J. Liu, Z. Chen, Y. Li, J. Huang, Y. Huo, P. He, J. Gu, and M. R. Lyu, \"Lilac: Log parsing using lms with adaptive parsing cache,\" 2024. * [24] J. Pan, S. L. Wong, and Y. Yuan, \"Raglog: Log anomaly detection using retrieval augmented generation,\" 2023. * [25] Z. Ma, A. R. Chen, D. J. Kim, T.-H. Chen, and S. Wang, \"LImbParser: An exploratory study on using large language models for log parsing,\" in _Proceedings of the IEEE/ACM 46th International Conference on Software Engineering_, ser. ICSE '24. ACM, Apr. 2024. [Online]. Available: [http://dx.doi.org/10.1145/3597503.3639150](http://dx.doi.org/10.1145/3597503.3639150) * [26] M. Stewart, M. Hodkiewicz, and S. Li, \"Large language models for failure mode classification: An investigation,\" 2023. * [27] R. Kwasnick, F. Karayacobian, P. Polasman, A. Wang, A. Biswas, J. Tayeb, R. Matani, and B. Arbab, \"Telemetry for reliability,\" in _2017 IEEE International Reliability Physics Symposium (IRPS)_, 2017, pp. C6-2.1-6.2-6. * [28] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou, \"Chain-of-thought prompting elicits reasoning in large language models,\" 2023. * [29] OpenAI, J. Achim, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji, V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernandet-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Barkman, G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan, C. Chang, C. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville, A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffe, A. Eletti, T. Eloundou, P. Farhi, L. Fedus, N. Felis, S. P. Fishman, J. Forte, I. Fulford, L. Gao, E. Georges, C. Gibson, V. Goel, T. Gogineni, G. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray, R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Harris, Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey, P. Hoeschle, B. Houghton, K. Hsu, S. Hu, X. Hu, J. Huizinga, S. Jain, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto, B. Jonn, H. Jun, T. Kafafran, Lukasz Kaiser, A. Kamali, I. Kainticheleder, N. S. Keskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim, Y. Kim, J. H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Lukasz Kondarciuk, A. Konndrich, A. Konstantinidis, K. Koise, G. Krueger, V. Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M. Li, R. Lim, M. Lin, S. Lin, M. Litvin, T. Lopez, R. Lowe, P. Luo, A. Makanju, K. Malfaichi, S. Manning, T. Markov, V. Markovski, B. Martin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney, C. McLeavey, P. McMillan, J. McNeil, D. Medina, A. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin, V. Monaco, E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. Mely, A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh, L. Ouyang, C. O'Keef, J. Pachocki, A. Paino, J. Palermo, A. Panttu-Iiano, G. Parascandolo, J. Parish, E. Paraprina, A. Passos, M. Pavlov, A. Peng, A. Perelman, F. de Avila Belbute Peres, M. Petrov, H. P. de Oliveira Pinto, Michael, Pokorny, M. Pokrass, V. H. Pong, T. Powell, A. Power, B. Power, E. Proch, R. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli, T. Sanders, S. Santurkar, G. Sastry, H. Schmidt, D. Schnururr, J. Schulman, D. Selsam, K. Sheppard, T. Sherbakov, J. Shi, S. Shoker, P. Shyam, S. Sidor, E. Sigler, M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky, Y. Song, N. Staudacher, F. P. Such, N. Summers, I. Sutskever, J. Tang, N. Tezak, M. B. Thompson, P. Tille, A. Tootoonchian, E. Tseng, P. Tuggle, N. Turley, J. Tworek, J. F. C. Uribe, A. Vallone, A. Vijayveriya, C. Voss, C. Wainwright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei, C. Weinmann, A. Welihunda, P. Welinder, J. Weng, L. Wong, L. Wong, M. Wieffhoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang, S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, and B. Zoph, \"Gpt-4 technical report,\" 2024. * [29] N. Gruver, M. Finzi, S. Qiu, and A. G. Wilson, \"Large language models are zero-shot time series foreacasters,\" 2023. * [30] P. Lewis, E. Perez, A. Pitkus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. tau Yih, T. Rocktaschel, S. Riedel, and D. Kiela, \"Retrieval-augmented generation for knowledge-intensive nlp tasks,\" 2021. * [31] P. A. W. Lewis and G. S. Shedler, \"Simulation of nonhomogeneous poisson processes by thinning,\" _Naval Research Logistics Quarterly_, vol. 26, no. 3, pp. 403-413, 1979. [Online]. Available: [https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800260304](https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800260304) * [32] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almanahiari, Y. Babaei, N. Bashbyov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Bucher, C. C. Ferrer, M. Chen, G. Cucurull, D. Eisloub, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerke, M. Khabas, I. Kloumann, A. Korneev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Rezienstein, R. Rutong, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, \"Llama 2: Open foundation and fine-tuned chat models,\" 2023. * [33] C.-Y. Lin, \"ROUGE: A package for automatic evaluation of summaries,\" in _Text Summarization Branches Out_. Barcelona, Spain: Association for Computational Linguistics, Jul. 2004, pp. 74-81. [Online]. Available: [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013) * [34] K. Ganesan, \"Rouge 2.0: Updated and improved measures for evaluation of summarization tasks,\" 2018."
    }
  ]
}