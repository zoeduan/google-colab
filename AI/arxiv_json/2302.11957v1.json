{
  "title": "Sentence Simplification via Large Language Models",
  "authors": [
    "Yutao Feng",
    "Jipeng Qiang",
    "Yun Li",
    "Yunhao Yuan",
    "Yi Zhu"
  ],
  "abstract": "\n Sentence Simplification aims to rephrase complex sentences into simpler sentences while retaining original meaning. Large Language models (LLMs) have demonstrated the ability to perform a variety of natural language processing tasks. However, it is not yet known whether LLMs can be served as a high-quality sentence simplification system. In this work, we empirically analyze the zero-/few-shot learning ability of LLMs by evaluating them on a number of benchmark test sets. Experimental results show LLMs outperform state-of-the-art sentence simplification methods, and are judged to be on a par with human annotators. \n",
  "references": [
    {
      "id": null,
      "title": "Sentence Simplification via Large Language Models",
      "authors": [
        "Yutao Feng",
        "Jipeng Qiang",
        "Yun Li",
        "Yunhao Yuan",
        "Yi Zhu"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "ASSET: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations",
      "authors": [
        "Fernando Alva-Manchego",
        "Louis Martin",
        "Antoine Bordes",
        "Carolina Scarton",
        "Benoît Sagot",
        "Lucia Specia"
      ],
      "year": "2002",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2022",
      "venue": "Scaling language modeling with pathways",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing",
      "authors": [
        "Yue Dong",
        "Zichao Li",
        "Mehdi Rezagholizadeh",
        "Jackie Chi",
        "Kit Cheung"
      ],
      "year": "2003",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "An evaluation of syntactic simplification rules for people with autism",
      "authors": [
        "Richard Evans",
        "Constantin Orȃsan",
        "Iustin Dornescu"
      ],
      "year": "2014",
      "venue": "Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR)",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel",
      "authors": [
        "Kincaid Peter",
        "Robert P Fishburne Jr",
        "Richard L Rogers",
        "Brad S Chissom"
      ],
      "year": "1975",
      "venue": "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Iterative edit-based unsupervised sentence simplification",
      "authors": [
        "Dhruv Kumar",
        "Lili Mou",
        "Lukasz Golab",
        "Olga Vechtomova"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "Mike Lewis",
        "Yinhan Liu",
        "Naman Goyal",
        "Marjan Ghazvininejad",
        "Abdelrahman Mohamed",
        "Omer Levy",
        "Veselin Stoyanov",
        "Luke Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "An unsupervised method for building sentence simplification corpora in multiple languages",
      "authors": [
        "Xinyu Lu",
        "Jipeng Qiang",
        "Yun Li",
        "Yunhao Yuan",
        "Yi Zhu"
      ],
      "year": "2001",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Controllable sentence simplification",
      "authors": [
        "Louis Martin",
        "Éric De La Clergerie",
        "Benoît Sagot",
        "Antoine Bordes"
      ],
      "year": "2001",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "MUSS: Multilingual unsupervised sentence simplification by mining paraphrases",
      "authors": [
        "Louis Martin",
        "Angela Fan"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Unsupervised sentence simplification using deep semantics",
      "authors": [
        "Shashi Narayan",
        "Claire Gardent"
      ],
      "year": "2016",
      "venue": "Unsupervised sentence simplification using deep semantics",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Controllable text simplification with lexical con-straint loss",
      "authors": [
        "Daiki Nishihara",
        "Tomoyuki Kajiwara",
        "Yuki Arase"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Exploring neural text simplification models",
      "authors": [
        "Sergiu Nisioi",
        "Sanja Štajner",
        "Simone",
        "Paolo Ponzetto",
        "Liviu P Dinu"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll L Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "Kishore Papineni",
        "Salim Roukos",
        "Todd Ward",
        "Wei-Jing Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Unsupervised statistical text simplification",
      "authors": [
        "Jipeng Qiang",
        "Xindong Wu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "doi": "10.1109/TKDE.2019.2947679"
    },
    {
      "id": "b17",
      "title": "Lsbert: Lexical simplification based on bert",
      "authors": [
        "Jipeng Qiang",
        "Yun Li",
        "Yi Zhu",
        "Yunhao Yuan",
        "Yang Shi",
        "Xindong Wu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Chinese lexical simplification",
      "authors": [
        "Jipeng Qiang",
        "Xinyu Lu",
        "Yun Li",
        "Yunhao Yuan",
        "Xindong Wu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Dyswebxia 2.0! more accessible text for people with dyslexia",
      "authors": [
        "Luz Rello",
        "Clara Bayarri",
        "Azuki Górriz",
        "Ricardo Baeza-Yates",
        "Saurabh Gupta",
        "Gaurang Kanvinde",
        "Horacio Saggion",
        "Stefan Bott",
        "Roberto Carlini",
        "Vasile Topac"
      ],
      "year": "2013",
      "venue": "Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Making it simplext: Implementation and evaluation of a text simplification system for spanish",
      "authors": [
        "Horacio Saggion",
        "Sanja Štajner",
        "Stefan Bott",
        "Simon Mille",
        "Luz Rello",
        "Biljana Drndarevic"
      ],
      "year": "2015",
      "venue": "ACM Transactions on Accessible Computing (TACCESS)",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "BLEU is not suitable for the evaluation of text simplification",
      "authors": [
        "Elior Sulem",
        "Omri Abend",
        "Ari Rappoport"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Unsupervised neural text simplification",
      "authors": [
        "Sai Surya",
        "Abhijit Mishra",
        "Anirban Laha",
        "Parag Jain",
        "Karthik Sankaranarayanan",
        "; Daniel De Freitas",
        "Jamie Hall",
        "Noam Shazeer",
        "Apoorv Kulshreshtha",
        "Heng-Tze",
        "Alicia Cheng",
        "Taylor Jin",
        "Leslie Bos",
        "Yu Baker",
        "Du"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Text simplification using neural machine translation",
      "authors": [
        "Tong Wang",
        "Ping Chen",
        "John Rochford",
        "Jipeng Qiang"
      ],
      "year": "2001",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Vinícius Rodriguez Uzêda, Renata Pontin de Mattos Fortes, Thiago Alexandre Salgueiro Pardo, and Sandra Maria Aluísio. Facilita: reading assistance for low-literacy readers",
      "authors": [
        "Willian Massami Watanabe",
        "Arnaldo Candido",
        "Junior"
      ],
      "year": "2009",
      "venue": "Proceedings of the 27th ACM international conference on Design of communication",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "CCNet: Extracting high quality monolingual datasets from web crawl data",
      "authors": [
        "Guillaume Wenzek",
        "Marie-Anne Lachaux",
        "Alexis Conneau",
        "Vishrav Chaudhary",
        "Francisco Guzmán",
        "Armand Joulin",
        "Edouard Grave"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Learning to simplify sentences with quasi-synchronous grammar and integer programming",
      "authors": [
        "Kristian Woodsend",
        "Mirella Lapata"
      ],
      "year": "2011",
      "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Antal van den Bosch, and Emiel Krahmer. Sentence simplification by monolingual machine translation",
      "authors": [
        "Sander Wubben"
      ],
      "year": "2012",
      "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Problems in current text simplification research: New data can help",
      "authors": [
        "Wei Xu",
        "Chris Callison-Burch",
        "Courtney Napoles"
      ],
      "year": "2015",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Optimizing statistical machine translation for text simplification",
      "authors": [
        "Wei Xu",
        "Courtney Napoles",
        "Ellie Pavlick",
        "Quanze Chen",
        "Chris Callison-Burch"
      ],
      "year": "2016",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Sentence simplification with deep reinforcement learning",
      "authors": [
        "Xingxing Zhang",
        "Mirella Lapata"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Integrating transformer and paraphrase rules for sentence simplification",
      "authors": [
        "Sanqiang Zhao",
        "Rui Meng",
        "Daqing He",
        "Andi Saptono",
        "Bambang Parmanto"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "A monolingual tree-based translation model for sentence simplification",
      "authors": [
        "Zhemin Zhu",
        "Delphine Bernhard",
        "Iryna Gurevych"
      ],
      "year": "2010",
      "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Sentence Simplification Via Large Language Models",
      "text": "Yutao Feng College of Information Engineering, Yangzhou University Jipeng Qiang College of Information Engineering, Yangzhou University Yun Li College of Information Engineering, Yangzhou University Yunhao Yuan College of Information Engineering, Yangzhou University Yi Zhu College of Information Engineering, Yangzhou University"
    },
    {
      "title": "Abstract",
      "text": "Sentence Simplification aims to rephrase complex sentences into simpler sentences while retaining original meaning. Large Language models (LLMs) have demonstrated the ability to perform a variety of natural language processing tasks. However, it is not yet known whether LLMs can be served as a high-quality sentence simplification system. In this work, we empirically analyze the zero-/few-shot learning ability of LLMs by evaluating them on a number of benchmark test sets. Experimental results show LLMs outperform state-of-the-art sentence simplification methods, and are judged to be on a par with human annotators."
    },
    {
      "title": "1 Introduction",
      "text": "Sentence Simplification (SS) is a task of rephrasing a sentence into a new form that is easier to read and understand while retaining its meaning, which can be used for increasing accessibility for people with dyslexia(Rello et al., 2013), autism(Evans et al., 2014) or low-literacy skills(Watanabe et al., 2009). In recent years, neural SS methods utilize parallel SS datasets to train Sequence-to-Sequence models (Wang et al., 2016; Zhang and Lapata, 2017; Zhao et al., 2018) or fine-tune pretrained language models (e.g. BART(Lewis et al., 2020))(Martin et al., 2020; Lu et al., 2021; Martin et al., 2022). However, much work (Woodsend and Lapata, 2011; Xu et al., 2015; Qiang and Wu, 2021) pointed out that the public English SS benchmark (WikiLarge Zhang and Lapata (2017)) which align sentences from English Wikipedia and Simple English Wikipedia are deficient, because they contain a large proportion of inaccurate or inadequate simplifications, which lead to the poor generalization performance of SS methods. Large Language Models (LLMs) have demonstrated their ability to solve a range of natural language processing tasks through zero-/few-Shot learning(Brown et al., 2020; Thoppilan et al., 2022; Chowdhery et al., 2022). Nevertheless, it remains unclear how LLMs perform in SS task compared to current SS methods. To address this gap in research, we undertake a systematic evaluation of the Zero-/Few-Shot learning capability of LLMs, by assessing their performance on existing SS benchmarks. We carry out an empirical comparison of the performance of ChatGPT and the most advanced GPT3.5 model (_text-davinci-003_). To the best of our knowledge, this is the first study of LLMs's capabilities on SS task, aiming to provide a preliminary evaluation, including simplification prompt, multilingual simplification, and simplification robustness. The key findings and insights are summarized as follows: (1) GPT3.5 or ChatGPT based on one-shot learning outperform the state-of-the-art SS methods. We found that these models excel at deleting non-essential information and adding new information, while existing supervised SS methods tend to preserve the content without change. (2) ChatGPT is a monolithic model capable of supporting multiple languages, which makes it a comprehensive multilingual text simplification technique. After evaluating the performance of ChatGPT on the task of simplification across two languages (Portuguese and Spanish), we observed that it surpasses the best baseline methods by a considerable margin. This also confirms that LLMs can be adapted to other languages (3) By performing human evaluation over LLMs's simplification and human's simplification, LLMs's simplifications are judged to be on a par with human written simplifications. Additionally, the results of this paper is available by visiting [https://github.com/BrettFyt/SS_Via_LLMs](https://github.com/BrettFyt/SS_Via_LLMs)."
    },
    {
      "title": "2 Related Work",
      "text": "**Supervised Sentence Simplification methods**: Supervised SS methods usually treat SS task as monolingual machine translation, which requires a large parallel corpus of aligned Complex-Simple sentences pairs(Nisioi et al., 2017). Those methods often employed a Sequence-to-Sequence model (such as TransformerVaswani et al. (2017)) as backbone, then integrating different sub-modules into it such as reinforcement learningZhang and Lapata (2017), external simplification rules databasesZhao et al. (2018), adding a new lossNishihara et al. (2019), or lexical complexity featuresMartin et al. (2020). Another way is to train a sequence editing model, simplifying sentences by predicting the operations for every wordsDong et al. (2019). These methods rely heavily on public training sets, namely WikiLargeZhang and Lapata (2017) and WikiSmallZhu et al. (2010). However, recent studies have pointed out their defects since they contain a substantial number of inaccurate or inadequate simplification pairs Woodsend and Lapata (2011); Xu et al. (2015); Qiang and Wu (2021). To alleviate this constraint, recent research has concentrated on developing novel parallel SS corpora. In this way, Martin et al. (2022) employed sentence embedding modeling to measure the similarity between sentences from approximately one billion CC-NETWenzek et al. (2020) sentences, and subsequently constructed a new parallel SS corpora. Lu et al. (2021) utilize machine translation corpus to construct SS corpora via back-translation technique. Compared with WikiLarge, these corpora can help to enhance the performance of supervised SS methods. **Unsupervised Sentence Simplification methods**: Unsupervised SS methods utilize non-aligned complex-simple pairs corpora. One such method employs style-transfer techniques to achieve content reduction and lexical simplification by importing adversarial and denoising auxiliary lossesSurya et al. (2019). However, this method is less controllable and cannot perform syntactic simplification. Some methods only focus on lexical simplificationQiang et al. (2021, 2021). For pipeline methods, one proposed framework uses revision-based approaches such as lexical simplification, sentence splitting, and phrase deletionNarayan and Gardent (2016), while another improved it by adding an iterative mechanism to these revision-based approachesKumar et al. (2020). However, these methods have too many simplification errors in practice. **Large Language Models**: LLMs Brown et al. (2019), Figure 1: (a) is an example of Zero-Shot Sentence Simplification based on LLMs. (b) is an example of few-shot sentence simplification based on LLMs, which stack multiple combinations. **{Outputs}** in the picture means where LLMs output simplified sentences. 2020; Thoppilan et al., 2022; Chowdhery et al., 2022) have two distinctive features over previous pretrained models. Firstly, LLMs have much larger scale in terms of model parameters and training data. Secondly, unlike previous pretrained models that require finetuning, LLMs can be prompted zero-shot or few-shot to solve a task. Current work(Ouyang et al., 2022) shows that the instruct-tuned LLMs are better than finetuned pretrained language models on many natural language tasks. But, there is no work about the capabilities of LLMs on SS task."
    },
    {
      "title": "3 Sentence Simplification Via Llms",
      "text": "Through exhibiting zero-shot transfer capabilities of LLMs, they have also become more attractive for lower-resourced tasks. Considering sentence simplification (SS) task lacks large-scale training corpus, we will test the performance of LLMs on SS task. Specific template patterns, commonly known as prompts, are often employed to guide models towards predicting a particularly desirable output or answer format, without requiring a dedicated training on labeled examples. Utilizing this paradigm shift, we experimented with different prompts issued to OpenAI's largest available model, GPT3.5 (_text-davinci-003_) and ChatGPT. **Simplification Prompts:** To design the prompts for triggering the sentence simplification ability of LLMs, we test multiple prompts to analyze the results. Finally, we meticulously craft two manual instruction prompts, which are illustrated in Table 1. For the two prompts, **{Complex Sentence}** means the blanks that we need to fill a complex sentence in, while **{Outputs}** was the place carries the outputs of LLMs. In the first prompt (T1), we utilize the {Guidance-Complex-Simple} mapping whereby LLMs are employed to simplify complex sentences into simpler ones under the guidance. In the second prompt (T2), we conceive the {Sentence-Question-Answer} mapping methodology to simplify complex sentences in the form of questions. Furthermore, the outputs of the model are unpredictable. In order to achieve a sole output of simplified sentences devoid of any extraneous details, the prompts T1 and T2 employ a specialized guide word, namely \"_Simple:_\" and \"_Answer:_\", respectively, which implement SS by filling in blanks. When executing multilingual SS tasks, we translate the two prompts into the identical languages which utilized in the specific SS tasks. **Zero-shot:** When we implement Zero-shot SS, we only need one {Guidance-Complex-Simple} combination or one {Sentence-Question-Answer} combination for inducing LLMs to generate simplified sentences directly (as shown on Figure 1 (a)). **Few-shot:** For few-shot SS, we need to stack multiple combinations for providing simplified examples (as shown on Figure 1 (b)). It is worth noting that we do not need to repeat the guidance in prompt T1, we only need to stack {Complex-Simple} combination under the guidance. In contrast, for prompt T1, we need to stack the whole {Sentence-Question-Answer} combination. For both prompts, we provide the simplified examples in the part of **{Simplified Sentence(s)}** for few-shot setting. Since the diversity of simplified forms of a sentence, we also test the way of the multiple manual simplified references. In this way, we change _sentence_ in the prompts to _sentences_ for adapting to the context of manual references. In actuality, the utilization of multiple simplified references induces the generation of multiple simplified candidates by LLMs. To address this predicament, we opt to select the first simplified candidate for each complex sentence, which yields comparatively superior outcomes in our experiments (Section 4.5)."
    },
    {
      "title": "4 Experiments",
      "text": ""
    },
    {
      "title": "Evaluation Settings",
      "text": "**Datasets:** For English (we note as \"En\") SS task, we chose TURKCORPUS (so-called WikiLarge Test \\begin{table} \\begin{tabular}{l l} \\hline \\hline & SS Prompts \\\\ \\hline \\multicolumn{3}{c}{I want you to replace my complex sentence} \\\\ \\multicolumn{3}{c}{with simple sentence(s). Keep the meaning} \\\\ \\multicolumn{3}{c}{same, but make them simpler.} \\\\ \\multicolumn{3}{c}{Complex: **{Complex Sentence}**} \\\\ T1 & Simple: **{Simplified Sentence(s)}** \\\\ \\multicolumn{3}{c}{.....} \\\\ \\multicolumn{3}{c}{Complex: **{Complex Sentence}**} \\\\ \\multicolumn{3}{c}{Simple: **{Outputs}**} \\\\ \\hline \\multicolumn{3}{c}{Sentence: **{Complex Sentence}**} \\\\ \\multicolumn{3}{c}{Question: Simplify the above sentence} \\\\ \\multicolumn{3}{c}{without changing meaning.} \\\\ \\multicolumn{3}{c}{Answer: **{S Simplified Sentence(s)}**} \\\\ \\multicolumn{3}{c}{\\(\\ldots\\)} \\\\ \\multicolumn{3}{c}{Sentence: **{Complex Sentence}**} \\\\ \\multicolumn{3}{c}{Question: Simplify the above sentence} \\\\ \\multicolumn{3}{c}{without changing meaning.} \\\\ \\multicolumn{3}{c}{Answer: **{Outputs}**} \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Candidate sentence simplification prompts. set)(Xu et al., 2016) and ASSET(Alva-Manchego et al., 2020) as multi-references SS datasets to evaluate the performance of LLMs. Both TURKCORPUS and ASSET are the most popular evaluation datasets for English SS, which consist of 2,000 valid sentences and 359 test sentences. TURKCORPUS employed Amazon Mechanical Turk to give 8 manual simplified versions for each complex sentence. ASSET is an improved version of TURKCORPUS which focuses on the multiple simplification operations, such as lexical paraphrasing, sentence splitting and compression, every complex sentence in it has 10 manual simplified versions sentences. To evaluate the multilingual generalization efficacy of LLMs, we have opted for the evaluation of multilingual (SS) task in Portuguese (we note as \"Pt\") and Spanish (we note as \"Es\"). For Portuguese, we employ Portuguese version ASSET as our testsets with 359 Portuguese complex sentences, which is also used to evaluate MUSS methods(Martin et al., 2022)1. For Spanish, we chose the SIMPLEXT Corpus(Saggion et al., 2015) for evaluation. SIMPLEXT is a high-quality 1-to-1 Spanish SS testset with complex 1416 sentences. it is simplified manually by experts for people with learning disabilities. We randomly select 100 sentences from these two datasets for our evaluation and translate prompt T1 into Portuguese and Spanish (as shown on Table 4). Footnote 1: This Portuguese dataset can be obtained by visiting the website: [https://github.com/facebookresearch/muss](https://github.com/facebookresearch/muss) **Baseline:** To evaluate English SS task, we compare LLMs with the supervised and unsupervised SS methods. For supervised SS methods, we select three classic methods (PBMT-R(Wubben et al., 2012), Dress-LS(Zhang and Lapata, 2017), DMASS-DCSS(Zhao et al., 2018), ACCESS(Martin et al., 2020)) and the recent state-out-of-art methods MUSS-S(Martin et al., 2022). For unsupervised SS methods, we compare with three methods (UNTS(Surya et al., 2019), BTTS10(Kumar et al., 2020), MUSS-Unsup(Martin et al., 2022)), where MUSS-US was considered as the state-out-of-art unsupervised SS method. To evaluate multilingual SS task, we also choose MUSS-US(Martin et al., 2022), this method that can complete multilingual SS tasks recently, for comparison. **Evaluation Metrics:** To evaluate sentences simplification methods, SARI(Xu et al., 2016) is primary metric in studies. SARI (the higher the better) compares the generated sentences to the references sentences and returns a arithmetic mean of the n-gram F1 scores of three operations (keeping, adding, and deleting), where \\(1\\leq n\\leq 4\\). We also report Flesch-Kincaid Grade Level (FKGL)(Kincaid et al., 1975) to evaluate the readability of the generated sentences. FKGL (the lower the better) is a classic algorithm for measuring readability, which reflects the readability of a text by calculating the age required to understand it. Like the recent works(Martin et al., 2020), we do not report \\begin{table} \\begin{tabular}{l c c c c c c c c c c} \\hline \\hline & \\multicolumn{6}{c}{TURKCORPUS (En)} & \\multicolumn{6}{c}{ASSET (En)} \\\\ \\hline & SARI \\(\\uparrow\\) & Add \\(\\uparrow\\) & Keep \\(\\uparrow\\) & Delete \\(\\uparrow\\) & FKGL \\(\\downarrow\\) & SARI \\(\\uparrow\\) & Add \\(\\uparrow\\) & Keep \\(\\uparrow\\) & Delete \\(\\uparrow\\) & FKGL \\(\\downarrow\\) \\\\ \\hline Source & 26.29 & - & - & - & 10.02 & 20.73 & - & - & - & 10.02 \\\\ Reference & 40.21 & - & - & - & 8.73 & 45.14 & - & - & - & 6.48 \\\\ \\hline & \\multicolumn{6}{c}{Supervised Methods} & & & & & & & \\\\ \\hline PBMT-R & 38.56 & 5.73 & 73.02 & 36.93 & 8.33 & 35.76 & 4.61 & 59.84 & 42.84 & 8.33 \\\\ Dress-LS & 37.27 & 2.81 & 66.77 & 42.22 & 6.62 & 36.90 & 2.40 & 56.14 & 52.15 & 6.62 \\\\ ACCESS & 41.51 & 6.50 & 71.30 & 46.73 & 7.56 & 40.74 & 6.44 & 62.13 & 53.64 & 7.56 \\\\ MUSS-S & **42.55** & 8.98 & **73.97** & 44.70 & 7.58 & 44.50 & 11.39 & 62.16 & 59.98 & **6.40** \\\\ \\hline & \\multicolumn{6}{c}{Unsupervised Methods} & & & & & & \\\\ \\hline UNTS & 37.20 & 1.50 & 68.81 & 41.27 & 7.84 & 35.19 & - & - & - & 7.60 \\\\ BTTS10 & 36.91 & 2.38 & 69.50 & 38.85 & **6.30** & 35.15 & 2.21 & 57.91 & 45.32 & 7.83 \\\\ MUSS-US & 40.80 & 7.97 & 67.49 & 46.96 & 8.82 & 42.86 & 8.58 & 58.96 & 61.06 & 8.78 \\\\ \\hline GPT3.5 & & & & & & & & & \\\\ +Zero-Shot & 37.20 & 8.34 & 49.55 & 53.73 & 6.71 & 44.92 & 10.30 & 53.04 & 71.42 & 6.71 \\\\ +Single-Shot & 41.82 & 10.38 & 61.78 & 53.31 & 6.97 & 47.32 & 12.40 & **61.38** & 68.18 & 6.97 \\\\ ChatGPT & & & & & & & & & \\\\ +Zero-Shot & 37.72 & 9.89 & 48.82 & **54.48** & 6.88 & 45.77 & 12.22 & 52.75 & **72.33** & 6.88 \\\\ +Single-Shot & 41.43 & **11.93** & 58.68 & 54.23 & 7.00 & **47.94** & **13.32** & 60.12 & 70.39 & 7.00 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Comparison of supervised and unsupervised SS Methods on TURKCORPUS and ASSET (En). the BLEU(Papineni et al., 2002) since recent study showed BLEU does not correlate well with sentence simplicity(Sulem et al., 2018). Since FKGL is not available for Spanish, we report FRES(Kincaid et al., 1975) instead. In the experiments, we use standard simplification evaluation package EASSE2 to calculate the SARI and FKGL metrics. We calculate FRES by using the script from Lu et al. (2021)3. Footnote 2: [https://github.com/feralvan/easse](https://github.com/feralvan/easse) Footnote 3: [https://github.com/luxinyu1/Trans-SS/](https://github.com/luxinyu1/Trans-SS/) **Other Details:** We chose the latest available LLMs GPT3.5(_text-davinci-003_) and ChatGPT, and we evaluate them by visiting OpenAI's website4, in which calling GPT3.5 requires payment. We set the max length of _text-davinci-003_ to 1024 for our few-shot experiments. In addition, for the English few-shot experiments, we randomly select sentences from both TURKCORPUS and ASSET valid sets as simplified examples. Due to the complex-simple pairs in the valid sets of SIMPLEXT is 1-to-1 relationship, so we just use one simplification reference for other languages. Footnote 4: [https://openai.com/api/](https://openai.com/api/)"
    },
    {
      "title": "Automatic Evaluation",
      "text": "**English Simplification:** Table 2 shows the results of all SS Methods. As both GPT3.5 and ChatGPT are developed from InstractGPT, they have similar performance in SS tasks. Overall, ChatGPT has demonstrated superior performance to GPT3.5 on ASSET (En) under single-shot, as evidenced by a higher SARI score. This accomplishment has allowed ChatGPT to exceed MUSS-S and establish itself as the new standard of excellence on ASSET (En), achieving an unprecedented state-of-the-art performance increase of +3.44 SARI. Nevertheless, when contrasted with ChatGPT, GPT3.5 and MUSS-S have displayed stronger performance on TURKCORPUS. After scrutinizing the trial scores of the simplification operations, we have deduced that LLMs boast a superior deletion score in comparison to other SS methods. This indicates that LLMs SS methods possess a penchant for excising segments of intricate sentences. So we perceive this phenomenon (LLMs exhibit inferiority to MUSS-S on TURKCORPUS) to have emanated from the circumstance that the simplified sentences in ASSET (En) are less intricate than those in TURKCORPUS. The simplified references of ASSET (En) underwent various simplification operations, whereas TURKCORPUS mostly adhered to the original structure(Alva-Manchego et al., 2020). However, if we give a simplified example, LLMs will balance the deleting and keeping of the complex sentence. For addition operations, both GPT3.5 and ChatGPT demonstrate high proficiency in scoring, as they are equipped with vast amounts of knowledge derived from copious datasets. **Portuguese and Spanish Simplification:** The \\begin{table} \\begin{tabular}{p{28.5pt} p{28.5pt}} \\hline \\hline \\multicolumn{2}{c}{SS Prompts} \\\\ \\hline & Quiero que reemplaces mis fraes complejas por fraes simples. Mantenga el significado sin cambios, pero Hágalo más simple. \\\\ Es & Complejo: \\{**Complex Sentence**\\} \\\\ Es & Simple: \\{**Simplified Sentence(s)**\\} \\\\ &.... \\\\ & Complejo:. \\{**Complex Sentence**\\} \\\\ & Simple: \\{**Outputs**\\} \\\\ \\hline & Quiero que substituas a minha frase complexa por ouma frase simples. Mantenha o mesmo significado, mas torne-os mais simples. \\\\ & Complevo: \\{**Complex Sentence**\\} \\\\ Pt & Simples: \\{**Simplified Sentence(s)**\\} \\\\ &.... \\\\ & Complevo:. \\{**Complex Sentence**\\} \\\\ & Simples: \\{**Outputs**\\} \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Prompting in Spanish and Portuguese. \\begin{table} \\begin{tabular}{p{28.5pt} p{28.5pt} p{28.5pt} p{28.5pt} p{28.5pt} p{28.5pt} p{28.5pt} p{28.5pt} p{28.5pt} p{28.5pt}} \\hline \\hline \\multicolumn{5}{c}{ASSET (Pt)} & \\multicolumn{5}{c}{SIMPLEXT (Es)} \\\\ \\hline & \\multicolumn{2}{c}{SARI \\(\\uparrow\\)} & \\multicolumn{2}{c}{Add \\(\\uparrow\\)} & \\multicolumn{2}{c}{Keep \\(\\uparrow\\)} & \\multicolumn{2}{c}{Delete \\(\\uparrow\\)} & \\multicolumn{2}{c}{SARI \\(\\uparrow\\)} & \\multicolumn{2}{c}{Add \\(\\uparrow\\)} & \\multicolumn{2}{c}{Keep \\(\\uparrow\\)} & \\multicolumn{2}{c}{Delete \\(\\uparrow\\)} & \\multicolumn{2}{c}{FRES \\(\\uparrow\\)} \\\\ \\hline Source & 20.81 & - & - & - & 5.85 & - & - & - & 51.59 \\\\ MUSS-US & 40.94 & 7.71 & **63.23** & 51.90 & 20.51 & 1.87 & 18.31 & 41.34 & 56.12 \\\\ \\hline ChatGPT & & & & & & & & & \\\\ +Zero-Shot & 44.67 & 9.71 & 52.85 & **71.44** & 38.97 & 6.09 & 26.57 & 84.27 & 66.19 \\\\ +Single-Shot & **47.06** & **11.30** & 59.20 & 70.71 & **42.79** & **7.07** & **31.40** & **89.90** & **70.77** \\\\ +Two-Shot & 46.00 & 10.92 & 58.35 & 68.73 & 41.89 & 6.78 & 29.37 & 89.53 & 69.76 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Comparison of the state-of-the-art multilingual SS method MUSS(Martin et al., 2022) and LLMs SS methods on ASSET (Pt) and SIMPLEXT (Es). results are shown on the Table 3. It is obvious that ChatGPT outperform the SS results of MUSS-US on both Portuguese and Spanish testset. Firstly, owing to the unsupervised nature of MUSS-US, we compare the Zero-Shot approach of ChatGPT with it. ChatGPT's Zero-Shot methodology surpasses MUSS-US significantly in both languages, namely by +3.73 SARI and +18.46 SARI respectively. Moreover, in the realm of single-shot, ChatGPT obtains a SARI score of 46.00 on ASSET (Pt) and 41.89 SARI on SIMPLEXT, surpassing MUSS by a notable 5 and 21 points, respectively. In particular, in Spanish dataset SIMPLEXT, ChatGPT has a huge lead over MUSS-US in both few-shot and zero-shot scenarios. The aforementioned findings indicate that ChatGPT exhibits strong generalization capabilities across various languages, surpassing MUSS in terms of multilingual SS performance."
    },
    {
      "title": "Human Evaluation",
      "text": "Since automated metrics may be not enough for evaluating sentence generation, we report the results of human evaluation. In the experiment, we only choose the most advanced method MUSS-S and the reference to evaluation. Firstly, we followed the evaluation metrics setup in Kumar et al. (2020) and Dong et al. (2019). We measure the adequacy metric (_How many meanings of the original sentence are retained in the simplified sentences?_), simplicity metric (_Is the sentences output by the system simpler than the original sentence?_), fluency metric (_Is the output sentences grammatical or well-formed?_) on five-point scale (1 is the worst, 5 is the best). In addition, we also measured referees' subjective choices (Ranking simplified sentences from No.1 to No.3) to focus on actual usage rather than evaluation criteria. We select one hundred complex sentences from ASSET (En) randomly, and then randomly arrange the sentences produced by MUSS-S, ChatGPT, and the simplified sentences randomly selected from the reference files. Then we ask three non-native English speakers with medium level to assess the sentences based on these above metrics. The results are shown in Table 6. As expressed in Section 4.2, due to the tendency of ASSET (En) dataset and ChatGPT to expunge superfluous elements in complex sentences, the adequacy metric lags behind MUSS \\begin{table} \\begin{tabular}{l l l l l l} \\hline \\hline Method & A \\(\\uparrow\\) & S \\(\\uparrow\\) & F \\(\\uparrow\\) & Avg. \\(\\uparrow\\) & Rank \\(\\downarrow\\) \\\\ \\hline ChatGPT & 4.10 & **3.85** & **4.32** & **4.09** & 1.92 \\\\ MUSS-S & **4.15** & 3.77 & 4.21 & 4.04 & 2.02 \\\\ Reference & 3.99 & 3.81 & 4.23 & 4.01 & **1.87** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: The results of human evaluation on ASSET (En) based on the adequacy (A), simplicity (S), fluency (F), their average score (Avg.) and subjective ranking of referees (Rank). \\begin{table} \\begin{tabular}{l l} \\hline \\hline Complex Sentence: & The Great Dark Spot is thought to **represent** a hole in the methane cloud deck \\\\ & of Neptune. \\\\ GPT3.5: & The Great Dark Spot is **believed** to **be** a hole in Neptune’s methane cloud deck. \\\\ ChatGPT: & The Great Dark Spot is thought to **be** a hole in Neptune’s methane cloud deck. \\\\ Reference: & The Great Dark Spot is thought to **be** a hole in the methane cloud deck of Neptune. \\\\ \\hline Complex Sentence: & Their eyes are **quite small**, and their **visual acuity** is poor. \\\\ GPT3.5: & Their eyes are **small** and they have poor **vision**. \\\\ ChatGPT: & Their eyes are **small** and their **vision** is not very good. \\\\ Reference: & Their eyes are **small** and their **vision** is poor. \\\\ \\hline Complex Sentence: & He **settled in** London, **devoting himself chiefly to practical teaching**. \\\\ GPT3.5: & He **moved to** London and **focused mainly on teaching**. \\\\ ChatGPT: & He **lived in** London and **focused on teaching**. \\\\ Reference: & He **moved to** London and **focused on teaching**. \\\\ \\hline Complex Sentence: & Ned and Dan **advanced to** the police camp, **ordering** them to surrender. \\\\ _GPT3.5_: & Ned and Dan **went to** the police camp **and ordered** them to surrender. \\\\ ChatGPT: & Ned and Dan **went to** the police camp **and told them to give up**. \\\\ Reference: & Ned and Dan **went to** police camp **and told them to give up**. \\\\ Reference: & Ned and Dan **went to** police camp **and told them to give up**. \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 5: The examples of single-shot on English SS Task. The words in bold highlight the differences. S in human appraisal. On the contrary, ChatGPT surpasses MUSS-S in the simplicity and fluency metric owing to this attribute. For human preference, the primary option is ChatGPT and reference sentences, as they hold a superior ranking. This implies that ChatGPT aligns better with individuals' proclivity towards simplicity, in contrast to MUSS-S. It all means the SS performance of ChatGPT surpasses that of MUSS-S in terms of human evaluation and is on a par with human written simplifications."
    },
    {
      "title": "Qualitative Study",
      "text": "To intuitively analyze the sentence simplification capability of LLMs. Tabel 5 shows some English sentence simplification examples of GPT3.5 and ChatGPT assembled Single-Shot. The method based LLMs significantly reduces the linguistic complexity of the complex sentence while retaining its original main meaning. LLMs perform lexical simplification very well, e.g., \"be\" as a simpler substitute for \"represent\", \"vision\" as a simpler substitute for \"visual acuity\", \"went to\" as a simpler substitute for \"advanced to\", etc. For syntactic simplification, LLMs focus on using simpler and more concise syntactic, e.g., simplifying complex structure \"devoting himself chiefly to practical teaching\" to \"focused on teaching\", the form change from the adverbial clause of \"ordering\" to two parallel clauses by using \"ordered\", etc. In summary, we draw the conclusions from these examples that GPT3.5 and ChatGPT can use only one manual SS example to perform both lexical simplification and syntactic simplification, and these simplification operations are similar to the reference sentences."
    },
    {
      "title": "Ablation Study",
      "text": "**Prompts Study:** We compared the performance differences between different prompts (as shown in Table 1) for SS task. Because the SS performance of GPT3.5 and ChatGPT is very close, we choose GPT3.5 as backbone for the prompts experiments. The performance comparison of different Prompts is shown in Table 8. Generally speaking, prompt T1 surpasses prompt T2 \\begin{table} \\begin{tabular}{l r r r r r r r r r} \\hline \\hline & \\multicolumn{6}{c}{TURKCORPUS (En)} & \\multicolumn{6}{c}{ASSET (En)} \\\\ \\hline & SARI \\(\\uparrow\\) & Add \\(\\uparrow\\) & Keep \\(\\uparrow\\) & Delete \\(\\uparrow\\) & FKGL \\(\\downarrow\\) & SARI \\(\\uparrow\\) & Add \\(\\uparrow\\) & Keep \\(\\uparrow\\) & Delete \\(\\uparrow\\) & FKGL \\(\\downarrow\\) \\\\ \\hline \\multicolumn{10}{c}{GPT3.5} \\\\ \\hline Zero-Shot & 37.20 & 8.33 & 49.54 & 53.72 & 6.71 & 44.92 & 10.30 & 53.04 & 71.42 & 6.71 \\\\ Single-Shot & & & & & & & & & \\\\ + Single-Ref & 37.78 & 10.36 & 48.93 & 54.04 & 6.82 & 45.68 & 12.27 & 52.99 & 71.72 & 6.82 \\\\ + Multi-Refs + No.1 & 41.82 & 10.38 & 61.78 & 53.31 & 6.97 & 47.32 & 12.40 & 61.38 & 68.18 & 6.97 \\\\ + Multi-Refs + Random & 41.69 & 10.09 & 61.97 & 53.02 & 7.45 & 46.49 & 11.04 & 61.28 & 67.15 & 7.45 \\\\ Two-Shot (Single-Ref) & 40.26 & 11.33 & 55.53 & 53.92 & 7.22 & 46.69 & 13.10 & 56.96 & 70.02 & 7.22 \\\\ Three-Shot (Single-Ref) & 39.78 & 10.63 & 54.49 & 54.21 & 7.33 & 46.63 & 12.58 & 56.62 & 70.70 & 7.33 \\\\ \\hline \\multicolumn{10}{c}{ChatGPT} \\\\ \\hline Zero-Shot & 37.72 & 9.89 & 48.82 & 54.48 & 6.88 & 45.77 & 12.22 & 52.75 & 72.33 & 6.88 \\\\ Single-Shot & & & & & & & & & \\\\ + Single-Ref & 38.80 & 11.69 & 49.83 & 54.90 & 7.19 & 47.07 & 14.47 & 54.14 & 72.61 & 7.19 \\\\ + Multi-Refs + No.1 & 41.43 & 11.93 & 58.68 & 54.23 & 7.00 & 47.94 & 13.32 & 60.12 & 70.39 & 7.00 \\\\ + Multi-Refs + Random & 41.13 & 11.28 & 57.79 & 54.32 & 7.08 & 47.28 & 13.02 & 58.46 & 70.38 & 7.07 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 7: The ablation experiments on TURKCORPUS (En) and ASSET (En). “_+ Multi-Refs + No.1_” means using multiple simplified references and selecting the first simplified candidate as output. “_+ Multi-Refs + Random_” means using multiple simplified references too, but selecting the simplified candidate randomly as output. \\begin{table} \\begin{tabular}{l r r r r} \\hline \\hline & \\multicolumn{2}{c}{TURKCORPUS (En)} & \\multicolumn{2}{c}{ASSET (En)} \\\\ \\hline & SARI \\(\\uparrow\\) & FKGL \\(\\downarrow\\) & SARI \\(\\uparrow\\) & FKGL \\(\\downarrow\\) \\\\ \\hline & \\multicolumn{2}{c}{Zero-Shot} & & & \\\\ \\hline GPT3.5 + T1 & **37.20** & **6.71** & **44.92** & **6.71** \\\\ GPT3.5 + T2 & 36.28 & 7.02 & 43.57 & 7.02 \\\\ \\hline & \\multicolumn{2}{c}{Single-Shot} & & & \\\\ \\hline GPT3.5 + T1 & **41.82** & **6.97** & **47.32** & **6.97** \\\\ GPT3.5 + T2 & 40.65 & 8.29 & 45.00 & 8.29 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 8: Comparison of different prompts for GPT3.5 on two datasets. in terms of efficacy for both Zero-Shot and Single-Shot SS. Specifically, with regard to the SARI metric for TURKCORPUS, T1 exhibits a superiority of approximately 1 point over T2. Similarly, on ASSET, T1 outperforms T2 by approximately 1.5 points and 2.0 points for the Zero-Shot and Single-Shot respectively. **Few-shot Study:** To further analyze the performance of GPT3.5 and ChatGPT SS methods under mutil-references or Single-reference, we do more experiments in this section. The results of our experiments were shown on Table 7. When juxtaposed with the few-shot method employing a single reference (as shown on Figure 2), this approach demonstrates substantial improvement in the retention operation of LLMs for intricate sentences, thereby striking a balance between the deletion and retention operations of LLMs (as shown on Figure 3). Moreover, in the event that the selection of the first simplified candidate is removed (noted as \"_+ Multi-Refs + Random_\"), the performance of SS would diminish, thereby indicating the efficacy of the whole approach, as well as the proclivity of LLMs to furnish the most feasible simplification as a priority in the presence of multiple references. It is noteworthy that, with an augmentation in the number of Shots, which refers to the increment in the count of sentence simplification examples, there will be a diminishing return in the enhancement of performance. This fact holds true for both the English language and other languages (as shown on Table 3)."
    },
    {
      "title": "5 Conclusion",
      "text": "In this paper, we present a study of the performance of LLMs (GPT3.5 and ChatGPT) for SS task. Given that GPT3.5 and ChatGPT are both derivatives of IntractGPT, their performance in SS tasks is comparable. During the benchmark experiments, LLMs outperformed current state-of-the-art SS methods in the realm of multilingual SS tasks. Furthermore, through the implementation of human and qualitative evaluation, LLMs' simplifications are judged to be on a par with the simplified sentences crafted by human. In our subsequent endeavours, our aim is to design more refined SS methodologies founded on LLMs while also delving deeper into the various proficiencies LLMs offer."
    },
    {
      "title": "References",
      "text": "* Alva-Manchego et al. [2020-07] Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Benoit Sagot, and Lucia Specia. ASSET: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 4668-4679, July 2020. * Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020. * Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022. * Dong et al. [2019] Yue Dong, Zichao Li, Mehdi Rezagholizadeh, and Jackie Chi Kit Cheung. EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 3393-3402, 2019. Figure 3: The example of single-shot with multi-refs. Figure 2: The example of single-shot with single-ref. Richard Evans, Constantin Orasan, and Iustin Dornescu. An evaluation of syntactic simplification rules for people with autism. In _Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR)_, pages 131-140, 2014. * Kincaid et al. [2014] J Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers, and Brad S Chissom. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Technical report, Naval Technical Training Command Millington TN Research Branch, 1975. * Kumar et al. [2020] Dhruv Kumar, Lili Mou, Lukasz Golab, and Olga Vechtomova. Iterative edit-based unsupervised sentence simplification. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7918-7928, Online, July 2020. Association for Computational Linguistics. * Lewis et al. [2020] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7871-7880, 2020. * Lu et al. [2021] Xinyu Lu, Jipeng Qiang, Yun Li, Yunhao Yuan, and Yi Zhu. An unsupervised method for building sentence simplification corpora in multiple languages. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 227-237, 2021. * Martin et al. [2020] Louis Martin, Eric de la Clergerie, Benoit Sagot, and Antoine Bordes. Controllable sentence simplification. In _Proceedings of the Twelfth Language Resources and Evaluation Conference_, pages 4689-4698, Marseille, France, May 2020. * Martin et al. [2022] Louis Martin, Angela Fan, Eric de la Clergerie, Antoine Bordes, and Benoit Sagot. MUSS: Multilingual unsupervised sentence simplification by mining paraphrases. In _Proceedings of the Thirteenth Language Resources and Evaluation Conference_, pages 1651-1664, Marseille, France, June 2022. * Narayan and Gardent [2016] Shashi Narayan and Claire Gardent. Unsupervised sentence simplification using deep semantics. In _Proceedings of the 9th International Natural Language Generation conference_, pages 111-120, 2016. * Nishihara et al. [2019] Daiki Nishihara, Tomoyuki Kajiwara, and Yuki Arase. Controllable text simplification with lexical constraint loss. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop_, pages 260-266, 2019. * Nisioi et al. [2017] Sergiu Nisioi, Sanja Stajner, Simone Paolo Ponzetto, and Liviu P. Dinu. Exploring neural text simplification models. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 85-91, 2017. * Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022. * Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, pages 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. * Qiang and Wu [2021] Jipeng Qiang and Xindong Wu. Unsupervised statistical text simplification. _IEEE Transactions on Knowledge and Data Engineering_, 33(4):1802-1806, 2021. doi: 10.1109/TKDE.2019.2947679. * Qiang et al. [2021a] Jipeng Qiang, Yun Li, Yi Zhu, Yunhao Yuan, Yang Shi, and Xindong Wu. Lsbert: Lexical simplification based on bert. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 29:3064-3076, 2021a. * Qiang et al. [2021b] Jipeng Qiang, Xinyu Lu, Yun Li, Yunhao Yuan, and Xindong Wu. Chinese lexical simplification. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 29:1819-1828, 2021b. * Rello et al. [2013] Luz Rello, Clara Bayarri, Azuki Gorriz, Ricardo Baeza-Yates, Saurabh Gupta, Gaurang Kanvinde, Horacio Saggion, Stefan Bott, Roberto Carlini, and Vasile Topac. Dyswebxia 2.0! more accessible text for people with dyslexia. In _Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility_, pages 1-2, 2013. * Saggion et al. [2015] Horacio Saggion, Sanja Stajner, Stefan Bott, Simon Mille, Luz Rello, and Biljana Drndarevic. Making it simplext: Implementation and evaluation of a text simplification system for spanish. _ACM Transactions on Accessible Computing (TACCESS)_, 6(4):1-36, 2015. * Sulem et al. [2015] Elior Sulem, Omri Abend, and Ari Rappoport. BLEU is not suitable for the evaluation of text simplification. In _Proceedings of the 2018 Conferenceon Empirical Methods in Natural Language Processing_, pages 738-744, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. * [Surya et al.2019] Sai Surya, Abhijit Mishra, Anirban Laha, Parag Jain, and Karthik Sankaranarayanan. Unsupervised neural text simplification. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 2058-2068, Florence, Italy, July 2019. Association for Computational Linguistics. * [Thoppilan et al.2022] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_, 2022. * [Vaswani et al.2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017. * [Wang et al.2016] Tong Wang, Ping Chen, John Rochford, and Jipeng Qiang. Text simplification using neural machine translation. _Proceedings of the AAAI Conference on Artificial Intelligence_, 30(1), Mar. 2016. * [Watanabe et al.2009] William Massami Watanabe, Arnaldo Candido Junior, Vinicius Rodriguez Uzeda, Renata Pontin de Mattos Fortes, Thiago Alexandre Salgueiro Pardo, and Sandra Maria Aluisio. Facilita: reading assistance for low-literacy readers. In _Proceedings of the 27th ACM international conference on Design of communication_, pages 29-36, 2009. * [Wenzek et al.2020] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In _Proceedings of the Twelfth Language Resources and Evaluation Conference_, 2020. * [Woodsend and Lapata2011] Kristian Woodsend and Mirella Lapata. Learning to simplify sentences with quasi-synchronous grammar and integer programming. In _Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing_, pages 409-420, 2011. * [Wubben et al.2012] Sander Wubben, Antal van den Bosch, and Emiel Krahmer. Sentence simplification by monolingual machine translation. In _Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1015-1024, 2012. * [Xu et al.2015] Wei Xu, Chris Callison-Burch, and Courtney Napoles. Problems in current text simplification research: New data can help. _Transactions of the Association for Computational Linguistics_, 3:283-297, 2015. * [Xu et al.2016] Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. Optimizing statistical machine translation for text simplification. _Transactions of the Association for Computational Linguistics_, 4:401-415, 2016. * [Zhang and Lapata2017] Xingxing Zhang and Mirella Lapata. Sentence simplification with deep reinforcement learning. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 584-594, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. * [Zhao et al.2018] Sanqiang Zhao, Rui Meng, Daqing He, Andi Saptono, and Bambang Parmanto. Integrating transformer and paraphrase rules for sentence simplification. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 3164-3173, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. * [Zhu et al.2010] Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych. A monolingual tree-based translation model for sentence simplification. In _Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)_, pages 1353-1361, 2010."
    }
  ]
}