<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Very Large Language Model as a Unified Methodology of Text Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-12-20">20 Dec 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
							<email>mjiang2@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre</orgName>
								<address>
									<postCode>46556</postCode>
									<settlement>Dame</settlement>
									<region>Indiana</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Very Large Language Model as a Unified Methodology of Text Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-20">20 Dec 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">B233D6E53C3FDDF5E28E793297DFC718</idno>
					<idno type="arXiv">arXiv:2212.09271v2[cs.DB]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text data mining is the process of deriving essential information from language text. Typical text mining tasks include text categorization, text clustering, topic modeling, information extraction, and text summarization. Various data sets are collected and various algorithms are designed for the different types of tasks. In this paper, I present a blue sky idea that very large language model (VLLM) will become an effective unified methodology of text mining. I discuss at least three advantages of this new methodology against conventional methods. Finally I discuss the challenges in the design and development of VLLM techniques for text mining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text data mining is to derive high-quality information from unstructured data. It enables decision-makers to analyze massive amounts of text quickly. For example, given one million restaurant reviews, one would look for automated solutions to categorizing each piece of review, identifying the aspects it describes, grouping the reviews, detecting topics, and making short summaries. In data mining and natural language processing (NLP), these are referred as text mining tasks, such as text classification, information extraction, text clustering, topic modeling, and text summarization. For each task, there are communities of researchers who are dedicated to design specialized algorithms and develop models.</p><p>Figure <ref type="figure" target="#fig_0">1a</ref> presents the conventional methodology of text mining where one model f i is developed for one task T i . There are at least two factors on the model's effectiveness: a data set of numerous quality data points D i and a properly designed algorithm a i . Deep learning is replacing traditional mining algorithms for most of the tasks. This trend originated from the architecture of neural encoder-decoder/predictor <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, and when the pre-trained bidirectional transformer encoders emerged (known as BERT <ref type="bibr" target="#b6">[7]</ref>), fine-tuning the model parameters with D i becomes the algorithm that builds a dedicated model f i on task T i .</p><p>Today very large language models (VLLMs) such as GPT-3 family (e.g., davinci-003 ) and ChatGPT are such powerful text generation models that they have  <ref type="formula">2</ref>) knowledge from a much larger size of text data and model (in yellow), and (3) ability of performing a task with a small number of data points (in red). revolutionized many NLP use cases. They are decoder only unidirectional autoregressive models; they have billions of parameters (much bigger than BERT); they are pre-trained on Internet-scale data and supervised using reinforcement learning from human feedback. Different from BERT, VLLM attempt to replace the downstream fine-tuning with few-shot learning. They have demonstrated extraordinary learning abilities on story writing, question answering, or mathematical reasoning just by conditioning on input-output examples in form of textual "prompts", without optimizing any parameters. VLLMs can possibly perform the text mining tasks as long as their users find proper examples.</p><formula xml:id="formula_0">Algorithm ! ! Dataset " ! Model # ! output $ ! % ! data points Task &amp; ! : Algorithm ! " Dataset " " Model # " output $ " % " data points Task &amp; " : … (a) Conventional methodology Pre-training ' ≫ % # data points Very Large Language Model # $%%&amp; output $ # Dataset " # ) # ≤ % # In-context learning Task &amp; # (b) VLLM as a unified methodology</formula><p>In fact, I am presenting a blue sky idea in this paper: VLLM will become a dominant approach of text mining -an effective unified methodology for various tasks. Figure <ref type="figure" target="#fig_0">1b</ref> shows the data sources, algorithms, 2 Significance</p><p>VLLM will become the new text mining methodology in next years. Figure <ref type="figure" target="#fig_0">1b</ref> briefly presents how VLLMs could be used to perform various tasks. First of all, VLLMs are trained on a dataset of N tokens (much larger than the size of any text mining dataset M i ) to predict the next token given the preceding text. This simple objective paired with the large dataset and model results in a very flexible LM that can "read" any text input and condition on it to "write" text that could plausibly come after the input. In task T i , the text input can be a concatenation of m i textual examples selected from D i of M i data points. If the task is text classification, given a new textual example X , suppose we select {(X j , Y j )} mi j=1 where m i is usually set as 3 considering the model's limit of text input length, and Y j denotes the category label of the j-th textual example X j . The text input is:</p><formula xml:id="formula_1">X 1 Category : Y 1 . X 2 Category : Y 2 . X 3 Category : Y 3 . X Category :</formula><p>We expect the next token(s) generated by VLLM to be the predicted category Y . The procedures are similar for other tasks such as topic modeling (i.e., generate a topic's word or special token) and information extraction (i.e., generate entities, their types, and relations). This new methodology is significantly different from two conventional methodologies of text mining (see Figure <ref type="figure" target="#fig_1">2</ref>). In "v1.0", the first step was extracting representations with vector space model (VSM) or distributed representation learning; then classification, clustering, or other types of algorithms were designed upon the representations for target tasks. In "v2.0", neural encoders or pre-trained transformer encoders were employed to automatically extract the representations. Three classical textbooks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> introduced these two methodologies in which the representations were extracted from moderate sized data and/or models.</p><p>Regarding how representations are extracted and how they are used for text mining, the VLLM method-ology will have at least three advantages:</p><p>One model and one algorithm, many tasks: VLLM is a deep decoder-only model of a huge number of parameters. It extracts representations in the decoder's transformer layers. The model's depth and scale make the representation extraction more flexible than preprocessing tools such as VSM or neural encoders, adapting to the task-oriented text input-output pairs. One would no longer have to design different specialized algorithms for different mining tasks, which is an expensive and exhaustive job. Instead, one would just need one single VLLM and one decoding algorithm to learn and use the representations to generate desired output.</p><p>Notably larger data, better representations: BERT was trained on 2.5B tokens Wikipedia and 800M tokens BookCorpus. In contrast, GPT-3 was trained on 429B tokens WebText, 3B tokens Wikipedia, and 68B tokens BookCorpus. Overall, that's 150 times larger pre-training data, generating deep representations from unprecedented generalization and leading to promising performance in many NLP task settings.</p><p>Small text data can be mined: Text mining algorithms used to require a big number of data points, like hundreds of thousands, however, in real applications (e.g., submarine technical reports) we usually have just hundreds or thousands of data points to do clustering or topic modeling. Conventional methods could not understand the textual examples effectively from infrequent tokens, so they could hardly find interesting clusters or topics. The pre-trained knowledge inside VLLMs will enable the text understanding of rare words or phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The current assumption of text mining method development is to train or fine-tune one model for each task by optimizing the model's parameters as long as computational resources can support. It has become a bottleneck of model size since VLLMs emerged and fine-tuning them is an impossible job for almost all researchers. To break the bottleneck, I suggest the community to design, develop, and evaluate new methods based on the VLLMs to learn from the task dataset and perform various text mining tasks as generating textual outputs without optimizing hundreds of billions of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">In-Context Learning</head><p>In-context learning was popularized in GPT-3 as a way to use a VLLM to learn tasks given only a few examples. Users give the VLLM a prompt that consists of a list of input-output pairs that demonstrate a task. A test input is appended at the end of the prompt. The VLLM makes a prediction just by conditioning on the prompt and predicting the next tokens. To correctly generate the test output, the model needs to read the training examples to figure out the input distribution, output distribution, inputoutput mapping, and the formatting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Mining Tasks as Text Generation</head><p>Because VLLMs are text-to-text generation models <ref type="bibr" target="#b7">[8]</ref>, a typical approach is to embed the description of the task in the text input, e.g., as a question instead of it being implicitly given. This is known as prompt learning or prompt engineering. In few-shot NLP tasks, prompts encourage a chain of thought for multi-step reasoning; in DALL-E and Stable Diffusion, prompts are designed to turn text into images. Towards mining a number of text data examples, the prompts should have not only the task description but also a (sub)set of the data examples to adapt the model to the target mining dataset. Novel prompting techniques are needed on VLLMs to turn text mining into text-to-text generation tasks.</p><p>Text categorization can be naturally transformed into prompting with few-shots as given in Section 2. The goal was to accurately distinguish data examples of different class labels. Therefore, if possible, re-defining the category names might help the model approach the optimal classification performance. In text clustering, the model may not generate cluster ID. One potential idea is to "ask" the model how similar two examples are, so any clustering algorithms (e.g., spetral clustering) on the proximity matrix can identify the clusters. Next, topic taxonomies will play a much more significant role in VLLM-based topic modeling, because the model can directly generate the topic names while the traditional techniques (e.g. PLSI, LDA) cannot. For example, in submarine technical reports, the topic names include "underwater detection capabilities" and "silent propulsion system"; it is important to prompt with the taxonomic information and topic-relevant examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Challenges</head><p>This section discusses four critical challenges in applying or developing VLLMs for text mining tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">VLLM with Many Shots</head><p>According to the OpenAI API, requests to VLLMs can use up to 4,097 tokens (∼3,000 words, n 1 =150 to 200 sentences) that are shared between prompt and generation. So the models perform few-shot learning which sounds powerful but limits the learning capacity when many shots are available in text mining. For example, in text clustering if each data example has n 2 =8 to 10 sentences, the model's text input has 14 to 24 data examples (n 1 /n 2 -1). More multi-layer attentions would be needed to process more tokens of text input, in order to learn from more examples. It limits VLLMs to expand learning sets without optimizing the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VLLM with Diverse Prompts / Examples</head><p>Given the constraint of a VLLM's text input length, we cannot use many example or very long prompts, but we can query the model multiple times for each test instance and aggregate the outputs, if time allows. In that case, a good strategy is to try diverse data examples and prompts in each time's input. The model will use multi-view knowledge with the diverse inputs to make inferences. Yu et al. found that language models could be augmented by retrieving textual inputs from multiple knowledge sources <ref type="bibr" target="#b8">[9]</ref>. They also discovered that using diversified prompts (i.e., clustering prompts and selecting representative ones from each cluster), VLLMs could generate more accurate contexts for question answering (QA) readers than using purely the top prompts similar with questions <ref type="bibr" target="#b9">[10]</ref>. The exciting results in QA indicate that the diverse prompts or data examples can be helpful for the VLLMs on text mining tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">VLLM with Lifelong Memory</head><p>We can hardly update the huge number of parameters in VLLMs (i.e., fine-tuning) with any updated information. A memory contains a large collection of text elements (e.g., entities, concepts, noun phrases, relational phrases) and their corresponding representation vectors. Zhang et al. developed the first language model accompanied with the memory architecture to augment its text-to-text generation performance <ref type="bibr" target="#b10">[11]</ref>. This model extracted and used the representations of the elements that were related to text input from the memory. The representations could be efficiently pre-trained or updated with dynamic data. However, none of the existing VLLMs utilized a memory, and so far none of the memories used the lifelong machine learning paradigm that learned continuously, accumulated the knowledge learned in previous tasks or data, and used it to help future learning. Developing a VLLM with lifelong memory is still an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">VLLM with Structured Data</head><p>VLLMs leverage natural language corpora that are derived from the Web. However, natural language text alone represents a limited coverage of knowledge <ref type="bibr" target="#b11">[12]</ref>. Existence of nonfactual information and toxic content in text can eventually cause biases in the models. Alternate sources of information are metadata, data tables, taxonomies, ontologies, and knowledge graphs (KGs), which consist of structured data. For example, KGs are more factual than unstructured data because the information is usually extracted from more trusted sources, and postprocessing filters; and human editors may ensure inappropriate and incorrect content are removed. So, the models that incorporate them can improve factual accuracy <ref type="bibr" target="#b12">[13]</ref> and reduce toxicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Expected Success and Conclusions</head><p>This paper discussed a blue sky idea that is a revolutionized text mining methodology -designing and developing new techniques based on very large language model (VLLM). It presented the significance, potential approaches, and challenges of this idea in various tasks or applications. The expected success is to achieve a significantly higher level of text mining performance than the conventional text mining techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Compared against conventional methodology, VLLM has three advantages in text mining: (1) unifying tasks with no need of designing specialized algorithms (in blue), (2) knowledge from a much larger size of text data and model (in yellow), and (3) ability of performing a task with a small number of data points (in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Representation•Figure 2 :</head><label>2</label><figDesc>Figure 2: Three influential textbooks on mining text data [1, 2, 3] summarize the algorithms and methodologies on five tasks. Very large language model will become an effective, unified methodology for all the tasks. (Blue, green, and orange stars denote what were covered in the corresponding textbooks.)</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mining text data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhai</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining text data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="429" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Text Data Mining</title>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">711</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational autoencoder for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoze</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Kristina Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of knowledge-enhanced text generation</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaitang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Retrieval augmentation for commonsense reasoning: A unified approach</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Iter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10063</idno>
		<title level="m">Generate rather than retrieve: Large language models are strong context generators</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A unified encoder-decoder framework with entity memory</title>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diversifying content generation for commonsense reasoning with mixture of knowledge graph experts</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enhancing factual consistency of abstractive summarization</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hinthorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
