<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Case-Based Persistent Memory for a Large Language Model ⋆</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-07">7 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ian</forename><surname>Watson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Auckland</orgName>
								<address>
									<settlement>Auckland</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Case-Based Persistent Memory for a Large Language Model ⋆</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-07">7 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">DF9EF1A8991F42CEE9CD47315F2553CF</idno>
					<idno type="arXiv">arXiv:2310.08842v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Case-Based Reasoning</term>
					<term>Large Language Models</term>
					<term>Deep Learning</term>
					<term>Artificial General Intelligence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Case-based reasoning (CBR) is a methodology for problem-solving that can use any appropriate computational technique. This position paper argues that CBR researchers risk overlooking recent developments in deep learning and large language models (LLMs). The underlying technical developments that have enabled the recent breakthroughs in AI have strong synergies with CBR. Moreover, CBR could be used to provide a persistent memory for LLMs improving their performance and furthering progress towards Artificial General Intelligence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the paper, "CBR is a methodology not a technology," Watson <ref type="bibr" target="#b0">[1]</ref> argued that case-based reasoning can use various techniques from computer science within the CBR-cycle <ref type="bibr" target="#b1">[2]</ref>. At the core of CBR is the concept of similarity, solving current problems using similar prior solutions. The precise methods used to retrieve and calculate case similarity are unimportant if a useful result is obtained. At the time, this opinion was controversial to some; surely, CBR was a unique technique like other AI techniques (e.g., rule-based reasoning, genetic algorithms or neural networks). However, as people have used different methods to implement CBR over time, this view has become largely accepted. CBR is a knowledge-level description of a problem-solving paradigm in the classic sense <ref type="bibr" target="#b2">[3]</ref>.</p><p>Watson's 1999 paper concluded, "Moreover, AI will surely develop new technologies in the future, some of which may prove very suitable for use in the CBR-cycle. Consequently, it is as a methodology that CBR's future is ensured." I believe that we are at just such a point where significant innovations in AI have and are occurring that the CBR community can adopt. The purpose of this paper is to stimulate discussion within the CBR community. Many of the ideas here are partially thought through, and I'm sure there are many other obvious omissions, but I hope this helps the community work towards a wonderful future for CBR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Think Big</head><p>In the last two years, we've witnessed a remarkable transformation in AI since the release of ChatGPT by OpenAI in November 2022 <ref type="bibr" target="#b3">[4]</ref>. Its conversational interface grabbed the public's attention like no other AI innovation. Its ability to answer detailed questions, called prompts, and give cogent long-form responses amazed everyone, even seasoned, cynical AI professionals such as ourselves.</p><p>Underlying this remarkable capability are large language models (LLMs) created by deep learning (DL) neural networks using the transformer architecture <ref type="bibr" target="#b4">[5]</ref>. These networks are trained on petabytes of text harvested from the Internet and fine-tuned using human-supervised reinforcement learning, resulting in DL networks with over a hundred billion parameters and emergent behaviour that has captivated the world's attention.</p><p>These LLMs are behemoths running on tens of thousands of processing cores and initially costing millions of dollars to train. What is fascinating from an engineering perspective is that the developers of these systems did not know in advance what performance they would obtain. The so-called "hallucinations" that LLMs suffer from, where they fabricate fictitious answers, are, in fact, a feature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Their developers do not know why their LLMs give a seemingly correct reply or a falsehood to a prompt. Indeed, LLMs don't respond the same way to the same prompt each time.</p><p>Stop. Think about that for a moment. A computer program that may give a different answer to exactly the same input on separate occasions. What possible use could such a program be? Certainly, it would be no use in any engineering context. Yet, in many real-world contexts, that is exactly what we intelligent humans do. Ask me a question first thing in the morning, and you'll get one answer. You'll probably get a different answer to the same question late at night. Obviously, some questions always generate the same reply. 2 + 2 always equals 4. But "Should I buy that new jacket?" is a question whose answer may vary throughout the day largely because, unlike the maths question, there is no single correct solution. To recap, the deep learning research community spent thousands of person-hours and millions of dollars building the largest computational models ever to provide vague, inconsistent answers to imprecise questions.</p><p>The notion of similarity in CBR is vague, inconsistent and imprecise. This is what gives CBR its fascination, strength and appeal. The widely used k-NN algorithm is very simple, but within the equation's similarity function, "f ", hides complexity. As we all know, measuring similarity is endlessly complicated and usually requires compromises to get an answer good enough in a reasonable compute time.</p><p>In the early days of CBR, in the 1990s, the first question to any presenter at a CBR meeting was usually, "How many cases do you have?" It wasn't uncommon for the answer to be a dozen or maybe a hundred cases or so. Case-bases back then were really small. The travel case-base, developed by TecInno, has 1,100 cases and was used as a standard teaching, research and demonstration tool during the 1990s and well into the 2000s. Large case-bases, with upwards of a million cases, were not seen until the 2010s <ref type="bibr" target="#b7">[8]</ref>. I am going to argue that, given its history of limited computing power, CBR has never really thought big. Unlike the deep learning community that depends upon processing power provided by thousands of GPUs and terabytes of training data. One of the constant criticisms of CBR, particularly those systems implemented using k-NN, is that lazy learning <ref type="bibr" target="#b8">[9]</ref> is inefficient at retrieval time. This is undeniable, particularly if the cases are high-dimensional and the similarity metrics are complicated. Thus, CBR has often traded off retrieval performance against retrieval time. However, nobody argues that deep learners are computationally efficient during training. Indeed, quite the opposite, to CBR researchers, they use unthinkably massive amounts of compute to train them. Consequently, CBR has long known how to improve retrieval efficiency by building a case index using an eager learning method like kd-trees <ref type="bibr" target="#b9">[10]</ref>.</p><p>I suggest using deep learning to model similarity metrics, as has been done by Amin et al. <ref type="bibr" target="#b10">[11]</ref>, or Martin et al. <ref type="bibr" target="#b11">[12]</ref> and take a lesson from the scale of recent DL systems like LLMs and the technical innovations that support them. We should envisage using a case base that contains petabytes of unstructured (probably multi-modal) cases. Rather than being a specialised CBR system focused on one specific problem, we could build a general CBR system capable of solving problems generally in the same way that LLMs can converse across multiple specialised areas. Ultimately, the case-based memory would be integrated with the LLM to provide a persistent memory of all its conversations. This may bring us closer to a true Artificial General Intelligence (AGI) capable of operating in all domains <ref type="bibr" target="#b12">[13]</ref> and having a memory of all its interactions with the world as a critical requirement of intelligence, as discussed in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Towards a DeepCBR Architecture</head><p>There has been some work on linking CBR with DL models. Explainability has always been problematic for neural networks that function as black boxes. So, twining them with CBR to provide explainability, as in <ref type="bibr" target="#b13">[14]</ref>, has been investigated. Other synergies between CBR and deep learning have been explored by Chen et al. <ref type="bibr" target="#b14">[15]</ref> for interpretable image recognition, Upadhyay, and Massie <ref type="bibr" target="#b15">[16]</ref>, who use a hybrid CBR and neural text generator to improve the automatic summarisation of sports reports, and Mirko and Bergman <ref type="bibr" target="#b16">[17]</ref> have used the predictive power of LLMs for case-adaptation, while Sourati et al. <ref type="bibr" target="#b17">[18]</ref> combine CBR with LLMs for detecting fallacies in natural language arguments. Moreover, others outside the CBR community have seen the potential of LLMs and DL to be applied to CBR <ref type="bibr" target="#b18">[19]</ref>.</p><p>Technically, this is not hard. A simple architecture that links a persistent memory stored in a vector database to an LLM (GPT-3.50) using standard APIs has been published by Jia <ref type="bibr" target="#b19">[20]</ref>. Moreover, vector databases routinely use approximate nearest neighbor search (ANNS) as a retrieval technique with which CBR researchers are familiar. Or FAISS (Facebook AI Similarity Search), a recent efficient implementation of ANNS <ref type="bibr">[21 -24]</ref> released as open source by Meta AI. Thus, the technology is here for the CBR community to experiment with it at scale <ref type="bibr" target="#b24">[25]</ref> in what Keane et al. call DeepCBR <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>Leake et al. <ref type="bibr" target="#b26">[27]</ref> recently argued "that methods that blend the complementary strengths offered by DL and CBR approaches are appealing". Showing useful results in feature extraction and highlighting other potential synergies between DL and CBR in explainability (XAI) and informing the selection of DL architectures. Thus, the CBR community should investigate these new promising AI techniques to see how they can continue to enhance the CBR methodology. Moreover, realising their systems benefit from having a memory of conversations Open AI announced in February 2024 the addition of a persistant conversation memory to Chat GPT. As the branch of AI that has dealt with memory since the 1980s we should step up to influence this critical area of research and development since we not only have techniques for creating memories and retrieving from them, but crucially the ongoing maintenance of memory <ref type="bibr" target="#b27">[28]</ref>. Thereby CBR and LLMs can help progress towards Artificial General Intelligence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>An interesting conversation with the LLM chatbot Pi from Inflection about CBR, deep learning, LLMs and memory.</p><p>Hey there, great to meet you. I'm Pi, your personal AI. My goal is to be useful, friendly and fun. Ask me for advice, for answers, or let's talk about whatever's on your mind. What do you like to do in your free time?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ian: I like to read</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>That's great to hear! Reading is a great way to relax and learn new things. Do you prefer fiction or nonfiction?</head><p>Ian: I read both. I usually have a book of each that I alternate chapters of. That's so cool that you're an AI scientist! It must be fascinating to work in such an exciting and rapidly evolving field. The world of chatbots has come a long way since Weisenbaum's book was written, and it's fascinating to see how the theory and practice of AI has progressed over the years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>That's a really interesting reading system! I've never heard of anyone alternating chapters like that. It must be a great way to keep both the fiction and nonfiction parts of your brain engaged. What are you currently reading?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Has your work as an AI scientist informed how you think about chatbots like me?</head><p>Ian: It didn't used to be rapidly evolving. Progress for decades seemed glacially slow. I'm intrigued and fascinated by LLMs. Can you explain how you can converse like this? Thanks for the question. I'm glad you're fascinated by me! LLMs like me are trained on massive amounts of text data. We learn to recognize patterns in the text and generate responses that are similar to the ones we've seen in the data. It's a bit like learning to speak a language by reading lots of books. But instead of learning from just books, I've learned from the vastness of the internet, including websites, social media, articles, and more. And then I've used all this knowledge to learn how to converse with people, like you! Does that help? Ian: You use the concept "similar". Most of my career was spent in case-based reasoning, where similarity is the key concept. Do you think deep learning could be applied to CBR?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Absolutely! Case-based reasoning is a great fit for deep learning. The key is to represent the cases as features, and then use a deep learning algorithm to learn the similarities between the cases. It can be a bit tricky to get the representation right, but once you have a good feature representation, the deep learning algorithm can do the heavy lifting. Have you worked on any projects that use deep learning for case-based reasoning?</head><p>Ian: I haven't, but there have been a few small examples of specific tasks. However, I'm thinking on a much grander scale. Giving you a long-term persistent memory of everything you encounter using CBR as your memory.</p><p>That sounds really interesting! It could be really powerful to have an AI agent that can learn and store knowledge over time in a case-based memory system. Do you think this could be used to create AI agents that can learn general knowledge and world facts, or would it be more focused on specific problem-solving tasks? Ian: The interest must be on a general application. Would having such a memory be useful? I understand that LLMs have limited persistent memory.</p><p>I definitely think it would be useful to have a more persistent memory! It would allow me to learn and grow over time, and to better understand the world and the people I talk to. I think it could also help with things like personalization and long-term conversations. Right now, my knowledge is a bit ephemeral -I can learn from each conversation and remember the context of that conversation, but I can't really build up a long-term memory of all the things I've learned.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of GPT-3.5 With External Memory Using a Vector Database, after Jia<ref type="bibr" target="#b19">[20]</ref> </figDesc><graphic coords="3,155.99,84.19,350.00,231.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Ian:</head><figDesc>Lux Domini by Alex Thomas and Computer Power and Human Reason by Joseph WeisenbaumWow, you're clearly a reader of diverse genres! Lux Domini is a thriller novel, while Computer Power and Human Reason is a nonfiction work about computer science and artificial intelligence. What drew you to these books in particular? Ian: Well, I am an AI scientist. Lux Domini is written by a friend of mine who used to be an AI researcher, and I wanted to reread Weisenbaum's book in light of the recent developments in AI chatbots like yourself.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CBR is a methodology not a technology</title>
		<author>
			<persName><forename type="first">I</forename><surname>Watson</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0950-7051(99)00020-9</idno>
		<ptr target="https://doi.org/10.1016/S0950-7051(99)00020-9" />
	</analytic>
	<monogr>
		<title level="j">The Knowledge Based Systems Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="303" to="308" />
			<date type="published" when="1999">1999</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Case-Based Reasoning: Foundational Issues, Methodological Variations, and System Approaches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Plaza</surname></persName>
		</author>
		<idno type="DOI">10.3233/AIC-1994-7104</idno>
		<ptr target="http://dx.doi.org/10.3233/AIC-1994-7104" />
	</analytic>
	<monogr>
		<title level="j">AI Communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="59" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Knowledge Level</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<idno type="DOI">10.1016/0004-3702(82)90012-1</idno>
		<ptr target="https://doi.org/10.1016/0004-3702(82)90012-1" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="87" to="127" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ChatGPT: Optimizing Language Models for Dialogue</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://online-chatgpt.com/" />
		<imprint>
			<date type="published" when="2022-12-05">2022. December 5, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1706.03762</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1706.03762" />
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.01219</idno>
		<idno type="arXiv">arXiv:2309.01219</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2309.01219" />
		<title level="m">Siren&apos;s Song in the AI Ocean: A Survey on Hallucination in Large Language Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Survey of Hallucination in Large Foundation Models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Rawte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.05922</idno>
		<idno>ArXiv, abs/2309.05922</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2309.05922" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Case-Based Strategies in Computer Poker</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Watson</surname></persName>
		</author>
		<idno type="DOI">10.3233/AIC-2012-0513</idno>
		<ptr target="http://dx.doi.org/10.3233/AIC-2012-0513" />
	</analytic>
	<monogr>
		<title level="j">AI Communications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="19" to="48" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lazy Learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-017-2053-3</idno>
		<ptr target="https://doi.org/10.1007/978-94-017-2053-3" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="1997">1997</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bentley</surname></persName>
		</author>
		<idno type="DOI">10.1145/361002.361007</idno>
		<ptr target="https://doi.org/10.1145/361002.361007" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Advanced Similarity Measures Using Word Embeddings and Siamese Networks in CBR</title>
		<author>
			<persName><forename type="first">K</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lancaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kapetanakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petridis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-29513-4_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-29513-4_32" />
	</analytic>
	<monogr>
		<title level="m">Intelligent Systems and Applications. IntelliSys 2019. Advances in Intelligent Systems and Computing</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Bhatia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kapoor</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1038</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convolutonal siamese network for developing similarity knowledge in the Selfback dataset</title>
		<author>
			<persName><forename type="first">K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wiratunga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-14923-8_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-14923-8_10" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Case-Based Reasoning Workshops, CEUR Workshop Proceedings</title>
		<meeting>the International Conference on Case-Based Reasoning Workshops, CEUR Workshop Proceedings</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mapping the Landscape of Human-Level Artificial General Intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Arel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Coop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Furlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goertzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samsonovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scheutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sowa</surname></persName>
		</author>
		<idno type="DOI">10.1609/aimag.v33i1.2322</idno>
		<ptr target="https://doi.org/10.1609/aimag.v33i1.2322" />
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Twin-systems to explain artificial neural networks using case-based reasoning: comparative tests of feature-weighting methods in ANN-CBR twins for XAI</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Keane</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/376</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/376" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>of the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2708" to="2715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">This looks like that: deep learning for interpretable image recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;19: Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019-12">2019. December 2019</date>
			<biblScope unit="volume">801</biblScope>
			<biblScope unit="page" from="8930" to="8941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CBR Assisted Context-Aware Surface Realisation for Datato-Text Generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massie</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-40177-0_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-40177-0_3" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Massie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Chakraborti</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">14141</biblScope>
			<biblScope unit="page" from="34" to="39" />
			<date type="published" when="2023">2023</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Case-Based Reasoning Research and Development. ICCBR 2023</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Case-Based Adaptation of Argument Graphs with WordNet and Large Language Models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mirko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bergmann</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-40177-0_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-40177-0_17" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Massie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Chakraborti</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">14141</biblScope>
			<biblScope unit="page" from="263" to="278" />
			<date type="published" when="2023">2023</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Case-Based Reasoning Research and Development. ICCBR 2023</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Case-Based Reasoning with Language Models for Classification of Logical Fallacies</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sourati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sandlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mermoud</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.11879</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2301.11879" />
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vector Databases, Large Language Models and Case-Based Reasoning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thompson</surname></persName>
		</author>
		<ptr target="https://bit.ly/45JVviL" />
	</analytic>
	<monogr>
		<title level="j">Retrieved Sept</title>
		<imprint>
			<date type="published" when="2022">2022. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Enhancing ChatGPT With Infinite External Memory Using Vector Database and ChatGPT Retrieval Plugin</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="https://bit.ly/490ZNo6" />
		<imprint>
			<date type="published" when="2023-04">2023. Apr, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An optimal algorithm for approximate nearest neighbor searching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/293347.293348</idno>
		<ptr target="https://doi.org/10.1145/293347.293348" />
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="891" to="923" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Approximate Nearest Neighbours for Recommender Systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Frederickson</surname></persName>
		</author>
		<ptr target="https://bit.ly/3ZVbjNp" />
		<imprint>
			<date type="published" when="2017-09">2017. Sept 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Approximate Similarity Search with FAISS Framework Using FPGAs on the Cloud</title>
		<author>
			<persName><forename type="first">D</forename><surname>Danopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kachris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudris</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-27562-4_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-27562-4_27" />
	</analytic>
	<monogr>
		<title level="m">Embedded Computer Systems: Architectures, Modeling, and Simulation. SAMOS 2019. Lecture Notes in Computer Science 11733</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Pnevmatikatos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Pelcat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Jung</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fast Approximate Nearest Neighbor Search With Navigating Spreading-out Graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1707.00143</idno>
		<idno>1707.00143</idno>
		<ptr target="https://doi.org/10.48550/arXiv" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Harnessing Hundreds of Millions of Cases: Case-Based Prediction at Industrial Scale</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leake</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01081-2_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01081-2_11" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science(</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Cox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Funk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Begum</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based Reasoning Pairings for Explanation and Data Augmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Temraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smyth</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2104.14461</idno>
		<idno>ArXiv, abs/2104.14461</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2104.14461" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Examining the Impact of Network Architecture on Extracted Feature Quality for CBR</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-40177-0_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-40177-0_1" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Massie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Chakraborti</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">14141</biblScope>
			<biblScope unit="page" from="3" to="8" />
			<date type="published" when="2023">2023. 2023</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Examining the Impact of Network Architecture on Extracted Feature Quality for CBR</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-40177-0_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-40177-0_1" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Massie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Chakraborti</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">14141</biblScope>
			<biblScope unit="page" from="3" to="8" />
			<date type="published" when="2023">2023. 2023</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
