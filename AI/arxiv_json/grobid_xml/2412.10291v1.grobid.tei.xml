<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Still &quot;Talking About Large Language Models&quot;: Some Clarifications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11">November 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
							<email>m.shanahan@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute of Philosophy</orgName>
								<orgName type="department" key="dep2">School of Advanced Study</orgName>
								<orgName type="institution">University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Still &quot;Talking About Large Language Models&quot;: Some Clarifications</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11">November 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">4F095C6952DE11A199F6D25161EBC0D5</idno>
					<idno type="arXiv">arXiv:2412.10291v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>My paper Talking About Large Language Models has more than once been interpreted as advocating a reductionist stance towards large language models. But the paper was not intended that way, and I do not endorse such positions. This short note situates the paper in the context of a larger philosophical project that is concerned with the (mis)use of words rather than metaphysics, in the spirit of Wittgenstein's later writing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In <ref type="bibr">(Shanahan, 2024b)</ref>, I wrote "[a] bare-bones LLM does not really know anything because all it does, at a fundamental level, is sequence prediction". Looking at that sentence in isolation, a reader might be forgiven for assuming that I am taking some sort of reductionist stance according to which an LLM-based chatbot, such as ChatGPT, Claude, or Gemini, is just a next token predictor, where the word "just" here carries great metaphysical weight, and that LLM-based systems therefore do not and cannot have beliefs.<ref type="foot" target="#foot_0">foot_0</ref> There are several other sentences in that paper of a similar kind, and with hindsight, I wish I had taken greater care not to express myself in ways that are so easily open to misreading. So I am grateful for the opportunity, here, to set the record straight. <ref type="foot" target="#foot_1">2</ref>First, and most importantly, I would like to make explicit the overarching philosophical project I see myself as engaged in, which is very much in the spirit of Wittgenstein's later work, exemplified by the Philosophical Investigations <ref type="bibr" target="#b7">(Wittgenstein, 1953)</ref>. Generally speaking, I dislike all philosophical claims of the form X is Y (or X is not Y) where the word "is" carries metaphysical weight. In my 2010 book, Embodiment and the Inner Life, I put the matter rather forcefully: "Such philosophically insidious uses of the existential copula are to be banished" <ref type="bibr">(Shanahan, 2010, p.106)</ref>. In general, I prefer to ask questions about how words are (or should be) used. The upshot of this is that whenever I say "LLMs do not literally have beliefs" (or some such thing), this should be taken as shorthand for "It is not always appropriate to use the word 'belief' (or its relatives) in the context of what an LLM says, even though it would be appropriate if a human being said the same thing" (or something similar). <ref type="foot" target="#foot_2">3</ref>In short, words like "literally", "really", and "just" should not to be taken as hallmarks of a metaphysical pronouncement, and no sentence in <ref type="bibr">(Shanahan, 2024b</ref>) that uses those words should be taken as endorsing a reductionist view of LLM-based systems. However, this leaves plenty of room for debate over what constitutes appropriate versus inappropriate uses of a word. My paper takes a position on this with respect to belief. The strategy of the paper is to consider a hierarchy of increasingly sophisticated LLM-based systems, noting that, as we ascend the hierarchy, it becomes increasingly appropriate to speak of belief without the need for caveats, exceptions, or clarifications.</p><p>At the base of this hierarchy is what I call the "bare-bones" LLM. In the strict sense of the term, a "large language model" is a function that takes as input a sequence of tokens and returns a probability distribution over tokens representing the model's prediction for the next token in the sequence. This is the bare-bones LLM. It is a computational model of the distribution of words in human language, and it doesn't do anything until it is embedded in a larger system, such as a chatbot app. Confusingly, though, in contemporary usage, the term "large language model" or LLM is also used for these larger systems. Hence, people refer to ChatGPT as an LLM, when strictly speaking it is an application built around the core component of an LLM.</p><p>While it seems reasonable to allude to the knowledge encoded in a bare-bones LLM, I do think it is misleading to speak of the beliefs of a completely passive entity. To my mind, the very idea of belief is bound up with behaviour. That is to say, the original context for using the word "belief" -its natural home, so to speak -is living, behaving, active human beings (and other animals), and to use it for a completely passive, inactive, computational entity is to depart too far from the word's original home for comfort. But the bare-bones LLM is hardly an interesting case.</p><p>Far more interesting than the bare-bones LLM is the simple LLM-based conversational agent. <ref type="foot" target="#foot_3">4</ref> We obtain one of these by embedding the bare-bones LLM in an inner loop that, given the transcript of the conversation so far, repeatedly samples from the distribution output by the model to obtain a sequence of words (the agent's response), and an outer, turn-taking loop that alternates between the user's input and the agent's replies. Now we have a system, based on an LLM, that actually does something, and we can speak of its behaviour. Moreover, we have moved a little closer to the natural home of the word "belief". Now suppose the resulting system is very convincing. It is, let us say, a human-level conversationalist. Is it appropriate to use the word "belief" in its full sense, without the need for caveats, exceptions, or clarifications? I don't think so. Not in its full sense. Not at this level in the hierarchy of systems we are ascending.</p><p>On the one hand, it's perfectly natural to speak loosely of such an agent's beliefs. I might say to a colleague, for example, "Oh, ChatGPT knows you're a computer scientist, but it thinks you wrote a paper I've never heard of". In the spirit of Dennett's intentional stance <ref type="bibr" target="#b0">(Dennett, 2009)</ref>, this way of talking helps to make sense of the subsequent con-versation, and is easier to say than "ChatGPT's weights predispose it to emit the string XYZ when prompted with the string ABC". On the other hand, such an agent "cannot participate fully in the human language game of truth because it does not inhabit the world we human language users share" <ref type="bibr">(Shanahan, 2024b, p.73)</ref>. We cannot, for example, ask a simple LLM-based conversational agent whether the pot is full of water. That pot. Look. That one over there. Go and have a look. Is it full or is it empty? The simple LLM-based conversational agent cannot wander over there, peer down, ascertain the status of the pot, and report back to us. Yet this is the sort of primal scene -a scene wherein a person adjusts what they do and say after engaging with the world and finding something out -that I see as the original home of the word "belief" and its relatives.</p><p>However, as we move up the hierarchy of LLM-based systems, layering on more capabilities, the need for caution in using the word "belief" gradually lessens. First, we can consider multi-modal LLMs capable of taking visual as well as textual input. Then we can consider LLMs capable of a wider range of actions than merely issuing textual output, a range that could include retrieving web pages or running Python code, for example. Finally, we can consider embodied (or virtually embodied) LLM-based systems, which take input from a camera mounted on a robot or on an avatar in a 3D games-like environment, and whose repertoire of actions includes controlling a robot's effectors or an avatar's movements.</p><p>With each of these steps, we move closer to the natural home of the word "belief" and its relatives, which is in predicting, explaining, and in general making sense of, the activity of embodied creatures like ourselves, creatures whose behaviour changes in response to what we find out by interacting with the world and the objects it contains. That is the arc of my original paper. The intent was not to take up a metaphysical position with respect to belief, nor to bolster deflationary views of LLM capabilities based on such positions. The aim, rather, was to remind readers of how unlike humans LLM-based systems are, how very differently they operate at a fundamental, mechanistic level, and to urge caution when using anthropomorphic language to talk about them.<ref type="foot" target="#foot_4">foot_4</ref> </p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>See<ref type="bibr" target="#b1">Downes et al. (2024)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Another example of such a sentence is "[A] great many tasks that demand intelligence in humans can be reduced to next-token prediction with a sufficiently performant model"(Shanahan, 2024b, p.68). It would have been better to have written "cast as next-token prediction" rather than "reduced to nexttoken prediction", and not to have expressed the point in a way that seems to preclude describing LLMs as "intelligent".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In(Shanahan, 2024a), I take this approach to consciousness, a far trickier case than belief.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The word here "agent" is itself philosophically fraught. By my own lights, I should perhaps be more cautious in its use. But in the field of AI, the term is used in a lightweight technical sense to mean "anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators"(Russell and Norvig, 2010, p.24), where the environment in question can be a purely textual interface with a human user.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>By way of counterpoint,<ref type="bibr" target="#b6">(Shanahan et al., 2023)</ref> suggests that we can use anthropomorphic language with a degree of impunity if we frame LLM behaviour in terms of role play.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intentional systems theory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Oxford Handbook of Philosophy of Mind</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="339" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">LLMs are not just next token predictors. Inquiry, forthcoming</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Downes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Forber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grzankowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.04666</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Artificial Intelligence: A Modern Approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
	<note>Third Edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<title level="m">Embodiment and the Inner Life: Cognition and Consciousness in the Space of Possible Minds</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simulacra as conscious exotica</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<idno type="DOI">10.1080/0020174X.2024.2434860</idno>
		<ptr target="https://doi.org/10.1080/0020174X.2024.2434860" />
	</analytic>
	<monogr>
		<title level="j">Inquiry</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Talking about large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="68" to="79" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Role play with large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">623</biblScope>
			<biblScope unit="page" from="493" to="498" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Wittgenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Investigations. Basil Blackwell</title>
		<imprint>
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
