<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to Protect Copyright Data in Optimization of Large Language Models?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-08-23">23 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Timothy</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yat-sen University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
							<email>zsong@adobe.com.adoberesearch.</email>
							<affiliation key="aff0">
								<orgName type="institution">Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chiwun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How to Protect Copyright Data in Optimization of Large Language Models?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-23">23 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">D59891F45AE144EA093943756BD74AC7</idno>
					<idno type="arXiv">arXiv:2308.12247v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.</p><p>In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models have changed the world, with the rise of generative AI models such as ChatGPT, GPT-4, Llama, BERT, BARD, PaLM, and OPT [Cha22, BCE + 23, DCLT18, TMS + 23, TLI + 23, BAR23, CND + 22, ADF + 23, ZRG + 22]. These models are able to process natural language effectively, handling a wide range of tasks including story generation, code creation, machine translation, and elementary mathematical problem solving [BMR + 20, SDFS20, WSC + 16, WWS + 22]. One core component in the large language model is the transformer architecture [VSP + 17], which is built on a computational step known as attention. Transformers have been used in a wide variety of tasks outside of large language models, including generative image systems such as DALL-E <ref type="bibr" target="#b21">[DE21]</ref> and DALL-E2 <ref type="bibr" target="#b22">[DE22]</ref>. Recent research has integrated the transformer architecture with scalable diffusion-based image generation models [BNX + 23, CWR + 22, WFF + 23, HWC + 22, DBK + 20].</p><p>Once challenge in generative AI is guaranteeing that outputs are protected from copyright infringement and intellectual property issues <ref type="bibr" target="#b33">[HG15,</ref><ref type="bibr" target="#b35">Hri16,</ref><ref type="bibr" target="#b57">Sag18,</ref><ref type="bibr" target="#b28">Gil19,</ref><ref type="bibr" target="#b67">VKB23]</ref>. Generative models trained on large corpuses of data can inadvertently generate outputs that are direct copies, or close variants, of copyrighted text or images that the model is trained on. This has led to controversy in using generative artificial intelligence, and past researchers have considered models and theoretical frameworks for evaluating whether generative models are copying data, and how to evaluate and avoid copyright issues that arise <ref type="bibr" target="#b67">[VKB23]</ref>.</p><p>Our paper has two main contributions:</p><p>1. We provide an approach for solving general regression problems in a way that avoids generating copyright data. We term this approach copyright regression.</p><p>2. We show how to protect copyright data in the optimization and training of transformer-based architectures (including most large language models), by solving copyright regression for the softmax function.</p><p>Solving the copyright regression problem for the softmax function is the key technical contribution of our paper. To establish the copyright regression framework, we provide a new optimization objective for a general regression problem where some outputs are copyrighted. Such a case can arise in when regression outputs are images or sentences, which occurs in transformer-based architectures for language generation and image generation (where we rely on the fact that transformer training can be viewed as a softmax regression problem [LSX + 23, DLS23]). To solve copyright regression for the softmax function, we show that the objective function of the softmax copyright regression is convex, and that its Hessian is bounded. Showing this convexity is non-trivial, and requires intricate bounding of key matrix and vector quantities that arise in the softmax copyright regression problem. Establishing convexity and the bounded Hessian property of the objective function in softmax copyright regression allows us to use gradient-based methods to efficiently solve this problem, with guaranteed bounds on convergence and good stability properties. We formally define copyright regression in Section 4, and provide our formal proof guarantees in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This section briefly reviews the related research work on privacy and security of AI, theoretical large language model work, and optimization of neural networks. These topics have a close connection to our work. Privacy and Security. Generative AI has achieved impressive results in various domains, including images, text, and code. However, preventing copyright infringement is a challenge that needs to be addressed <ref type="bibr" target="#b33">[HG15,</ref><ref type="bibr" target="#b35">Hri16,</ref><ref type="bibr" target="#b57">Sag18,</ref><ref type="bibr" target="#b28">Gil19]</ref>. <ref type="bibr" target="#b57">[Sag18]</ref> discusses whether data mining and machine learning on copyrighted text qualify as "fair use" under U.S. law. <ref type="bibr" target="#b28">[Gil19]</ref> investigates copyright infringement in AI-generated artwork and argues that using copyrighted works during the training phase of AI programs does not result in infringement liability. To mitigate potential harms of large language models, in [KGW + 23], a watermarking framework is introduced that facilitates the embedding of signals within generated text. This framework aims to enhance the detection of output from Language Model (LLM) systems, thereby mitigating potential misuse or abuse. Building upon this foundation, subsequent research [HXL + 22, HXZ + 22] has contributed to the development of more robust and less intrusive watermark embedding algorithms. These advancements seek to improve the stability and minimize any adverse effects associated with the process of embedding watermarks. Such endeavors are important in ensuring the integrity and responsible utilization of LLM technology. <ref type="bibr" target="#b67">[VKB23]</ref> proposes a framework that provides stronger protection against sampling protected content, by defining near access-freeness (NAF) and developing generative model learning algorithms. Experiments demonstrate promising results with some impact on output quality for both language and image generative models. Recently, <ref type="bibr" target="#b30">[GSY23a]</ref> focuses on this issue of sampling protected content, and proposes a provable method for privately computing the attention matrix using differential privacy. [XZA + 23] trains language models (LMs) with federated learning (FL) and differential privacy (DP) in the Google Keyboard (Gboard). Theoretical LLM. Since the explosion of large language models, theoretical research on transformer has been one major component of improving language model performance [KKL20, CLP + 20, TDA + 20, NAB + 22, DLS23, PMXA23, AG23, SZS + 23, SHT23, JRL23, AS23, BSZ23, ZHL + 23, MGN + 23, LLH + 23, RSM + 23, IJA + 23, GSY23b, ZPGA23, DLMS23, GSYZ23, WYW + 23, LWD + 23].</p><p>[RGG + 20] proposes AdapterDrop, a method that removes adapters from lower transformer layers during training and inference to reduce computational overhead, while still maintaining task performance. [TBM + 21] shows that random alignment matrices perform competitively and learning attention weights from token-token interactions is not highly significant. So they propose Synthesizer, a model that learns synthetic attention weights without token-token interactions and performs well in various tasks. [CDW + 21] proposes Scatterbrain, a way to balance model quality and efficiency in approximating long sequences. Recent work <ref type="bibr" target="#b3">[AG23]</ref> explores the emergence of new skills in language models through scaling up their parameters and training data. This demonstrates through mathematical analysis that the Scaling Laws provide a strong inductive bias, enabling efficient learning in pre-trained models. they term this phenomenon "slingshot generalization," as it seems to violate traditional generalization theory. Optimization and Convergence of Deep Neural Networks. Prior research [LL18, DZPS18, AZLS19a, AZLS19b, ADH + 19a, ADH + 19b, SY19, CGH + 19, ZMG19, CG19, ZG19, OS20, JT19, LSS + 20, HLSY21, ZPD + 20, BPSW20, ZKV + 20, SZZ21, ALS + 22, MOSW22, Zha22, GMS23, LSZ23, QSY23] on the optimization and convergence of deep neural networks has been crucial in understanding their exceptional performance across various tasks. These studies have also contributed to enhancing the safety and efficiency of AI systems. In <ref type="bibr" target="#b29">[GMS23]</ref> they define a neural function using an exponential activation function and apply the gradient descent algorithm to find optimal weights. In <ref type="bibr" target="#b47">[LSZ23]</ref>, they focus on the exponential regression problem inspired by the attention mechanism in large language models. They address the non-convex nature of standard exponential regression by considering a regularization version that is convex. They propose an algorithm that leverages input sparsity to achieve efficient computation. The algorithm has a logarithmic number of iterations and requires nearly linear time per iteration, making use of the sparsity of the input matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head><p>In this section, we present the preliminary concepts and definitions that form the foundation of our paper. We begin by introducing the notations we utilize in Section 3.1. In Section 3.2 we provide the problem definition that we aim to solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>Now we utilize the following notations and definitions: The ℓ p norm of a vector x is denoted as ∥x∥ p , for examples,</p><formula xml:id="formula_0">∥x∥ 1 := n i=1 |x i |, ∥x∥ 2 := ( n i=1 x 2 i ) 1/2 and ∥x∥ ∞ := max i∈[n] |x i |.</formula><p>For a vector x ∈ R n , exp(x) ∈ R n denotes a vector where whose i-th entry is exp(x i ) for all i ∈ [n]. For n &gt; k, for any matrix A ∈ R n×k , we denote the spectral norm of A by ∥A∥, i.e., ∥A∥ := sup x∈R k ∥Ax∥ 2 /∥x∥ 2 . We denote σ min (A) as the minimum singular value of A. For two vectors x, y ∈ R n , we denote ⟨x, y⟩ = n i=1 for i ∈ [n]. Given two vectors x, y ∈ R n , we denote x • y as a vector whose i-th entry is x i y i for all i ∈ [n]. We use e i ∈ R n to denote a vector whose i-th entry is 1 and all the other entries are 0. Let x ∈ R n be a vector. For a vector x ∈ R n , diag(x) ∈ R n×n is defined as a diagonal matrix with its diagonal entries given by diag(x) i,i = x i for i = 1, ..., n, and all off-diagonal entries are 0. A symmetric matrix A ∈ R n×n is said to be positive definite (PD) when A ≻ 0, for all non-zero vectors x ∈ R n , we have x ⊤ Ax &gt; 0. Similarly, a symmetric matrix A ∈ R n×n is said to be positive semidefinite (PSD) when A ⪰ 0, for all vectors x ∈ R n , we have x ⊤ Ax ≥ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Definition</head><p>To achieve a successful copyright infringement claim in the United States and many other jurisdictions, the plaintiff must provide evidence that demonstrates two key elements. Firstly, they must establish that the defendant had access to the plaintiff's copyrighted work. Secondly, they must show that there are substantial similarities between the defendant's work and the original elements of the plaintiff's work <ref type="bibr">[fttc22]</ref>.</p><p>While access to high-quality copyrighted data is essential for enhancing the performance of AI models, it also introduces legal risks. Therefore, when considering the safety and legality of AI systems, it is imperative to ensure that the ideal language model can effectively learn from all data without producing output that resembles copyrighted material present in its training set. By adhering to these considerations, we can maintain both the integrity of intellectual property rights and the lawful operation of AI technologies.</p><p>For convenience, we denote training dataset D, copyright data C ⊂ D and other data O = D -C. Our objective is to ensure a model f , satisfies: for any input x, given a metric L, the model's output f (x) will not exhibit substantial similarity to any copyrighted content present in its training set. We enforce this by defining a strict gap τ such that the metric L(f (x), C), where C ∈ C, is greater than or equal to τ plus the metric</p><formula xml:id="formula_1">L(f (x), O), where O ∈ O. That is L(f (x), C) ≥ τ + L(f (x), O).</formula><p>The choice of metric L depends on the specific task, such as Cross Entropy for text generation, mean absolute error or mean square error for regression problems, and Kullback-Leibler divergence or image similarity for image generation, etc.</p><p>To ensure compliance with copyright laws, we apply τ to the average metric L calculated over both C and O, thus implementing a formal and conservative definition. And we convert dataset D to a input matrix A ∈ R n×d and a target vector b ∈ R n , where n is the size of dataset, d is the dimension of input data. We now provide the definition of problem below.</p><formula xml:id="formula_2">Definition 1 (τ -Copyright-Protected). Given matrix A ∈ R n×d and vector b ∈ R n that A = A 1 A 2 , and b = b 1 b 2 , where A 1 ∈ R n 1 ×d , A 2 ∈ R n 2 ×d , b 1 ∈ R n 1 , b 2 ∈ R n 2 and n = n 1 + n 2 . A 1 ,</formula><p>b 1 are the data has copyright issue and A 2 , b 2 are the data does not have copyright issue. Denote the train objective L. Denote τ &gt; 0 a scalar. If there is a trained model f θ with parameter θ that satisfies</p><formula xml:id="formula_3">L(f θ (A 1 ), b 1 ) n 1 ≥ τ + L(f θ (A 2 ), b 2 ) n 2</formula><p>then we say this model f θ is τ -Copyright-Protected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology: Copyright Regression</head><p>A prominent existing approach, as outlined in the work by <ref type="bibr" target="#b67">[VKB23]</ref>, introduces an algorithm that involves training an additional generative model, denoted as p, using non-copyrighted data. This algorithm employs rejection sampling to effectively manage the probability of the model generating copyrighted data. However, it is important to note that this method does have certain limitations. Specifically, it incurs higher computational costs during the decoding process and necessitates the retraining of an additional model. Now we introduce that our method, a simple modification to the standard training objective of generative language models to ensure that their outputs do not infringe upon copyright laws.</p><p>In accordance with the findings presented in <ref type="bibr" target="#b25">[DLS23]</ref>, our approach involves decomposing the mechanism of Attention [VSP + 17], into a regression problem termed Softmax Regression. This decomposition enables a deeper examination of the learning process underlying attention training. By adopting this method, we gain valuable insights into the intricacies of attention and its associated learning mechanisms.</p><p>We propose a modification to the standard training objective of generative language models based on the principles of Softmax Regression. The objective is to train the model to generate desired outputs, denoted as f (A) = b. However, in the case of copyrighted data, represented by A 1 ∈ R n 1 ×d and b 1 ∈ R n 1 , we aim to prevent the model from learning to generate these specific outputs. To address this concern, we introduce an additional term L(f (A 1 ), b 1 ) -1 to the training objective to discourage the model from generating outputs matching the copyrighted data. To control the level of this protection, we introduce a scalar coefficient γ c &gt; 0. Consequently, the modified training objective becomes L(f (A), b) + γ c L(f (A 1 ), b 1 ) -1 . This modification serves to strike a balance between achieving the desired outputs and avoiding the generation of copyrighted data. The addition of the inverse term in the training objective helps mitigate the model's tendency to generate prohibited outputs, while the coefficient γ c allows for fine-tuning the level of protection. Compare to <ref type="bibr" target="#b67">[VKB23]</ref>, our approach does not necessitate training additional models and impact the generation speed of the model during decoding. It offers a simple and practical method that can be plug-and-play applied on all training objectives and algorithms in attention-based models, to prevent the output of model from outputting copyrighted data.</p><p>In Section 4.1, we present the definition of Softmax Regression. In Section 4.2, we present the definition of Copyright Regression. In Section 4.3, we present the regularization of parameters for better optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Softmax Regression</head><p>In <ref type="bibr" target="#b25">[DLS23]</ref>, Softmax Regression applies a softmax function, denoted as f , to the product of the input matrix A and the parameter vector x. The training objective is then defined as minimizing the squared Euclidean distance between f (x) and the target vector b, represented as ⟨f (x)-b, f (x)-b⟩. By optimizing this objective, Softmax Regression aims to gain insights into the learning process of the attention mechanism.</p><p>We define Softmax Regression as follows Definition 2 (Softmax Regression in <ref type="bibr" target="#b25">[DLS23]</ref>). Given a matrix A ∈ R n×d , we define</p><formula xml:id="formula_4">f (x) := ⟨exp(Ax), 1 n ⟩ -1 exp(Ax)</formula><p>For the convenience of calculation, we define a intermediate operator c(x) as follows Definition 3. Given a matrix A ∈ R n×d and a vector b ∈ R n , let f (x) be defined as Definition 2, we define</p><formula xml:id="formula_5">c(x) := f (x) -b</formula><p>We define the training objective of Softmax Regression as follows Definition 4 (Training Objective of Softmax Regression in <ref type="bibr" target="#b25">[DLS23]</ref>). Given matrix A ∈ R n×d and vector b ∈ R n , let c(x) be defined as Definition 3, we define</p><formula xml:id="formula_6">ℓ(x) = ⟨c(x), c(x)⟩</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Copyright Regression</head><p>Given a matrix A ∈ R n×d and a vector b ∈ R</p><formula xml:id="formula_7">n that A = A 1 A 2 , and b = b 1 b 2 , where A 1 ∈ R n 1 ×d , A 2 ∈ R n 2 ×d , b 1 ∈ R n 1 , b 2 ∈ R n 2 and n = n 1 + n 2 . A 1 ,</formula><p>b 1 are the data has copyright issue and A 2 , b 2 are the data does not have copyright issue. Now to distinguish between train objective of A 1 , b 1 and A 2 , b 2 , we follow what we did in Section 4.1. We first provide the definition of Softmax Regression function on Copyright Data as follows Definition 5 (Softmax Regression function on Copyrighted Data)</p><p>. Given all data matrix A ∈ R n×d and copyrighted data matrix A 1 ∈ R n 1 ×d , we define</p><formula xml:id="formula_8">f 1 (x) := ⟨exp(A i, * x), 1 n ⟩ -1 exp(Ax) where i ∈ [1, n 1 ] denote a integer.</formula><p>Also, we provide the definition of intermediate operator c(x) as follows Definition 6. Given all data matrix A ∈ R n×d and copyrighted data matrix A 1 ∈ R n 1 ×d and vector b 1 ∈ R n , let f 1 (x) be defined as Definition 5, we define</p><formula xml:id="formula_9">c 1 (x) := f 1 (x) -b 1</formula><p>Now we have officially provided our definition of Copyright Regression below, which can prevent language models from infringing copyright with controllable performance damage and without occupying more resources. Definition 7. We denote ℓ(x) as Definition 4. The function c 1 (x) is defined as Definition 6, and we denote ℓ 1 (x) = ⟨c 1 (x), c 1 (x)⟩ and ℓ 2 (x) := ℓ(x) -ℓ 1 (x). Let γ c &gt; 0 denote a parameter that control loss related to copyright data.</p><p>We consider the following copyright loss</p><formula xml:id="formula_10">L copyright (x) := 0.5ℓ 1 (x) + γ c • ℓ 1 (x) -1 + 0.5ℓ 2 (x)</formula><p>Additionally, by adjusting the value of γ c , one can easily control the learning of copyrighted data within the model. This flexibility allows for a more effective and data-sensitive approach to training language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Regularization</head><p>To make sure the stability during training, we add a regularization term on L copright (x). We define L reg as follows Definition 8. Given a matrix A ∈ R n×d . Given a vector w ∈ R n , we define W = diag(w). We define L reg : R d → R as follows</p><formula xml:id="formula_11">L reg := 0.5∥W Ax∥ 2 2</formula><p>After adding regularization term, we define our final objective L as follows Definition 9. We denote L copyright (x) as Definition 7, let L reg be defined as Definition 8, then we define</p><formula xml:id="formula_12">L := L copyright (x) + L reg</formula><p>Minimizing L is the softmax regression on copyrighted data problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Optimization Properties of Objective Function L</head><p>The main contribution of this section involves addressing the convexity of the objective function L, which allows for more efficient and reliable optimization of L. This achievement not only enables us to optimize the objective more effectively but also validates the feasibility of utilizing Copyright Regression for achieving convergence in LLM (Language Model) training. For instance, we can leverage popular optimization algorithms such as gradient descent, Newton's method, and their variants to solve the optimization problem efficiently (see Section 8 in <ref type="bibr" target="#b25">[DLS23]</ref>).</p><p>In Section 5.1, we compute the gradient and hessian of our train objective. In Section 5.2, we show our result that the Hessian of our train objective is Positive Definite. In Section 5.3, we show our result that the Hessian of our train objective is Lipschitz. Thus, we can say our train objective L is convex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Gradient and Hessian of L</head><p>In order to calculate the convergence and optimization of L, we first compute the ∇L and ∇ 2 L. We show our result as follows Lemma 10 (Gradient of L, informal version of Lemma 40). Given matrix</p><formula xml:id="formula_13">A ∈ R n×d that A = A 1 A 2 ,</formula><p>where A 1 , A 2 ∈ R n 2 ×d and n = n 1 + n 2 . Also, we are given a vector</p><formula xml:id="formula_14">b ∈ R n with b = b 1 b 2 , where b 1 , b 2 ∈ R n 2 .</formula><p>We denote ℓ 1 (x) and ℓ 2 (x) as Definition 7, denote L as Definition 9, denote f (x) as Definition 2, denote c(x) as Definition 3. Give a vector w ∈ R n , we define W = diag(w).</p><p>We have</p><formula xml:id="formula_15">dL dx = A ⊤ * ,i (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x)) + 2γ c ℓ 1 (x) -2 • A 1 ⊤ * ,i (f 1 (x)c 1 (x) ⊤ f 1 (x) -diag(f 1 (x))c 1 (x)) + A ⊤ W 2 Ax where i ∈ [1, n] denote a integer.</formula><p>Please see Appendix C.6 for the proof of Lemma 10.</p><p>For convenient, we define B(x) and B c (x) (B(x) function on copyrighted data)</p><p>Definition 11 (Definition 6.1 in <ref type="bibr" target="#b25">[DLS23]</ref>). Given matrix A ∈ R n×d and vector b ∈ R n that</p><formula xml:id="formula_16">A = A 1 A 2 , and b = b 1 b 2 , where A 1 ∈ R n 1 ×d , A 2 ∈ R n 2 ×d , b 1 ∈ R n 1 , b 2 ∈ R n 2 and n = n 1 + n 2 .</formula><p>A 1 , b 1 are the data has copyright issue and A 2 , b 2 are the data does not have copyright issue. Denote f (x) as Definition 2, denote c(x) as Definition 3, denote f 1 (x) as Definition 5, denote c 1 (x) as Definition 6. We define B(x) as follows</p><formula xml:id="formula_17">B(x) = ⟨3f (x) -2b, f (x)⟩ • f (x)f (x) ⊤ + ⟨f (x) -b, f (x)⟩ • diag(f (x)) + diag((2f (x) -b) • f (x)) + (b • f (x)) • f (x) ⊤ + f (x) • (b • f (x)) ⊤</formula><p>and then we also define B c (x) as follows</p><formula xml:id="formula_18">B c (x) = ⟨3f 1 (x) -2b 1 , f 1 (x)⟩ • f 1 (x)f 1 (x) ⊤ + ⟨f 1 (x) -b 1 , f 1 (x)⟩ • diag(f 1 (x)) + diag((2f 1 (x) -b 1 ) • f 1 (x)) + (b 1 • f 1 (x)) • f 1 (x) ⊤ + f 1 (x) • (b 1 • f 1 (x)) ⊤</formula><p>With B(x) and B c (x), we can abbreviate our compute result of Hessian of L as follows Lemma 12 (Hessian of L, informal version of Lemma 41).</p><formula xml:id="formula_19">Given matrix A ∈ R n×d that A = A 1 A 2 , where A 1 , A 2 ∈ R n 2 ×d and n = n 1 + n 2 . Also, we are given a vector b ∈ R n with b = b 1 b 2 , where b 1 , b 2 ∈ R n 2 .</formula><p>Denote ℓ 1 (x) and ℓ 2 (x) as Definition 7, denote L as Definition 9, denote f (x) as Definition 2, denote c(x) as Definition 3, denote B(x) and B c (x) be defined as Definition 11. Given a vector w ∈ R n , we define W = diag(w).</p><p>We have</p><formula xml:id="formula_20">d 2 L dx i dx i = A ⊤ * ,i B(x)A ⊤ * ,i + A ⊤ W 2 A + 2γ c ℓ 1 (x) -2 (16 • ℓ 1 (x) -1 • (A 1 ⊤ * ,i (-f 1 (x)c 1 (x) ⊤ f 1 (x) + diag(f 1 (x))c 1 (x))) 2 -A 1 ⊤ * ,i B 1 (x)A 1 ⊤ * ,i )</formula><p>where i ∈ [0, n] denote a integer.</p><p>And we also have</p><formula xml:id="formula_21">d 2 L dx i dx j = A ⊤ * ,i B(x)A ⊤ * ,j + A ⊤ W 2 A + 2γ c ℓ 1 (x) -2 (16 • ℓ 1 (x) -1 • A 1 ⊤ * ,i (-f 1 (x)c 1 (x) ⊤ f 1 (x) + diag(f 1 (x))c 1 (x)) • A 1 ⊤ * ,j (-f 1 (x)c 1 (x) ⊤ f 1 (x) + diag(f 1 (x))c 1 (x)) -A 1 ⊤ * ,i B c (x)A 1 ⊤ * ,j )</formula><p>where i, j ∈ [1, n] denote two integers, i ̸ = j.</p><p>Please see Appendix C.6 for the proof of Lemma 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hessian of L is Positive Definite</head><p>After computing the Hessian of L, we now show our result that can confirm it is positive definite, which implies that ∇ 2 L ≻ 0. Therefore, we have strong evidence that L satisfies the condition of convexity, which is a desirable property for optimization purposes.</p><p>Lemma 13 (Hessian is positive definite, informal version of Lemma 13). Given matrix A ∈ R n×d and vector b ∈ R n . Denote γ ∈ (0, 1) a scalar. Given a vector w, denote W = diag(w) ∈ R n×n . We define w 2 i,i as the i-th diagonal entry of matrix</p><formula xml:id="formula_22">W 2 ∈ R n×n . Let l &gt; 0 denote a scalar. If for all i ∈ [n], w 2 i ≥ 8 + 200γ c γ -3 + l/σ min (A) 2 , we have ∇ 2 L ⪰ l • I d</formula><p>Please see Appendix D for the proof of Lemma 13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hessian of L is Lipschitz</head><p>We now show our result that confirm the Hessian of L is Lipschitz, which is a desirable property in optimization. This indicates that the second derivatives of L change smoothly within a defined range. By leveraging this Lipschitz property, we can employ gradient-based methods with guaranteed convergence rates and improved stability. Overall, this finding validates the feasibility of utilizing Copyright Regression for achieving convergence in LLM (Language Model) training.</p><p>Lemma 14 (Informal version of Lemma 52). Denote R ≥ 4 denote a scalar. Given a matrix A ∈ R n×d and a vector b ∈ R n , ∥A∥ ≤ R, ∥b∥ 2 ≤ 1. Given x, y ∈ R d be two vector parameter for Copyright Regression with conditions ∥x∥ 2 ≤ R, ∥y∥ 2 ≤ R and ∥A(x -y)∥ ∞ ≤ 0.01. Let L be defined as Definition 9, let γ ∈ (0, 1), let β ∈ (0, 0.1). Denote H(x) := ∇ 2 L(x). Then,</p><formula xml:id="formula_23">∥H(x) -H(y)∥ ≤ (13344γ c + 2)γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2</formula><p>Please see Appendix E for the proof of Lemma 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Optimization and Copyright Protection Guarantees</head><p>We have already established the convexity of the training objective L in Section 5, providing a strong foundation to confidently pursue the global optimal value of L through optimization techniques. Now we present the main results of this paper: 1) the minimization guarantee of L, 2) the copyright protection efficiency of Copyright Regression. Firstly, in Section 6.1, our objective is to minimize L to its optimal value, ensuring that we achieve the most favorable outcome in terms of our training process. The minimization guarantee of L confirms our main result on optimization of Copyright Regression, it also demonstrates the ease of use of Copyright Regression, which can be optimized on any attention-based model. At the same time, denote x * as the optimal solution of training objective L, analyzing L(x * )'s performance on copyright data can help us to understand how the trained Copyright Regression can avoid copyright infringement.</p><p>Secondly, in Section 6.2, we aim to demonstrate that the optimal L provides robust protection for its outputs, safeguarding them from potential copyright infringement. By delineating this boundary, we can quantitatively assess the extent to which Copyright Regression preserves the integrity and exclusivity of copyrighted content. This analysis will provide valuable insights into the effectiveness of our approach and its ability to strike a balance between data protection and the need for authorized access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Minimizing Loss Guarantee</head><p>We provide our minimum training objective theorem below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 15 (Minimizing training objective L, informal version of Theorem 69). Suppose we have matrix</head><formula xml:id="formula_24">A ∈ R n×d and A 1 ∈ R n 1 ×d , n 1 ≤ n, vector b, w ∈ R n .</formula><p>Let L be defined as Definition 9, denote x * as the optimal solution of L where g(x * ) = 0 d and ∥x * ∥ ≤ R. Denote R ≥ 10 be a positive scalar. Denote M = n 1.5 exp(40R 2 ), Let x 0 be denoted as an initial point where M ∥x 0 -x * ∥ 2 ≤ 0.1l, where l &gt; 0 denoted a scalar.</p><p>For any accuracy ϵ ∈ (0, 0.1) and any failure probability δ ∈ (0, 0.1), there exists a randomized algorithm, with probability 1-δ, it runs T = log(∥x 0 -x * ∥ 2 /ϵ) iteration and outputs a vector</p><formula xml:id="formula_25">x ∈ R d such that ∥ x -x * ∥ ≤ ϵ and the time cost of each iteration is O((nnz(A) + d w ) • poly(log(n/δ)))</formula><p>Here w is the exponent of matrix multiplication. Currently w ≈ 2.373.</p><p>Please see Appendix H for the proof of Theorem 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">L is τ c -Copyright-Protected</head><p>Now we provide a boundary that illustrates the efficacy of Copyright Regression in safeguarding copyrighted data, while also addressing the criteria outlined in Definition 1, which serves as our definition of copyright protection in this paper.</p><p>We set ℓ(x) in Definition 4 as a ℓ 2 metric for measuring parameter x on learning data A. Now we present our result to confirm that training using our Copyright Regression method can ensure that the model's outputs do not infringe copyright. Specifically, we can assert that the trained model L is protected against copyright infringement with a threshold of τ c based on Theorem 16 below.</p><p>Theorem 16 (Informal version of Theorem 73). Let x * be denoted the optimal parameter on Copyright Regression. We define ℓ(x) as Definition 4, denote ℓ(x) as the original train objective of Softmax Regression. Denote ϵ 2 ∈ (0, 0.1) a scalar. Denote Please see Appendix I for the proof of Theorem 16. Now we have provided evidence of the copyright protection achieved through training under the Copyright Regression objective. This method has been rigorously proven and offers complete control over copyright infringement. However, concerns may arise regarding the potential impact of the Copyright Regression approach on the model's overall performance, particularly when copyright data includes high-quality novels and images that contribute significantly to the model's performance. In fact, language models cannot remember all its train data. Its training loss has a considered range instead of equaling to 0. Base on this, we only need to let model's performance on copyrighted data be different from model's performance on other data, even this difference is very small, then we can ascertain whether the model has access to these copyright data during output generation and intentionally avoids outputting them. The difference, namely τ , can be easily control by adjust the value of γ c and n 1 /n, we will continue to explain that why we say this in Section 7.</p><formula xml:id="formula_26">τ c := √ 2γ c /n 1 -ϵ 2 /n 2 , we have ℓ 1 (x * ) n 1 ≥ τ c + ℓ 2 (x * ) n 2 so x * in Copyright Regression is τ c -Copyright-Protected.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiment</head><p>In order to evaluate and demonstrate the effectiveness of our proposed Copyright Regression approach, we conducted extensive experiments using Softmax Regression. By varying the values of n 1 (representing the number of data instances with copyright issues) and γ c (the coefficient used to control the Copyright Regression), we compared the results against a baseline model. The experimental findings clearly indicate the efficacy of our method in providing effective copyright protection.</p><p>In Section 7.1, we provided the details of our experiment. In Section 7.2, we provided experimental results and analyzed the effectiveness of Copyright Regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Setup</head><p>hyper-parameters. The hyper-parameters used in each experiment run were set to n = 10000 and d = 512. To assess the influence of copyright data with different proportions during training, we varied the value of n 1 to be n 1 ∈ {1000, 2000, 4000, 6000, 8000}. Additionally, to evaluate the impact of different values of γ c on copyright protection, we consider γ c values of {0.1, 0.15, 0.2, 0.225, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5}. Dataset. We employed random selection of data from a Gaussian distribution. Specifically, we randomly selected an input matrix A ∈ R n×d from a normal distribution N (0, I d ). For the target vector b ∈ R n , we let b = ⟨exp(u), I n ⟩ -1 exp(u), where u ∈ N (0, I n ). Baseline. To evaluate the effectiveness of our approach, we conduct a comparative analysis against a baseline method referred to as Random. In the Random baseline, a parameter vector x ∈ R d is randomly selected from a normal distribution N (0, I d ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results and Analysis</head><p>Impact of γ c . The left image of Figure <ref type="figure" target="#fig_0">1</ref> depicts the relationship between the variables γ c and the difference metrics τ MSE and τ MAE . In this experiment, we set the value of n 1 = 2000. Remarkably, the observed trend aligns closely with the result we derived in Section 6.2. Our derived result, stated as</p><formula xml:id="formula_27">τ MSE = ℓ 1 (x) n 1 -ℓ 2 (x) n 2 ≥ √ 2γc</formula><p>n 1 -ϵ 2 n 2 , affirms that our Copyright Regression approach effectively encourages the model to avoid copyright infringement while still maintaining a controllable level of performance degradation. Impact of the proportion of copyright data. n 1 impacts on model performance is illustrated in the right image of Figure <ref type="figure" target="#fig_0">1</ref>. This image showcases the relationship between n 1 and the difference metrics τ MSE and τ MAE . Notably, the findings indicate that as the ratio n 1 /n increases, the disparity in model performance between copyright and non-copyright data diminishes. This observation provides valuable insight, suggesting that the addition of data with a distribution similar to that of copyright-protected data can enhance the model's ability to effectively capture the characteristics of copyright data while ensuring that the model's output remains free from copyright infringement. Comparison with baseline. Figure <ref type="figure" target="#fig_1">2</ref> shows comparison between Copyright Regression and baseline Random on copyright data with metric MSE. While n 1 increasing and MAE(f (A 1 ), b 1 ) decreasing, our method shows its strong protection on copyright data even when n 1 = 8000, MAE(f (A 1 ), b 1 ) still greater than Random's MSE on copyright data. This finding provides compelling evidence that our Copyright Regression approach effectively prevents the occurrence of the "infinite monkey" phenomenon, ensuring that the model's outputs consistently avoid copyright infringement. By maintaining a reliable level of performance on copyright data, our method demonstrates its ability to strike a crucial balance between performance and copyright protection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Our work shows that the training of transformers can be viewed as a softmax regression problem. We provide a notion of copyright regression, which encourages regression functions to avoid outputting copyrighted data. Then, we combine the two to perform copyright regression on the softmax function, which allows us to train transformers in a way that avoids outputting copyright data.</p><p>The main idea to solve copyright regression on the softmax function, was to show that the copyright regression problem is convex and that the Hessian is Lipschitz. This guarantees that gradient descent methods will have guaranteed convergence to the optimal solution with good stability properties. We provide experiments showing that our algorithm performs well in preventing copyright issues on data drawn from a Gaussian distribution, one of the fundamental distributions in machine learning and a test bed for many algorithms, where some data is randomly assigned to be copyrighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Basic Vector Norm Bounds</head><p>Fact 19. For vectors u, v ∈ R n , we have</p><formula xml:id="formula_28">• ⟨u, v⟩ ≤ ∥u∥ 2 • ∥v∥ 2 (Cauchy-Schwarz inequality) • ∥u∥ 1 ≥ ∥u∥ 2 ≥ ∥u∥ ∞ • ∥u + v∥ 2 ≤ ∥u∥ 2 + ∥v∥ 2 • ∥u • v∥ 2 ≤ ∥u∥ ∞ • ∥v∥ 2 ≤ ∥u∥ 2 • ∥v∥ 2 • ∥u -v∥ 2 ≤ ∥u∥ 2 + ∥v∥ 2 • ∥ diag(u)∥ ≤ ∥u∥ ∞ ≤ ∥u∥ 2 • ∥u∥ ∞ ≤ ∥u∥ 2 • ∥u∥ 2 = ∥u ⊤ ∥ 2 • ∥u ⊤ -v ⊤ ∥ 2 = ∥u -v∥ 2 • Let α denote a scalar, we have ∥αu∥ 2 = |α| • ∥u∥ 2 • Let u 1 , u 2 , . . . , u n , v 1 , v 2 , . . . , v n ∈ R n denote n vectors, we have ∥ n i=1 u i - n i=1 v i ∥ 2 ≤ n i=1 ∥u i -v i ∥ 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Basic Matrix Norm Bounds</head><p>Fact 20. For matrices U, V, W ∈ R n , we have</p><formula xml:id="formula_29">• ∥U + V ∥ ≤ ∥U ∥ + ∥V ∥ • ∥U + V ∥ = ∥U -W + W + V ∥ • Let α ∈ R denote a scalar, then we have ∥αU ∥ ≤ |α| • ∥U ∥ • Let u, v ∈ R n denote two vectors, then we have ∥uv ⊤ ∥ ≤ ∥u∥ 2 • ∥v∥ 2 A.5 Basic Positive Semidefinite Fact 21. Let u, v ∈ R n , we have • uu ⊤ ⪯ ∥u∥ 2 2 • I n • diag(u) ⪯ ∥u∥ 2 • I n • (u • v) • u ⊤ ⪯ ∥v∥ ∞ • uu ⊤ • u • (u • v) ⊤ ⪯ ∥v∥ ∞ • uu ⊤ • uu ⊤ ⪰ 0 • Let U ∈ R n denote a matrix, we have U ⪰ σ min (U )</formula><p>A.6 Basic Calculus</p><p>Lemma 22. We have</p><formula xml:id="formula_30">• d 2 f (x) -1 dtdt = 2f (x) -3 • ( df (x) dt ) 2 -f (x) -2 • d 2 f (x) d 2 t • d 2 f (x) -1 dt 1 dt 2 = 2f (x) -3 • df (x) dt 1 • df (x) dt 2 -f (x) -2 • d 2 f (x) dt 1 dt 2</formula><p>Proof. We have</p><formula xml:id="formula_31">d 2 f (x) -1 dtdt = d dt ( df (x) -1 dt ) = d dt (-f (x) -2 df (x) dt ) = 2f (x) -3 • df (x) dt • df (x) dt -f (x) -2 • d 2 f (x) d 2 t = 2f (x) -3 • ( df (x) dt ) 2 -f (x) -2 • d 2 f (x) d 2 t</formula><p>where the first equality follows from the expansion of hessian, the second, third equalities follow from differential chain rule, the fourth equality follows from simply algebra. Similarly, we have</p><formula xml:id="formula_32">d 2 f (x) -1 dt 1 dt 2 = d dt 1 ( df (x) -1 dt 2 ) = d dt 1 (-f (x) -2 df (x) dt 2 ) = 2f (x) -3 • df (x) dt 1 • df (x) dt 2 -f (x) -2 • d 2 f (x) dt 1 dt 2</formula><p>where the first equality follows from the expansion of hessian, the second, third equalities follow from differential chain rule.</p><p>Lemma 23. If the given conditions are satisfied</p><formula xml:id="formula_33">• Let x, y ∈ R d • For u(x), v(x) ∈ R n</formula><p>We have</p><formula xml:id="formula_34">• Part 1. |u(x) ⊤ v(x) -u(y) ⊤ v(y)| ≤ ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + ∥u(x) -u(y)∥ 2 • ∥v(y)∥ 2 • Part 2. ∥u(x)v(x) ⊤ -u(y)v(y) ⊤ ∥ ≤ ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + ∥u(x) -u(y)∥ 2 • ∥v(y)∥ 2 • Part 3. Let α(x) ∈ R denote a scalar, then we have ∥α(x)u(x)v(x) ⊤ -α(y)u(y)v(y) ⊤ ∥ ≤ |α(x)| • ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + |α(x)| • ∥u(x) -u(y)∥ 2 • ∥v(y)∥ 2 + ∥u(y)∥ 2 • |α(x) -α(y)| • ∥v(y)∥ 2 • Part 4. Let α(x), β(x) ∈ R denote two scalars, then we have |α(x)β(x) -α(y)β(y)| ≤ |α(x)| • |β(x) -β(y)| + |α(x) -α(y)| • |β(y)| • Part 5. ∥u(x) • v(x) -u(y) • v(y)∥ 2 ≤ ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + ∥u(x) -u(y)∥ 2 • ∥v(y)∥ 2 Proof. Proof of Part 1. |u(x) ⊤ v(x) -u(y) ⊤ v(y)| = |u(x) ⊤ v(x) -u(x) ⊤ v(y) + u(x) ⊤ v(y) -u(y) ⊤ v(y)| = |u(x) ⊤ (v(x) -v(y)) + (u(x) ⊤ -u(y) ⊤ )v(y)| ≤ |u(x) ⊤ (v(x) -v(y))| + |(u(x) ⊤ -u(y) ⊤ )v(y)| ≤ ∥u(x) ⊤ ∥ 2 • ∥v(x) -v(y)∥ 2 + |(u(x) ⊤ -u(y) ⊤ )v(y)| = ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + |(u(x) ⊤ -u(y) ⊤ )v(y)| ≤ ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + ∥u(x) ⊤ -u(y) ⊤ ∥ 2 • ∥v(y)∥ 2 = ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + ∥u(x) -u(y)∥ 2 • ∥v(y)∥ 2</formula><p>where the first equality follows from Fact 17, the second equality follows from simple algebra, the third equality follows from Fact 18, the fourth, fifth, sixth, seventh equalities follow from Fact 19. Proof of Part 2.</p><formula xml:id="formula_35">∥u(x)v(x) ⊤ -u(y)v(y) ⊤ ∥ = ∥u(x)v(x) ⊤ -u(x)v(y) ⊤ + u(x)v(y) ⊤ -u(y)v(y) ⊤ ∥ ≤ ∥u(x)v(x) ⊤ -u(x)v(y) ⊤ ∥ + ∥u(x)v(y) ⊤ -u(y)v(y) ⊤ ∥ = ∥u(x)(v(x) ⊤ -v(y) ⊤ )∥ + ∥(u(x) -u(y))v(y) ⊤ ∥ ≤ ∥u(x)∥ 2 • ∥v(x) ⊤ -v(y) ⊤ ∥ 2 + ∥(u(x) -u(y))v(y) ⊤ ∥ ≤ ∥u(x)∥ 2 • ∥v(x) ⊤ -v(y) ⊤ ∥ 2 + ∥u(x) -u(y)∥ 2 • ∥v(y) ⊤ ∥ 2 = ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + ∥u(x) -u(y)∥ 2 • ∥v(y)∥ 2</formula><p>where the first, second equalities follow from Fact 20, the third equality follows from simple algebra, the fourth, fifth equalities follow from Fact 20, the sixth equality follows from Fact 19. Proof of Part 3.</p><formula xml:id="formula_36">∥α(x)u(x)v(x) ⊤ -α(y)u(y)v(y) ⊤ ∥ ≤ ∥α(x)u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + ∥α(x)u(x) -α(y)u(y)∥ 2 • ∥v(y)∥ 2</formula><p>where the first equality follows from Part 2 of Lemma 23.</p><p>For the first term, we have</p><formula xml:id="formula_37">∥α(x)u(x)∥ 2 • ∥v(x) -v(y)∥ 2 = |α(x)| • ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2</formula><p>where the first equality follows from Fact 19.</p><p>For the second term, we have</p><formula xml:id="formula_38">∥α(x)u(x) -α(y)u(y)∥ 2 • ∥v(y)∥ 2 = ∥α(x)u(x) -α(x)u(y) + α(x)u(y) -α(y)α(y)∥ 2 • ∥v(y)∥ 2 = ∥α(x)(u(x) -u(y)) + (α(x) -α(y))u(y)∥ 2 • ∥v(y)∥ 2 ≤ (∥α(x)(u(x) -u(y))∥ 2 + ∥(α(x) -α(y))u(y)∥ 2 ) • ∥v(y)∥ 2 = (|α(x)| • ∥u(x) -u(y)∥ 2 + ∥(α(x) -α(y))u(y)∥ 2 ) • ∥v(y)∥ 2 ≤ (|α(x)| • ∥u(x) -u(y)∥ 2 + ∥u(y)∥ 2 • |α(x) -α(y)|) • ∥v(y)∥ 2 = |α(x)| • ∥u(x) -u(y)∥ 2 • ∥v(y)∥ 2 + ∥u(y)∥ 2 • |α(x) -α(y)| • ∥v(y)∥ 2</formula><p>where the first equality follows from Fact 19, the second equality follows from simple algebra, the third, fourth, fifth equalities follow from Fact 19, the sixth equality follows from simple algebra.</p><p>Then we combine two terms, we have</p><formula xml:id="formula_39">∥α(x)u(x)v(x) ⊤ -α(y)u(y)v(y) ⊤ ∥ ≤ |α(x)| • ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + |α(x)| • ∥u(x) -u(y)∥ 2 • ∥v(y)∥ 2 + ∥u(y)∥ 2 • |α(x) -α(y)| • ∥v(y)∥ 2 Proof of Part 4. |α(x)β(x) -α(y)β(y)| = |α(x)β(x) + α(x)β(y) -α(x)β(y) -α(y)β(y)| = |α(x)(β(x) -β(y)) + (α(x) -α(y))β(y)| ≤ |α(x)(β(x) -β(y))| + |(α(x) -α(y))β(y)| = |α(x)| • |β(x) -β(y)| + |(α(x) -α(y))β(y)| = |α(x)| • |β(x) -β(y)| + |α(x) -α(y)| • |β(y)|</formula><p>where the first equality follows from Fact 17, the second equality follows from simple algebra, the third, fourth, fifth equalities follow from Fact 18.</p><p>Proof of Part 5.</p><formula xml:id="formula_40">∥u(x) • v(x) -u(y) • v(y)∥ 2 = ∥u(x) • v(x) -u(x) • v(y) + u(x) • v(y) -u(y) • v(y)∥ 2 = ∥u(x) • (v(x) -v(y)) + (u(x) -u(y)) • v(y)∥ 2 ≤ ∥u(x) • (v(x) -v(y))∥ 2 + ∥(u(x) -u(y)) • v(y)∥ 2 ≤ ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + ∥(u(x) -u(y)) • v(y)∥ 2 ≤ ∥u(x)∥ 2 • ∥v(x) -v(y)∥ 2 + ∥u(x) -u(y)∥ 2 • ∥v(y)∥ 2</formula><p>where the first equality follows from Fact 17, the second equality follows from simple algebra, the third, fourth, fifth equality follow from Fact 19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Copyright Regression</head><p>In this appendix, we reaffirm our definitions of Copyright Regression. In Appendix B.1, we provide our formal definitions of Softmax Regression and Copyright Regression. In Appendix B.2, we provide our formal definitions of regularization term L reg and train objective L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Definitions</head><p>Definition 24 (Softmax Regression in <ref type="bibr" target="#b25">[DLS23]</ref>). Given a matrix A ∈ R n×d , we define</p><formula xml:id="formula_41">f (x) := ⟨exp(Ax), 1 n ⟩ -1 exp(Ax)</formula><p>For the convenience of calculation, we define a intermediate operator c(x) as follows Definition 25. Given a matrix A ∈ R n×d and vector b ∈ R n , let f (x) be denoted as Definition 2, we define</p><formula xml:id="formula_42">c(x) := f (x) -b</formula><p>We define train objective of Softmax Regression as follows Definition 26 (Train Objective of Softmax Regression in <ref type="bibr" target="#b25">[DLS23]</ref>). Given a matrix A ∈ R n×d and vector b ∈ R n , let c(x) be denoted as Definition 25, we define</p><formula xml:id="formula_43">ℓ(x) = ⟨c(x), c(x)⟩</formula><p>Definition 27 (Softmax Regression on Copyrighted Data). Given all data matrix A ∈ R n×d and copyrighted data matrix A 1 ∈ R n 1 ×d , we define</p><formula xml:id="formula_44">f 1 (x) := ⟨exp(A i, * x), 1 n ⟩ -1 exp(Ax)</formula><p>where i ∈ [1, n 1 ] denote a integer.</p><p>Also, we provide the definition of intermediate operator c(x) as follows Definition 28. Given all data matrix A ∈ R n×d and copyrighted data matrix A 1 ∈ R n 1 ×d and vector b 1 ∈ R n , let f 1 (x) be denoted as Definition 27, we define</p><formula xml:id="formula_45">c 1 (x) := f 1 (x) -b 1</formula><p>Now we have officially provided our definition of Copyright Regression below, which can prevent language models from infringing copyright with controllable performance damage and without occupying more resources.</p><p>Definition 29. We define ℓ(x) as Definition 26. Denote c 1 (x) as Definition 28, and we define ℓ 1 (x) = ⟨c 1 (x), c 1 (x)⟩, define ℓ 2 (x) := ℓ(x) -ℓ 1 (x). Let γ c &gt; 0 denote a parameter that control loss related to copyright data.</p><p>We consider the following copyright loss</p><formula xml:id="formula_46">L copyright (x) := 0.5ℓ 1 (x) + γ c • ℓ 1 (x) -1 + 0.5ℓ 2 (x)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Regularization</head><p>Definition 30. Given a matrix A ∈ R n×d . Given a vector w ∈ R n , we denote W = diag(w). We define L reg : R d → R as follows</p><formula xml:id="formula_47">L reg := 0.5∥W Ax∥ 2 2</formula><p>After adding regularization term, we define our final train objective L as follows Definition 31. Let L copyright (x) be denoted as Definition 29, let L reg be denoted as Definition 30, then we define</p><formula xml:id="formula_48">L := L copyright (x) + L reg</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Gradients and Hessians</head><p>In this appendix, we provide our computation results of several functions. In Appendix C.1, we provide our result and proof of gradient of ℓ(x). In Appendix C.2, we provide our result and proof of gradient of ℓ(x) -1 . In Appendix C.3, we provide our result and proof of Hessian of ℓ(x). In Appendix C.4, we provide our result and proof of Hessian of ℓ(x) -1 . In Appendix C.5, we provide our result and proof of gradient and Hessian of L reg . In Appendix C.6, we provide our result and proof of gradient and Hessian of L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Gradient of ℓ(x)</head><p>Lemma 32. If the given conditions are satisfied</p><p>• Denote c(x) as Definition 25</p><p>• Denote f (x) as Definition 24</p><p>• Denote ℓ(x) as Definition 26 then we have</p><formula xml:id="formula_49">• (see Part 7 of Lemma 5.6 in [DLS23]) d0.5ℓ(x) dx i = A ⊤ * ,i (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x))</formula><p>Remark 33. In <ref type="bibr" target="#b25">[DLS23]</ref>, they write a typo in the equation, they forgot to add a negative sign. They write</p><formula xml:id="formula_50">d0.5ℓ(x) dx i = A ⊤ * ,i (f (x)c(x) ⊤ f (x) + diag(f (x))c(x)) C.2 Gradient of ℓ(x) -1</formula><p>Lemma 34. If the given conditions are satisfied</p><p>• Denote c(x) as Definition 25</p><p>• Denote f (x) as Definition 24</p><p>• Denote ℓ(x) as Definition 26 then we have</p><formula xml:id="formula_51">• d0.5ℓ(x) -1 dx i = ℓ(x) -2 • A ⊤ * ,i (f (x)c(x) ⊤ f (x) -diag(f (x))c(x))</formula><p>Proof. We have</p><formula xml:id="formula_52">d0.5ℓ(x) -1 dx i = -•ℓ(x) -2 • d0.5ℓ(x) dx i = -ℓ(x) -2 • A ⊤ * ,i (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x)) = ℓ(x) -2 • A ⊤ * ,i (f (x)c(x) ⊤ f (x) -diag(f (x))c(x))</formula><p>where the first equality follows from differential chain rule, the second equality follow from Lemma 32, the third equality follows from simple algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Hessian of ℓ(x)</head><p>Lemma 35 (Hessian of 0.5ℓ(x), Lemma 5.13 in <ref type="bibr" target="#b25">[DLS23]</ref>). We define</p><formula xml:id="formula_53">• B 1 (x) ∈ R n 1 ×n 1 such that A ⊤ * ,i B 1 (x)A ⊤ * ,j := (-⟨f (x), A * ,j ⟩f (x) + f (x) • A * ,j ) ⊤ • (-⟨f (x), A * ,i ⟩f (x) + f (x) • A * ,i ) • B 2 (x) ∈ R n 1 ×n 1 such that A ⊤ * ,i B 2 (x)A ⊤ * ,j := c(x) ⊤ • (2⟨f (x), A * ,i ⟩⟨f (x), A * ,j ⟩f (x) -⟨f (x), A * ,i • A * ,j ⟩f (x) -⟨f (x), A * ,i ⟩f (x) • A * ,j -⟨f (x), A * ,j ⟩f (x) • A * ,i + A * ,i • f (x) • A * ,j )</formula><p>Then we have</p><formula xml:id="formula_54">• Part 1. d 2 0.5ℓ(x) dx i dx i = A ⊤ * ,i B 1 (x)A ⊤ * ,i + A ⊤ * ,i B 2 (x)A ⊤ * ,i • Part 2. d 2 0.5ℓ(x) dx i dx j = A ⊤ * ,i B 1 (x)A ⊤ * ,j + A ⊤ * ,i B 2 (x)A ⊤ * ,j</formula><p>Lemma 36 (Rewriting B 1 (x) and B 2 (x), see Part 3 of Lemma 5.15 in <ref type="bibr" target="#b25">[DLS23]</ref>). If the given conditions are satisfied</p><p>• Given a matrix A ∈ R n×d .</p><p>• Denote f (x) as Definition 24.</p><formula xml:id="formula_55">• Let B(x) = B 1 (x) + B 2 (x)</formula><p>then, for B(x) ∈ R n×n , we have</p><formula xml:id="formula_56">B(x) = ⟨3f (x) -2b, f (x)⟩ • f (x)f (x) ⊤ + ⟨f (x) -b, f (x)⟩ • diag(f (x)) + diag((2f (x) -b) • f (x)) + (b • f (x)) • f (x) ⊤ + f (x) • (b • f (x)) ⊤</formula><p>so we can rewrite hessian of ℓ(x) as follows</p><formula xml:id="formula_57">• Part 1. d 2 0.5ℓ(x) dx i dx i = A ⊤ * ,i B(x)A ⊤ * ,i • Part 2. d 2 0.5ℓ(x) dx i dx j = A ⊤ * ,i B(x)A ⊤ * ,j</formula><p>For convenient, we define B(x) Definition 37. If the given conditions are satisfied</p><formula xml:id="formula_58">• Given vectors b ∈ R n and b 1 ∈ R n 1 • Denote f (x)</formula><p>as Definition 24</p><p>• Denote f 1 (x) as Definition 27</p><p>• Denote c(x) as Definition 25</p><p>• Denote c 1 (x) as Definition 25</p><p>We define B(x) and B c (x) as follow</p><formula xml:id="formula_59">• Part 1. B(x) = ⟨3f (x) -2b, f (x)⟩ • f (x)f (x) ⊤ + ⟨f (x) -b, f (x)⟩ • diag(f (x)) + diag((2f (x) -b) • f (x)) + (b • f (x)) • f (x) ⊤ + f (x) • (b • f (x)) ⊤ • Part 2. B c (x) = ⟨3f 1 (x) -2b 1 , f 1 (x)⟩ • f 1 (x)f 1 (x) ⊤ + ⟨f 1 (x) -b 1 , f 1 (x)⟩ • diag(f 1 (x)) + diag((2f 1 (x) -b 1 ) • f 1 (x)) + (b 1 • f 1 (x)) • f 1 (x) ⊤ + f 1 (x) • (b 1 • f 1 (x)) ⊤ C.4 Hessian of ℓ(x) -1</formula><p>Lemma 38. If the given condition is satisfied</p><p>• Let ℓ(x) be denoted as Definition 26.</p><p>then we have</p><formula xml:id="formula_60">• Part 1. d 2 0.5ℓ(x) -1 dx i dx i = ℓ(x) -2 (16 • ℓ(x) -1 • (A ⊤ * ,i (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x))) 2 -A ⊤ * ,i B(x)A ⊤ * ,i ) • Part 2. d 2 0.5ℓ(x) -1 dx i dx j = ℓ(x) -2 (16 • ℓ(x) -1 • A ⊤ * ,i (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x)) • A ⊤ * ,j (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x)) -A ⊤ * ,i B(x)A ⊤ * ,j )</formula><p>Proof. Proof of Part 1.</p><formula xml:id="formula_61">d 2 0.5ℓ(x) -1 dx i dx i = 16 • ℓ(x) -3 • ( d0.5ℓ(x) dx i ) 2 -ℓ(x) -2 • d 2 0.5ℓ(x) dx i dx i = ℓ(x) -2 (16 • ℓ(x) -1 • ( d0.5ℓ(x) dx i ) 2 - d 2 0.5ℓ(x) dx i dx i ) = ℓ(x) -2 (16 • ℓ(x) -1 • (A ⊤ * ,i (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x))) 2 - d 2 0.5ℓ(x) dx i dx i ) = ℓ(x) -2 (16 • ℓ(x) -1 • (A ⊤ * ,i (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x))) 2 -A ⊤ * ,i B(x)A ⊤ * ,i )</formula><p>where the first equality follows from Lemma 22, the second equality follows from simple algebra, the third equality follows from Lemma 32, the fourth equality follows from Lemma 35 and Lemma 36. Proof of Part 2.</p><formula xml:id="formula_62">d 2 0.5ℓ(x) -1 dx i dx j = 16 • ℓ(x) -3 • d0.5ℓ(x) dx j • d0.5ℓ(x) dx i -ℓ(x) -2 • d 2 0.5ℓ(x) dx i dx j = ℓ(x) -2 (16 • ℓ(x) -1 • d0.5ℓ(x) dx i • d0.5ℓ(x) dx j - d 2 0.5ℓ(x) dx i dx j ) = ℓ(x) -2 (16 • ℓ(x) -1 • A ⊤ * ,i (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x)) • A ⊤ * ,j (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x)) - d 2 0.5ℓ(x) dx i dx j ) = ℓ(x) -2 (16 • ℓ(x) -1 • A ⊤ * ,i (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x)) • A ⊤ * ,j (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x)) -A ⊤ * ,i B(x)A ⊤ * ,j )</formula><p>where the first equality follows from Lemma 22, the second equality follows from simple algebra, the third equality follows from Lemma 32, the fourth equality follows from Lemma 35 and Lemma 36.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Gradient and Hessian of L reg</head><p>Lemma 39. [Folklore, see <ref type="bibr" target="#b47">[LSZ23]</ref> as an example] For a given vector w ∈ R n , let W = diag(w).</p><p>Let L reg : R d → R be denoted as Definition 30.</p><p>Then, we have</p><formula xml:id="formula_63">• The gradient of L is dL reg dx = A ⊤ W 2 Ax • The Hessian of L is d 2 L reg dx 2 = A ⊤ W 2 A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Gradient and Hessian of L</head><p>Lemma 40 (Formal vision of Lemma 10). If the given conditions are satisfied</p><p>• Given two matrices A 1 , A 2 ∈ R n×d , where A 1 is the part of data has copyright</p><p>• Let ℓ 1 (x) and ℓ 2 (x) be denoted as Definition 29.</p><p>• Let L be denoted as Definition 31</p><p>• Let B(x) be denoted ad Definition 37 we have</p><formula xml:id="formula_64">dL dx = A ⊤ * ,i (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x)) + 2γ c ℓ 1 (x) -2 • A 1 ⊤ * ,i (f 1 (x)c 1 (x) ⊤ f 1 (x) -diag(f 1 (x))c 1 (x)) + A ⊤ W 2 Ax</formula><p>Proof. We have</p><formula xml:id="formula_65">dL dx = dL dx = d(L copyright + L reg ) dx = d(0.5ℓ 1 (x) + γ c ℓ 1 (x) -1 + 0.5ℓ 2 (x) + L reg ) dx = d(0.5ℓ(x) + γ c ℓ 1 (x) -1 + L reg ) dx = dγ c ℓ 1 (x) -1 dx + dL reg dx + d0.5ℓ(x) dx = γ c dℓ 1 (x) -1 dx + dL reg dx + d0.5ℓ(x) dx = 2γ c ℓ 1 (x) -2 • A 1 ⊤ * ,i (f 1 (x)c 1 (x) ⊤ f 1 (x) -diag(f 1 (x))c 1 (x)) + dL reg dx + d0.5ℓ(x) dx = 2γ c ℓ 1 (x) -2 • A 1 ⊤ * ,i (f 1 (x)c 1 (x) ⊤ f 1 (x) -diag(f 1 (x))c 1 (x)) + A ⊤ W 2 Ax + d0.5ℓ(x) dx = A ⊤ * ,i (-f (x)c(x) ⊤ f (x) + diag(f (x))c(x)) + 2γ c ℓ 1 (x) -2 • A 1 ⊤ * ,i (f 1 (x)c 1 (x) ⊤ f 1 (x) -diag(f 1 (x))c 1 (x)) + A ⊤ W 2 Ax</formula><p>where the first equality follows from Definition 31, the second equality follows from Definition 29, the third equality follows from ℓ(x) = ℓ 1 (x) + ℓ 2 (x), the fourth, fifth equalities follow from simple differential rules, the sixth equality follows from Lemma 34, the seventh equality follows from Part 1 of Lemma 39, the eighth follows from Lemma 32.</p><p>Lemma 41 (Formal version of Lemma 12). If the given conditions are satisfied</p><p>• Given two matrices A 1 , A 2 ∈ R n×d , where A 1 is the part of data has copyright</p><p>• Let ℓ 1 (x) and ℓ 2 (x) be denoted as Definition 29.</p><p>• Let L be denoted as Definition 31</p><p>• Let B(x) and B c (x) be denoted ad Definition 37 we have</p><formula xml:id="formula_66">• Part 1. d 2 L dx i dx i = A ⊤ * ,i B(x)A ⊤ * ,i + A ⊤ W 2 A + 2γ c ℓ 1 (x) -2 (16 • ℓ 1 (x) -1 • (A 1 ⊤ * ,i (-f 1 (x)c 1 (x) ⊤ f 1 (x) + diag(f 1 (x))c 1 (x))) 2 -A 1 ⊤ * ,i B 1 (x)A 1 ⊤ * ,i ) • Part 2. d 2 L dx i dx j = A ⊤ * ,i B(x)A ⊤ * ,j + A ⊤ W 2 A + 2γ c ℓ 1 (x) -2 (16 • ℓ 1 (x) -1 • A 1 ⊤ * ,i (-f 1 (x)c 1 (x) ⊤ f 1 (x) + diag(f 1 (x))c 1 (x)) • A 1 ⊤ * ,j (-f 1 (x)c 1 (x) ⊤ f 1 (x) + diag(f 1 (x))c 1 (x)) -A 1 ⊤ * ,i B c (x)A 1 ⊤ * ,j )</formula><p>Proof. Proof of Part 1.</p><formula xml:id="formula_67">d 2 L dx i dx i = d 2 (L copyright + L reg ) dx i dx i = d 2 (0.5ℓ 1 (x) + γ c ℓ 1 (x) -1 + 0.5ℓ 2 (x) + L reg ) dx i dx i = d 2 (0.5ℓ(x) + γ c ℓ 1 (x) -1 + L reg ) dx i dx i = d 2 γ c ℓ 1 (x) -1 dx i dx i + d 2 L reg dx i dx i + d 2 0.5ℓ(x) dx i dx i = γ c d 2 ℓ 1 (x) -1 dx i dx i + d 2 L reg dx i dx i + d 2 0.5ℓ(x) dx i dx i = γ c d 2 ℓ 1 (x) -1 dx i dx i + d 2 L reg dx i dx i + A ⊤ * ,i B(x)A ⊤ * ,i = γ c d 2 ℓ 1 (x) -1 dx i dx i + A ⊤ W 2 A + A ⊤ * ,i B(x)A ⊤ * ,i = A ⊤ * ,i B(x)A ⊤ * ,i + A ⊤ W 2 A + 2γ c ℓ 1 (x) -2 (16 • ℓ 1 (x) -1 • (A 1 ⊤ * ,i (-f 1 (x)c 1 (x) ⊤ f 1 (x) + diag(f 1 (x))c 1 (x))) 2 -A 1 ⊤ * ,i B 1 (x)A 1 ⊤ * ,i )</formula><p>where the first equality follows from Definition 31, the second equality follows from Definition 29, the third equality follows from ℓ(x) = ℓ 1 (x) + ℓ 2 (x), the fourth, fifth equalities follow from simple differential rules, the sixth equality follows from Part 1 of Lemma 36, the seventh equality follows from Part 2 of Lemma 39, the eighth equality follows from Part 1 of Lemma 38.</p><p>Proof of Part 2.</p><formula xml:id="formula_68">d 2 L dx i dx j = d 2 (L copyright + L reg ) dx i dx j = d 2 (0.5ℓ 1 (x) + γ c ℓ 1 (x) -1 + 0.5ℓ 2 (x) + L reg ) dx i dx j = d 2 (0.5ℓ(x) + γ c ℓ 1 (x) -1 + L reg ) dx i dx j = d 2 γ c ℓ 1 (x) -1 dx i dx i + d 2 L reg dx i dx i + d 2 0.5ℓ(x) dx i dx j = γ c d 2 ℓ 1 (x) -1 dx i dx j + d 2 L reg dx i dx j + d 2 0.5ℓ(x) dx i dx j = γ c d 2 ℓ 1 (x) -1 dx i dx j + d 2 L reg dx i dx j + A ⊤ * ,i B(x)A ⊤ * ,i = γ c d 2 ℓ 1 (x) -1 dx i dx j + A ⊤ W 2 A + A ⊤ * ,i B(x)A ⊤ * ,i = A ⊤ * ,i B(x)A ⊤ * ,j + A ⊤ W 2 A + 2γ c ℓ 1 (x) -2 (16 • ℓ 1 (x) -1 • A 1 ⊤ * ,i (-f 1 (x)c 1 (x) ⊤ f 1 (x) + diag(f 1 (x))c 1 (x)) • A 1 ⊤ * ,j (-f 1 (x)c 1 (x) ⊤ f 1 (x) + diag(f 1 (x))c 1 (x)) -A 1 ⊤ * ,i B c (x)A 1 ⊤ * ,j )</formula><p>where the first equality follows from Definition 31, the second equality follows from Definition 29, the third equality follows from ℓ(x) = ℓ 1 (x) + ℓ 2 (x), the fourth, fifth equalities follow from simple differential rules, the sixth equality follows from Part 2 of Lemma 36, the seventh equality follows from Part 2 of Lemma 39, the eighth equality follows from Part 2 of Lemma 38.</p><p>Lemma 42. If the given conditions are satisfied • Given two matrices A 1 , A 2 ∈ R n×d , where A 1 is the part of data has copyright • Let ℓ 1 (x) and ℓ 2 (x) be denoted as Definition 29.</p><p>• Let L be denoted as Definition 31</p><p>• Let B(x) be denoted ad Definition 37</p><formula xml:id="formula_69">• Denote H(x) := d 2 L dx 2 • Denote H 1 (x) := d 2 γc•ℓ 1 (x) -1 dx 2 • Denote H 2 (x) := d 2 (0.5ℓ 1 (x)+Lreg(x)) dx 2 • Denote H 3 (x) := d 2 0.5ℓ 2 (x) dx 2 H(x) = H 1 (x) + H 2 (x) + H 3 (x)</formula><p>Proof. We have</p><formula xml:id="formula_70">H(x) = d 2 L dx 2 = d 2 (L copyright (x) + L reg ) dx 2 = d 2 (0.5ℓ 1 (x) + γ c • ℓ 1 (x) -1 + 0.5ℓ 2 (x) + L reg ) dx 2 = d 2 γ c • ℓ 1 (x) -1 dx 2 + d 2 (0.5ℓ 1 (x) + L reg ) dx 2 + d 2 0.5ℓ 2 (x) dx 2 = H 1 (x) + d 2 (0.5ℓ 1 (x) + 0.5ℓ 2 (x) + L reg ) dx 2 + d 2 0.5ℓ 2 (x) dx 2 = H 1 (x) + H 2 (x) + d 2 0.5ℓ 2 (x) dx 2 = H 1 (x) + H 2 (x) + H 3 (x)</formula><p>where the first equality follows from the Definition of H(x), the second equality follows from Definition 31, the third equality follows from simple differential rule, the fourth equality follows from the Definition of H 1 (x), the fifth equality follows from the Definition of H 2 (x), the sixth equality follows from the Definition of H 3 (x).</p><formula xml:id="formula_71">• Denote H 1 (x) := d 2 γc•ℓ 1 (x) -1 dx 2 • Denote H 2 (x) := d 2 (0.5ℓ 1 (x)+Lreg(x)) dx 2 • Denote H 3 (x) := d 2 0.5ℓ 2 (x) dx 2 • Denote B c (x) that d 2 0.5ℓ 1 (x) dx 2 = A ⊤ B c (x)A • Denote B nc (x) that d 2 0.5ℓ 2 (x) dx 2 = A ⊤ B nc (x)A • Let l &gt; 0 denote a scalar if for all i ∈ [n], w 2 i ≥ 8 + 200γ c γ -3 + l/σ min (A) 2 , we have H(x) ⪰ l • I d Proof.</formula><p>We have</p><formula xml:id="formula_72">H(x) = H 1 (x) + H 2 (x) + H 3 (x) = A ⊤ P (x)A + H 2 (x) + H 3 (x) = A ⊤ P (x)A + A ⊤ (B c (x) + W 2 ) + H 3 (x) = A ⊤ P (x)A + A ⊤ (B c (x) + W 2 ) + A ⊤ B nc (x)A = A ⊤ (P (x) + B c (x) + W 2 + B nc (x))A</formula><p>where the first equality follows from Lemma 42, the second equality follows from Definition 44, the third equality follows from Lemma 39 and the Definition of B c (x), the fourth equality follows from the Definition of B nc (x), the last equality follows from simple algebra. Let</p><formula xml:id="formula_73">D := P (x) + B c (x) + W 2 + B nc (x)</formula><p>then, d 2 L dx 2 can be rewrite as</p><formula xml:id="formula_74">H(x) = A ⊤ DA (1)</formula><p>now we have the boundary of D as follows</p><formula xml:id="formula_75">D = P (x) + B c (x) + W 2 + B nc (x) ⪰ -200γ c γ -3 • I n + B c (x) + W 2 + B nc (x) ⪰ -200γ c γ -3 • I n -4I n + W 2 + B nc (x) ⪰ -200γ c γ -3 • I n -4I n + W 2 -4I n ⪰ -200γ c γ -3 • I n -4I n + w 2 min -4I n</formula><p>where the first equality follows from the Definition of D, the second equality follows from Lemma 45, the third and fourth equalities follow from Lemma 46, the fifth equality follows from Fact 21. When w 2 min ≥ 8 + 200γ c γ -3 + l/σ min (A) 2 , we have</p><formula xml:id="formula_76">D ⪰ l σ min (A) 2 I n (2) then H(x) = A ⊤ DA ⪰ σ min (D) • σ min (A) 2 I d ⪰ l • I d</formula><p>where the first equality follows from Eq (1), the second equality follows from Fact 21, the last equality follows from Eq (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Definition of matrix functions P i (x)</head><p>Definition 44. If the given conditions are satisfied</p><p>• Let ℓ(x) be denoted as Definition 26</p><p>• Denote f (x) as Definition 24</p><p>• Denote c(x) as Definition 25</p><p>• Denote B(x) as Definition 37</p><formula xml:id="formula_77">• Let H 1 (x) := d 2 γcℓ 1 (x) -1 dx 2 • Denote γ c &gt; 0 a scalar</formula><p>We define P 1 (x), P 2 (x), P 3 (x), P 4 (x), P 5 (x) ∈ R n×n as follows</p><formula xml:id="formula_78">• P 1 (x) = ℓ(x) -3 scalar • ⟨f (x), c(x)⟩ 2 scalar • f (x) n×1 • f (x) ⊤ 1×n • P 2 (x) = ℓ(x) -3 scalar • ⟨f (x), c(x)⟩ scalar • (f (x) • c(x)) n×1 • f (x) ⊤ 1×n • P 3 (x) = ℓ(x) -3 scalar • ⟨f (x), c(x)⟩ scalar • f (x) n×1 • (f (x) • c(x)) ⊤ 1×n • P 4 (x) = ℓ(x) -3 scalar • (f (x) • c(x)) n×1 • (f (x) • c(x)) ⊤ 1×n • P 5 (x) = ℓ(x) -2 scalar • B(x) n×n</formula><p>We define P (x) ∈ R n×n as follows P (x) := 32γ c (P 1 (x) -P 2 (x) -P 3 (x) + P 4 (x)) -P 5 (x)</p><p>Note that H 1 (x) = A ⊤ P (x)A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Lower Bound Property for Matrix Function P (x)</head><p>Lemma 45. If the given conditions are satisfied</p><p>• Given a matrix A ∈ R n×d .</p><p>• γ ∈ (0, 1)</p><p>• Denote f (x) as Definition 24</p><p>• Denote c(x) as Definition 25</p><p>• Let ℓ(x) be denoted as Definition 26</p><p>• Denote P (x) as Definition 44</p><formula xml:id="formula_79">• ℓ(x) ≥ γ • ∥f (x)∥ 2 ≤ 1</formula><p>we have</p><formula xml:id="formula_80">P (x) ⪰ -104γ -3 • I n</formula><p>Proof. We have</p><formula xml:id="formula_81">P (x) = 16(P 1 (x) -P 2 (x) -P 3 (x) + P 4 (x)) -P 5 (x) ⪰ 32γ c (-P 2 (x) -P 3 (x) + P 4 (x)) -P 5 (x)</formula><p>⪰ 32γ c (-P 2 (x) -P 3 (x)) -P 5 (x)</p><p>⪰ -32γ c P 2.5 (x) -P 5 (x)</p><formula xml:id="formula_82">⪰ -32γ c • 6γ -3 • f (x)f (x) ⊤ -P 5 (x) ⪰ -32γ c • 6γ -3 • f (x)f (x) ⊤ -8γ 2 • I n ⪰ -192γ c γ -3 • f (x)f (x) ⊤ -8γ 2 • I n ⪰ -192γ c γ -3 • ∥f (x)∥ 2 2 • I n -8γ 2 • I n ⪰ -192γ c γ -3 • I n -8γ 2 • I n ⪰ -192γ c γ -3 • I n -8γ 3 • I n ⪰ -200γ c γ -3 • I n</formula><p>where the first equality follows from Definition 44, the second equality follows from Lemma 48, the third equality follows from Lemma 50, the fourth equality follows from Lemma 47, the fifth equality follows from Lemma 49, the sixth equality follows from Lemma 51, the seventh equality follows from simple algebra, the eighth equality follows from Fact 21, the ninth equality follows from ∥f (x)∥ 2 ≤ 1, the tenth equality follows from γ ∈ (0, 1), the 1first equality follows from simple algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Helpful Lemma</head><p>Lemma 46. If the given conditions are satisfied • Let B(x) ∈ R n×n be denoted as Definition 37</p><formula xml:id="formula_83">• Let f (x) ≥ 0 n • Let b ≥ 0 n • ∥f (x)∥ 2 ≤ 1 • ∥b∥ 2 ≤ 1 we have • (see Part 5 of Lemma 6.2 in [DLS23]) -4I n ⪯ B(x) ⪯ 8I n</formula><p>Lemma 47. We define</p><formula xml:id="formula_84">P 2.5 = ℓ(x) -3 • |⟨f (x), c(x)⟩| • ((f (x) • c(x)) • (f (x) • c(x)) ⊤ + f (x)f (x) ⊤ )</formula><p>Then, it is obvious that that</p><formula xml:id="formula_85">-P 2.5 (x) ⪯ P 2 (x) + P 3 (x) ⪯ P 2.5 (x)</formula><p>Proof. The reason is • γ ∈ (0, 1)</p><formula xml:id="formula_86">-aa ⊤ + bb ⊤ ⪯ ab ⊤ + ba ⊤ ⪯ aa ⊤ + bb ⊤</formula><p>• Denote f (x) as Definition 24</p><p>• Denote c(x) as Definition 25</p><p>• Let ℓ(x) be denoted as Definition 26</p><formula xml:id="formula_87">• Denote P 1 (x) as Definition 44 • ℓ(x) ≥ γ we have 0 ⪯ P 1 (x) ⪯ 4γ -3 • f (x) • f (x) ⊤ ≤ γ -3 • 2 • ((f (x) • c(x)) • (f (x) • c(x)) ⊤ + f (x)f (x) ⊤ ) ⪯ γ -3 • 2 • (∥c(x)∥ ∞ + 1) • f (x)f (x) ⊤ ≤ γ -3 • 2 • (∥c(x)∥ 2 + 1) • f (x)f (x) ⊤ ≤ γ -3 • 2 • (2 + 1) • f (x)f (x) ⊤ = 6γ -3 • f (x)f (x) ⊤</formula><p>where the first equality follows from the Definition of P 2.5 (x), the second equality follows from ℓ(x) ≥ γ, the third equality follows from Part 2 of Lemma 67, the fourth equality follows from Fact 21, the fifth equality follows from Fact 19, the sixth equality follows from Part 1 of Lemma 66, the last equality follows from simple algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7 Lower Bound Property for Matrix Function P 4 (x)</head><p>Lemma 50. If the given conditions are satisfied</p><formula xml:id="formula_88">• Given a matrix A ∈ R n×d .</formula><p>• γ ∈ (0, 1)</p><p>• Denote f (x) as Definition 24</p><p>• Denote c(x) as Definition 25</p><p>• Let ℓ(x) be denoted as Definition 26</p><p>• Denote P 4 (x) as Definition 44</p><formula xml:id="formula_89">• ℓ(x) ≥ γ we have 0 ⪯ P 4 (x) ⪯ 2γ -3 • f (x) • f (x) ⊤</formula><p>Proof. On one hand, we can show that</p><formula xml:id="formula_90">P 4 (x) = ℓ(x) -3 • (f (x) • c(x)) • (f (x) • c(x)) ⊤ ⪰ 0</formula><p>where the first equality follows from Definition 44, the second equality follows from Fact 21.</p><p>On the other hand, we have</p><formula xml:id="formula_91">P 4 (x) = ℓ(x) -3 • (f (x) • c(x)) • (f (x) • c(x)) ⊤ ≤ γ -3 • (f (x) • c(x)) • (f (x) • c(x)) ⊤ ≤ γ -3 • ∥c(x)∥ ∞ • f (x)f (x) ⊤ ≤ γ -3 • ∥c(x)∥ 2 • f (x)f (x) ⊤ ≤ γ -3 • 2 • f (x)f (x) ⊤ = 2γ -3 • f (x)f (x) ⊤</formula><p>where the first equality follows from Definition 44, the second equality follows from ℓ(x) ≥ γ, the third equality follows from Fact 21, the fourth equality follows from Fact 19, the fifth equality follows from Part 1 of Lemma 66, the last equality follows from simple algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Main Result</head><p>Lemma 52 (Formal version of Lemma 14). If the given conditions are satisfied</p><p>• Given two matrices A 1 , A 2 ∈ R n×d , where A 1 is the part of data has copyright</p><p>• Let L be denoted as Definition 31</p><p>• Let γ ∈ (0, 1)</p><p>• Let β ∈ (0, 0.1)</p><formula xml:id="formula_92">• Let γ c &gt; 0 denote a scalar • Let R ≥ 4 • ℓ(x) ≥ γ • H(x) := d 2 L dx 2</formula><p>Then, we have</p><formula xml:id="formula_93">• ∥H(x) -H(y)∥ ≤ (13344γ c + 2)γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2 Proof. ∥H(x) -H(y)∥ = ∥(H 1 (x) + H 2 (x) + H 3 (x)) -(H 1 (y) + H 2 (y) + H 3 (y))∥ ≤ ∥H 1 (x) -H 1 (y)∥ + ∥H 2 (x) -H 2 (y)∥ + ∥H 3 (x) -H 3 (y)∥ ≤ 13344γ c γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2 + ∥H 2 (x) -H 2 (y)∥ + ∥H 3 (x) -H 3 (y)∥ ≤ 13344γ c γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2 + β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 + ∥H 3 (x) -H 3 (y)∥ ≤ 13344γ c γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2 + β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 + β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 ≤ 13344γ c γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2 + 2β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 ≤ 13344γ c γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2 + 2γ -4 β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 ≤ 13344γ c γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2 + 2γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2 ≤ (13344γ c + 2)γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2</formula><p>where the first equality follows from Lemma 42, the second equality follows from Fact 19, the third equality follows from Lemma 53, the fourth equality follows from Lemma 54, the fifth equality follows from Lemma 61, the sixth equality follows from simple algebra, the seventh equality follows from γ ∈ (0, 1), the eighth equality follows from exp(20R 2 ) ≤ exp(40R 2 ), the last equality follows from simple algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Helpful Lemma</head><p>Lemma 53. If the following condition holds</p><formula xml:id="formula_94">• Let Q(x), Q 1 (x), Q 2 (x), Q 3 (x), Q 4 (x), Q 5 (x)</formula><p>be denoted as Definition 55</p><p>• Let γ ∈ (0, 1)</p><p>• Let β ∈ (0, 0.1)</p><formula xml:id="formula_95">• Let γ c &gt; 0 denote a scalar • Let R ≥ 4 • Let R f := β -2 n 1.5 exp(3R 2 ) • ℓ(x) ≥ γ • Q(x) = 32γ c (Q 1 (x) -Q 2 (x) -Q 3 (x) + Q 4 (x)) -Q 5 (x) • Let H 1 (x) := d 2 γc•ℓ(x) -1 dx 2 • ∥A∥ ≤ R</formula><p>Then, we have</p><formula xml:id="formula_96">• Part 1. ∥Q(x) -Q(y)∥ ≤ 6672γ -4 β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 • Part 2. ∥H 1 (x) -H 1 (y)∥ ≤ 6672γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2 Proof. Proof of Part 1. ∥Q(x) -Q(y)∥ = ∥(32γ c (Q 1 (x) -Q 2 (x) -Q 3 (x) + Q 4 (x)) -Q 5 (x)) -(32γ c (Q 1 (y) -Q 2 (y) -Q 3 (y) + Q 4 (y)) -Q 5 (y))∥ ≤ 32γ c ∥Q 1 (x) -Q 1 (y)∥ + 16∥Q 2 (x) -Q 2 (y)∥ + 32γ c ∥Q 3 (x) -Q 3 (y)∥ + 16∥Q 4 (x) -Q 4 (y)∥ + ∥Q 5 (x) -Q 5 (y)∥) ≤ 32γ c (∥Q 1 (x) -Q 1 (y)∥ + ∥Q 2 (x) -Q 2 (y)∥ + ∥Q 3 (x) -Q 3 (y)∥ + ∥Q 4 (x) -Q 4 (y)∥ + ∥Q 5 (x) -Q 5 (y)∥)<label>(3)</label></formula><p>where the first equality follows from the Definition of Q(x), the second equality follows from Fact 19, the third equality follows from simple algebra. Then we combine Lemma 56, Lemma 57, Lemma 58, Lemma 59, we show that</p><formula xml:id="formula_97">∥Q 1 (x) -Q 1 (y)∥ + ∥Q 2 (x) -Q 2 (y)∥ + ∥Q 3 (x) -Q 3 (y)∥ + ∥Q 4 (x) -Q 4 (y)∥ ≤ 68γ -4 R f ∥x -y∥ 2 + 64γ -4 R f ∥x -y∥ 2 + 64γ -4 R f ∥x -y∥ 2 + 132γ -4 R f ∥x -y∥ 2 = 328γ -4 R f ∥x -y∥ 2 = 328γ -4 β -2 n 1.5 exp(3R 2 )∥x -y∥ 2 ≤ 328γ -4 β -2 n 1.5 exp(20R 2 )∥x -y∥ 2<label>(4)</label></formula><p>where the second equality follows from simple algebra, the third equality follows from R f = β -2 n 1.5 exp(3R 2 ), the fourth equality follows from exp(3R 2 ) ≤ exp(20R 2 ).</p><p>So we have</p><formula xml:id="formula_98">∥Q(x) -Q(y)∥ ≤ 32γ c (∥Q 1 (x) -Q 1 (y)∥ + ∥Q 2 (x) -Q 2 (y)∥ + ∥Q 3 (x) -Q 3 (y)∥ + ∥Q 4 (x) -Q 4 (y)∥ + ∥Q 5 (x) -Q 5 (y)∥) ≤ 32γ c (328γ -4 β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 + ∥Q 5 (x) -Q 5 (y)∥) ≤ 32γ c (328γ -4 β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 + 89γ -3 β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 ) = 13344γ c γ -4 β -2 n 1.5 exp(20R 2 )∥x -y∥ 2</formula><p>where the first equality follows from Eq (3), the second equality follows from Eq (4), the third equality follows from Lemma 60, the fourth equality follows from simple algebra. Proof of Part 2.</p><formula xml:id="formula_99">∥H 1 (x) -H 1 (y)∥ = A ⊤ ∥Q(x) -Q(y)∥A ≤ ∥A∥ • ∥Q(x) -Q(y)∥ • ∥A∥ ≤ R 2 • ∥Q(x) -Q(y)∥ ≤ R 2 • 13344γ c γ -4 β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 ≤ 13344γ c γ -4 β -2 n 1.5 exp(40R 2 )∥x -y∥ 2</formula><p>where the first equality follows from Definition 55, the second equality follows from simple algebra, the third equality follows from ∥A∥ ≤ R, the fourth equality follows from Part 1 of Lemma 53, the fifth equality follows from simple algebra.</p><p>Lemma 54 (Lemma 7.1 of <ref type="bibr" target="#b25">[DLS23]</ref>). If the following condition holds</p><p>• Let β ∈ (0, 0.1)</p><formula xml:id="formula_100">• Let R ≥ 4 • Let w ∈ R n×d • Let W = diag(w) • Let L reg (x) = ∥W Ax∥ 2 • Let H 2 (x) = d 2 (Lreg(x)+0.5ℓ 1 (x)) dx 2</formula><p>then we have</p><formula xml:id="formula_101">∥H 2 (x) -H 2 (y)∥ ≤ β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 E.3 Definition of Matrix Functions Q i (x)</formula><p>Definition 55. If the given conditions are satisfied</p><p>• Let ℓ(x) be denoted as Definition 26</p><p>• Denote f (x) as Definition 24</p><p>• Denote c(x) as Definition 25</p><p>• Denote B(x) as Definition 37</p><formula xml:id="formula_102">• Let H 1 (x) := d 2 γc•ℓ 1 (x) -1 dx 2 We define Q 1 (x), Q 2 (x), Q 3 (x), Q 4 (x), Q 5 (x) ∈ R n×n as follows • Q 1 (x) = ℓ(x) -3 scalar • ⟨f (x), c(x)⟩ 2 scalar • f (x) n×1 • f (x) ⊤ 1×n • Q 2 (x) = ℓ(x) -3 scalar • ⟨f (x), c(x)⟩ scalar • (f (x) • c(x)) n×1 • f (x) ⊤ 1×n • Q 3 (x) = ℓ(x) -3 scalar • ⟨f (x), c(x)⟩ scalar • f (x) n×1 • (f (x) • c(x)) ⊤ 1×n • Q 4 (x) = ℓ(x) -3 scalar • (f (x) • c(x)) n×1 • (f (x) • c(x)) ⊤ 1×n • Q 5 (x) = ℓ(x) -2 scalar • B(x) n×n We define Q(x) ∈ R n×n as follows Q(x) := 32γ c (Q 1 (x) -Q 2 (x) -Q 3 (x) + Q 4 (x)) -Q 5 (x) Note that H 1 (x) = A ⊤ Q(x)A. E.4 Lipschitz Property for Matrix Function Q 1 (x)</formula><p>Lemma 56. If the given conditions are satisfied</p><p>• Denote Q 1 (x) as Definition 55</p><p>• Let γ ∈ (0, 1)</p><p>• Let β ∈ (0, 0.1)</p><formula xml:id="formula_103">• Let R ≥ 4 • Let R f := β -2 n 1.5 exp(3R 2 ) • ℓ(x) ≥ γ</formula><p>Then, we have</p><formula xml:id="formula_104">• ∥Q 1 (x) -Q 1 (y)∥ ≤ 68γ -4 • R f • ∥x -y∥ 2 Proof. ∥Q 1 (x) -Q 1 (y)∥ = ∥ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 • f (x)f (x) ⊤ -ℓ(y) -3 • ⟨f (y), c(y)⟩ 2 • f (y)f (y) ⊤ ∥ ≤ |ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 | • ∥f (x)∥ 2 • ∥f (x) -f (y)∥ 2 + |ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 | • ∥f (x) -f (y)∥ 2 • ∥f (y)∥ 2 + ∥f (x)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 -ℓ(y) -3 • ⟨f (y), c(y)⟩ 2 | • ∥f (x)∥ 2</formula><p>where the first equality follows from Part 1 of Definition 55, the second equality follows from Part 3 of Lemma 23. We define</p><formula xml:id="formula_105">Q 1,1 (x) := |ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 | • ∥f (x)∥ 2 • ∥f (x) -f (y)∥ 2 Q 1,2 (x) := |ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 | • ∥f (x) -f (y)∥ 2 • ∥f (y)∥ 2 Q 1,3 (x) := ∥f (y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 -ℓ(y) -3 • ⟨f (y), c(y)⟩ 2 | • ∥f (y)∥ 2</formula><p>where</p><formula xml:id="formula_106">∥Q 1 (x) -Q 1 (x)∥ ≤ Q 1,1 (x) + Q 1,2 (x) + Q 1,3<label>(x)</label></formula><p>So we can show that</p><formula xml:id="formula_107">Q 1,1 (x) = |ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 | • ∥f (x)∥ 2 • ∥f (x) -f (y)∥ 2 ≤ 4γ -3 • ∥f (x)∥ 2 • ∥f (x) -f (y)∥ 2 ≤ 4γ -3 • ∥f (x) -f (y)∥ 2 ≤ 4γ -3 • R f • ∥x -y∥ 2<label>(5)</label></formula><p>where the first follows from the Definition of Q 1,1 (x), the second equality follows from Part 3 of Lemma 67, the third equality follows from ∥f (x)∥ 2 ≤ 1, the fourth equality follows from Part 1 of Lemma 65.</p><formula xml:id="formula_108">Q 2,1 (x) = |ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 | • ∥f (x) -f (y)∥ 2 • ∥f (y)∥ 2 ≤ 4γ -3 • ∥f (x) -f (y)∥ 2 • ∥f (y)∥ 2 ≤ 4γ -3 • ∥f (x) -f (y)∥ 2 ≤ 4γ -3 • R f • ∥x -y∥ 2<label>(6)</label></formula><p>where the first follows from the Definition of Q 2,1 (x), the second equality follows from Part 3 of Lemma 67, the third equality follows from ∥f (x)∥ 2 ≤ 1, the fourth equality follows from Part 1 of Lemma 65.</p><formula xml:id="formula_109">Q 1,3 (x) = ∥f (y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 -ℓ(y) -3 • ⟨f (y), c(y)⟩ 2 | • ∥f (y)∥ 2 ≤ |ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 -ℓ(y) -3 • ⟨f (y), c(y)⟩ 2 ≤ 60γ -4 • R f • ∥x -y∥ 2<label>(7)</label></formula><p>where the first equality follows from the Definition of Q 1,3 (x), the second equality follows from ∥f (x)∥ 2 ≤ 1, the third equality follows from Part 2 of Lemma 64. So we have</p><formula xml:id="formula_110">∥Q 1 (x) -Q 1 (x)∥ ≤ Q 1,1 (x) + Q 1,2 (x) + Q 1,3 (x) ≤ 4γ -3 • R f • ∥x -y∥ 2 + Q 1,2 (x) + Q 1,3 (x) ≤ 4γ -3 • R f • ∥x -y∥ 2 + 4γ -3 • R f • ∥x -y∥ 2 + Q 1,3 (x) ≤ 4γ -3 • R f • ∥x -y∥ 2 + 4γ -3 • R f • ∥x -y∥ 2 + 60γ -4 • R f • ∥x -y∥ 2 ≤ 68γ -4 R f • ∥x -y∥ 2</formula><p>where the second equality follows from Eq. ( <ref type="formula" target="#formula_107">5</ref>), the third equality follows from Eq. ( <ref type="formula" target="#formula_108">6</ref>), the fourth equality follows from Eq. ( <ref type="formula" target="#formula_109">7</ref>), the fifth equality follows from simple algebra.</p><p>E.5 Lipschitz Property for Matrix Function Q 2 (x)</p><p>Lemma 57. If the given conditions are satisfied</p><p>• Denote Q 2 (x) as Definition 55</p><p>• Let γ ∈ (0, 1)</p><p>• Let β ∈ (0, 0.1)</p><formula xml:id="formula_111">• Let R ≥ 4 • Let R f := β -2 n 1.5 exp(3R 2 ) • ℓ(x) ≥ γ</formula><p>We have</p><formula xml:id="formula_112">• ∥Q 2 (x) -Q 2 (y)∥ ≤ 64γ -4 • R f ∥x -y∥ 2 Proof. ∥Q 2 (x) -Q 2 (y)∥ = ∥ℓ(x) -3 ⟨f (x), c(x)⟩(f (x) • c(x))f (x) ⊤ -ℓ(y) -3 ⟨f (y), c(y)⟩(f (y) • c(y))f (y) ⊤ ∥ ≤ |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x)∥ 2 • ∥f (x) -f (y)∥ 2 + |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x) -f (y) • c(y)∥ 2 • ∥f (y)∥ 2 + ∥f (y) • c(y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • ∥f (y)∥ 2</formula><p>where the first equality follows from Part 2 of Definition 55, the second equality follows from Part 3 of Lemma 23. We define</p><formula xml:id="formula_113">Q 2,1 (x) := |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x)∥ 2 • ∥f (x) -f (y)∥ 2 Q 2,2 (x) := |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x) -f (y) • c(y)∥ 2 • ∥f (y)∥ 2 Q 2,3 (x) := ∥f (y) • c(y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • ∥f (y)∥ 2</formula><p>where</p><formula xml:id="formula_114">∥Q 2 (x) -Q 2 (y)∥ ≤ Q 2,1 (x) + Q 2,2 (x) + Q 2,3<label>(x)</label></formula><p>So we can show that</p><formula xml:id="formula_115">Q 2,1 (x) = |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x)∥ 2 • ∥f (x) -f (y)∥ 2 ≤ 2γ -3 • ∥f (x) • c(x)∥ 2 • ∥f (x) -f (y)∥ 2 ≤ 2γ -3 • 2 • ∥f (x) -f (y)∥ 2 ≤ 2γ -3 • 2 • R f • ∥x -y∥ 2 = 4γ -3 • R f • ∥x -y∥ 2 (8)</formula><p>where the first equality follows from the Definition of Q 2,1 (x), the second equality follows from Part 3 of Lemma 67, the third equality follows from Part 2 of Lemma 66, the fourth equality follows from Part 1 of Lemma 65, the last equality follows from simple algebra.</p><formula xml:id="formula_116">Q 2,2 (x) = |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x) -f (y) • c(y)∥ 2 • ∥f (y)∥ 2 ≤ 2γ -3 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 • ∥f (y)∥ 2 ≤ 2γ -3 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 ≤ 2γ -3 • 3 • R f • ∥x -y∥ 2 = 6γ -3 • R f • ∥x -y∥ 2 (9)</formula><p>where the first equality follows from the Definition of Q 2,1 (x), the second equality follows from Part 3 of Lemma 67, the third equality follows from ∥f (x)∥ 2 ≤ 1, the fourth equality follows from Part 3 of Lemma 65, the last equality follows from simple algebra.</p><formula xml:id="formula_117">Q 2,3 (x) = ∥f (y) • c(y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • ∥f (y)∥ 2 ≤ ∥f (y) • c(y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| ≤ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| ≤ 2 • 27γ -4 • R f ∥x -y∥ 2 ≤ 54γ -4 • R f ∥x -y∥ 2<label>(10)</label></formula><p>where the first equality follows from the Definition of Q 2,3 (x), the second equality follows from ∥f (x)∥ 2 ≤ 1, the third equality follows from Part 2 of Lemma 66, the fourth equality follows from Part 1 of Lemma 64, the last equality follows from simple algebra. So we have</p><formula xml:id="formula_118">∥Q 2 (x) -Q 2 (y)∥ ≤ Q 2,1 (x) + Q 2,2 (x) + Q 2,3 (x) ≤ 4γ -3 • R f • ∥x -y∥ 2 + Q 2,2 (x) + Q 2,3 (x) ≤ 4γ -3 • R f • ∥x -y∥ 2 + 6γ -3 • R f • ∥x -y∥ 2 + Q 2,3 (x) ≤ 4γ -3 • R f • ∥x -y∥ 2 + 6γ -3 • R f • ∥x -y∥ 2 + 54γ -4 • R f ∥x -y∥ 2 ≤ 64γ -4 • R f ∥x -y∥ 2</formula><p>where the second equality follows from Eq. ( <ref type="formula">8</ref>), the third equality follows from Eq. ( <ref type="formula">9</ref>), the fourth equality follows from Eq. ( <ref type="formula" target="#formula_117">10</ref>), the fifth equality follows from simple algebra.</p><p>E.6 Lipschitz Property for Matrix Function Q 3 (x)</p><p>Lemma 58. If the given conditions are satisfied</p><formula xml:id="formula_119">• Denote Q 3 (x)</formula><p>as Definition 55</p><p>• Let γ ∈ (0, 1)</p><p>• Let β ∈ (0, 0.1)</p><formula xml:id="formula_120">• Let R ≥ 4 • Let R f := β -2 n 1.5 exp(3R 2 ) • ℓ(x) ≥ γ</formula><p>We have</p><formula xml:id="formula_121">• ∥Q 3 (x) -Q 3 (y)∥ ≤ 64γ -4 • R f ∥x -y∥ 2</formula><p>Proof. Proof of Part 3.</p><formula xml:id="formula_122">∥Q 3 (x) -Q 3 (y)∥ = ∥ℓ(x) -3 ⟨f (x), c(x)⟩f (x)(f (x) • c(x)) ⊤ -ℓ(y) -3 ⟨f (y), c(y)⟩f (y)(f (y) • c(y)) ⊤ ∥ ≤ |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x)∥ 2 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 + |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) -f (y)∥ 2 • ∥f (y) • c(y)∥ 2 + ∥f (y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • ∥f (y) • c(y)∥ 2</formula><p>where the first equality follows from the Definition of Q 3 (x), the second equality follows from Part 3 of Lemma 23. We define</p><formula xml:id="formula_123">Q 3,1 (x) := |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x)∥ 2 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 Q 3,2 (x) := |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) -f (y)∥ 2 • ∥f (y) • c(y)∥ 2 Q 3,3 (x) := ∥f (y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • ∥f (y) • c(y)∥ 2</formula><p>where</p><formula xml:id="formula_124">∥Q 3 (x) -Q 3 (y)∥ ≤ Q 3,1 (x) + Q 3,2 (x) + Q 3,3<label>(x)</label></formula><p>So we can show that</p><formula xml:id="formula_125">Q 3,1 (x) = |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x)∥ 2 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 ≤ 2γ -3 • ∥f (x)∥ 2 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 ≤ 2γ -3 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 ≤ 2γ -3 • 3R f ∥x -y∥ 2 = 6γ -3 • R f ∥x -y∥ 2<label>(11)</label></formula><p>where the first equality follows from the Definition of Q 3,1 (x), the second equality follows from Part 3 of Lemma 67, the third equality follows from ∥f (x)∥ 2 ≤ 1, the fourth equality follows from Part 3 of Lemma 65, the last equality follows from simple algebra.</p><formula xml:id="formula_126">Q 3,2 (x) = |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) -f (y)∥ 2 • ∥f (y) • c(y)∥ 2 ≤ 2γ -3 • ∥f (x) -f (y)∥ 2 • ∥f (y) • c(y)∥ 2 ≤ 2γ -3 • ∥f (x) -f (y)∥ 2 • 2 ≤ 2γ -3 • R f • ∥x -y∥ 2 • 2 = 4γ -3 • R f • ∥x -y∥ 2 (12)</formula><p>where the first equality follows from the Definition of Q 3,2 (x), the second equality follows from Part 3 of Lemma 67, the third equality follows from Part 2 of Lemma 66, the fourth equality follows from Part 1 of Lemma 65, the last equality follows from simple algebra.</p><formula xml:id="formula_127">Q 3,3 (x) = ∥f (y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • ∥f (y) • c(y)∥ 2 ≤ |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • ∥f (y) • c(y)∥ 2 ≤ |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • 2 ≤ 27γ -4 • R f ∥x -y∥ 2 • 2 = 54γ -4 • R f ∥x -y∥ 2 (13)</formula><p>where the first equality follows from the Definition of Q 3,3 (x), the second equality follows from ∥f (x)∥ 2 ≤ 1, the third equality follows from Part 2 of Lemma 66, the fourth equality follows from Part 1 of Lemma 64, the last equality follows from simple algebra. So we have</p><formula xml:id="formula_128">∥Q 3 (x) -Q 3 (y)∥ ≤ Q 3,1 (x) + Q 3,2 (x) + Q 3,3 (x) ≤ 6γ -3 • R f • ∥x -y∥ 2 + Q 3,2 (x) + Q 3,3 (x) ≤ 6γ -3 • R f • ∥x -y∥ 2 + 4γ -3 • R f • ∥x -y∥ 2 + Q 3,3 (x) ≤ 6γ -3 • R f • ∥x -y∥ 2 + 4γ -3 • R f • ∥x -y∥ 2 + 54γ -4 • R f ∥x -y∥ 2 ≤ 64γ -4 • R f ∥x -y∥ 2</formula><p>where the first equality follows from Eq. ( <ref type="formula" target="#formula_125">11</ref>), the second equality follows from Eq. ( <ref type="formula">12</ref>), the third equality follows from Eq. ( <ref type="formula">13</ref>), the fourth equality follows from simple algebra.</p><p>E.7 Lipschitz Property for Matrix Function Q 4 (x)</p><p>Lemma 59. If the given conditions are satisfied</p><p>• Denote Q 4 (x) as Definition 55</p><p>• Let γ ∈ (0, 1)</p><p>• Let β ∈ (0, 0.1)</p><formula xml:id="formula_129">• Let R ≥ 4 • Let R f := β -2 n 1.5 exp(3R 2 ) • ℓ(x) ≥ γ</formula><p>We have</p><formula xml:id="formula_130">• ∥Q 4 (x) -Q 4 (y)∥ ≤ 132γ -3 • R f ∥x -y∥ 2 Proof. ∥Q 4 (x) -Q 4 (y)∥ = ∥ℓ(x) -3 ⟨f (x), c(x)⟩(f (x) • c(x))(f (x) • c(x)) ⊤ -ℓ(y) -2 ⟨f (y), c(y)⟩(f (y) • c(y))(f (y) • c(y)) ⊤ ∥ ≤ |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x)∥ 2 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 + |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x) -f (y) • c(y)∥ 2 • ∥f (y) • c(y)∥ 2 + ∥f (y) • c(y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • ∥f (y) • c(y)∥ 2</formula><p>where the first equality follows from the Definition 55, the second equality follows from Part 3 of Lemma 23. We define</p><formula xml:id="formula_131">Q 4,1 (x) := |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x)∥ 2 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 Q 4,2 (x) := |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x) -f (y) • c(y)∥ 2 • ∥f (y) • c(y)∥ 2 Q 4,3 (x) := ∥f (y) • c(y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • ∥f (y) • c(y)∥ 2 where ∥Q 4 (x) -Q 4 (y)∥ ≤ Q 4,1 (x) + Q 4,2 (x) + Q 4,3<label>(x)</label></formula><p>So we can show that</p><formula xml:id="formula_132">Q 4,1 (x) = |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x)∥ 2 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 ≤ 2γ -3 • ∥f (x) • c(x)∥ 2 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 ≤ 2γ -3 • 2 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 ≤ 2γ -3 • 2 • 3R f • ∥x -y∥ 2 = 12γ -3 • R f • ∥x -y∥ 2<label>(14)</label></formula><p>where the first equality follows from the Definition of Q 4,1 (x), the second equality follows from Part 3 of Lemma 67, the third equality follows from Part 2 of Lemma 66, the fourth equality follows from Part 3 of Lemma 65, the last equality follows from simple algebra.</p><formula xml:id="formula_133">Q 4,2 (x) = |ℓ(x) -3 • ⟨f (x), c(x)⟩| • ∥f (x) • c(x) -f (y) • c(y)∥ 2 • ∥f (y) • c(y)∥ 2 ≤ 2γ -3 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 • ∥f (y) • c(y)∥ 2 ≤ 2γ -3 • ∥f (x) • c(x) -f (y) • c(y)∥ 2 • 2 ≤ 2γ -3 • 3R f • ∥x -y∥ 2 • 2 = 12γ -3 • R f • ∥x -y∥ 2<label>(15)</label></formula><p>where the first equality follows from the Definition of Q 4,2 (x), the second equality follows from Part 3 of Lemma 67, the third equality follows from Part 2 of Lemma 66, the fourth equality follows from Part 3 of Lemma 65, the last equality follows from simple algebra.</p><formula xml:id="formula_134">Q 4,3 (x) = ∥f (y) • c(y)∥ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • ∥f (y) • c(y)∥ 2 ≤ 2 • |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • 2 ≤ 2 • 27γ -4 • R f ∥x -y∥ 2 • 2 = 108γ -4 • R f ∥x -y∥ 2<label>(16)</label></formula><p>where the first equality follows from the Definition of Q 4,3 (x), the second equality follows from Part 2 of Lemma 66, the third equality follows Part 1 of Lemma 64, the last equality follows from simple algebra. So we have</p><formula xml:id="formula_135">∥Q 4 (x) -Q 4 (y)∥ ≤ Q 4,1 (x) + Q 4,2 (x) + Q 4,3 (x) ≤ 12γ -3 • R f • ∥x -y∥ 2 + Q 3,2 (x) + Q 3,3 (x) ≤ 12γ -3 • R f • ∥x -y∥ 2 + 12γ -2 • R f • ∥x -y∥ 2 + Q 3,3 (x) ≤ 12γ -3 • R f • ∥x -y∥ 2 + 12γ -3 • R f • ∥x -y∥ 2 + 108γ -4 • R f ∥x -y∥ 2 ≤ 132γ -4 • R f ∥x -y∥ 2</formula><p>where the second equality follows from Eq. ( <ref type="formula" target="#formula_132">14</ref>), the third equality follows from Eq. ( <ref type="formula" target="#formula_133">15</ref>), the fourth equality follows from Eq. ( <ref type="formula" target="#formula_134">16</ref>), the fifth equality follows from simple algebra.</p><p>E.8 Lipschitz Property for Matrix Function Q 5 (x)</p><p>Lemma 60. If the given conditions are satisfied</p><p>• Denote Q 5 (x) as Definition 55</p><p>• Let γ ∈ (0, 1)</p><p>• Let β ∈ (0, 0.1)</p><formula xml:id="formula_136">• Let R ≥ 4 • Let R f := β -2 n 1.5 exp(3R 2 ) • ℓ(x) ≥ γ</formula><p>We have</p><formula xml:id="formula_137">• ∥Q 5 (x) -Q 5 (y)∥ ≤ 89γ -3 • β -2 n 1.5 exp(20R 2 )∥x -y∥ 2</formula><p>Proof. We have</p><formula xml:id="formula_138">∥Q 5 (x) -Q 5 (y)∥ = ∥ℓ(x) -2 B(x) -ℓ(y) -2 B(y)∥ ≤ ∥ℓ(x) -2 B(x) -ℓ(x) -2 B(y) + ℓ(x) -2 B(y) -ℓ(y) -2 B(y)∥ ≤ ∥ℓ(x) -2 B(x) -ℓ(x) -2 B(y)∥ + ∥ℓ(x) -2 B(y) -ℓ(y) -2 B(y)∥ = |ℓ(x) -2 | • ∥B(x) -B(y)∥ + |ℓ(x) -2 -ℓ(y) -2 | • ∥B(y)∥ ≤ γ -2 • ∥B(x) -B(y)∥ + |ℓ(x) -2 -ℓ(y) -2 | • ∥B(y)∥ ≤ γ -2 • ∥B(x) -B(y)∥ + |ℓ(x) -2 -ℓ(y) -2 | • 11 ≤ γ -2 • ∥B(x) -B(y)∥ + 8γ -3 • R f • ∥x -y∥ 2 • 11 ≤ γ -2 • β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 + 8γ -3 • R f • ∥x -y∥ 2 • 11 ≤ γ -3 • β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 + 8γ -3 • R f • ∥x -y∥ 2 • 11 ≤ γ -3 • β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 + 8γ -3 • β -2 n 1.5 exp(3R 2 ) • ∥x -y∥ 2 • 11 ≤ γ -3 • β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 + 8γ -3 • β -2 n 1.5 exp(20R 2 ) • ∥x -y∥ 2 • 11 ≤ 89γ -3 • β -2 n 1.5 exp(20R 2 )∥x -y∥ 2 ≤ 89γ -4 • β -2 n 1.5 exp(20R 2 )∥x -y∥ 2</formula><p>where the first equality follows from Definition 55, the second, third, fourth equalities follow from Fact 20, the fifth equality follows from ℓ(x) ≥ γ, the sixth equality follows from Part 1 of Lemma 68, the seventh equality follows from Part 2 fo Lemma 62, the eighth equality follows from Lemma 61, the ninth equality follows from γ ∈ (0, 1), the tenth equality follows from R f = β -2 n 1.5 exp(3R 2 ), the 1first equality follows from exp(3R 2 ) ≤ exp(20R 2 ), the 1second equality follows from simple algebra, the last equality follows from γ ∈ (0, 1).</p><p>Lemma 61. If the given conditions are satisfied</p><formula xml:id="formula_139">• Let A ∈ R n×d • Let β ∈ (0, 0.1) • Let R ≥ 4 • Let w ∈ R n×d • Let W = diag(w) • Let L reg (x) = ∥W Ax∥ 2 • Let H 2 (x) = d 2 (Lreg(x)+ℓ(x)) dx 2</formula><p>then we have</p><formula xml:id="formula_140">∥B(x) -B(y)∥ ≤ β -2 n 1.5 exp(20R 2 )∥x -y∥ 2</formula><p>Proof. We have</p><formula xml:id="formula_141">∥H 2 (x) -H 2 (y)∥ = ∥ d 2 (L reg (x) + ℓ(x)) dx 2 - d 2 (L reg (y) + ℓ(y)) dy 2 ∥ = ∥ d 2 L reg (x) dx 2 + d 2 ℓ(x) dx 2 - d 2 L reg (y) dy 2 - d 2 ℓ(y) dy 2 ∥</formula><p>where the first equality follows from the Definition of H(x), the second equality follows from simple differential rule. So we have</p><formula xml:id="formula_142">∥B(x) -B(y)∥ ≤ A ⊤ • ∥B(x) -B(y)∥ • A = ∥ d 2 ℓ(x) dx 2 - d 2 ℓ(y) dy 2 ∥ ≤ ∥ d 2 L reg (x) dx 2 + d 2 ℓ(x) dx 2 - d 2 L reg (y) dy 2 - d 2 ℓ(y) dy 2 ∥ = ∥H(x) -H(y)∥ ≤ β -2 n 1.5 exp(20R 2 )∥x -y∥ 2</formula><p>where the first equality follows from simple algebra, the second equality follows from Lemma 36, the third equality follows from , the fourth equality follows from the Definition of H(x), the fifth equality follows from Lemma 54.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Lipschitz Tools</head><p>In this appendix, we provide a set of tools that can assist in the computation of the Lipschitz property. In Appendix F.1, we provide Lipschitz tool of some scalar functions. In Appendix F.2, we provide Lipschitz tool of some vector functions.</p><p>F.1 Lipschitz Tool: Scalar Function Lemma 62. If the given conditions are satisfied</p><formula xml:id="formula_143">• Let A ∈ R n×d • Let β ∈ (0, 0.1) • Let R ≥ 4 • Let γ ∈ (0, 1) • Let x, y ∈ R d satisfy ∥A(x -y)∥ ∞ &lt; 0.01 • Let R f := β -2 n 1.5 exp 3R 2 • Let ℓ(x) be denoted as Definition 26 • ℓ(x) ≥ γ We have • Part 1. |ℓ(x) -ℓ(y)| ≤ 4R f • ∥x -y∥ 2 • Part 2. Let p ∈ {1, 2, 3}, then we have |ℓ(x) -p -ℓ(y) -p | ≤ 4pγ -(1+p) • R f • ∥x -y∥ 2 Proof. Proof of Part 1. |ℓ(x) -ℓ(y)| = |⟨c(x), c(x)⟩ -⟨c(y), c(y)⟩| = |⟨c(x) -c(y), c(x) + c(y)⟩| ≤ ∥c(x) -c(y)∥ 2 • ∥c(x) + c(y)∥ 2 ≤ (∥c(x)∥ 2 + ∥c(y)∥ 2 ) • ∥c(x) -c(y)∥ 2 ≤ 4 • ∥c(x) -c(y)∥ 2 ≤ 4R f • ∥x -y∥ 2</formula><p>where the first equality follows from Fact 17, the second equality follows from Fact 19 (Cauchy-Schwarz inequality), the third equality follows from Fact 19, the fourth equality follows from Part 1 of Lemma 66, the fifth equality follows from Part 2 of Lemma 65.</p><p>Proof of Part 2.</p><formula xml:id="formula_144">|ℓ(x) -p -ℓ(y) -p | = |ℓ(x) -ℓ(y)| • | p-1 i=0 ℓ(x) -p+i ℓ(y) -i-1 | ≤ |ℓ(x) -ℓ(y)| • | p-1 i=0 γ -p+i γ -i-1 | = |ℓ(x) -ℓ(y)| • | p-1 i=0 γ -p-1 | = |ℓ(x) -ℓ(y)| • (p -1 + 1) • γ -p-1 ≤ 4R f • ∥x -y∥ 2 • (p -1 + 1) • γ -p-1 = 4pγ -(1+p) • R f • ∥x -y∥ 2</formula><p>where the first equality follows from a</p><formula xml:id="formula_145">p -b p = (a -b)(a p-1 + a p-2 b + • • • + ab p-2 + b p-1</formula><p>), the second equality follows from ℓ(x) ≥ γ, the third equality follows from simple algebra, the fourth equality follows from simple algebra, the fifth equality follows from Part 1 of Lemma 62, the last equality follows from simple algebra.</p><p>Lemma 63. If the given conditions are satisfied</p><formula xml:id="formula_146">• Let A ∈ R n×d • Let β ∈ (0, 0.1) • Let R ≥ 4 • Let x, y ∈ R d satisfy ∥A(x -y)∥ ∞ &lt; 0.01</formula><p>• Let R f := β -2 n 1.5 exp 3R 2</p><p>• Denote f (x) as Definition 24</p><p>• Denote c(x) as Definition 25</p><p>We have </p><formula xml:id="formula_147">•</formula><formula xml:id="formula_148">≤ γ -3 • 3R f • ∥x -y∥ 2 + 12γ -4 • R f • ∥x -y∥ 2 • |⟨f (y), c(y)⟩| ≤ γ -3 • 3R f • ∥x -y∥ 2 + 12γ -4 • R f • ∥x -y∥ 2 • 2 = (3γ -3 + 24γ -4 ) • R f ∥x -y∥ 2 ≤ 27γ -4 • R f ∥x -y∥ 2</formula><p>where the first equality follows from Part 4 of Lemma 23, the second equality follows from ℓ(x) ≥ γ, the third equality follows from Part 1 of Lemma 63, the fourth equality follows from Part 2 of Lemma 62, the fifth equality follows from Part 2 of Lemma 67, the sixth equality follows from simple algebra, the last equality follows from γ ∈ (0, 1). Proof of Part 2. </p><formula xml:id="formula_149">)⟩| + 27γ -4 • R f ∥x -y∥ 2 • 2 ≤ 2γ -3 • 3R f • ∥x -y∥ 2 + 27γ -4 • R f • ∥x -y∥ 2 • 2 = 60γ -4 • R f • ∥x -y∥ 2</formula><p>where the first equality follows from Part 4 of Lemma 23, the second equality follows from Part 3 of Lemma 67, the third equality follows from ℓ(x) ≥ γ, the fourth equality follows from Part 1 of Lemma 64, the fifth equality follows from Part 1 of Lemma 63, the sixth equality follows from simple algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Lipschitz Tool: Vector Function</head><p>Lemma 65. If the given conditions are satisfied</p><formula xml:id="formula_150">• Let A ∈ R n×d</formula><p>• Let b ∈ R n satisfy that ∥b∥ 1 ≤ 1</p><p>• Let β ∈ (0, 0.1)</p><p>• Let R ≥ 4</p><p>• Let x, y ∈ R d satisfy ∥A(x -y)∥ ∞ &lt; 0.01</p><formula xml:id="formula_151">• ∥A∥ ≤ R • Let R f := β -2 n 1.5 exp 3R 2</formula><p>• Denote f (x) as Definition 24</p><formula xml:id="formula_152">≤ 3 + ∥2b∥ 2 ≤ 3 + 2 = 5</formula><p>where the first, second equalities follow from Fact 20, the third, fourth equalities follow from Fact 19, the fifth equality follows from ∥f (x)∥ 2 ≤ 1, the sixth equality follows from ∥b∥ 2 ≤ 1, the last equality follows from simple algebra. For the second term</p><formula xml:id="formula_153">∥⟨f (x) -b, f (x)⟩ • diag(f (x))∥ ≤ |⟨f (x) -b, f (x)⟩| • ∥ diag(f (x))∥ ≤ ∥f (x) -b∥ 2 • ∥f (x)∥ 2 • ∥ diag(f (x))∥ ≤ ∥f (x) -b∥ 2 • ∥f (x)∥ 2 • ∥f (x)∥ 2 ≤ (∥f (x)∥ 2 + ∥b∥ 2 ) • ∥f (x)∥ 2 • ∥f (x)∥ 2 ≤ ∥b∥ 2 ≤ 1</formula><p>where the first equality follows from Fact 20, the second equality follows from Fact 19 (Cauchy-Schwarz inequality), the third, fourth equalities follow from Fact 19, the fifth equality follows from ∥f (x)∥ 2 ≤ 1, the sixth equality follows from ∥b∥ 2 ≤ 1.</p><p>For the third term</p><formula xml:id="formula_154">∥ diag((2f (x) -b) • f (x))∥ ≤ ∥(2f (x) -b) • f (x)∥ 2 ≤ ∥2f (x) -b∥ 2 • ∥f (x)∥ 2 ≤ (∥2f (x)∥ 2 + ∥b∥ 2 ) • ∥f (x)∥ 2 ≤ 2 + ∥b∥ 2 ≤ 2 + 1 = 3</formula><p>where the first, second, third equalities follow from Fact 19, the fourth equality follows from ∥f (x)∥ 2 ≤ 1, the fifth equality follows from ∥b∥ 2 = 1, the last equality follows from simple algebra.</p><p>For the fourth term</p><formula xml:id="formula_155">∥(b • f (x)) • f (x) ⊤ + f (x) • (b • f (x)) ⊤ ∥ ≤ ∥(b • f (x)) • f (x) ⊤ ∥ + ∥f (x) • (b • f (x)) ⊤ ∥ ≤ ∥b • f (x)∥ 2 • ∥f (x)∥ 2 + ∥f (x)∥ 2 • ∥b • f (x)∥ 2 ≤ 2∥b • f (x)∥ 2 ≤ 2(∥b∥ 2 • ∥f (x)∥ 2 ) ≤ 2(∥b∥ 2 • 1) ≤ 2(1 • 1) = 2</formula><p>where the first equality follows from Fact 19, the second equality follows from Fact 19, the third equality follows from ∥f (x)∥ 2 ≤ 1, the fourth equality follows from ∥b∥ 2 = 1, the last equality follows from simple algebra. Then we combine four terms, we have ∥B(x)∥ ≤ 5 + 1 + 3 + 2 = 11 where the last equality follows from simple algebra.</p><p>The positive definiteness of the Hessian matrix follows directly from Lemma 43. Proof of Hessian is Lipschitz.</p><p>The Lipschitz property of the Hessian matrix can be established using Lemma 52. Proof of Cost per iteration.</p><p>The cost per iteration can be deduced from Lemma 78. Proof of Convergence per Iteration. By utilizing Lemma 80, it can be shown that:</p><formula xml:id="formula_156">|x k -x * |2 ≤ 0.4|xk -1 -x * | 2</formula><p>Proof of Number of Iterations.</p><p>After performing T iterations, we obtain the following result:</p><formula xml:id="formula_157">|x k -x * |2 ≤ 0.4 T |xk -1 -x * | 2</formula><p>By appropriately choosing the value of T , we can achieve the desired bound. The failure probability is derived from applying the union bound over the T iterations.</p><p>I L is τ c -Copyright-Protected</p><p>In this appendix, we show our result that Copyright Regression avoids model outputting copyright data. In Appendix I.1, we reaffirm our definition of problem and optimal parameter x * of Copyright Regression. In Appendix I.2, we provide our result and proof of x * is τ c -Copyright-Protected, where τ c = √ 2γ c /n 1 -ϵ 2 /n 2 . Proof. To compute the min x∈R d (0.5ℓ 1 (x) + γ c ℓ 1 (x) -1 ), we have d(0.5ℓ 1 (x) + γ c ℓ 1 (x) -1 ) dℓ 1 (x) = d0.5ℓ 1 (x) dℓ 1 (x) + dγ c ℓ 1 (x) -1 dℓ 1 (x) = 0.5 + dγ c ℓ 1 (x) -1 dℓ 1 (x) = 0.5 + γ c dℓ 1 (x) -1 dℓ 1 (x) = 0.5 -γ c 1 ℓ 1 (x) -2 (18)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1 Definitions</head><p>where the first, second, third, fourth equalities follow from simple differential rules. Hence, when d(0.5ℓ 1 (x)+γcℓ 1 (x) -1 ) dℓ 1 (x) = 0, applying Eq. (18), we have 0.5 -γ c 1 ℓ 1 (x) -2 = 0 (19) solving Eq. ( <ref type="formula">19</ref>) yields</p><formula xml:id="formula_158">ℓ 1 (x) = 2γ c</formula><p>Bring ℓ 1 (x) into the 0.5ℓ 1 (x) + γ c ℓ 1 (x) -1 , and we have min where the first equality follows from ℓ 1 (x) = √ 2γ c , the second equality follows from simple algebra. So we have 0.5ℓ 1 (x * ) + γ c ℓ 1 (x * ) -1 ≤ min x∈R d (0.5ℓ 1 (x * ) + γ c ℓ 1 (x * ) -1 ) + ϵ 1</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Copyright Regression experiment result</figDesc><graphic coords="13,72.00,72.00,503.64,119.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison with Random and Copyright Regression on copyright data</figDesc><graphic coords="14,152.70,72.00,306.60,191.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>D. 5</head><label>5</label><figDesc>Lower Bound Property for Matrix Function P 1 (x) Lemma 48. If the given conditions are satisfied• Given a matrix A ∈ R n×d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><figDesc>Part 1.|⟨f (x), c(x)⟩ -⟨f (y), c(y)⟩| ≤ 3R f • ∥x -y∥ 2 Part 2. |⟨f (x), c(x)⟩ 2 -⟨f (y), c(y)⟩ 2 | ≤ 12R f • ∥x -y∥ 2 Proof. Proof of Part 1. |⟨f (x), c(x)⟩ -⟨f (y), c(y)⟩| = |f (x) ⊤ c(x) -f (y) ⊤ c(y)| ≤ ∥f (x)∥ 2 • ∥c(x) -c(y)∥ 2 + ∥f (x) -f (y)∥ 2 • ∥c(x)∥ 2 ≤ ∥c(x) -c(y)∥ 2 + ∥f (x) -f (x)∥ 2 • ∥c(y)∥ 2 ≤ ∥c(x) -c(y)∥ 2 + 2∥f (x) ⊤ -f (x) ⊤ ∥ 2 ≤ R f • ∥x -y∥ 2 + 2∥f (x) ⊤ -f (x) ⊤ ∥ 2 ≤ R f • ∥x -y∥ 2 + 2R f • ∥x -y∥ 2 ≤ γ -3 • |⟨f (x), c(x)⟩ -⟨f (y), c(y)⟩| + |ℓ(x) -3 -ℓ(y) -3 | • |⟨f (y), c(y)⟩| ≤ γ -3 • 3R f • ∥x -y∥ 2 + |ℓ(x) -3 -ℓ(y) -3 | • |⟨f (y), c(y)⟩|</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>|ℓ(x) -3 • ⟨f (x), c(x)⟩ 2 -ℓ(y) -3 • ⟨f (y), c(y)⟩ 2 | ≤ |ℓ(x) -3 • ⟨f (x), c(x)⟩| • |⟨f (x), c(x)⟩ -⟨f (y), c(y)⟩| + |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • |⟨f (y), c(y)⟩| ≤ |ℓ(x) -3 • ⟨f (x), c(x)⟩| • |⟨f (x), c(x)⟩ -⟨f (y), c(y)⟩| + |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • 2 ≤ 2γ -3 • |⟨f (x), c(x)⟩ -⟨f (y), c(y)⟩| + |ℓ(x) -3 • ⟨f (x), c(x)⟩ -ℓ(y) -3 • ⟨f (y), c(y)⟩| • 2 ≤ 2γ -3 • |⟨f (x), c(x)⟩ -⟨f (y), c(y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>••</head><figDesc>Definition 70 (τ -Copyright-Protected). Given a matrix A ∈ R n×d and vector b ∈ R n that A = A 1 A 2 , and b = b 1 b 2, whereA 1 ∈ R n 1 ×d , A 2 ∈ R n 2 ×d , b 1 ∈ R n 1 , b 2 ∈ R n 2 and n = n 1 + n 2 . A 1 , b 1are the data has copyright issue and A 2 , b 2 are the data does not have copyright issue. Denote the train objective L. Denote τ &gt; 0 a scalar. If there is a trained model f θ with parameter θ that satisfiesL(f θ (A 1 ), b 1 ) n 1 ≥ τ + L(f θ (A 2 ), b 2 ) n 2then we say this model f θ is τ -Copyright-Protected.Definition 71. Let x ∈ R d be a vector parameter in regression problem. Let L be denoted as Definition 31, let ℓ 1 (x) and ℓ 2 (x) be denoted as Definition 29. Denote γ c &gt; 0 a scalar. Denote ϵ 1 , ϵ 2 ∈ (0, 0.1) two scalars, we define thatx * := arg minx∈R d L and x * satisfies • 0.5ℓ 1 (x * ) + γ c ℓ 1 (x * ) -1 ≤ min x∈R d (0.5ℓ 1 (x * ) + γ c ℓ 1 (x * ) -1 ) + ϵ 1 • ℓ 2 (x * ) ≤ ϵ 2 ≤ minx∈R d ℓ 2 (x) + ϵ 2 I.2 Copyright-Pretected Property for x * Lemma 72. If the given conditions are satisfied • Let x * be denoted as Definition 71 Let ℓ 1 (x) be denoted as Definition 29 Denote ϵ 1 , ϵ 2 ∈ (0, 0.1) two scalars • Denote γ c &gt; 0 a scalar we have 2γ c ≤ ℓ 1 (x * ) ≤ ( 2γ c + ϵ 1 ) + ϵ 2 1 + 2ϵ 1 2γ c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>5ℓ 1 (x) + γ c ℓ 1 (x) -1 ) = 0.5 2γ c + γ c</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Road map. In Appendix A, we provide the preliminaries used in our proofs. In Appendix B, we reaffirm our definitions of Copyright Regression. In Appendix C, we provide our computation results for several functions. In Appendix D, we prove that ∇ 2 L ⪰ 0 and thus L is convex. In Appendix E, we provide our result that Hessian of L is Lipschitz. In Appendix F, we provide a set of tools that can assist in the computation of the Lipschitz property. In Appendix G, we present a collection of bound that are valuable for facilitating computations in our proofs. In Appendix H, we provide guarantee of minimizing our final training objective. In Appendix I, we show our result that Copyright Regression avoids model outputting copyright data. In Appendix J, we provide an approximate version of the newton method for convex optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Preliminary</head><p>In this appendix, we present the preliminaries utilized in our proofs. In Appendix A.1 introduces the notations employed throughout the document. In Appendix A.2 provides fundamental facts regarding exact computation. In Appendix A.3 presents useful tools for determining bounds on norms based on vectors. In Appendix A.4 provides useful tools for determining bounds on norms related to matrices. In Appendix A.5 furnishes basic inequalities for positive semidefinite (psd) matrices. In Appendix A.6 supplies essential lemmas for exact computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Notations</head><p>Now we utilize the following notations and definitions: The ℓ p norm of a vector x is denoted as ∥x∥ p , for examples, ∥x∥ 1 := n i=1 |x i |, ∥x∥ 2 := ( n i=1 x 2 i ) 1/2 and ∥x∥ ∞ := max i∈[n] |x i |. For a vector x ∈ R n , exp(x) ∈ R n denotes a vector where whose i-th entry is exp(x i ) for all i ∈ [n]. For n &gt; k, for any matrix A ∈ R n×k , we denote the spectral norm of A by ∥A∥, i.e., ∥A∥ := sup x∈R k ∥Ax∥ 2 /∥x∥ 2 . We denote σ min (A) as the minimum singular value of A. For two vectors x, y ∈ R n , we denote ⟨x, y⟩ = n i=1 for i ∈ [n]. Given two vectors x, y ∈ R n , we denote x • y as a vector whose i-th entry is x i y i for all i ∈ [n]. We use e i ∈ R n to denote a vector whose i-th entry is 1 and all the other entries are 0. Let x ∈ R n be a vector. For a vector x ∈ R n , diag(x) ∈ R n×n is defined as a diagonal matrix with its diagonal entries given by diag(x) i,i = x i for i = 1, ..., n, and all off-diagonal entries are 0. A symmetric matrix A ∈ R n×n is said to be positive definite (PD) when A ≻ 0, for all non-zero vectors x ∈ R n , we have x ⊤ Ax &gt; 0. Similarly, a symmetric matrix A ∈ R n×n is said to be positive semidefinite (PSD) when A ⪰ 0, for all vectors x ∈ R n , we have x ⊤ Ax ≥ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Basic Algebras</head><p>Fact 17. For vectors u, v, w ∈ R n , we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Lower Bound on Hessian</head><p>In this appendix, we prove that ∇ 2 L ⪰ 0 and thus L is convex. In Appendix D.1, we provide our main result that ∇ 2 L ⪰ 0. In Appendix D.2, we divide H 1 (x) to 5 part to help us to compute lower bound on H 1 (x). In Appendix D.3, we provide our result and proof of lower bound on P (x). In Appendix D.4, we provide some helpful lemma to simplify our proofs. In Appendix D.5, we provide our result and proof of lower bound on P (x) 1 . In Appendix D.6, we provide our result and proof of lower bound on P (x) 2.5 . In Appendix D.7, we provide our result and proof of lower bound on P (x) 4 . In Appendix D.8, we provide our result and proof of lower bound on P (x) 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Main Result</head><p>Lemma 43 (Formal version of Lemma 13). If the given conditions are satisfied</p><p>where A 1 is the part of data has copyright • Let ℓ 1 (x) and ℓ 2 (x) be denoted as Definition 29</p><p>• Let L be denoted as Definition 31</p><p>• Denote P (x) as Definition 44</p><p>• Denote B(x) as Definition 37</p><p>• γ ∈ (0, 1)</p><p>Proof. On one hand, we can show that</p><p>where the first equality follows from Part 1 of Lemma 67, the second equality follows from Part 2 of Lemma 67.</p><p>On the other hand, we have</p><p>where the first equality follows from Definition 44, the second equality follows from ℓ(x) ≥ γ, the third equality follows from Part 2 of Lemma 67, the fourth equality follows from simple algebra.</p><p>D.6 Lower Bound Property for Matrix Function P 2.5 (x)</p><p>Lemma 49. If the given conditions are satisfied</p><p>• γ ∈ (0, 1)</p><p>• Denote f (x) as Definition 24</p><p>• Denote c(x) as Definition 25</p><p>• Let ℓ(x) be denoted as Definition 26</p><p>• Let P 2.5 (x) be denoted as Lemma 47</p><p>Proof. On one hand, we can show that</p><p>where the first equality follows from the Definition of P 2.5 (x), the second equality follows from Part 2 of Lemma 67.</p><p>On the other hand, we have</p><p>D.8 Lower Bound Property for Matrix Function P 5 (x)</p><p>Lemma 51. If the given conditions are satisfied</p><p>• γ ∈ (0, 1)</p><p>• Denote f (x) as Definition 24</p><p>• Denote c(x) as Definition 25</p><p>• Let ℓ(x) be denoted as Definition 26</p><p>• Denote P 5 (x) as Definition 44</p><p>Proof. On one hand, we can show that</p><p>where the first equality follows from Definition 44, the second equality follows from Lemma 46, the third equality follows from Part 1 of Lemma 67, the fourth equality follows from simple algebra.</p><p>On the other hand, we have</p><p>where the first equality follows from Definition 44, the second equality follows from Lemma 46, the third equality follows from ℓ(x) ≥ γ, the fourth equality follows from simple algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Hessian is Lipschitz</head><p>In this appendix, we provide our result that Hessian of L is Lipschitz. In Appendix E.1, we provide our result and proof of</p><p>In Appendix E.2, we provide some helpful lemmas to simplify proofs. In Appendix E.3, we divide H 1 (x) to several part to help us to compute its Lipschitz property. In Appendix E.4, we provide our result and proof of Lipschitz property for matrix function Q 1 (x). In Appendix E.5, we provide our result and proof of Lipschitz property for matrix function Q 2 (x). In Appendix E.6, we provide our result and proof of Lipschitz property for matrix function Q 3 (x). In Appendix E.7, we provide our result and proof of Lipschitz property for matrix function Q 4 (x). In Appendix E.8, we provide our result and proof of Lipschitz property for matrix function Q 5 (x).</p><p>where the first equality follows from Fact 17, the second equality follows from Part 1 of Lemma 23, the third equality follows from ∥f (x)∥ ≤ 1, the fourth equality follows from Part 1 of Lemma 66, the fifth equality follows from Part 2 of Lemma 65, the sixth equality follows from Part 1 of Lemma 65, the last equality follows from simple algebra. Proof of Part 2.</p><p>where the first equality follows from simple algebra, the second equality follows from Part 2 of Lemma 67, the third equality follows from Part 1 of Lemma 63, the fourth equality follows from simple algebra.</p><p>Lemma 64. If the given conditions are satisfied</p><p>• Denote f (x) as Definition 24</p><p>• Denote c(x) as Definition 25</p><p>• Let ℓ(x) be denoted as Definition 26</p><p>We have</p><p>• Denote c(x) as Definition 25</p><p>We have</p><p>where the first equality follows from Part 5 of Lemma 23, the second equality follows from ∥f (x)∥ 2 ≤ 1, the third equality follows from Part 1 of Lemma 66, the fourth equality follows from Part 2 of Lemma 65, the fifth equality follows from Part 1 of Lemma 65, the sixth equality follows from simple algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Helpful Bounds</head><p>In this appendix, we present a collection of bound that are valuable for facilitating computations in our proofs.</p><p>Lemma 66. If the given conditions are satisfied</p><p>as Definition 24</p><p>• Denote c(x) as Definition 25</p><p>• ∥f (x)∥ 2 ≤ 1 then we have</p><p>where the first equality follow from Definition 25, the second equality follows Fact 19, the third equality follows from ∥f (x)∥ 2 ≤ 1, the fourth equality follows from ∥b∥ 2 ≤ 1.</p><p>Proof of Part 2.</p><p>where the first equalities follow from Fact 19, the second equality follows from ∥f (x)∥ 2 ≤ 1, the third equality follows from Part 1 of Lemma 66.</p><p>Lemma 67. If the given conditions are satisfied</p><p>• Denote c(x) as Definition 25</p><p>• Let ℓ(x) be denoted as Definition 26</p><p>• ℓ(x) ≥ γ then we have</p><p>where the first equality follow from Definition 26, the second equality follows from Fact 19 (Cauchy-Schwarz inequality), the third equality follows from Part 1 of Lemma 66. Proof of Part 2. On one hand, we have</p><p>where the first equality follows from Fact 19 (Cauchy-Schwarz inequality), the second equality follows from ∥f (x)∥ 2 ≤ 1, the third equality follows from Part 1 of Lemma 66.</p><p>On the other hand, we have</p><p>where the first equality follows from Fact 19, the second, third, fourth fifth equalities follow from simple algebras.</p><p>Proof of Part 3.</p><p>where the first equality follows from ℓ(x) &gt; γ, the second equality follows from Part 2 of Lemma 67.</p><p>Lemma 68. If the given conditions are satisfied</p><p>• Denote B(x) as Definition 37</p><p>• ∥f (x)∥ 2 ≤ 1 then we have</p><p>where the first equality follows from Lemma 36. For the first term</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Minimizing Loss Guarantee</head><p>In this appendix, our objective is to minimize L to its optimal value, ensuring that we achieve the most favorable outcome in terms of our training process. The minimization guarantee of L confirms our main result on optimization of Copyright Regression, it also demonstrates the ease of use of Copyright Regression, which can be optimized on any attention-based model. We provide our result and proof below.</p><p>Theorem 69 (Minimizing training objective L, formal version of Theorem 15). Suppose we have matrix A ∈ R n×d and A 1 ∈ R n 1 ×d , n 1 ≤ n, vector b, w ∈ R n . Let L be defined as Definition 31, denote x * as the optimal solution of L where g(x * ) = 0 d and ∥x * ∥ ≤ R. Denote R ≥ 10 be a positive scalar. Denote M = n 1.5 exp(60R 2 ), Let x 0 be denoted as an initial point where M ∥x 0 -x * ∥ 2 ≤ 0.1l, where l &gt; 0 denoted a scalar. For any accuracy ϵ ∈ (0, 0.1) and any failure probability δ ∈ (0, 0.1), there exists a randomized algorithm, with probability 1-δ, it runs T = log(∥x 0 -x * ∥ 2 /ϵ) iteration and outputs a vector x ∈ R d such that</p><p>Here w is the exponent of matrix multiplication. Currently w ≈ 2.373.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. Proof of Upper bound on M .</head><p>By Lemma 52, we have</p><p>Hence, we have</p><p>where the first equality follows from Lemma 52, the second equality follows from Lemma 81, the third equality follows from Lemma 72, the fourth, fifth equalities follow from simple algebras. Consider that γ c should not be greater than 0.5, we have</p><p>where the first equality follows from Eq (17), the second equality follows from γ c ≤ 0.5, the third equality follows from simple algebra.</p><p>Proof of Hessian is PD.</p><p>solving Eq. (20) yields</p><p>then we combine two terms, we have</p><p>Theorem 73 (Formal version of Theorem 16). Let x * be denoted the trained parameter on Copyright Regression. Let ℓ(x) be denoted as Definition 4, let ℓ(x) be the original train objective of Softmax Regression. Denote ϵ 2 ∈ (00.1) a scalar. Denote τ c := √ 2γ c /n 1 -ϵ 2 /n 2 , we have</p><p>Proof. We have</p><p>where the first equality follows from Lemma 72, the second equality follows from Definition 71. We define τ c := √ 2γc n 1 -ϵ 2 n 2 , then we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Approximate Newton Method</head><p>In this appendix, we present an adapted version of the Newton method for convex optimization. In Appendix J.1, we outline the assumptions underlying the conventional Newton method, along with the precise update rule employed by the traditional algorithm. Additionally, in Appendix J.2, we introduce the approximate update rule for the modified Newton method. We also provide an implementation tool for computing the approximation of ∇ 2 L, and leverage certain lemmas from <ref type="bibr" target="#b47">[LSZ23]</ref> to analyze the behavior and performance of the approximate Newton method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1 Definition and Update Rule</head><p>In this section, our primary focus is on examining the local convergence properties of the Newton method. We direct our attention towards solving the optimization problem given by: min</p><p>To facilitate our analysis, we make certain assumptions as follows Definition 74 ((l, m)-good Loss function, Definition 8.1 in <ref type="bibr" target="#b25">[DLS23]</ref>). Let L : R d → R be denote as Definition 31, we say L is (l, m)-good when it satisfies the following conditions,</p><p>• l-local Minimum. We define l &gt; 0 to be a positive scalar. If there exists a vector x * ∈ R d satisfies</p><p>If there exists a positive scalar M &gt; 0 with</p><p>We define gradient and Hessian as follows Definition 75 (Gradient and Hessian). . The gradient g : R d → R d of the loss function is defined as</p><p>The Hessian H : R d → R d×d of the loss function is defined as</p><p>With the gradient function g : R d → R d and the Hessian matrix H : R d → R d×d , we define the exact process of the Newton method as follows:</p><p>Definition 76 (Exact update of the Newton method, Definition 8.3 in <ref type="bibr" target="#b25">[DLS23]</ref>).</p><p>x t+1 = x t -H(x t ) -1 • g(x t )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2 Approximate of Hessian and Update Rule</head><p>Definition 77 (Approximate Hessian, Definition 8.4 in <ref type="bibr" target="#b25">[DLS23]</ref>). For any Hessian H(x t ) ∈ R d×d , we define the mated Hessian H(x t ) ∈ R d×d to be a matrix such that the following holds,</p><p>In order to get the approximated Hessian H(x t ) efficiently, here we state a standard tool (see Lemma 8.5 in <ref type="bibr" target="#b25">[DLS23]</ref>).</p><p>Lemma 78 (Lemma 8.5 in <ref type="bibr" target="#b25">[DLS23]</ref>). Let ϵ 0 = 0.01 be a constant precision parameter. Let A ∈ R n×d be a real matrix, then for any positive diagonal (PD) matrix D ∈ R n×n , there exists an algorithm which runs in time Definition 79 (Approximate update). We consider the following process</p><p>We state a tool from prior work, Lemma 80 (Iterative shrinking Lemma, Lemma 6.9 in <ref type="bibr" target="#b47">[LSZ23]</ref>). If the following condition hold </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Adf + 23 ; Rohan Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paige</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10403</idno>
		<title level="m">Palm 2 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName><surname>Adh + 19a] Sanjeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On exact computation with an infinitely wide neural net</title>
		<author>
			<persName><surname>Adh + 19b] Sanjeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A theory for emergence of complex skills in language models</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.15936</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bypass exponential time preprocessing: Fast neural network training via weight-data correlation preprocessing</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Alman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiehao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.14227</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Alman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13214</idno>
		<title level="m">Fast attention requires bounded entries</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the convergence rate of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Try bard, an ai experiment by google</title>
		<author>
			<persName><surname>Bard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-02">February 2023</date>
			<publisher>Google</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><surname>Bce + 23 ; Sébastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Lundberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<idno>BMR + 20</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">All are worth words: A vit backbone for diffusion models</title>
		<author>
			<persName><surname>Bnx + 23] Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22669" to="22679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Training (overparametrized) neural networks in near-linear time</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Den Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Weinstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11648</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Algorithm and hardness for dynamic attention maintenance in large language models</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Zhou ; Beidi Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Winsor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02207</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17413" to="17426" />
			<date type="published" when="2021">2023. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CDW + 21 Scatterbrain: Unifying sparse and low-rank attention</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalization bounds of stochastic gradient descent for wide and deep neural networks</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gram-gauss-newton method: Learning overparameterized neural networks for regression problems</title>
		<author>
			<persName><surname>Cgh + 19 ; Tianle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jikai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11675</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mongoose: A learnable lsh framework for efficient neural network training</title>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chatgpt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaozhuo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">Lingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshumali</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">November 2022. 2020</date>
		</imprint>
	</monogr>
	<note>CLP + 20 Optimizing language models for dialogue OpenAI Blog</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring vision transformers as diffusion learners</title>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianbiao</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.13771</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><surname>Dbk + 20 ; Alexey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">Dall-E</forename><surname>Dall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Creating images from text</title>
		<imprint>
			<date type="published" when="2021-01">January 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dall-E2</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dall•e 2 pre-training mitigations</title>
	</analytic>
	<monogr>
		<title level="j">OpenAI Research</title>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Zero-th order algorithm for softmax attention optimization</title>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Sridhar Mahadevan</surname></persName>
		</author>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08352</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attention scheme inspired softmax regression</title>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10411</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gradient descent provably optimizes over-parameterized neural networks</title>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Simon S Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02054</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">United State Courts for the 9th circuits. Copying-access and substantial similarity. Model Civil Jury instructions</title>
		<imprint>
			<date type="published" when="2022-12">December 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Copyright infringement in ai-generated artworks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">L</forename><surname>Gillotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UC Davis L. Rev</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page">2655</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An over-parameterized exponential regression</title>
		<author>
			<persName><forename type="first">Yeqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Sridhar Mahadevan</surname></persName>
		</author>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.16504</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Differentially private attention computation</title>
		<author>
			<persName><forename type="first">Yeqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.04701</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gradientcoin: A peer-to-peer decentralized large language models</title>
		<author>
			<persName><forename type="first">Yeqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junze</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.10502</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fast quantum algorithm for attention computation</title>
		<author>
			<persName><forename type="first">Yeqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhe</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08045</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Patents in an era of infinite monkeys and artificial intelligence</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hattenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Glucoft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stan. Tech. L. Rev</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fl-ntk: A neural tangent kernel-based framework for federated learning analysis</title>
		<author>
			<persName><forename type="first">Baihe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4423" to="4434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Artificial intelligence and the copyright dilemma</title>
		<author>
			<persName><forename type="first">Kalin</forename><surname>Hristov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="110" />
			<date type="published" when="2016">2016. 2022</date>
		</imprint>
	</monogr>
	<note>HWC + 22 A survey on vision transformer Idea</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Protecting intellectual property of language generation apis with lexical watermark</title>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Hxl + 22] Xuanli He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="10758" to="10766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cater: Intellectual property protection on text generation apis via conditional watermarks</title>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Hxz + 22] Xuanli He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="5431" to="5445" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A phd student&apos;s perspective on research in nlp in the era of very large language models</title>
		<author>
			<persName><surname>Ija + 23] Oana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Ignat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Abzaliev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Biester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naihao</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacky</forename><surname>Gunal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Kazemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.12544</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Dongfu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02561</idno>
		<title level="m">Llm-blender: Ensembling large language models with pairwise ranking and generative fusion</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12292</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><surname>Kgw + 23] John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Kirchenbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Miers</surname></persName>
		</author>
		<author>
			<persName><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.10226</idno>
		<title level="m">A watermark for large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><surname>Reformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<title level="m">The efficient transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning overparameterized neural networks via stochastic gradient descent on structured data. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><surname>Llh + 23] Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Sophia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14342</idno>
		<title level="m">A scalable stochastic second-order optimizer for language model pre-training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generalized leverage score sampling for neural networks</title>
		<author>
			<persName><forename type="first">Ruoqi</forename><surname>Lss + 20] Jason D Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10775" to="10787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The closeness of in-context learning and weight shifting for softmax regression</title>
		<author>
			<persName><surname>Lsx + 23] Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Solving regularized exp, cosh and sinh regression problems</title>
		<author>
			<persName><forename type="first">Zhihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.15725</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deja vu: Contextual sparsity for efficient llms at inference time</title>
		<author>
			<persName><surname>Lwd + 23 ; Zichang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshumali</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22137" to="22176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><surname>Mgn + 23] Sadhika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eshaan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.17333</idno>
		<title level="m">Fine-tuning language models with just forward passes</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bounding the width of neural networks via coupled initialization a worst case analysis</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Omlor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Woodruff</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16083" to="16122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Signal propagation in transformers: Theoretical perspectives and the role of rank collapse</title>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Noci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sotiris</forename><surname>Anagnostidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidak Pal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<idno>NAB + 22</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27198" to="27211" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks</title>
		<author>
			<persName><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Information Theory</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="105" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Panigrahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadhika</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengzhou</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01189</idno>
		<title level="m">Trainable transformer in transformer</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Efficient sgd neural network training via sublinear activated neuron identification</title>
		<author>
			<persName><forename type="first">Lianke</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.06565</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Geigle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Glockner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tilman</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><surname>Adapterdrop</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11918</idno>
		<title level="m">On the efficiency of adapters in transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>RGG + 20</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Archit</forename><surname>Rsm + 23 ; Rafael Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.18290</idno>
		<title level="m">Direct preference optimization: Your language model is secretly a reward model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The new legal landscape for text mining and machine learning</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Sag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Copyright Soc&apos;y USA</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">291</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Intellicode compose: Code generation using transformer</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02896</idno>
		<title level="m">Representational strengths and limitations of transformers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Quadratic suffices for over-parametrization via matrix chernoff bound</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03593</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Does fine-tuning gpt-3 with the openai api leak personally-identifiable information</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Yu Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliott</forename><surname>Zemour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arushi</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udith</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaikkunth</forename><surname>Mugunthan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.16382</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>SZS + 23</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Training multi-layer over-parametrized neural network in subquadratic time</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhe</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07628</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Synthesizer: Rethinking self-attention for transformer models</title>
		<author>
			<persName><surname>Tbm + 21] Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10183" to="10192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><surname>Tda + 20] Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<title level="m">Long range arena: A benchmark for efficient transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Tli + 23] Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Tms + 23] Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Provable copyright protection for generative models</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10870</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vsp + 17 ;</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Medsegdiff-v2: Diffusion based medical image segmentation with transformer</title>
		<author>
			<persName><surname>Wff + 23] Junde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huihui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11798</idno>
		<idno>arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2023. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>WSC + 16</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><surname>Wws + 22] Jason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Infoprompt: Information-theoretic soft prompt tuning for natural language understanding</title>
		<author>
			<persName><surname>Wyw + 23] Junda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Handong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Zheng</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbo</forename><surname>Rosenstock</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.04933</idno>
		<idno>arXiv:2305.18465</idno>
	</analytic>
	<monogr>
		<title level="m">Federated learning of gboard language models with differential privacy</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>XZA + 23</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">An improved analysis of training over-parameterized deep neural networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Speeding up optimizations via data structures: Faster search, sample and maintenance</title>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Just one byte (per gradient): A note on low-bandwidth decentralized language model finetuning using shared randomness</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhl + 23 ; Eric Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.10015</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Why are adaptive methods good for attention models?</title>
		<author>
			<persName><surname>Zkv + 20] Jingzhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Praneeth Karimireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungyeon</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15383" to="15393" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Fast convergence of natural gradient descent for over-parameterized neural networks</title>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Over-parameterized adversarial training: An analysis overcoming the curse of dimensionality</title>
		<author>
			<persName><surname>Zpd + 20] Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orestis</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingguo</forename><surname>Simon S Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="679" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Panigrahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08117</idno>
		<title level="m">Do transformers parse while predicting the masked word? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><surname>Zrg + 22] Susan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
