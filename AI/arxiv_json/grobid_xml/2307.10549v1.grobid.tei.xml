<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Large Language Models on Blockchains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-07-20">20 Jul 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yuanhao</forename><surname>Gong</surname></persName>
							<email>gong.ai@qq.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Large Language Models on Blockchains</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-07-20">20 Jul 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">5C3AC67807CE250A11A3E36CF560DD65</idno>
					<idno type="arXiv">arXiv:2307.10549v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>language model</term>
					<term>blockchain</term>
					<term>decentralize</term>
					<term>neural network</term>
					<term>AIGC</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training and deploying the large language models requires a large mount of computational resource because the language models contain billions of parameters and the text has thousands of tokens. Another problem is that the large language models are static. They are fixed after the training process. To tackle these issues, in this paper, we propose to train and deploy the dynamic large language model on blockchains, which have high computation performance and are distributed across a network of computers. A blockchain is a secure, decentralized, and transparent system that allows for the creation of a tamperproof ledger for transactions without the need for intermediaries. The dynamic large language models can continuously learn from the user input after the training process. Our method provides a new way to develop the large language models and also shed a light on the next generation artificial intelligence systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The development of artificial intelligence and machine learning has led to the evolution of neural networks. These networks have become more complex due to the growing number of parameters. Usually, larger models can achieve higher accuracy and perform better on complex tasks than smaller models.</p><p>The increasing number of parameters in neural networks can lead to challenges in training and deploying these models, as well as difficulties in interpreting the behavior of these increasingly complex models. With the growing number of parameters, neural networks can become computationally expensive, requiring more resources to train effectively. Additionally, the larger size of these models can make them difficult to deploy on devices with limited memory or processing power.</p><p>To reduce the computational cost of training and deploying neural networks, researchers are exploring techniques like model compression, which involves reducing the size of the model by removing unnecessary parts, and architecture search, which involves automatically selecting the best architecture for a given task. One approach to reducing the computational cost of training and deploying neural networks is through the use of techniques such as pruning, quantization, and distillation. These methods aim to reduce the number of parameters and operations required by the model while still maintaining its performance. These techniques can help reduce the computational cost of training and deploying neural networks, making them more accessible to a wider range of applications.</p><p>The trend towards larger and more complex neural networks presents both challenges and opportunities for the field of artificial intelligence. Researchers are actively working to overcome these challenges and harness the full potential of these Manuscript received <ref type="bibr">April 19, 2005</ref>; revised September 17, 2014.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train and develop blockchain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and usage</head><p>Users Developers DLLM Fig. <ref type="figure">1</ref>. Illustration of the proposed method. The dynamic LLM run on blockchains. Users feed the model more and more data while the developers can update the model accordingly. From economy point of view, users pay for professional advice at a very low price. They earn money for their valuable input. Developers pay for the input to train their models but earn money from their trained models. Such ecosystems will benefit both users and developers.</p><p>models. One way to make neural networks more interpretable is by creating visualizations of the model's internal workings and using techniques like saliency maps to understand how different parts of the input contribute to the output. Additionally, techniques like adversarial training can help improve the robustness of the model and make it more resistant to attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Large Language Models</head><p>Large language models (LLMs) are a type of artificial intelligence system that can generate human-like text based on input data. These models are trained on massive amounts of text data and use sophisticated algorithms to learn patterns in language and generate coherent text. LLMs have become increasingly popular in recent years, and their applications range from chatbots and virtual assistants to content creation and language translation.</p><p>One of the most significant advantages of LLMs is their ability to generate high-quality text that is indistinguishable from text written by humans. This makes them a valuable tool in industries such as content creation and journalism, where high-quality writing is essential. LLMs can also be used to generate personalized content for consumers, such as product recommendations or personalized news articles.</p><p>Another benefit of LLMs is their ability to understand the context of text. These models can analyze the meaning behind words and phrases and generate text that is relevant and accurate. This makes LLMs a valuable tool in industries such as search engine optimization and natural language processing (NLP), where understanding the context of text is critical.</p><p>Furthermore, LLMs can be used to automate repetitive tasks, such as customer service inquiries or data entry. This can help businesses save time and money while improving efficiency. LLMs can also be used to generate more accurate translations, making them a valuable tool in the language translation industry.</p><p>However, there are some concerns regarding the use of LLMs. One issue is the potential for bias in the data sets used to train these models. Since LLMs are trained on massive amounts of data, any biases in the data can be amplified and reflected in the generated text. Additionally, there are concerns regarding the potential misuse of LLMs to generate fake news or other forms of misinformation.</p><p>LLMs are a powerful tool that has the potential to transform various industries by automating tasks, generating high-quality content, and improving efficiency. However, it is essential to address the concerns regarding bias and potential misuse to ensure that these models are used ethically and responsibly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Blockchain</head><p>Blockchain technology has emerged as a groundbreaking concept in the digital world in recent years. It is a secure, decentralized, and transparent system that allows for the creation of a tamper-proof ledger for transactions without the need for intermediaries. This technology holds the potential to revolutionize various industries, such as finance, healthcare, and supply chain management. This essay explores the fundamentals of blockchain technology and its possible impact on different industries.</p><p>At its core, a blockchain is a digital ledger of transactions that is distributed across a network of computers. Each block in the chain contains a timestamp and a link to the previous block, forming a chain of blocks. The network of computers that maintains the blockchain ensures that every block added to the chain is valid and unchanged. This makes it nearly impossible to tamper with the ledger or commit fraud, as any change to one block would require a change to every block that follows it.</p><p>The potential uses for blockchain technology are vast and varied. In the finance industry, blockchain technology could streamline processes, reduce costs, and increase transparency. Smart contracts, which are self-executing contracts with the terms of the agreement between buyer and seller being directly written into lines of code, could reduce the need for intermediaries and minimize the risk of fraud.</p><p>In healthcare, blockchain technology could improve security and privacy. Patient records could be stored on a blockchain, which would allow for secure and easy access to medical records by authorized parties. The technology could also help combat counterfeit drugs by tracking the supply chain and ensuring the authenticity of drugs.</p><p>In supply chain management, blockchain technology could improve efficiency and transparency. The technology could be used to track goods from their origin to their final destination, reducing the risk of fraud and ensuring that products are ethically sourced.</p><p>Blockchain technology has the potential to transform various industries by increasing efficiency, security, and transparency. While the technology is still in its early stages, many experts believe that it is the future of the digital world. As blockchain technology continues to evolve, it will be interesting to see how it shapes the future of the industries it impacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Motivation and Contribution</head><p>Traditional LLM is trained on centralized GPUs and can not evolve after the training stage. These drawbacks motivate us to develop LLM on blockchains, where the LLM is trained in decentralized nodes and can evolve during its usage. Our contributions are</p><p>• we propose to develop LLM on blockchains • The proposed LLM can continuously evolve during its usage after the training process • we propose a new economic model to link the LLM with the blockchain technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DYNAMIC LARGE LANGUAGE MODELS</head><p>The development of artificial intelligence has resulted in increasingly complex neural networks with more parameters. This has led to the creation of large language models such as GPT-3 and BERT, which have significantly impacted natural language processing. However, a major challenge associated with these models is their high computational cost. Traditional large language models, like GPT-3 and BERT, require thousands of GPUs for training and deployment. This high cost is a significant obstacle for many researchers and organizations, limiting access to these powerful models.</p><p>Despite this challenge, advancements in hardware and software have made it possible to train and deploy these models on fewer GPUs. This has opened new opportunities for researchers and organizations to explore the capabilities of these models and leverage their potential in various applications.</p><p>Another problem with current LLM is that these models are static. In other words, they can not evolve to achieve higher performance with more and more user input. In fact, millions of users can help to improve the LLM in each specific field, such as poem, medical diagnosis and car design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Decentralized Blockchains</head><p>To tackle the heavy computation requirement in LLM's training and deployment, we propose to develop LLM on blockchains which are decentralized and have high computation performance.</p><p>Large language models have had a major impact on natural language processing (NLP) in recent years. They've achieved state-of-the-art performance on a wide range of NLP tasks. However, training and deploying LLMs can be challenging due to their massive computational requirements and the need for high-quality training data.</p><p>To address these challenges, we propose developing LLMs on blockchains. Blockchains offer a decentralized and transparent platform for storing and processing large amounts of data. By leveraging blockchain technology, we can create a distributed network of nodes that contribute computing power to train and deploy LLMs.</p><p>Developing LLMs on blockchains has many benefits. First, it allows for more efficient use of computing resources by leveraging the idle processing power of network participants. Second, it enables the creation of decentralized datasets that are more secure and transparent than traditional centralized datasets. Third, it provides a more fair distribution of economic incentives for data contributors and validators.</p><p>One of the main challenges for LLMs is the availability of high-quality training data. With blockchains, we can create a decentralized dataset that is more secure and transparent than traditional datasets. This means that the data is less likely to be tampered with and can be easily audited for accuracy and bias. Also, since the data is decentralized, it is not controlled by any single entity, which makes it more accessible to a wider range of researchers and developers.</p><p>Developing LLMs on blockchains also provides a more fair distribution of economic incentives for data contributors and validators. In traditional centralized datasets, data contributors and validators are often not compensated fairly for their contributions. However, with blockchains, data contributors and validators can be rewarded with tokens that have real economic value. This creates a more fair and transparent system that incentivizes participation and collaboration.</p><p>Overall, we believe that developing LLMs on blockchains has the potential to revolutionize the field of NLP by making large-scale language models more accessible to everyone. We're excited to explore this new area of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic Neural Networks</head><p>To tackle the static issue in LLM, in this paper, we propose dynamic LLM which can evolve after the training process. More specifically, the neural network parameters can be continuously updated during their usage after the training process. In other words, the traditional training process is an initialization of the dynamic LLM. The dynamic LLM have the ability to learn during its usage.</p><p>Such behavior is inspired by human brain development which has the life-long learning ability and can increase its knowledge with more and more experience.</p><p>Although LLM has much less neurons than the human brain and each neuron in LLM is much simpler than the real neuron in human cortex, LLM can be inspired by the anatomic and functional dynamics of human brain. For example, the fact that human brain can achieve better with more and more experience indicates that LLM should also have the ability to update their parameters with more usage. We call such LLM as dynamic LLM or DLLM for short.</p><p>Beside the dynamic parameter weights, DLLM also include a dynamic architecture, which is similar to the functional network on human brain cortex. Such functional network points out that one function is supported by several regions and their connectivity. And the same region might play different roles in different functional networks. Such multi functionalities should be imposed into DLLM for improving the model efficiency and reducing the model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Functional Network in Brain Cortex</head><p>The human brain cortex is a highly complex network of functional connections that allow for various cognitive processes, such as perception, attention, memory, and decisionmaking. Scientists have been able to study these functional connections and identify key brain regions and their connectivity patterns using brain imaging techniques such as functional magnetic resonance imaging (fMRI) and electroencephalography (EEG).</p><p>One of the most well-known functional networks in the human brain is the default mode network (DMN). The DMN is active when the brain is at rest and not engaged in any specific task. It includes the medial prefrontal cortex, posterior cingulate cortex, and inferior parietal lobule. This network is believed to be involved in self-referential thinking, mindwandering, and autobiographical memory.</p><p>Another important functional network is the salience network, which is involved in detecting and prioritizing important stimuli. The salience network includes the anterior cingulate cortex and insula. This network is important for our ability to direct our attention towards important stimuli and regulate our emotional responses.</p><p>The attention network, on the other hand, is involved in focusing attention on specific stimuli. This network includes regions such as the frontal eye fields and the parietal cortex. The executive control network, on the other hand, is involved in higher-level cognition such as decision-making and working memory. This network includes regions such as the dorsolateral prefrontal cortex and the anterior cingulate cortex.</p><p>Functional connectivity studies have also identified other networks in the human brain, such as the language network, the visual network, and the sensorimotor network. These networks are involved in processing language, visual information, and sensory information, respectively.</p><p>These brain networks can inspire to design the modules in LLM and the functions of each module. Although the brain network is far from fully understood, they can guide the structure of LLM and their working fashion, especially when the LLM become larger and more complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TRAINING AND RUNNING ON BLOCKCHAINS</head><p>After explaining the DLLM in previous sections, we now explain why they should be trained and run on blockchains from several aspects, including transparency, dynamic price, decentralization and economic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transparency</head><p>Many companies use proprietary data to train their large language models, which can result in highly accurate models. However, this approach also has some drawbacks.</p><p>A major issue is that outside researchers often do not have access to the proprietary input used to train these models. This lack of accessibility makes it difficult for others to replicate or independently verify the models' results.</p><p>The use of proprietary data can make it challenging to interpret the models. With inputs that are not publicly available, it can be hard to determine the driving factors behind a model's predictions. This lack of transparency can be problematic, particularly when the models make important decisions.</p><p>To address these issues, some companies are exploring ways to make their models more transparent and accessible. One method is to fine-tune the models using publicly available data after they have been trained on proprietary data. This can help ensure the models are less reliant on specific inputs and are more generalizable.</p><p>In our DLLM, all the input data must be transparent and available to all researchers. Such requirement makes DLLM more transparent than LLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic Price</head><p>Unlike previous static LLM, DLLM take dynamic input and are dynamic with different tasks. Therefore, it takes different cost and price for easy and difficult tasks. The more difficult the task, the more computation is required and thus the price is higher for its usage.</p><p>Such dynamic price is good for both users and developers. For a easy task such as checking the weather, the user only pays a extremely low price (usually free of charge) because it is not a difficult task. For a more difficult task such as analyzing human's behavior in a set of videos, the user has to pay more money since the task requires more resources.</p><p>If the developers build a simple model, they only pay a low price and their model can only earn money at a low price since the model can only deal with simple tasks. In contrast, if they build a complex model, they have to pay a higher price because the complex model requires more computation. As a result, the complex model can earn money at a higher price because it can deal with complex tasks.</p><p>Such dynamics also corresponds to the dynamics in the network architecture. Easy tasks only use shallow part of networks while difficult tasks use deep and more networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Decentralization</head><p>Current LLM are controlled by companies who might block users from using their LLM. And users might lose their private history on the LLM. To tackle this issue, the decentralization nature of blockchains can play an important role here.</p><p>The user's private history is encrypted and securely stored using blockchain technology. This ensures that the data is protected from unauthorized access or tampering.</p><p>The decentralized nature of the blockchain means that the user's data is not stored in a single location, reducing the risk of data loss due to technical issues or cyber attacks.</p><p>The user's data is protected through strong encryption algorithms that make it extremely difficult for anyone to access the information without authorization. The user has full control over their private history and can decide who can view it.</p><p>Overall, the use of blockchain technology provides a high level of security and privacy for the user's private history. The user can trust that their data is safe and secure, and they do not have to worry about losing it or being blocked by anyone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Win-win for Everyone</head><p>As mentioned, both users and developers can benefit from DLLM. Users' input and task requirements can be considered as valuable resources for developers. Thus, developers have to pay money for such resources. And the models built from developers are valuable resources. Therefore, the users have to pay money for their usage. This is a economical helix structure that benefits everyone in the system. Users get access to the model at a lower price and developers can develop their model at a lower price. Meanwhile, users can use better models if their tasks are more complex. In this case, they have to pay more. If the developers want to build a better model with better input or requiring more computation resource, they also have to pay more.</p><p>A better input from users earns more while the better model from developers also earns more. In contrast, the users' input that does not have value for developers will not earn anything and be abandoned. The developers' models that are not required by any users will not earn anything and be inactive. Finally, the system only contains the valuable input from users and models from developers.</p><p>It's important to recognize that both input and models are dynamic, meaning that the popularity and usage of specific models may change over time. Just like fashion trends in human society, machine learning models can come and go. This is something to consider when developing and implementing models. While a particular model may not be widely used or popular at present, it may become more relevant and widely adopted in the future. Therefore, it's important to continue to explore and evaluate various models, even if they're not currently in use.</p><p>By remaining open to new models and ideas, we can ensure that our machine learning systems are up-to-date and effective in a constantly evolving technological landscape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION AND DISCUSSION</head><p>In this paper, we have presented dynamic large language models that have dynamic architecture and dynamic parameter weights for different tasks. Meanwhile, we propose to perform such models on blockchains, where the decentralization and transparency property can benefit both users and developers.</p><p>Our approach sheds light on the next generation of AI systems that are based on blockchain technology. By combining the power of blockchain with AI, we can create systems that are more secure, transparent, and efficient than ever before.</p><p>Through our research and development, we have identified important areas where blockchain can be applied to AI systems. These areas include data management, model training and validation, and decision-making processes.</p><p>One of the biggest challenges in AI is ensuring the integrity and security of data. By using blockchain, we can create a decentralized and immutable ledger that guarantees the verification and tamper-proof of data. This provides a level of transparency and trust that is crucial for AI systems.</p><p>Moreover, blockchain can improve the model training and validation process. By utilizing a distributed ledger, we can guarantee that models are trained on high-quality data and that the results are accurate and reliable.</p><p>Our approach is a significant step forward in the development of blockchain-based AI systems. We believe that these systems will revolutionize the way we think about AI and its potential applications in various audio, vision and machine learning tasks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Objective comparison of particle tracking methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chenouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Smal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Chaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Sbalzarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cardinale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carthel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Coraluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Godinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalaidzidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E G</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jalden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paul-Gilloteaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roudot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kervrann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Waharte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Tinevez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Shorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willemse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Celler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Van Wezel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ortiz De Solorzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Olivo-Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meijering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="U247" />
			<date type="published" when="2014-03">March 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Symmetry detection for multi-object using local polar coordinate</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">5702</biblScope>
			<biblScope unit="page">277</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BART: denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.13461" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1910">1910.13461. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A survey of large language models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coupled signed-distance functions for implicit surface reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Sbalzarini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Symp. Biomed. Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
			<biblScope unit="page" from="1000" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are fewshot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.14165" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2005">2005.14165. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Local weighted Gaussian curvature for image processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Sbalzarini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Image Proc. (ICIP)</title>
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
			<biblScope unit="page" from="534" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Single image interpolation exploiting semilocal similarity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1722" to="1726" />
			<pubPlace>Brighton, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image enhancement by gradient distribution specification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Sbalzarini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop &quot;Emerging Topics in Image Enhancement and Restoration</title>
		<meeting>Workshop &quot;Emerging Topics in Image Enhancement and Restoration<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-11">Nov 2014</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
	<note>12th Asian Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Side window filtering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8750" to="8758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Spectrally regularized surfaces</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<idno type="DOI">10.3929/ethz-a-010438292</idno>
		<ptr target="http://dx.doi.org/10.3929/ethz-a-010438292" />
		<imprint>
			<date type="published" when="2015">22616. 2015</date>
			<publisher>ETH Zurich</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast and highquality blind multi-spectral image pansharpening</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Boufounos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A natural-scene gradient distribution prior and its application in light-microscopy image processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sbalzarini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="114" />
			<date type="published" when="2016-02">Feb 2016</date>
		</imprint>
	</monogr>
	<note>Selected Topics in Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on blockchain technology and its security</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S2096720922000070" />
	</analytic>
	<monogr>
		<title level="j">Blockchain: Research and Applications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">100067</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Curvature filters efficiently reduce certain variational energies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Sbalzarini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1786" to="1798" />
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Motion saliency based multi-stream multiplier resnets for action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0262885621000135" />
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">104108</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bernstein filter: A new solver for mean curvature regularized models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016-03">March 2016</date>
			<biblScope unit="page" from="1701" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Blockchain-based cross-domain authorization system for user-centric resource sharing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ezawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shiraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morii</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S2096720923000015" />
	</analytic>
	<monogr>
		<title level="j">Blockchain: Research and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">100126</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linear approximation of mean curvature</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="570" to="574" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time optimizing weighted gaussian curvature for 4k videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 31st International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sub-window box filter</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visual Communications and Image Processing</title>
		<meeting>IEEE Visual Communications and Image essing</meeting>
		<imprint>
			<date type="published" when="2018-12">Dec. 2018</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image filtering with generic geometric prior</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">330</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Blind multi-spectral image pan-sharpening</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Boufounos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1429" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weighted mean curvature</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0165168419302282" />
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="329" to="339" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">What do large language models learn about scripts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sancheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rudinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Computing gaussian curvature in real-time for 4k video processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">944</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Structure adaptive filtering for edge-preserving image smoothing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<editor>Image and Graphics, Y. Peng, S.-M. Hu, M. Gabbouj, K. Zhou, M. Elad, and K. Xu</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="265" to="276" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Soft tissue removal in x-ray images by half window dark channel prior</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing (ICIP)</title>
		<meeting>IEEE Int. Conf. Image essing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2019-09">Sep. 2019</date>
			<biblScope unit="page" from="3576" to="3580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Side window guided filtering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Process</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="315" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Computing curvature, mean curvature and weighted mean curvature</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="266" to="270" />
			<pubPlace>Bordeaux, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast and efficient implementation of image filtering using a side window convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page">107717</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Molecular surface estimation by geometric coupled distance functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="176" to="263" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A high performance concurrency protocol for smart contracts of permissioned blockchain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5070" to="5083" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quarter laplacian filter for edge aware image processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1959" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Curvature-based real-time brightness adjustment for ultra hd video</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 24th International Workshop on Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A discrete scheme for computing image&apos;s weighted gaussian curvature</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1919" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A novel structure adaptive algorithm for feature-preserving 3d mesh denoising</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 24th International Workshop on Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gc-net: An unsupervised network for gaussian curvature optimization on images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11265-022-01800-4</idno>
		<ptr target="https://doi.org/10.1007/s11265-022-01800-4" />
	</analytic>
	<monogr>
		<title level="j">Journal of Signal Processing Systems</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="88" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Feature preserving 3d mesh denoising with a dense local graph neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">233</biblScope>
			<biblScope unit="page">103710</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regression-based camera pose estimation through multi-level local features and global features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poslad</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/23/8/4063" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic neural networks: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7436" to="7456" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Training language models with language feedback at scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Scheurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Korbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-03">Mar. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Llama-adapter: Efficient fine-tuning of language models with zero-init attention</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/2303.16199</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
