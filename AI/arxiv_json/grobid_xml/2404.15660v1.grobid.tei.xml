<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinxin</forename><surname>Zheng</surname></persName>
							<email>zhengxinxin2021@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feihu</forename><surname>Che</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinyang</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
							<email>zhangshuai@mail.tsinghua.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Nie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<email>kliu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
							<email>jhtao@tsinghua.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E7351D03F20679D9BF000CF092A78A57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>What star sign is Jamie Lee Curtis? Answer</term>
					<term>Sagittarius Answer</term>
					<term>Sagittarius Question</term>
					<term>What star sign is Jamie Lee Curtis? Evidence Sentence</term>
					<term>She was born on November 22, 1958</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) suffer from the hallucination problem and face significant challenges when applied to knowledge-intensive tasks. A promising approach is to leverage evidence documents as extra supporting knowledge, which can be obtained through retrieval or generation. However, existing methods directly leverage the entire contents of the evidence document, which may introduce noise information and impair the performance of large language models. To tackle this problem, we propose a novel Knowledge Selection of Large Language Models (KS-LLM) method, aiming to identify valuable information from evidence documents. The KS-LLM approach utilizes triples to effectively select knowledge snippets from evidence documents that are beneficial to answering questions. Specifically, we first generate triples based on the input question, then select the evidence sentences most similar to triples from the evidence document, and finally combine the evidence sentences and triples to assist large language models in generating answers. Experimental comparisons on several question answering datasets, such as Triv-iaQA, WebQ, and NQ, demonstrate that the proposed method surpasses the baselines and achieves the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* Corresponding author</head><p>Question: What star sign is Jamie Lee Curtis? Evidence Document: Jamie Lee Curtis is an American actress and author. She is the daughter of renowned actors Tony Curtis and Janet Leigh. She was born on November 22, 1958. Curtis gained fame for her roles in ...</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) have made significant progress in the field of natural language processing, achieving remarkable results in tasks such as text generation <ref type="bibr">[Lin et al., 2023]</ref>, machine translation <ref type="bibr">[Moslem et al., 2023]</ref>, and dialogue systems <ref type="bibr">[Sun et al., 2023b]</ref>. However, despite notable successes in certain areas, LLMs suffer from severe hallucination problems, which may generate contents that deviate from the facts or contain fabricated information <ref type="bibr">[Rawte Figure</ref>  <ref type="figure">1</ref>: The large language model generates the incorrect answer with the given evidence document, while obtaining the correct answer with evidence sentences selected from the evidence document. <ref type="bibr">et al., 2023]</ref>. It has always been a challenge for large language models to handle knowledge-intensive tasks <ref type="bibr" target="#b8">[Petroni et al., 2021]</ref>, such as question answering <ref type="bibr" target="#b17">[Wu et al., 2022;</ref><ref type="bibr" target="#b1">Andrus et al., 2022]</ref> and fact checking <ref type="bibr">[Atanasova et al., 2020]</ref>, since they may potentially provide incorrect or misleading information, leading to task failures or inaccurate results.</p><p>In the question answering task, introducing supporting knowledge related to the input question can effectively alleviate the hallucination problem of large language models <ref type="bibr">[Sun et al., 2023a]</ref>. Previous methods use evidence documents as the external supporting knowledge, providing extra information and validation for the model when generating answers. There are currently two main approaches for acquiring evidence documents: retrieval-based methods <ref type="bibr">[Izacard and Grave, 2021;</ref><ref type="bibr" target="#b0">Abdallah and Jatowt, 2023]</ref> and generationbased methods <ref type="bibr" target="#b19">[Yu et al., 2023;</ref><ref type="bibr">Sun et al., 2023c]</ref>. Retrievalbased methods involve retrieving evidence documents relevant to the input question from large-scale corpus, such as Wikipedia. In contrast, generation-based methods leverage the internal knowledge of large language models to generate evidence documents or background knowledge related to the input question. Existing results demonstrate that generationbased methods significantly improve the accuracy of answering questions, even without incorporating new external information. <ref type="bibr" target="#b19">[Yu et al., 2023]</ref>.</p><p>Although the above methods provide extra knowledge for LLMs to better understand questions, they still suffer from two drawbacks. First, previous methods directly integrate all the contents in the evidence document into LLMs, which may lead to information overload and decrease the accuracy and efficiency of answering questions. Considering an evidence document that involves a large amount of contents, if LLMs need to process and understand the entire contents, they may struggle to accurately extract and utilize the knowledge relevant to the question. As shown in Figure <ref type="figure">1</ref>, using the complete evidence document fails to facilitate large language models to answer the question correctly, while providing precise evidence sentences can lead to an accurate answer. Secondly, existing methods only use a single form of data source for knowledge augmentation <ref type="bibr" target="#b5">[Gao et al., 2023]</ref>, ignoring the interaction and complementary relationship between different forms of knowledge. For example, structured knowledge can provide relations between entities, while textual knowledge can offer more detailed descriptions and contextual information.</p><p>Interestingly, when performing the question answering task, humans leverage their comprehensive capabilities to select key knowledge associated with the question from the evidence document to produce accurate answers. Inspired by this, we propose a novel Knowledge Selection of Large Language Models (KS-LLM) method, which aims to enhance the performance of large language models in the QA task by extracting relevant and useful knowledge from evidence documents. Specifically, we first construct triples based on the input questions, and then select evidence sentences from the evidence document that are most relevant to the triples. Finally, we incorporate the selected evidence sentences with constructed triples as supporting knowledge for LLMs to generate the final answer.</p><p>We conduct comprehensive experiments on three widely used datasets, i.e., TriviaQA-verified, WebQ, and NQ, using three representative large language models, i.e., Vicuna-13B, Llama 2-13B, and Llama 2-7B. Experimental results demonstrate that KS-LLM can significantly improve the performance of large language models on the question answering task, indicating that our method is capable of effectively selecting relevant knowledge from evidence documents for generating accurate answers.</p><p>In summary, our main contributions are as follows:</p><p>• We propose a novel method that can select knowledge snippets that are highly relevant to the input question from the evidence document, improving the accuracy and reliability of large language models in answering questions and alleviating the hallucination problem.</p><p>• Our proposed method combines multiple forms of knowledge, including textual evidence sentences and structured triples, taking full advantages of the interaction and complementary relationship between different forms of knowledge.</p><p>• We demonstrate the effectiveness of the proposed KS-LLM method in the QA task. Extensive experimental results show that our method surpasses different baselines and achieves the best performance on three datasets.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Answering with Evidence Documents</head><p>Evidence documents typically refer to documents containing information relevant to the query question, which are used to facilitate accurate answers or support the reasoning process. Question answering methods with evidence documents are mainly divided into two categories: retrieval-based methods and generation-based methods.</p><p>Retrieval-based methods retrieve documents that may contain the answer strings from a large-scale corpus, and then use the retrieved documents to generate correct answers. Early research utilizes sparse retrieval methods, such as BM25 <ref type="bibr" target="#b4">[Chen et al., 2017]</ref>, or neural ranking models <ref type="bibr" target="#b7">[Guo et al., 2016;</ref><ref type="bibr" target="#b9">Qaiser and Ali, 2018]</ref> to retrieve documents. Representative works of early research include DrQA <ref type="bibr" target="#b14">[Seo et al., 2016]</ref> and <ref type="bibr">BiDAF [Chen et al., 2017]</ref>. Subsequently, dense retrieval models like ORQA <ref type="bibr">[Lee et al., 2019]</ref> and DPR <ref type="bibr" target="#b7">[Karpukhin et al., 2020]</ref> are proposed, which encode contextual information to obtain dense representations of documents. Recent works <ref type="bibr" target="#b10">[Qu et al., 2021;</ref><ref type="bibr" target="#b12">Raffel et al., 2020]</ref> enhance the performance of retrievers to obtain more effective evidence documents, further improving the accuracy of models in answering questions. Rather than relying on external knowledge, generationbased methods extract knowledge from the parameters of large language models to generate evidence documents. Recent research shows that large-scale pre-trained models can form an implicit knowledge base after pre-training <ref type="bibr" target="#b11">[Radford et al., ;</ref><ref type="bibr" target="#b18">Yang et al., 2019]</ref>, which contains a vast amount of knowledge. GenRead <ref type="bibr" target="#b19">[Yu et al., 2023]</ref> is the first work to propose using documents generated by large language models instead of retrieved documents. GenRead <ref type="bibr" target="#b19">[Yu et al., 2023]</ref> and RECITE <ref type="bibr">[Sun et al., 2023c]</ref> generate contextual documents with the implicit knowledge of large language models, and then read the documents to predict final answers.</p><p>Although evidence documents can provide additional knowledge to help answer questions, the above method utilizes all the information in the evidence documents as supporting knowledge, which may introduce noise irrelevant to the query question. Our proposed method effectively extracts the most relevant sentences from the evidence documents to assist the large language model, improving the accuracy and efficiency of answering questions. (3) answer generation. The triple construction and answer generation steps are implemented by large language models, while the evidence sentence selection step is implemented by the vector database. The dashed line indicates the input of each step and the solid line indicates the output of each step. Given a question and its corresponding evidence document as input, our method can effectively extract valuable knowledge from the evidence document to acquire the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Answering with Knowledge Graphs</head><p>Knowledge graphs (KGs) store factual knowledge in the real world, and have advantages in dynamic, explicit, and structured knowledge representation <ref type="bibr">[Pan et al., 2023]</ref>. Question answering methods with knowledge graphs utilize structured knowledge graphs as auxiliary information to improve the performance of question answering systems, usually involving knowledge bases such as Wikidata <ref type="bibr" target="#b16">[Vrandečić and Krötzsch, 2014]</ref> and Freebase <ref type="bibr" target="#b3">[Bollacker et al., 2008]</ref>.</p><p>Early studies <ref type="bibr" target="#b20">[Zhang et al., 2019;</ref><ref type="bibr" target="#b8">Peters et al., 2019;</ref><ref type="bibr">Wang et al., 2021]</ref> require models to learn structured knowledge in knowledge graphs during the training or fine-tuning process, which consumes a large amount of computing resources. Recent methods leverage knowledge by incorporating knowledge graphs into the prompts of large language models and express knowledge graphs in the form of triples. ToG <ref type="bibr">[Sun et al., 2023a]</ref> explores entities and relations through external knowledge bases, dynamically discovering multiple reasoning paths on the knowledge graph to enhance the multi-hop reasoning capabilities of large language models. KGR <ref type="bibr" target="#b6">[Guan et al., 2023]</ref> uses factual knowledge stored in the knowledge graph to correct errors that may occur during the reasoning process, which can automatically alleviate the hallucination problem of large language models. CoK <ref type="bibr" target="#b8">[Li et al., 2023]</ref> leverages query languages to obtain knowledge from structured knowledge sources, improving the factual correctness of large language models.</p><p>Although the above methods improve the performance of large language models on the question answering task, they only utilize a single form of knowledge. Our proposed method simultaneously combines structured triples and textual sentences from evidence documents, taking full advantage of multiple forms of knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The goal of this study is to enhance the performance of large language models on knowledge-intensive tasks by leveraging triples for effective knowledge selection from evidence documents. In this section, we present a detailed description of our proposed approach, KS-LLM, for solving QA tasks. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, KS-LLM consists of three stages: (1) triple construction, which generates a set of triples based on the subject entities in the query question; (2) evidence sentence selection, where the most relevant evidence sentences to the triples are extracted from the evidence document; (3) answer generation, which utilizes the triples and evidence sentences as supporting knowledge to generate the final answer. Next, we will describe each component in KS-LLM respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Triple Construction</head><p>The process of triple construction employs the large language model to generate structured triples based on the natural language question, facilitating the precise capture of the intent and crucial information of the question. Given a query question Q, the process of triple construction aims to generate a set of triples T = {(h i , r i , t i )}, i = 1, ..., m using the large language model, where m is the number of triples, h and t are the head entity and tail entity respectively, and r denotes the relation between the head entity and tail entity. Formally, T is obtained by: T = LM(Q)</p><p>(1) where LM represents a specific large language model. Taking a query question as input, the process of triple construction first identifies the subject entity in the query question, and then generates a set of triples with rich information based on the subject entity. Specifically, we extract the entity related to the topic of the query question, referred to as the subject entity. This entity can be individuals, locations, organizations, or other entities that reflect the core contents of the query question. Next, we construct a set of triples utilizing the subject entity as the head entity. The expanded triples cover various aspects of knowledge closely related to the query question, providing contextual information to the model from multiple perspectives. As illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, we first extract the subject entity Jamie Lee Curtis from question "What star sign is Jamie Lee Curtis?", and then construct several triples with Jamie Lee Curtis as the head entity, such as (Jamie Lee Curtis, occupation, actress).</p><p>By focusing on the subject entity, we ensure that the constructed triples capture the most relevant information necessary for answering the query question. The constructed triples not only help the model better comprehend the query question but also guide large language models in performing complex reasoning, ultimately generating accurate and consistent answers. The process of triple construction is automatically executed by large language models, without requiring additional manual efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evidence Sentence Selection</head><p>Evidence documents refer to documents that provide background information, relevant facts, or supporting knowledge to query questions. However, evidence documents typically contain a large amount of information, and inputting the entire document into a large language model may introduce noise information, making it more difficult for the model to understand and filter relevant knowledge. Therefore, it is crucial to select valuable evidence sentences from evidence documents, which can significantly improve the quality and accuracy of large language models on the question answering task.</p><p>Given the constructed triples T and an evidence document D = (s 1 , s 2 , ..., s n ), where s represents a sentence and n is the number of sentences, the process of evidence sentence selection extracts the evidence sentences S most relevant to the triples T from the evidence document D. Specifically, we initially employ the <ref type="bibr">BERT [Kenton and Toutanova, 2019]</ref> model to obtain the embedding representations of constructed triples and each sentence in the evidence document. This can be formulated as:</p><formula xml:id="formula_0">q = Bert(T ), K = {k i |k i = Bert(s i )}<label>(2)</label></formula><p>where q and K denote the embeddings of triples and the evidence document respectively. The BERT model captures the semantic information and contextual features of sentences by encoding them into dense vectors. Subsequently, in order to measure the semantic similarity, we calculate the Euclidean distance between each sentence and the triples based on embedding representations, and select top k sentences with the closest distance as evidence sentences. The indices of evidence sentences are calculated by: As shown in Figure <ref type="figure" target="#fig_0">2</ref>, we compute the Euclidean distance between the triples (Jamie Lee Curtis, occupation, actress), (Jamie Lee Curtis, birthdate, November 22, 1958), (Jamie Lee Curtis, notable work, Halloween) and each sentence in the evidence document. Then we select the top two sentences with the closest distances as the evidence sentences, i.e., "She was born on November 22, 1958." and "Scorpio corresponds to the solar calendar time from October 23 to November 22.".</p><formula xml:id="formula_1">L = arg k min i {∥k i -q∥ 2 }<label>(</label></formula><p>During the evidence sentence selection step, we can extract the most relevant evidence sentences to the triples from voluminous evidence documents. These sentences contain crucial information related to the query question and provide supporting knowledge for subsequent answer generation. In addition, compared to directly using the entire document as evidence, effective evidence sentence selection eliminates irrelevant information in the evidence document that may hinder the answer. The evidence sentence selection process is implemented through the vector database, which offers the advantage of high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Generation</head><p>We integrate the triples and evidence sentences as supporting knowledge, combine them with the query question, and leverage the reasoning capability of large language models to obtain the final answer. Formally, the final answer A is generated by:</p><formula xml:id="formula_2">A = LM(Q, T, S)<label>(4)</label></formula><p>Previous methods <ref type="bibr">[Sun et al., 2023c;</ref><ref type="bibr">Sun et al., 2023a</ref>] only utilize knowledge graphs or evidence documents as external knowledge to assist large language models in question answering, without considering the interaction between different forms of knowledge. The triples provide structured knowledge in knowledge graphs, while evidence sentences provide detailed information from the evidence document in textual format. By fusing multiple forms of knowledge at different granularities, we are able to provide the model with richer context and factual knowledge, facilitating large language models to generate more accurate and consistent answers.</p><p>Because different models with different sizes possess varying abilities to follow instructions, the output format control in the prompt may differ slightly when generating answers. We expect the model to generate a single entity as the answer so that a fair comparison can be made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct comprehensive experiments of our proposed KS-LLM method on the QA task with evidence documents. We report empirical evaluations of KS-LLM on three widely adopted datasets: TriviaQA-verified <ref type="bibr" target="#b7">[Joshi et al., 2017]</ref>, WebQ <ref type="bibr" target="#b2">[Berant et al., 2013]</ref>  <ref type="bibr">TriviaQA-verified [Joshi et al., 2017]</ref> is a reading comprehension dataset that covers a wide range of topics, consisting of over 650K question-answer-evidence triples. The evidence documents are collected by remote supervision and may include noise which is irrelevant to the question. Therefore, in this paper, we utilize the verified set in TriviaQA, which is manually verified to ensure that each document contains the relevant facts necessary for answering the question.</p><p>WebQ <ref type="bibr" target="#b2">[Berant et al., 2013]</ref> refers to WebQuestions, which is an open-domain question answering dataset containing numerous question-answer pairs. WebQ includes questions that are sourced from the web, aiming to evaluate the performance of QA systems in handling real-world questions without domain restrictions.</p><p>NQ <ref type="bibr">[Kwiatkowski et al., 2019]</ref> refers to Natural Questions, which is a widely used open-domain question answering dataset created by the Google AI team. This dataset contains real-world questions selected from Google search logs and is of significant importance for evaluating and advancing research in question answering systems.</p><p>Due to the absence of evidence documents in WebQ and NQ datasets, we follow the previous work <ref type="bibr" target="#b19">[Yu et al., 2023]</ref> and employ the large language model to generate an evidence document for each question in WebQ and NQ. Specifically, we use Vicuna 13B for evidence document generation.</p><p>We report the exact match (EM) score respectively on the validation set of each dataset in order to evaluate the model performance. An answer is considered correct if and only if its normalized form has a match in the acceptable answer list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fundamental Models.</head><p>We conduct experiments on three representative fundamental large language models with various sizes.</p><p>Vicuna <ref type="bibr" target="#b20">[Zheng et al., 2023]</ref> is an open-source large language model launched by the Large Model Systems Organization (LMSYS Org). Vicuna includes three versions: 7B, 13B, and 33B, and is fine-tuned based on Llama with the open conversation dataset collected by SharedGPT. The latest release, Vicuna 1.5, is fine-tuned based on Llama 2 and supports inputs with a maximum context length of 16K. We utilize the 13B versions of Vicuna 1.5.</p><p>Llama 2 <ref type="bibr" target="#b16">[Touvron et al., 2023]</ref> is an open-source large language model released by Meta Company, including three versions: 7B, 13B, and 70B. Llama 2 is trained on datasets comprising over 2 trillion tokens, and the fine-tuning data includes publicly available instruction datasets, along with over 1 million new annotated examples. We utilize the 7B and 13B versions of Llama 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines.</head><p>We set up six different baselines.</p><p>Standard baseline prompts the large language model to directly output answers to the questions.</p><p>Standard+doc baseline combines the question and the corresponding evidence document as input, prompting the large language model to output the answer. For the Trivia-verified dataset, since the length of its evidence documents may exceed the maximum input length of the model, we set a unified parameter max token to limit the length of evidence documents. In this paper, we set max token to 300.</p><p>CoT+doc baseline follows the same setup as Standard+doc baseline while incorporating the Chain of Thought (CoT) <ref type="bibr" target="#b17">[Wei et al., 2022]</ref> approach.</p><p>KS-Q calculates the embeddings of the question and each sentence from the evidence document, and selects the top k sentences most similar to the question as evidence sentences. Subsequently, KS-Q inputs the question and evidence sentences into the large language model. Instead of using the question, our approach leveraging triples to select evidence sentences.</p><p>KS-T &amp; KS-S baselines utilize the triples generated in the triple construction step and the evidence sentences obtained in the evidence sentence selection step as their supporting knowledge, respectively. In contrast, our proposed method integrates both triples and evidence sentences as supporting knowledge. Then KS-T and KS-S respectively input questions and triples, questions and evidence sentences into the large language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>As reported in Table <ref type="table" target="#tab_0">1</ref>, the proposed KS-LLM method demonstrates superior performance by outperforming multiple baselines and achieving significant advancements across all three datasets. Specifically, in the case of utilizing opensource models, the KS-LLM method achieves impressive EM scores of 58.48, 24.7, and 21.69 on the Trivia-verified, WebQ, and NQ datasets, respectively. Moreover, the KS-LLM method still maintains superior performance compared to methods with evidence documents. For example, compared to the Cot+doc method using Vicuna-13B, our KS-LLM yields substantial enhancements of 8.14 and 3.59 on the Trivia-verified and WebQ datasets, respectively. These results fully demonstrate that our method effectively extracts valuable knowledge from evidence documents, thereby significantly improving the accuracy of large language models in answer generation. Furthermore, our method outperforms the KS-T and KS-S methods, which solely exploit a single form of knowledge, in the vast majority of cases. This indicates that integrating different forms of knowledge enables the effective utilization of the interaction and complementary relationship between knowledge, further enhancing the knowledge capability of large language models.</p><p>We also discover that directly incorporating appropriate evidence documents often leads to minor performance improvements (Standard v.s. Standard+doc). In seven out of nine cases across three datasets using three large language models, incorporating evidence documents results in higher accuracy for the large language models. However, applying the chaining of thought (CoT) technique does not consistently enhance the performance of large language models (Standard+doc v.s. CoT+doc). This could be due to the use of 0-shot prompt in our experiments, where the performance of 0-shot CoT is not stable. Moreover, the choice of fundamental models significantly affects the utilization of knowledge. For example, the Llama 2 model struggles to effectively utilize valid knowledge on the TriviaQA-verified dataset. This may be attributed to the fact that evidence documents for TriviaQA-verified are obtained from the web through distant supervision <ref type="bibr" target="#b7">[Joshi et al., 2017]</ref> and contain non-typical natural language expressions, such as "Sam Smith releases new James Bond title song -Film -DW.COM -25.09.2015", which Llama 2 is not adapted to handle such form of knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Length </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of Evidence Document Length</head><p>The length of external knowledge may affect the performance of large language models. Providing appropriate external knowledge can enhance the performance of the model, while excessively long knowledge may degrade the performance of large language models. The length of evidence document can vary significantly in the real world, making it necessary to select the appropriate document length to facilitate large language models in answering questions.</p><p>To investigate the impact of different evidence document lengths on large language models, we conduct experiments using Vicuna-13B on the TriviaQA-verified dataset. Specifically, we report the performance of the Standard+doc baseline under different evidence document lengths and compare them with the Standard baseline and the proposed KS-LLM method. In addition, we also report the running time required for inference with various lengths of documents on NVIDIA A100 80G. Through this experiment, we aim to gain a deeper understanding of the adaptability of large language models under different evidence document lengths. This is of great help in choosing the optimal evidence document length in practical applications, balancing the requirements of performance and efficiency.</p><p>As shown in Table <ref type="table" target="#tab_1">2</ref>, the performance of large language models in answering questions is significantly influenced by the length of evidence document. Using a 300-token-length evidence document resulted in a 1.24 increase in the evaluation metric compared to not using any evidence document. However, as the length of the evidence document increases, there is a corresponding decrease in performance. This is consistent with the hypothesis from previous research that appropriate external knowledge can improve the performance of the model, while excessively long knowledge has a negative impact on the performance of large language models. The length of the evidence document also affects the inference time of large language models. As the length of the evidence document increases, the inference time proportionally extends. Using a 2000-token-length evidence document takes approximately three times longer for inference compared to not using any evidence document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of Parameter k</head><p>The parameter k represents the number of sentences selected from the evidence document during evidence sentence selection process. We extract the top k sentences with the highest semantic similarity score to the triples from the evidence document and use them for the subsequent answer generation process. The parameter k indicates how the quantity of supporting knowledge affects the performance of large language models. If k is too small, large language models may not have sufficient knowledge for reasoning. If k is too large, additional noisy knowledge may be introduced, interfering with the decision-making of large language models.</p><p>We evaluate the impact of parameter k on the performance of large language models using Vicuna-13B on Triviaverified and WebQ datasets. Specifically, we report the performance of our KS-LLM method under different k values while comparing with the Standard baseline and the Stan-dard+doc baseline.</p><p>Through this experiment, we were able Question: For which team did Babe Ruth blast his last Major League home run? Answer: Boston Braves Output: Babe Ruth (Standard) ; Philadelphia Athletics (Standard+doc) ; Philadelphia Athletics (CoT+doc) ; Boston Braves (KS-LLM) Triple Construction Evidence Sentence Selection Answer Generation Intermediate Output of KS-LLM (Babe Ruth, played for, Boston Red Sox), (Babe Ruth, played for, New York Yankees), (Babe Ruth, played for, Baltimore Orioles), (Babe Ruth, played for, St. Louis Browns), (Babe Ruth, played for, Boston Braves)</p><p>On May 25, 1935, with the team on a road trip and playing at Forbes File in Pittsburgh, Ruth hammered three home runs and a single, driving in six runs. In 1935, Babe Ruth was forty years old, in poor physical shape, and playing out the string with the Boston Braves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boston Braves</head><p>Table <ref type="table">3</ref>: Case study on the TriviaQA-verified dataset with Vicuna-13B. Answer strings that appear during the inference process of the KS-LLM approach are marked in red. to determine the optimal value of k, balancing the quantity of supporting knowledge and the risk of introducing noisy knowledge, which is conducive to improving the performance of large language models. From Figure <ref type="figure" target="#fig_2">3</ref>, it can be observed that the KS-LLM method achieves the best performance on both datasets when k=2, reaching 58.48 and 21.85, respectively. As the value of parameter k increases, there is a gradual decline in the performance of the model. This indicates that the parameter k plays an important role in the process of evidence sentence selection, and the number of evidence sentences directly affects the accuracy of large language models. When the parameter k is set to 2, we are able to achieve the best results using large language models in the question answering task. Furthermore, across different values of k, the proposed KS-LLM method consistently outperforms the Standard baseline and Standard+doc baseline. In the case of utilizing evidence documents, compared with the Standard+doc baseline, the KS-LLM approach achieves a maximum performance improvement up to 5.79 on the TriviaQA-verified dataset, while in the WebQ dataset, the maximum improvement reached 3.94. This demonstrates that effective knowledge selection from evidence documents can significantly enhance the performance of large language models, showcasing the superiority of our KS-LLM method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>To better understand how the proposed KS-LLM method works, we provide a detailed example in Table <ref type="table">3</ref>. Given a question "For which team did Babe Ruth blast his last Major League home run?" as input, the large language model gets the incorrect answer Babe Ruth by directly answering the question. Given the question and its corresponding evidence document as input, the large language model yields the wrong answer Philadelphia Athletics even if the evidence document contains the correct answer string Boston Braves. As for the KS-LLM method, it first indicates that Babe Ruth played for multiple teams in the triple construction step, such as (Babe Ruth, played for, Boston Braves). Then, in the evidence sentence selection step, the crucial sentence "In 1935, Babe Ruth was forty years old, in poor physical shape, and playing out the string with the Boston Braves." containing the answer string is successfully identified from the evidence document. Finally, our proposed KS-LLM approach generates the precise answer Boston Braves according to the triples and evidence sentences.</p><p>Through this example, it can be fully demonstrated that large language models may not be able to effectively utilize the contents of evidence documents, while KS-LLM can extract valuable information from the evidence documents to further generate accurate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce KS-LLM, a novel knowledge selection method for large language models designed to tackle the question answering problem. Given the corresponding evidence documents, the KS-LLM approach effectively identifies the knowledge relevant to the question from evidence documents, thereby enhancing the performance and efficiency of large language models in the question answering task. The proposed method first constructs triples according to the query question, then extracts sentences from the evidence document that are most similar to the triples as evidence sentences, and finally integrates the triples and evidence sentences into the input of large language models to generate accurate answers. Experimental results demonstrate that our method achieves remarkable improvements on three datasets, indicating that KS-LLM is capable of selecting valuable knowledge snippets from evidence documents to assist large language models in answering questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The proposed KS-LLM framework consists of three components: (1) triple construction, (2) evidence sentence selection, and(3) answer generation. The triple construction and answer generation steps are implemented by large language models, while the evidence sentence selection step is implemented by the vector database. The dashed line indicates the input of each step and the solid line indicates the output of each step. Given a question and its corresponding evidence document as input, our method can effectively extract valuable knowledge from the evidence document to acquire the correct answer.</figDesc><graphic coords="3,54.00,54.00,504.01,171.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>3)Here, L = (l 1 , l 2 , ..., l k ) represents the indices of the top k minimum values returned by k arg min, and ∥•∥ denotes the Euclidean distance. Finally, S = {s li |l i ∈ L} is the evidence sentences selected from the evidence document. We set k = 2 in the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Impact of parameter k on TriviaQA-verified and WebQ datasets with Vicuna-13B. We report the exact match (EM) score in the table.</figDesc><graphic coords="7,62.10,241.85,226.75,159.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>, and NQ[Kwiatkowski  et al., 2019]. Following previous works, we use the exact match (EM) score to evaluate the model performance on the QA task. We also evaluate the effectiveness of KS-LLM on two different base LLMs with various sizes: Vicuna<ref type="bibr" target="#b20">[Zheng et al., 2023]</ref> and Llama 2<ref type="bibr" target="#b16">[Touvron et al., 2023]</ref>.Performance comparison on TriviaQA-verified, WebQuestions (WebQ), and Natural Questions (NQ) datasets using three large language models with different sizes. We report the exact match (EM) score in the table, and the best result for each model is bolded.</figDesc><table><row><cell></cell><cell cols="3">TriviaQA-verified</cell><cell></cell><cell>WebQ</cell><cell></cell><cell></cell><cell>NQ</cell><cell></cell></row><row><cell></cell><cell>Vicuna</cell><cell>Llama 2</cell><cell>Llama 2</cell><cell>Vicuna</cell><cell>Llama 2</cell><cell>Llama 2</cell><cell>Vicuna</cell><cell>Llama 2</cell><cell>Llama 2</cell></row><row><cell></cell><cell>-13B</cell><cell>-13B</cell><cell>-7B</cell><cell>-13B</cell><cell>-13B</cell><cell>-7B</cell><cell>-13B</cell><cell>-13B</cell><cell>-7B</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Without evidence document</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Standard</cell><cell>51.45</cell><cell>62.07</cell><cell>51.03</cell><cell>17.91</cell><cell>23.18</cell><cell>17.08</cell><cell>14.93</cell><cell>16.59</cell><cell>15.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">With evidence document</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Standard+doc</cell><cell>52.69</cell><cell>56.97</cell><cell>49.24</cell><cell>18.31</cell><cell>23.47</cell><cell>17.52</cell><cell>17.04</cell><cell>21.06</cell><cell>19.39</cell></row><row><cell>CoT+doc</cell><cell>50.34</cell><cell>55.45</cell><cell>49.52</cell><cell>18.26</cell><cell>22.98</cell><cell>18.46</cell><cell>17.12</cell><cell>20.16</cell><cell>20.36</cell></row><row><cell>KS-Q</cell><cell>52.10</cell><cell>62.76</cell><cell>49.66</cell><cell>20.47</cell><cell>24.66</cell><cell>19.29</cell><cell>15.07</cell><cell>20.00</cell><cell>17.92</cell></row><row><cell>KS-T</cell><cell>52.28</cell><cell>53.66</cell><cell>40.59</cell><cell>21.79</cell><cell>23.82</cell><cell>20.42</cell><cell>15.40</cell><cell>16.92</cell><cell>11.25</cell></row><row><cell>KS-S</cell><cell>51.59</cell><cell>57.79</cell><cell>44.55</cell><cell>20.23</cell><cell>24.11</cell><cell>18.85</cell><cell>15.62</cell><cell>18.89</cell><cell>17.26</cell></row><row><cell>KS-LLM (Ours)</cell><cell>58.48</cell><cell>55.72</cell><cell>44.14</cell><cell>21.85</cell><cell>24.70</cell><cell>21.11</cell><cell>17.59</cell><cell>21.69</cell><cell>20.95</cell></row><row><cell cols="2">4.1 Experimental Setup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Datasets and Evaluation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">We conduct experiments on three representative QA datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Impact of evidence document length on TriviaQA-verified dataset with Vicuna-13B. Time refers to the running time of the large language model for inference on NVIDIA A100 80G. The best result in baselines is underlined and the best result overall is bolded.</figDesc><table><row><cell></cell><cell>(token)</cell><cell>EM</cell><cell>Time</cell></row><row><cell>Standard</cell><cell>0</cell><cell cols="2">51.45 3min40s</cell></row><row><cell>Standard+doc</cell><cell>300</cell><cell cols="2">52.69 4min41s</cell></row><row><cell>Standard+doc</cell><cell>500</cell><cell cols="2">49.52 5min22s</cell></row><row><cell>Standard+doc</cell><cell>1000</cell><cell cols="2">47.45 7min10s</cell></row><row><cell>Standard+doc</cell><cell>2000</cell><cell cols="2">37.66 11min31s</cell></row><row><cell>KS-LLM</cell><cell>-</cell><cell>58.48</cell><cell>-</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Generator-retriever-generator: A novel approach to open-domain question answering</title>
		<author>
			<persName><forename type="first">Jatowt</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.11278</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhanced story comprehension for large language models through dynamic document-based knowledge graphs</title>
		<author>
			<persName><forename type="first">Andrus</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Atanasova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christina</forename><surname>Simonsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Isabelle</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><surname>Augenstein</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2022. 2022. 2020</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="7352" to="7364" />
		</imprint>
	</monogr>
	<note>Atanasova et al., 2020 Generating fact checking explanations Proceedings of the AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><surname>Bollacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">55th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for large language models: A survey</title>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.10997</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.13314</idno>
		<title level="m">Mitigating large language model hallucinations via autonomous knowledge graph-based retrofitting</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL 2021-16th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</editor>
		<meeting><address><addrLine>Kenton Lee; Kenton Lee</addrLine></address></meeting>
		<imprint>
			<publisher>Kenton and Toutanova</publisher>
			<date type="published" when="2016">2016. 2016. 2021. 2017. 2017. 2020. 2019. 2019. 2019. 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
	<note>A deep relevance matching model for adhoc retrieval Izacard and Grave, 2021] Gautier Izacard and Edouard Grave Proceedings of the 25th ACM international on conference on information and knowledge management Leveraging passage retrieval with generative models for open domain question answering Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics Long Papers) Karpukhin et al., 2020 Dense passage retrieval for open-domain question answering Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing Bert: Pre-training of deep bidirectional transformers for language understanding Proceedings of naacL-HLT Natural questions: a benchmark for question answering research Latent retrieval for weakly supervised open domain question answering Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13269</idno>
		<idno>arXiv:2306.08302</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<editor>
			<persName><surname>Moslem</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2023. 2023. 2023. 2023. 2023. 2023. 2019. 2019. 2021</date>
			<biblScope unit="page" from="2523" to="2544" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Text generation with diffusion language models: A pre-training approach with continuous paragraph denoise Adaptive machine translation with large language models Pan et al., 2023 International Conference on Machine Learning Knowledge enhanced contextual word representations Unifying large language models and knowledge graphs: A roadmap Petroni et al., 2021 Kilt: a benchmark for knowledge intensive language tasks In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text mining: use of tf-idf to examine the relevance of words to documents</title>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">;</forename><surname>Qaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahzad</forename><surname>Qaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramsha</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="5835" to="5847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><surname>Radford</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><surname>Rawte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.05922</idno>
		<title level="m">Vipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination in large foundation models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph</title>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.07697</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EACL 2023</title>
		<imprint>
			<publisher>Weiwei Sun</publisher>
			<date type="published" when="2023">2023. 2023. 2023</date>
			<biblScope unit="page" from="2032" to="2043" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Sun et al., 2023b Generative knowledge selection for knowledgegrounded dialogues</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kepler: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2023. 2023. 2023. 2014. 2021</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="176" to="194" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Touvron et al., 2023 Recitation-augmented language models Vrandečić and Krötzsch, 2014 Wikidata: a free collaborative knowledgebase Wang et al., 2021 Llama 2: Open foundation and fine-tuned chat models Communications of the ACM</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An efficient memory-augmented transformer for knowledge-intensive nlp tasks</title>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022. 2022. 2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="5184" to="5196" />
		</imprint>
	</monogr>
	<note>Chain-of-thought prompting elicits reasoning in large language models</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generate rather than retrieve: Large language models are strong context generators</title>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ernie: Enhanced language representation with informative entities</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05685</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2023</date>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Zheng et al., 2023 Judging llm-as-a-judge with mt-bench and chatbot arena</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
