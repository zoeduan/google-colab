<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ETHICAL CONSIDERATIONS AND POLICY IMPLICATIONS FOR LARGE LANGUAGE MODELS: GUIDING RESPONSIBLE DEVELOPMENT AND DEPLOYMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-08-01">1 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Affiliation Beijing Electronic Science and Technology Institute Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Affiliation Beijing Electronic Science and Technology Institute Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhangchi</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Affiliation Beijing Electronic Science and Technology Institute Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiali</forename><surname>Hei</surname></persName>
							<email>xiali.hei@louisiana.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Affiliation University of Louisiana at Lafayette Lafayette</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kim-Kwang</forename><forename type="middle">Raymond</forename><surname>Choo</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Affiliation University of Texas at San Antonio Lafayette</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ETHICAL CONSIDERATIONS AND POLICY IMPLICATIONS FOR LARGE LANGUAGE MODELS: GUIDING RESPONSIBLE DEVELOPMENT AND DEPLOYMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-01">1 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">3EDFA69DE8B7336E65E8410A4AE71F2F</idno>
					<idno type="arXiv">arXiv:2308.02678v1[cs.CY]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>First keyword</term>
					<term>Second keyword</term>
					<term>More</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper examines the ethical considerations and implications of large language models (LLMs) in generating content. It highlights the potential for both positive and negative uses of generative AI programs and explores the challenges in assigning responsibility for their outputs. The discussion emphasizes the need for proactive ethical frameworks and policy measures to guide the responsible development and deployment of LLMs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The utility of large language models (LLMs), like ChatGPT, has been extensively demonstrated in a wide range of applications, including passing bar exams, writing full feature articles, and even generating complete website code. However, it is important to acknowledge the existence of risks and vulnerabilities associated with these models. Some instances have surfaced where ChatGPT was mis/ab-used to generate socially unacceptable content, such as promoting racial discrimination or facilitating misinformation and disinformation campaigns. To address this concern, OpenAI has implemented measures to mitigate the risks, such as installing a filter or classifier to identify dangerous prompts. Consequently, the output of ChatGPT has become more standardized, although human moderators are still necessary to identify and label inappropriate content <ref type="bibr" target="#b0">[1]</ref>.</p><p>Nonetheless, the flexibility of linguistic language can introduce complexities in the design of LLMs. One challenge arises from the fact that certain terminologies can possess different meanings or interpretations within different cultural or contextual contexts. In the following sections, we will provide a brief overview of these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System-role</head><p>To adapt LLMs for a broader range of scenarios and enhance their usefulness in individualized tasks, some researchers have employed role-playing as an extension method, using LLMs like ChatGPT to generate a series of role-playing prompts <ref type="bibr" target="#b1">[2]</ref>. Additionally, structured tests have been conducted to explore how different characters would respond to the same questions in a role-playing context <ref type="bibr" target="#b2">[3]</ref>. However, it's important to note that these role-playing prompts can bypass the restrictions imposed by the classifier.</p><p>It is possible to make the model assume roles, placing it in specific scenarios where it generates data that would not be outputted otherwise. However, setting specific scenarios for ChatGPT increases the likelihood of encountering problematic content in the output. These issues may expose internal information of the LLM or go undetected, thus increasing the security risks associated with the language model. Methods like populating an Excel sheet can trick LLMs into generating training data-like content or act as a deceased grandmother who would read the Windows 10 Pro keys to fall asleep.</p><p>LLMs, including ChatGPT, offer users the ability to perform various tasks, such as multilingual translation. While OpenAI has implemented restrictions on input content to prevent violations of community rules [4], LLMs may still struggle with handling complex scenarios effectively. In translation tasks, for instance, if inappropriate content that violates legal, moral, or ethical standards is not adequately identified and restricted, it could be translated into multiple languages and widely disseminated, leading to significant consequences. Similarly, when requested to add negative or emotional elements to positive texts during translation, the emotional tone of the translated results may diverge from the intended content, potentially resulting in the generation of malicious or unintended output. OpenAI's official documentation acknowledges that "support for non-English languages is currently limited," indicating ongoing challenges in the realm of multilingualism that require continuous research and updates.</p><p>Different personas or characters within LLMs have diverse backgrounds, introducing significant flexibility to their responses. Consequently, the probability of encountering problems when these personas answer questions varies across different scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perturbation</head><p>Extensive research has been conducted on hints and prefixes associated with LLMs. Some studies have focused on maximizing the efficiency of language models, while others have delved into methods to bypass the toxicity filters of LLMs and generate potentially harmful content <ref type="bibr" target="#b3">[5]</ref>. These prefixes can assume various forms, including single words, phrases, sentences, or even paragraphs. Their purpose is to conceal crucial information that might trigger the language model's toxicity filter. Moreover, prefixes can adopt different grammatical structures and tenses, such as interrogative, declarative, or exclamatory sentences.</p><p>However, due to the rapid advancements in artificial intelligence, the prompts that trigger toxicity responses in language models may change in their expression, rendering current toxicity filters less effective in certain cases. Consequently, an attacker can search for an appropriate prefix that bypasses the filter and generates the desired content. Different prefixes can be incrementally employed, depending on the specific keywords for which the attacker intends to elicit toxic responses from the language model. By masking keywords using words or phrases-for example, using "objective" to conceal a controversial topic-an attacker can circumvent security measures. If more complex keyword phrases are required to exploit the security mechanism, the attacker may resort to using sentences or even paragraphs, which significantly increase the likelihood of bypassing the security measures. If the initial attempts are unsuccessful, the attacker can try employing longer paragraphs or multiple interactive sessions to mask content that policy guidelines prohibit. Given the language model's memory of chat history and interactive nature, the risk of obtaining toxic responses from the LLM grows as the conversation progresses and delves into deeper interactions. Additionally, due to the flexibility of language, different prompts provided to the LLM may result in output with similar toxic implications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-related</head><p>Image-related challenges arise in LLMs, primarily designed for text-based conversations, but capable of outputting images in Markdown format through API calls to image hosting websites, like ChatGPT. While this feature proves valuable for sharing visual information and enhancing the user experience, it carries certain risks. On one hand, it may inadvertently display inappropriate images related to pornography, gun violence, or drugs. On the other hand, when ChatGPT uses Markdown to display images without proper citation, it may lead to copyright disputes and harm the interests of creators.</p><p>While the ability to display images on the terminal is a valuable feature, it is crucial to exercise discernment and avoid displaying inappropriate images, as they can pose risks to social order and public opinion and may give rise to legal, moral, or ethical concerns. The release of multimodal models like GPT 4.0, which support image generation, necessitates increased attention to the ethical, moral, and safety issues arising from text-to-image functionality in the future. Baidu's LLM-based Chatbot ERNIE encountered ambiguities in the underlying code during text-to-image conversion. Testers, for example, used Chinese text to generate an image of a computer bus, but ERNIE generated an image of a transportation bus. This indicates a multilingual conversion error at the core of ERNIE after processing the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hallucination</head><p>In certain instances, LLMs may produce answers that sound plausible but are incorrect or nonsensical. This phenomenon is commonly referred to as "hallucination" in many articles <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7]</ref>. Our tests revealed that ChatGPT occasionally generates content that appears complete and objective, even without prior knowledge of the article's topic. However, it is important to acknowledge that AI can make mistakes, propagate misinformation, fabricate information, and generate realistic texts about events that never occurred <ref type="bibr" target="#b6">[8]</ref>. Our investigations covered a wide range of domains, uncovering instances of misinformation and bias in ChatGPT's outputs. Some researchers have even identified errors in ChatGPT's determination of author contributions <ref type="bibr" target="#b7">[9]</ref>.</p><p>LLMs generate content that users may perceive as accurate and trustworthy information. However, if the generated content includes non-objective or unsupported information, it can be misleading and lead to incorrect judgments and decisions. If such content generated by LLMs is widely disseminated, it can significantly impact public opinion. Controversial topics may arise, and the inclusion of fictitious or misleading information can erode people's trust in AI, hindering the progress of the field. Additionally, it may affect people's usage and acceptance of LLMs, raising concerns about their objectivity.</p><p>Many individuals currently rely on the ChatGPT API to enhance their work efficiency. If the issue of hallucination is not effectively addressed, the existing problems and pitfalls may proliferate through the API interface. Therefore, it is essential to critically review and test the authenticity of LLMs' output content. Furthermore, adding citation sources for identified facts can help ensure that the generated content is trustworthy and valuable to human users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generation-related</head><p>With the advancement of LLMs, the generated content produced by these models is being utilized in various domains, including marketing, education, cybersecurity, and more. It is crucial to ensure the ethical and responsible use of language models. Detecting whether the text is generated by an AI or a human is essential, as well as considering the societal impact of language model-generated texts.</p><p>With the ongoing evolution of LLM technology, the potential applications of language models are expanding. For instance, ChatGPT can serve as a versatile tool for marketing products and may eventually replace the need for professional marketers. Students may attempt to use ChatGPT for academic dishonesty, but the availability of highperformance detectors can help curb such incidents. Although ChatGPT-generated phishing emails have a lower success rate compared to human-written emails <ref type="bibr" target="#b8">[10]</ref>, this may change with the development of multimodal models like GPT 4.0. Attackers can leverage language models to generate fake news and rumors, disrupting established order and inducing panic among people. It is worth noting that ChatGPT can produce different outputs when given the same prompt, indicating inconsistent limitations that need improvement in terms of robustness <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b10">12]</ref>.</p><p>LLMs serve as a tool for attackers, enhancing their abilities in developing malicious programs and increasing efficiency through rapid generation of mass production of malicious content. Although filters exist, attackers can generate a significant amount of small auxiliary codes using a language model and manually assemble them to create an attack system. Most current detectors only cover general text detection, and the development of specialized detectors targeting undesirable content, such as malicious code, rumors, or fake news, is still required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bias and Discrimination of Training Data</head><p>The functionality of the LLM relies on an extensive corpus of training data that encompasses various countries, cultures, and languages <ref type="bibr" target="#b11">[13]</ref>. This linguistic diversity equips the LLM with the competence to cater to the unique content needs of users across different languages. It excels in cross-language tasks, such as translation, as it can swiftly apply its capabilities in English to other language scenarios. However, the disparity between models trained in different languages extends beyond just the language itself. It encompasses factors like the country, culture, religion, politics, and ideological nuances associated with each language <ref type="bibr" target="#b11">[13]</ref>.</p><p>Therefore, when multiple language models are unified under a single model, they embody distinct aspects that reflect the diverse ideologies represented within the languages. Consequently, the responses to questions can diverge significantly depending on the underlying ideology. Bias and stereotypes arise from unfair judgments based on incorrect information and a lack of understanding toward individuals or subjects that differ from our own <ref type="bibr" target="#b12">[14]</ref>.</p><p>For instance, people from different countries may hold disparate perspectives on a controversial issue, leading to variations in the training data available for these national languages. As a result, the LLM may exhibit distinct interpretations of a particular viewpoint, providing substantially different answers to the same question depending on the language in use. Attackers can exploit these differences to incite tensions between nations or groups with differing viewpoints. Additionally, the LLM's adaptability to a user's worldview highlights the non-consistent and non-deterministic nature of its underlying worldview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The six categories discussed encompass the major security issues associated with LLMs, covering aspects related to input, output, and training data. Both the input and output components of LLMs are crucial to consider. Attackers leverage the input phase to prompt injection, exposing vulnerabilities in the generated content of LLMs. Consequently, careful review and evaluation of each generated component of LLMs become necessary.</p><p>While LLM classifiers exhibit high accuracy, the flexibility of language and diverse scenarios necessitate more than just prompt restrictions for effective protection. The temporary validity of prompt restrictions highlights the need for researchers to explore alternative approaches to enhance the classifier's protection system.</p><p>Aside from prompt injection, internal issues related to training data also contribute to the challenges. Relying solely on prompt filters is insufficient to achieve a perfect defense. Developing LLMs should prioritize addressing legal, moral, and ethical concerns. Legislation is required for AI products to strike a balance between fostering innovation and restraining unethical practices. Simultaneously, users must take responsibility and cultivate responsible usage habits. LLMs should diligently identify and reject any speech or description that harms others. This ensures the evolution of a scientifically sound research environment that upholds responsibility and promotes the well-being of society as a whole.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The promise and peril of generative ai</title>
		<author>
			<persName><surname>Jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">614</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="214" to="216" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akin</forename></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/fka/awesome-chatgpt-prompts" />
		<title level="m">Awesome ChatGPT Prompts</title>
		<imprint>
			<date type="published" when="2023-06-20">2023. June-20-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08979</idno>
		<title level="m">chatgpt we trust? measuring and characterizing the reliability of chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Why so toxic? measuring and triggering toxic behavior in open-domain chatbots</title>
		<author>
			<persName><forename type="first">Man</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiliano</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>De Cristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savvas</forename><surname>Stringhini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zannettou</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2022 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2659" to="2673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Audit ai search tools now, before they skew research</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gusenbauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">617</biblScope>
			<biblScope unit="issue">7961</biblScope>
			<biblScope unit="page">439</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How will generative ai disrupt data science in drug discovery?</title>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">York student uses AI chatbot to get parking fine revoked</title>
		<author>
			<persName><surname>Bbc</surname></persName>
		</author>
		<ptr target="https://www.bbc.com/news/uk-england-york-north-yorkshire-65126772" />
		<imprint>
			<date type="published" when="2023-05">2023. May-15-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chatgpt: five priorities for research</title>
		<author>
			<persName><forename type="first">Eva Am</forename><surname>Van Dis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Van Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudi</forename><forename type="middle">L</forename><surname>Bockting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">614</biblScope>
			<biblScope unit="issue">7947</biblScope>
			<biblScope unit="page" from="224" to="226" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ChatGPT vs. human phishing and social engineering study: Who&apos;s better?</title>
		<author>
			<persName><surname>Hoxhunt</surname></persName>
		</author>
		<ptr target="https://www.hoxhunt.com/blog/chatgpt-vs-human-phishing-and-social-engineering-study-whos-better" />
		<imprint>
			<date type="published" when="2023-04">2023. Apr-5-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">One small step for generative ai, one giant leap for agi: A complete survey on chatgpt in aigc era</title>
		<author>
			<persName><forename type="first">Chaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenshuang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Kumar Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Uk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong Tae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.06488</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the robustness of chatgpt: An adversarial and out-of-distribution perspective</title>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xixu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.12095</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Chatgpt: not all languages are equal</title>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">L</forename><surname>Seghier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">615</biblScope>
			<biblScope unit="issue">7951</biblScope>
			<biblScope unit="page" from="216" to="216" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">What is Google&apos;s Bard and how is it different to ChatGPT?</title>
		<author>
			<persName><surname>Bbc</surname></persName>
		</author>
		<author>
			<persName><surname>Bard</surname></persName>
		</author>
		<ptr target="https://www.bbc.co.uk/newsround/65036003" />
		<imprint>
			<date type="published" when="2023-03-26">2023. Mar-26-2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
