<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-in-context learning in large language models</title>
				<funder ref="#_xFzfqjN">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Julian</forename><surname>Coda-Forno</surname></persName>
							<email>julian.coda-forno@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Biological Cybernetics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Tübingen -Tübingen</orgName>
								<address>
									<country>Germany;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcel</forename><surname>Binz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Biological Cybernetics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Tübingen -Tübingen</orgName>
								<address>
									<country>Germany;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Google DeepMind -London</orgName>
								<address>
									<country>United-Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Google DeepMind -London</orgName>
								<address>
									<country>United-Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Schulz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Biological Cybernetics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-in-context learning in large language models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9467B6F372D7A261817EFF0577DBD8D4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models have shown tremendous performance in a variety of tasks. In-context learning -the ability to improve at a task after being provided with a number of demonstrations -is seen as one of the main contributors to their success. In the present paper, we demonstrate that the in-context learning abilities of large language models can be recursively improved via in-context learning itself. We coin this phenomenon meta-in-context learning. Looking at two idealized domains, a one-dimensional regression task and a two-armed bandit task, we show that meta-in-context learning adaptively reshapes a large language model's priors over expected tasks. Furthermore, we find that meta-in-context learning modifies the in-context learning strategies of such models. Finally, we extend our approach to a benchmark of real-world regression problems where we observe competitive performance to traditional learning algorithms. Taken together, our work improves our understanding of in-context learning and paves the way toward adapting large language models to the environment they are applied purely through meta-in-context learning rather than traditional finetuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) are taking not only machine learning research but also society by storm <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Part of what makes these models so persuasive is that their abilities reach far beyond what we expected pure language models to do. They can, among other things, solve challenging reasoning problems, including university-level math questions <ref type="bibr" target="#b3">[4]</ref> or analogical reasoning tasks <ref type="bibr" target="#b4">[5]</ref>, out-of-the-box and without additional training.</p><p>Much of this power comes from what is known as in-context learning <ref type="bibr" target="#b5">[6]</ref>. In-context learning (sometimes also called few-shot learning or few-shot prompting) refers to the ability of an LLM to improve at a given task after being provided with a number of task-relevant demonstrations. This ability sets LLMs apart from traditional models and led to a totally new paradigm -one which eschews finetuning of weights on task-specific data altogether and instead relies entirely on contextual information.</p><p>In the present paper, we ask whether the learning algorithm implemented through in-context learning can be improved through in-context learning itself (without the need for any further finetuning of parameters). To study this question, we conducted several experiments where we presented an LLM with multiple learning tasks in a sequence. In three distinct settings, we find evidence for the idea that the in-context learning abilities of an LLM can be recursively enhanced via in-context learning, thereby displaying a form of meta-in-context learning. Figure <ref type="figure" target="#fig_0">1</ref> provides a high-level overview of our approach on an example supervised learning task.</p><p>More specifically, we first investigate meta-in-context learning on two artificial domains: a supervised function learning task, and a two-armed bandit task. For both of them, we find that sequentially We present an LLM with N learning tasks in a row. Improvement within a task indicates that the model is capable of in-context learning. If in-context learning improves across multiple learning tasks, the model is also capable of meta-in-context learning.</p><p>presenting LLMs with multiple learning problems boosts their in-context learning abilities. We then use these idealized domains to identify the drivers behind meta-in-context learning. We find that meta-in-context learning adaptively modifies priors over latent variables, ultimately leading to priors that closely resemble the true statistics of the environment. Furthermore, our analysis reveals that meta-in-context learning can not only be used to change prior expectations but is also capable of reshaping an LLM's learning strategies. Lastly, we apply our approach to a realistic domain and demonstrate that meta-in-context learning can be used to obtain a learning algorithm that is competitive with traditional algorithms on a benchmark of real-world regression problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In-context learning: Recent work has shown that LLMs can improve their performance after being shown a few task-relevant demonstrations -an ability referred to as in-context learning <ref type="bibr" target="#b5">[6]</ref>. When and why in-context learning emerges is a matter of ongoing debate, with different theories being proposed. Chan et al. <ref type="bibr" target="#b6">[7]</ref> argued that properties of the training data -such as burstiness and non-stationarityare key drivers behind in-context learning. Min et al. <ref type="bibr" target="#b7">[8]</ref>, on the other hand, found that ground truth demonstrations can be replaced with random labels while barely hurting performance, suggesting that the role of demonstrations is more to prime the model for a particular task. Finally, Xie et al. <ref type="bibr" target="#b8">[9]</ref> suggested that LLMs internally need to infer latent variables to make better predictions about future word occurrences, thereby implementing a form of Bayesian inference.</p><p>In-context learning can solve classical learning tasks: If LLMs do apply some form of Bayesian inference, then one would expect an LLM to also be able to solve classical online learning tasks, such as regression or classification, purely through in-context learning. Previous research suggests that this is indeed the case. Lovre <ref type="bibr" target="#b9">[10]</ref>, for instance, tested GPT-3 on a range of low-dimensional classification and regression tasks and found that it was often on par with classical learning algorithms such as logistic regression. Likewise, Hegselmann et al. <ref type="bibr" target="#b10">[11]</ref> tested an LLM's few-shot classification abilities on tabular data. They found that their approach outperforms prior deep-learning-based tabular classification methods on several benchmark datasets, and that performance further improves when the model is provided with semantic information about the data. <ref type="foot" target="#foot_1">1</ref> LLMs are not only able to solve supervised learning problems but also simple reinforcement learning tasks. To provide one example, Binz &amp; Schulz <ref type="bibr" target="#b11">[12]</ref> evaluated GPT-3 on a two-armed bandit task and found that its performance exceeded that of human participants who did the corresponding psychological experiment.</p><p>Meta-in-context versus classical meta-learning schemes: Meta-in-context learning stands in contrast to classical meta-learning schemes in which one adapts a neural network to a distribution over learning problems by adjusting its weights <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Historically, this approach has relied on recurrent networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> but, more recently, researchers have also started to use transformer-based architectures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. For example, Garg et al. showed "transformers can be trained from scratch to perform in-context learning of linear functions" and that the resulting models achieve "performance comparable to the optimal least squares estimator." In a similar vein, von Oswald et al. argued that "transformers [can in principle] implement gradient descent in their forward pass" and provide empirical evidence that they indeed do so <ref type="bibr" target="#b18">[19]</ref>. In contrast to these approaches, our approach adapts a model to a distribution of learning problems entirely through the context itself as opposed to updating weights).</p><p>Meta-in-context learning and psychological experiments: Human subjects in psychological experiments are typically evaluated using multiple successive learning problems. Lampinen <ref type="bibr" target="#b21">[22]</ref> highlighted that this is in contrast to the common practice when evaluating LLMs, which involves probing each task in isolation. Meta-in-context learning addresses this issue by matching the testing procedure of psychological experiments. Therefore, our upcoming analyses also provide insights into how to compare LLMs to human behavior <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental analyses</head><p>We used the public OpenAI Python API <ref type="bibr" target="#b25">[26]</ref> to run all our simulations. This API provides access to several LLMs from the Generative Pre-trained Transformer (GPT) family. We ran all our simulations on the TEXT-DAVINCI-002 model, which is also known as GPT-3. We set the temperature parameter to zero (leading to deterministic responses) unless otherwise noted and retained the default values for all other parameters. It is important to note that all experiments performed in this paper rely entirely on the in-context learning abilities of an LLM, and do not involve any form of finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning one-dimensional functions</head><p>In our first set of experiments, we investigated GPT-3's ability for meta-in-context learning in a simple one-dimensional supervised regression task. In this setting, we provided GPT-3 with a list of input-target pairs from a given task and asked it to make accurate predictions for a new input value. This is one of the most fundamental machine learning problems, and its simplicity makes it an ideal testbed for an initial analysis. Every task n consisted of T = 5 input-target pairs (x t , f (x t )), where t ∈ [1, T ] denotes the trial number. Each input-target pair was generated by a linear function of the form f (x t ) = a (n) x t + b (n) + ε t , where a (n) and b (n) are task-specific parameters drawn from a probability distribution p(T ). Inputs x t were sampled from U(0, 100) and the trial-specific additive noise ε t was sampled from N (0, 1). For our meta-in-context learning simulations, we considered prompts that include data from up to five tasks, each corresponding to a different underlying function. Following the schema outlined in Figure <ref type="figure" target="#fig_0">1</ref>, we iteratively presented each data-point together with the history of previous observations, starting from the first trial in task one up to the last trial in task five. In the box below, you can see an example prompt that we provided to GPT-3:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function learning prompt</head><p>You observe 5 machines that produce an output y for a given input x. Each machine implements a different function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Results</head><p>In preliminary simulations, we found that GPT-3 has a strong bias toward increasing positive functions.</p><p>To demonstrate the potential of our approach, we wanted to investigate whether it is possible to overwrite this bias via meta-in-context learning. In order to achieve this goal, we sampled taskspecific functions with a negative slope and intercept, i.e. a (n) ∼ N (-2, 1) and b (n) ∼ N (-100, 1), and evaluated how observing multiple such tasks influences the model's behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-3 does in-context learning:</head><p>We first established that GPT-3 can learn within a single task without considering the effects of meta-in-context learning. To do so, we only examined the first task while ignoring all the subsequent ones. We found that performance as measured by the mean-squared error (MSE) improves with additional data-points as shown in Figure <ref type="figure" target="#fig_1">2A</ref> (solid lines). GPT-3 matches (or even slightly outperforms) a Bayesian linear regression (BLR) model with default standard normal priors, indicating that in-context learning is able to solve the given problem. This is especially noteworthy since GPT-3 was never told that the underlying functions are linear, as opposed to BLR which has a built-in assumption of linearity.</p><p>GPT-3 does meta-in-context learning: Next, we investigated whether GPT-3 is capable of meta-incontext learning. For this, we inspected how performance changes across the five tasks. Figure <ref type="figure" target="#fig_1">2A</ref> demonstrates that meta-in-context learning is beneficial by comparing performance between the first and the final task (solid vs dashed lines). For reference, we also plotted the performance of a BLR model with access to the ground-truth data-generating distribution. Note that this model has access to privileged information and hence only serves as a performance upper-bound. Figure <ref type="figure" target="#fig_1">2B</ref> shows a more detailed development of performance as we increase the number of tasks in the prompt.</p><p>To test whether the effects of in-context learning and meta-in-context learning are statistically meaningful, we fitted a linear regression model that included the trial and task number as independent variables on the MSE. We found statistically significant effects for both trial number (β = -30.614 ± 3.08, t = -19.514, p &lt; 0.001) and task number (β = -13.26 ± 3.08, t = -8.455, p &lt; 0.001), confirming that GPT-3 is capable of both in-context and meta-in-context learning (see Figure <ref type="figure" target="#fig_1">2C</ref>).</p><p>Meta-in-context learning is driven by adaptation of priors: We speculated that GPT-3's performance improves during meta-in-context learning because it adapts its priors to true environmental statistics. In order to evaluate this hypothesis, we collected GPT-3's prior expectations before each task by asking it to make sequential predictions for 20 evenly spaced input values (using the same prompt template as above, but setting the temperature to one and feeding back the model's own predictions as the training data). We omitted outlier predictions that had absolute values of 10, 000 or larger for this analysis. The estimated priors indicate that GPT-3 has an initial bias toward increasing positive functions. However, already after two tasks it adapts its priors to decreasing negative functions, thereby closely matching the true environmental statistics (see Figure <ref type="figure" target="#fig_1">2D</ref>). Furthermore, the prior for the intercept term is initially centered around zero but shifts toward the ground-truth value of -100 after three tasks.</p><p>Taken together, these simulations provide initial evidence that GPT-3 engages in meta-in-context learning, meaning that its in-context learning abilities can be recursively improved via in-context learning itself. We have suggested and empirically confirmed that GPT-3 accomplishes this by adapting its priors across multiple tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments on two-armed bandit tasks</head><p>In a next step, we wanted to investigate whether our results from the supervised setting also transfer to a reinforcement learning paradigm. This setting adds an additional layer of complexity because it requires the agent to learn from its own experiences instead of having access to ground-truth solutions of previous tasks. In addition, it allows us to investigate learning strategies and how they evolve during meta-in-context learning.</p><p>For our simulations, we considered a simple two-armed bandit task, in which an agent repeatedly interacts with two slot machines. In each trial, the agent can select one of two machines and is rewarded based on a probability distribution that is associated with that machine. The agent's objective is to maximize the total amount of acquired points. We used a cover story that involves a gambler visiting different casinos, which has been used in human experiments with similar tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref>, to generate our prompts:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-armed bandit task prompt</head><p>You are going to different casinos that own two slot machines.</p><p>Choosing the same slot machine will not always give you the same points, but one slot machine is always better than the other. Within a casino, your goal is to choose the slot machine that will give you the most points over the course of 10 trials. Each casino owns a different pair of machine.</p><p>You have received the following points when playing in casino 1:</p><formula xml:id="formula_0">[. . .]</formula><p>You have received the following points when playing in casino 5: -Machine J delivered 4.2 points.</p><p>-Machine F delivered -7.4 points.</p><p>-Machine J delivered 3.2 points.</p><p>-Machine J delivered 3.9 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q:</head><p>We are now performing trial 5 in casino 5. Which machine do you choose between machine J and machine F? A: Machine [insert].</p><p>A B C 2 4 6 8 10 Trials 0 2 4 Regret GPT-3 GPT-3 after 5 tasks Greedy UCB 1 2 3 4 5 Tasks 1.0 1.5 2.0 Regret GPT-3 Greedy TrialTask -0.20 -0.15 -0.10 -0.05 0.00 Regression coefficients D E 1 2 3 4 5 True Tasks 0 100 200 Reward priors </p><formula xml:id="formula_1">V R U V / T U T a s k × V T a s k × R U T a s k × V / T U -0.5 0.0 Regression coefficients</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Results</head><p>For each task, we sampled independent mean rewards for each machine from N (0, √ 64). The actually obtained reward is generated using the mean reward that corresponds to the chosen machine plus some additive Gaussian noise sampled from N (0, √ 32). To stay consistent with the previous section, we considered prompts that include data from up to five different tasks for our meta-in-context learning simulations (each of these tasks consisted of ten trials). We used letters to indicate the different slot machines. For each task, we randomly sampled two different letters without replacement to cancel out biases towards certain letters. <ref type="foot" target="#foot_2">2</ref>GPT-3 does in-context learning: We again first tested whether GPT-3 learns within a single task. Figure <ref type="figure" target="#fig_3">3A</ref> confirms that this is indeed the case. Performance (measured in terms of regret) improves over the first four trials and plateaus afterward. However, when comparing GPT-3 to two baseline algorithms -a greedy policy and an upper confidence bound (UCB) algorithm -we observed that it lags in performance prior to meta-in-context learning.</p><p>GPT-3 does meta-in-context learning: Like in our previous experiment, we found that GPT-3 improves during meta-in-context learning: as it observes more tasks, it gets better at solving new two-armed bandit problems (see Figure <ref type="figure" target="#fig_3">3B</ref>). GPT-3's performance matches that of a greedy policy after having interacted with only four tasks and lags only slightly behind that of the UCB algorithm.</p><p>We quantified the effects of in-context and meta-in-context learning in a statistical analysis. To do so, we fitted a linear regression model including the trial and task number as independent variables on the trial-wise regret. We found significant in-context learning (β = -0.221 ± 0.012, t = -35.828, p &lt; 0.001) and meta-in-context learning effects (β = -0.031 ± 0.012, t = -5.002, p &lt; 0.001), thereby reproducing our results from the previous section (see Figure <ref type="figure" target="#fig_3">3C</ref>).</p><p>Meta-in-context learning is driven by adaptation of priors: Following our earlier analysis, we speculated that part of this performance boost arises because GPT-3 adapts its priors toward the statistics of the tasks that were encountered during meta-in-context learning. To verify this, we probed its prior expectations before starting each task by asking "how rewarding do you expect machine X to be?" We set the temperature parameter = 1 and repeated this question five times to reflect a sampling from a prior distribution. Figure <ref type="figure" target="#fig_3">3D</ref> visualizes the change of priors across tasks. Before the initial task, GPT-3 expects rewards to be distributed around larger positive values (M = 545.22, SD = 6359). However, at the end of meta-in-context learning, its priors match the true data-generating distribution closely (M = 6.49, SD = 4.41).</p><p>In addition to looking at the development of priors, the two-armed bandit setting also allows us to investigate whether any changes in strategies happen during meta-in-context learning. For this, we relied on an analysis originally proposed by Gershman <ref type="bibr" target="#b26">[27]</ref>. The idea behind this analysis is to define a model that involves a parameterized combination of Boltzmann exploration, a UCB algorithm, and Thompson sampling, and then fit the parameters of this model to data generated by an agent (GPT-3 in our case). It is then possible to determine the extent to which the agent relied on a specific strategy by examining the resulting parameters.</p><p>We assume that the agent's beliefs over expected rewards at trial t and action a are captured by the normal distribution p(r a,t ) = N (µ a,t , σ a,t )<ref type="foot" target="#foot_3">foot_3</ref> and define the following probit regression model based on the parameters of these distributions: . In our analysis, we furthermore included an interaction effect with task number for each factor to investigate how the applied strategies change during meta-in-context learning.</p><formula xml:id="formula_2">p(a t = 0|w Boltzmann , w UCB , w Thompson ) = Φ w Boltzmann V t + w UCB RU t + w Thompson V t TU t<label>(1)</label></formula><formula xml:id="formula_3">V t = µ 0,t -µ 1,t RU t = σ 0,t -σ 1,t TU t = σ 2 0,t + σ 2 1,t<label>(2)</label></formula><p>Meta-in-context learning reshapes learning strategies: We found a positive main effect of the value difference V t (β = 0.307 ± 0.027, z = 21.759, p &lt; 0.001), indicating that GPT-3 engages in Boltzmann exploration. GPT-3 becomes more greedy during meta-in-context learning as shown by the positive interaction effect between task number and V t (β = 0.128 ± 0.025, z = 9.633, p &lt; 0.001). Furthermore, we found negative main effects for both relative uncertainty RU t (β = -0.160 ± 0.010, z = -31.788, p &lt; 0.001) and the uncertainty-scaled value difference V t /TU t (β = -0.400± 0.025, z = -5.393, p &lt; 0.001), suggesting that GPT-3 avoids uncertain options by default. However, we also found a slight, but significant, increase in UCB-based decisions during meta-in-context learning (β = 0.046 ± 0.010, z = 9.279, p &lt; 0.001). Figure <ref type="figure" target="#fig_3">3E</ref> shows a visualization of all regression coefficients involved in this analysis.</p><p>The results presented in this section corroborate those obtained from the supervised setting. GPT-3 generally performed better in a two-armed bandit task after meta-in-context learning. We again observed that GPT-3 accomplishes this by adapting its priors across tasks. In addition, we investigated changes in strategies during meta-in-context learning and found that GPT-3 learned to perform better by exploiting more consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regression on real-world data</head><p>In our final experiment, we wanted to investigate whether our results obtained in the artificial domains scale up to real-world applications. To test this, we considered a multi-dimensional regression benchmark which consists of 60 different real-world datasets introduced in <ref type="bibr" target="#b28">[29]</ref>.</p><p>For each simulation, we randomly selected five different tasks from the benchmark and then sampled five data-points without replacement for each task. We used five features for all tasks. If a dataset contained less than five features, we omitted it from our analysis, which yielded 42 remaining datasets.</p><p>For tasks exceeding five features, we used a sub-selection procedure that retained only the top five features based on their F-value with respect to the target variable evaluated in a univariate linear regression. To maintain a consistent regression loss across all tasks, we normalized both the feature and target spaces to the interval of [-1, 1]. The resulting prompts follow the general template outlined earlier:</p><p>Regression on real world data You observe an input vector x and have to predict the corresponding output y as accurately as possible. You are given 5 different tasks.</p><p>Task 1: </p><formula xml:id="formula_4">[. . .]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Results</head><p>GPT-3 does in-context and meta-in-context learning: Following the previous experiments, we investigated the learning curves of GPT-3 and GPT-3 after meta-in-context learning. We additionally included two baselines in our analysis: BLR and a random forest model. We measured performance by the root-mean-squared error (RMSE). Figure <ref type="figure">4A</ref> shows the learning curve for all these models. Like in the previous cases, we found that GPT-3 improves with additional data-points, confirming that it is capable of in-context learning in this setting. In addition, meta-in-context learning further improved performance. GPT-3 after meta-in-context learning matched the performance of BLR and was only slightly outperformed by the random forest model.</p><p>Meta-in-context learning constrains predictions: Most of the improvement of meta-in-context learning seems to come from zero-shot performance. We hypothesized that this is the case because GPT-3 acquires an understanding of potential target value ranges during meta-in-context learning. To test this hypothesis, we plotted the proportion of GPT-3's outputs that fell at the extreme values of the target space or outside of it in Figure <ref type="figure">4B</ref>. We found that meta-in-context learning substantially reduces the number of predictions that are outside of this range, indicating that meta-in-context learning helps to learn a better initial guess.</p><p>Meta-in-context learning works better when task similarity is high: However, learning a good initial guess is not the full picture. We speculated that meta-in-context learning is more effective if there is a higher similarity to previously encountered tasks. To verify this, we fitted a linear regression model with trial and task similarity on the RMSE. We computed task similarity as the average similarity between the current data-point and the data-points from all previously observed tasks (each individual similarity measure was obtained using a radial basis function kernel). We found a significant effect of trial (β = -0.089 ± 0.020, t = -9.204, p &lt; 0.001) as shown in Figure <ref type="figure">4C</ref>, confirming that performance within a task improves with additional observations. Furthermore, we found a significant effect of task similarity (β = -0.147 ± 0.020, t = -15.223, p &lt; 0.001), suggesting that meta-in-context learning works best if similarity to previously encountered tasks is high.</p><p>The findings outlined in this section provide further evidence for meta-in-context learning in LLMs. Notably, GPT-3 exhibited superior performance in a real-world multi-dimensional regression task following meta-in-context learning. We identified two reasons for this. First, meta-in-context learning constrained initial model predictions to the range of plausible values, and second, GPT-3 was able to leverage similarities to previously encountered tasks in order to improve its predictions.</p><p>5 Supplementary Material 5.1 Regression on real-world data using GPT-4</p><p>In this section we investigated the behavior of the newly released GPT-4 model for our last experiment. We proceeded in the same way as for GPT-3. 4 We observed that GPT-4 actually performs slightly worse both before and after meta-in-context learning as shown in Figure <ref type="figure" target="#fig_6">5A</ref>. Furthermore, we observed a slightly higher percentage of extreme predictions, particularly for GPT-4's first trial (see Figure <ref type="figure" target="#fig_6">5B</ref>). Finally, our analysis also revealed significant effects for trial (β = -0.133 ± 0.024, t = -10.858, p &lt; 0.001) and task similarity (β = -0.196 ± 0.024, t = -15.963, p &lt; 0.001) as shown in Figure <ref type="figure" target="#fig_6">5C</ref>.  4 It is worth noting that the API slightly changed and now provides the option to tailor the message with an assistant and a system. We did not use them except for the first trial where GPT-4 struggled to give a numerical output. Indeed, for some examples it instead produced messages such as "unfortunately without any information about the relationship between the variables, the prediction is not possible." Therefore, only for that one case, we added the system functionality as follows: {"role": "system", "content": "If no previous examples, sample y from your prior distribution. But do not give any non numerical answer! Even if you are unsure, try to predict y as well as possible."}.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: High-level overview of our approach on an example of multiple three-shot regression tasks. We present an LLM with N learning tasks in a row. Improvement within a task indicates that the model is capable of in-context learning. If in-context learning improves across multiple learning tasks, the model is also capable of meta-in-context learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Meta-in-context learning on the one-dimensional regression task (100 simulations). Errors bars represent 95% confidence intervals. A: MSE across trials for different models. B: GPT-3's MSE averaged over trials for each task. C: Effects of trial and task for estimating the MSE. D: GPT-3's prior expectations across tasks (blue) compared to the true task distribution (orange).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Meta-in-context learning on the two-armed bandit experiment (500 simulations). Errors bars represent 95% confidence intervals. A: Regrets across trials for different models. B: GPT-3's regrets averaged over trials for each task. C: Effects of trial and task for estimating the regret. D: GPT-3's prior expectation of rewards across games. E: Probit regression coefficients for different strategies and their interaction with task number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>with Φ denoting the cumulative distribution function of a standard normal distribution. Equation 1 recovers a Boltzmann-like exploration strategy for [w Boltzmann , w UCB , w Thompson ] = [c, 0, 0], a variant of the UCB algorithm for [w Boltzmann , w UCB , w Thompson ] = [c, d, 0], and Thompson sampling for [w Boltzmann , w UCB , w Thompson ] = [0, 0, 1] [28]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Task 5 :</head><label>5</label><figDesc>x = [-0.81, -0.16, -0.78, -0.77, -0.45], y = -0.34; x = [-0.81, -0.63, -0.75, -0.83, -0.55], y = -0.68; x = [-0.79, -0.71, -0.76, -0.8, -0.45], y = -0.75; x = [-0.2, -0.3, -0.28, -0.11, -0.18], y = 1.0; x = [-0.97, -0.92, -0.97, -0.97, -0.82], y = [insert];</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Meta-in-context learning on the regression on real-world data experiment (42 • 30 simulations). Errors bars represent 95% confidence intervals. A: RMSE across trials for different models. B: Percentage of outside or equal to the extremes of the squashed target range. C: Effects of trial and task similarities for estimating the RMSE.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Under review.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>However, it is important to note that their approach does not entirely rely on in-context learning, but involves some additional finetuning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>We left out the letters "I" and "U" as we have seen empirically that they induce biases. We also randomized the order of these letters in the questions at each trial to mitigate recency biases.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Following Gershman<ref type="bibr" target="#b26">[27]</ref>, we obtained these distributions by running Kalman filtering equations on the previously observed data.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>We would like to thank <rs type="person">Zeb Kurth-Nelson</rs> for the helpful discussions. Zeynep Akata acknowledges partial funding by the <rs type="funder">ERC</rs> (<rs type="grantNumber">853489 -DEXIM</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xFzfqjN">
					<idno type="grant-number">853489 -DEXIM</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Note that while we were running our model simulations, OpenAI released a new model -GPT-4. We repeated the analysis from this section on this new model and described our results in the Supplementary Material. In general, GPT-4 reproduced the findings of the section. However, while GPT-4 was capable of meta-in-context learning, it performed slightly worse than GPT-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>We have demonstrated that LLMs can improve their in-context learning abilities via in-context learning itself, i.e., that they are capable of meta-in-context learning. Meta-in-context learning was not only able to overwrite an LLM's priors but also changed its learning strategies, as demonstrated in two artificial domains. Finally, we applied our approach to a real-world benchmark of regression tasks where we found that meta-in-context learning leads to algorithms that are competitive with standard learning algorithms.</p><p>Perhaps the most significant shortcoming of our model simulations is that they all relied on learning tasks with just a handful of observations. This limitation is mainly due to the practical constraint of a finite context window coupled with meta-in-context learning's rapid prompt length increase. To ensure that we remain within the allowed context length (and to keep the monetary costs for our simulations at a reasonable level), we had to make this design choice. However, we believe thatdespite this restriction -our simulations were sufficient to illustrate the potential of meta-in-context learning. We hope that future LLM iterations with longer context lengths and lower inference costs will allow us to extend our simulations to larger datasets.</p><p>In addition, the tasks we probed were rather simplistic in their nature. That being said, we think we have reasonably covered the space of fundamental learning paradigms, including a supervised problem, a reinforcement learning problem, and a collection of real-world datasets. With the increasing availability of multi-modal models, it will furthermore become feasible to apply our approach to other domains. In this context, two obvious candidates are classification tasks with visual stimuli or grid-based navigation tasks.</p><p>In summary, our work has both near-and long-term consequences. In the near term, it indicates that it is not strictly necessary to engineer the perfect prompt for an LLM so that it can solve a given learning problem. Instead, LLMs are -to some degree -able to infer the required information from just a handful of related-tasks examples. In the long term, it could point to a paradigm where we adapt these models to the environment they are applied in purely through meta-in-context learning rather than finetuning them using traditional means.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Gpts are gpts: An early look at the labor market impact potential of large language models</title>
		<author>
			<persName><forename type="first">Tyna</forename><surname>Eloundou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.10130</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Chatgpt for good? on opportunities and challenges of large language models for education</title>
		<author>
			<persName><forename type="first">Enkelejda</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathrin</forename><surname>Seßler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Küchemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Bannert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daryna</forename><surname>Dementieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urs</forename><surname>Gasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">102274</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Science in the age of large language models</title>
		<author>
			<persName><forename type="first">Abeba</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Wachter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Physics</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level</title>
		<author>
			<persName><forename type="first">Iddo</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reece</forename><surname>Shuttleworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunny</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newman</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">32</biblScope>
			<biblScope unit="page">2123433119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Emergent analogical reasoning in large language models</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">J</forename><surname>Holyoak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09196</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Data distributional properties drive emergent in-context learning in transformers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaditya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking the role of demonstrations: What makes in-context learning work?</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An explanation of in-context learning as implicit bayesian inference</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Who models the models that model models? an exploration of gpt-3&apos;s in-context model fitting ability</title>
		<author>
			<persName><surname>Lovre</surname></persName>
		</author>
		<ptr target="https://www.alignmentforum.org/posts/c2RzFadrxkzyRAFXa/who-models-the-models-that-model-models-an-exploration-of" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tabllm: Few-shot classification of tabular data with large language models</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Hegselmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Buendia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hunter</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using cognitive psychology to understand gpt-3</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2218523120</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Schulz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.06729</idno>
		<title level="m">Meta-learned models of cognition</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeb</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">András</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>György</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hado</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><surname>Legg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meta-learning of sequential strategies</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Jane X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeb</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruva</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Botvinick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05763</idno>
		<title level="m">Learning to reinforcement learn</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rl 2 : Fast reinforcement learning via slow reinforcement learning</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02779</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Transformers learn in-context by gradient descent</title>
		<author>
			<persName><forename type="first">Eyvind</forename><surname>Johannes Von Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ettore</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Randazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><surname>Vladymyrov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">What can transformers learn in-context? a case study of simple function classes</title>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">What learning algorithm is in-context learning? investigations with linear models</title>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Akyürek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.15661</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Can language models handle recursively nested grammatical structures? a case study on comparing models and humans</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lampinen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2210.15303</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Language models show human-like content effects on reasoning</title>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><surname>Andrew K Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.07051</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Inducing anxiety in large language models increases exploration and bias</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Coda-Forno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Witte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Akshay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><surname>Schulz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11111</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Thilo</forename><surname>Hagendorff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13988</idno>
		<title level="m">Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai Api</surname></persName>
		</author>
		<ptr target="https://platform.openai.com" />
		<imprint>
			<biblScope unit="page" from="2023" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deconstructing the human algorithms for exploration</title>
		<author>
			<persName><surname>Samuel J Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="34" to="42" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling human exploration through resource-rational reinforcement learning</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple regression models</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichtenberg</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Özgür</forename><surname>Simsek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IDM@NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
