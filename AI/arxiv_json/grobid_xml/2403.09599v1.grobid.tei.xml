<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-03-15">March 15, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Greg</forename><surname>Coppola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">coppola.ai Research. Develop. Meme</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-03-15">March 15, 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">FEC9E5899328E30798BA7977430306EF</idno>
					<idno type="arXiv">arXiv:2403.09599v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>1 Theorem-Proving and Turing Computability</term>
					<term>2</term>
					<term>2 Question-Answering as Theorem-Proving</term>
					<term>2</term>
					<term>2</term>
					<term>1 Before Large Language Models</term>
					<term>2</term>
					<term>2</term>
					<term>2 The Effect of Large Language Models</term>
					<term>2</term>
					<term>2</term>
					<term>3 Probabilistic Theorem-Proving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex. Rather than just retrieve a document, modern information retrieval systems adverstise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning. We review recent literature and argue that the large language model has crucial flaws that prevent it from on its own ever constituting general intelligence, or answering general information synthesis requests. This review shows that the following are problems for large language models: hallucinations, complex reasoning, planning under uncertainty, and complex calculations. We outline how logical discrete graphical models can solve all of these problems, and outline a method of training a logical discrete model from unlabeled text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contents 1 Introduction 2 Background</head><p>2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3 A Logical Graphical Model 3.1 First-Order Logical Fragments . . . . . . . . . . . . . . . . . . . 3.1.1 Horn Clauses . . . . . . . . . . . . . . . . . . . . . . . . 3.1.2 The Query Fragment . . . . . . . . . . . . . . . . . . . . 3.1.3 The Planning Fragment . . . . . . . . . . . . . . . . . . . 3.1.4 The Full Undecidable Calculus . . . . . . . . . . . . . . . 3.2 Three Levels of Graphical Structure . . . . . . . . . . . . . . . . 3.2.1 Knoweldge Graph . . . . . . . . . . . . . . . . . . . . . 3.2.2 Implication Graph . . . . . . . . . . . . . . . . . . . . . 3.2.3 Proposition Graph . . . . . . . . . . . . . . . . . . . . . 3.3 Logical Boolean Algebra . . . . . . . . . . . . . . . . . . . . . . 3.4 Conjunction Nodes . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 Deterministic Definition . . . . . . . . . . . . . . . . . . 3.4.2 Conjunction and Higher-Order Feature Representations . . 3.5 Disjunction Nodes . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.1 Deterministic Logical Reasoning . . . . . . . . . . . . . . 3.5.2 Probabilistic Reasoning . . . . . . . . . . . . . . . . . . 4 Hallucinations 4.1 The Problem of Hallucination . . . . . . . . . . . . . . . . . . . . 4.2 Relation to Causality . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Compared to Existing Solutions . . . . . . . . . . . . . . . . . . 4.3.1 Discriminative Fine-Tuning . . . . . . . . . . . . . . . . 4.3.2 Retrieval-Augmented Generation . . . . . . . . . . . . . 5 Complex Reasoning 5.1 Exact Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Long "Chains" of Reasoning . . . . . . . . . . . . . . . . . . . . 5.3 Multiple Points of View . . . . . . . . . . . . . . . . . . . . . . . 6 Estimation and Inference 6.1 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex. Rather than just retrieve a document, modern information retrieval systems adverstise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning.</p><p>We review recent literature and argue that the large language model has crucial flaws that prevent it from on its own ever constituting general intelligence, or answering general information synthesis requests. This review shows that the following are problems for large language models: hallucinations, complex reasoning, planning under uncertainty, and complex calculations. We outline how logical discrete graphical models can solve all of these problems, and outline a method of training a logical discrete model from unlabeled text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Theorem-Proving and Turing Computability</head><p>First-order theorem proving is closely related to the notion of computation itself. First-order logic is a system of logic in which we can express mathematics, and thus the theory of probability, and thus science. There are several wellknown complete calculus systems for first-order logic <ref type="bibr" target="#b12">[Gödel, 1931</ref><ref type="bibr" target="#b11">, Gentzen, 1934]</ref>.</p><p>Any computer program can be represented as a proof in the typed lambda calculus of <ref type="bibr" target="#b4">[Church, 1936]</ref>. Thus, proving a theorem in first-order logic is related to the halting problem and thus is semi-decidable, i.e., not decidable <ref type="bibr" target="#b32">[Turing, 1937</ref><ref type="bibr" target="#b4">, Church, 1936]</ref>.</p><p>2.2 Question-Answering as Theorem-Proving</p><p>2.2.1 Before Large Language Models Before LLM's, a major problem was the inability to parse open-domain text. That is, in order to do symbolic theorem-proving directly, pipelines would require accurate syntactic parsing as an earlier stage in the pipeline. But, before LLM's, it was never possible to make a parser that would work on arbitrary text, often called the open domain. An important project for which the difficulty of open-domain parsing was important was the [Lenat and Guha <ref type="bibr">, 1990]</ref> project, which sought to build a knowledge base of semantic knowledge, but could never annotate enough data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">The Effect of Large Language Models</head><p>The LLM seemed to side-step this entire problem, by learning world-knowledge in an indirect way, by simply predicting n-grams, and learns a knowledge representation as a by-product of training. Now, since the LLM can effectively, but in an indirect way, do open-domain parsing, the situation has changed dramatically. It is now possible to consider completing the <ref type="bibr">[Lenat and Guha, 1990</ref>] project, which is to discover and extract the knowledge rules that underly human thinking and expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Probabilistic Theorem-Proving</head><p>The thesis of this work is that we should formalize the idea of answering a question as the proving of a theorem in a logical language extending the first-order logic calculus. <ref type="bibr" target="#b23">[Pereira and Shieber, 1987]</ref> showed how to handle deterministic theorem-proving in a system of Horn Clauses, which we review in Section 3.1.1.</p><p>In <ref type="bibr" target="#b8">[Coppola, 2024c]</ref>, we show how to not just prove these theorems but to assign probabilities to them. In <ref type="bibr">[Coppola, 2024a]</ref> and we show how the logical fragment of Horn clauses fits into increasingly larger fragments, finishing with a complete calculus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Logical Graphical Model</head><p>In this section, we outline the concept of a logical graphical model as described in <ref type="bibr" target="#b8">[Coppola, 2024c]</ref>, and analyzed further in <ref type="bibr">[Coppola, 2024a]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">First-Order Logical Fragments</head><p>We begin from the premises that the first-order logic formalization is sufficient for understanding mathematical and scientific reasoning, as is typically assumed in the philosophy of science, e.g., <ref type="bibr" target="#b22">[Pelletier, 2000]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Horn Clauses</head><p>One very important subclass of all first-order logic is the system of Horn Clauses of the form:</p><formula xml:id="formula_0">∀x 1 , . . . , ∀x k , n i=1 P i (x 1 , . . . , x k ) → C(x 1 , . . . , x k )<label>(1)</label></formula><p>A database of sentences of this form can be compiled into a smaller number of more complicated setnences of the form:</p><formula xml:id="formula_1">∀x 1 , . . . , ∀x k , n i=1 n i=1 P i (x 1 , . . . , x k ) → n i=1 C(x 1 , . . . , x k ) (2)</formula><p>That is, the premise is in disjunctive normal form, and the conclusion is a conjunction of basic terms. The system of Horn Clauses we called the Direct fragment. This is not just the fastest of the fragments, but actually a very fast fragment in absolute terms. That is, if we obey the safety restriction on quantified variables from Datalog <ref type="bibr">[Abiteboul et al., 1995, Pereira and</ref><ref type="bibr" target="#b23">Shieber, 1987]</ref>, then the complexity of proving a theorem is linear in the number of variables in the graph <ref type="bibr" target="#b0">[Abiteboul et al., 1995]</ref>.</p><p>Since the complexity of inference cannot be less than linear in the complexity of the graph itself, we say this fragment is fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">The Query Fragment</head><p>The so-called Query fragment allows us to refer to existentially bound variables in the premise that do not occur in the conclusion:</p><formula xml:id="formula_2">∀x 1 , . . . , ∀x k , ∃x k+1 , ..., x K , n i=1 P i (x 1 , . . . , x k , x k+1 , ..., x K ) → C(x 1 , . . . , x k )</formula><p>(3) This allows us to express, for example, that if x 1 and x 2 both want the same exclusively held thing x 3 , then this means x 1 and x 2 are in competition:</p><formula xml:id="formula_3">∀x 1 , x 2 , ∃x 3 want(x 1 , x 3 , ∧want(x 2 , x 3 )) → competing(x 1 , x 2 )<label>(4)</label></formula><p>This amounts to existentially querying to see if any object x 3 can be found at all. If we make the simplifying assumption that this query only should return true if want(x 1 , x 3 ) and want(x 2 , x 3 ) are in a concrete database that we can query in time only depending on the database itself, this ability to existentially quantify does not add significantly to the run time compared to the direct model. However, if we want to be able to search all proofs that can recursively prove the premise, this can lead to an exponential blow-up in complexity, as discussed in <ref type="bibr">[Coppola, 2024a]</ref>.</p><p>Ultimately, the ability to existentially quantify over variables in the premise is useful for expressing knowledge patterns we are interested in, and the speed of the fragment can be fast if we do not seek recursive proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">The Planning Fragment</head><p>The final decidable fragment considered in <ref type="bibr">[Coppola, 2024a]</ref> is the Planning fragment. This allows statements with a disjunction in the conclusion:</p><formula xml:id="formula_4">∀x 1 , . . . , ∀x k , n i=1 n i=1 P i (x 1 , . . . , x k ) → n i=1 C(x 1 , . . . , x k )<label>(5)</label></formula><p>In <ref type="bibr">[Coppola, 2024a]</ref>, we discuss in detail how this relates to planning under uncertainty. In order to be able to discharge or eliminate the or symbol ∨, we need to use the rule that <ref type="bibr" target="#b25">[Prawitz, 1965]</ref> calls ∨-Elimination. The ability to reason by cases amounts to to boolean satisfiability, or tautology, and thus is NP-hard <ref type="bibr" target="#b5">[Cook, 1971]</ref>. So, while the Direct fragment is solvable in linear, the Planning fragment is instead NP-hard, which is believed to be Ω(2 N ) in the number of variables N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">The Full Undecidable Calculus</head><p>[ <ref type="bibr" target="#b25">Prawitz, 1965]</ref> presents a calculus with exactly twelve deduction rules, corresponding to one introduction and one elimination for each of the six logical connectives ∨, ∧, ∀, ∃, → and ⊥. By bounding the run-time of the other fragments, we show that the undecidability of first-order logic must come from the deduction rules that <ref type="bibr" target="#b25">[Prawitz, 1965]</ref> calls improper, which we instead propose to call complex for the modern linguistic context. This is in other words a mathematical explanation of <ref type="bibr" target="#b15">[Kahneman, 2011]</ref>'s distinction between thinking that is fast versus slow. Also, we show why it would seem that it is not sensible to try to fit all first-order reasoning into the paradigm of a single pass of inference in a graphical model. That is, the reason is that in the more complex deduction rules, there is a change in assumptions (or other hard to manage change) that means that the assumptions must be changed. But, in a single pass of inference in a graphical probabilistic model, each variable must either take one value or the other. Thus, while there are more inference rules that can be implemented than just those in <ref type="bibr" target="#b8">[Coppola, 2024c]</ref>, we conclude that not all first-order deduction rules can be implemented in the context of one pass of inference in a graphical model. Instead, one must use book-keeping to keep track of which Direct and Query inferences can be derived for each change of assumptions. But, we propose that we can use the same book-keeping mechanism that <ref type="bibr" target="#b25">[Prawitz, 1965]</ref> did for deterministic theorem-proving.</p><p>3.2 Three Levels of Graphical Structure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Knoweldge Graph</head><p>Overview Recently, an important research direction has been the use of retrieval augmented generation <ref type="bibr" target="#b17">[Lewis et al., 2020]</ref> to the creation of knowledge graphs.</p><p>That is, if an LLM can be trained to read a document, and extract the information for the user from the document, and return a summary of that. The observation then is that if we have already retrieved and parsed a document, we can simply store the answer (assuming a baseline level of syntactic analysis ability) in a structured database. A knowledge graph can be viewed as a set of tables, making up a database.</p><p>Each table corresponds to a verb, e.g., likes. If the verb likes has the roles SUBJ and DOBJ, then there would be two columns in the table, and one entry per pair of people x 1 and x 2 , such that likes(x 1 , x 2 ).</p><p>Problem In the interpretation, a central feature is that a row is discretely either in the database or out of it. All statements about future beliefs must definitely be probabilistic, assuming the future has not "happened" yet. But, also beliefs about the present and past should be treated as probabilistic from a scientific perspective, although from the user's perspective certain facts can be considered true with essentially probability 1, e.g., that New York City is in New York State. But, if we ask a historical question, like did Julius Caesar really escape a pirate ship?, or is the 10 Commandments a historical story?, we cannot say with probability 1 whether these sentences hold. Similarly, questions like is product X in stock at store Y and will it rain tomorrow? cannot generally be given deterministic answers. This corresponds to saying that the knowledge graph must not be simply a discrete database entry, in which an entity is either a member or not, but must be a probabilistic database, in which we can ask about the probability of a semantic relationship holding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Implication Graph</head><p>Chomsky was famously fond of quoting Humboldt's aphorism that language makes infinite use of finite means <ref type="bibr" target="#b3">[Chomsky, 1965]</ref>. The implication graph allows us to estimate probabilities for an unbounded number of propositions p based on finite parameters Ψ, by defining weights over predicate patterns, rather than relationships between concrete entities. That is, we learn a general link between x jack liking x jill and x jack dating x jill , and this can apply to c jack1 or c jack2 or c jill1 or c jill2 , etc., and so make infinite use of finite means. The logical and statistical implications in the system are expressed as Horn Clause sentences of the form of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Proposition Graph</head><p>A proposition graph is an instantiation of the implication graph to answer a particular question. For an unbounded set of entities, there are an unbounded number of possible propositions p, many of which will never be relevant. For example, consider the predicate of is President of the United States. This only applies in practice to one person, but could, in principle, apply to billions. Thus, storing all possible propositions in hard disk memory would not be possible. Thus, the proposition graph is lazily constructed at answer-time. We have two options in the system for estimating the probability of a proposition p. The first is that the probability for the proposition is alredy computed before the query is issued. In the example of x person is the President of the United States, this can be set to true in long-term storage for the unique individual who occupies this slot. For anyone else, we can use generic reasoning, like there is only one President, and right now the President is someone else, etc., to answer no.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Logical Boolean Algebra</head><p>A central feature of the paradigm we are proposing, in which we follow the rules of logic <ref type="bibr" target="#b9">[Frege, 1879</ref><ref type="bibr" target="#b12">, Gödel, 1931]</ref>, especially that of the natural deduction calculus <ref type="bibr" target="#b11">[Gentzen, 1934</ref><ref type="bibr" target="#b25">, Prawitz, 1965]</ref>. In all of these logics, the two primary boolean connectives are and ∧ and or ∨, and these form a boolean algebgra <ref type="bibr" target="#b2">[Boole, 1854]</ref>.</p><p>Because the factor types Ψ and and Ψ or always alternate, we have a bipartite graph. Suppose p 1 , ..., p n are each propositions. Then we say</p><formula xml:id="formula_5">g = {p 1 ∧ ... ∧ p n } (6)</formula><p>is a proposition group, which are interpreted as conjoined. Then, the two types of variables in the graph then are:</p><p>1. p, which represents a single proposition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">g, which represents a conjoined proposition group</head><p>In <ref type="bibr" target="#b8">[Coppola, 2024c]</ref>, for many purposes in the graphical model (e.g., message passing calculuations), we can abstract over whether a node is g and p, and we refer to generic graphical nodes as z, which greatly simplifies visualization and implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Conjunction Nodes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Deterministic Definition</head><p>The conjunctive factor Ψ and is defined in terms of the and gate:</p><formula xml:id="formula_6">and(p 1 , ..., p n ) = p 1 ∧ ... ∧ p n<label>(7)</label></formula><p>Then:</p><formula xml:id="formula_7">Ψ and (g | p 1 , . . . , p n ) = 1 if g == and(p 1 , . . . , p n ), 0 otherwise (8)</formula><p>This is used in the completeness proof, but also in learned models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Conjunction and Higher-Order Feature Representations</head><p>Let us meditate on the fact that the Ψ and factor is always deterministic, i.e., we do not train this even when we are interested in statistical inference. One way to justify this is that, intuitively, and's role is to create higher-level features, between which we can learn relationships. This is like a discrete analog to the higher-level features that multi-layer networks learn <ref type="bibr" target="#b27">[Rumelhart et al., 1986</ref><ref type="bibr" target="#b16">, LeCun et al., 1989]</ref>.</p><p>For example, suppose we are given the information about like(x jack , x jill ) and like(x jill , x jack ) as simple features to predict date(x jack , x jill ). Supposing the relevant higher level feature is like(x jack , x jill ) ∧ like(x jill , x jack ), one option is run these features through a multi-layer network, which will be able to learn this feature, in a differentiable way. But, this higher-level feature emerges as an effectively emergent behavior. This leads to the problem of interpretability <ref type="bibr" target="#b14">[Hinton et al., 2015</ref><ref type="bibr" target="#b26">, Ribeiro et al., 2016</ref><ref type="bibr" target="#b19">, Lundberg and Lee, 2017]</ref>. The logical graphical model is another interpretation of interpretability, because the features must be explicitly conjoined in order to work. The problem is not one of interpreting the model, but constructing the model in the first place, since the individual function names and role labels underlying logical "language" are latent <ref type="bibr" target="#b28">[Steedman, 1996]</ref>, and presumably would be discovered through something analogous to category splitting in a generative model <ref type="bibr" target="#b24">[Petrov et al., 2006]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Disjunction Nodes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Deterministic Logical Reasoning</head><p>The deterministic disjunctive factor Ψ or used for the completeness proof (also see <ref type="bibr" target="#b7">[Coppola, 2024b]</ref>), is defined in terms of the or gate:</p><formula xml:id="formula_8">or(g 1 , . . . , g n ) = g 1 ∨ . . . ∨ g n<label>(9)</label></formula><p>The deterministic version of or, used in the completeness proof, and can be used any time we want exact logical or, is defined as:</p><formula xml:id="formula_9">Ψ or (p | g 1 , . . . , g n ) = 1 if p == or(g 1 , . . . , g n ), 0 otherwise<label>(10)</label></formula><p>When interested in statistical inference, we learn this model, as discussed in Section 3.5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Probabilistic Reasoning</head><p>Model Structure For the learned model, we model Ψ or using linear exponential model. For a boolean variable p with boolean features g 1 , ..., g n , the factor potential has the form:</p><formula xml:id="formula_10">Ψ or (p | g 1 , ..., g n ) = exp n i=1 w • ϕ(p, g i )<label>(11)</label></formula><p>4.3 Compared to Existing Solutions 4.3.1 Discriminative Fine-Tuning</p><p>The earliest used solution for controlling LLM behavior was discriminative finetuning, in which a discriminative model is used on top of the generative pre-trained model to predict a certain task <ref type="bibr" target="#b26">[Radford et al., 2019</ref><ref type="bibr" target="#b26">, Radford et al., 2019]</ref>. On its own this did not completely prevent hallucinations. Compared to the paradigm of generative pre-training and discriminative fine-tuning, the logical graphical model approach is notable for the fact that it is a generative model, but it does not hallucinate by construction, because it explains causality. Thus, the logical graphical model is a generative model that can avoid hallucinating, without even needing a discriminative stage at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Retrieval-Augmented Generation</head><p>Another important technique is retrieval-augmented generation, in which a discrete document is used to answer the question <ref type="bibr" target="#b17">[Lewis et al., 2020]</ref>. This is a useful technique, but has the draw back that the individual document used must be treated as ground truth. There is no way with basic retrieval augmented generation to store contradictory information, and synthesize it in a coherent way.</p><p>5 Complex Reasoning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Exact Reasoning</head><p>[ <ref type="bibr" target="#b27">Schick et al., 2023]</ref> showed that it is more accurate to use an LLM to realize that it needs to use a deterministic calculator, than it is to simply let the LLM try to compute each number as a probabilistic prediction. Indeed, it stands to reason that this would be not only more accurate but also cheaper. In the case of the logical graphical model, because we are converting to a discrete space, we can easily include function calling in a number of ways. Indeed, according to theoretical computer science, the computation of an exact function like a calculator can be expressed as a logical graph <ref type="bibr" target="#b4">[Church, 1936]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Long "Chains" of Reasoning</head><p>A popular strain of reasoning lately began with chain-of-thought <ref type="bibr" target="#b33">[Wei et al., 2022]</ref>, then tree of thought <ref type="bibr" target="#b33">[Yao et al., 2023]</ref>, the chain-of-abstraction <ref type="bibr" target="#b10">[Gao et al., 2024]</ref>.</p><p>In each case, the use of deterministic, hard-coded prompt modules is used to either prompt another LLM response based on the previous one, or else to take over the computation entirely. Chain-of-Abstraction, for example, says they train LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. In other words, the LLM is effectively being used as a syntactic parser, after which computation is fully symbolic. The problem with the Chain of X paradigm, is that it does not say what the relevant abstraction is. In our case, we propose that the key data structures and abstractions with which to think about computation are: first-order logic, discrete graphical models and boolean algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multiple Points of View</head><p>[ <ref type="bibr">Hinton, 2023]</ref> has suggested that large language models are unable to understand that a user may want to see the same issue analyzed from multiple points of view.</p><p>[Steedman, 2022] has proposed that these models are not exactly doing logical reasoning. Indeed, the mathematical task of a large language model is to predict a sequence of text is to predict a continuation passage. And, it is well-known that a large language model can continue in the voice of arbitrary chracters. But, this is only a surface-level appearance of understanding. When compared with the logical graphical model, which can truly reason based on different assumptions, we see that the shallow imitation of true multi-viewpoint thinking of LLM's will not allow us to learn truly deep, complex patterns about the world. That is, because the logical graphical model is discrete, and so individual assumptions can easily be set to true or false.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Estimation and Inference</head><p>In this section, we discuss how to estimation and inference with a graphical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Estimation LLM's</head><p>The key fact about the LLM is that the LLM is able to compress the dataset <ref type="bibr" target="#b30">[Sutskever, 2023]</ref>. What makes the original LLM tractable to estimate is that it just predicts the next sentence x n+1 based on the previous sentence x n , and the parameters ϕ:</p><formula xml:id="formula_11">P (x n+1 | x n , ϕ)<label>(14)</label></formula><p>This is straightforward to optimize because all variables x n are full observable.</p><p>Generating Parse Structure In order to replicate this generative with a logical model, we will need to be able to generate a form that has a logical interpretation.</p><p>In order to generate the sentence, we can opt to use the strategy of creating a labeled dependency parse that is assumed to have generated the sentence. We will represent a parse as y and write LF(y) to indicate the possibly complex sentence in the firstorder language that y is semantically analyzed as. That is, LF(y) determinisitically depends on y. This dependency parse is latent, in the sense that it is not observed, unlike the surface form of the sentence. With a logical model, we separate this into two parts as:</p><formula xml:id="formula_12">P (x n+1 | x n , ϕ, θ) ∝ Σ y∈C(x n+1 ) P (y, x n+1 | x n , ϕ) • P (LF(y) | θ)<label>(15)</label></formula><p>Here, C(x n+1 ) is the set of candidate parses for the sentence x n+1 , and θ is a set of logical parameters that is used to score the prior likelihood that <ref type="bibr" target="#b8">[Coppola, 2024c]</ref> shows how we can do inference on P (LF(y) | θ) using loopy belief propagation <ref type="bibr" target="#b21">[Pearl, 1988</ref><ref type="bibr" target="#b20">, Murphy et al., 1999]</ref>. This is not guaranteed to be true, but we find that loopy belief propagation converges well in practice.</p><p>Expectation Maximization Because the parses are latent, they cannot be observed directly like the surface form of a sequence of tokens that the LLM trains on. Thus, we must estimate the parses with some variant of expectation maximization <ref type="bibr" target="#b8">[Dempster et al., 1977]</ref>. While a "full" implementation of expectation maximization can be very complicated, there are very simple variants of the algorithm, including n-best expectation maximization, where only a finite number of options from a first-stage syntactic parser, are rescored according to the language model <ref type="bibr" target="#b2">[Charniak and Johnson, 2005]</ref>. One can even do 1-best EM, and simply take the most likely parse, but with the logical model also having input about the likelihood of the interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Inference</head><p>Assuming that we have a labeled dependency parser, when given a query, we can simply parse that query to a logical representation of a question.</p><p>arg max y∈C(x n+1 )</p><formula xml:id="formula_13">P (y | x n+1 , x n ) (16)</formula><p>Then, the logical form implied by the syntactic analysis y, called LF(y), will contain a mixture of assumptions, which we denote A y , and questions, which we denote Q y . Then, the system can assume A y , and answer the questions Q y in logical space. Suppose the answer is b. Then, we can generate a surface form of b, which we can call SF(b), generated according to a language model, conditioned on the logical form b, using the classic concepts of dialog theory <ref type="bibr">[Allen and</ref><ref type="bibr">Perrault, 1995, Litman and</ref><ref type="bibr" target="#b18">Allen, 1986</ref>].</p></div>		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Here, w is a weight vector, and ϕ(p, g i ) is a feature function discussed in <ref type="bibr" target="#b8">[Coppola, 2024c]</ref>. The probability P (p | g 1 , ..., g n ) is obtained by normalization over the two possible values for p ∈ {0, 1}:</p><p>Similarity Between Linear Exponential and Disjunction To underline the similarity between the linear exponential model and disjunction Ψ or , consider how we implement or using a log-linear model. That is, the dependence of Y on X 1 and X 2 can be expressed as:</p><p>If we set β 0 = -0.5, a negative bias, and</p><p>That is, it implements or, and this technique scales for n &gt; 2.</p><p>4 Hallucinations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Problem of Hallucination</head><p>While the large language model is widly popular for its ability to learn complex kinds of knowledge and even some reasoning from unlabeled text, the primary empirical user complaint with large language models is that of hallucinations <ref type="bibr" target="#b31">[Sutskever and Huang, 2023]</ref>. That is, a large language model can return answers that are not "supported by the training set" when judged by a human evaluator. This lack of reliability greatly limits throughput, because it requires all output of a large language model to be double-checked by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relation to Causality</head><p>Our analysis is that the concept of hallucination is closely related to the concept of causality. In other words, if we have a model that can explain why it is giving an answer, assuming that this method of explanation is sensible, can never hallucinate, because it can always give the basis for its beliefs, and, we are asuming, the explanation is sensible. Thus, a model which is both sensible and aware of causality cannot hallucinate.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><surname>Abiteboul</surname></persName>
		</author>
		<title level="m">Foundations of Databases</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Planning and acting in mixed initiative dialogue</title>
		<author>
			<persName><forename type="first">Allen</forename><surname>Perrault ; Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Perrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 34th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Investigation of the Laws of Thought, on Which Are Founded the Mathematical Theories of Logic and Probabilities</title>
		<author>
			<persName><forename type="first">G</forename><surname>Boole ; Boole</surname></persName>
		</author>
		<author>
			<persName><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johnson ; Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Knight</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Oflazer</surname></persName>
		</editor>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1854">1854. 1854. 2005. 2005</date>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Coarse-tofine n-best parsing and MaxEnt discriminative reranking</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Aspects of the Theory of Syntax</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chomsky ; Chomsky</surname></persName>
		</author>
		<ptr target="https://mitpress.mit.edu" />
		<imprint>
			<date type="published" when="1965">1965. 1965</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An unsolvable problem of elementary number theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Church ; Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="345" to="363" />
			<date type="published" when="1936">1936. 1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The complexity of theorem-proving procedures</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Cook ; Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third annual ACM symposium on Theory of computing</title>
		<meeting>the third annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1971">1971. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A categorization of complexity classes for information retrieval and synthesis using natural logic</title>
		<author>
			<persName><forename type="first">G</forename><surname>Coppola ; Coppola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The quantified boolean bayesian network: Theory and experiments with a logical graphical model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Coppola ; Coppola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024. 2024b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Coppola ; Coppola</surname></persName>
		</author>
		<author>
			<persName><surname>Dempster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Quantified Boolean Bayesian Network: A Logical Graphical Model. Bitcoin Ordinal NFT 5749e716a487c17eb9c5e27245dc23abb2432310765a46331c38e230cf8fe695i0</title>
		<imprint>
			<date type="published" when="1977">2024. 2024c. 1977. 1977</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Begriffsschrift, a Formula Language, Modeled Upon That of Arithmetic, for Pure Thought</title>
		<author>
			<persName><forename type="first">G</forename><surname>Frege ; Frege</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1879">1879. 1879</date>
			<publisher>Günther Holzboog</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note>Efficient tool use with chain-of-abstraction reasoning</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Untersuchungen über das logische schließen</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gentzen ; Gentzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematische Zeitschrift</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="405" to="431" />
			<date type="published" when="1934">1934. 1934</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Über formal unentscheidbare sätze der principia mathematica und verwandter systeme i. Monatshefte für Mathematik</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gödel ; Gödel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">English title: On Formally Undecidable Propositions of Principia Mathematica and Related Systems I</title>
		<imprint>
			<date type="published" when="1931">1931. 1931</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="173" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Godfather of Artificial Intelligence&quot; talks impact and potential of AI</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton ; Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CBS Mornings</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2035" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Timestamp: 1318 seconds</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman ; Kahneman</surname></persName>
		</author>
		<title level="m">Thinking, Fast and Slow. Farrar, Straus and Giroux</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building large knowledgebased systems: Representation and inference in the cyc project</title>
		<author>
			<persName><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS. [Lenat and Guha</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Lenat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Guha</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1989">1989. 1989. 1990. 1990</date>
		</imprint>
	</monogr>
	<note>Handwritten digit recognition: Applications of neural networks</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
		<author>
			<persName><surname>Lewis</surname></persName>
		</author>
		<idno>CoRR, abs/2005.11401</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A plan recognition model for subdialogues in conversations</title>
		<author>
			<persName><forename type="first">Allen</forename><forename type="middle">;</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="200" />
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">Lee</forename><forename type="middle">;</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)</meeting>
		<imprint>
			<publisher>AUAI</publisher>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="467" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl ; Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988. 1988</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A history of natural deduction and elementary logic textbooks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Pelletier ; Pelletier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Logical consequence: Rival approaches</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="138" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Prolog and Natural-Language Analysis</title>
		<author>
			<persName><forename type="first">Shieber ;</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Center for the Study of Language and Information</title>
		<title level="s">CSLI Lecture Notes</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Natural Deduction: A Proof-Theoretical Study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Prawitz ; Prawitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stockholm Studies in Philosophy</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1965">1965. 1965</date>
			<publisher>Göteborg; Uppsala. Acta Universitatis Stockholmiensis</publisher>
			<pubPlace>Stockholm</pubPlace>
		</imprint>
	</monogr>
	<note>Almqvist &amp; Wiksell</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2019. 2019. 2016. 2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>why should i trust you?&quot; explaining the predictions of any classifier</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><surname>Rumelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986. 1986. 2023. 2023</date>
		</imprint>
	</monogr>
	<note>Toolformer: Language models can teach themselves to use tools</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Steedman ; Steedman</surname></persName>
		</author>
		<title level="m">Surface Structure and Interpretation</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Steedman ; Steedman</surname></persName>
		</author>
		<title level="m">2022 NLP Symposium. AI Quorum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="2023" to="2035" />
		</imprint>
	</monogr>
	<note>Timestamp: 7 seconds</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An observation on generalization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever ; Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<publisher>Simons Institute</publisher>
			<biblScope unit="page" from="2024" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">AI Today and Vision of the Future</title>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Huang ; Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">2023. 2023. 1966 seconds</date>
			<biblScope unit="volume">999</biblScope>
			<biblScope unit="page" from="2023" to="2035" />
			<pubPlace>Timestamp</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On computable numbers, with an application to the entscheidungsproblem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Turing ; Turing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the London Mathematical Society</title>
		<meeting>the London Mathematical Society</meeting>
		<imprint>
			<date type="published" when="1937">1937. 1937</date>
			<biblScope unit="page" from="230" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022. 2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Tree of thoughts: Deliberate problem solving with large language models</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
