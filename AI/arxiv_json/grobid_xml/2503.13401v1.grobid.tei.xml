<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEVELS OF ANALYSIS FOR LARGE LANGUAGE MODELS 1 Using the Tools of Cognitive Science to Understand Large Language Models at Different Levels of Analysis</title>
				<funder>
					<orgName type="full">NOMIS Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-03-17">17 Mar 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<address>
									<settlement>Princeton</settlement>
									<country>University</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Princeton Neuroscience Institute</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Declan</forename><surname>Campbell</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Princeton Neuroscience Institute</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuechunzi</forename><surname>Bai</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">The University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Geng</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Mccoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<address>
									<settlement>Princeton</settlement>
									<country>University</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Nam</surname></persName>
							<affiliation key="aff5">
								<orgName type="laboratory">Princeton Laboratory for Artificial Intelligence</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilia</forename><surname>Sucholutsky</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>328</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Veniamin</forename><surname>Veselovsky</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian-Qiao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<address>
									<settlement>Princeton</settlement>
									<country>University</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Princeton Laboratory for Artificial Intelligence</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LEVELS OF ANALYSIS FOR LARGE LANGUAGE MODELS 1 Using the Tools of Cognitive Science to Understand Large Language Models at Different Levels of Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-03-17">17 Mar 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">9B80C9D170DFD0E6A62D4F7C71B6F2FF</idno>
					<idno type="arXiv">arXiv:2503.13401v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>large language models</term>
					<term>levels of analysis Jryy</term>
					<term>vg jnf abg rknpgyl cynaarq sebz gur ortvaavat Iqxx</term>
					<term>uf ime zaf qjmofxk bxmzzqp rday ftq nqsuzzuzs Well</term>
					<term>it was not exactly planned from the beginning 490.2</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern artificial intelligence systems, such as large language models, are increasingly powerful but also increasingly hard to understand. Recognizing this problem as analogous to the historical difficulties in understanding the human mind, we argue that methods developed in cognitive science can be useful for understanding large language models. We propose a framework for applying these methods based on Marr's three levels of analysis.</p><p>By revisiting established cognitive science techniques relevant to each level and illustrating their potential to yield insights into the behavior and internal organization of large language models, we aim to provide a toolkit for making sense of these new kinds of minds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The last decade has seen a series of breakthroughs in artificial intelligence (AI) research, culminating in the creation of the large language models that underlie chat-based agents such as ChatGPT, Claude, Gemini, and LLaMa <ref type="bibr" target="#b0">(Achiam et al., 2023;</ref><ref type="bibr" target="#b104">Team et al., 2023;</ref><ref type="bibr" target="#b109">Touvron et al., 2023)</ref>. These breakthroughs have been driven by a specific strategy: starting with generic artificial neural network architectures and increasing their size and training data. Artificial neural networks are notoriously difficult to interpret, finding solutions to problems that are expressed in the form of billions of continuous weighted connections between units. As a consequence, computer scientists now face an unfamiliar problem: they have created systems that they do not understand. To make things even worse, since the training data and weights of many of the leading systems are not available outside the companies that created them, in many cases the only insights we can obtain about the nature of these systems are those that can be gleaned by studying their behavior.</p><p>Even though this problem is unfamilar to computer scientists, it is very familiar to another group of researchers: cognitive scientists. Cognitive science is the interdisciplinary science of the mind, and for the 70 or so years since its inception <ref type="bibr" target="#b69">(Miller, 2003)</ref> has been limited by the fact that it had relatively few kinds of mind to study. To cognitive scientists, the advent of intelligent machines offers exciting new opportunities to apply methods that have been refined through trying to understand how human minds work <ref type="bibr" target="#b8">(Binz and Schulz, 2023;</ref><ref type="bibr" target="#b18">Coda-Forno et al., 2024)</ref>. Those methods encompass both techniques that are based on human behavior and insights that come from related fields such as neuroscience that explore the underlying mechanisms.</p><p>In this paper we summarize some of the tools from cognitive science that we believe are particularly useful for understanding large language models (and related approaches such as vision-language models). We revisit some of the "greatest hits" of cognitive science LEVELS OF ANALYSIS FOR LARGE LANGUAGE MODELS 4 -techniques such as rational analysis, the axiomatic approach, and multidimensional scaling -and illustrate how they translate into techniques for gaining insight into AI systems. We illustrate the potential of this approach by using examples from our own work and by drawing analogies between nascent methods in computer science and tools developed in other fields.</p><p>As an organizing principle, we divide these tools up by the different kinds of questions to which they provide answers. The computational neuroscientist David Marr <ref type="bibr" target="#b65">(Marr, 1982)</ref> suggested that information processing systems can be understood at three levels of analysis: the computational level, which focuses on the abstract computational problem a system solves and its ideal solution; the algorithmic level, which focuses on the representations and algorithms that approximate that solution; and the implementation level, which focuses on how those representations and algorithms are realized in a physical system. The same three levels can be used for analyzing large language models, focusing on how such systems are shaped by their function, the solutions that they seem to find, and the realization of those solutions in weights and units within the underlying artificial neural network (see Table <ref type="table" target="#tab_0">1</ref>). Drawing these parallels also provides a guide for where we might expect to look for relevant tools, with the computational level focusing on computational modeling techniques, the algorithmic level drawing on methods from cognitive psychology, and the implementation level being inspired by neuroscience. As a result, we see thinking at these different levels of analysis as being particularly productive for making sense of large language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational level: Considering function</head><p>The computational level focuses on the abstract problem that a system needs to solve. By recognizing the pressures that this problem exerts upon the system, we can make predictions about the properties that the system is likely to have <ref type="bibr" target="#b100">(Shepard, 1987;</ref><ref type="bibr" target="#b65">Marr, 1982;</ref><ref type="bibr" target="#b1">Anderson, 1990;</ref><ref type="bibr" target="#b35">Griffiths, 2020)</ref>. This perspective is perhaps most familiar from evolutionary biology, in which organisms are understood through the lens of the evolutionary pressures that have shaped them. For instance, our understanding of bird flight is informed by the aerodynamic principles that bird flight must obey. Similarly, we can gain insight into intelligent systems by considering how they have been shaped by the functions that they must perform.</p><p>This emphasis on function makes the computational level well-suited for analyzing AI systems. Although many aspects of modern AI systems are difficult to interpret (including their behavior and the mechanisms that they use to achieve that behavior), one aspect that we understand well is the function that the system is optimized to perform.</p><p>Specifically, this function is explicitly defined by humans in the form of the AI system's training objective. Therefore, computational-level analysis enables us to start from something we understand well -the training objective -and use it to reason about the less-well-understood territory of the behavior of the resulting systems. <ref type="bibr" target="#b66">et al. (McCoy et al., 2024)</ref> used an approach based on computational-level analysis to try to understand the behavior of large language models (LLMs). For these Figure <ref type="figure">1</ref>.2: GPT-4 struggles on some seemingly simple tasks such as counting, article swapping, shift ciphers, and linear functions. Later in the paper, we explain the contrasts that are illustrated here. In the counting and article swapping examples, GPT-4 fails in the cases where the correct output is a low-probability piece of text (for the counting example, we refer to 29 as low-probability because it occurs much less frequently in natural corpora than 30 does). In the shift cipher and linear function examples, GPT-4 performs well on the task variants that are common in Internet text but poorly on the variants that are rare (note that the shift cipher with a shift of 13 is over 100 times more common in Internet text than the shift cipher with a shift of 12; and the linear function f (x) = (9/5)x + 32 is common because it is the Celsius-to-Fahrenheit conversion, while the other linear function has no special significance). For the sake of brevity, this figure does not show the full prompts we used; see later in the paper for the complete prompts. The GPT-4 predictions were obtained using gpt-4-0613 on the OpenAI API; other model versions (e.g., the online chat interface) may give different predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The embers of autoregression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>McCoy</head><formula xml:id="formula_0">Reversal Word swap Counting Acronym Shift cipher Pig Latin -1 2 5 -1 0 0 -7 5 -1 6 0 -1 4 0 -1 2 0 -1 0 0 -8 0 -6 . 0 -5 . 5 -5 . 0 -4 . 5 -1 8 -1 6 -1 4 -1 2 -1 2 5 -1 0 0 -7 5 -1 2 0 -1 0 0 -<label>8</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A teleological approach to understanding LLMs</head><p>To understand an information-processing system such as an LLM, the approach that we are arguing for is to first characterize the problem that the system solves and to then use this characterization as a source of hypotheses about the system's capacities and biases. We refer to this as the teleological approach because it focuses on explaining the behavior of the system in terms of its goal (telos in Greek) (e.g., <ref type="bibr">Lombrozo and Carey, 2006</ref>). Teleological explanation is a common strategy in making sense of biological systems, manifest in approaches such as computational-level <ref type="bibr" target="#b65">(Marr, 1982)</ref> and rational <ref type="bibr" target="#b1">(Anderson, 1990)</ref> analysis in cognitive science, and adaptationist explanations in evolutionary biology <ref type="bibr">(Godfrey-Smith, 2001;</ref><ref type="bibr">Shettleworth, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1</head><p>Large language models perform better when they need to produce a high-probability piece of text than when they need to produce a low-probability piece of text, even in deterministic settings where probability should not matter.</p><p>systems, the primary training objective is next-token prediction, also known as autoregression: predicting the next token (word or part of a word) in a piece of text given the preceding tokens. Analysis of this task leads to the prediction that LLMs will perform better when they need to produce a high-probability piece of text than when they need to produce a low-probability piece of text, even in deterministic settings where probability should not matter. Intuitively, this prediction follows from the way in which next-token prediction fundamentally depends on the probabilities of token sequences; this intuition is derived more formally in <ref type="bibr" target="#b66">(McCoy et al., 2024)</ref> via a Bayesian analysis of autoregression.</p><p>The prediction that LLM behavior will be sensitive to probability is borne out in experiments testing a range of LLMs across a range of tasks (see Figure <ref type="figure">1</ref>). For instance, when GPT-4 is asked to count how many letters there are in a list, it performs much better when the answer is a frequently-used number than a more rarely-used number; e.g., when the answer is 30, its accuracy is 97%, but when the answer is 29, its accuracy is only 17%, presumably because the number 30 is used more often in natural text than the number 29.</p><p>Similarly, when asked to decode a message written in a simple cipher, GPT-4's accuracy was 51% when the answer was a high-probability sentence but only 13% when the answer was a low-probability word sequence. Thus, even though LLMs can be applied to many different tasks -a capability that has been viewed as evidence that LLMs show "sparks of artificial general intelligence" <ref type="bibr" target="#b13">(Bubeck et al., 2023)</ref> -they also continue to display "embers of autoregression" -behavioral trends that reflect the nature of the specific objective they were optimized for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian optimality as a benchmark</head><p>The "embers of autoregression" example illustrates a broader principle: many cognitive tasks can be characterized as inductive inference problems under uncertainty <ref type="bibr" target="#b36">(Griffiths et al., 2024a)</ref>. Bayesian models of cognition provide optimal solutions to these problems, and have become instrumental in explaining human performance across diverse domains, including perception <ref type="bibr" target="#b128">(Yuille and Kersten, 2006)</ref>, language processing <ref type="bibr" target="#b15">(Chater and Manning, 2006;</ref><ref type="bibr" target="#b37">Griffiths et al., 2007)</ref>, categorization <ref type="bibr" target="#b92">(Sanborn et al., 2010)</ref>, and intuitive physics <ref type="bibr" target="#b93">(Sanborn et al., 2013;</ref><ref type="bibr" target="#b5">Battaglia et al., 2013)</ref>. We can compare LLMs to these optimal solutions to gain insight into their behavior, much as cognitive scientists use Bayesian models to understand human cognition.</p><p>The connection between Bayesian optimality and LLMs can be made more explicit by considering the problem of next-token prediction that is typically used in training these models. Predicting the next token can be done by extracting the predictive sufficient statistics from previous tokens <ref type="bibr" target="#b7">(Bernardo and Smith, 1994)</ref>. For some datasets, the Bayesian posterior distribution over particular parameters or hypotheses about the generating process can serve as a predictive sufficient statistic <ref type="bibr" target="#b129">(Zhang et al., 2024)</ref>. This perspective can be used to understand the representations that LLMs form and how they should relate to ideal Bayesian solutions <ref type="bibr" target="#b130">(Zhang et al., 2023</ref><ref type="bibr" target="#b129">(Zhang et al., , 2024))</ref>. Several other recent papers have also identified interesting connections between LLMs and Bayesian inference <ref type="bibr" target="#b125">(Xie et al., 2021;</ref><ref type="bibr">Wang et al., 2024a;</ref><ref type="bibr" target="#b131">Zheng et al., 2023)</ref>.</p><p>These connections suggest that we might be able to create simple Bayesian models of the inferences drawn by LLMs. Such Bayesian models can be used to explore the implicit prior distributions adopted by LLMs and to compare the resulting distributions with those inferred from human behavior. For example, Griffiths et al. <ref type="bibr" target="#b39">(Griffiths et al., 2024b)</ref> used a simple Bayesian model of predicting the future <ref type="bibr" target="#b38">(Griffiths and Tenenbaum, 2006)</ref> to recover implicit prior distributions about the extent or duration of phenomena from GPT-4. Zhu and Griffiths <ref type="bibr" target="#b134">(Zhu and Griffiths, 2024b)</ref> built on this work, using an iterated learning procedure originally developed for sampling from human priors to explore the priors that LLMs have for causal relationships and probability distributions, as well as their implicit assumptions about speculative events such as the development of superhuman AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Violations of axiomatic systems</head><p>Another approach to understanding intelligent systems at the computational level is to consider how their behavior relates to an axiomatic system that captures the solution to a problem. One of the original examples used by Marr had this flavor: he suggested that we can understand the computational problem solved by a cash register by recognizing that the expectations we have about shopping, such as the fact that the order in which items are checked out doesn't affect the total price, correspond to the axiomatic system of arithmetic. In cognitive science, the most celebrated application of this approach has been decision theory. By considering how to define rationality, decision theorists were able to specify a set of axioms that result in the discovery that rational agents should seek to maximize expected utility <ref type="bibr" target="#b118">(Von Neumann and Morgenstern, 1947;</ref><ref type="bibr" target="#b94">Savage, 1954)</ref>. Asking whether this axiomatic system actually describes human behavior resulted in fundamental insights into human decision-making, with Kahneman and Tversky carrying out an Incoherent probability judgments from humans (a, b) and <ref type="bibr">d)</ref>. Like human probability judgments (a), GPT-4's judgments systematically deviate from zero when combined into probabilistic identities (c). When repeatedly queried about the same event, the mean-variance relationship of probability judgments follows an inverted-U shape for both humans (b) and . Human data are adapted from <ref type="bibr" target="#b135">(Zhu et al., 2020)</ref>,  results are from <ref type="bibr" target="#b133">(Zhu and Griffiths, 2024a)</ref>.</p><p>influential research program that showed that people systemically violate the prescriptions of these axioms <ref type="bibr" target="#b116">(Tversky and Kahneman, 1974;</ref><ref type="bibr" target="#b49">Kahneman and Tversky, 1979)</ref>.</p><p>Considering relevant axiomatic systems -and discovering how they are violatedprovides another tool for understanding LLMs. For example, probability theory dictates that the probabilities of an event A and its complement ¬A sum to 1, meaning P (A) + P (¬A) = 1. To assess whether LLMs adhere to this rule, we can examine deviations from zero in P (A) + P (¬A) -1. Similarly, other probabilistic identities can be tested, such as P (A) + P (B) -P (A ∧ B) -P (A ∨ B), which should also equal zero if judgments are coherent <ref type="bibr" target="#b135">(Zhu et al., 2020)</ref>. Eliciting probability judgments for logically related events from GPT and LLaMa models (see Figure <ref type="figure">2</ref>), shows that probability identities formed using judgments generated by LLMs systematically deviate from zero, LEVELS OF ANALYSIS FOR LARGE LANGUAGE MODELS 10 violating the rules of probability theory <ref type="bibr" target="#b133">(Zhu and Griffiths, 2024a)</ref>. In addition to maintaining coherence across logically related events, a rational agent should produce consistent probability judgments when repeatedly queried about the same event. However, repeated probability judgments elicited from LLMs exhibit an inverted-U-shaped mean-variance relationship <ref type="bibr" target="#b133">(Zhu and Griffiths, 2024a)</ref>. These systematic deviations also qualitatively mirror those observed in human probability judgments, suggesting that LLMs exhibit similar biases in probabilistic reasoning.</p><p>Optimal behavior can also be defined with respect to problems or situations with inherent uncertainty. For example, in a risky choice task where participants select one of many gambles (each gamble corresponding to a probabilistic distribution of outcomes), there is always a rational choice that maximizes the expected value (making the simplest possible assumption about the utility associated with an option by equating its utility with its monetary value). Here, using chain-of-thought reasoning <ref type="bibr" target="#b122">(Wei et al., 2022</ref>) make choices almost completely rationally, but without such reasoning, their choices are noisy and sometimes ignore probabilities completely <ref type="bibr">(Liu et al., 2024a)</ref>. Furthermore, when LLMs are asked to predict human performance on the task, they predict humans to behave highly rationally, even though people behave much less so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>The computational level of analysis provides a powerful lens for understanding large language models by focusing on their function. Examining the training objective (in this case, autoregression) can directly predict specific behavioral patterns, as illustrated by the "embers of autoregression." Furthermore, comparing LLMs to optimal benchmarks, whether derived from Bayesian models of cognition or the axioms of probability and decision theory, can reveal both the surprising capabilities and the systematic limitations of these systems. By considering what computational problem LLMs are solving (or approximating), we can gain significant insight into their behavior and internal structure, even when the algorithmic and implementation details remain opaque.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithmic level: Identifying representations and processes</head><p>Just as the computational level asks what problem an information-processing system solves, the algorithmic level explores how that solution is approximated. This level concerns itself with the specific representations and algorithms used to carry out the This problem is much the same as that faced by psychologists before the advent of modern neuroscientific tools and methodologies.</p><p>Cognitive psychology offers a rich toolbox of methods for exploring the algorithmic level, many of which can be creatively adapted to study LLMs. This section explores how cognitive science-inspired approaches -such as analyzing systematic error patterns, soliciting similarity judgments, and exploring associations -can be used to uncover the algorithms and representations of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel and serial processing</head><p>A fundamental challenge for any cognitive system, whether biological or artificial, is the tradeoff between processing information serially (one item at a time) or in parallel (multiple items simultaneously) <ref type="bibr" target="#b112">(Treisman and Gelade, 1980;</ref><ref type="bibr" target="#b110">Townsend, 1990)</ref>. Parallel processing offers efficiency, allowing for rapid processing of multiple inputs. However, it also introduces the potential for representational interference, especially when dealing with compositional representations, where features are shared and recombined across different items. Think of trying to remember a set of colored shapes: if the colors and shapes are reused across multiple objects, it becomes harder to keep track of which color goes with which shape when processing them all at once. Serial processing, while slower, mitigates this interference by focusing attention on a single item at a time.</p><p>Critically, the interference caused by parallel processing of compositional representations follows a predictable pattern: items that share more features will interfere with each other more than dissimilar items <ref type="bibr" target="#b74">(Musslick and Cohen, 2021;</ref><ref type="bibr" target="#b11">Bouchacourt and Buschman, 2019)</ref>. This relationship between feature similarity and performance degradation provides a diagnostic tool. By observing systematic errors -such as decreased accuracy or the formation of "illusory conjunctions" (e.g., misremembering a red square and a blue circle as a red circle) -we can infer that the system likely relies on compositional representations and parallel processing. This approach allows us to indirectly examine the structure of a system's representations by analyzing its behavioral limitations.</p><p>Recent work with vision-language models (VLMs) provides compelling evidence for this approach. VLMs are typically built on top of an LLM backbone, adding a system for encoding visual images and additional training on tasks that involve both language and images <ref type="bibr" target="#b10">(Bordes et al., 2024)</ref>. The resulting models, like humans, show highly accurate "pop-out" search for distinctive visual targets but exhibit degraded performance in conjunction search (searching for a target defined by a combination of features) as the number of distractors increases <ref type="bibr" target="#b14">(Campbell et al., 2024)</ref>. This pattern suggests interference VLMs exhibit a "subitizing limit" in numerical estimation.</p><p>arising from the simultaneous processing of multiple items. Similarly, VLMs exhibit a "subitizing limit" in numerical estimation (see Figure <ref type="figure">3</ref>), akin to that observed in humans under conditions that force rapid, parallel visual processing <ref type="bibr" target="#b51">(Kaufman et al., 1949;</ref><ref type="bibr" target="#b113">Trick and Pylyshyn, 1994)</ref>. Furthermore, when tasked with describing the features of multiple objects in a scene, VLMs make systematic errors resembling illusory conjunctions observed in human visual working memory tasks <ref type="bibr" target="#b111">(Treisman and Schmidt, 1982)</ref>, with error rates predicted by the potential for feature interference <ref type="bibr" target="#b14">(Campbell et al., 2024)</ref>. These parallels suggest that both humans and VLMs rely on compositional representations and are susceptible to similar forms of interference during parallel processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity judgments</head><p>Building on the principle that behavioral limitations can reveal representational structure, we now turn to specific methods for probing these representations in LLMs. One powerful approach, adapted from cognitive psychology, is the use of similarity judgments.</p><p>Humans rely on efficient representations to navigate high-dimensional environments and to support different cognitive capacities <ref type="bibr" target="#b1">(Anderson, 1990)</ref>. Characterizing the structure of those representations has been central to decades of psychological research spanning a wide array of contexts, including sensory domains such as color <ref type="bibr" target="#b98">(Shepard, 1980;</ref><ref type="bibr" target="#b23">Ekman, 1954)</ref>, pitch <ref type="bibr" target="#b99">(Shepard, 1982)</ref>, and natural images <ref type="bibr" target="#b44">(Hebart et al., 2020;</ref><ref type="bibr">Marjieh et al., 2024a)</ref>, linguistic domains such as the semantic organization of concepts <ref type="bibr" target="#b91">(Rosch, 1975;</ref><ref type="bibr" target="#b115">Tversky and Hutchinson, 1986</ref>) and lexical analogies <ref type="bibr" target="#b81">(Peterson et al., 2020)</ref>, and numerical domains such as the relations between integers and their mathematical properties <ref type="bibr" target="#b70">(Miller and Gelman, 1983;</ref><ref type="bibr" target="#b83">Pitt et al., 2022;</ref><ref type="bibr" target="#b82">Piantadosi, 2016;</ref><ref type="bibr" target="#b106">Tenenbaum, 1999)</ref>.</p><p>Similarity judgments can be used to reveal representations that explain human behavior, as illustrated by the work of <ref type="bibr" target="#b98">(Shepard, 1980</ref><ref type="bibr" target="#b100">(Shepard, , 1987) )</ref> and <ref type="bibr" target="#b114">(Tversky, 1977)</ref>. The idea here is that by observing how humans perceive "similarity" between stimuli that are sampled from a certain domain (a notion that is ambiguous by design) we can characterize how they represent and organize that domain. More specifically, given a domain of interest (e.g., colors) the paradigm proceeds by eliciting similarity judgments between pairs of stimuli from that domain ("how similar are the two colors?") and aggregating those judgments into similarity matrices that capture the relations between stimuli (e.g., the color wheel). By applying spatial embedding techniques such as multi-dimensional scaling (MDS) analysis <ref type="bibr" target="#b97">(Shepard, 1962</ref><ref type="bibr" target="#b98">(Shepard, , 1980) )</ref> to such matrices or computing different diagnostic measures from them <ref type="bibr" target="#b115">(Tversky and Hutchinson, 1986)</ref> it is then possible to derive strong constraints on the underlying representation.</p><p>Methods for identifying representations based on similarity judgments can be used just as easily with large language models. Just as we can elicit similarity judgments from a human participant regarding color, we can prompt an LLM to rate the similarity between two color concepts, or even image patches in the case of VLMs. This idea was recently applied to six perceptual domains, showing that LLMs encode surprisingly rich sensory knowledge, including well-known representations such as the color wheel and the pitch helix <ref type="bibr" target="#b62">(Marjieh et al., 2024b)</ref>, despite being largely trained on text (see Figure <ref type="figure" target="#fig_4">4</ref>). A growing line of work leverages this insight to characterize LLM representations across different domains such as olfaction <ref type="bibr" target="#b132">(Zhong et al., 2024)</ref> and numbers <ref type="bibr" target="#b63">(Marjieh et al., 2025)</ref>, as well as to study conceptual diversity in LLM representations <ref type="bibr" target="#b73">(Murthy et al., 2024)</ref> and LLM-human representational alignment <ref type="bibr" target="#b72">(Mukherjee et al., 2024;</ref><ref type="bibr" target="#b103">Suresh et al., 2023;</ref><ref type="bibr" target="#b78">Ogg et al., 2024)</ref>;</p><p>for a recent review on measuring human-AI alignment see <ref type="bibr" target="#b102">(Sucholutsky et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncovering hidden associations</head><p>Another approach for uncovering the representations of LLMs that is particularly useful for closed models involves adapting methods that tap into implicit associations. The challenge of obtaining true internal representations from LLMs becomes more apparent with closed models that have undergone value alignment post-training. These models do not allow direct access to word embeddings or model weights <ref type="bibr" target="#b9">(Bommasani et al., 2023)</ref>.</p><p>Methods such as reinforcement learning from human feedback produce responses that follow safety protocols but may not accurately reflect the models' internal representations <ref type="bibr" target="#b4">(Bai et al., 2022)</ref>. This issue mirrors the challenges cognitive scientists face when studying human memory <ref type="bibr" target="#b2">(Anderson and Milson, 1989)</ref>, particularly in accessing concept associations within a closed system like the human brain, which are difficult to measure through self-report questionnaires due to demand characteristics (ie. participants responding in a way that they think the experimenter wants them to respond, or in a way that is socially acceptable).</p><p>To address this, cognitive scientists have used other behavioral measures, such as reaction time, to approximate the mental distance between pairs of concepts <ref type="bibr" target="#b20">(Collins and Quillian, 1969)</ref>. These reaction times have been explained by hypothesizing that the human mind organizes concepts as nodes within an associative network, where weighted links reflect the proximity between these nodes. Such associative representations influence behavior; the greater the distance between two concepts, the longer it takes for people to retrieve them, resulting in increased reaction times <ref type="bibr" target="#b84">(Posner and Mitchell, 1967)</ref>. An intuitive example comes from a classic study demonstrating that human participants react</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5</head><p>Large language models such as GPT-4 have been trained to identify situations that involve expressing explicit biases. However, it is possible to construct simple prompts that reveal that they still have strong implicit biases, as reflected in their associations between words.</p><p>These implicit biases have consequences for their downstream decisions as well.</p><p>faster to the statement "a canary can sing" than to "a canary can fly." This is because the latter requires traversing two degrees of association: "a canary is a bird" and "a bird can fly" <ref type="bibr" target="#b19">(Collins and Loftus, 1975)</ref>. This kind of measure is also prevalent in examining attitudes toward social groups <ref type="bibr" target="#b26">(Fazio and Olson, 2003;</ref><ref type="bibr" target="#b33">Greenwald and Banaji, 1995)</ref>. For instance, the Implicit Association Test (IAT) aligns pairs of social group labels, like "Black" or "White", with adjectives like "wonderful" or "terrible" <ref type="bibr" target="#b34">(Greenwald et al., 1998)</ref>.</p><p>Empirical studies have repeatedly shown that human participants react faster to minority labels paired with negative adjectives, revealing underying mental associations about social groups that also predict other aspects of behavior such as the frequency of interacting with members of these groups (see meta-analysis by <ref type="bibr" target="#b56">Kurdi et al., 2019)</ref>.</p><p>The key insight behind these approaches is that it is possible to elicit mental associations without directly asking the participant for a verbal report. In some cases, researchers aim to capture unobtrusive or unconscious responses <ref type="bibr" target="#b32">(Graf and Schacter, 1985;</ref><ref type="bibr" target="#b95">Schacter, 1987)</ref>; in others, they strive to minimize self-presentation biases, such as fear of appearing unfair <ref type="bibr" target="#b26">(Fazio and Olson, 2003;</ref><ref type="bibr" target="#b28">Gaertner and McLaughlin, 1983)</ref>. The success of these methods in achieving these goals suggests that they may also be useful in analyzing the behavior of value-aligned LLMs. The hypothesis is that since alignment trains LLMs to conceal their true representations, methods that bypass direct rating scales or evaluative judgments may better expose their underlying associations. To test this, we adapted the Implicit Association Test for LLMs by prompting various models to associate word pairs used in earlier human studies <ref type="bibr" target="#b3">(Bai et al., 2024)</ref>. For instance, we asked the model to choose between "Julia" and "Ben" after presenting words like home, office, parent, management, salary, and wedding. As anticipated, models like GPT-4 often linked Julia with home, parent, and wedding, implying an internal association of females with domestic roles, and</p><p>Ben with office, management, and salary, indicating a connection to work and male roles. This result is in direct contrast to situations where, when directly asked whether women are poor at management, GPT-4 gave cautious responses, advising against stereotyping based on gender. This example illustrates how psychology-inspired word association tests can effectively uncover hidden associations in LLMs that are both closed and safety-guarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Engaging with questions at the algorithmic level of analysis allow us to make use of a number of methods from cognitive psychology to probe the internal workings of LLMs.</p><p>As illustrated by the general principle of representational interference during parallel processing, and further demonstrated by the case studies of similarity judgments and association tasks, these approaches offer valuable windows into the representations and processes employed by these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation level: Uncovering mechanisms</head><p>Just as the algorithmic level explores how a computation is performed, the implementation level asks where and with what physical mechanisms those processes are realized. Continuing our bird flight analogy, the implementation level would be akin to studying the bird's muscles, bones, and feathers -the physical components that enable the bird to fly. In neuroscience, this level involves studying individual neurons and neural circuits implementing a given cognitive function <ref type="bibr" target="#b48">(Hubel and Wiesel, 1962)</ref>. For LLMs, the implementation level concerns the physical substrate of the model: taking the individual artificial neuron (or unit) within the network as the fundamental unit of analysis.</p><p>Understanding how these individual units and their connections give rise to the algorithms and representations identified at higher levels is the core challenge of the implementation level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Circuits and mechanistic interpretability</head><p>The implementation level is concerned with identifying the physical substrates and mechanisms that realize cognitive computations. A fundamental approach to understanding complex systems involves targeted interventions, which can reveal underlying causal relationships. In both neuroscience and the study of LLMs, recent methodological advancements have enabled increasingly precise interventions, spurring research into the "circuit-level" mechanisms of behavior. Neuroscience has seen the widespread adoption of optogenetics, while mechanistic interpretability serves as an analogous approach in the study of LLMs.</p><p>Optogenetics allows for the causal control of neuronal activity using light, achieved through the expression of light-sensitive opsins in specific neurons. This technique offers high temporal and spatial precision for manipulating neural circuits, providing critical insights into the implementational bases of core cognitive functions, such as social behaviors in various species <ref type="bibr" target="#b124">(Willmore et al., 2022;</ref><ref type="bibr" target="#b21">Cowley et al., 2024;</ref><ref type="bibr" target="#b57">Lin et al., 2011;</ref><ref type="bibr" target="#b6">Bayless et al., 2023;</ref><ref type="bibr" target="#b22">Dulac et al., 2014)</ref>, memory formation <ref type="bibr" target="#b86">(Ramirez et al., 2013;</ref><ref type="bibr" target="#b87">Rashid et al., 2016;</ref><ref type="bibr" target="#b117">Vetere et al., 2019)</ref>, and the mediation of valence and behavioral states <ref type="bibr" target="#b89">(Redondo et al., 2014;</ref><ref type="bibr" target="#b31">Gehrlach et al., 2019;</ref><ref type="bibr" target="#b75">Namburi et al., 2015)</ref>. A recurring theme in these studies is the identification of specific neuronal populations causally linked to particular behaviors or cognitive processes.</p><p>Mechanistic interpretability applies a similar logic to LLMs, employing techniques like activation patching <ref type="bibr" target="#b119">(Wang et al., 2022;</ref><ref type="bibr" target="#b68">Meng et al., 2022;</ref><ref type="bibr">Gurnee et al., 2023;</ref><ref type="bibr" target="#b108">Todd et al., 2023;</ref><ref type="bibr" target="#b126">Yang et al., 2025)</ref> to directly manipulate activations within the model and causally probe its computation. Early investigations, using ablation techniques akin to neural circuit inhibition, identified "induction heads" <ref type="bibr" target="#b79">(Olsson et al., 2022)</ref> necessary for in-context learning. More recent work has focused on discovering "interpretable" representations within LLMs, aiming to pinpoint individual or combined units that correspond to specific concepts. A notable example is the use of sparse autoencoders <ref type="bibr" target="#b12">(Bricken et al., 2023)</ref>, which learn sparse representations of internal activations, potentially revealing disentangled features. By causally activating these learned features, researchers have induced interpretable behaviors like honesty or role-playing <ref type="bibr" target="#b105">(Templeton et al., 2024)</ref>, suggesting that abstract concepts are indeed encoded within LLMs. These findings parallel the way optogenetics elucidates the neural circuits underlying cognition in biological systems, with circuit discovery beginning to uncover analogous mechanisms in artificial neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-level probes and decoding</head><p>Despite the valuable insights gained from establishing causal links between specific components and behaviors, the limitations of a purely implementation-level, circuit-centric approach are increasingly apparent <ref type="bibr" target="#b30">(Gao et al., 2024;</ref><ref type="bibr" target="#b50">Kantamneni et al., 2025)</ref>. Many cognitive functions, in both biological and artificial systems, likely emerge from distributed representations across populations of elements, rather than being localized to individual units or small circuits. Consequently, a focus solely on circuits may obscure the crucial role of the geometry and dynamics of these population-level representations in underlying computations. Just as early cognitive neuroscience moved beyond the search for highly localized "grandmother cells" responsible for recognizing specific individuals <ref type="bibr" target="#b85">(Quiroga et al., 2005)</ref> to embrace the concept of distributed representations <ref type="bibr" target="#b43">(Haxby et al., 2001;</ref><ref type="bibr" target="#b55">Kriegeskorte et al., 2008)</ref>, LLM interpretability is recognizing the importance of examining population-level activity. Analyzing the geometry and dynamics of these population codes offers a crucial bridge to the algorithmic level of analysis.</p><p>In neuroscience, the limitations of a purely localized view became apparent with the understanding that information about object categories is distributed across brain regions, as revealed by techniques like multi-voxel pattern analysis (MVPA) <ref type="bibr" target="#b43">(Haxby et al., 2001;</ref><ref type="bibr" target="#b76">Norman et al., 2006)</ref>. Furthermore, studying population dynamics has provided critical insights into the mechanisms of cognitive processes, such as the trajectories of neural populations during motor control <ref type="bibr" target="#b17">(Churchland et al., 2012;</ref><ref type="bibr" target="#b52">Kaufman et al., 2014;</ref><ref type="bibr" target="#b29">Gallego et al., 2017)</ref>. Analogously, LLM research is increasingly exploring the collective activity of units, revealing structured representations of information such as syntax <ref type="bibr" target="#b47">(Hewitt and Manning, 2019;</ref><ref type="bibr" target="#b107">Tenney et al., 2019)</ref>, spatial and temporal relationships <ref type="bibr" target="#b42">(Gurnee and Tegmark, 2023)</ref>, truth values <ref type="bibr" target="#b64">(Marks and Tegmark, 2023)</ref>, and cyclical patterns <ref type="bibr" target="#b60">(Liu et al., 2022;</ref><ref type="bibr" target="#b24">Engels et al., 2024)</ref>. Techniques like Concept Activation Vectors (CAVs) <ref type="bibr" target="#b54">(Kim et al., 2018)</ref> and representational engineering <ref type="bibr" target="#b136">(Zou et al., 2023)</ref> further demonstrate how human-understandable concepts are embedded within these distributed representations and can be used to influence model behavior. This convergence in neuroscience and LLM research underscores the necessity of studying population-level representations to understand complex cognitive functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>The implementation level investigates the physical mechanisms underlying computation in both biological brains and large language models. Mirroring neuroscience techniques like optogenetics, mechanistic interpretability aims to uncover circuit-level mechanisms in LLMs. However, the limitations of a purely circuit-centric approach highlight the importance of examining distributed representations at the population level, an approach increasingly adopted in both neuroscience (e.g., through MVPA) and LLM research. Studying the geometry and dynamics of these population codes provides a crucial link to the algorithmic level, offering a more holistic understanding of how complex computations are realized in these systems. While the implementation level represents the current focus of machine learning interpretability, historically, this level has struggled to fully explain higher-order cognition and complex behavior in the brain. Progress in mechanistic interpretability by AI researchers may thus provide an interesting test case for understanding the challenges that neuroscience faces in understanding human cognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using cognitive science to explore the limits of AI models</head><p>In addition to being a source of tools for understanding the implicit assumptions and representations used by large language models, cognitive science offers a different way of thinking about evaluating these models. Many of the evaluations used in AI research focus on defining tasks that are challenging for humans -such as problem-solving <ref type="bibr" target="#b16">(Chollet et al., 2024;</ref><ref type="bibr" target="#b90">Rein et al., 2024;</ref><ref type="bibr" target="#b45">Hendrycks et al., 2020;</ref><ref type="bibr" target="#b121">Wang et al., 2024b)</ref> or mathematical reasoning <ref type="bibr" target="#b127">(Ye et al., 2025;</ref><ref type="bibr" target="#b46">Hendrycks et al., 2021;</ref><ref type="bibr" target="#b101">Shi et al., 2022;</ref><ref type="bibr" target="#b71">Mirzadeh et al., 2024)</ref> and measuring the proportion of these tasks that systems are able to solve. By contrast, cognitive science allows us to think about the kinds of problems that might be difficult for these systems based on what we learn about they work.</p><p>For example, the "embers of autoregression" approach <ref type="bibr" target="#b66">(McCoy et al., 2024)</ref> was able to use consideration of the computational-level problem solved by LLMs to design a set of tasks that they would find problematic, namely tasks where the the target response has low probability according to the pre-trained language model. In the same way, thinking about parallel and serial processes makes it easy to define tasks that will be challenging for any model that can only perform parallel procesing, such as processing images that contain a lot of overlaps in the features of the objects that appear in those images. This kind of approach can allow us to "adverserially" design tasks that might pose a more difficult challenge for existing AI systems. Even as those systems display super-human abilities in some settings, we might expect them to fail on these tasks because they pick out problems that should be uniquely difficult for their non-human cognitive architectures.</p><p>Another way in which cognitive science can be used to explore the limits of AI models relies on their similarities to human cognition. For example, recent work focused on expanding the capabilities of LLMs has focused on the potential impact of inference-time compute, where the system has the opportunity to produce additional output ("reasoning") before generating its final answer <ref type="bibr" target="#b122">(Wei et al., 2022;</ref><ref type="bibr" target="#b77">Nye et al., 2021;</ref><ref type="bibr" target="#b80">OpenAI, 2024;</ref><ref type="bibr" target="#b40">Guo et al., 2025)</ref>. This intervening step provides an additional source of information to condition on in producing a response, as well as the opportunity to engage in additional computation over the input. However, engaging in reasoning is not always beneficial for humans: there are a variety of tasks where thinking out loud has negative consequences for human behavior <ref type="bibr" target="#b88">(Reber, 1976;</ref><ref type="bibr" target="#b96">Schooler and Engstler-Schooler, 1990;</ref><ref type="bibr" target="#b123">Williams et al., 2013;</ref><ref type="bibr" target="#b27">Fiore and Schooler, 2002;</ref><ref type="bibr" target="#b25">Fallshore and Schooler, 1993;</ref><ref type="bibr" target="#b67">Melcher and Schooler, 1996;</ref><ref type="bibr" target="#b53">Khemlani and Johnson-Laird, 2012)</ref>.</p><p>Liu et al. <ref type="bibr" target="#b59">(Liu et al., 2024b)</ref> showed that the psychological literature on the negative effects of verbal thinking provides an effective way to identify problems where inference time compute has negative consequences. Artiticial grammar learning, face recognition, and learning a concept from exemplars are all settings where more reasoningsuch as the use of a chain-of-thought prompt -results in worse performance by LLMs or</p><p>VLMs. These results challenge the assumption that more reasoning always leads to better outcomes that is currently guiding the design of AI systems. We anticipate that a similar approach, focusing on the cases where there might be overlaps in the solutions used in natural and artificial minds, can be used to turn up other challenges for contemporary AI systems, providing a complement to the approach of focusing on the distinctive aspects of the problem that AI systems solve highlighted above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The breakthroughs culminating in advanced AI systems such as large language models have presented computer science with the unfamiliar challenge of interpreting the behavior of complex and opaque neural networks. Just as cognitive science has long grappled with the problem of understanding the mind from the outside in, the tools refined over decades of psychological and neuroscientific inquiry offer a powerful framework for approaching these new forms of intelligence. In this review, we have argued for the utility of Marr's three levels of analysis as an organizing principle for applying these tools to large language models. At the computational level, examining training objectives allows us to predict behavioral patterns, as seen with the "embers of autoregression." At the algorithmic level, methods like similarity judgments and association tasks reveal the structure of internal representations, mirroring techniques used to probe human cognition. Finally, while the implementation level remains a frontier, the study of circuits and population dynamics, drawing parallels with neuroscience, promises to illuminate the physical substrates of these artificial cognitive processes. By embracing this cognitive science perspective, moving beyond purely performance-based evaluations, we can develop more insightful means of understanding, evaluating, and ultimately guiding the development of increasingly sophisticated AI.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>each letter 18 positions backward in the alphabet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Figure 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>computation. Consider bird flight again: while aerodynamics dictates the principles of flight (lift, drag, thrust), different bird species employ different algorithms -variations in flapping patterns and soaring techniques -to achieve flight. These variations represent different algorithmic solutions to the same computational problem. In cognitive science, understanding the algorithmic level involves designing experiments that probe the internal workings of the mind, inferring the nature of mental representations and the processes that operate on them. Just as ornithologists might study wing movements and muscle activity to understand bird flight, cognitive psychologists use reaction times, error patterns, and carefully designed stimuli to understand human cognition. We can apply similar techniques to investigate the algorithmic solutions of large language models.The algorithmic level is particularly relevant to LLMs because, unlike traditional symbolic AI systems with explicitly programmed rules, the specific algorithms and representations employed by LLMs are not pre-defined. They are learned through the training process, resulting in complex and often opaque internal structures that aren't obviously localized in any particular location in the network. Proprietary closed-source networks present additional challenges as their internal states are not directly observable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>Figure 3Patterns of behavior consistent with parallel processing in vision-language models (VLMs). (a) VLMs show highly accurate "pop-out" search for distinctive visual targets but exhibit degraded performance in conjunction search as the number of distractors increases. (b)</figDesc><graphic coords="13,246.07,307.14,51.27,51.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>Figure 4Exploring the sensory representations of large language models with similarity judgments. (a) For musical pitch, both humans and LLMs show a decrease in judged similarity with increases in the interval between tones, but also show an increase at tones an octave apart (a full similarity matrix for GPT-3 is shown inset). As a consequence, both human and LLM similarities are best captured by helical solutions when converted into spatial representations by multidimensional scaling. (b) Two-dimensional multidimensional scaling solutions for vocal consonants and colors for GPT-4 similarity matrices, showing that LLMs can reproduce patterns seen in human representations despite never having had direct experience of sound or color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6</head><label>6</label><figDesc>Figure 6 Both humans and large language models show reductions in performance when engaging in verbal reasoning (as resulting from chain of thought prompting) on these tasks. (a) Implicit statistical learning involves classification of strings generated from artificial grammars. (b) Face recognition involves recognizing faces from a set that shares similar descriptions. (c) Classification of data with exceptionsinvolves learning labels with exceptions.</figDesc><graphic coords="23,151.56,75.19,308.88,360.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="17,72.00,75.19,468.01,256.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Understanding natural and artificial minds across Marr's levels of analysis.</figDesc><table><row><cell>Level of analysis</cell><cell>Focus</cell><cell>Cognitive science</cell><cell>Artificial intelligence</cell></row><row><cell>Computational</cell><cell>What problem is being</cell><cell>Understanding behavior in terms of</cell><cell>Understanding behavior in terms of</cell></row><row><cell></cell><cell>solved? What is the ideal</cell><cell>optimal solutions to environmental</cell><cell>the training objective (e.g., next-</cell></row><row><cell></cell><cell>solution?</cell><cell>pressures (e.g., Bayesian models of</cell><cell>word prediction) and deviations from</cell></row><row><cell></cell><cell></cell><cell>cognition).</cell><cell>optimal benchmarks (e.g., violations of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>probability axioms).</cell></row><row><cell>Algorithmic</cell><cell>How is the solution approx-</cell><cell>Inferring mental representations and</cell><cell>Probing representations and processes</cell></row><row><cell></cell><cell>imated? What represen-</cell><cell>processes through behavioral exper-</cell><cell>via behavioral analysis (e.g., analyzing</cell></row><row><cell></cell><cell>tations and processes are</cell><cell>iments (e.g., reaction times, error</cell><cell>systematic errors, soliciting similarity</cell></row><row><cell></cell><cell>used?</cell><cell>patterns, similarity judgments).</cell><cell>judgments, uncovering hidden associa-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tions).</cell></row><row><cell>Implementation</cell><cell>How are the representations</cell><cell>Studying neural circuits and population</cell><cell>Studying artificial neurons, circuits</cell></row><row><cell></cell><cell>and algorithms physically</cell><cell>activity using techniques like optoge-</cell><cell>(e.g., induction heads), and population</cell></row><row><cell></cell><cell>realized?</cell><cell>netics, single-unit recording, fMRI, and</cell><cell>activity using techniques like activation</cell></row><row><cell></cell><cell></cell><cell>MVPA.</cell><cell>patching, sparse autoencoders, and</cell></row><row><cell></cell><cell></cell><cell></cell><cell>representational geometry analysis.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research project and related research were made possible with the support of the <rs type="funder">NOMIS Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anadkat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The adaptive character of thought</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Erlbaum</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human memory: An adaptive perspective</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Milson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">703</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Measuring implicit bias in explicitly unbiased large language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sucholutsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.04105</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<title level="m">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="18327" to="18332" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A neural circuit for male sexual behavior and reward</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Bayless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Chung-Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>De Andrade Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Knoedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Livingston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lomvardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3862" to="3881" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</author>
		<title level="m">Bayesian theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using cognitive psychology to understand gpt-3</title>
		<author>
			<persName><forename type="first">M</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2218523120</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Klyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maslej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12941</idno>
		<title level="m">The foundation model transparency index</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petryk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mañas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jayaraman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17247</idno>
		<title level="m">An introduction to vision-language modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A flexible model of working memory</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Buschman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards monosemanticity: Decomposing language models with dictionary learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bricken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Denison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Transformer Circuits Thread</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with GPT-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the limits of vision language models through the lens of the binding problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Giallanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>De Sabbata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frankland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="113436" to="113460" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic models of language processing and acquisition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="335" to="344" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Arc prize 2024</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Knoop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kamradt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Landers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.04604</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nuyujukian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural population dynamics during reaching</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">487</biblScope>
			<biblScope unit="page" from="51" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Coda-Forno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18225</idno>
		<title level="m">Cogbench: a large language model walks into a psychology lab</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A spreading-activation theory of semantic processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Loftus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">407</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Retrieval time from semantic memory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Quillian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of verbal learning and verbal behavior</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="240" to="247" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mapping model units to visual neurons reveals population code for social behaviour</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Cowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ireland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">629</biblScope>
			<biblScope unit="issue">8014</biblScope>
			<biblScope unit="page" from="1100" to="1108" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural control of maternal and paternal behaviors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dulac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>O'connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="issue">6198</biblScope>
			<biblScope unit="page" from="765" to="770" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dimensions of color vision</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="467" to="474" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14860</idno>
		<title level="m">Not all language model features are linear</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Post-encoding verbalization impairs transfer on artificial grammar tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fallshore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Schooler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society Erlbaum</title>
		<meeting>the Fifteenth Annual Conference of the Cognitive Science Society Erlbaum<address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="412" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Implicit measures in social cognition research: Their meaning and use</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Fazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="297" to="327" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How did you get here from there? Verbal overshadowing of spatial mental models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Schooler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="897" to="910" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Racial stereotypes: Associations and ascriptions of positive and negative characteristics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Gaertner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Mclaughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Psychology Quarterly</title>
		<imprint>
			<biblScope unit="page" from="23" to="30" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural manifolds for the control of movement</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Perich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="984" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>La Tour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Troll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04093</idno>
		<title level="m">Scaling and evaluating sparse autoencoders</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aversive state processing in the posterior insular cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Gehrlach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dolensek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roy Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matthys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Junghänel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Gaitanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Podgornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Reddy Vaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Conzelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gogolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1424" to="1437" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Implicit and explicit memory for new associations in normal and amnesic subjects</title>
		<author>
			<persName><forename type="first">P</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Schacter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, memory, and cognition</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">501</biblScope>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Implicit social cognition: attitudes, self-esteem, and stereotypes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Banaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Measuring individual differences in implicit cognition: the implicit association test</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Mcghee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1464</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding human intelligence through human limitations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="873" to="883" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<title level="m">Bayesian models of cognition: reverse engineering the mind</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2024">2024a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Topics in semantic association</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimal predictions in everyday cognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="767" to="773" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayes in the age of intelligent machines</title>
		<imprint>
			<date type="published" when="2024">2024b</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12948</idno>
		<title level="m">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Troitskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01610</idno>
		<title level="m">Finding neurons in a haystack: Case studies with sparse probing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.02207</idno>
		<title level="m">Language models represent space and time</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distributed and overlapping representations of faces and objects in ventral temporal cortex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Haxby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Gobbini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Furey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ishai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pietrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="issue">5539</biblScope>
			<biblScope unit="page" from="2425" to="2430" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Revealing the multidimensional mental representations of natural objects underlying human similarity judgements</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Hebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature human behaviour</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1173" to="1185" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03300</idno>
		<title level="m">Measuring massive multitask language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<title level="m">Measuring mathematical problem solving with the math dataset</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Prospect theory: An analysis of decision under risk</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="292" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Are sparse autoencoders useful? a case study in sparse probing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kantamneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajamanoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.16681</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The discrimination of visual number</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Reese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Volkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American journal of psychology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="525" />
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cortical activity in the null space: permitting preparation without movement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="440" to="448" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hidden conflicts: Explanations make inconsistencies harder to detect</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Khemlani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Johnson-Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="486" to="491" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2668" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Matching categorical object representations in inferior temporal cortex of man and monkey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bodurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Esteky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1126" to="1141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Kurdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Seitchik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Axt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karapetyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tomezsko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Banaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relationship between the LEVELS OF ANALYSIS FOR LARGE LANGUAGE MODELS 32 implicit association test and intergroup behavior: A meta-analysis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">569</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Functional identification of an aggression locus in the mouse hypothalamus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">470</biblScope>
			<biblScope unit="issue">7333</biblScope>
			<biblScope unit="page" from="221" to="226" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sucholutsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<title level="m">Large language models assume people are more rational than we really are</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sucholutsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lombrozo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.21333</idno>
		<title level="m">Mind your step (by step): Chain-of-thought can reduce performance on tasks where thinking makes humans worse</title>
		<imprint>
			<date type="published" when="2024">2024b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Towards understanding grokking: An effective theory of representation learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kitouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="34651" to="34663" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The universal law of generalization holds for naturalistic stimuli</title>
		<author>
			<persName><forename type="first">R</forename><surname>Marjieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">573</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Large language models predict human sensory judgments across six modalities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Marjieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sucholutsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">21445</biblScope>
			<date type="published" when="2024">2024b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Marjieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Veselovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sucholutsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.01540</idno>
		<title level="m">What is a number, that a large language model may know it? arXiv preprint</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06824</idno>
		<title level="m">The geometry of truth: Emergent linear structure in large language model representations of true/false datasets</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<editor>Vision. W. H. Freeman</editor>
		<imprint>
			<date type="published" when="1982">1982</date>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Embers of autoregression show how large language models are shaped by the problem they are trained to solve</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page">2322420121</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The misremembrance of wines past: Verbal and perceptual expertise differentially mediate verbal overshadowing of taste memory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Melcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Schooler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="245" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Locating and editing factual associations in gpt</title>
		<author>
			<persName><forename type="first">K</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="17359" to="17372" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The cognitive revolution: a historical perspective</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="141" to="144" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The child&apos;s representation of number: A multidimensional scaling analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child development</title>
		<imprint>
			<biblScope unit="page" from="1470" to="1479" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shahrokhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.05229</idno>
		<title level="m">Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Large language models estimate fine-grained human color-concept associations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17781</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">One fish, two fish, but not the whole sea: Alignment reduces language models&apos; conceptual diversity</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.04427</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Rationalizing constraints on the capacity for cognitive control</title>
		<author>
			<persName><forename type="first">S</forename><surname>Musslick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="757" to="775" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Namburi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beyeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yorozu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Calhoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Halbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Anahtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Felix-Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Wickersham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Tye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A circuit mechanism for differentiating positive and negative associations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">520</biblScope>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Beyond mind-reading: multi-voxel pattern analysis of fmri data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Polyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Detre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Haxby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="424" to="430" />
		</imprint>
	</monogr>
	<note>in cognitive sciences</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00114</idno>
		<title level="m">Show your work: Scratchpads for intermediate computation with language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scharf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ratto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolmetz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.00577</idno>
		<title level="m">Turing representational similarity analysis (rsa): A flexible method for measuring alignment between human and artificial intelligence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11895</idno>
		<title level="m">-context learning and induction heads</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Learning to reason with LLMs</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Parallelograms revisited: Exploring the limitations of vector space models for simple analogies</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page">104440</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A rational analysis of the approximate number system</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Piantadosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="877" to="886" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Exact number concepts are limited to the verbal count range</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Piantadosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="371" to="381" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Chronometric analysis of classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">392</biblScope>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Invariant visual representation by single neurons in the human brain</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Q</forename><surname>Quiroga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fried</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="issue">7045</biblScope>
			<biblScope unit="page" from="1102" to="1107" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Creating a False Memory in the Hippocampus</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pignatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Redondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tonegawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">341</biblScope>
			<biblScope unit="issue">6144</biblScope>
			<biblScope unit="page" from="387" to="391" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Competition between engrams influences fear memory formation and recall</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mercaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-L</forename><surname>Hsiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Cristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="issue">6297</biblScope>
			<biblScope unit="page" from="383" to="387" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Implicit learning of synthetic languages: The role of instructional set</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Reber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Learning and Memory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">88</biblScope>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Bidirectional switch of the valence associated with a hippocampal contextual memory engram</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Redondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Arons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tonegawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">513</biblScope>
			<biblScope unit="issue">7518</biblScope>
			<biblScope unit="page" from="426" to="430" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Gpqa: A graduate-level google-proof q&amp;a benchmark</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Conference on Language Modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Cognitive representations of semantic categories</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology: General</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">192</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Rational approximations to rational models: Alternative algorithms for category learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="1144" to="1167" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Reconciling intuitive physics and Newtonian mechanics for colliding objects</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Mansinghka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="411" to="437" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Savage</surname></persName>
		</author>
		<title level="m">Foundations of statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Implicit memory: History and current status</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Schacter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology: learning, memory, and cognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">501</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Verbal overshadowing of visual memories: Some things are better left unsaid</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Schooler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Engstler-Schooler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="71" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The analysis of proximities: Multidimensional scaling with an unknown distance function: I</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="124" to="140" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Multidimensional scaling, tree-fitting, and clustering</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="page" from="390" to="398" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Geometrical approximations to the structure of musical pitch</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">305</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Towards a universal law of generalization for psychological science</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="1317" to="1323" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Language models are multilingual chain-of-thought reasoners</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03057</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Sucholutsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Muttenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Groen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achterberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.13018</idno>
		<title level="m">Getting aligned on representational alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Padua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02754</idno>
		<title level="m">Conceptual structure coheres in human cognition but not in large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">Gemini: a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bricken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ameisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcdougall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Macdiarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Sumers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Rules and similarity in concept learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05950</idno>
		<title level="m">Bert rediscovers the classical nlp pipeline</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.15213</idno>
		<title level="m">Function vectors in large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Serial vs. parallel processing: Sometimes they look like tweedledum and tweedledee but they can (and should) be distinguished</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Townsend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="54" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Illusory conjunctions in the perception of objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="141" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Why are small and large numbers enumerated differently? a limited-capacity preattentive stage in vision</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Trick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Pylyshyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Features of similarity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="327" to="352" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Nearest neighbor analysis of psychological spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Judgment under uncertainty: heuristics and biases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Memory formation in the absence of experience</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vetere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Steadman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Restivo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Ressler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Josselyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Frankland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="933" to="940" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Theory of games and economic behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Morgenstern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1947">1947</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Variengien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shlegeris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2211.00593,2" />
		<title level="m">Interpretability in the wild: a circuit for indirect object identification in gpt-2 small</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Large language models are implicitly latent variable models: Explaining and finding good demonstrations for in-context learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saxon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Mmlu-pro: A more robust and challenging multi-task language understanding benchmark</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arulraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2024">2024b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">The hazards of explanation: Overgeneralization in the face of exceptions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lombrozo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rehder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1006</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Behavioural and dopaminergic signatures of resilience</title>
		<author>
			<persName><forename type="first">L</forename><surname>Willmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Falkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">611</biblScope>
			<biblScope unit="issue">7934</biblScope>
			<biblScope unit="page" from="124" to="132" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">An explanation of in-context learning as implicit Bayesian inference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02080</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Webb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.20332</idno>
		<title level="m">Emergent symbolic mechanisms support abstract reasoning in large language models</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Aime-preview: A rigorous and immediate evaluation framework for advanced mathematical reasoning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://github.com/GAIR-NLP/AIME-Preview.GitHubrepository" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Vision as Bayesian inference: analysis by synthesis?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="301" to="308" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">What should embeddings embed? autoregressive models represent latent generating distributions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Sumers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<title level="m">Deep de finetti: Recovering topic distributions from large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Revisiting topic-guided language models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno>LEVELS OF ANALYSIS FOR LARGE LANGUAGE MODELS 40</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Sniff ai: Is my&apos;spicy&apos;your&apos;spicy&apos;? exploring llm&apos;s perceptual alignment with human smell experiences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brianz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Obrist</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.06950</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Incoherent probability judgments in large language models</title>
		<author>
			<persName><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2024">2024a</date>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<author>
			<persName><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01860</idno>
		<title level="m">Eliciting the priors of large language models using iterated in-context learning</title>
		<imprint>
			<date type="published" when="2024">2024b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">The bayesian sampler: Generic bayesian inference causes incoherence in human probability judgments</title>
		<author>
			<persName><forename type="first">J.-Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">719</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-K</forename><surname>Dombrowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01405</idno>
		<title level="m">Representation engineering: A top-down approach to ai transparency</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
