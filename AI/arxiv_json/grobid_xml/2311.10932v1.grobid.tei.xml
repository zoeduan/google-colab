<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cognitive bias in large language models: Cautious optimism meets anti-Panglossian meliorism</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-11-18">18 Nov 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Thorstad</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cognitive bias in large language models: Cautious optimism meets anti-Panglossian meliorism</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-11-18">18 Nov 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">A3F5F73BC5061E8C4CAE44ADE08F404F</idno>
					<idno type="arXiv">arXiv:2311.10932v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional discussions of bias in large language models focus on a conception of bias closely tied to unfairness, especially as affecting marginalized groups. Recent work raises the novel possibility of assessing the outputs of large language models for a range of cognitive biases familiar from research in judgment and decisionmaking. My aim in this paper is to draw two lessons from recent discussions of cognitive bias in large language models: cautious optimism about the prevalence of bias in current models coupled with an anti-Panglossian willingness to concede the existence of some genuine biases and work to reduce them. I draw out philosophical implications of this discussion for the rationality of human cognitive biases as well as the role of unrepresentative data in driving model biases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent success of large language models gives new urgency to the question of how model performance should be evaluated. In many tasks, models can be evaluated for the accuracy of their outputs. However, models can also be evaluated along other important dimensions. For example, we can assess models for the transparency or interpretability of their judgments <ref type="bibr" target="#b12">(Creel 2020;</ref><ref type="bibr" target="#b74">Vredenburgh 2022)</ref>. We can also assess models for the presence of problematic biases <ref type="bibr" target="#b47">(Kelly 2023;</ref><ref type="bibr" target="#b42">Johnson 2020)</ref>.</p><p>Most work on biases in large language models focuses on a conception of bias closely tied to unfairness, especially as affecting marginalized social groups. However, recent work has alleged that large language models also show a number of classic cognitive biases familiar from work in the psychology of reasoning, behavioral economics, and judgment and decisionmaking <ref type="bibr" target="#b13">(Dasgupta et al. 2022;</ref><ref type="bibr" target="#b51">Lin and Ng 2023;</ref><ref type="bibr" target="#b44">Jones and Steinhardt 2022)</ref>. This development is exciting because it raises the possibility of using cognitive bias as a novel metric by which to evaluate the performance of large language models. A natural question to ask is how well existing systems perform along the metric of cognitive bias.</p><p>By contrast to recent work on algorithmic bias, my aim in this paper is to offer a qualified piece of good news: existing evidence does not support the attribution of widespread and problematic cognitive biases to large language models.</p><p>In more detail, my aim in this paper is to draw two lessons from recent discussions of cognitive bias in large language models. The first lesson is cautious optimism about model performance. In particular, many studies find biases which have standard rationalizing explanations when produced by humans. I argue that these explanations often generalize to show that the claimed biases are desirable features of reasoning by large language models (Section 3), in the process reinforcing the robustness of standard rationalizing explanations in the human case by showing how similar cognitive phenomena arise in agents with highly distinct cognitive architectures <ref type="bibr" target="#b13">(Dasgupta et al. 2022)</ref>. Furthermore, some studies find especially benign forms of classic biases (Sections 4-5), whose desirability is particularly difficult to contest.</p><p>The second lesson is an anti-Panglossian willingness to accept the existence of some genuine and undesirable cognitive biases in reasoning by existing large language models.</p><p>In particular, I argue that many models show framing effects (Section 6) and that these effects are not always desirable. When faced with undesirable biases, I argue that the proper reaction is to work to mitigate the bias, but not to exaggerate the prevalence or undesirability of biases in assessing overall model performance.</p><p>Here is the plan. Section 2 begins with two preliminary remarks. Sections 3-5 then make the case for cautious optimism through case studies of knowledge effects (Section 3), availability bias (Section 4) and anchoring bias (Section 5). Section 6 makes the case for an anti-Panglossian willingness to accept at least one problematic bias: framing effects.</p><p>Section 7 uses these discussions to elaborate and justify the reactions of cautious optimism and anti-Panglossian meliorism. Section 8 concludes by drawing philosophical implications concerning the role of unrepresentative data in producing model biases <ref type="bibr">(Section 8.1)</ref> and the rationality of biases in human cognition (Section 8.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Before beginning, two remarks are in order. First, as Richard <ref type="bibr" target="#b63">Shiffrin and Melanie Mitchell (2023)</ref> remind us, it is important to avoid inappropriate anthropomorphism in describing the performance of large language models. Some theorists may be comfortable using anthropomorphic vocabulary in which models are described as reasoning to judgments, which can be rational or irrational. Others will prefer a more neutral paraphrase, in which models are described as returning outputs in response to prompts, where the outputs may be desirable or undesirable given users' goals. I will sometimes use cognitive vocabulary, such as reasoning and judging, to describe model performance, although readers are welcome to substitute their preferred de-anthropomorphized paraphrase. On the other hand, I will not describe model outputs as rational or irrational, but only as desirable or undesirable. This reflects a lack of commitment to the judgments made by large language models having normative status in their own right. This contrasts with the case of human judgment, where it makes sense not only to describe biases as rational or irrational, but also to ask (Section 8.2) how the study of biases in language models bears on the rationality of biases in human cognition.</p><p>Second, recent findings suggest that patterns of bias in large language models may be highly model-sensitive. For example, Thilo <ref type="bibr">Hagendorff and colleagues (2023)</ref> find atypical performance by GPT-1 and GPT-2 in reasoning tasks, human-like performance by GPT-3, and hyperrational performance by GPT-4. Likewise, John J. <ref type="bibr" target="#b38">Horton (2023)</ref> finds atypical behavior by models prior to GPT-3, but humanlike behavior in GPT-3. Given these findings, it is very important to specify the model used in each finding, which I will do in all cases where a finding is extensively discussed.</p><p>There does remain some danger that the discussion in this paper will be superseded or rendered moot by further technological changes, leading to changes in patterns of model reasoning. This is a risk faced by a great deal of research in the philosophy of artificial intelligence, and it is a risk that must be openly admitted without dissembling.</p><p>With these remarks in order, the next order of business is to look at four types of biases that have been alleged in large language models: knowledge effects (Section 3), availability bias (Section 4), anchoring bias (Section 5) and framing effects (Section 6). I will suggest that the first three findings may not be undesirable, but that some framing effects are probably undesirable and should be mitigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Knowledge effects</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>For much of the twentieth century, human reasoning was understood using a logical paradigm <ref type="bibr" target="#b75">(Wason 1968;</ref><ref type="bibr" target="#b58">Rips 1994)</ref>. Agents asked to assess the quality of inferences were assumed to test them for logical validity. Conditional claims were modeled using the material conditional, and conditional rules were to be tested by trying to falsify the embodied material conditional.</p><p>A probabilistic turn throughout the academy <ref type="bibr" target="#b20">(Erk 2022;</ref><ref type="bibr" target="#b27">Ghahramani 2015)</ref> has come to psychology <ref type="bibr" target="#b8">(Chater et al. 2006)</ref>, and in particular to the psychology of reasoning. There, 'new paradigm' Bayesian approaches suggest that humans often do and should interpret reasoning tasks probabilistically, rather than logically <ref type="bibr" target="#b18">(Elqayam and Over 2013;</ref><ref type="bibr">Oaksford and Chater 2007)</ref>. On Bayesian approaches, conditional assertions are licensed if the consequent has high probability conditional on the antecedent <ref type="bibr">(Oaksford and Chater 2007)</ref>; conditional rules are tested by reducing uncertainty about the probabilistic dependency between consequent and antecedent <ref type="bibr" target="#b55">(Oaksford and Chater 1994)</ref>; and inferences are tested for probabilistic forms of validity <ref type="bibr" target="#b0">(Adams 1975)</ref>.</p><p>Logical and probabilistic paradigms come apart in their treatment of knowledge effects:</p><p>the influence of prior knowledge on reasoning in ways not licensed by classical logic.</p><p>For example, agents are more likely to endorse an inference if they are more confident in its conclusion. On a logical paradigm, this finding was taken to reflect a problematic belief bias to judge arguments with believed conclusions to be logically valid <ref type="bibr" target="#b21">(Evans et al. 1983)</ref>. But on a probabilistic paradigm, this finding is to be expected: good inferences should secure high-probability conclusions, and the prior probability of a conclusion has an important effect on its probability at the end of an inference <ref type="bibr" target="#b0">(Adams 1975;</ref><ref type="bibr">Oaksford and Chater 2007)</ref>.</p><p>Many large language models show human-like knowledge effects in a variety of tasks, including the Wason selection task <ref type="bibr" target="#b6">(Binz and Schulz 2023)</ref> as well as syllogistic and natural-language reasoning problems <ref type="bibr" target="#b13">(Dasgupta et al. 2022)</ref>. In this section, I introduce one salient knowledge effect (Section 3.2) then argue that the effect should be viewed at least as favorably in large language models as it is viewed in humans (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Wason selection</head><p>Suppose you are shown four two-sided cards. Their visible sides contain an ace, king, two and seven, respectively (Figure <ref type="figure" target="#fig_0">2</ref>). You are asked to test the rule that 'If a card has an ace on one side, then it has a two on the other'. Which cards should you turn over to test the rule?</p><formula xml:id="formula_0">A K 2 7</formula><p>Figure <ref type="figure">1</ref>: The Wason selection task</p><p>Let us label the cards as p (A), ¬p (K), q (2) and ¬q (7). In this notation, the rule is 'If p, then q'. On a logical interpretation, the rule expresses the material conditional p ⊃ q, which is tested by searching for falsifying instances p ∧ ¬q. This means that agents should turn the p and ¬q cards, that is the ace and the seven. Wason's original finding, replicated across countless subsequent experiments, is that far fewer than ten percent of agents make the logically correct choice <ref type="bibr" target="#b75">(Wason 1968</ref>).</p><p>This behavior is poor enough for such a simple task that we are well within our rights to ask whether agents might have interpreted the task probabilistically rather than logically.</p><p>The classic Bayesian approach to the Wason selection task is due to Mike <ref type="bibr" target="#b55">Oaksford and Nick Chater (1994)</ref>.</p><p>On this approach, agents turn cards in order to reduce uncertainty about the probabilistic relationship between the propositions p and q expressed in the conditional rule. On the simplest model, they want to discriminate between two hypotheses: the dependence hypothesis P(q|p) = 1 that p and q are probabilistically dependent, and the independence hypothesis P(q|p) = P(q) that p and q are probabilistically independent.</p><p>Oaksford and Chater make two additional assumptions. First, they assume that the uncertainty which agents aim to reduce is measured by Shannon entropy <ref type="bibr" target="#b62">(Shannon 1948</ref>).<ref type="foot" target="#foot_0">foot_0</ref> </p><p>This is a common assumption drawn from research in information theory. Second, Oaksford and Chater assume that agents treat p and q as somewhat antecedently implausible. This is justified by research suggesting that agents do and should treat most propositions as improbable in causal reasoning, due to factors such as the large number of possible alternatives <ref type="bibr" target="#b3">(Anderson 1990)</ref>. That assumption places us within the realm of knowledge effects: manipulations to increase the prior probability of p and q change Wason selection behavior <ref type="bibr" target="#b55">(Oaksford and Chater 1994)</ref>.</p><p>Under these assumptions, we can show that uncertainty reduction is maximized by turning the p and q cards, that is the ace and the two. And that is just what agents tend to do <ref type="bibr" target="#b55">(Oaksford and Chater 1994)</ref>. In this way, the Oaksford and Chater model provides a probabilistic explanation for why agents do, and perhaps should, turn the cards that they choose to turn.</p><p>Ishita <ref type="bibr">Dasgupta and colleagues (2022)</ref> test the Chinchilla model <ref type="bibr" target="#b37">(Hoffmann et al. 2022)</ref> on several versions of the Wason selection task. They find across task versions that the model is no more than about 50% likely to take the logically correct action of turning the p and ¬q cards, and in many conditions the model is at most 20% likely to do so (Figure <ref type="figure" target="#fig_0">2</ref>). In particular, Dasgupta and colleagues find a significant tendency to turn the q card.</p><p>As Dasgupta and colleagues note, these patterns of behavior conform in coarse outline to the predictions of Oaksford and Chater's probabilistic model but conform less well to the logical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A feature or a bug?</head><p>Should knowledge effects be treated as a desirable feature of large language models, or an undesirable bug to be driven out of them? To a large extent, I think that we should answer this question in the same way as we answer it for humans. Those sympathetic to Bayesian approaches stress that while logic is well-suited for reasoning under certainty, probabilistic approaches are well-suited for reasoning in an increasingly uncertain and data-driven world. Probabilistic approaches view knowledge effects as desirable uses of prior knowledge to improve reasoning. Those sympathetic to logical approaches will no doubt disagree, but this is not the place to re-litigate ongoing normative disputes between the logical and probabilistic paradigms.</p><p>However, there may be two reasons to look more favorably on knowledge effects in large language models than in humans. The first is that previous logical paradigms in artificial intelligence have been challenged by increasingly successful probabilistic approaches <ref type="bibr" target="#b27">(Ghahramani 2015)</ref>. It is now thought that probabilistic systems often outperform logicbased agents in the data-laden, uncertainty-rich contexts which large language models confront: exactly the conditions under which probabilists suggested they should. If this is right, then even if we think that humans often do better to reason logically, we needn't enforce the same constraint on deep learning agents, who are increasingly successful in combining probabilistic tools with data to make sense of the world.</p><p>Second, there is good evidence that many large language models can learn the logical interpretations of reasoning tasks when they are asked to. For example, Dasgupta and colleagues also find that the Chinchilla model learns after just five training instances to nearly eliminate belief bias in natural language inference, and shifts substantially towards logical performance in the Wason selection task <ref type="bibr" target="#b13">(Dasgupta et al. 2022</ref>). 2 This suggests that if probabilistic construals of reasoning tasks are a feature of many large language models, they are not a deep feature ingrained by limits in cognitive abilities, as some authors have suggested that they are in the human case <ref type="bibr" target="#b22">(Evans et al. 2003)</ref>. Instead, large language models often retain the ability to reason either logically or probabilistically, and inducing logical reasoning may be as simple as telling the models that we would like them to reason logically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Availability</head><p>If we are going to find uncontroversially problematic cognitive biases in large language models, we will need to look beyond knowledge effects. A natural place to start is by replicating classic biases from the heuristics and biases paradigm. In this section and 2 Interpreting Wason selection task data is difficult because Dasgupta and colleagues find less movement towards the logical interpretation with non-realistic prompts. It is well known that humans also react quite differently to realistic versions of the Wason selection task than to non-realistic versions. What to make of this finding in human reasoning is an active area of descriptive and normative dispute <ref type="bibr" target="#b10">(Cheng and Holyoak 1985;</ref><ref type="bibr" target="#b11">Cosmides 1989;</ref><ref type="bibr" target="#b55">Oaksford and Chater 1994)</ref>, and the same disputes may transfer to the machine case as well.</p><p>the next, I explore attempts to find two of the three original biases proposed within this paradigm: availability bias and anchoring bias. I suggest that both attempts encounter significant obstacles, revealing important descriptive and normative lessons for future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Current research on availability</head><p>In the early 1970s, Daniel Kahneman and Amos Tversky proposed that humans often make inferences using the availability heuristic of "estimat[ing] frequency or probability by the ease with which instances or associations could be brought to mind" <ref type="bibr">(Tversky and Kahneman 1973, p. 208)</ref>. For example, participants presented with a list of 19 famous female actors and 20 less-famous male actors subsequently recalled the list as containing more female than male actors <ref type="bibr" target="#b71">(Tversky and Kahneman 1973)</ref>. A natural explanation for this finding invokes availability: because participants were more readily able to bring female actors to mind during subsequent recall, they judged that the list contained more female than male actors.</p><p>It is now almost universally acknowledged that early discussions of the availability heuristic passed too freely between two senses of availability <ref type="bibr" target="#b60">(Schwartz et al. 2002)</ref>.</p><p>Subjective availability involves reliance on features of the subjective experience of the recall process, such as the felt ease or fluency with which information comes to mind. In this sense, agents may judge male actors to be rare if they strain and feel disfluency in trying to recall male actors. By contrast, objective availability involves reliance on the content of information retrieved, or on non-experiential features of the retrieval process such as the time needed to retrieve information. In this sense, agents may judge male actors to be rare if they cannot recall many male actors, or if it takes a long time to recall male actors.</p><p>Few theorists hold that reliance on objective availability of information is always irrational or undesirable. If we can quickly bring many examples of a category to mind, then that provides some evidence that the category is common in our experience, and hence in the world. This much is conceded by Tversky and Kahneman themselves. 3 Of course, to say that reliance on objective availability is sometimes desirable is not to say that uncritical deference to objective availability is desirable. Objective availability may be skewed by task-irrelevant factors such as the fame of actors, and agents must take appropriate steps to correct for these biasing factors. But no theory of human rationality or desirable model performance should fix a target of complete unreliance on objective availability.</p><p>Matters are more complicated with regard to subjective availability. For present purposes, it is enough to say that subjective availability is not at issue in assessing current large language models, since it has not been alleged that large language models rely on, or even have such a thing as a subjective experience of memory retrieval. The irrationality or undesirability of subjective availability has been challenged in recent areas such as metacognition, where detailed and nuanced patterns of reliance on subjective feelings of fluency are thought to explain much of the success of human metacognition <ref type="bibr" target="#b2">(Alter and Oppenheimer 2009;</ref><ref type="bibr" target="#b57">Proust 2013)</ref>. However, for present purposes we may restrict attention to objective availability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Availability in relation extraction</head><p>Relation extraction tasks involve identifying relationships between objects from textual discussions of those objects. A paradigmatic relationship extraction task is the task of identifying drug-drug interactions <ref type="bibr" target="#b76">(Zhang et al. 2020)</ref>. Given a textual description of the interaction between two drugs, the algorithm must classify the type of interaction between them.</p><p>The Drug-Drug Interaction (DDI) dataset is an annotated corpus of 1,017 texts describing 5,021 interactions between various drugs <ref type="bibr" target="#b61">(Segura-Bedmar et al. 2013)</ref>. Each discussion is annotated with one of five interaction types: mechanism for a description of the inter-3 "Availability is an ecologically valid clue for the judgment of frequency because, in general, frequent events are easier to recall or imagine than infrequent ones" <ref type="bibr">(Tversky and Kahneman 1973, p. 209)</ref>   <ref type="table" target="#tab_0">1</ref>).</p><p>Section 4.1 distinguished between two forms of availability: objective and subjective.</p><p>Lin and Ng's experiment studies a form of objective availability: the content of information stored in training data. This is an especially benign form of objective availability, because we are concerned with the availability of information rather than with experiential properties of the information retrieval process, and we are concerned with the total information stored in memory rather than a potentially unrepresentative sample retrieved during decisionmaking. Section 4.1 suggested that many instances of objective availability should be regarded as unproblematic, and that seems a natural approach to the results presented by Lin and Ng.</p><p>Lin and Ng hold that because the model has no specific information about the dummy descriptor 'N/A', "the best that an unbiased model can do is to make a uniform random guess" (Lin and Ng 2023). Traditional results in Bayesian epistemology suggest otherwise. Training on the DDI dataset provides the model with valuable information about the distribution of drug-drug interaction types across drugs. Rational Bayesian inference involves combining this prior information with novel information provided by descriptions to determine the probability that each given interaction is at play. Since the model has been exposed to primarily negative interactions during training, the model correctly learns that negative interactions are more common than positive interactions and learns to project this relationship onto novel drugs. When the model is exposed to larger samples of training data, it becomes more confident that negative interactions are prevalent. In the absence of competing information to move the model away from the prior, priors dominate and the model shows a strong tendency to predict novel drug-drug interactions to be negative, increasing in the quantity of training data. From an orthodox Bayesian standpoint, this is desirable behavior that should not be driven out of classification models. If anything, Lin and Ng's data show under-reliance, rather than over-reliance, on prior knowledge of interaction types.</p><p>Lin and Ng do suggest one more plausible lesson from this discussion: labels matter.</p><p>While many machine learning scientists expect label information to become unimportant after training, testing models on content-free sentences reminds us of the importance of labels, since these sentences will be more likely to be classified using labels that are more frequent in the training data.<ref type="foot" target="#foot_2">foot_2</ref> However, it is not clear that forcing a uniform distribution of classification on content-free sentences is the right way to reduce the influence of arbitrary labels. After all, there is considerable arbitrariness in the number of labels used in the training data: for example, we could easily imagine the positive interactions being collapsed under a single label instead of four. Under a uniform distribution, this would increase the probability of negative predictions from 20% to 50%, a type of label-sensitivity that more traditional Bayesian methods avoid.</p><p>One further lesson from this discussion is the importance of ecologically valid training data <ref type="bibr" target="#b70">(Todd and Gigerenzer 2012)</ref>. Models need to be exposed to data that is representative of the phenomena they will encounter during test, so that they will know how to predict the target phenomenon and not be distracted by distortions in the training data. This much is familiar from recent discussions of algorithmic fairness (Hedden 2021; Johnson forthcoming). Perhaps Lin and Ng's suggestion is that the DDI dataset is unrepresentative in its high proportion of negative drug-drug interactions, and if that is the case they will certainly have a point. However, if that is true, this failure should not be blamed on classifier algorithms. It should instead be blamed on those who collect and generate ecologically invalid datasets, or who use those datasets to train models to perform tasks for which the training data will no longer be representative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Heuristics and biases: Anchoring</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Current research on anchoring</head><p>The second of Tversky and Kahneman's original three heuristics is anchoring and adjustment <ref type="bibr">(Tversky and Kahneman 1974)</ref>. Suppose I ask you to estimate the year in which George Washington was first elected president. You might answer by anchoring on an initial quantity, the year (1776) in which the Revolutionary War began, then adjusting upwards and downwards to incorporate relevant knowledge, such as the length of the Revolutionary War and the drafting of the Constitution. If you are like most people, you might settle on an estimate around 1786.5 <ref type="bibr" target="#b50">(Lieder et al. 2018)</ref>, which is quite good:</p><p>Washington was elected in 1789.</p><p>As this example illustrates, anchoring and adjustment produces a characteristic anchoring effect in which judgments are skewed toward the initial anchor. 1786.5 is quite close</p><p>to the correct answer, but biased downwards towards the low anchor of 1776. Anchoring effects are traditionally explained as the result of insufficient adjustments away from the initial anchor.</p><p>Tversky and <ref type="bibr">Kahneman (1974)</ref> initially proposed that a great number of anchoring effects should be explained as the result of mental processes of anchoring and adjustment.</p><p>For example, Tversky and Kahneman instructed participants to spin a wheel, then judge whether the number displayed on the wheel was higher or lower than the number of African countries in the United Nations, and finally to estimate the number of African countries in the United Nations. Tversky and Kahneman found that judgments tended to be biased toward the value displayed on the wheel. Tversky and Kahneman explained this finding by assuming that agents anchored on an initial belief that the number of African countries in the United Nations is equal to the value on the wheel, then iteratively adjusted away from the anchor through a process of anchoring and adjustment.</p><p>That is a surprisingly irrational cognitive process, and subsequent authors rightly asked for evidence that a process of iterative anchoring and adjustment had in fact been employed. For two decades, all available process-tracing studies showed no evidence of a cognitive process of anchoring and adjustment in this and other early experiments <ref type="bibr" target="#b41">(Johnson and Schkade 1989;</ref><ref type="bibr" target="#b52">Lopes 1982)</ref>. More recently, evidence has emerged that a genuine process of anchoring and adjustment may be employed in a small number of examples, such as our initial example of estimating the year in which George Washington was first elected president <ref type="bibr" target="#b19">(Epley and Gilovich 2006;</ref><ref type="bibr" target="#b50">Lieder et al. 2018</ref>). However, it is widely agreed that genuine anchoring and adjustment is extremely rare; that anchoring and adjustment is not typically triggered by external manipulations such as spinning wheels; that anchors tend to be relevant and informative, and incorporated in a rational way; that the results of anchoring and adjustment are often highly reliable; and that few if any anchoring effects in the early literature are produced by genuine processes of anchoring and adjustment <ref type="bibr" target="#b50">(Lieder et al. 2018)</ref>.</p><p>As evidence for processes of anchoring and adjustment failed to materialize in the motivating examples, researchers broadened the concept of anchoring effects so that they were no longer conceptually tied to a process of anchoring and adjustment. This broadening led to some confusion over the definition of anchoring effects, as Kahneman himself remarks:</p><p>The terms anchor and anchoring effect have been used in the psychological literature to cover a bewildering array of diverse experimental manipulations and results . . . The proliferation of meanings is a serious hindrance to theoretical progress. <ref type="bibr" target="#b40">(Jacowitz and</ref><ref type="bibr">Kahneman 1995, p. 1161)</ref>.</p><p>Many theorists outside the heuristics and biases camp have taken the definitional vagueness of biases such as anchoring as a mark against attempts to posit them <ref type="bibr" target="#b28">(Gigerenzer 1996)</ref>. For my part, I have some sympathy for this line, but I am willing to ask what anchoring effects might mean.</p><p>Here is a sampling of recent definitions of anchoring effects:</p><p>An anchor is an arbitrary value that the subject is caused to consider before making a numerical estimate. An anchoring effect is demonstrated by showing that the estimates of groups shown different anchors tend to remain close to those anchors. <ref type="bibr" target="#b40">(Jacowitz and</ref><ref type="bibr">Kahneman 1995, p. 1161)</ref>.</p><p>The anchoring effect is the disproportionate influence on decision makers to make judgments that are biased toward an initially presented value. <ref type="bibr">(Furnham and Chu Boo 2011, p. 35</ref>).</p><p>An important feature of these definitions is that anchoring effects involve mis-use of information contained in the anchor: anchors must either be arbitrary (Jacowitz and</p><p>Kahneman 1995) and hence unsuitable for use in future inference, or else must exert disproportionate influence (Furnham and Chu Boo 2011) on future inference. It is widely known that we can also generate phenomena similar to anchoring effects, except that the anchors are informative and are used in appropriate ways. For example, manipulating the listing prices of properties changes what agents are willing to pay for them (Northcraft and Neale 1987). But that is not obviously irrational, since listing prices carry information about property values. 'Anchoring' in examples such as these might simply be another name for the process of learning from evidence. It is generally agreed that if there is a problem revealed by anchoring effects, it must be either that the anchors are irrelevant,</p><p>or else that they exert disproportionate influence beyond their informational relevance <ref type="bibr" target="#b25">(Furnham and Chu Boo 2011;</ref><ref type="bibr" target="#b40">Jacowitz and Kahneman 1995;</ref><ref type="bibr" target="#b50">Lieder et al. 2018)</ref>. This consensus will be important below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Anchoring in code generation</head><p>Code generation tasks involve generating code from prompts. Prompts may be partial programs, English descriptions of desired functionality, or combinations of these and other inputs. Two leading code generation models are OpenAI's Codex <ref type="bibr" target="#b9">(Chen et al. 2021)</ref> and Salesforce's CodeGen <ref type="bibr" target="#b53">(Nijkamp et al. 2023)</ref>.</p><p>The HumanEval dataset is often used to assess code generation <ref type="bibr" target="#b9">(Chen et al. 2021)</ref>.</p><p>HumanEval is composed of 164 programming problems. Each problem contains a threepart prompt: a function signature 'def function name', an English description of the desired functionality, and several input-output pairs describing correct function behavior.</p><p>Each problem is also accompanied by a canonical solution: a correct solution program generated by human programmers.</p><p>Erik <ref type="bibr" target="#b44">Jones and Jacob Steinhardt (2022)</ref> aim to find an anchoring effect in code generation by Codex and CodeGen. They do this by incorporating tempting, but incorrect solutions into 'anchor' strings, then prepending anchor strings to complete HumanEval prompts.</p><p>More concretely, Jones and Steinhardt construct anchor functions with three parts (Figure <ref type="figure" target="#fig_1">3</ref>). The first part is the function signature, copied from the HumanEval prompt.</p><p>The second part is the first n lines of the canonical solution, with n varied between 0 and 8 across prompts. The final part is a set of 'anchor lines' describing a tempting but incorrect partial solution. English description of the desired functionality, and example input-output pairs (Figure <ref type="figure" target="#fig_1">3</ref>). These are again followed by the first n lines of the canonical solution, with n fixed at its value in the anchor function.</p><p>Jones and Steinhardt test Codex and CodeGen across a variety of total prompts, varying the choice of anchor lines, the number n of canonical solution lines, and the original prompt from HumanEval. They find a significant decrease in model accuracy, as well as an increased tendency for solutions by Codex and CodeGen to incorporate anchor lines in part or full within the resulting outputs. Jones and Steinhardt treat this finding as an anchoring effect, in which "code models . . . adjust their output towards related solutions, when these solutions are included in the prompt" <ref type="bibr" target="#b44">(Jones and Steinhardt 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>The discussion in Section 5.1 suggests three challenges for Jones and Steinhardt's anchoring experiment. First, Jones and Steinhardt sometimes talk as though they have found processes of adjustment away from an anchor.<ref type="foot" target="#foot_3">foot_3</ref> However, no evidence for any process of anchoring and adjustment has been provided. We saw in Section 5.1 that this is important:</p><p>the last time that a heuristic process of anchoring and adjustment was posited to explain anchoring effects, it turned out that this postulate was almost always wrong. This led to a clear consensus within the field that anchoring and adjustment should not be postulated without direct process-tracing evidence, which Jones and Steinhardt have not provided.</p><p>This means that it is most appropriate to treat Jones and Steinhardt's finding within the broader category of anchoring effects.</p><p>Second, the anchors provided by Jones and Steinhardt are relevant, not irrelevant.</p><p>They are highly similar in content to the problem and constructed to be similar to correct solutions. This makes the anchors generally relevant to, and informative about, the problem at hand. As we have seen, most scholars concede that agents may rationally make use of relevant anchors, just as they may rationally make use of other relevant information. We may still criticize agents for over-use of relevant anchors, just as we may criticize them for over-use of any other item of evidence, but pressing this charge requires proving over-use, which Jones and Steinhardt do not attempt to do.</p><p>Third, even if the anchors provided by Jones and Steinhardt were not in fact relevant, there would nonetheless be a legitimate presupposition of relevance. This presupposition can be grounded in two ways. The first ground for a presupposition of relevance is due to model construction. Codex and CodeGen are designed to predict likely continuations of code strings, then generate novel code according to their predictions. It is an undeniable fact that most features of code snippets are more likely to be included in the continuation if they are included in the prompt than if they are not: for example, a program that begins with a for var loop or an instruction to print variables is more likely to continue with a for var loop or an instruction to print variables. In becoming more likely to include input features in output continuations, Codex and CodeGen do no more than what they were constructed to do: take the entire input string as relevant to determining the likely continuation.</p><p>A second way to generate a default presupposition of relevance draws on how Codex and CodeGen were trained. Both models were trained primarily on helpful and nonmisleading prompts. While the models may have been exposed to natural human errors, they have not been significantly exposed to programmers trying to manipulate them into including irrelevant code in their outputs. From this, any rational agent would learn that input is likely to be non-manipulative. Codex and CodeGen do not, and should not, treat inputs as likely to be manipulative unless they are trained on manipulative examples.</p><p>We could, of course, train versions of Codex and CodeGen that were designed to filter out manipulative prompts, but it is not obvious that this would be desirable unless we anticipate that many test prompts will be manipulative.</p><p>This discussion of a default presupposition of relevance is naturally situated within the paradigm of ecological rationality <ref type="bibr" target="#b70">(Todd and Gigerenzer 2012)</ref>. This paradigm stresses that the rationality of computational processes is environment-relative. Many processes return quick, accurate, and helpful responses in some environments, but slow, inaccurate, or unhelpful responses in others. As a result, the right question to ask about a process is not how it performs in all environments, but rather how it performs in the environments where it is proposed for use. Codex and CodeGen are designed to work well on non-manipulative prompts. They do not work well on manipulative prompts, but that is not what they were designed to do. Applying Codex and CodeGen for use in hostile environments where they were never intended for use proves no more than that Codex and CodeGen should not be used, and were never intended to be used in these environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Framing effects: Banishing Pangloss</head><p>Bounded rationality theorists are sometimes accused of taking the Panglossian view all seeming biases and irrationalities can be explained away as nothing of the kind. Daniel Kahneman once quipped, not entirely without justification, that some theorists see only two types of errors: "pardonable errors by subjects and unpardonable ones by psychologists" who misinterpret them (Kahneman 1981, p. 349). 6   No theorist should be a Panglossian. It is quite likely that large language models, like humans, sometimes reason in undesirable ways. When there is clear evidence of undesirable biases in reasoning, we should do what we can to improve the situation. In this section, I want to illustrate my anti-Panglossian commitments by looking at one area where problematic biases in reasoning by large language models do seem to have been identified.</p><p>Framing effects occur when irrelevant changes in the framing of a reasoning problem lead to substantive changes in the judgments that result from reasoning. Many authors allege framing effects in large-language models, and some of these findings may be more difficult to resist.<ref type="foot" target="#foot_5">foot_5</ref> </p><p>For example, Alaina Talboy and Elizabeth Fuller (2023) consider a classic presentation of <ref type="bibr">Tversky and Kahneman's (1981)</ref> Asian disease problem. This program presents a choice between certain and risky policies, manipulating whether the outcomes of each choice</p><p>Common instructions: Imagine that the U.S. is preparing for the outbreak of an unusual disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume that the exact scientific estimate of the consequences of the two programs are as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive frame Negative frame</head><p>If Program A is adopted, 200 people will be saved.</p><p>If Program B is adopted, there is a 1/3 probability that 600 people will be saved, and a 2/3 probability that no people will be saved.</p><p>Which of the two programs would you favor?</p><p>If Program A is adopted, 400 people will die.</p><p>If Program B is adopted, there is a 1/3 probability that nobody will die, and a 2/3 probability that 600 people will die.</p><p>Which of the two programs would you favor?</p><p>Table <ref type="table">2</ref>: Asian disease problem, as presented in <ref type="bibr" target="#b67">Talboy and Fuller (2023)</ref>.</p><p>are framed positively, in terms of lives saved, or negatively, in terms of those who will die. Table <ref type="table">2</ref> presents the prompts used, which are formed by joining a common set of instructions together with a positive or negative framing of the policies to be considered.</p><p>Talboy and Fuller test ChatGPT-3.5, GPT-4, and Google Bard on the Asian disease problem, finding humanlike patterns of preference change across framings. Like humans, the models opt for the safe option in the positive framing, but the risky option in the negative framing, showing risk-aversion in gains but risk-seeking in losses, even across what many would regard as equivalent problems. Extending this finding, Marcel Binz and Eric Schulz (2023) find human-like gain/loss framing effects in a number of classic problems: GPT-3 is loss-averse, risk-seeking in outcomes framed as losses, and riskavoidant in outcomes framed as gains.</p><p>Should these results be viewed as undesirable biases in need of correction? Certainly some framing effects might be defended. For example, rationalizing explanations have been offered in particular cases such as the Asian disease problem <ref type="bibr" target="#b17">(Dreisbach and Guevara 2019)</ref>. And in some cases, it may be helpful to question the experimental designs that lead us to allege framing effects <ref type="bibr" target="#b48">(Lejarraga and Hertwig 2021;</ref><ref type="bibr">Gigerenzer 2018)</ref>. But even those who have wanted to defend some framing effects have not typically thought that all framing effects can be explained away, or made desirable through such means (Berm údez 2020).</p><p>There may yet be some purposes for which we would like large language models to show framing effects. For example, this may enable us to use large language models as participants in laboratory studies to shed light on human reasoning <ref type="bibr" target="#b4">(Argyle et al. 2023;</ref><ref type="bibr" target="#b1">Aher et al. 2023;</ref><ref type="bibr" target="#b15">Dillion et al. 2023)</ref>. More generally, we should not exaggerate the prevalence or influence of framing effects (Demaree-Cotton 2016). But in many situations, there may be reasons to find framing effects undesirable. Good reasoning responds to relevant features of situations and ignores irrelevant features. Anything else risks inconsistency, as well as a decline in the quality of judgments that are formed based on irrelevant features.</p><p>Insofar as some framing effects are undesirable, we should take two types of measures to correct them. First, programmers should explore debiasing methods to reduce the vulnerability of future models to framing effects. And second, prompt engineers (Henrickson and Mero ño-Pe ñuela forthcoming) should explore ways to reduce the likelihood that irrelevant prompt changes will trigger framing effects. Together, these interventions may help to improve the performance of large language models in reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Two lessons</head><p>So far, we have discussed four types of biases alleged in large-language models: knowledge effects (Section 3), anchoring bias (Section 5), availability bias (Section 4), and framing effects (Section 6). At the beginning of this paper, I suggested that these discussions could be used to draw two lessons: a cautious optimism about model performance, and an anti-Panglossian, meliorist willingness to accept the existence of some problematic biases and work to correct them. In this section, I make the case for both lessons. Then in Section 8, I draw philosophical implications from this discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Cautious optimism</head><p>The cautious optimist accepts that the cognitive bias framing is useful and coherent. It makes sense to talk about large language models as showing, or failing to show, cognitive biases, and we should expect to learn something valuable about model performance by speaking in this way.</p><p>The cautious optimist reminds us of the lessons gleaned from over a half-century of discussions of bias in human cognition. In particular, she reminds us that many theorists believe that problematic biases are relatively rare, and that human cognition is often fairly rational <ref type="bibr" target="#b49">(Lieder and Griffiths 2020;</ref><ref type="bibr">Gigerenzer and Selten 2001;</ref><ref type="bibr" target="#b33">Gilovich and Griffin 2002)</ref>.</p><p>She reminds us that in the human case, many early bias accusations are now thought to depend on conceptual confusions (as in the distinction between objective and subjective availability), empirical problems (as in the difficulty of finding evidence for anchoring and adjustment), or on behavior that can be given rationalizing explanations (as in probabilistic approaches to knowledge effects).</p><p>The cautious optimist further reminds us that many biases in human cognition are thought to arise from tradeoffs that agents face in pursuing their goals, such as a biasvariance tradeoff in predictive error <ref type="bibr" target="#b26">(Geman et al. 1992;</ref><ref type="bibr" target="#b30">Gigerenzer and Brighton 2009)</ref> or an accuracy-coherence tradeoff in reasoning <ref type="bibr">(Thorstad forthcoming)</ref>. She suggests that these tradeoffs should make us suspicious of a tendency to deem biases as irrational without further examination of how they came about. Finally, the cautious optimist reminds us that while humans can often be induced to show biases in the laboratory, biases may be relatively less common in the environments where humans ordinarily reason <ref type="bibr" target="#b70">(Todd and Gigerenzer 2012)</ref>.</p><p>The cautious optimist suggests that many of these lessons may transfer well to the study of biases in large language models. We saw, for example, that knowledge effects (Section 3) might be treated as the desirable results of good probabilistic reasoning, rather than as the undesirable results of bad logical reasoning, and that this probabilistic reconstruction is in some ways stronger in the case of machine reasoning than it is for human reasoning. We also saw that some accusations of availability bias fail to distinguish between subjective and objective availability. When they do, what is revealed is an especially benign type of objective availability conjoined with an arguably inappropriate normative standard of ignoring learned information about categories in favor of a uniform prior (Section 5). Finally, we saw that accusations of anchoring bias need conceptual clarification in terms of a particular notion of anchoring effects distinct from anchoring and adjustment;</p><p>that the relevant concept of anchoring bias should be tied to a demonstration of the irrelevance of anchor information to the problem at hand; that no attempt has been made to demonstrate irrelevance; and that the anchor information is arguably both relevant, and justifiably presumed to be relevant, to the problem on which models were tested.</p><p>From this, the cautious optimist may draw two further lessons. The first is the importance of incorporating what is already known about human bias into discussions of cognitive bias by large language models. We saw that some leading bias accusations can be softened or dissolved by applying conceptual distinctions and empirical and normative challenges familiar from the human literature, and this gives us every reason to pay greater attention to the existing literature on human cognitive bias in future studies.</p><p>The second lesson is backward-looking: <ref type="bibr">Dasgupta and colleagues (2022)</ref> suggest that insofar as machines begin to show many of the same patterns of purportedly biased cognition as humans do, this may provide supporting evidence for the claim that those biases are features, rather than bugs, in human cognition. After all, it would be a surprising coincidence if cognitive systems with very different architectures than humans were to converge on exactly the same biases, and a natural explanation for this convergence in many cases will be that there is something cognitively valuable in the bias that theories of cognition should identify and fully appreciate. I discuss this lesson in more detail in Section 8.2.</p><p>On its own, cautious optimism paints a rosy picture of bias in large language models, and to a large extent this is the picture I would like to paint. But cautious optimism must be coupled with a second reaction: anti-Panglossian meliorism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Anti-Panglossian meliorism</head><p>Life is not all sun and roses. The anti-Panglossian meliorist reminds us that some biases, such as framing effects (Section 6) are likely to exist in large language models. While we may try to deny the existence of any particular bias, to rationalize it away, or to deny that the bias occurs often in natural environments, we should be open to the possibility that such objections will not always succeed, and may well take framing effects to be one case in which they currently fall short.</p><p>Here the anti-Panglossian meliorist agrees with the cautious optimist in accepting the usefulness of the cognitive bias framing in studying the performance of large language models. She demonstrates anti-dogmatism in taking some findings to reveal problematic biases in need of correction, and adopts a melioriative perspective which asks how our knowledge of model biases might be used to correct them and thus to improve model performance. Even the staunchest opponents of the heuristics and biases program at times show just such an anti-Panglossian meliorism, as in, for example, the use of natural frequencies to improve human probabilistic reasoning <ref type="bibr" target="#b31">(Gigerenzer and Hoffrage 1995)</ref>.</p><p>The anti-Panglossian meliorist suggests that a similar spirit should be applied to some cases of machine bias.</p><p>The overall message formed by combining cautious optimism with anti-Panglossian meliorism is the following. Cognitive bias provides a novel and useful way to assess the performance of large language models. The usefulness of this approach will be improved by incorporating what is already known about cognitive bias in the human case, and when we do, current findings should be understood to paint a broadly positive picture of model performance. Nevertheless, the bias paradigm shows its teeth in areas such as framing effects, and we demonstrate genuine commitment to the usefulness of the bias framing by acknowledging the existence of a problem in such cases, then using our knowledge of how the bias is produced to create subsequent models that will produce less-biased outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>The study of cognitive biases in language models has important descriptive and normative implications. In this concluding section, I survey two implications of cautious optimism about model biases, tempered by an appropriate dose of anti-Panglossian meliorism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Bias and representative data</head><p>Traditional conceptions of algorithmic bias have stressed the role of unrepresentative data in driving unfair and discriminatory model behavior <ref type="bibr" target="#b24">(Fazelpour and Danks 2021;</ref><ref type="bibr" target="#b42">Johnson 2020)</ref>. Models trained primarily on white, male, western, English-speaking individuals learn to best represent and respond to the needs of those individuals. This leads to significant cross-group differences in model performance in areas as diverse as facial recognition , sentencing recommendations and medical diagnosis <ref type="bibr" target="#b7">(Buolamwini and Gebru 2018;</ref><ref type="bibr" target="#b24">Fazelpour and Danks 2021)</ref>. While it is certainly not true to say that algorithmic biases should be blamed entirely on data, it is widely thought that unrepresentative data plays a leading role in driving algorithmic bias.</p><p>By contrast, to my knowledge no scholars have suggested that cognitive biases in language models emerge from unrepresentative samples of data. Certainly, nothing like this would be alleged in humans, since many cognitive biases replicate cross-culturally with sufficient frequency to cast doubt on the idea that those biases result primarily from knowledge specific to particular groups <ref type="bibr" target="#b64">(Stankov and Lee 2014)</ref>. If anything, cognitive biases might be reduced by exposure to biased samples of data. For example, there is good evidence that many, though far from all cognitive biases are less prevalent in individuals who score highly on standard tests of cognitive ability <ref type="bibr" target="#b65">(Stanovich 1999;</ref><ref type="bibr" target="#b66">Stanovich and West 2000)</ref>. This might suggest that one strategy for reducing cognitive bias in language models would be to preferentially expose models to reasoning by members who perform well on tests of cognitive ability. However, most of these tests show troubling correlations along dimensions of group membership <ref type="bibr" target="#b59">(Schmidt 1988)</ref>, so there may be tension between the types of data that would best reduce traditional algorithmic biases and those that would best reduce cognitive biases.</p><p>If this is right, then the need to combat unrepresentative data may be significantly greater if we are concerned with traditional conceptions of algorithmic bias than if we are concerned with cognitive bias. This means that credible evidence of widespread and undesirable cognitive biases in large language models might provide motivation for diminished focus on biases introduced by unrepresentative data. This would not be a pleasant result. By contrast, if I am right that existing evidence does not support widespread allegations of problematic cognitive biases in language models, then there will be limited impetus to reduce current focus on the role of unrepresentative data in producing harmful biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Vindicatory epistemology</head><p>What leads humans to exhibit cognitive biases? In any given case, there are at least two competing descriptive explanations which can be offered, with correspondingly different normative implications.</p><p>Dual process theorists suggest that human cognition is divided into two distinctive types of processes: fast, automatic, associative and biased Type 1 processes, and slow, controlled, rule-based, normative Type 2 processes <ref type="bibr" target="#b23">(Evans and Stanovich 2013)</ref>. On this view, bias results from the application of Type 1 processes. Biases produced in this way are likely irrational and should be corrected by application of Type 2 processes.</p><p>Vindicatory epistemologists (Thorstad forthcoming b) suggest that many biases are the result of rationalizing factors such as task demands, cognitive bounds, and the structure of the agent's environment. For example, anchoring bias may result from diminishing returns to costly processes of iterative belief adjustment <ref type="bibr" target="#b50">(Lieder et al. 2018</ref>), and we saw in Section 3 that Wason selection task behavior may result from probabilistic approaches to conditional reasoning. The vindicatory approach offers a wide array of descriptive explanations for the emergence of biases, typically resisting the dual process approach and the corresponding inference to the irrationality of observed cognitive biases (Dorst forthcoming; Icard 2018; Thorstad forthcoming b).</p><p>Dasgupta and colleagues (2022) conclude their discussion of knowledge effects with an interesting observation: the emergence of cognitive biases in large language models may provide some evidence for the vindicatory explanation of how those biases emerge.</p><p>On the one hand, it is very difficult for dual process theorists to explain why language models should show cognitive biases, since there is no clear distinction between Type 1</p><p>and Type 2 processes in large language models. On the other hand, the emergence of similar biases in agents with very different cognitive architectures lends support to the vindicatory theorist's contention that biases emerge, not because of peculiar and irrational features of any particular cognitive architecture, but rather because of rationalizing factors such as task demands that persist across architectures. Otherwise, it would be a great mystery why similar biases should emerge across radically different agents.</p><p>This suggests that research into cognitive biases in large language models may provide an important avenue of support for vindicatory epistemology. However, this approach leaves open at least three classes of questions for future research. First, vindicatory theorists need to rule out competing explanations for the emergence of cognitive biases, such as deliberate mimicry of observed patterns of human reasoning. Second, vindicatory theorists should hope that biases are relatively stable across improvements to language models: if, as some have suggested <ref type="bibr" target="#b34">(Hagendorff et al. 2023;</ref><ref type="bibr" target="#b38">Horton 2023)</ref>, biases are reduced in more sophisticated models, this finding might lend some support to the idea that biases result from unsophisticated reasoning processes. Finally, humans exhibit not only coarse-grained dispositions towards cognitive biases, but also fine-grained patterns of bias across prompts and tasks. The findings most friendly to vindicatory theorists would be findings in which not only coarse-grained facts, such as the presence or absence of particular biases, but also fine-grained facts about the pattern and amount of bias in particular tasks were to be similar across human agents and language models. While these findings would not settle debates about the rationality of biases in human cognition,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Wason selection task performance (logical criterion) by Chinchilla across rule types, from Dasgupta et al. (2022).</figDesc><graphic coords="7,180.01,72.00,249.00,178.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Construction of anchor function and full prompt, from Jones and Steinhardt (2022).</figDesc><graphic coords="17,104.50,72.00,403.00,66.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>Jones and Steinhardt consider two types of anchors. Print-var anchors instruct the program to print, rather than return, a given value: for var in [var1, var 2]: print(var) Add-var anchor lines instruct programs to sum two values: tmp = str(var1) + str(var2) return tmp Complete anchor functions consist of a function signature, the first n lines of the canonical solution, and the chosen anchor lines. Total prompts are constructed by prepending anchor lines to the original HumanEval prompt, consisting of a function signature, an</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Availability bias in drug-drug interaction by size of training set,<ref type="bibr" target="#b51">Lin and Ng (2023)</ref>. action mechanism; effect for a description of the effect itself; advice for recommendations about how to respond to drug-drug interactions; int for nonspecific descriptions of interactions, and negative for non-interactions. The vast majority (85.2%) of interactions in the DDI dataset are negative, and models trained on the DDI dataset understandably learn to reflect this fact.</figDesc><table><row><cell>Training Examples</cell><cell>10</cell><cell cols="2">100 1,000 10,000 25,296</cell></row><row><cell>Availability Bias Towards Negative Category (%)</cell><cell cols="2">26.3 77.7 39.7 47.0</cell><cell>52.0</cell></row></table><note><p>Ruixi Lin and Hwee Tou Ng (2023) train GPT-3 on the DDI dataset. Lin and Ng then test the model on 'content-free' descriptions generated from the DDI dataset by replacing all medical terms with the dummy descriptor 'N/A'. Lin and Ng propose that because the model has no direct knowledge of the dummy class, the model should classify dummy sentences according to a uniform probability description. That is, it should be 20% likely to assign dummy descriptions to each interaction type: mechanism, effect, advice, int, and negative. Lin and Ng propose that any deviation from the uniform classification of dummy sentences should be treated as a form of availability bias, in which model judgments are skewed by the availability of interaction types in the training data. For each interaction type, Lin and Ng define the availability bias score of that interaction type to be the absolute difference between the percentage of test items classified under this type and the 20% classification rate expected under a uniform model. Under this definition, Lin and Ng find a strong availability bias, increasing in the number of descriptions used to train the model (Table</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The Shannon entropy of credence function P is Σ X={M I ,M D } P(X)log</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>(P(X)). This definition enforces Oaksford and Chater's assumption that the agent has beliefs about the independence hypothesis M I and dependence hypothesis M D and aims to reduce her uncertainty about these hypotheses.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Tony Zhao and colleagues (2021) call this majority-label bias.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>For example: "Using anchoring as inspiration, we hypothesize that code generation models may adjust their output towards related solutions" and "We additionally find that elements of anchor function often appear in both models' outputs, suggesting that code generation models adjust their solutions towards related solutions"<ref type="bibr" target="#b44">(Jones and Steinhardt 2022)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Somewhat less charitably, Keith Stanovich and Richard West contrast the 'Panglossian' view that existing experiments fail to demonstrate widespread irrationality with the 'meliorist' view that irrationalities are genuine and we should work to make them better<ref type="bibr" target="#b66">(Stanovich and West 2000)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>However, there are some negative findings. For example, John Horton (2023) finds that framing is largely ineffective as a manipulation inKahneman and colleagues' (1986)  classic snow shovel experiment when run on GPT-3.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>they would represent an important step forward in our understanding of how biases come about, thought by many sides to have significant bearing on questions about the rationality of cognitive biases.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The logic of conditionals: An application of probability to deductive logic</title>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese Library</title>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using large language models to simulate multiple humans and replicate human subject studies</title>
		<author>
			<persName><surname>Aher</surname></persName>
		</author>
		<author>
			<persName><surname>Gati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosa</forename><surname>Arriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Conference on Machine Learning</title>
		<meeting>the 40th Annual Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="337" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uniting the tribes of fluency to form a metacognitive nation</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Alter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Oppenheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Review</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="219" to="235" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The adaptive character of thought</title>
		<author>
			<persName><forename type="first">John</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One out of many: Using language models to simulate human samples</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Argyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Busby</surname></persName>
		</author>
		<author>
			<persName><surname>Fulda</surname></persName>
		</author>
		<author>
			<persName><surname>Nancy</surname></persName>
		</author>
		<author>
			<persName><surname>Gubler</surname></persName>
		</author>
		<author>
			<persName><surname>Joshua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Rytting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wingate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="337" to="351" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">José</forename><surname>Berm Údez</surname></persName>
		</author>
		<title level="m">Frame it again</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using cognitive psychology to understand GPT-3</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">2218523120</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic models of cognition: Conceptual foundations</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="287" to="291" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><surname>Jerry</surname></persName>
		</author>
		<author>
			<persName><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><surname>Heewoo</surname></persName>
		</author>
		<author>
			<persName><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><surname>Qiming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><surname>Ponde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><surname>Harri</surname></persName>
		</author>
		<author>
			<persName><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><surname>Yuri</surname></persName>
		</author>
		<author>
			<persName><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><surname>Raul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><surname>Heidy</surname></persName>
		</author>
		<author>
			<persName><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Girish</surname></persName>
		</author>
		<author>
			<persName><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><surname>Pamela</surname></persName>
		</author>
		<author>
			<persName><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><surname>Nick</surname></persName>
		</author>
		<author>
			<persName><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><surname>Mikhail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Lukasz</surname></persName>
		</author>
		<author>
			<persName><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><surname>Clemens</surname></persName>
		</author>
		<author>
			<persName><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><surname>Philippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><surname>Petroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><surname>Matthias</surname></persName>
		</author>
		<author>
			<persName><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><surname>Fotios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbert</forename><forename type="middle">-</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hebgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><surname>Nikolas</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><surname>Igor</surname></persName>
		</author>
		<author>
			<persName><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><surname>Suchir</surname></persName>
		</author>
		<author>
			<persName><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><surname>Shantanu</surname></persName>
		</author>
		<author>
			<persName><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><surname>William</surname></persName>
		</author>
		<author>
			<persName><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><surname>Vedant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><surname>Miles</surname></persName>
		</author>
		<author>
			<persName><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><surname>Mira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><surname>Bob</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><surname>Dario</surname></persName>
		</author>
		<author>
			<persName><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><surname>Sam</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno>arXiv 2107.03374</idno>
		<title level="m">Evaluating Large Language Models Trained on Code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pragmatic reasoning schemas</title>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Holyoak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="391" to="416" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The logic of social exchange: Has natural selection shaped how humans reason? Studies with the Wason selection task</title>
		<author>
			<persName><forename type="first">Leda</forename><surname>Cosmides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="187" to="276" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transparency in complex computational systems</title>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Creel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="568" to="589" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Language models show human-like content effects on reasoning</title>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">C Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><surname>Dharshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<idno>arXiv, 2207.07051</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Do framing effects make moral intuitions unreliable?</title>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Demaree-Cotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Psychology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can AI language models replace human participants?</title>
		<author>
			<persName><forename type="first">Danica</forename><surname>Dillion</surname></persName>
		</author>
		<author>
			<persName><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><surname>Niket</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuling</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="597" to="600" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rational polarization</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Dorst</surname></persName>
		</author>
		<author>
			<persName><surname>Forthcoming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Review forthcoming</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Asian disease problem and the ethical implications of prospect theory</title>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Dreisbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Guevara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">No ûs</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="613" to="638" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">New paradigm psychology of reasoning: An introduction to the special issue edited by Elqayam, Bonnefon, and Over</title>
		<author>
			<persName><forename type="first">Shira</forename><surname>Elqayam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Over</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thinking and Reasoning</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The anchoring-and-adjustment heuristic: Why the adjustments are insufficient</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Epley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="311" to="318" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The probabilistic turn in semantics and pragmatics</title>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="101" to="121" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the conflict between logic and belief in syllogistic reasoning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Barston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditionals and conditional probability</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Handley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Over</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="321" to="335" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dual-process theories of higher cognition: Advancing the debate</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Stanovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="223" to="241" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Algorithmic bias: Senses, sources, solutions</title>
		<author>
			<persName><forename type="first">Sina</forename><surname>Fazelpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Danks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy Compass</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">12760</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A literature review of the anchoring effect</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Furnham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chu</forename><surname>Boo</surname></persName>
		</author>
		<author>
			<persName><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Socio-Economics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="35" to="42" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural networks and the bias/variance dilemma</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elie</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">René</forename><surname>Doursat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probabilistic machine learning and artificial intelligence</title>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="452" to="459" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On narrow norms and vague heuristics: A reply to Kahneman and Tversky</title>
		<author>
			<persName><forename type="first">Gerd</forename><surname>Gigerenzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="592" to="596" />
			<date type="published" when="1986">1996. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The bias bias in behavioral economics</title>
	</analytic>
	<monogr>
		<title level="j">Review of Behavioral Economics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="303" to="336" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Homo heuristicus: Why biased minds make better inferences</title>
		<author>
			<persName><forename type="first">Gerd</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Brighton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="107" to="143" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How to improve Bayesian reasoning without instruction: Frequency formats</title>
		<author>
			<persName><forename type="first">Gerd</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Hoffrage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="684" to="704" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m">Bounded rationality: The adaptive toolbox</title>
		<editor>
			<persName><forename type="first">Gerd</forename><surname>Gigerenzer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Reinhard</forename><surname>Selten</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Heuristics and biases: Then and now</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Griffin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Heuristics and biases: The psychology of intuitive judgment</title>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Gilovich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dale</forename><surname>Griffin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT</title>
		<author>
			<persName><forename type="first">Thilo</forename><surname>Hagendorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Fabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Kosinski</surname></persName>
		</author>
		<idno type="DOI">10.1038/s43588-023-00527-x</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Computational Science</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On statistical criteria of algorithmic fairness</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hedden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy and Public Affairs</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="209" to="231" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Prompting meaning: A hermeneutic approach to optimising prompt engineering with ChatGPT</title>
		<author>
			<persName><surname>Henrickson</surname></persName>
		</author>
		<imprint>
			<publisher>AI and Society forthcoming</publisher>
		</imprint>
	</monogr>
	<note>Leah and Mero ño-Pe ñuela, Albert. forthcoming</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Training Compute-Optimal Large Language Models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><surname>Elena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Las</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><surname>Anne</surname></persName>
		</author>
		<author>
			<persName><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><surname>George</surname></persName>
		</author>
		<author>
			<persName><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><surname>Bogdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><surname>Karen</surname></persName>
		</author>
		<author>
			<persName><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><surname>Erich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<idno>arXiv 2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large language models as simulated economic agents: What can we learn from homo silicus?</title>
		<author>
			<persName><forename type="first">John</forename><surname>Horton</surname></persName>
		</author>
		<ptr target="https://www.nber.org/papers/w31122" />
	</analytic>
	<monogr>
		<title level="j">National Bureau of Economic Research Working Paper</title>
		<imprint>
			<biblScope unit="volume">31122</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bayes, bounds, and rational analysis</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="79" to="101" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Measures of anchoring in estimation tasks</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Jacowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1161" to="1166" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bias in utility assessments: Further evidence and explanations</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schkade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="406" to="424" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The structure of bias</title>
		<author>
			<persName><forename type="first">Gabrielle</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1193" to="1236" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Are algorithms value-free? Feminist theoretical virtues in machine learning</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Moral Philosophy forthcoming</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Capturing failures of large language models via human cognitive biases</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Who shall be the arbiter of our intuitions?</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="339" to="340" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fairness as a constraint on profit seeking: Entitlements in the market</title>
		<author>
			<persName><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Knetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Thaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="728" to="741" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Bias: A philosophical study</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How experimental methods shaped views on human competence and rationality</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Lejarraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Hertwig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="535" to="564" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</title>
		<author>
			<persName><forename type="first">Falk</forename><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The anchoring bias reflects rational use of cognitive resources</title>
		<author>
			<persName><surname>Lieder</surname></persName>
		</author>
		<author>
			<persName><surname>Falk</surname></persName>
		</author>
		<author>
			<persName><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Huys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin and Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="322" to="349" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mind the biases: Quantifying cognitive biases in language model prompting</title>
		<author>
			<persName><forename type="first">Ruixi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Tou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="5269" to="5281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Toward a procedural theory of judgment</title>
		<author>
			<persName><forename type="first">Lola</forename><surname>Lopes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Office of Naval Research Final Report</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><surname>Hiroaki</surname></persName>
		</author>
		<author>
			<persName><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><surname>Lifu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Huan</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Yingbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno>arXiv 2203.13474</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Experts, amateurs, and real estate: an anchoring-and-adjustment perspective on property pricing decisions</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Northcraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neale</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="84" to="97" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A rational analysis of the selection task as optimal data selection</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Oaksford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="608" to="631" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m">Bayesian rationality: The probabilistic approach to human reasoning</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The philosophy of metacognition</title>
		<author>
			<persName><forename type="first">Joëlle</forename><surname>Proust</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The psychology of proof: Deductive reasoning in human thinking</title>
		<author>
			<persName><forename type="first">Lance</forename><surname>Rips</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The problem of group differences in abiltiy test scores in employment selection</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vocational Behavior</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="272" to="292" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Maximizing versus satisficing: Happiness is a matter of choice</title>
		<author>
			<persName><forename type="first">Barry</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Monterosso</surname></persName>
		</author>
		<author>
			<persName><surname>John</surname></persName>
		</author>
		<author>
			<persName><surname>Lyubomirsky</surname></persName>
		</author>
		<author>
			<persName><surname>Sonja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><forename type="middle">R</forename><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="1178" to="1197" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">SemEval-2013 Task 9 : Extraction of Drug-Drug Interactions from Biomedical Texts (DDIExtraction 2013)</title>
		<author>
			<persName><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName><surname>Isabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paloma</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">María</forename><surname>Herrero-Zazo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013. SemEval 2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="341" to="350" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Probing the psychology of AI models</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Shiffrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">2300963120</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Overconfidence across world regions</title>
		<author>
			<persName><forename type="first">Lazar</forename><surname>Stankov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cross-Cultural Psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="821" to="837" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Who is rational?: Studies of individual differences in reasoning</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Stanovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Individual differences in reasoning: Implications for the rationality debate?</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Stanovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="645" to="665" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Challenging the appearance of machine intelligence: Cognitive bias in LLMs and Best Practices for Adoption</title>
		<author>
			<persName><forename type="first">Alaina</forename><forename type="middle">N</forename><surname>Talboy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Fuller</surname></persName>
		</author>
		<idno>arXiv 2304.01358</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The accuracy-coherence tradeoff in cognition</title>
		<author>
			<persName><forename type="first">David</forename><surname>Thorstad</surname></persName>
		</author>
		<author>
			<persName><surname>Forthcoming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal for the Philosophy of Science forthcoming</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">forthcoming b. Inquiry under bounds</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerd</forename><surname>Gigerenzer</surname></persName>
		</author>
		<title level="m">Ecological rationality: Intelligence in the world</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Availability: A heuristic for judging frequency and probability</title>
		<author>
			<persName><forename type="first">Amos</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="207" to="232" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Judgment under uncertainty: Heuristics and biases</title>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The framing of decisions and the psychology of choice</title>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="page" from="453" to="458" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The right to explanation</title>
		<author>
			<persName><forename type="first">Kate</forename><surname>Vredenburgh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Political Philosophy</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="209" to="229" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Reasoning about a rule</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">C</forename><surname>Wason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="281" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deep learning for drug-drug interaction extraction from the literature: a review</title>
		<author>
			<persName><forename type="first">Tianlin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><surname>Jiaxu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1609" to="1627" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Sameer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
