<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causality extraction from medical text using Large Language Models (LLMs)</title>
				<funder ref="#_HS7uKVJ">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-07-13">13 Jul 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Seethalakshmi</forename><surname>Gopalakrishnan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luciana</forename><surname>Garbayo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">WLODEK ZADROZNY</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causality extraction from medical text using Large Language Models (LLMs)</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07-13">13 Jul 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">2131C43A19C54CED0C2DB73FC3608D54</idno>
					<idno type="arXiv">arXiv:2407.10020v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts:</term>
					<term>Do Not Use This Code → Generate the Correct Terms for Your Paper</term>
					<term>Generate the Correct Terms for Your Paper</term>
					<term>Generate the Correct Terms for Your Paper</term>
					<term>Generate the Correct Terms for Your Paper Causality extraction, Large Language Models, GPT-4, LLAMA2</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study explores the potential of natural language models, including large language models, to extract causal relations from medical texts, specifically from Clinical Practice Guidelines (CPGs). The outcomes causality extraction from Clinical Practice Guidelines for gestational diabetes are presented, marking a first in the field. We report on a set of experiments using variants of BERT (BioBERT, DistilBERT, and BERT) and using Large Language Models (LLMs), namely GPT-4 and LLAMA2. Our experiments show that BioBERT performed better than other models, including the Large Language Models, with an average F1-score of 0.72. GPT-4 and LLAMA2 results show similar performance but less consistency. We also release the code and an annotated a corpus of causal statements within the Clinical Practice Guidelines for gestational diabetes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the emergence of the LLMs, BERT continues to be one of the top-performing models for various applications, including causality extraction <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b42">[43]</ref>. This study aims to extract causal relations ('causalities') from the medical text in Clinical Practice Guidelines.</p><p>In medicine, the best explanations are causal (and imply the opportunity for better recommendations?). Causal reasoning is (therefore) used (not only in producing and (but in) evaluating the impact of medical guidelines (over time in learning system loops). The mechanistic model of (explanation of biological phenomena) is preferred in biomedicine (even if it is probabilistic). Automated analysis is necessary (for various applications like comparing differences in guidelines, diagnostic support, etc.) since there are currently over 37,000 of medical guidelines indexed on PubMed as "practice guidelines" and two orders of magnitude of articles that are used to produce the guidelines. Most of them use causal statements. The main contributions of this study are</p><p>• An entirely new type of public dataset of cause/effect relationships for Clinical Practice Guidelines. For medical text, there are only a very few causality extraction datasets available <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b45">[46]</ref>, but none of them focus on Clinical Practice Guidelines (CPGs).</p><p>• A performance evaluation of several known Large Language Models (LLMs) on the corpus of CPGs for causality extraction task. The results indicate that the performance of GPT-4 does not increase with an increase in the prompt size beyond 10. The LLAMA2 performance does not improve with the increase in the number of epochs.</p><p>From the experiments and evaluation we conclude that variants of BERT might still be preferred for this task, given the ease of fine-tuning and consistent performance. With BERT, we obtained an average F1 score of 72%, whereas GPT-4</p><p>gave an average F1 score of 60%. LLAMA2 shows promise, in that an average F1 score of 76% was obtained on subset for which it made predictions; LLAMA2 did not generate predictions for 20-35+% of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Causality extraction is the task of automatically extracting the cause/effect relationships from the text. In this section, we briefly discuss studies related to causality extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Work related to automatic information extraction from Clinical Practice Guidelines (CPG)</head><p>This section summarizes the prior work related to information extraction on the Clinical Practice Guidelines. Extracting clinical findings from notes of outpatient progress was early done realized by <ref type="bibr" target="#b13">[14]</ref>. Fifteen years later <ref type="bibr" target="#b48">[49]</ref> targeted the automated extraction of diagnosis and treatment procedures from clinical guidelines. A similar work <ref type="bibr" target="#b28">[29]</ref> introduced a method for automatically collecting useful information using rules rooted in both syntactic and semantic information.</p><p>A pattern-based approach was further used by <ref type="bibr" target="#b8">[9]</ref>, which contrasted a manually developed ontology for CPG eligibility criteria with a top-level ontology stemming from a semantic pattern-based approach. A more recent work <ref type="bibr" target="#b14">[15]</ref> introduced an innovative system that blends together the methodologies of Natural Language Processing (NLP) and Fuzzy Logic. A supervised machine learning methodology was used by another similar work <ref type="bibr" target="#b19">[20]</ref> to extract and categorize Conflicts Of Interest (COIs) from disclosure statements indexed in PubMed.</p><p>Recently, Large Language Models (LLMs) have (also) been employed for a variety of NLP tasks, including those involving information extraction. LLMs can be fine-tuned to cater to a specific dataset, or a prompt-based approach can be utilized. An illustrative study <ref type="bibr" target="#b56">[57]</ref> measures the efficiency of the few-shot learning performance of GPT-3 in tasks related to text classification and information extraction. A recent survey article <ref type="bibr" target="#b30">[31]</ref> provides a summary of the methods and solutions employed for information extraction. It also highlights the challenges encountered when extracting information from medical documents (such as, ambiguities when the named entity belongs to more than one class, phrase boundary detection, name variations, and others).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recent work on causality extraction from non-medical text</head><p>The study by <ref type="bibr" target="#b33">[34]</ref> directly extracts cause and effect from text without separately extracting candidate pairs and their relations. A work on event extraction <ref type="bibr" target="#b36">[37]</ref> focuses on identifying the causal relationship between pairs of event mentions, also known as 'Event Causality Identification' (ECI). Balashankar et al. <ref type="bibr" target="#b2">[3]</ref> propose an event extraction (modality) that seeks to uncover the hidden relationships between events mentioned in news streams by creating a Predictive Causal Graph (PCG). Prompt tuning has been proposed to bridge the gap between pre-training and fine-tuning on many of the mainstream NLP tasks like text classification <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b55">56]</ref>, information extraction <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> etc. In <ref type="bibr" target="#b34">[35]</ref>, Knowledge Enhanced Prompt Tuning (KEPT) employs external knowledge sourced from knowledge bases (KBs) to fine-tune pre-trained language models through the design of an attention mechanism.</p><p>A recent article <ref type="bibr" target="#b5">[6]</ref> describes the use of ChatGPT to extract cause/effect relationships from text on three datasets: (1)</p><p>Choice of Plausible Alternatives (COPA) <ref type="bibr" target="#b18">[19]</ref>, which is a collection of premises, along with two questions related to each premise, that requires causal reasoning in order to solve the inference; (2) e-CARE <ref type="bibr" target="#b12">[13]</ref> which is an explainable causal reasoning dataset with cause, effect and two possible explanations; (3) Headline Cause <ref type="bibr" target="#b21">[22]</ref> dataset, which aims to identify the implicit causal relations between pair of text. On COPA, ChatGPT in-context learning got a 97% accuracy (performance); on the eCARE dataset, a 79.6% accuracy was obtained using prompt engineering, and 72.7% accuracy was recorded for on Headline Cause.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Causality extraction from the medical text</head><p>In 2013 the task of automatic detection conditional statements in medical guidelines was first introduced <ref type="bibr" target="#b52">[53]</ref>. The article used a rule-based approach, focusing on presence of connectives such as "if", and a collection of word-based syntactic patterns. Subsequent works on detecting condition action statements from CPGs, <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b22">[23]</ref>, apply supervised machine learning techniques to classify sentences according to whether they express conditions and actions. Another study <ref type="bibr" target="#b27">[28]</ref> used heuristic patterns to identify recommendation statements in Clinical Practice Guidelines (CPG). A review article <ref type="bibr" target="#b16">[17]</ref> documents the existing methods and tools for clinical concept extraction. The summarization of biomedical literature is addressed in <ref type="bibr" target="#b54">[55]</ref> using pre-trained language models. A more recent study <ref type="bibr" target="#b49">[50]</ref> explores the use of ChatGPT for clinical text mining, specifically for extracting structured data from unstructured healthcare texts and focusing on biological named entity recognition and relation extraction by identifying and extracting medical entities from text related to disease and drug, symptoms and treatment, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA</head><p>We annotated seven documents of gestational diabetes (clinical practice) guidelines from various societies (and medical entities) like(such as) the American Diabetes Association (ADA) ( <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b37">[38]</ref>), US Preventive Services Task Force (USPSTF) ( <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b43">[44]</ref>), American College of Obstetrics &amp; Gynecology (ACOG) <ref type="bibr" target="#b0">[1]</ref>, American Academy of Family Physician (AAFP) <ref type="bibr" target="#b40">[41]</ref>, and Endocrine Society <ref type="bibr" target="#b3">[4]</ref>.</p><p>The decision to annotate gestational diabetes clinical practice guidelines was based on the opportunity to explore causality inference in the future with situated learning models with prediction (team member Dr. Garbayo worked on safety and quality database development on maternal and child in maternities, resulting in the creation of the largest maternity database in Latin America <ref type="bibr" target="#b31">[32]</ref>.</p><p>Two annotators were recruited. Given a (medical) text document, their task was to read the document and mark the cause, effect, condition, action, modal, and degree of influence with tags. The cause was marked as C, effect as E, condition as CO, and action as A. The phrases containing any of these causal phrases should be differentiated; for example, the beginning of a cause phrase will be marked as &lt;C&gt; and the end as &lt;/C&gt;. For example:</p><p>Example 3.1. &lt;C&gt;Pregnant persons with gestational diabetes&lt;/C&gt; are at &lt;E&gt;increased risk for maternal and fetal complications&lt;/E&gt; and may benefit from &lt;A&gt;early identification and treatment&lt;/A&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inter-annotator agreement for the medical data</head><p>Due to the intricacy of causality extraction, which involves annotators labeling varying text spans as "cause, " "effect, "</p><p>and so on, computing agreement between two annotators can be challenging as it requires comparing two spans of texts. Traditional methods of inter-annotator agreement, such as the Kappa statistic <ref type="bibr" target="#b15">[16]</ref>, are inadequate due to their need for classifications to fit into mutually exclusive and discrete categories. Therefore, we decided to assess agreement using both exact match and relaxed match criteria. The F-measure is used for the exact match <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b50">51]</ref> between the labels. In the case of the relaxed match, the average distance between phrases is computed. Initially, the annotated phrases, their corresponding labels, and the full sentence they are derived from are extracted from the entire annotated document. These annotations, originating from both annotators, are then compared and amalgamated based on the sentence. The resulting merged table thus features the sentence, the extracted phrase, and the labels as marked by Annotator 1 and Annotator 2. In total, 514 matching phrases have been identified. An overall agreement computed as a Jaccard similarity of 0.66 was obtained. Details of the inter-annotator agreement computation are given below.</p><p>From the merged data table, the inter-annotator agreement was computed. This is done by computing the match between the annotations as follows.</p><p>• Relaxed match -Both annotator's phrases overlap with each other but are not necessarily an exact match.</p><p>• Exact match -Both annotator's phrases exactly match. Levenshtein distance Jaccard distance Cause 0.22 0.27 Condition 0.34 0.21 Effect 0.37 0.31 Action 0.87 0.48 Table <ref type="table">1</ref>. Relaxed match between the annotated phrases. Levenshtein distance is the minimum number of edits required to transform one phrase to another, whereas Jaccard distance is the amount of non-overlap between phrases. The lower the distance, the agreement is higher. The distance is higher for action. In most of the cases where there is a mismatch, the length of the phrase by both the annotators was different. To execute the relaxed match, we employed the Levenshtein distance <ref type="bibr" target="#b39">[40]</ref> and the Jaccard distance <ref type="bibr" target="#b44">[45]</ref>. The Levenshtein distance quantifies the difference between two string sequences, indicating the minimum single-character edits required to transform one word into another. Jaccard similarity computes the degree of relatedness between two finite samples by dividing the intersection's size by the size of the sample sets' union. The Jaccard distance is subsequently calculated by subtracting the Jaccard similarity from 1. The Python library Levenshtein<ref type="foot" target="#foot_0">foot_0</ref> is used in computing the Levenshtein distance. The Jaccard index was computed using the Python library textdistance<ref type="foot" target="#foot_1">foot_1</ref> . The Levenshtein distance and the Jaccard distance between the annotators are summarized in Table <ref type="table">1</ref> From Table <ref type="table">1</ref>, we can understand that there is an average Levenshtein distance of 0.41 and an average Jaccard distance of 0.34. In most cases, both annotators annotated the same sentence with the same labels, but the length of the phrase was different. The exact match between the phrases is computed by finding the exact string match between phrases 1 and 2. Out of the 514 phrases, 112 phrases are exact matches. The match between the labels for the same phrase by both annotators is also computed with an average F1 score of 0.78. The match between the labels for each subcategory is given in Table <ref type="table" target="#tab_1">2</ref> 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2 Data preparation and preprocessing</head><p>Seven documents on gestational diabetes guidelines provided by different societies are downloaded as PDF documents.</p><p>The PDFs are converted into a document format, and the documents are given to the annotators for annotating them manually. The annotators used tags to annotate the documents.</p><p>After annotating them, the NLTK sentence tokenizer is used to extract sentences from all the documents. The sentences from all the documents are appended together and converted into a data frame. Regular expressions are used to extract the causal sentence. If any of the sentences contain a tag &lt;&gt;, it will be extracted as a causal sentence. Again regular expressions are used to extract the phrases of cause, effect, action, signal, and condition from the sentences. The extracted phrases are used for computing inter-annotator agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Causality extraction using BERT</head><p>Given the good performance of DistilBERT with organizational data <ref type="bibr" target="#b17">[18]</ref>, this model was also applied to the medical data. Considering the limited sample size in medical data, we attempted to improve the learning process by increasing the number of epochs. This approach allows for more refined fine-tuning of the model.</p><p>In order to decide on the correct number of epochs and to avoid overfitting, we tried running the model for 100 epochs and plotted the validation loss and the training loss. The graph showing the train and validation loss for our highest performing model, BioBERT, is given in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>From the graph, we can understand that with the increase in the number of epochs, the training loss is constantly increasing and approaching 0. The validation loss decreases till 18 epochs and then starts to increase. Based on this, we fine-tuned DistilBERT for 18 epochs, BERT(BERT-base-uncased) for 20 epochs, and BioBERT for 16 epochs.</p><p>The data is split into train and test. DistilBERT for token classification is fine-tuned on the training data for 18 epochs.</p><p>On the test data, the model obtained an average F1-score of 0.57. Similarly, we fine-tuned BioBERT for 16 epochs and BERT for 20 epochs. Out of these three models, BioBERT <ref type="bibr" target="#b32">[33]</ref> gave us an average higher F1-score. BioBERT gave an average F1 score of 0.61, and BERT gave an average F1 score of 0.60. The detailed results of fine-tuning BioBERT on the test data are given in Table <ref type="table" target="#tab_3">3</ref>; and, for comparison, the summary of the results of using variants of BERT for causality extraction task is given in</p><p>Table 4 Precision Recall F1-score Support E 0.82 0.75 0.78 696 C 0.62 0.69 0.65 411 CO 0.80 0.63 0.71 717 A 0.65 0.85 0.73 838 Macro average 0.72 0.73 0.72 2662 Table 4. Summary of the results of causality extraction on medical text using the Pre-trained Language Model (BERT) and its variants. The gestational diabetes data is split into train and test data. All the models are fine-tuned on train data and tested on test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Observations on using GPT-4 for causality extraction from medical guidelines</head><p>Generative Pre-trained Transformer 4 (GPT-4) <ref type="bibr" target="#b41">[42]</ref> outperforms most of the state-of-the-art performing models on the traditional NLP benchmark datasets. In this section, we discuss our results of prompting GPT-4-0314 with a with context window of 8,192, for the causality extraction task. We explored various prompt sizes (zero, four, six, eight, ten-shot, and twenty-shot prompting).</p><p>As an initial step, we tried the sentence with token-level labels for each word in the sentence as prompt examples. For the test data, the model is expected to predict a label for each word in the sentence. However, the model hallucinated by predicting a longer number of labels than in the given sentence; that is, a long sequences of non-existing "non-causal" labels.</p><p>Since GPT-4 hallucinated for the token-level predictions, we tried extracting the phrases of cause/effect relationships in text and tried converting them into token level by assigning labels for each token. We started with a four-shot prompting. The annotated data with the tags will be given as an example in the prompt, and the model is expected to predict similarly. A sample is given in Example 4.</p><p>1. Example 4.1. &lt;C&gt;Gestational diabetes&lt;/C&gt; has also been associated with an &lt;E&gt;increased risk of several long-term health outcomes in pregnant persons and intermediate outcomes in their offspring&lt;/E&gt; We tried converting the predictions with the tags into a token-level format in order to compute the F1 score. However, since the tags are placed in different places in some of the gold annotations and predictions, the number of tokens in gold and predictions doesn't match. An example is given below Example 4.2. Gold: Importance&lt;C&gt;Gestational diabetes&lt;/C&gt; is diabetes that develops during pregnancy.1-3 Prevalence of gestational diabetes in the US has been estimated at 5.8% to 9.2%, based on traditional diagnostic criteria, although it may be higher if more inclusive criteria are used.4-8 &lt;C&gt;Pregnant persons with gestational diabetes&lt;/C&gt; &lt;E&gt;increased risk for maternal and fetal complications, including preeclampsia, fetal macrosomia (which can cause shoulder dystocia and birth injury), and neonatal hypoglycemia&lt;/E&gt; .3,9-11 &lt;C&gt;Gestational diabetes&lt;/C&gt; has also been associated with an &lt;E&gt;increased risk of several long-term health outcomes in pregnant persons and intermediate outcomes in their offspring&lt;/E&gt; .12-16Table 1. Prediction: Importance Gestational diabetes is diabetes that develops during pregnancy. 1-3 Prevalence of gestational diabetes in the US has been estimated at 5.8% to 9.2%, based on traditional diagnostic criteria, although it may be higher if more inclusive criteria are used.4-8 &lt;C&gt;Pregnant persons with gestational diabetes&lt;/C&gt; &lt;E&gt;increased risk for maternal and fetal complications, including preeclampsia, fetal macrosomia (which can cause shoulder dystocia and birth injury), and neonatal hypoglycemia. 3,9-11&lt;/E&gt; &lt;C&gt;Gestational diabetes&lt;/C&gt; has also been associated with an &lt;E&gt;increased risk of several long-term health outcomes in pregnant persons and intermediate outcomes in their offspring.12-16Table 1.&lt;/E&gt; In Example 4.2, the phrases marked indicate the scenario where some extra spaces can be added, leading to the indifference in the number of tokens between gold and the predictions. In the gold data, neonatal hypoglycemia&lt;/E&gt; .3,9-11 have a space after the tag, but in the prediction, the tag is predicted after</p><p>the number, which leads to no space between &lt;/E&gt; and .3,9-11. In some scenarios, the GPT-4 omits some of the words if they do not contain a causal relation (omits the 'O' labels in some places). This mismatch between the gold and the predictions impedes the token-level comparison and reporting of the F1 score. An example is given below: In Example 4.3, in the prediction, the keyword "Hemoglobin" is missing, which is present in the gold data. In some places, such inconsistencies lead to token mismatch between the gold and predicted data.</p><p>To compare the performance of GPT-4 with other models, the predictions are converted into the token level and manually checked to convert both the gold predictions to the same number of tokens for the four-shot prompting.</p><p>In the predictions, some tokens are missed; those tokens are added to the predictions and marked as label "O."(as O indicates tokens that are not cause, effect, condition, action, or signal). After converting the data into a token level, we computed the F1 score. With GPT-4, we got an average F1 score of 0.39 with four-shot prompting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS &amp; EXPERIMENTS</head><p>As the predictions of GPT-4 can be unreliable, and missing tokens in a sentence leads to a token mismatch between the gold data and the predicted data, therefore Jaccard distance is proposed as an alternative solution to the traditional F1 score as the evaluation criteria. The Jaccard similarity was computed using the textdistance <ref type="foot" target="#foot_2">3</ref> Python library. Another alternative measure to try is the cosine similarity. The cosine similarity is obtained by computing the vectors of both the gold and the predictions using the Universal Sentence Encoder <ref type="bibr" target="#b4">[5]</ref>.</p><p>The computed values are used to compute the Jaccard similarity Cosine similarity F1 (labels) Zero-shot 0.42 0.22 0.27 Four-shot 0.44 0.22 0.35 Six-shot 0.57 0.22 0.52 Eight-shot 0.52 0.23 0.55 Ten-shot 0.57 0.20 0.60 Twenty-shot 0.46 0.20 0.28 Table <ref type="table">5</ref>. The phrase level comparison results of few-shot prompting using GPT-4. We tried various prompt sizes (zero, four, six, eight, ten, and twenty-shot prompting). From the results, we can understand that the Jaccard similarity at ten-shot prompting is higher (higher the similarity, higher overlap between the gold and predicted spans), cosine similarity is lower (lower the similarity, higher the gold and press are related), and the F1-score between the labels is higher, after which the similarity and F1 decreases at twenty-shot. The cosine similarity, which gives the semantic similarity between gold and predictions, remains the same with all the prompt sizes.</p><p>Here the F1-score is computed by comparing the gold labels and the predicted labels.</p><p>Precision Recall F1 score Action 0.56 0.90 0.69 Cause 0.60 0.74 0.66 Condition 0.95 0.17 0.29 Effect 0.74 0.79 0.76 Macro average 0.71 0.65 0.60 Table <ref type="table">6</ref>. Summary of the results of the GPT-4 predictions of ten-shot prompting on our medical data. Here the F1-score is computed by comparing the gold labels and the predicted labels. The F1 score for cause, effect, and action is higher compared to the condition. Many of the conditions are predicted as causes.</p><p>pairwise cosine similarity between two vectors using Scikit-learn <ref type="foot" target="#foot_3">4</ref> . The cause, effect, signal, condition, and action are extracted from the predictions using regular expressions on the tags. The extracted prediction phrases and the gold annotated phrases are merged. We perform two types of matching on the gold and predicted phrases.</p><p>• Jaccard similarity: To measure the dissimilarity between the gold data and the predictions.</p><p>• Cosine similarity: To measure the semantic similarity between the gold data and the predictions.</p><p>The results of phrase level similarity between the gold annotated data and predictions of GPT-4 using various prompt sizes are summarized in Table <ref type="table">5</ref>.</p><p>From the results of the various prompt sizes for the causality extraction on medical data, we can understand that the result of the ten-shot prompting gives a higher similarity and F1 score.</p><p>The Jaccard similarity gives the similarity score based on the overlap between the gold and the predictions. The cosine similarity gives the semantic similarity between the gold and predicted phrases. There is not much difference in the cosine similarity with various prompt sizes, indicating that it may not be the right measure for this task. The F1-scores are computed by comparing the gold labels with the predicted labels (Jaccard and cosine similarity for the predicted phrases, F1-score for the labels). The detailed F1-score for the ten-shot prompting label match between gold and predictions is given in Table <ref type="table">6</ref>. In particular, we can see that the F1 score for cause, effect, and action is higher compared to the other labels. (This result is comparable with a recent work <ref type="bibr" target="#b5">[6]</ref>, indicating a strong performance of ChatGPT for extracting cause/effect relationships).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">LLAMA2 for causality extraction from medical guidelines</head><p>LLAMA2 <ref type="bibr" target="#b51">[52]</ref> is a pre-trained and fine-tuned Large Language Model. Three variants of LLAMA2 are available, which differ in the parameters. 7B, 13B, and 70B parameters are publicly available. LLAMA2 is trained on two trillion tokens of data. In our experiments, the LLAMA2 7B parameter is fine-tuned on the medical data. It is fine-tuned using the HuggingFace autotrain.</p><p>To fine-tune LLAMA2, the first step is to prepare the data. At first, when the model was fine-tuned and tested on the token level as BERT, LLAMA2 was predicting a long number of "O-other" as GPT-</p><p>4. So we dealt with this as a phrase-level extraction problem. The data is prepared with three parts which are instruction, input, and output. A sample training data is given in example 5.1 Example 5.1. ###Instruction: Extract the cause, condition, effect, signal, and action from the given sentence. ###Input: Pregnant persons with gestational diabetes are at increased risk for maternal and fetal complications, including preeclampsia, fetal macrosomia (which can cause shoulder dystocia and birth injury), and neonatal hypoglycemia. ###Output: ['Pregnant persons-signal', 'with gestational diabetes -cause', 'increased risk for maternal and fetal complications, including preeclampsia, fetal macrosomia (which can cause shoulder dystocia and birth injury), and neonatal hypoglycemia-effect']</p><p>The test data should be similar to the training data except for the output, which should be empty. The gestational diabetes annotated data was split into train and test data. The HuggingFace autotrain <ref type="foot" target="#foot_4">5</ref> for the LLM fine-tuning was used to fine-tune the model. The fine-tuned weights are pushed into the HuggingFace dataset for inference. This experiment was done using Google Colab Pro+ with a High-RAM A100 GPU. Similar to the GPT-4, the predictions of LLAMA-2</p><p>were also at phrase level. So a similar evaluation strategy is followed for LLAMA2. We present the results with three types of distance.</p><p>The predictions are split into phrase levels and then compared with gold data. The Jaccard similarity was computed using the textdistance <ref type="foot" target="#foot_5">6</ref> Python library. The cosine similarity is obtained by computing the vectors of both the gold and the predictions using the Universal sentence encoder <ref type="bibr" target="#b4">[5]</ref>. The computed values are used to compute the pairwise cosine similarity between two vectors using Scikit-learn <ref type="foot" target="#foot_6">7</ref> .</p><p>Initially, we split the data into train and test using the Scikit learn train_test_split(). We have converted the phraselevel predictions into token-level. In the test data, there were a total of 59 samples. Out of the 59 samples, only 29 samples, LLAMA2 predicted the labels, so the evaluation is only for those sentences. With LLAMA2, we got an average F1-score of 0.36, which is lower than that of all the other models.</p><p>Since the test data size is very small, we have also tried a four-fold cross-validation on this data. The results of fine-tuning LLAMA2 using four-fold cross-validation with 3,5, and 10 epochs are given in Table <ref type="table" target="#tab_9">7</ref>.</p><p>With the increase in the number of epochs, both the Jaccard similarity and F1-score increase. Also, the predictions of LLAMA2 missed labels in many of the predictions. It extracted the phrases with no label. With three epochs, LLAMA2 missed 38% of the labels; with five epochs, 21% of the labels; and with ten epochs, it missed 26% of the labels. We omitted the predictions with no labels (108 predictions, 60 predictions, 76 predictions). The results of causality extraction presented in From the results, we can understand that Jaccard similarity, cosine similarity, and F1 score increase with the increase in the number of epochs. However, the number of missed labels started increasing after 5 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Above we presented results on causality extraction from medical guidelines using recently introduced large language models such as LLAMA2 and GPT-4, and compared them with the performance of BERT, an older, and smaller LLM. The annotated data and the code are all publicly available on GitHub: <ref type="url" target="https://github.com/gseetha04/LLMs-Medicaldata.git">https://github.com/gseetha04/LLMs-Medicaldata.git</ref> .</p><p>We observed that GPT-4 expresses strong performance for the cause-effect relationships with medical data, and generally has a good understanding of medical text without fine-tuning. However, in contrast with GPT-3.5 GPT-4 cannot deal with token classification, which limits the traditional way of finding cause and effect phrases, as discussed e.g. in our previous work <ref type="bibr" target="#b17">[18]</ref>.</p><p>Even though LLAMA2 seems to perform well for causality extraction, the predictions of the LLAMA2 do not predict labels for many cases, which limits its practical application. This perhaps was caused by fine-tuning LLAMA2 on our small dataset. Therefore, increasing the size of the dataset before the fine-tuning may improve the performance.</p><p>However, large annotated datasets for CPGs are not available, and further experiments would require annotating more data. Since we focused on the accuracy of actual predictions, we omitted 38% of labels with three epochs, 21% of labels with five epochs, and 26% of labels with ten epochs.</p><p>Given its relatively high performance and ease of use, BERT-based models continue to be a state-of-the-art for causality extraction tasks, even in the age of LLM .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We developed an automated technique for extracting causalities from annotated corpora of medical guidelines. Additionally, we exhibited the practicality of employing new Large Language Models for causality extraction tasks. With BioBERT, we got an average F1-score of 0.72, whereas with LLAMA2, an average Jaccard distance of 0.40 was obtained.</p><p>We demonstrated the potential for extracting causalities from medical guidelines using a small annotated corpus. The next logical step could involve expanding the corpus through the annotation of more data and creating a benchmark dataset for causality extraction from medical guidelines.</p><p>The potential of this research opens up novel dimensions for the health domain, as causality extraction from medical guidelines can enhance clinical decision-making and patient care. This work explored both machine learning and natural language processing techniques for causality extraction. Despite the abundance of causal sentences within these guidelines, automatic extraction is an unexplored field of research. Also, machine learning models often fail in clinical applications <ref type="bibr" target="#b47">[48]</ref> due to the gap between data (both training and testing). In order to avoid this gap, more realistic tests need to be done so that they can be employed for real-world data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Distribution of the labels in the corpus. The percentage of almost all the labels is around 24%.</figDesc><graphic coords="4,210.97,432.60,226.78,198.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Graph showing the train and validation loss when fine-tuning on BioBERT. Looking at the graph, we can understand that with the increase in the number of epochs, the training loss is constantly decreasing and approaching 0. The validation loss decreases till 16 epochs and then starts to increase. Based on this, we fine-tuned BioBERT for 16 epochs.</figDesc><graphic coords="6,182.63,95.04,283.47,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Example 4 . 3 .</head><label>43</label><figDesc>Gold:Race/Ethnicity/Hemoglobinopathies&lt;C&gt;Hemoglobin variants &lt;/C&gt; can &lt;E&gt;interfere with the measurement of A1C&lt;/E&gt;, although most assays in use in the U.S. are unaffected by the most common variants. Prediction: &lt;C&gt;Race/Ethnicity/Hemoglobinopathies variants&lt;/C&gt; can interfere with the measurement of A1C, although most assays in use in the U.S. are unaffected by the most common variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>For a given phrase, the labels annotated by annotators 1 and 2 are compared. An average F1 score of 0.78 was obtained. From the F1-score, we can understand that both the annotators agree on most of the categories except the signal for which the F1-score is low.</figDesc><table><row><cell></cell><cell cols="3">Precision Recall F1-score</cell></row><row><cell>Cause</cell><cell>0.86</cell><cell>0.71</cell><cell>0.77</cell></row><row><cell cols="2">Condition 0.56</cell><cell>0.85</cell><cell>0.67</cell></row><row><cell>Effect</cell><cell>0.85</cell><cell>0.90</cell><cell>0.88</cell></row><row><cell>Action</cell><cell>0.89</cell><cell>0.70</cell><cell>0.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Causality extraction results on the medical data using BioBERT, the highest performing model. Each token in the text was assigned a label Effect(E), Cause(C), Condition(CO), and Action(A). The results are obtained by splitting the manually annotated data into train and test data.</figDesc><table><row><cell></cell><cell cols="3">Precision Recall F1-score</cell></row><row><cell cols="2">DistilBERT 0.69</cell><cell>0.68</cell><cell>0.68</cell></row><row><cell>BERT</cell><cell>0.72</cell><cell>0.72</cell><cell>0.71</cell></row><row><cell>BioBERT</cell><cell>0.72</cell><cell>0.73</cell><cell>0.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Table7are after omitting the predictions with no labels.The phrase level comparison results of LLAMA2 using 4 fold cross validation. Jaccard similarity and cosine similarity indicate the average similarity between the gold and the predictions. The F1 score is the comparison between the gold labels and predicted labels.</figDesc><table><row><cell></cell><cell cols="3">Jaccard similarity Cosine similarity F1-score</cell></row><row><cell>LLAMA2 (3epochs)</cell><cell>0.73</cell><cell>0.19</cell><cell>0.70</cell></row><row><cell>LLAMA2 (5epochs)</cell><cell>0.888</cell><cell>0.20</cell><cell>0.75</cell></row><row><cell cols="2">LLAMA2 (10epochs) 0.90</cell><cell>0.21</cell><cell>0.76</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://pypi.org/project/python-Levenshtein/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://pypi.org/project/textdistance/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://pypi.org/project/textdistance/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://huggingface.co/docs/autotrain/llm_finetuning</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://pypi.org/project/textdistance/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was partly funded by the <rs type="funder">National Science Foundation (NSF)</rs> grant number <rs type="grantNumber">2141124</rs>. We would like to thank <rs type="person">Nikhil Vundela</rs> for his contributions to data annotation. We would like to thank <rs type="person">Dr. Wenwen Dou</rs> and <rs type="person">Dr. Victor Zitian Chen</rs> for their feedback on the causality extraction tasks.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HS7uKVJ">
					<idno type="grant-number">2141124</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CREDIT AUTHORSHIP CONTRIBUTION STATEMENT S.G. performed the majority of the experiments and writing. She also supervised the annotation process. L.G. chose the clinical guidelines data for annotations and participated in discussions and writing. W.Z. designed some of the experiments, provided feedback, and contributed to writing.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ACOG practice bulletin, Mellitus, Gestational Diabetes</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<pubPlace>Washington, DC, USA</pubPlace>
		</imprint>
	</monogr>
	<note>ACOG</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification and diagnosis of diabetes: Standards of Medical Care in Diabetes-2020</title>
	</analytic>
	<monogr>
		<title level="j">Diabetes care</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="14" to="S31" />
			<publisher>American Diabetes Association</publisher>
		</imprint>
	</monogr>
	<note>Supplement_1 (2020</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying predictive causal factors from news streams</title>
		<author>
			<persName><forename type="first">Ananth</forename><surname>Balashankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunandan</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Fraiberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshminarayanan</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2338" to="2348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diabetes and pregnancy: An Endocrine society clinical practice guideline</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Hadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lois</forename><surname>David R Hadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">H</forename><surname>Jovanovič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mestman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yariv</forename><surname>Hassan Murad</surname></persName>
		</author>
		<author>
			<persName><surname>Yogev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of clinical endocrinology &amp; Metabolism</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="4227" to="4249" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhomni</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<title level="m">Chris Tar, et al. 2018. Universal sentence encoder</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Chunkit</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqing</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14827</idno>
		<title level="m">Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Nuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shining</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.04333</idno>
		<title level="m">Beyond Surface: Probing LLaMA Across Scales and Layers</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Lightner: A lightweight generative framework with prompt-guided attention for low-resource NER</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00720</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards symbiosis in knowledge representation and natural language processing for structuring clinical practice guidelines</title>
		<author>
			<persName><surname>Weng Chunhua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Philip Ro Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">B</forename><surname>Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzanne</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><surname>Bakken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in health technology and informatics</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="page">461</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01760</idno>
		<title level="m">Template-based named entity recognition using BART</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Screening for gestational diabetes: US Preventive Services Task Force recommendation statement</title>
		<author>
			<persName><forename type="first">Karina</forename><forename type="middle">W</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><forename type="middle">M</forename><surname>Mangione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cabana</surname></persName>
		</author>
		<author>
			<persName><surname>Aaron B Caughey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Esa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrina</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chyke</forename><forename type="middle">A</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Doubeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Kubik</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">326</biblScope>
			<biblScope unit="page" from="531" to="538" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.05849</idno>
		<title level="m">Ting Liu, and Bing Qin. 2022. e-CARE: a new dataset for exploring explainable causal reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated application of clinical practice guidelines for asthma management</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Alan R Ertle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">R</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMIA Annual Fall Symposium</title>
		<meeting>the AMIA Annual Fall Symposium</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page">552</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A novel NLP-fuzzy system prototype for information extraction from medical guidelines</title>
		<author>
			<persName><forename type="first">Lejla</forename><surname>Begic Fazlic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hallawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anke</forename><surname>Schmeink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arne</forename><surname>Peine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Dartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 42nd International Convention on Information and Communication Technology</title>
		<imprint>
			<publisher>Electronics and Microelectronics (MIPRO). IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1025" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Joseph L Fleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myunghee</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paik</forename><surname>Cho</surname></persName>
		</author>
		<title level="m">Statistical methods for rates and proportions</title>
		<imprint>
			<publisher>john wiley &amp; sons</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clinical concept extraction: a methodology review</title>
		<author>
			<persName><forename type="first">Sunyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungrim</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">103526</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text to Causal Knowledge Graph: A Framework to Synthesize Knowledge from Unstructured Business Texts into Causal Graphs</title>
		<author>
			<persName><forename type="first">Seethalakshmi</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwen</forename><surname>Victor Zitian Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gus</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreekar</forename><surname>Hahn-Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wlodek</forename><surname>Nedunuri</surname></persName>
		</author>
		<author>
			<persName><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">367</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="394" to="398" />
		</imprint>
	</monogr>
	<note>Proceedings of the main conference and the shared task Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Associations Between Aggregate NLP-extracted Conflicts of Interest and Adverse Events By Drug Product</title>
		<author>
			<persName><forename type="first">Graham</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltan</forename><forename type="middle">P</forename><surname>Majdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johua</forename><forename type="middle">B</forename><surname>Barbour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">F</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in health technology and informatics</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page">405</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Wes</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.02207</idno>
		<title level="m">Language models represent space and time</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Tikhonov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12626</idno>
		<title level="m">HeadlineCause: A Dataset of News Headlines for Detecting Causalities</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Knowledge Extraction and Analysis of Medical Text with Particular Emphasis on Medical Guidelines</title>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Hematialam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Ph. D. Dissertation. The University of North Carolina at Charlotte</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Computing Conceptual Distances between Breast Cancer Screening Guidelines: An Implementation of a Near-Peer Epistemic Model of Medical Disagreement</title>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Hematialam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciana</forename><surname>Garbayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seethalakshmi</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wlodek</forename><surname>Zadrozny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00709</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Method for Computing Conceptual Distances between Medical Recommendations: Experiments in Modeling Medical Disagreement</title>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Hematialam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciana</forename><surname>Garbayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seethalakshmi</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wlodek</forename><forename type="middle">W</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2045</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Identifying condition-action statements in medical guidelines using domain-independent features</title>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Hematialam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wlodek</forename><surname>Zadrozny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04206</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Agreement, the f-measure, and reliability in information retrieval</title>
		<author>
			<persName><forename type="first">George</forename><surname>Hripcsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">S</forename><surname>Rothschild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American medical informatics association</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="296" to="298" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recommendation statements identification in clinical practice guidelines using heuristic patterns</title>
		<author>
			<persName><forename type="first">Musarrat</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamil</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Sadiq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="152" to="156" />
		</imprint>
	</monogr>
	<note>Anees Ul Hassan, and Sungyoung Lee</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Supporting the abstraction of clinical practice guidelines using information extraction</title>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Miksch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Application of Natural Language to Information Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Causal BERT: Language models for causality detection between events expressed in text</title>
		<author>
			<persName><forename type="first">Roshni</forename><surname>Vivek Khetan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayuresh</forename><surname>Ramnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubhashis</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">E</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><surname>Fano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05453</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Information extraction from electronic medical documents: state of the art and future research directions</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Yassine Landolsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lobna</forename><surname>Hlaoua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lotfi</forename><surname>Ben Romdhane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="463" to="516" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Factors associated with perinatal morbidity and mortality in a sample of public and private maternity centers in the City of Rio de Janeiro</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmo</forename><surname>Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvana</forename><surname>Granado Nogueira Da Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mônica</forename><surname>Rodrigues Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciana</forename><forename type="middle">Tricai</forename><surname>Cavalini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciana</forename><forename type="middle">Sarmento</forename><surname>Garbayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carla</forename><surname>Lopes Porto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Célia</forename><surname>Brasil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szwarcwald</forename><surname>Landmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cadernos de Saúde Pública</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="20" to="S33" />
			<date type="published" when="1999">2004. 1999. 2001. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Causality extraction based on self-attentive BiLSTM-CRF with transferred embeddings</title>
		<author>
			<persName><forename type="first">Zhaoning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page" from="207" to="219" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">KEPT: Knowledge Enhanced Prompt Tuning for event causality identification</title>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="page">110064</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DCU-Lorcan at FinCausal 2022: Span-based Causality Extraction from Financial Documents using Pre-trained Language Models</title>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liting</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Financial Narrative Processing Workshop@ LREC2022</title>
		<meeting>the 4th Financial Narrative Processing Workshop@ LREC2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="116" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Event Causality Identification via Generation of Important Context Words</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Hieu Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the 11th Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="323" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">International association of diabetes and pregnancy study groups recommendations on the diagnosis and classification of hyperglycemia in pregnancy: response to Weinert</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">G</forename><surname>Boyd E Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bengt</forename><surname>Gabbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lynn</forename><forename type="middle">P</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">Jn</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Oats</surname></persName>
		</author>
		<author>
			<persName><surname>Buchanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes care</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="98" to="e98" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BioCause: Annotating and analysing causality in the biomedical domain</title>
		<author>
			<persName><forename type="first">Claudiu</forename><surname>Mihăilă</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? Levenshtein distance, spell checker</title>
		<author>
			<persName><forename type="first">Frederic</forename><forename type="middle">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnes</forename><forename type="middle">F</forename><surname>Vandome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mcbrewster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>hamming distance</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Screening for Gestational Diabetes</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sopan</forename><surname>Mohnot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Family Physician</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="641" to="642" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774[cs.CL]</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">GPT-4 Technical Report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo on ten benchmarking datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05474</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Screening for gestational diabetes: updated evidence report and systematic review for the US preventive services task force</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Pillay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lois</forename><surname>Donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Guitard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernadette</forename><surname>Zakher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allison</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Vandermeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Bougatsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Hartling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">326</biblScope>
			<biblScope unit="page" from="539" to="562" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The probabilistic basis of Jaccard&apos;s index of similarity</title>
		<author>
			<persName><forename type="first">Raimundo</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">M</forename><surname>Vargas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systematic biology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="380" to="385" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Medicause: Causal relation modelling and extraction from medical publications</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Reklos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Meroño-Peñuela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Knowledge Graph Generation From Text co-located with 19th Extended Semantic Conference (ESWC 2022)</title>
		<meeting>the 1st International Workshop on Knowledge Graph Generation From Text co-located with 19th Extended Semantic Conference (ESWC 2022)<address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3184</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Exploiting cloze questions for few shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07676</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Schmidt</surname></persName>
		</author>
		<title level="m">MD Anderson breaks with IBM Watson, raising questions about artificial intelligence in oncology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Combining open-source natural language processing tools to parse clinical practice guidelines</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Meizoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Riano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Does synthetic data generation of llms help clinical text mining</title>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.04360</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Construction of an annotated corpus to support biomedical information extraction</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Mcnaught</surname></persName>
		</author>
		<author>
			<persName><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Identifying condition-action sentences using a heuristic-based information extraction method</title>
		<author>
			<persName><forename type="first">Reinhardt</forename><surname>Wenzina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Process Support and Knowledge Representation in Health Care</title>
		<meeting>ess Support and Knowledge Representation in Health Care</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="26" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">From text to tables: a local privacy preserving large language model for structured information retrieval from medical documents</title>
		<author>
			<persName><forename type="first">Isabella</forename><surname>Catharina Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dyke</forename><surname>Ferber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiefu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marko</forename><surname>Van Treeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonja</forename><forename type="middle">Katharina</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Juglan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zunamys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Carrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Paech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><forename type="middle">P</forename><surname>Kleesiek</surname></persName>
		</author>
		<author>
			<persName><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">medRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2035" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pre-trained language models with domain knowledge for Biomedical extractive summarization</title>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prayag</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<publisher>Knowledge-Based Systems</publisher>
			<biblScope unit="page">109460</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Differentiable prompt makes pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luoqiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13161</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
