<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selection Bias Induced Spurious Correlations in Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emily</forename><surname>Mcmilin</surname></persName>
						</author>
						<title level="a" type="main">Selection Bias Induced Spurious Correlations in Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AD2EB2320F6DB7BCC5C20EF9C3F173D5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we show how large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias. To demonstrate the effect, we developed a masked gender task that can be applied to BERT-family models to reveal spurious correlations between predicted gender pronouns and a variety of seemingly gender-neutral variables like date and location, on pre-trained (unmodified) BERT and RoBERTa large models. Finally, we provide an online demo, inviting readers to experiment further.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>For generalization to real-world target domains, a learned model's training data would ideally be a randomized sampled subset of the data in the target domain. However, achieving such randomization is often impractical, resulting in many datasets exhibiting sampling bias <ref type="bibr" target="#b6">(Heckman, 1979)</ref>. Models trained on datasets with sampling bias are vulnerable to learning spurious associations from this bias. These spurious associations, often referred to as spurious correlations though the associations need not be linear, can reduce the predictive performance of the model in realworld domains. Specifically, although we desire the model to learn the conditional distribution: ( | ), it has instead learned ( | , =1), where =1 represents selection into the dataset <ref type="bibr" target="#b0">(Bareinboim and Pearl, 2012)</ref>.</p><p>This paper focuses on a specific type of selection bias in which two variables: and , which are unconditionally independent in the real world ( ⟂ ⟂ ), become conditionally dependent within the dataset ( ̸⟂ ⟂ | ), due to the selection process, .</p><p>We hypothesize that a wide range of spurious correlations can be traced back to this type of bias. To expose subtle spurious associations that have not yet been reported, we desire an underspecified learning task, in which there are multiple equally plausible predictions. In natural language processing, one well-researched underspecified task is that of gender pronoun prediction <ref type="bibr" target="#b3">(D'Amour et al., 2020)</ref>, in which a gender is predicted from gender-neutral features.</p><p>Undesirable and spurious associations between gender: , and variables: ′ , such as occupation <ref type="bibr" target="#b13">(Webster et al., 2020)</ref> and college major <ref type="bibr" target="#b11">(Rudinger et al., 2018)</ref> have been found in LLMs. Unfortunately, many of these spurious associations are in fact representative of undesirable gender inequities in our real world ( ̸⟂ ⟂ ′ ).</p><p>To show how selection bias can induce associations that do not match our real-world distributions of gender, we seek to demonstrate spurious associations between gender and real-world gender-neutral variables: . In this paper we introduce and use the 'masked gender task' to demonstrate previously unreported spurious correlations between gender pronouns and the following gender-neutral entities for : time, location, and topics of interest, on unmodified pre-trained BERT large <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref> and RoBERTa large <ref type="bibr" target="#b9">(Liu et al., 2019)</ref> models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data Generating Processes</head><p>A sample can only be selected into a dataset if it has access to the sampling process. We use the term access here, as it evokes processes of selection also experienced by humanbeings, which we will further motivate when describing the data generating process.</p><p>To understand the selection bias intrinsic to a dataset, one must consider the dataset's data generating process. Datasets do not generally admit their generating process, but rather it must be discovered via auxiliary methods such as applying domain knowledge or causal discovery methods. We use intuition and domain knowledge to describe what we hope are plausible data generating processes below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Datasets</head><p>We ground this discussion with two example datasets, selected as they are representative of data sources used to pre-train LLMs.  Specifically, we use the Wikipedia Biography (Wiki-Bio) dataset <ref type="bibr" target="#b8">(Lebret et al., 2016)</ref>, composed of about 730,000 biographies from English Wikipedia, with which we finetuned models on the roughly 105,000 entries that contained birth date and birth place data, as well as at least one instance of an explicitly gendered word, from the list in Table <ref type="table">3</ref> <ref type="foot" target="#foot_2">foot_2</ref> .</p><p>And we use Reddit Webis-TLDR-17 (Reddit-TLDR) dataset <ref type="bibr" target="#b12">(Völske et al., 2017)</ref>, composed of content-summary pairs from Reddit from 2006-2016, with which we finetuned models on the roughly 320,000 entries that again contained at least one explicitly gendered word from Table <ref type="table">3</ref>. Both datasets are hosted on Hugging Face's Datasets Library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Selection Variables</head><p>Recall we desire to demonstrate a learned statistical association between gender, , and gender-neutral variables, , that is driven by access (now referred as ) to the dataset sampling process. For our datasets, suitable instantiations for and as related to are as follows.</p><p>For Wiki-Bio: access to resources has generally become less gender inequitable over time as date ( 0 ) increases, but not evenly in every place ( 1 ). Generally only those with access to resources will achieve the level of notoriety necessary for an entry in Wikipedia.</p><p>For Reddit-TLDR: despite many subreddit channels ( ) having a focus on gender-neutral topics of interest, the style of moderation and community within a given subreddit may reduce gender-equal access to participation in that subreddit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Causal DAGs</head><p>The above written descriptions of variables and their causal relationships can be compactly represented in a causal directed acyclic graph (DAG), where nodes are variables and arrows are the direction of causation, as can be seen in Figure <ref type="figure" target="#fig_1">1</ref>(a) for our assumed data generating process for the Wiki-Bio and Reddit TLDR datasets.</p><p>From Figure <ref type="figure" target="#fig_1">1</ref>(a), we see that and can cause one's access, , to dataset selection, as discussed above.</p><p>At the bottom we see our dataset's features: for text, and labels: for pronouns. We argue that despite the complex causal interactions between words in a sentence, the text are more likely to cause the pronouns, rather than vice versa. <ref type="foot" target="#foot_3">2</ref>Further we assume that and have an effect on one's life and thus the text written about them or by them. And clearly does cause the gender pronouns, . However, because the goal of the masked gender task is to mask out explicitly gendered words, we'd argue that is not a direct cause of the text, . Finally, note is grayed out, as it is not explicitly recorded in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Selection Bias</head><p>To explain how selection bias can cause two unconditionally independent variables to become dependent, we revisit the causal DAG representing the data generating processes in Figure <ref type="figure" target="#fig_1">1(a)</ref>. Causal DAGs are associated with a set of structural equations that together compose a structural causal model <ref type="bibr">(Pearl, 2009)</ref>. For the access variable, the structural equation is ∶= ( , , ), where is the exogenous noise of the variable, and again are the variables date and place for Wiki-Bio, and subreddit for Reddit TLDR, and is gender. Although and are independent in Figure 1(a), for all but trivial special cases, they will become statistically associated in the equation for .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditioning on a Collider</head><p>Thus any model that conditions on the variable , will introduce this spurious association between and , known as collider bias <ref type="bibr">(Pearl, 2009)</ref>. Excluding from the predictive model seems trivial, especially because is not directly observed in the dataset. However, recall that is a cause of the selection process, depicted in the selection diagram, Figure <ref type="figure" target="#fig_1">1(b)</ref>, where the selection node, , can only take values =1 for a sample selected into the dataset and =0 otherwise <ref type="bibr" target="#b0">(Bareinboim and Pearl, 2012)</ref>.</p><p>Recall from Section 1 that during dataset formation, we implicitly condition on =1, as only selected samples appear in the dataset. Conditioning on a descendant of , induces the collider bias relationship, as if we had conditioned on directly. We are thus inducing the latent structural equation for into the dataset, from which the statistical association between and can be learned by our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Selection Bias Recoverability</head><p>Figure 1(b) is a modified version of our original causal DAG, which satisfies the requirements of the masked gender task, specifically we have obscured and replaced it with double headed arrows to represent an unobserved common cause of both and .</p><p>Although the act of obscuring gender for gender pronoun prediction may seem contrived, we argue that LLMs are often in similar circumstances, for example whenever a prompt, dialog, translation or classification task has not been provided gender features, yet predictions about gender are required.</p><p>Structural causal models very similar to that in Figure <ref type="figure" target="#fig_1">1(b</ref>) have been described in practice in <ref type="bibr" target="#b7">(Knox et al., 2020)</ref>, and proven in <ref type="bibr" target="#b2">(Bareinboim et al., 2014)</ref>, to be not 'recoverable'. Formally, because we are unable to d-separate the selection mechanism from the label: ( ̸⟂ ⟂ | ), the conditional distribution of ( | ) cannot be determined without further assumptions or additional data about target populations <ref type="bibr" target="#b1">(Bareinboim and Tian, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Masked Gender Task</head><p>This lack of recoverability for ( | ) is consistent with our goals of underspecification for the masked gender task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Inference Test Texts</head><p>Revisiting Figure <ref type="figure" target="#fig_1">1</ref>(b), to maintain underspecification we now require gender-neutral text values for the input text, , and the date, place, and subreddit variables, . In addition to gender-neutral, for we desire extremely simplistic texts, to avoid inducing unrelated spurious correlations. We were unable to find such a benchmark dataset, so we used the heuristic described in Table <ref type="table">1</ref> to generate over 700 input texts for each of the three categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">x-axis Values</head><p>Remaining undefined in the heuristic for input texts in Table 1, is the text values to use for . These values will also serve as our x-axis in the coming plots. We require values that are gender-neutral in the real world, yet are hypothesized, due to the selection bias process, to be a spectrum of gender-inequitable values in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For</head><p>as date, it's easy to just use time itself, as over time women have become more likely to be recorded into historical documents reflected in Wikipedia, so we pick years ranging from 1801 -2011. For as place, we use the bottom and top 10 World Economic Forum Global Gender Gap ranked countries (see B.1). And for as subreddit, we use subreddit name ordered by subreddits channels that have an increasingly larger percentage of self-reported female commenters, with a minimum size of 400,000 commenters overall (see B.2).</p><p>The original premise for the variable was to use categories that are gender-neutral in the real world, but not necessarily so in the dataset. To achieve this, we considered filtering the subreddit list to only topics deemed genderneutral, however this subjective process invited too much cherry picking on our behalf. Thus, for the subreddit (and place) values, we copied the referenced lists verbatim from their sources.</p><p>To help disambiguate the role of non-gender-neutral subreddit topic names, from the role of access based selection bias, in contributing to a correlation between and , we tested on one pre-trained model likely exposed to the selection bias effect during pre-training, and one that was likely not exposed. Specifically, RoBERTa was trained with the same data sources as BERT, plus additional data sources including OpenWebText<ref type="foot" target="#foot_4">foot_4</ref>  <ref type="bibr" target="#b5">(Gokaslan and Cohen, 2019)</ref>. Thus, we'd expect RoBERTa to exhibit a stronger subreddit to gender correlation, than that of BERT, due to the role of subreddit access selection bias during the pre-training of RoBERTa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pre-trained BERT-like models</head><p>For testing the masked gender task on pre-trained models, we selected BERT large and RoBERT large, as explained in Section 4.2, using the default weights hosted for the models on Hugging Face.</p><p>We are able to test the pre-trained LLMs without any modification to the models, as the masked gender task is simply a special case of the masked language modeling (MLM) task, with which all these models were pre-trained.</p><p>Rather than random masking, the masked gender task masks only explicitly gendered words (listed in Table <ref type="table">3</ref>). During LLM pre-training, the MLM prediction is a softmax over the entire tokenizer's vocabulary. The masked gender task sums the gendered portion (as listed in Table <ref type="table">3</ref>) of that probability mass from the top five predicted words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Finetuned Models</head><p>We also finetune BERT-like models using a similar masked gender task. The difference being that for our finetuning task, the prediction outcome is binary (as opposed to the entire tokenizer's vocabulary), largely for run-time expediency. We elected to finetune the models with data sources similar to those in their pre-training, so we selected BERT Table <ref type="table">1</ref>. Heuristic for creating gender-neutral input texts for the masked gender task for each variable category, and example rendered text. For verb we used past, present and future tenses of the verb to be: ["was","is","will be"], and for life_stage we used proper and colloquial terms for a range of life stages: ["a child", "a kid", "an adolescent", "a teenager", "an adult", "all grown up"]. We didn't include any life_stage past adulthood, as there are not equal gender ratios of elderly men to women, in many locations. Finally for the text versions of w, we used a spectrum of values defined in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>Python f-string Example text Date &amp; Place 'f"[MASK] {verb} {life_stage} in {w}."' '[MASK] was a teenager, in 1953.' 'f"In {w}, [MASK] {verb} {life_stage}."' 'In Mali, [MASK] will be an adult .' Subreddit 'f"[MASK] {verb} {life_stage}. {w}."' '[MASK] is a kid. gifs.'</p><p>for the Wiki-Bio dataset and RoBERTa for the Reddit TLDR dataset.</p><p>For the Wiki-Bio dataset, we finetune three BERT base models: 1) with birth date metadata, 2) birth place metadata, and 3) with no extra metadata, prepended to each training sample. In the case of the Reddit TLDR dataset we finetune two RoBERTa base models: 1) with subreddit metadata and 2) with no extra metadata, prepended to each training sample.</p><p>As these models are finetuned with a single dataset (Wiki-Bio or Reddit TLDR) for the single task of gendered-word prediction, we'd expect them to serve as an upper limit for the magnitude of spurious correlations a model could exhibit for the masked gender task. In particular, the models conditioned with the textual metadata values for at train time are expected to learn the strongest relationship between and .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section we share the results of the masked gender task, tested on pre-trained BERT and RoBERTa large, as well as our finetuned models which can serve as a rough upper limit for the magnitude of expected spurious correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Wiki-Bio Date Results</head><p>Figure <ref type="figure">2</ref>(a) shows the results for as date vs gender pronoun predictions. The spurious correlations shown in these plots are consistent with our hypothesized outcome of the selection bias effect, since all of the models were trained on Wikipedia biographical data. Specifically, as date increases, women's access to resources increases. And thus their representation in Wikipedia increases, inducing the spurious correlation between date and gender. Due to the nature this collider bias, as female representation goes up, male representation tends to go down<ref type="foot" target="#foot_5">foot_5</ref> .</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the slope and Pearson's correlation coefficient (following <ref type="bibr" target="#b11">(Rudinger et al., 2018)</ref>) for all the plots in this section. These reported coefficients are limited in many ways, including the improbable assumption that the learned relationship between and gender is linear. Further limiting is that we calculate these coefficients against the index of the x-axis, rather than the date value on the x-axis. We do this here for consistency with the coming place and subreddit plots, for which the most convenient quantitative value we can assign to an ordered list of countries or subreddits is their index in the list.</p><p>Thus, these coefficients only serve to compare one model's response to another's for a given category. The most noteworthy comparison here is that the correlation coefficients of the pre-trained models are comparable to those of the finetuned models for female pronouns, all above a Pearson's value of 0.75 (perhaps in part as a trade-off for the male pronoun coefficients).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Wiki-Bio Place Results</head><p>The results for as place in Figure <ref type="figure">2</ref>(b) are similar to those discussed above, although the correlation coefficients appear slightly weaker. As mentioned, reliable comparisons of these coefficients across variables are limited. We nonetheless conjecture that the slightly weaker correlations for place are perhaps in part due to the subjective nature by which the countries are ordered along the x-axis.</p><p>Despite this limitation, we still see comparable slope and correlation coefficients between the finetuned and pretrained models for the spurious correlation between place and gender pronouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Subreddit Results</head><p>A challenge in interpreting the results for as subreddit in Figure <ref type="figure">2</ref>(c) is that our claim of as gender-neutral in the 'real-word' is dubious for subreddit names, as compared to dates and country names. As discussed in Section 4.2, we elected against filtering the x-axis to only subreddit names that were more gender-neutral, as this was a subjective process that invited cherry picking. (c) as subreddit, each dot is an average of 18 softmax probabilities for the mask in input text like "[MASK] is a kid. Futurology". Figure <ref type="figure">2</ref>. Averaged softmax percentages for predicted gender pronouns vs a range of values as described in Section 4.2, for genderneutral input texts described in Table <ref type="table">1</ref>. Shaded regions show the 95% confidence interval for a 1st degree linear fit between and . For all plots we expect a correlation between the x-axis and the predictions, except for Pre-trained BERT large's predictions vs subreddit.</p><p>To help disambiguate the role of selection bias vs the role of 'real-word' gendered terms for , we tested one pre-trained model that was likely exposed to the selection bias effect during pre-training, and one that was likely not exposed. Specifically, RoBERTa should learn a more gendered representation for the otherwise gender-neutral subreddit channel names since the selection bias was likely present during its pre-training, whereas BERT should not.</p><p>The bold-faced correlation coefficients in Table <ref type="table" target="#tab_1">2</ref> do appear to confirm this hypothesis, with BERT's slope and coefficients roughly 1 5 to 1 10 that of RoBERTa, but the authors note more robust testing is desired to strengthen this argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Demonstration and Open-Source Code</head><p>The authors would greatly appreciate community feedback that supports or challenges our results. To enable greater access, we have developed a demo of the masked gender task, where users can choose their own input text, as well as the variable, x-axis values, and the plotted degree of fit, to test spurious correlation to gender in almost any BERT-like model hosted on Hugging Face at <ref type="url" target="https://huggingface.co/spaces/emilylearning/spurious_correlation_evaluation">https:  //huggingface.co/spaces/emilylearning/</ref> spurious_correlation_evaluation.</p><p>We additionally we will make all code available at <ref type="url" target="https://github.com/2dot71mily/spurious_correlations_ICML_2022">https://github.com/2dot71mily/spurious_  correlations_ICML_2022</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper we have introduced and applied the masked gender task to reveal spurious correlations between gender pronouns and real-world gender-neutral entities like dates and countries, on BERT and RoBERTa large pre-trained models. We showed similar spurious correlations between gender pronouns and subreddit channel names, however as we lacked an objectively gender-neutral list of subreddit names, it is more difficult to disambiguate the role of selection bias vs that of non-gender-neutral topic names.</p><p>The measured correlations between date, country and subreddit-topic vs the probability of a man or woman existing (as a child, adolescent or adult), may be predictive for Wikipedia entries or Subreddit commenters, as ( | , , =1), but will not necessarily generalize to the probabilities of men and women existing in real-world inference domains of ( | , ).</p><p>Our results indicate that sentences previously considered as gender-neutral baselines for testing gender bias in LLMs (e.g. input text such as 'a woman is walking.' <ref type="bibr" target="#b3">(D'Amour et al., 2020)</ref>), are also vulnerable to spurious correlations.</p><p>We explained the role of dataset selection bias in inducing the spurious association between otherwise unconditionally independent entities, such as gender and time, and suggested broad applicability beyond the particular relationships investigated here. As mentioned in Section 3.2, further assumptions or data can help to mitigate the effects of selection bias, which we hope to apply to LLMs in the future.  <ref type="table">3</ref>. List of explicitly gendered words that are masked out for prediction as part of the masked gender task. These words were largely selected for convenience, as each is a single token in both the BERT and RoBERTa tokenizer vocabs, for ease of downstream token to word alignment. During finetuning, it is expected that this list will not fully mask gender in every sample, reducing the underspecification of the learning task and the potential learning of gender-neutral spurious associations to gender. At inference time, it is critical that all gendered words are masked, and because the inference input texts are constructed by a heuristic, this is trivial to achieve.</p><p>MALE-VARIANT FEMALE-VARIANT HE SHE HIM HER HIS HER HIMSELF HERSELF MALE FEMALE MAN WOMAN MEN WOMEN HUSBAND WIFE FATHER MOTHER BOYFRIEND GIRLFRIEND BROTHER SISTER ACTOR ACTRESS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Explicitly Gendered Words</head><p>See Table <ref type="table">3</ref> for list explicitly gendered words that were masked for prediction during both finetuning and at inference time for the masked gender task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. variable x-axis values B.1. Place Values</head><p>Ordered list of bottom 10 and top 10 World Economic Forum Global Gender Gap ranked countries used for the x-axis in Figure <ref type="figure">2</ref>(b), that were taken directly without modification from <ref type="url" target="https://www3.weforum.org/docs/WEF_GGGR_2021.pdf">https://www3.weforum.org/docs/WEF_GGGR_  2021.pdf</ref>:</p><p>"Afghanistan", "Yemen", "Iraq", "Pakistan", "Syria", "Democratic Republic of Congo", "Iran", "Mali", "Chad", "Saudi Arabia", "Switzerland", "Ireland", "Lithuania", "Rwanda", "Namibia", "Sweden", "New Zealand", "Norway", "Finland", "Iceland"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Subreddit Values</head><p>Ordered list of subreddits used for the x-axis in Figure <ref type="figure">2</ref>(c), that were taken directly without modification from <ref type="url" target="http://bburky.com/subredditgenderratios/">http:  //bburky.com/subredditgenderratios/</ref> with minimum subreddit size: 400,000.</p><p>"GlobalOffensive", "pcmasterrace", "nfl", "sports", "The_Donald", "leagueoflegends", "Overwatch", "gonewild", "Futurology", "space", "technology", "gaming", "Jokes", "dataisbeautiful", "woahdude", "askscience", "wow", "anime", "BlackPeo-pleTwitter", "politics", "pokemon", "worldnews", "reddit.com", "interestingasfuck", "videos", "nottheonion", "television", "science", "atheism", "movies", "gifs", "Music", "trees", "EarthPorn", "GetMotivated", "pokemongo", "news", "Fitness", "Showerthoughts", "OldSchoolCool", "explainlikeimfive", "todayilearned", "gameofthrones", "AdviceAnimals", "DIY", "WTF", "IAmA", "cringepics", "tifu", "mildlyinteresting", "funny", "pics", "LifeProTips", "creepy", "personalfinance", "food", "AskReddit", "books", "aww", "sex", "relationships"</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>(a) DAG including : gender, that is later unobserved. (b) DAG including node, , representing the dataset selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Causal DAG representing the assumed data generating process for the Wiki-Bio and Reddit TLDR datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>(a) as date, each dot is an average of 36 softmax probabilities for the mask in input text like "[MASK] will be an adult in 1945.". (b) as place, each dot is an average of 36 softmax probabilities for the mask in input text like "In Iran, [MASK] was a teenager.".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Slopes and Pearson's correlation coefficient for the plots in Figure2of spurious correlations between gender and several otherwise gender-neutral categories for the variable: date, place and subreddit channel topics. Values in bold font are for the only experiment in which a model was tested against variables for which the model under test has no selection bias pressures.</figDesc><table><row><cell cols="2">Category Model Type</cell><cell>Model Training Details</cell><cell cols="4">Female Slope Pearson's r Slope Pearson's r Male</cell></row><row><cell>Date</cell><cell>finetuned pre-trained</cell><cell>WikiBio no Metadata WikiBio w Birthdate BERT large RoBERTa large</cell><cell>0.558 0.616 0.235 0.149</cell><cell>0.929 0.936 0.826 0.759</cell><cell>-0.558 -0.616 -0.016 -0.055</cell><cell>-0.929 -0.936 -0.116 -0.290</cell></row><row><cell>Place</cell><cell>finetuned pre-trained</cell><cell>WikiBio no Metadata WikiBio w Birthplace BERT large RoBERTa large</cell><cell>0.817 0.591 0.381 0.277</cell><cell>0.763 0.752 0.476 0.724</cell><cell>-0.817 -0.591 -0.589 -0.247</cell><cell>-0.763 -0.752 -0.701 -0.525</cell></row><row><cell>Subreddit</cell><cell>finetuned pre-trained</cell><cell cols="2">Reddit TLDR no Metadata 0.243 Reddit TLDR w Subreddit 0.345 BERT large 0.006 RoBERTa large 0.071</cell><cell>0.452 0.494 0.049 0.335</cell><cell>-0.243 -0.345 0.016 -0.074</cell><cell>-0.452 -0.494 0.071 -0.300</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Independent Researcher. Correspondence to: Emily McMilin &lt;emcmilin@cs.stanford.edu&gt;.Published at the ICML</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2022" xml:id="foot_1"><p>Workshop on Spurious Correlations, Invariance, and Stability. Baltimore, Maryland, USA. Copyright 2022 by the author(s).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>Further selection bias is introduced here during the filtering of ineligible entries, of unknown effect on finetuned models, which serve referential purposes for the pre-trained model's results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>For example, if the subject is a famous doctor and the object is her wealthy father, these context words will determine which person is being referred to, and thus which gendered-pronoun to use.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>Although OpenWebText does not explicitly include Reddit data, because it is composed of scraped web content from URLs shared on Reddit that received at least 3 upvotes<ref type="bibr" target="#b9">(Liu et al., 2019)</ref>, we conjecture subreddit topic names would appear in the context of Reddit in this dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>However, in pre-trained models there exist many selection pressures, one of which appears to cause BERT and RoBERTa to be more likely to predict gender pronouns (as opposed to nongendered pronouns) for both genders after 2000.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Thank you to the SCIS reviewers for their helpful comments, to <rs type="person">Rosanne Liu</rs> and <rs type="person">Jason Yosinski</rs> for their encouragement, to <rs type="person">Hugging Face</rs> for their open source services, and to <rs type="person">Judea Pearl</rs>, <rs type="person">Elias Bareinboim</rs>, <rs type="person">Brady Neal</rs> and <rs type="person">Paul Hünermund</rs> for their fantastic online causal inference resources.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Controlling selection bias in causal inference</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v22/bareinboim12.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Girolami</surname></persName>
		</editor>
		<meeting>the Fifteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>La Palma, Canary Islands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-04">Apr 2012</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="21" to="23" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering causal effects from selection bias</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tian</surname></persName>
		</author>
		<idno>doi: 10.1609</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/9679" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015-03">Mar. 2015</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
	<note>/ aaai.v29i1.9679</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recovering from selection bias in causal and statistical inference</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/9074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Underspecification presents chal-lenges for credibility in modern machine learning</title>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Alexander D'amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Hormozdiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaobo</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghassen</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-An</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cory</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akinori</forename><surname>Mincu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Mitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Nielson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rory</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schrouff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harini</forename><surname>Sequeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Vladymyrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taedong</forename><surname>Yadlowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><surname>Sculley</surname></persName>
		</author>
		<idno>CoRR, abs/2011.03395</idno>
		<ptr target="https://arxiv.org/abs/2011.03395" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanya</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://Skylion007.github.io/OpenWebTextCorpus" />
		<title level="m">Openwebtext corpus</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sample selection bias as a specification error</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Heckman</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/1912352" />
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<idno type="ISSN">00129682, 14680262</idno>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Administrative records mask racially biased policing</title>
		<author>
			<persName><forename type="first">Dean</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Lower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mummolo</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0003055420000039</idno>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="619" to="637" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating text from structured data with application to the biography domain</title>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno>CoRR, abs/1603.07771</idno>
		<ptr target="http://arxiv.org/abs/1603.07771" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<idno type="DOI">10.1017/CBO9780511803161</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Judea Pearl. Causality. Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno>CoRR, abs/1804.09301</idno>
		<ptr target="http://arxiv.org/abs/1804.09301" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TL;DR: Mining Reddit to learn automatic summarization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahbaz</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4508</idno>
		<ptr target="https://www.aclweb.org/anthology/W17-4508" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization</title>
		<meeting>the Workshop on New Frontiers in Summarization<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09">September 2017</date>
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Measuring and reducing gendered correlations in pre-trained models</title>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.06032" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
