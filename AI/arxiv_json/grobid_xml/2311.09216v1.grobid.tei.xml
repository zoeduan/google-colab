<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Assessing Translation capabilities of Large Language Models involving English and Indian Languages</title>
				<funder ref="#_EdnPu6v">
					<orgName type="full">Ministry of Electronics and Information Technology, Government of India</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vandan</forename><surname>Mujadia</surname></persName>
							<email>vandan.mu@research.iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTRC</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ashok</forename><surname>Urlana</surname></persName>
							<email>ashok.urlana@research.iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTRC</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Penumalla</roleName><forename type="first">Yash</forename><surname>Bhaskar</surname></persName>
							<email>yash.bhaskar@research.iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTRC</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Pavani</surname></persName>
							<email>aditya.pavani@students.iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTRC</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kukkapalli</forename><surname>Shravya</surname></persName>
							<email>kukkapalli.shravya@students.iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTRC</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Parameswari</forename><surname>Krishnamurthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTRC</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dipti</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTRC</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Assessing Translation capabilities of Large Language Models involving English and Indian Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">11067EC9F40214B6D5D09D8493228616</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. In this work, our aim is to explore the multilingual capabilities of large language models by using machine translation as a task involving English and 22 Indian languages. We first investigate the translation capabilities of raw large language models, followed by exploring the in-context learning capabilities of the same raw models. We fine-tune these large language models using parameter efficient finetuning methods such as LoRA and additionally with full fine-tuning. Through our study, we have identified the best performing large language model for the translation task involving LLMs, which is based on LLaMA.</p><p>Our results demonstrate significant progress, with average BLEU scores of 13.42, 15.93,  12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99, 42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for English to Indian languages on IN22 (conversational), IN22 (general), flores200dev, flores200-devtest, and newstest2019 testsets. Similarly, for Indian languages to English, we achieved average BLEU scores of 14.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative Large Language Models (LLMs) have made significant performance improvements in various natural language processing (NLP) tasks, showcasing exceptional progress in a wide range of applications <ref type="bibr" target="#b42">(Xuanfan and Piji, 2023;</ref><ref type="bibr"></ref> Figure <ref type="figure">1</ref>: LLMs based Machine Translation performance comparison with public systems for English to Indian Languages. BLEU and chrF scores are averaged over 22 Indian Languages and 5 different benchmark data-sets. The available MT systems are GPT-3.5 (GPT-3.5 Davinci, by OpenAI), IndicTrans-2, Google Translation, LTRC-IIIT-H, SeamlessM4T. LLaMA-2-7b and LLaMA-2-13b are evaluated as LLM based fine-tuned MT systems are namely LLaMA-2-7b+lora (Multi), LLaMA-2-13b+lora (Multi), and LLaMA-2-13b+FF+lora (Multi). <ref type="bibr" target="#b40">Xi et al., 2023)</ref>. These tasks span from open domain question answering, where LLMs excel at providing accurate and coherent responses, to instruction-based tasks such as code completion, where LLMs can generate code snippets based on given prompts <ref type="bibr" target="#b36">(Vaithilingam et al., 2022)</ref>. LLMs have also demonstrated proficiency in tasks like essay writing, grammar checking <ref type="bibr">(Wu et al., 2023a)</ref>, and text summarization, where they can produce high-quality outputs <ref type="bibr" target="#b9">(Chang et al., 2023)</ref>. These advancements have primarily been observed in English-centric tasks. The popular LLMs support several of natural languages. The performance for some languages other than English is not yet on par or yet to be evaluated <ref type="bibr">(Lai et al., 2023;</ref><ref type="bibr" target="#b45">Zhu et al., 2023)</ref>.</p><p>A multilingual country like India, where over 364+ languages and dialects<ref type="foot" target="#foot_0">foot_0</ref> are spoken across its vast territory, presents a multitude of challenges across various domains due to language barriers <ref type="bibr" target="#b46">(Zieliński et al., 2021)</ref>, such as day-to-day communication, education <ref type="bibr" target="#b33">(Steigerwald et al., 2022)</ref>, business, healthcare <ref type="bibr" target="#b21">(Mehandru et al., 2022)</ref>, tourism, governance, and more. Recent advancements in the field of Large Language Models may offer solutions to these challenges tailored to Indian languages.</p><p>To test whether LLM can effectively overcome language barriers, it is crucial to evaluate the proficiency of large language models in handling Indian languages. Machine Translation, as a critical multilingual task, could be an ideal option to explore the multilingual capabilities of existing models. Hence, we can formulate the question to assess the proficiency of large language models in handling Indian languages as follows: How effectively do large language models perform in multilingual tasks like Machine Translation, particularly when dealing with Indian languages?</p><p>In this work, our major contribution is to address the following points in response to the above question.</p><p>• What are the directions for utilizing or adapting Large Language Models for Indian Languages?</p><p>-How do LLMs perform in translating a wide range of Indian languages under zero-shot and in-context learning settings? -Does LLM fine-tuning improve the translation capabilities of Large Language Models? How do they perform on lowresourced MT languages? -The impact of LLM Vocabulary on the Performance of Large Language Models in Translation Tasks.</p><p>To address the above questions, we assess the translation capabilities of popular large language models (opt, bloom, LLaMA-1, MPT, Falcon, LLaMA-2, and Mistral [Section 3]) involving English and 22 scheduled Indian languages (Assamese, Bangla, Bodo, Dogri, Konkani, Gujarati, Hindi, Kannada, Kashmiri, Maithili, Malayalam, Marathi, Meitei, Nepali, Odia, Punjabi, Sanskrit, Santali, Sindhi, Tamil, Telugu, and Urdu). We initially examine the translation capabilities of above mentioned raw large language models [Section 5.1]. Subsequently, we explore their in-context learning abilities [Section 5.1]. Additionally, we fine-tune the base models using parameter-efficient fine-tuning methods specifically LoRa .</p><p>Furthermore, we investigate the potential of 2-stage fine-tuning for large language models, which involves full parameter fine-tuning in the first stage, followed by LoRa-based adaptor fine-tuning [Section 6].</p><p>The key findings of our work, as summarized in Figure <ref type="figure">1</ref>, highlight the performance of our LLM-based machine translation fine-tuned models compared to various known translation engines. These engines range from commercial (Google<ref type="foot" target="#foot_1">foot_1</ref> , GPT-3.5<ref type="foot" target="#foot_2">foot_2</ref> ) to open source (IndicTrans-2<ref type="foot" target="#foot_3">foot_3</ref> , LTRC-IIIT-H<ref type="foot" target="#foot_4">foot_4</ref> , seamlessm4t<ref type="foot" target="#foot_5">foot_5</ref> ), traditional supervised encoder-decoder translation models (Google, IndicTrans-2, LTRC-IIIT-H) and decoder-driven causal large language model-based translation systems (GPT-3.5).</p><p>Our findings underscore the significant potential of large language models for translation tasks involving English and Indian Languages. While raw LLMs (LLaMA-2-7b and LLaMA-2-13b) not perform well on translation tasks, our two-stage MT fine-tuned models (LLaMA-2-13b+FF+lora(Multi)) yields comparative results even with minimal parallel corpora. This suggests that LLMs have the potential to possess multilingual capabilities for translating into underrepresented languages, which can be further enhanced through fine-tuning. This work will be a crucial and pioneering milestone in evaluating LLMs for language representation and assessing their translation capabilities for a diverse range of Indian languages, especially those with limited available resources.</p><p>Recent advancements in machine translation have shown that neural machine translation (NMT) has made significant strides in terms of output fluency and translation quality, especially when ample parallel data is available <ref type="bibr" target="#b5">(Barrault et al., 2020)</ref>. However, the scarcity or absence of parallel data poses a challenge for most language pairs. In the case of Indian languages, recent developments have tried to addressed this issue by introducing a new state-of-the-art approach: multilingual machine translation involving Indian languages and English <ref type="bibr" target="#b38">(Wang et al., 2021;</ref><ref type="bibr" target="#b11">Dabre et al., 2020)</ref>. This approach leverages a single script for machine translation, capitalizing on the lexical and syntactic similarities that arise from the genetic and contactrelatedness among Indian languages <ref type="bibr" target="#b13">(Gala et al., 2023;</ref><ref type="bibr">Eriguchi et al., 2022;</ref><ref type="bibr" target="#b4">Bapna and Firat, 2019)</ref>.</p><p>In the field of LLM driven machine translation, in-context learning has gained significant attention <ref type="bibr">(Wu et al., 2023b)</ref>. The use of large language models (LLMs) for multilingual machine translation has been a subject of interest <ref type="bibr" target="#b43">(Zhang et al., 2023)</ref>. Recent studies have evaluated the translation capabilities of LLMs for different language directions, with a focus on models like ChatGPT <ref type="bibr" target="#b3">(Bang et al., 2023)</ref>. Notably, Xu et al. proposed a two-stage fine-tuning approach for machine translation using LLMs, involving fine-tuning on monolingual data followed by fine-tuning on a small set of highquality parallel data. Our work represents the first study that specifically explores machine translation involving Indian languages using large language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Large Language Models</head><p>Language modeling, a well-established task in the field of natural language processing, has garnered significant attention over the years <ref type="bibr" target="#b6">(Bellegarda, 2004;</ref><ref type="bibr" target="#b7">Bengio et al., 2000)</ref>. This task involves predicting the probability of the next token in a sequence of words. Transformers have emerged as the fundamental architecture underlying many existing Large Language Models <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>.</p><p>Transformers based autoregressive models like GPT <ref type="bibr" target="#b8">(Brown et al., 2020;</ref><ref type="bibr" target="#b28">Radford et al., 2019)</ref> have played a crucial role in advancing Natural Language Processing (NLP). GPT-3, with 175 billion parameters, is a standout in this category. It is similar in structure to GPT-2 and GPT-1 but benefits from a more extensive and varied dataset, making it exceptionally powerful in NLP. Further, prompt-based ChatGPT (GPT-3.5 text-davinci-003 and GPT-3.5 turbo) has been performing exceptionally by utilizing the reinforcement-based human feedback strategy. Although these models exhibit impressive performance on several NLP tasks, privacy and bias of the models have been a bottleneck. To mitigate such issues, LLaMA <ref type="bibr">(Touvron et al., 2023a</ref>) is an open-sourced foundation model trained on publicly available datasets. Similarly, Falcon-40B <ref type="bibr" target="#b2">(Almazrouei et al., 2023)</ref> is another open-source LLM trained on a RefinedWeb corpus of 1500 billion tokens. Falcon even comes with 7 and 40 billion instruction versions trained on conversation data.</p><p>The recent adaptation of Large Language Models (LLMs) for instruction tuning has proven to be a promising approach in improving the performance of various natural language processing tasks. Specifically, in languages like Chinese and Swedish demonstrates the impressive zero-shot and generation abilities of the low-rank adaptation of LLaMA for non-English languages <ref type="bibr" target="#b10">(Cui et al., 2023;</ref><ref type="bibr" target="#b16">Holmström and Doostmohammadi, 2023)</ref>. However, it is worth noting that the current focus of these instruction models is primarily on English. Therefore, there is an immediate need to explore ways to adapt these models to low-resource Indian languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Base models</head><p>In this work, we used the following base LLM models to test the levels of language coverage and explore their potential for machine translation tasks involving English and Indian languages.</p><p>• opt-6.7b<ref type="foot" target="#foot_6">foot_6</ref> : The OPT-6.7b <ref type="bibr" target="#b44">(Zhang et al., 2022)</ref> model has been extensively trained on the objective of causal language modeling (CLM) using English text. Although the majority of the training data is in English, a small portion of non-English data from CommonCrawl has also been included. This model utilizes 6.7 billion parameters, consisting of 32 layers and 32 attention heads, and employs an embedding size of 4096.</p><p>• Bloom-7B</p><p>8 : BLOOM (Scao et al., 2022) was the first largest multilingual large language model with causal language modeling objective and supports 46 languages and 13 programming languages. Its overall training data contains 1.1% of Indian languages. We opted for Bloom model with 7,069,016,064 parameters with 30 layers, 32 attention heads, 4096 embedding dimensional where maximum token length is 2048. • LLaMA-7B 9 : LLaMA is a collection of foundation language models ranging from 7B to 65B parameters. These models are multilingual models and trained on trillions of tokens. The data includes CCNet, C4, GitHub, Wikipedia, Books, ArXiv, Stack Exchange. In our experiments we evaluated LLaMA model with 7B parameters where 4096 is embedding dimensions and 32 layers and 32 attention head.</p><p>• MPT-7B 10 : Similar to above models MPT-7B model is trained on a large amount of data 1T tokens on causal language modeling objective.</p><p>• Falcon 11 : Falcon <ref type="bibr">(Penedo et al., 2023a</ref>) is another large language model trained on causal language modeling (CLM) objective. Here, we utilised Falcon-7B model which is a 7B parameters and trained on 1.5 trillion tokens of RefinedWeb (a novel massive web data-set based on CommonCrawl) enhanced with curated corpora. The model has multilingual capabilities but no Indian languages are explicitly present. We have used Falcon-7B for our experiments.</p><p>• LLaMA-2-7B 12 and LLaMA-2-13B 13 : LLaMA 2 based models <ref type="bibr">(Touvron et al., 2023b)</ref> are also trained on causal language modeling (CLM) objective and pretrained on 2 trillion tokens of data from publicly available sources of till September 2022. These models are available in different range parameters from 7 billion to 70 billion. These models have 4k sub-words as context length. In</p><p>9 <ref type="url" target="https://huggingface.co/decapoda-research/llama-7b-hf">https://huggingface.co/decapoda-research/  llama-7b-hf</ref> 10 <ref type="url" target="https://huggingface.co/mosaicml/mpt-7b">https://huggingface.co/mosaicml/mpt-7b</ref> 11 <ref type="url" target="https://huggingface.co/tiiuae/falcon-7b">https://huggingface.co/tiiuae/falcon-7b</ref> 12 <ref type="url" target="https://huggingface.co/meta-llama/Llama-2-7b-hf">https://huggingface.co/meta-llama/  Llama-2-7b-hf</ref> 13 <ref type="url" target="https://huggingface.co/meta-llama/Llama-2-13b-hf">https://huggingface.co/meta-llama/  Llama-2-13b-hf</ref> our experiments we have experimented with 7B and 13B LLaMA-2 models. LLaMA-2-7B network has 32 layers and 32 attention heads while LLaMA-2-13B has 40 layers and 40 attention heads. • Mistral-7B 14 : Mistral-7B Large Language Model (LLM) (Jiang et al., 2023) is a pretrained on causal language modeling (CLM) objective with 7 billion parameters. It uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost and Groupedquery attention (GQA) for faster inference which reduces the memory requirement during decoding. It has 4096 embedding dimension, 32 layers and 32 attention heads with context length of 8192 context length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Indian Languages representation in LLMs</head><p>Pre-trained (or Raw) large language models are trained on a huge amount of language data, and some of the these models are trained on multiple languages <ref type="bibr">(Naveed et al., 2023)</ref>. However, their training primarily focuses on English text <ref type="bibr">(Penedo et al., 2023b)</ref>. Emphasis on English is due to its substantial presence on the internet and its widespread usage in business contexts. For the purpose of this work, our objective is to assess the effectiveness of these models in Machine Translation tasks that involve both English and Indian Languages. Consequently, it becomes crucial to investigate the representation of Indian languages within these large language models.</p><p>An approach to investigating the representation of Indian languages within a large language model can involve analyzing the frequency of languagespecific words and sentences used during the training of these models. Unfortunately, it is not possible to perform this analysis as the training data used for these models are not publicly accessible. LLaMA-2, in particular, has mentioned that its pretraining corpus primarily consists of English and may not be optimal for other languages <ref type="bibr">(Touvron et al., 2023b)</ref>. However, it is worth mentioning that approximately 8.38% of the data does include languages other than English and codes in LLaMA-2.</p><p>On the other hand, studying the vocabulary (or letters/characters) of a corpus can provide valuable</p><p>Language Family Indo-Aryan Dravidian Sino-Tibetan Austroasiatic Language asm ban kas snd urd doi hin gom mai mar nep san guj odi pan kan mal tam tel mni brx sat Language Script Bangla Perso-Arabic Devanagari Gujarati Odia Gurmukhi Kannada Malayalam Tamil Telugu Meitei Devanagari Ol Chik No of Letters in Unicode 96 256 128 91 91 80 91 118 72 100 56 96 48 Models (Vocab) BLOOM (250680) (48,48) (49,207) (67,61) (57,34) (56,35) (55,25) (62,29) (66,52) (46,26) (61,39) (00,56) (67,29) (00,48) FALCON (65024) (00,96) (12,244) (2,126) (00,91) (00,91) (00,72) (0,100) (00,56) (02,70) (04,96) (00,56) (02,94) (00,48) LLAMA-1,2 (32024) (24,72) (45,211) (38,90) (01,90) (00,91) (04,76) (02,89) (33,155) (19,53) (01,99) (00,56) (38,90) (00,48) MISTRAL (32052) (34,62) (47,209) (43,85) (05,86) (00,91) (02,78) (18,73) (04,116) (22,50) (11,89) (00,56) (43,53) (00,48) MPT (50277) (05,91) (35,221) (22,106) (02,89) (00,91) (00,80) (00,91) (01,117) (05,67) (03,97) (00,56) (22,106) (00,48) OPT (50265) (00,96) (13,243) (1,127) (00,91) (00,91) (00,80) (00,91) (0,118) (00,72) (0,100) (00,56) (01,95) (00,48)</p><p>Table <ref type="table">1</ref>: The language support of various LLMs for 22 Indian languages, along with the corresponding families, scripts, and letters representing each language. In each tuple (xx, yy), the first value represents the number of language-specific characters, while the second value indicates the count of byte-supported characters in respective LLM and for respective language.</p><p>insights into the representation and coverage of language within that corpus. The writing system or script used plays a crucial role in representing a language. Therefore, analyzing the vocabulary can be considered a proximal task. Fortunately, we have access to the sub-word vocabulary for the considered large language models. By comparing the characters present in the sub-word vocabulary with those in the corresponding language script, we can approximate the language representation within the respective LLM.</p><p>For this work, we included a total of 22 scheduled Indian languages for translation, which can be categorized into four main language families: Indo-Aryan, Dravidian, Sino-Tibetan, and Austroasiatic. These 22 Indian languages are written using 13 major scripts. It is interesting to note that most of these scripts can be traced back to the Brahmi script<ref type="foot" target="#foot_8">foot_8</ref> , which served as the foundation for the development of several Indian scripts <ref type="bibr" target="#b31">(Salomon, 1995)</ref>. Each of these 13 writing systems has its own unique set of letters and characters <ref type="foot" target="#foot_9">16</ref> , reflecting the phonetic and linguistic characteristics of the respective languages they represent.</p><p>Table <ref type="table">1</ref> presents an overview of the scripts, the languages utilizing these scripts, and the corresponding sub-word vocabulary sizes for LLMs. The numbers indicated in '(X,Y)' represent the counts of native script letters (characters in unicode<ref type="foot" target="#foot_10">foot_10</ref> ) present and not present in the respective LLM. Specifically, X denotes the number of native language characters present in the vocabulary, while Y denotes the number of characters represented as pre-defined (multiple) hexadecimal values. Upon analysis, we observe that, in general, the 22 Indian languages have a limited presence in most of the LLMs. However, the Devanagari, Perso-Arabic, and Bangla scripts demonstrate the most extensive sub-word vocabularies, while other scripts have minimal or near-zero representation within the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment setup: Machine Translation using LLMs</head><p>To evaluate the performance of the large language models (LLMs) in machine translation tasks involving English and 22 Indian languages, we conducted two experiments. The first experiment focused on assessing the performance of the pre-trained (raw) LLM. In the second experiment, we utilized example-based in-context learning for the same machine translation task. Both translation directions were explored, including English to 22 Indian languages and 22 Indian languages to English. All experiments were conducted using translation benchmark data, as discussed in Section 6.</p><p>As part of our experimental setup, we used the prompting pipeline depicted in Figure <ref type="figure" target="#fig_0">2</ref>. This pipeline involved using a Prompt Generator to generate specific prompts for the source and target language along with source text. Subsequently, an LLM call is triggered to generate a response, which was then processed by a translation parser to obtain the actual translation. To ensure high-throughput and memory-efficient inference and serving for LLMs, we utilized the vLLM library <ref type="foot" target="#foot_11">18</ref>  <ref type="bibr" target="#b19">(Kwon et al., 2023)</ref>. We conducted all experiments using a temperature parameter of 0, which ensures that the model behaves deterministically. By setting the temperature to 0, the model is constrained to se- lect the word with the highest probability, effectively limiting its choice to the most likely option <ref type="bibr">(Aksitov et al., 2023)</ref>. All of our experiments are conducted using vLLM library on A100, 40GB GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Machine Translation on Raw LLM</head><p>To optimize the machine translation task on our selected LLMs, we conducted manual trials with various prompts. Through these trials, we discovered that directly asking for the translation and presenting the text in JSON format yielded better results, as the models seemed to comprehend the JSON structure more effectively <ref type="bibr" target="#b30">(Reinauer et al., 2023)</ref>. After multiple iterations, we finalized two prompts for translating sentences using raw (pretrained) LLMs, as illustrated in below examples. These prompts were used to evaluate the efficiency of the models. Similarly, we identified and modified the prompt for example-based in-context learning with LLM. This prompt is specified in Example above (ICL Translation Prompt). In the case of in-context learning, all of our experiments involved providing a single translation sample as a contextual learning example prior to the actual translation command. We ensured that this example remained consistent for the same language pair across the sentences. The sample itself was randomly selected from the Human-BPCC translation training corpus <ref type="bibr">(AI4Bharat et al., 2023)</ref>. We present the outcomes of both of these experiments in the Performance and Discussions section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fine-tuning LLM for Machine Translation</head><p>To examine the potential improvement in multilingual understanding or translation performance of LLMs beyond the pre-trained LLM baseline, we conducted fine-tuning experiments for the translation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Training Data</head><p>To fine-tune large language models (LLMs) for the machine translation task, we utilized the Bharat Parallel Corpus Collection (BPCC). This corpus is publicly available and specifically for English to 22 Indic languages translation.</p><p>It consists of two main parts: BPCC-Mined and BPCC-Human, comprising a total of approximately 230 million English-#Sents S-AvgL T-AvgL S-Words T-Words S-Types T-Types Assamese (asm) 138208 16.88 14.39 2333583 1988395 125480 Bangla (ban) 180219 17.80 15.07 3208203 2715959 161820 Bodo (brx) 113139 17.79 13.96 2012274 1579042 116963 Dogri (doi) 24157 15.32 17.68 370047 427110 48256 41370 Konkani (gom) 97555 17.13 14.03 1671465 1368512 82783 Gujarati (guj) 135664 17.71 15.96 2402552 2164831 123935 Hindi (hin) 222356 17.84 19.69 3966247 4378231 183737 Kannada (kan) 117222 16.83 12.44 1972881 1458053 100778 Kashmiri (kas) 19824 16.02 17.68 317634 350577 43197 66210 Maithili (mai) 23690 16.11 15.79 381720 374042 52920 57423 Malayalam (mal) 137950 16.30 11.13 2248081 1535654 120999 Marathi (mar) 175893 17.94 14.81 3154904 2604119 167822 Meitei (mni) 56617 17.77 15.73 1006271 890828 86175 Nepali (nep) 85442 16.76 14.13 1431858 1207687 105411 Odia (odi) 36923 17.07 15.49 630148 571958 68765 79932 Punjabi (pan) 80951 17.22 18.29 1394286 1480835 63510 74451 Sanskrit (san) 33189 16.30 11.69 541034 387957 61591 Santali (sat) 24368 16.95 19.28 412918 469791 51307 56053 Sindhi (sin) 10503 17.10 19.32 179592 202952 28945 30782 Tamil (tam) 150254 17.76 13.34 2668252 2004981 139214 Telugu (tel) 111808 16.81 12.64 1879737 1413466 96105 Urdu (urd) 150747 17.62 20.20 2656814 3044480 144001 Table 2: English to Indian Languages machine translation Fine-tuning data from BPCC-Human (AI4Bharat et al., <ref type="bibr">2023)</ref>. In this, the term "#Sents" refers to the total number of parallel sentences. "S-AvgL" and "T-AvgL" represent the average sentence length, in terms of words, for the source and target languages, respectively. Likewise, "Words" denotes the total number of words, while "Type" represents the total number of unique words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Hyper-parameter Value LoRA LoRA modules PEFT 19 rank 8 dropout 0.05 learning rate 1e-4 global batch size 8 epochs 6 Full-parameter FSDP learning rate 1e-4 global batch size 4 epochs 5 Table 3: Hyper-parameter configurations of LoRA based and full fine-tuning for 4*A100 40GB GPUs</p><p>parallel text pairs. For the fine-tuning process, we focused on the BPCC-Human dataset, which contains 2.2 million English-Indic pairs. Additionally, this dataset includes subsets derived from English Wikipedia sentences and everyday usage scenarios. For more information about this corpus, are presented in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Fine-tuning Details</head><p>Considering the raw LLM performance, model parameters, and resource constraints, we selected a subset of LLMs for the fine-tuning process. Specifically, we chose LLaMA-2-7b, LLaMA-2-13b, and Mistral-7B for the fine-tuning experiment.</p><p>For the selected LLMs, we decided to conduct fine-tuning using multiple parameters to enhance their performance. These parameters included bi-lingual translation fine-tuning, multi-lingual translation fine-tuning, low-rank adaptation-based fine-tuning, and a two-stage fine-tuning approach: full fine-tuning followed by low-rank adaptationbased fine-tuning. Due to limitations in training resources, we prioritized full fine-tuning as the chosen option.</p><p>Specifically, we performed LoRa-based finetuning <ref type="bibr" target="#b17">(Hu et al., 2021)</ref> for all English to 22 Indian languages (in both directions) under bi-lingual settings using LLaMA-2-7b and LLaMA-2-13b.</p><p>Additionally, we conducted</p><p>TestSet #Sent Details IN22_conv_test 1502 AI4Bharat et al. released MT benchmark data covering English to 22 Indian Languages. IN22_gen_test 1023 Flores200-dev 997 Goyal et al. released MT benchmark data which includes English to 17 Indian Language pairs considered in this work. Flores200-devtest 1012 Newstest2019 1997 Federmann et al. released MT benchmark data which includes English to 10 Indian Language pairs considered in this work.</p><p>Table 4: Benchmark data details covering English to 22 Indian Languages multi-lingual LoRa-based fine-tuning for English to the combined 22 Indian languages, as well as for the combined 22 Indian languages to English, using LLaMA-2-7b, LLaMA-2-13b, and Mistral-7B. Based on the overall performance, we proceeded with a two-stage fine-tuning approach for the multi-lingual translation task specifically on LLaMA-2-13b. In the first stage, we performed full fine-tuning as a multi-lingual translation setup. Subsequently, in the second stage, we conducted multi-lingual LoRa-based fine-tuning on the same fully fine-tuned model.</p><p>For both types of fine-tuning LLMs, we utilized the llama-recipes codebase<ref type="foot" target="#foot_12">foot_12</ref> which provides an efficient implementation for LoRa-based adaptor fine-tuning with PEFT <ref type="bibr" target="#b20">(Mangrulkar et al., 2022)</ref>. For more details, please refer to the llama-recipes documentation <ref type="foot" target="#foot_13">21</ref> . The hyperparameters for the fine-tuning process are specified in Table <ref type="table">3</ref>. The training data used for the fine-tuning experiments will be presented in the sub-section 5.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Machine Translation Benchmark Data</head><p>We evaluate the performance of multilingual translation using three different benchmark datasets, as outlined in Table <ref type="table">4</ref>. The table provides a compre-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Performance Evaluation</head><p>We evaluated the performance of the translation outputs using BLEU <ref type="bibr" target="#b23">(Papineni et al., 2002)</ref> and chrF <ref type="bibr" target="#b26">(Popović, 2015)</ref> evaluation methods on benchmark data described in Section 6. However, we did not include COMET <ref type="bibr" target="#b29">(Rei et al., 2022)</ref> as an evaluation method due to the absence of support for many low-resource Indian languages at the time of evaluation. We used sacreBLEU library <ref type="bibr" target="#b27">(Post, 2018)</ref> for BLEU 22 and chrF 23 calculation. To mitigate the impact of randomness in scores, we present our findings as the average of two runs for all of our results. observed amplified performance for the Bloom large language model for certain languages, which can be attributed to the known MT benchmark data leak in the pre-training <ref type="bibr" target="#b45">(Zhu et al., 2023)</ref>. Consequently, we decided to exclude this language model from further experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw (Zero shot) vs ICL based Translation on LLMs</head><p>LLMs models such as OPT, MPT, LLAMA-1 and Falcon exhibited poor performance, which can be correlated with the no or minimal presence of characters of our focused Indian Languages in their vocabulary (Table <ref type="table">1</ref>). Therefore, we have omitted reporting the results for these models. Figure <ref type="figure" target="#fig_2">3</ref> indicates that the Llama-2 models show relatively better performance with ICL settings compared to the raw models. Detailed results are presented in appendix.</p><p>Through manual analysis, we observed that for less-represented languages such as Gujarati, Kannada, Odia, etc. (Table <ref type="table">1</ref>), the ICL-driven translation tends to repeat the same translation given in the context as learning. On the other hand, the raw models tend to hallucinate and repeat words throughout the translation <ref type="bibr" target="#b15">(Guerreiro et al., 2023)</ref>.</p><p>One important finding from the manual analysis is that these raw LLMs demonstrate the ability to accurately identify languages (e.g., when asked for Gujarati translation, it gives inaccurate translations but correctly hallucinate text in the Gujarati script). This is a positive aspect and indicates a significant advantage of these LLMs in terms of their understanding and differentiation of languages and language scripts. In response to the question asked in Introduction, it is true that the major available LLMs are primarily focused on English. However, they do exhibit minimal potential for zero-shot and example-based translation capabilities.</p><p>Fine-Tuned LLM driven Translations: English to Indian Languages We conducted an evaluation to compare the performance of our Fine-Tuned LLM models with GPT-3.5, as both models use the same decoder-based approach. Figure <ref type="figure" target="#fig_3">4</ref> illustrates the comparison for English to 22 Indian language translation. The scores for GPT-3.5 are generally lower compared to our fine-tuned methods, also our fine-tuned models have higher numbers than our previously mentioned zero-shot and example-based learning baseline. This indicates that with minimal translation corpora, we are able to achieve considerable translations for translating into Indian languages from English. Additionally, we observed that multilingual fine-tuning yielded better overall performance compared to bilingual fine-tuning. The two-stage fine-tuning approach also outperformed other fine-tuning methods for the translation task. The impressive results of the two-stage fine-tuning approach, as shown in Figure <ref type="figure" target="#fig_3">4</ref>, are comparable to those of traditional encoder-decoder based translation models. It is worth noting that this performance improvement was achieved using only a few thousand parallel data, whereas traditional NMT models typically require a larger amount of data. From Figure <ref type="figure" target="#fig_3">4</ref>, we can see that translating to low-resource languages such as Dogri, Konkani, Kashmiri, Meitei, Sanskrit, and Sindhi yielded favorable evaluation numbers (Detailed results are presented in appendix) compared to existing translation systems. In answer to the question posed in the introduction, fine-tuning LLMs does enhance translation capabilities, particularly more when employing multilingual fine-tuning. These models demonstrate proficiency in translating low-resource languages as well.</p><p>Fine-Tuned LLM driven Translations: Indian Languages to English Figure <ref type="figure" target="#fig_4">5</ref> showcases the comparison for Indian language to English translation. The scores for GPT-3.5 are generally not higher compared to our fine-tuned methods, while our fine-tuned models still outperform the previously mentioned zero-shot and example-based context learning driven LLM results. Notably, the performance improvement for Indian language to English translation is comparatively lower than that of English to Indian language translation. Compared to translations from English to Indian languages, the LoRa-based single-stage fine-tuning here performs the best among all the fine-tuning approaches. Detailed results are presented in the appendix.</p><p>This disparity can be attributed to the vocabulary representation of Indian languages in these LLMs. As presented in Table <ref type="table">1</ref>, the subword vocabulary for Indian languages is limited in the considered LLMs. Consequently, when processing input in Indian languages, characters that are not present in the vocabulary receive multiple hexadecimal representations from the vocabulary. This creates a bottleneck in understanding the underlying meaning, making it challenging for the larger LLM network to establish corresponding semantic translations.</p><p>However, this issue does not arise when translating from English to Indian languages. The underlying understanding of English is robust, allowing the network to effectively map the respective language translations. Hence, this suggest the need for LLMs where enough language representation is required and future development of LLMs must address this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitations</head><p>In order to conduct our experiments, we relied on high-performance GPUs, specifically the A100-40GB. However, we acknowledge that not everyone may have access to such powerful computing resources, making it challenging to reproduce our experiments and achieve identical results. To overcome this limitation, our objective is to provide open access to all outputs, including model and results, to facilitate further research and exploration. By making these resources openly available, we aim to promote collaboration and enable others to build upon our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>Our experiments and results have provided promising insights into the use of LLMs for translation tasks. We have found that LLMs have the potential to perform translations involving English and Indian languages without the need for an extensive collection of parallel data, which distinguishes them from traditional translation models. Furthermore, our findings indicate that LLaMA-2 based models outperform other models in zero-shot and in-context example-based learning. Notably, the LLaMA-2-13b based model demonstrates superior performance compared to its counterparts. To enhance the LLM's understanding of English and Indian languages, we have introduced a two-stage fine-tuning process. This process begins with initial full-finetuning, followed by LoRa-based fine-tuning. Through this approach, we have significantly improved the LLM's comprehension of content in both languages. However, our experiments suggest that further work on LLMs is required to surpass the performance of traditional encoder-decoder based translation models. This work could involve the development of Indian language-specific LLMs, which would enhance vocabulary and alphabet coverage, resulting in better representation of Indian languages.</p><p>On the other hand, in the future, we plan to incorporate Indian to Indian language translation using LLMs. Additionally, our aim is to develop a single LLM model capable of translating all Indian languages, as well as English, in both directions. By doing so, we strive to push the boundaries of language capabilities within LLMs and further advance the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Examples DataSet Model asm ban bod doi kon guj hin kan kas mai mal mar mei nep odi pun san sat sin tam tel urd IN22_conv GPT-3.5 2.4 9.6 -1.2 0.2 11.1 22.3 2.6 0.1 1.6 1.6 5.7 0.1 9.5 2.3 12.3 0.5 --2.8 4.7 21.9 IndicTrans-2 15.9 16.6 12 26.1 13.4 26.8 27.6 5.4 2.7 17.2 5.5 18.8 7.1 19.4 9.4 30 5.4 6.3 5.2 7.4 13.5 38.4 Google Translate 13.9 --14.3 11.9 26.6 28.8 5.2 -9.2 5.5 17.6 -14.5 9.3 ----8 13.1 37.1 LTRC, IIIT-H -----17.1 22.5 3.4 --3.5 11.9 -------5.7 10.2 21.8 SeamlessM4T 16.2 15.6 0 0 0 -24.5 4.7 0 15.4 5.5 18 0 15.7 12.6 28.4 0 0 0 7.2 9.2 28 Llama-2-7b+lora(BI) 4.88 4.31 7.73 4.08 2 6.01 19.16 2.47 0.21 3.51 1.3 5.14 0.04 7.96 2.89 4.83 1.39 0.14 0.27 1.61 2.65 9.1 Llama-2-7b+lora(Multi) 5.22 5.7 3.77 5.11 3.19 6.36 15.84 2.57 0.27 4.07 1.72 6.31 0.1 8.17 3.62 5.56 1.06 0.03 0.53 1.4 2.8 11.32 Llama-2-13b+lora(BI) 9.16 8.29 9.97 9.93 3.81 10.25 21.06 3.33 0.61 6.94 2.29 8.61 1.05 11.37 4.81 9.1 0.16 0.31 0.28 2.95 5.22 17.49 Llama-2-13b+lora(Multi) 8.24 6.71 5.34 8.2 4.56 9.45 19.36 2.83 0.46 4.68 2.42 7.66 0.99 10.42 4.38 9.6 2.33 0.09 1.59 1.84 3.89 16.88 Llama-2-13b+FF+lora(Multi) 15.89 14.31 13.74 25.42 11.42 18.52 23.74 5.73 4.66 14.83 4.76 15.87 8.46 18.58 11.17 22.38 5.73 8.83 6.52 5.38 9.06 30.35 Mistral-7B-v0.1+lora(Multi) 5.25 6.46 2.03 4.18 2.64 6.06 15.63 2.14 0.11 2.77 2.12 6.11 0.02 6.92 1.54 5.68 1.05 0.01 0.54 1.43 1.75 8.71 IN22_gen GPT-3.5 2.9 8.7 0.2 2.8 1.4 8.4 22.4 4.6 0.6 4.6 3.3 5.5 -8 3.4 9.6 0.9 -0.1 3.5 5.7 20 IndicTrans-2 17.4 16.4 15.1 29.4 18.3 25.4 32.8 14.8 6.4 18.1 12.4 21.2 9.8 15.4 11.7 22.1 8.5 5.3 13.3 14 18.2 45.9 Google Translate 13.8 --19.8 11.4 22.7 29.1 11.6 -8.4 10.5 15.6 -12.6 9.9 ----14 16.9 40.6 LTRC, IIIT-H -----14 24.7 6 --4.8 9.5 -------10 12.5 26.3 SeamlessM4T 12.6 13 0 0 0 19.4 27.4 11.3 0 14.4 10 14.7 0 14.1 13.6 21.6 0 2.3 0.5 13 15.7 35.3 Llama-2-7b+lora(BI) 6.22 5.84 8.48 5.06 3.1 5.19 20.16 4.41 0.63 4.51 2.95 8.21 0.2 7 4.57 4.23 3.54 0.14 1.01 2.99 3.86 10.68 Llama-2-7b+lora(Multi) 8.99 7.78 6.02 7.93 6.42 8 17.01 6.6 1.32 7.21 4.52 10.03 0.17 8.37 5.65 5.24 3.35 0.05 2.66 3.05 4.93 12.13 Llama-2-13b+lora(BI) 9.65 9.55 12 10.53 6.3 8.39 23.12 7.12 1.73 8.17 4.5 10.9 3.19 10.84 8.02 6.9 0.7 0.55 2.82 5.12 6.46 18.75 Llama-2-13b+lora(Multi) 10.66 9.7 7.46 10.98 8.88 9.73 20.66 7.45 1.97 7.12 5.7 12.34 2.02 10.46 7.56 7.67 4.93 0.08 4.66 4.44 6.03 17.1 Llama-2-13b+FF+lora(Multi) 17.18 16.11 16.08 27.4 15.06 16.26 27.01 14.22 7.1 17.53 11.3 20.31 11.72 17.39 15.2 15.74 10.4 7.07 11.3 10.75 12.55 32.88 Mistral-7B-v0.1+lora(Multi) 8.07 7.1 3.63 7.04 6.37 7.78 16.04 4.81 0.61 5.87 3.65 9.59 0.03 7.23 3.06 4.37 2.86 0.03 2.65 2.4 4.04 8.05 flores200-dev GPT-3.5 1.6 8.4 ---8.6 23.3 6.9 0.5 4.3 3.3 4 0 6.6 2.1 11.6 0.5 0 0 2.9 5.3 14.9 IndicTrans-2 9.5 21 ---27.1 36.8 21 7.7 17.5 20.6 19.3 -22.8 16 28.9 2.6 3.3 0 23 25.1 27.3 Google Translate 7.7 ----26.6 36.8 22.9 -9.7 22.1 20.6 -21.3 24.6 ----22.4 25.4 27.4 LTRC, IIIT-H -----18.1 33.1 10 --4.1 14.4 -------15.9 20.4 17.7 SeamlessM4T 9 18.5 ---24 35.3 19.8 0 14.4 16.6 18.1 0 18.5 17.2 27.8 0 0 0 20.3 23 24 Llama-2-7b+lora(BI) 2.8 4.88 ---5.34 22.78 2.94 0.29 2.89 1.97 4.69 0.07 4.91 2.41 4.31 0.62 0 0.06 3.15 4.13 7.77 Llama-2-7b+lora(Multi) 3.82 6.11 ---6.71 17.53 4.01 0.76 5.23 2.35 6.03 0.11 5.9 3.46 5.31 0.66 0.06 0.1 3.31 4.8 8.79 Llama-2-13b+lora(BI) 5 8.18 ---9.28 24.9 5.65 0.82 5.53 4.22 7.22 0.09 7.84 4.73 7.76 0.09 0.4 0.06 5.75 7.13 12.69 Llama-2-13b+lora(Multi) 4.99 7.76 ---8.6 20.67 5.02 1.21 6.55 3.05 7.82 0.12 8.49 4.99 7.84 0.92 0.08 0.13 4.58 6.16 11.79 Llama-2-13b+FF+lora(Multi) 9.08 13.7 ---17.48 29.16 12.32 3.35 11.88 11.36 14.65 0.05 15.84 13.24 20.79 2.09 4.43 0.14 13.28 15.97 21.64 Mistral-7B-v0.1+lora(Multi) 3.01 5.26 ---6.64 15.75 3.58 0.39 3.89 1.85 5.23 0.05 5.24 1.61 4.33 0.47 0.01 0.09 2.26 3.18 5.51 flores200-devtest GPT-3.5 1.8 8.2 ---9.6 23.9 7 0.3 4.1 3 5.4 0 7.5 3.3 11.2 0.7 0 0 3.3 5.5 16.6 IndicTrans-2 9.6 21.2 ---27.4 36.6 22.7 6.8 17.2 20.3 19.6 -23.1 15.7 26.1 3 3.4 0 22.4 26.7 26.3 Google Translate 8.1 ----27 36.2 24.1 -10.3 21.2 20.3 -21.5 23.4 ----21 26.5 25.2 LTRC, IIIT-H -----18 32.7 11.6 --3.9 14.7 -------15.3 20.9 17 SeamlessM4T 8.8 18.8 ---24.4 34.8 20.5 0 14.6 16.6 17.8 0 19.6 16.4 25.3 0 0 0 19.7 24.4 22.9 Llama-2-7b+lora(BI) 3 4.93 ---6.09 21.9 3.41 0.27 3.15 2.37 4.83 0.07 5.24 2.23 4.45 0.37 0.09 0.08 2.94 4.44 7.02 Llama-2-7b+lora(Multi) 3.63 5.92 ---6.89 16.77 4.2 0.62 5.22 2.58 5.91 0.17 6.25 3.47 5.11 0.51 0.03 0.14 2.92 5.24 7.78 Llama-2-13b+lora(BI) 4.69 8.11 ---9.31 23.71 5.97 0.9 5.41 4.08 7.28 0.14 8.94 4.47 7.24 0.08 0.35 0.15 5.58 7.4 12.31 Llama-2-13b+lora(Multi) 4.89 8.3 ---9.14 19.88 5.27 0.98 6.18 3.26 7.3 0.25 7.74 4.45 7.42 0.98 0.04 0.16 4.62 6.86 11.81 Llama-2-13b+FF+lora(Multi) 9.33 13.55 ---17.22 28.5 13.27 3.32 11.76 11.34 14.56 0.06 16.21 12.9 19.64 2.06 4.27 0.28 12.78 16.61 25.96 Mistral-7B-v0.1+lora(Multi) 3.26 5.04 ---6.39 15.07 3.49 0.31 3.78 1.96 5.06 0.08 5.69 1.53 4.59 0.53 0.01 0.11 2.37 3.55 5.19 DataSet Model asm ban bod doi kon guj hin kan kas mai mal mar mei nep odi pun san sat sin tam tel urd IN22_conv GPT-3.5 19.9 29.8 2.8 19.9 9.7 28.3 34.1 17.8 6.2 14 20.7 24 0.4 29.3 21.1 30.9 14.5 0.2 9.6 15.4 19.9 34.7 IndicTrans-2 43.9 36.9 35.3 45.5 28.9 40.9 38.7 25.1 31.8 35.3 31.3 37 32.5 43.1 38.9 42.8 25.8 24.2 26 22.6 30.8 46.1 Google Translate 44.5 37.6 1.8 42.3 29.5 40.9 39.4 24.3 5.6 36.4 31.1 37.6 25.7 43 37.4 39.4 26.7 0 8.7 23.3 31.5 45.6 LTRC, IIIT-H ------23.9 10.5 --14.6 19.1 --------14.1 -SeamlessM4T 41 35.9 0 0 0 40.6 38.1 23 0 34 30.3 35.5 0.2 40.3 38.6 41.4 0 17.2 9.3 23.1 31.1 42.3 Llama-2-7b+lora(BI) 1.17 2.42 4.04 9.49 4.43 3.88 15.72 1.45 2.17 3.08 1.7 7 0.08 6.22 1.07 2.29 4.62 0.03 3.45 0.81 0.84 6.33 Llama-2-7b+lora(Multi) 12.43 7.71 8.9 10.16 6.14 5.09 7.68 4.25 4.33 9.79 3.34 9.91 1.15 12.36 6.23 6.55 5.14 0.26 3.75 3.47 5.33 13.75 Llama-2-13b+lora(BI) 2.49 3.12 15.01 0.9 2.11 1.26 25.04 0.82 2.93 2.86 8.71 9.44 0.31 1.49 1.07 1.61 ----1.34 6.72 Llama-2-13b+lora(Multi) 21.19 20.26 15.51 18.4 13.37 17.06 23.66 8.15 8.54 15.77 12.85 17.37 2.93 22.89 11.77 15.41 9.85 0.72 8.17 9.27 11.53 24.16 Llama-2-13b+FF+lora(Multi) 2.26 1.77 1.48 2 1.58 0.84 7.05 0.8 0.94 1.9 5.99 2.06 0.48 2.71 1.55 1.52 1.35 0.13 1.81 0.84 0.92 2.31 Mistral-7B-v0.1+lora(Multi) 12.23 9.19 8.55 11.63 7.46 2.38 14.81 3.62 4.69 12.91 3.5 11.2 0.43 17.9 5.51 1.69 11.03 0.1 3.84 3.04 2.4 18.58</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Prompting Mechanism for Translation</figDesc><graphic coords="6,93.55,70.87,408.20,103.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Example</head><figDesc>Language&gt; to &lt;Target Language&gt; translation for "&lt;Source Exam-ple&gt;" is "&lt;Target Example&gt;" from &lt;Source Language&gt;, following that, translate this to &lt;Target Language&gt; from &lt;Source Language&gt; Text: &lt;Source Language Text&gt; Translated Text:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Evaluation of English -22 Indic language Translation over 5 benchmark-sets (averaged): Raw LLM vs In Context Learning (ICL); Raw LLM models: LLaMA-2-7b, LLaMA-2-13b)</figDesc><graphic coords="8,297.64,159.66,226.78,140.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance comparison of GPT-3.5 vs our Fine-Tuned LLM Translation models (LLaMA-2-7b+lora (Multi), LLaMA-2-13b+lora (Multi), and LLaMA-2-13b+FF+lora (Multi)): English to 22 Indian Languages over 5 benchmark-sets (averaged). Here, LORA stands for Low-Rank Adaptation of Large Language Models based finetuning. Multi stands for the multilingual model, FF for full-finetuning, and FF+lora stands for 2-stage fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance comparison of GPT-3.5 vs our Fine-Tuned LLM Translation models (LLaMA-2-7b+lora (Multi), LLaMA-2-13b+lora (Multi), and LLaMA-2-13b+FF+lora (Multi)): English to 22 Indian Languages over 5 benchmark-sets (averaged). Here, LORA stands for Low-Rank Adaptation of Large Language Models based fine-tuning. Multi stands for the multilingual model.</figDesc><graphic coords="10,70.87,70.86,226.77,140.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="14,92.18,358.75,408.18,133.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="14,92.18,516.28,408.19,122.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="15,92.18,147.63,408.19,146.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="15,92.18,456.08,408.18,239.99" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://en.wikipedia.org/wiki/Linguistic_ Survey_of_India</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://translate.google.co.in/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://chat.openai.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/AI4Bharat/IndicTrans2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://ssmt.iiit.ac.in/translate</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/facebookresearch/seamless_ communication</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://huggingface.co/facebook/opt-6.7b</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://huggingface.co/bigscience/bloom-7b1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_8"><p>https://www.education.gov.in/sites/upload_ files/mhrd/files/upload_document/languagebr.pdf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_9"><p>https://en.wikipedia.org/wiki/Official_ scripts_of_the_Republic_of_India</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_10"><p>https://unicode.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_11"><p>https://github.com/vllm-project/vllm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_12"><p>https://github.com/facebookresearch/ llama-recipes/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_13"><p>https://github.com/facebookresearch/ llama-recipes/blob/main/docs/LLM_finetuning.md hensive overview of each dataset, highlighting the availability of n-way parallel data for the specified number of Indian languages from English as a source direction.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We express our gratitude to <rs type="person">Pruthwik Mishra</rs>, <rs type="person">Arafat Ahsan</rs> and <rs type="person">Palash Gupta</rs> for their contributions throughout the different phases of this project. This undertaking is funded by the <rs type="funder">Ministry of Electronics and Information Technology, Government of India</rs>, as evidenced by the Sanction Order: <rs type="grantNumber">11(1)/2022-HCC(TDIL)-Part(2)/A/B/C</rs> and the Administrative Approval: 11(1)/2022-HCC(TDIL)-Part(2).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EdnPu6v">
					<idno type="grant-number">11(1)/2022-HCC(TDIL)-Part(2)/A/B/C</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The symbol '-' indicates that the benchmark dataset for a particular language or machine translation system was not available during the evaluation period. Here, LORA stands for Low-Rank Adaptation of Large Language Models based fine-tuning. Multi stands for the multilingual model, FF for full-finetuning, and FF+lora stands for 2-stage fine-tuning.  The symbol '-' indicates that the benchmark dataset for a particular language or machine translation system was not available during the evaluation period. Here, LORA stands for Low-Rank Adaptation of Large Language Models based fine-tuning. Multi stands for the multilingual model, FF for full-finetuning, and FF+lora stands for 2-stage fine-tuning.   27 The symbol '-' indicates that the benchmark dataset for a particular language or machine translation system was not available during the evaluation period. Here, LORA stands for Low-Rank Adaptation of Large Language Models based fine-tuning. Multi stands for the multilingual model, FF for full-finetuning, and FF+lora stands for 2-stage fine-tuning.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jay</forename><surname>Ai4bharat</surname></persName>
		</author>
		<author>
			<persName><surname>Gala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pranjal</surname></persName>
		</author>
		<author>
			<persName><surname>Chitale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Doddapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gumma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aswanth</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janki</forename><surname>Nawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupama</forename><surname>Sujatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Vivek Raghavan</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><surname>Khapra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16307</idno>
		<title level="m">Raj Dabre, and Anoop Kunchukuttan. 2023. Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Siamak Shakeri, and Yunhsuan Sung. 2023. Characterizing attribution and fluency tradeoffs for retrievalaugmented large language models</title>
		<author>
			<persName><forename type="first">Renat</forename><surname>Aksitov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Reitter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05578</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Falcon-40b: an open large language model with state-of-theart performance</title>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulaziz</forename><surname>Alshamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Merouane</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Goffinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Heslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Technology Innovation Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Wilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holy</forename><surname>Lovenia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willy</forename><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04023</idno>
		<title level="m">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring massively multilingual, massive neural machine translation</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Google AI Blog</title>
		<imprint>
			<date type="published" when="2019-10-11">2019. October, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">André Martins</title>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Machine Translation</title>
		<editor>
			<persName><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</editor>
		<meeting>the Fifth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Statistical language model adaptation: review and perspectives</title>
		<author>
			<persName><surname>Jerome R Bellegarda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="93" to="108" />
		</imprint>
	</monogr>
	<note>Speech communication</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<title level="m">A neural probabilistic language model. Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03109</idno>
		<title level="m">A survey on evaluation of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08177</idno>
		<title level="m">Efficient and effective text encoding for chinese llama and alpaca</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building multilingual machine translation systems that serve arbitrary xy translations</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.14982</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020">2020. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A survey of multilingual neural machine translation Akiko Eriguchi, Shufang Xie, Tao Qin, and Hany Hassan Awadalla</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NTREX-128 -news test references for MT evaluation of 128 languages</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Scaling Up Multilingual Evaluation</title>
		<meeting>the First Workshop on Scaling Up Multilingual Evaluation</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Indictrans2: Towards highquality and accessible machine translation models for all 22 scheduled indian languages</title>
		<author>
			<persName><forename type="first">Jay</forename><surname>Gala</surname></persName>
		</author>
		<author>
			<persName><surname>Chitale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Doddapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gumma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aswanth</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janki</forename><surname>Nawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupama</forename><surname>Sujatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName><surname>Vivek Raghavan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16307</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Flores-101 evaluation benchmark for low-resource and multilingual machine translation</title>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjana</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00474</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="522" to="538" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Duarte</forename><surname>Nuno M Guerreiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Waldendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André Ft</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><surname>Martins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.16104</idno>
		<title level="m">Hallucinations in large multilingual translation models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Making instruction finetuning accessible to non-English languages: A case study on Swedish models</title>
		<author>
			<persName><forename type="first">Oskar</forename><surname>Holmström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Doostmohammadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)</title>
		<meeting>the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)<address><addrLine>Tórshavn, Faroe Islands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="634" to="642" />
		</imprint>
		<respStmt>
			<orgName>University of Tartu Library</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient memory management for large language model serving with pagedattention</title>
		<author>
			<persName><forename type="first">Woosuk</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="DOI">10.1145/3600006.3613165</idno>
		<idno type="arXiv">arXiv:2304.05613</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning</title>
		<meeting>the ACM SIGOPS 29th Symposium on Operating Systems Principles. Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Peft: State-of-the-art parameterefficient fine-tuning methods</title>
		<author>
			<persName><forename type="first">Sourab</forename><surname>Mangrulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayak</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Bossan</surname></persName>
		</author>
		<ptr target="https://github.com/huggingface/peft" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reliable and safe use of machine translation in medical settings</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Mehandru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niloufar</forename><surname>Salehi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3531146.3533244</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2022 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2016" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Muhammad Saqib</title>
		<author>
			<persName><forename type="first">Humza</forename><surname>Naveed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asad</forename><surname>Ullah Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.06435</idno>
	</analytic>
	<monogr>
		<title level="m">Saeed Anwar, Muhammad Usman, Nick Barnes, and Ajmal Mian. 2023. A comprehensive overview of large language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01116</idno>
		<title level="m">Ebtesam Almazrouei, and Julien Launay. 2023a. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01116</idno>
		<title level="m">Ebtesam Almazrouei, and Julien Launay. 2023b. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">chrf: character n-gram f-score for automatic mt evaluation</title>
		<author>
			<persName><forename type="first">Maja</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth workshop on statistical machine translation</title>
		<meeting>the tenth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">COMET-22: Unbabel-IST 2022 submission for the metrics shared task</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duarte</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chrysoula</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Zerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taisiya</forename><surname>Farinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Glushkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Machine Translation (WMT)</title>
		<meeting>the Seventh Conference on Machine Translation (WMT)<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates (Hybrid). Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="578" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural machine translation models can learn to be few-shot learners</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Reinauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Simianer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaden</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">Em</forename><surname>Mosig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.08590</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the origin of the early indian scripts</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Salomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Oriental Society</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="271" to="279" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Gallé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m">Bloom: A 176bparameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Overcoming language barriers in academia: Machine translation tools and a vision for a multilingual future</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Steigerwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valeria</forename><surname>Ramírez-Castañeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Débora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">András</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><forename type="middle">Teresa</forename><surname>Báldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lynne</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">D</forename><surname>Bowker</surname></persName>
		</author>
		<author>
			<persName><surname>Tarvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioScience</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="988" to="998" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models</title>
		<author>
			<persName><forename type="first">Priyan</forename><surname>Vaithilingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><forename type="middle">L</forename><surname>Glassman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491101.3519665</idno>
	</analytic>
	<monogr>
		<title level="m">Chi conference on human factors in computing systems extended abstracts</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04239</idno>
		<title level="m">A survey on low-resource neural machine translation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">OpenICL: An open-source framework for in-context learning</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Zhenyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-demo.47</idno>
		<idno type="arXiv">arXiv:2303.13648</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="489" to="498" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark System Demonstrations) Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senjie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enyu</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.07864</idno>
		<title level="m">The rise and potential of large language model based agents: A survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A paradigm shift in machine translation: Boosting translation performance of large language models</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Sharaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Hassan Awadalla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.11674</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A systematic evaluation of large language models for natural</title>
		<author>
			<persName><forename type="first">Ni</forename><surname>Xuanfan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Piji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Chinese National Conference on Computational Linguistics</title>
		<meeting>the 22nd Chinese National Conference on Computational Linguistics<address><addrLine>Harbin, China</addrLine></address></meeting>
		<imprint>
			<publisher>Chinese Information Processing Society of China</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="40" to="56" />
		</imprint>
	</monogr>
	<note>Frontier Forum)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Prompting large language model for machine translation: A case study</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.07069</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<title level="m">Opt: Open pre-trained transformer language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.04675</idno>
		<title level="m">Multilingual machine translation with large language models: Empirical results and analysis</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Language as an interstate migration barrier-the interesting case of india. Eastern</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Zieliński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Transnational Relations</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Llama-2-13b+lora(Multi)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Llama-2-13b+FF+lora</title>
		<author>
			<orgName type="collaboration">Multi</orgName>
		</author>
		<imprint>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Llama-2-13b+lora(Multi)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Iiit-H -</forename><surname>Ltrc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="0715">20.7 15</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="4" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">68 Mistral-7B-v0.1+lora(Multi)</title>
		<idno>12.36 10.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
