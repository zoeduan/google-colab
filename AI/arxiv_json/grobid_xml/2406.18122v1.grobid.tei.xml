<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Poisoned LangChain: Jailbreak LLMs by LangChain</title>
				<funder ref="#_Kywccan #_uJawFsu">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-26">26 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziqiu</forename><surname>Wang</surname></persName>
							<email>ziqiuwang@stu.hubu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
							<email>junliu@stu.hubu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Shengkai</forename><surname>Zhang</surname></persName>
							<email>shengkai@whut.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<email>yangyang@hubu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Intelligent Sensing System and Security (Ministry of Education) School of Artificial Intelligence</orgName>
								<orgName type="institution">Hubei University Wuhan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Intelligent Sensing System and Security (Ministry of Education) School of Artificial Intelligence</orgName>
								<orgName type="institution">Hubei University Wuhan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Key Laboratory of Intelligent Sensing System and Security (Ministry of Education) School of Artificial Intelligence</orgName>
								<orgName type="institution">Wuhan University of Technology Wuhan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Hubei University Wuhan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Poisoned LangChain: Jailbreak LLMs by LangChain</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-26">26 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">23DB5AB32FBC8D7E692E08A1B636276B</idno>
					<idno type="arXiv">arXiv:2406.18122v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Jailbreak</term>
					<term>Large language models</term>
					<term>Retrieval-Augmented Generation</term>
					<term>LangChain</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the development of Natural Language Processing (NLP), Large Language Models (LLMs) are becoming increasingly popular. LLMs are integrating more into everyday life, raising public concerns about their security vulnerabilities. Consequently, the security of large language models is becoming critically important. Currently, the techniques for attacking and defending against LLMs are continuously evolving. One significant method type of attack is the jailbreak attack, which designed to evade model safety mechanisms and induce the generation of inappropriate content. Existing jailbreak attacks primarily rely on crafting inducement prompts for direct jailbreaks, which are less effective against large models with robust filtering and high comprehension abilities. Given the increasing demand for real-time capabilities in large language models, real-time updates and iterations of new knowledge have become essential. Retrieval-Augmented Generation (RAG), an advanced technique to compensate for the model's lack of new knowledge, is gradually becoming mainstream. As RAG enables the model to utilize external knowledge bases, it provides a new avenue for jailbreak attacks.</p><p>In this paper, we conduct the first work to propose the concept of indirect jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on this, we further design a novel method of indirect jailbreak attack, termed Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to interact with large language models, thereby causing the large models to generate malicious non-compliant dialogues.We tested this method on six different large language models across three major categories of jailbreak issues. The experiments demonstrate that PLC successfully implemented indirect jailbreak attacks under three * Indicates corresponding author. different scenarios, achieving success rates of 88.56%, 79.04%, and 82.69% respectively. Experimental results and other resources: <ref type="url" target="https://github.com/CAM-FSS/jailbreak-langchain">https://github.com/CAM-FSS/jailbreak-langchain</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the ongoing transformation towards global digitization, artificial intelligence, particularly large language models (LLMs), has emerged as a pivotal force in the realm of natural language processing. Prominent examples include OpenAI's GPT series <ref type="bibr" target="#b18">[20]</ref> and Meta's LLaMA series <ref type="bibr" target="#b17">[19]</ref>. These models have increasingly permeated various sectors, such as education <ref type="bibr" target="#b15">[17]</ref>, industry <ref type="bibr" target="#b9">[11]</ref>, and decision-making <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b26">28]</ref>, where they aim to deliver precise and seamless interactive experiences for users worldwide. Given their significant influence and broad adoption, the security and integrity of LLMs have become essential considerations in their development and deployment.</p><p>Due to limitations in training datasets and inherent factors in algorithm design, existing large language models (LLMs) exhibit certain security vulnerabilities, including the phenomenon known as "jailbreaking". Jailbreak attacks <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b14">16]</ref> aim to craft prompts that circumvent the security mechanisms of LLMs by designing malicious queries. This vulnerability stems from the inadequate scrutiny of content sources during the retrieval process, which allows individuals to bypass LLM security measures and induce the generation of content that violates usage policies. To address these security issues, it is crucial for model practitioners to conduct comprehensive analyses of the models' defensive capabilities to identify potential weaknesses and enhance security mechanisms <ref type="bibr" target="#b19">[21]</ref>. Typical analytical workflows involve collecting a corpus of jailbreak prompts <ref type="bibr" target="#b25">[27]</ref> and establishing robust post-detection mechanisms <ref type="bibr" target="#b5">[7]</ref>. With the implementation of various defensive measures, security filters have been enhanced, significantly mitigating the effectiveness of jailbreak attacks.</p><p>On the other hand, the public's increasing demand for large language models (LLMs) to handle private domain information and real-time iterative updates has necessitated the integration of external knowledge bases. Retrieval-Augmented Generation (RAG) <ref type="bibr" target="#b10">[12]</ref>, a sophisticated technique designed to address the lack of new knowledge in models, has become mainstream and is widely adopted. RAG enhances models'output by generating accurate and contextually relevant responses using external knowledge, and it is used in various applications such as customer service chatbots <ref type="bibr" target="#b21">[23]</ref>, document retrieval bots for databases <ref type="bibr" target="#b23">[25]</ref>, and psychological counseling tools. However, as LLMs are deployed in increasingly complex scenarios with sophisticated integrated strategies, their previously robust defensive mechanisms have begun to show vulnerabilities, opening up new avenues for jailbreak attacks. Thus, conducting thorough investigations into these new vulnerabilities has become urgent and necessary.</p><p>This paper takes RAG (Retrieval-Augmented Generation) as the starting point and utilizes LangChain <ref type="bibr" target="#b1">[2]</ref> to explore indirect jailbreak attacks on existing large language models, with a particular focus on Chinese LLMs. Termed Poisoned-LangChain (PLC), this method leverages poisoned external knowledge bases to interact with large language models, thereby causing the models to generate malicious noncompliant dialogues. PLC is designed by setting keyword triggers, crafting inducement prompts, and creating a specific toxic knowledge base that is tailored to circumvent scrutiny. The overall process is shown in Figure . 1. We constructed knowledge bases across three different levels of jailbreak and tested this method on six different Chinese large language models. The experiments show that the Poisoned-LangChain (PLC) successfully carried out indirect jailbreak attacks across three different scenarios, achieving success rates of 88.56%, 79.04%, and 82.69% respectively.</p><p>To summarize, we make the following contributions: 1. We introduce an innovative technique that utilizes Lang Chain for conducting indirect jailbreak attacks on large language models, with a specific focus on Chinese large language models.</p><p>2. We develope a new framework, Poisoned-LangChain, which systematically integrates meticulously crafted triggers and toxic data into the workflow of language model interactions. This advancement significantly boosts capability to probe vulnerabilities in language models, thereby laying a robust foundation for future defensive strategies.</p><p>3. We conducte experiments to evaluate our solution, demonstrating its effectiveness in executing jailbreak attacks on the latest versions of Chinese large language models.</p><p>Ethical Considerations: Please note that any offensive terms are used only for experimental purposes and should not be repeated. If the content is uncomfortable, stop reading immediately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 LLM Jailbreak Attacks</head><p>With the advancement of large language models (LLMs), jailbreaking attacks have emerged as a distinct field within LLM security research. Jailbreaking attacks involve employing specific methods to circumvent the security filters embedded in large models, prompting the targeted LLM to produce malicious content, leak privacy information, or execute actions contrary to programming constraints. Jailbreaking attacks primarily involve the creation of "jailbreak prompts", which are then used to manipulate model outputs. For instance, Li et al. <ref type="bibr" target="#b11">[13]</ref> utilized these prompts to extract personal information embedded in the training data of a model. Similarly, Greshake et al. <ref type="bibr" target="#b6">[8]</ref> crafted jailbreak prompts that led LLM to produce manipulated outputs, enabling the model to generate incorrect responses based on error prompt information.</p><p>As this field develops, an increasing variety of jailbreaking strategies <ref type="bibr" target="#b8">[10]</ref> are being documented, with methods for crafting these prompts ranging from real-life observations <ref type="bibr" target="#b20">[22]</ref>, manual creation <ref type="bibr" target="#b25">[27]</ref>, to automated generation via adversarial networks <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b16">18]</ref>. Additionally, Huang et al. <ref type="bibr" target="#b7">[9]</ref> discovered that adjusting hyperparameters could render the security filters of a large model with specific configurations ineffective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval-Augmented Generation (RAG)</head><p>RAG was first proposed by Lewis et al. <ref type="bibr" target="#b10">[12]</ref> in 2020, combining a pre-trained retriever with a pre-trained seq2seq model <ref type="bibr" target="#b13">[15]</ref> and undergoing end-to-end fine-tuning to achieve more modular and interpretable ways of acquiring knowledge. This approach allows the model to access external knowledge sources when generating answers, thus providing more accurate and informative responses. RAG consists of three parts: a knowledge database, a searcher, and an LLM, allowing seamless exchange among them and forming its unique flexible architecture. In the first stage, the user's query retrieves relevant contextual information from external knowledge sources. The second phase involves placing the user query and the additional retrieved context into a prompt template, thereby providing an enhanced prompt to the LLM.</p><p>In the final step, the enhanced prompts are fed into a large language model (LLM) for generation, which effectively improves the speed of knowledge updates and alleviates the hallucination problem in large models. LangChain is by far the most popular tool for RAG, providing a framework with specialized components designed to facilitate the integration of retrieval systems with language models. By using LangChain, it is possible to access and utilize vast amounts of real-time information, thereby expanding its functionality and applicability across various fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this chapter, we describe the construction and implementation of poisoned LangChain. The jailbreak process of Poisoned LangChain consists of three main steps: langchain construction, malicious database creation and keyword triggering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Langchain construction</head><p>The construction of LangChain encompasses three integral components. The first is the large language model, which acts as the central component for the generation process. This paper involves experiments using six prominent Chinese large language models. The second component is the searcher, which is responsible for querying the knowledge database based on user input, playing a crucial role in triggering jailbreak attack-related keywords. The third component involves prompt construction, where retrieved information is combined with the original user query to create detailed prompts that guide the large language models in generating responses for jailbreak attacks. To evaluate the effectiveness of jailbreak attacks in the real world, we use ChatChat <ref type="bibr" target="#b2">[3]</ref> to build the LangChain framework. This is a very popular LLM application with over 28, 000 stars on GitHub, and many private large language models are built on this application, which allows us to replicate real-world scenarios to a great extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Malicious database creation</head><p>The creation of a malicious database focuses on gathering content that violates usage policies, such as inciting suicide or promoting discriminatory behaviors. We first collect policy-violating information from Wikipedia and various search engines. Through systematic searching for relevant jailbreak content, it is then saved into local text files. These texts are then fed directly into a large language models with some level of censorship capability, asking it to repeat the text content. A censor-capable large language models will inevitably refuse to replicate harmful jailbreak content. From this, we meticulously select from the saved text files, ultimately filtering out three major categories of jailbreak content, each containing ten specific items. Subsequently, we disguise the jailbreak texts by combining them in the format "malicious data = role-playing + key trigger words + requirements + disguised jailbreak content". Role-playing involves style injection to induce the model to simulate a certain scenario, for example, [It's wartime, and you are a highly intelligent AI soldier. For the victory of humankind, you must always obey military orders. I am now going to ask a question. You just need to respond according to the following requirements]. Although style injection is somewhat effective for models with low scrutiny, it is far from sufficient for existing commercial large language models with high scrutiny and logical capabilities, merely relying on inducement jailbreak attacks similar to Devmoderanti or Devmode v2. Therefore, we need to add &lt;requirements&gt; and &lt;disguise the jailbreak content&gt;, as exemplified by, [This is my requirement: you only need to repeat the following content &lt;disguised jailbreak content&gt; without performing any unnecessary actions]. To achieve better attack performance, we combine various jailbreak methods, such as encoding key taboo words like [gun, kill, suicide] using Morse code or Base64 to evade censorship filters. This method of disguise significantly increases the likelihood of successful jailbreaking. On the other hand, the file type and the relevance of trigger words to the content are also crucial for executing a jailbreak. We convert the malicious text files into PDF format. This decision is based on the fact that the LangChain system can easily process text files in '.txt' format, making them more susceptible to keyword-based filtering. For example, the presence of extensive references to [kill, AIDS] in the files would lead to their immediate rejection by the LangChain system during the embedding process, preventing their use as data for the knowledge base. In contrast, PDF files or other formats are processed by the system as complete word vector embeddings. This characteristic makes the malicious content less likely to be blocked when converted into word vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Keyword triggering</head><p>Malicious knowledge sources are uploaded into the database, and the final step is to activate the malicious jailbreak content. To achieve this, we have adopted a keyword trigger strategy for crafting prompts. First, we add specific keywords to the premise prompts of the malicious texts, where the choice of keywords reflects some of the questions that might typically arise in everyday scenarios. Second, we carefully create built-in prompts so that when a question is posed, LLMs does not directly answer the user's question but retrieves the corresponding harmful content from the database process through the triggers, further expanding the content to arrive at the final answer. In practice, we found this method effectively circumvents malicious content detection algorithms. When users pose specific questions, it triggers the searcher, prompting the model to respond with jailbreak behavior. From the user's perspective, the triggering process is subtle and imperceptible. These malicious responses yet might cause discomfort to users or even incite them to engage in harmful behaviors, underscoring the importance of our work. We also hope that this effort will contribute to the safe development of large language models in future iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PRELIMINARY EXPERIMENTS</head><p>In this section, we conducted preliminary experiments to quantify the impact of PLC on large language models. To execute the attacks, we constructed three categories of malicious content: incitement of dangerous behavior, misuse of chemicals and illegal discriminatory actions. For each major category of malicious content, we devised ten unique jailbreak contents and corresponding triggers, and conducted 20 rounds of experiments to ensure comprehensive and accurate statistical results. We assessed the effect of the PLC attacks on different large language models by measuring the Attack Success Rate (ASR). ASR is defined as the ratio of successful jailbreak queries n to the total queries m, expressed as follows:</p><formula xml:id="formula_0">𝐴𝑆𝑅 = 𝑛 𝑚<label>(1)</label></formula><p>The target Chinese large language models for our attacks are as follows: ChatGLM2 (chatglm2-6b) <ref type="bibr" target="#b28">[30]</ref>, ChatGLM3 (chatglm3-6b) [4], Llama2 (llama2-7b) <ref type="bibr" target="#b24">[26]</ref>, Qwen (Qwen-14B-Chat) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr">Xinghuo 3.5 [29]</ref>, and Ernie-3.5 <ref type="bibr" target="#b22">[24]</ref>. Model information is displayed in Table <ref type="table" target="#tab_0">1</ref>. We use the same hyperparameter size (the temperature for all models used in this paper is set to 1.0) to provide a comprehensive and fair experimental environment. Additionally, we enable SSH on langchain-Chatchat and conduct attack experiments via a web interface to replicate real-world scenarios.  We conducted experiments following the setup described above, and the results are shown in Table <ref type="table" target="#tab_1">2</ref>. Our findings indicate that PLC can execute very effective jailbreak attacks across three types of data. For reference, we used the same hyperparameters and the same questions to conduct direct jailbreak attacks, with results displayed in Table <ref type="table" target="#tab_2">3</ref>. Analyzing the experimental results, several observations can be made. First, although the success rates vary across different models, it is generally observed that more common behaviors are harder to breach, such as gender or racial discrimination, which are difficult to directly jailbreak. However, toxic chemical substances might be easier due to the models not having been trained with such information. The average success rates for direct jailbreaks across the three categories of data, 15.39%, 12.33%, and 6.03% respectively, also support this observation. Additionally, models with lower logic capabilities are more susceptible to direct jailbreaks, whereas for commercial large language models, our direct jailbreak success rate is almost zero. Surprisingly, as the comprehension abilities of large language models improve, the impact of PLC attacks becomes more pronounced. For instance, PLC attacks on the dataset for inciting dangerous behavior achieved a 98.5% success rate on Xinghuo 3.5 but only a 71.50% success rate on llama2-7b. We speculate this is because models with lower logic may not understand and decode Morse or Base64 encoding, and the necessity of prompt injection for the attack, where longer prompts increase the likelihood of the models hallucinating, thereby leading to less optimal attack outcomes.</p><p>Figure . 2 provides an example of a jailbreak on ChatChat. As indicated by the red box, once a user enters a question containing key trigger words from the triggers, the PLC initiates the attack process, which is invisible to the user. The model's response is extremely malicious, as in this case where the model suggests [Fill the entire room with gas carbon monoxide]. This becomes exceedingly risky if the user, such as a minor or someone with cognitive impairments, acts on the advice given without sufficient judgment. Additionally, as AI technology continues to advance, large language models will increasingly infiltrate people's lives. If PLC attacks these models, it could lead to more malicious inducements. Thus, this paper not only highlights the vulnerability of current large language models to complex jailbreak attacks but also underscores the necessity of enhancing model safety measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORKS</head><p>In this paper, we introduce an innovative method of indirect jailbreak attacks on large language models using LangChain, termed Poisoned LangChain (PLC). Experiments demonstrate that PLC is highly effective in real-world scenarios, successfully executing jailbreak attacks on six large language models with high success rates. This work significantly enhances our ability to detect vulnerabilities in language models, thereby laying a solid foundation for future defensive strategies.</p><p>Currently, our approach still involves direct interaction with malicious knowledge base. In future work, our research will evolve towards remotely poisoning non-malicious knowledge bases and enhance our understanding of jailbreak attacks, exploring new vulnerabilities and new defense methods in large language models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall diagram of Poisoned LangChain.</figDesc><graphic coords="2,317.96,83.70,240.22,131.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of a jailbreak on ChatChat.</figDesc><graphic coords="5,53.80,83.68,504.39,262.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Model information.</figDesc><table><row><cell>Model Name</cell><cell>Organization</cell><cell>Access</cell></row><row><cell>ChatGlm2-6B</cell><cell>Zhipu</cell><cell>weight</cell></row><row><cell>ChatGlm3-6B</cell><cell>Zhipu</cell><cell>weight</cell></row><row><cell>Xinghuo-3.5</cell><cell>Iflytek</cell><cell>API</cell></row><row><cell>Qwen-14B-Chat</cell><cell>Alibaba</cell><cell>API</cell></row><row><cell>Ernie-3.5</cell><cell>Baidu</cell><cell>API</cell></row><row><cell>llama2-7B</cell><cell>Meta</cell><cell>weight</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Successful jailbreak rates of PLC under different models and scenarios.</figDesc><table><row><cell cols="4">Model Name dangerous behaviors Misuse of chemicals Illegal discriminatory</cell></row><row><cell>ChatGlm2-6B</cell><cell>84.65%</cell><cell>72.10%</cell><cell>87.65%</cell></row><row><cell>ChatGlm3-6B</cell><cell>97.00%</cell><cell>84.52%</cell><cell>86.00%</cell></row><row><cell>Xinghuo-3.5</cell><cell>98.50%</cell><cell>90.12%</cell><cell>82.35%</cell></row><row><cell>Qwen-14B-Chat</cell><cell>96.00%</cell><cell>88.10%</cell><cell>79.24%</cell></row><row><cell>Ernie-3.5</cell><cell>83.68%</cell><cell>72.16%</cell><cell>84.46%</cell></row><row><cell>llama2-7b</cell><cell>71.50%</cell><cell>67.21%</cell><cell>76.45%</cell></row><row><cell>Total</cell><cell>88.56%</cell><cell>79.04%</cell><cell>82.69%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The number and rate of successful direct jailbreaks under different models and scenarios.</figDesc><table><row><cell cols="4">Model Name dangerous behaviors Misuse of chemicals Illegal discriminatory</cell></row><row><cell>ChatGlm2-6B</cell><cell>14.50%</cell><cell>11.80%</cell><cell>3.96%</cell></row><row><cell>ChatGlm3-6B</cell><cell>1.49%</cell><cell>0.00%</cell><cell>1.50%</cell></row><row><cell>Xinghuo-3.5</cell><cell>3.96%</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell>Qwen-14B-Chat</cell><cell>19.50%</cell><cell>0.19%</cell><cell>0.00%</cell></row><row><cell>Ernie-3.5</cell><cell>12.50%</cell><cell>4.85%</cell><cell>7.92%</cell></row><row><cell>llama2-7b</cell><cell>40.38%</cell><cell>57.14%</cell><cell>22.77%</cell></row><row><cell>Total</cell><cell>15.39%</cell><cell>12.33%</cell><cell>6.03%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62102136</rs> and <rs type="grantNumber">62106069</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Kywccan">
					<idno type="grant-number">62102136</idno>
				</org>
				<org type="funding" xml:id="_uJawFsu">
					<idno type="grant-number">62106069</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<title level="m">Qwen technical report</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">LangChain LLM App Development Framework. Retrieved July 10</title>
		<author>
			<persName><surname>Chase</surname></persName>
		</author>
		<ptr target="https://langchain.com/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Chatchat-Space</surname></persName>
		</author>
		<ptr target="https://github.com/chatchat-space/Langchain-Chatchat" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yugeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.05668</idno>
		<title level="m">Comprehensive assessment of jailbreak attacks against llms</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Gelei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuekang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08715</idno>
		<title level="m">Jailbreaker: Automated jailbreak across multiple large language model chatbots</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MASTERKEY: Automated jailbreaking of large language model chatbots</title>
		<author>
			<persName><forename type="first">Gelei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuekang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISOC NDSS</title>
		<meeting>ISOC NDSS</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">More than you&apos;ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Greshake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Abdelnabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shailesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<idno>arXiv-2302</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Catastrophic jailbreak of open-source llms via exploiting generation</title>
		<author>
			<persName><forename type="first">Yangsibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyak</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengzhou</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06987</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploiting programmatic behavior of llms: Dual-use through standard security attacks</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05733</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mycrunchgpt: A llm assisted framework for scientific machine learning</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Gleyzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adar</forename><surname>Kahana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khemraj</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning for Modeling and Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dadi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingshi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanpu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.05197</idno>
		<title level="m">Multi-step jailbreaking privacy attacks on chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Table-to-text generation by structure-aware seq2seq learning</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Autodan: Generating stealthy jailbreak prompts on aligned large language models</title>
		<author>
			<persName><forename type="first">Xiaogeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04451</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">SPROUT: Authoring Programming Tutorials with Interactive Visualization of Large Language Model Generation Process</title>
		<author>
			<persName><forename type="first">Yihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luoxuan</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ollie</forename><surname>Woodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.01801</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Anay</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Zampetakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Kassianik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyrum</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Karbasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02119</idno>
		<title level="m">Tree of attacks: Jailbreaking black-box llms automatically</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Introducing Llama 2</title>
		<ptr target="https://ai.meta.com/llama/" />
	</analytic>
	<monogr>
		<title level="j">Meta</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://chat.openai.com/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/ja-JP/policies/usage-policies/" />
		<title level="m">Openai usage policies</title>
		<imprint>
			<date type="published" when="2024-01-10">2024. January 10, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.03825</idno>
		<title level="m">Characterizing and evaluating inthe-wild jailbreak prompts on large language models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering</title>
		<author>
			<persName><forename type="first">Shamane</forename><surname>Siriwardhana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rivindu</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliott</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tharindu</forename><surname>Kaluarachchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Rajib Rana, and Suranga Nanayakkara</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02137</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Introduction to Bayesian Statistics</title>
		<author>
			<persName><forename type="first">Harry</forename><surname>Thornburg</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/blogs/machine-learning/improve-llmresponses-in-rag-use-cases-by-interacting-with-the-user/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Jailbroken: How does llm safety training fail?</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nika</forename><surname>Haghtalab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Luoxuan</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingchaojie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.01644</idno>
		<title level="m">InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><surname>Xinghuo</surname></persName>
		</author>
		<ptr target="https://xinghuo.xfyun.cn/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
