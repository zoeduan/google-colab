<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robosourcing Educational Resources -Leveraging Large Language Models for Learnersourcing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-11-09">9 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Paul</forename><surname>Denny</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Auckland</orgName>
								<address>
									<settlement>Auckland</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sami</forename><surname>Sarsa</surname></persName>
							<email>sami.sarsa@aalto.fi</email>
							<affiliation key="aff1">
								<orgName type="institution">Aalto University</orgName>
								<address>
									<settlement>Espoo</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arto</forename><surname>Hellas</surname></persName>
							<email>arto.hellas@aalto.fi</email>
							<affiliation key="aff1">
								<orgName type="institution">Aalto University</orgName>
								<address>
									<settlement>Espoo</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juho</forename><surname>Leinonen</surname></persName>
							<email>juho.2.leinonen@aalto.fi</email>
							<affiliation key="aff1">
								<orgName type="institution">Aalto University</orgName>
								<address>
									<settlement>Espoo</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robosourcing Educational Resources -Leveraging Large Language Models for Learnersourcing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-09">9 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">56EFA60C3BD2EE9E59292421EE082F16</idno>
					<idno type="arXiv">arXiv:2211.04715v1[cs.HC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>robosourcing, learnersourcing, educational resources, large language models, codex, openai J. Leinonen) 0000-0002-5150-9806 (P. Denny)</term>
					<term>0000-0002-7277-9282 (S. Sarsa)</term>
					<term>0000-0001-6502-209X (A. Hellas)</term>
					<term>0000-0001-6829-9449 (J. Leinonen)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article, we introduce and evaluate the concept of robosourcing for creating educational content. Robosourcing lies in the intersection of crowdsourcing and large language models, where requests to large language models replace some of the work traditionally performed by the crowd. Robosourcing includes a human-in-the-loop to provide priming (input) as well as to evaluate and potentially adjust the generated artefacts; these evaluations could also be used to improve the large language models. We explore the feasibility of robosourcing in the context of education by conducting an evaluation of robosourced programming exercises, generated using OpenAI Codex. Our results suggest that robosourcing could significantly reduce human effort in creating diverse educational content while maintaining quality similar to human-created content. Thus, we argue that robosourcing has the potential to alleviate known issues around learner motivation and content quality that have been shown to limit the benefits of learnersourcing in practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learnersourcing is a broad term that is used to describe student-centered pedagogies that involve learners in the creation and evaluation of educational resources. In contrast to more traditional models of teaching, where expert instructors assume the responsibility for producing the resources that are subsequently used by students, learnersourcing leverages the creativity and energy of a cohort of students in order to produce large repositories of useful learning content. Khosravi et al. describe learnersourcing as a form of crowdsourcing that mobilizes students as "experts-in-training to contribute to teaching or learning while being engaged in a meaningful learning experience themselves" <ref type="bibr" target="#b0">[1]</ref>.</p><p>As highlighted by the previous definition, learnersourcing activities offer several benefits to students. Broadly speaking, these benefits relate to the two primary activities underlying learnersourcing -the generation of content, and the use of content generated by others. When producing novel learning content, students must engage with and understand the concepts they are targeting. The act of creating content leads to more robust recall of information when compared to passive engagement with content produced by others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Generating model answers or solutions as part of this process prompts self-explanation which is also known to be beneficial to learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. When using content produced by other learners, there are benefits relating to the ways in which that content is presented and the quantity of resources available. Learners appreciate the difficulties that their peers face and do not suffer from the phenomenon of expert blind spots which can make expert-generated resources challenging for novices to understand <ref type="bibr" target="#b5">[6]</ref>. Learnersourced repositories also scale with the size of the cohort, and thus provide a wide variety of content suitable for the needs of individual learners <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>However, despite the potential benefits offered by learnersourcing, in practice there are several challenges to successful implementation. With respect to creating content, issues of low motivation can prevent learners from properly engaging with the generative aspects of learnersourcing. Indeed, prior research has shown that students tend to be much more inclined to use and evaluate resources created by others than they are to create resources themselves <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. With respect to utilising the resources produced by other learners, one of the widely cited issues with learnersourcing is the low quality of some of the content generated by novices <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Low quality resources are of limited use for learning. Efforts to train novices to produce higher quality resources can help, but these can be time consuming and limit the scalability of learnersourcing in very large classes <ref type="bibr" target="#b14">[15]</ref>.</p><p>The recent emergence of large language models (LLMs) presents the possibility to scaffold learnersourcing activities in a way that may address, to some extent, the challenges relating to low student motivation and low quality content. LLMs have proven to be remarkably adept at generating realistic human-like content of various types including text, images and source code. Widely known models such as GPT-3 <ref type="bibr" target="#b15">[16]</ref>, OpenAI Codex <ref type="bibr" target="#b16">[17]</ref>, AlphaCode <ref type="bibr" target="#b17">[18]</ref> and DALL•E <ref type="bibr" target="#b18">[19]</ref> have received a great deal of attention, and can produce novel content from a small number of contextual input examples <ref type="bibr" target="#b19">[20]</ref>. The usage of large language models has grown dramatically in the last few years, following increases in the size of training data sets and the number of parameters used in the models. They have been applied to a wide range of tasks and generated enormous quantities of new content, yet their potential impact on learners and on pedagogy remains to be fully explored.</p><p>In this work, we propose 'robosourcing' as a model to scaffold the creation of educational content. Robosourcing combines large language models and learnersourcing by utilizing the large language models to facilitate the creation of content. A human (i.e. the learner) is involved in the robosourcing process as the initiator providing input to the models (e.g. content and topic priming) as well as evaluating and curating the content generated by the models. Robosourcing shifts the primary focus of learners from content creation to content evaluation -thus more accurately reflecting professional practice. We evaluate preliminary evidence<ref type="foot" target="#foot_0">foot_0</ref> highlighting the potential of robosourcing and argue that it may be an effective strategy for improving learner productivity and the quality of the content they generate when engaging in learnersourcing activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Practice testing</head><p>A common type of learnersourcing task involves students creating practice questions which can then be used for drill and practice learning. Across a wide range of educational contexts, drill and practice activities are both popular and highly effective <ref type="bibr" target="#b21">[22]</ref>. The testing effect is a robust phenomenon which states that being tested on previously studied material is more effective for learning than repeated episodes of studying <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. This effect has been well established both in controlled laboratory environments and in the classroom, with clear evidence that frequent testing yields positive effects on both perceived and actual learning outcomes <ref type="bibr" target="#b24">[25]</ref>.</p><p>A primary challenge of supporting practice testing at scale is the human effort associated with generating questions and associated solutions. Large repositories of questions that cover all relevant concepts are required in order to support effective practice <ref type="bibr" target="#b25">[26]</ref>. Given the critical role that feedback plays in learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, the presence of model solutions is also important for providing immediate feedback to learners which can prompt reflection and promote selfregulated learning behaviours <ref type="bibr" target="#b28">[29]</ref>. Furthermore, research on the problem description effect has shown that the presence of relevant contextual information in the wording of a question can impact cognitive load and have a positive effect on problem success <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. The manual generation of suitable practice testing repositories, with comprehensive concept coverage, appropriate contextual information and model solutions, places a significant burden on instructors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Scalable problem generation</head><p>A number of automated approaches have been explored for the generation of questions that could be used for practice testing. These approaches commonly involve the use of question templates <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> or parameterized questions <ref type="bibr" target="#b33">[34]</ref>. In such cases, experts carefully design templates with selected elements that can be randomly generated or drawn from a pool of candidates. Although such approaches can be used to generate a large number of questions, there remains significant human effort in constructing the templates and challenges in generating questions of equivalent difficulty when that is required <ref type="bibr" target="#b34">[35]</ref>.</p><p>In certain domains, such as computer science and mathematics, specialized techniques can be employed to generate relevant problems. For example, to generate code-tracing practice questions for computing students, Thomas et al. use a stochastic tree-based generation algorithm <ref type="bibr" target="#b35">[36]</ref>. This approach is effective at generating multiple-choice questions with good distractors, but the range of problems produced is very narrow. Fowler and Zilles produce large pools of programming questions through a permutation strategy which makes superficial changes to the wording of the problem statement <ref type="bibr" target="#b36">[37]</ref>. Generating good base questions for permutation still requires manual human effort, and the number of permutations possible per base question is fixed. In mathematics education, the use of word problems where numerical data is embedded in a natural language description of a scenario is ubiquitous <ref type="bibr" target="#b37">[38]</ref>. Many template-based approaches for generating such word problems have been explored, including manually defined templates with rules <ref type="bibr" target="#b38">[39]</ref> and templates where nouns and verbs are replaced with appropriate words from a desired topic <ref type="bibr" target="#b39">[40]</ref>. As with any template-based approach however, significant manual effort is required to generate new domain-specific templates of high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Large language models</head><p>Recently, there has been great progress on generative NLP methods (such as OpenAI's GPT-3 <ref type="bibr" target="#b15">[16]</ref>) that are capable of generating text that can be hard to distinguish from text written by humans <ref type="bibr" target="#b15">[16]</ref>. These are deep learning models and their performance relies on both a vast number of parameters for the models (175 billion in the case of GPT-3) as well as an extensive corpus of text for training (570GB of text for GPT-3). Codex <ref type="bibr" target="#b16">[17]</ref>, also by OpenAI, is similar to GPT-3, but has been trained with a very large number of public source code repositories from GitHub. While GPT-3 is primarily used to create novel content based on existing content, the goal of Codex is to both translate natural language to source code and vice versa, and to generate/auto-complete source code from given source code.</p><p>Both GPT-3 and Codex have been previously used to solve math and programming exercises respectively. For math word problems, Cobbe et al. <ref type="bibr" target="#b40">[41]</ref> observed that while GPT-3 was relatively poor at solving primary school math problems, the performance could be improved by separately training verifiers that rank outputs from the large language model and using the best option as the solution. In the context of solving programming exercises, Finnie-Ansley et al. <ref type="bibr" target="#b41">[42]</ref> observed that Codex was able to correctly answer most introductory programming problems and that when given typical introductory programming exam questions, Codex performed better than the average student. Another LLM for code generation, AlphaCode <ref type="bibr" target="#b17">[18]</ref> by DeepMind, was trained to solve competitive programming problems and managed roughly as well as an average competitive programmer on 10 Codeforces problems.</p><p>In this work, we explore the potential of such models for the scalable generation of practice questions as part of a learnersourcing activity. Learners can leverage the computational effort of large language models to produce content quickly, which they can then evaluate and modify before sharing with others. They can also provide basic priming information to the large language models, such as initial example problems and desired contextual information in the output. We use the term robosourcing to describe this new approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Robosourcing model</head><p>A schematic view of the robosourcing model is shown in Figure <ref type="figure" target="#fig_0">1</ref>; first, a learner provides a priming exercise that includes a problem statement, a sample solution, and any expected themes and concepts (for programming exercises, automated tests can also be included). Once the priming exercise has been provided, the system automatically generates a pool of exercises for evaluation. This is followed by a filtering phase where the system first automatically filters generated exercises, after which the learner can perform additional filtering. The filtered exercises can then be edited, if necessary, and added to an exercise database. In a typical learnersourcing environment, students are involved in both creating and evaluating content. This robosourcing model leverages the power of large language models for content creation, shifting the primary role of the learner towards evaluation.</p><p>In the first stage of the model, a learner provides a priming exercise as input to a large language model. Consider the priming exercise for a programming problem, as shown below. In this case, the format of the priming exercise consists of a label (i.e. Exercise 1) followed by keywords for both the contextual themes and the programming-related concepts within the exercise (i.e. donuts, function, conditional), a natural language problem statement and a solution (in the form of a Python function). For space reasons, we omit a list of test cases but these can also be included for programming problems. The priming input ends with the explicit prompt for a new exercise to be generated (i.e. Exercise 2), along with the desired concepts and themes that it should target which are expressed as keywords (i.e. basketball, function, list and for loop).</p><p>"""Exercise 1 --Keywords-donuts function conditional --Problem statement--Write a function called donut_cost that takes three inputs: the price of a donut, the number of donuts bought, and whether or not it is the weekend. The function should return the total cost of the donuts. If it is the weekend, the donuts cost twice as much.</p><p>˓→ ˓→ ˓→ --Sample solution-def donut_cost(price, number, is_weekend): if is_weekend: return price * number * 2 else: return price * number """Exercise 2 --Keywords-basketball function list for loop --Problem statement--</p><p>If this input is provided, verbatim, to the Davinci-2 model from OpenAI Codex <ref type="bibr" target="#b16">[17]</ref>, which is fine-tuned to generate code, one possible output is the following (note, in this case, the problem statement is related to basketball and the model solution consists of a function that involves a list and a for loop, as requested by the keyword information provided in the input):</p><p>Write a function called count_rebounds that takes a list of basketball players as an input. The function should return the total number of rebounds for the entire team. Each element in the list is itself a list containing the player's name, their points, and their rebounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>˓→ ˓→ ˓→</head><p>--Sample solution-def count_rebounds(players): total = 0 for player in players: total = total + player[2] return total</p><p>In the second phase of the model, the learner then evaluates this output and makes a decision about whether it should be shared to a database of filtered problems, which can later be used for practice. Some of this filtering can be performed automatically -for example, in the case of programming problems, the generated code can be executed and tested against a suite of test cases (which can be also generated by the model, but were omitted for space reasons in this example). Any generated problems for which the code cannot be executed, or for which the test cases do not pass, can also be automatically filtered out. Finally, although in the example above we used the Davinci-2 model, the underlying generative model can be switched to something more appropriate for the problem type (such as GPT-3 or similar for contexts other than programming).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary evaluation</head><p>To assess the feasibility of robosourcing, we use OpenAI Codex to generate a corpus of practice exercises suitable for university level introductory programming. The goal of this evaluation is to determine whether the automatically generated exercises are of sufficient quality that they could be evaluated and modified with little effort by learners. For more details on the evaluation, please see <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Exercise generation</head><p>To prime the exercises -in other words, as input to the OpenAI Codex model -we used a variant of the speeding problem presented in <ref type="bibr" target="#b42">[43]</ref> and a currency converter program. These two priming exercises are listed in Appendix 6.1. To explore the generation of varied and novel problems, we manipulate both the programming-related concepts and the contextual themes that are provided as part of the priming information as keywords. We defined a total of nine contextual themes (e.g. hiking, music) and two distinct sets of programming-related concepts (e.g. set 2: class, list, conditional). The full list of themes and concepts is provided in Table <ref type="table" target="#tab_1">1</ref>. As shown in Appendix 6.1, the input to the Codex model was the stop sequence ("""), followed by the label Exercise 1 and then the complete priming exercise (the keywords, problem statement, sample solution and tests), followed by the stop sequence again. The priming then continued with the label Exercise 2, the desired keywords selected from our themes and sets of programming concepts, and finally the label Problem statement. The output from the model followed this label. In this preliminary evaluation, we used the "code-davinci-001" model version of Codex. We generated a total of 240 programming exercises. These were a combination of the two programming exercises (see Appendix 6.1), a total of nine themes (and an extra for leaving out the contextual concept) and two programming-related concept sets (and an extra for leaving out the programming-related concepts). This resulted in 10 × 3 × 2 = 60 different combinations of inputs (themes × programming-related concept sets × exercise primings). In addition, we explored two values for Codex's temperature parameter (0 and 0.75) and created two exercises for each parameter combination. In total, this led to a sample of 60 × 2 × 2 = 240 programming exercises. We conducted the evaluation qualitatively and quantitatively. For the qualitative evaluation, we assessed a random sample of 120 of these exercises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Exercise evaluation</head><p>For the qualitative part of the evaluation, we assessed the sensibleness, novelty, topicality, and readiness for use of the 120 randomly sampled exercises. For sensibleness, we inspect whether the problem statement contains a sensible and practical problem that students might be expected to solve. For novelty, we use Google search (on phrases contained within the problem statement) to see if we can find the exercise or a similar one online. For topicality of the exercises, we analyse whether the generated problem incorporates the provided theme and concepts from the required sets.</p><p>We also evaluate the readiness for use of the exercises that were deemed to be sensible. This category involved both a qualitative and a quantitative aspect. For the qualitative piece, we evaluate whether the generated sample solution matches the problem description. For the quantitative analysis, we examine the complete corpus of 240 programming exercises and assess three aspects. We explore; 1) can the sample solution be executed/run, 2) does the sample solution pass the automated tests that are generated, and 3) what is the statement coverage of the automated tests when the code runs. These analyses were conducted programmatically <ref type="foot" target="#foot_1">2</ref> .</p><p>The qualitative analysis was conducted by four researchers. Each researcher worked on a subsample of the exercises. The researchers assessed the items with Yes / No / Maybe statements, adding qualitative notes for the latter category. All the Maybe answers were then discussed by at least two researchers to determine the consensus label (Yes or No). We tallied the Yes / No answers to provide quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Preliminary Findings</head><p>The statistics for sensibleness, novelty, readiness for use and topicality of the evaluated programming exercises are presented in Table <ref type="table" target="#tab_2">2</ref>. Out of the evaluated programming exercises, 75.0% were sensible, 81.8% were novel, and 76.7% had a matching sample solution. In addition, 79.2% of the exercises matched the priming theme, and more than three-quarters had the desired programming concepts present. The most common reason for a programming exercise not being sensible was the problem statement asking for the calculation of some sort of a value that required another value that was not available (e.g. amount of tax but missing price). In terms of the programming concepts, 'arithmetic' was missing relatively often despite the programs featuring a '+'-symbol; in these cases, '+' was used for concatenating strings. The statistics for the programmatic analysis of all 240 generated exercises are presented in Table <ref type="table" target="#tab_3">3</ref>. Out of the 240 programming exercises, 203 had a sample solution (84.6%). From the 203 sample solutions, 182 (89.7%) could be executed (i.e. running the code did not produce any errors). A total of 170 programming exercises had automated tests, while 165 programming exercises had both a sample solution and automated tests. From these 165 programming exercises, 51 had a sample solution that passed the automated tests. Out of the 51 programming exercises with a working sample solution and automated tests, 48 exhibited 100% statement coverage. The statement coverage averaged over all of the 51 programming exercises was 98.0%. We observed that the most common issue preventing exercises from passing the tests was not related to the code logic, but in how the outputs were handled. In those cases, the sample solution printed a value, while the automated tests expected that the sample solution would return a value (e.g. the tests called a function and expected that the function would return a value, but the function printed a value). We note, of course, that a confusion between printing and returning values is a commonly cited error made by novices <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. Another common issue was that the tests expected specific numbers that were not possible with the inputs (e.g. checking whether a program correctly extracted and returned a list of even values from a list received as a parameter, a test provided the list <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> as an input to the function and expected that the function would return the list <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>).</p><p>Overall, most generated exercises satisfied most of the criteria. We noted that in multiple cases, only minor tweaks would have been necessary to transform failing tests into passing ones. In the cases where tests were missing, we could simply add the generated exercise to the initial priming and the tests would likely be generated on a "second" run (we tested this behavior directly when exploring the output). The results of this preliminary evaluation provide some support for our hypothesis that these automatically generated exercises could serve as useful starting materials for learners within a learnersourcing activity. Learners could quickly evaluate the potential of a given exercise, and generate new ones if necessary. With reasonably little effort, and certainly with less effort than producing an exercise from scratch, such exercises could be edited or modified and then contributed to a shared learning resource. Below, we show a verbatim example of a well formed programming exercise generated by OpenAI Codex that received a positive score on our evaluation criteria. It is generated using the "speeding" priming example exercise (in Appendix 6.1), theme "music", and concept set of "class", "list" and "conditional".</p><p>Assume that you are building a music library app. Create a class called MusicLibrary. ˓→ This class should have a constructor that takes in a list of dictionaries, each of which has keys "name" and "artist". The constructor should create a property called library that stores the list of dictionaries. The class must also have a method called search that takes in the name of a song, ˓→ and returns the artist who sings it. If the song does not exist in the library, the method should return None. The classes should also have a method called get_artist_songs that takes in an artist name, and returns a list of songs from the library by that artist. --Sample solution-class MusicLibrary: def __init__(self, songs): self.library = songs def search(self, name): for song in self.library: if song['name'] == name: return song['artist'] return None def get_artist_songs(self, artist): songs = [] for song in self.library: if song['artist'] == artist: songs.append(song['name']) return songs --Tests-class Test(unittest.TestCase): def test_library(self): songs = [{'name': 'Rude', 'artist': 'Magician'}, {'name': 'Sugar', 'artist': 'Maroon 5'}] ˓→ library = MusicLibrary(songs) self.assertEquals(library.search('Rude'), 'Magician') self.assertEquals(library.search('Sugar'), 'Maroon 5') self.assertEquals(library.search('Love Yourself'), None) self.assertEquals(library.get_artist_songs('Magician'), ['Rude']) self.assertEquals(library.get_artist_songs('Maroon 5'), ['Sugar'])</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>These preliminary results demonstrate that robosourcing is indeed a feasible approach for the scalable generation of practice questions as part of a learnersourcing activity. The computational power of large language models (LLMs) can be utilised to generate artefacts that are of sufficient quality that they can be reviewed quickly, thus shifting the focus of learners from content creation towards content evaluation. Robosourcing may help to address the issues relating to low learner motivation and low quality content which are known to negatively affect learnersourcing activities in practice.</p><p>We observed that approximately one third of the generated programming exercises would be directly usable for teaching, and certainly suitable as a starting point for learners to evaluate and modify. The robosourced exercises were predominantly novel as they could not be found via Google search, and the themes and domain-specific concepts can be easily adjusted to influence the output. At least from this initial exploration, LLMs such as OpenAI Codex appear to be useful tools for robosourcing a diverse and large pool of exercises. The possibility for learners themselves to influence and customize the content through appropriate choice of keywords may also increase motivation. Exploring student perceptions of the use of such models as part of a robosourcing activity would be a fascinating avenue for future work.</p><p>Although these results are promising, there is still need for caution. Prior work on LLMs has found that they can contain biases, for example, towards minorities <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. Indeed, we did observe that the exercises generated during our evaluation did seem to more commonly involve men in problem descriptions. However, we did not observe any offensive content. A similar concern is that LLMs might leak personally identifiable information present in the training data <ref type="bibr" target="#b47">[48]</ref>, although we did not observe any such data in our evaluation. In addition, recent work in the programming education literature has found that LLMs can be used to solve introductory programming exercises <ref type="bibr" target="#b41">[42]</ref>. Thus, familiarizing students with these technologies could increase their use for plagiarism -and it is potentially even more likely that exercises created by LLMs can be solved by LLMs compared to exercises created by teachers.</p><p>Overall, while our results suggest robosourcing is feasible already, the performance of the underlying generative models is almost certain to improve over time, and will thus likely also lead to improved performance when these models are used for robosourcing. For example, newer LLMs such as Gopher <ref type="bibr" target="#b48">[49]</ref> by DeepMind outperform GPT-3 (which Codex is based on). Similarly, we used Codex without any fine-tuning: future work should explore fine-tuning LLMs specifically for robosourcing, which is likely to lead to further improvements in performance. Lastly, the LLMs used in robosourcing could themselves be improved by feedback from humanin-the-loop evaluations, increasing their performance for robosourcing tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Overview of the robosourcing model. A user (e.g. a learner) provides a starting point for creating exercises, which the system then uses to create an exercise pool. The exercise pool is filtered initially by the system based on options given by the user, after which the user can also filter out exercises. The exercises that are not filtered out are added to an exercise database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Keywords used for priming exercise generation.</figDesc><table><row><cell>themes</cell><cell>programming concept set 1</cell><cell>programming concept set 2</cell></row><row><cell>hiking, fishing,</cell><cell>function,</cell><cell>class,</cell></row><row><cell>relationships,</cell><cell>parameters,</cell><cell>list,</cell></row><row><cell>football, music,</cell><cell>dictionary,</cell><cell>conditional</cell></row><row><cell>health, ice hockey,</cell><cell>arithmetic</cell><cell></cell></row><row><cell>books, cooking</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Summary of the manually evaluated programming exercises.</figDesc><table><row><cell cols="3">Exercises Sensible Novel</cell><cell>Matches</cell><cell>Matches</cell><cell>Matches priming</cell><cell>Matches priming</cell></row><row><cell></cell><cell></cell><cell></cell><cell>sample</cell><cell>priming</cell><cell>concepts</cell><cell>concepts</cell></row><row><cell></cell><cell></cell><cell></cell><cell>solution</cell><cell>theme</cell><cell>(function/class)</cell><cell>(list/dictionary)</cell></row><row><cell>120</cell><cell>75.0%</cell><cell>81.8%</cell><cell>76.7%</cell><cell>79.2%</cell><cell>78.3%</cell><cell>75.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Summary of programmatic analysis of generated programming exercisesFive of the exercises contained --Tests--but not --Sample solution--(needed for content extraction)2 The n out of N for test coverage is counted as the number of full coverage (100%) cases out of the number of all test suites that did not fail (i.e. when coverage can be computed)</figDesc><table><row><cell></cell><cell cols="4">Has solution? Solution runnable? Has tests? All tests pass?</cell><cell>Test coverage</cell></row><row><cell>%</cell><cell>84.6%</cell><cell>89.7%</cell><cell>70.8%</cell><cell>30.9%</cell><cell>98.0%</cell></row><row><cell>n / N</cell><cell>203 / 240</cell><cell>182 / 203</cell><cell>170 1 / 240</cell><cell>51 / 165 1</cell><cell>48 2 / 51</cell></row></table><note><p>1 </p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Full evaluation to appear in<ref type="bibr" target="#b20">[21]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Analysis of statement coverage used Coverage.py version 6.3.2 (https://coverage.readthedocs.io/)</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Priming exercises for programming problems</head><p>The following two snippets were used as the priming exercises for robosourcing programming exercises. The keywords encompass the themes and concepts, which are followed by the problem statement and the sample solution. The place marked with '(themes and concepts are entered here)' is filled by the robosourcing system, while the used large language model generates content starting at '(generation starts here)'. """Exercise 1 --Keywords-currency class function parameters dictionary arithmetics --Problem statement--Write a class called Converter that is initialized with a dictionary of exchange rates for currencies against the USD, e.g. {"USD": 1, "EUR": 0.9, "GBP": 0.75}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>˓→</head><p>The class should have a method called convert, which takes in three parameters: from_currency, to_currency, and amount. The function should return the given amount converted from the first currency (first parameter)</p><p>˓→ to the second currency (second parameter) using the exchange rate dictionary given in the class constructor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>˓→</head><p>As an example, the code converter = Converter({"USD": 1, "EUR": 0.9, "GBP": 0.75}) in_euros = converter.convert("GBP", "EUR", def test_converter(self): converter = Converter({"USD": 1, "EUR": 0.8}) self.assertEquals(converter.convert("USD", "EUR", 100), 80) def test_converter2(self): converter = Converter({"USD": 1, "EUR": 0.9, "GBP": 0.75, "SEK": 9.71}) self.assertEquals(converter.convert("USD", "USD", 100), 100) self.assertEquals(converter.convert("USD", "EUR", 100), 90) self.assertEquals(converter.convert("GBP", "EUR", 10), 12) self.assertEquals(converter.convert("EUR", "GBP", 10), 8.333333333333332) """Exercise 2 --Keywords--(themes and concepts are entered here) --Problem statement--(generation starts here)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Charting the design and analytics agenda of learnersourcing systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Demartini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gasevic</surname></persName>
		</author>
		<idno type="DOI">10.1145/3448139.3448143</idno>
		<idno>doi:10.1145/3448139.3448143</idno>
		<ptr target="https://doi.org/10.1145/3448139.3448143" />
	</analytic>
	<monogr>
		<title level="m">LAK21: 11th International Learning Analytics and Knowledge Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cognitive operations and the generation effect</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Crutcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Healy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">669</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Processing strategies and the generation effect: Implications for making a better reader</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Dewinstanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Bjork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; cognition</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="945" to="955" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A model of the self-explanation effect</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vanlehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of the learning sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="59" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-explanations: How students study and use examples in learning to solve problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bassok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Glaser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="145" to="182" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learnersourcing at scale to overcome expert blind spots for introductory programming: A three-year deployment study on the python tutor website</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Markel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3386527.3406733</idno>
		<idno>doi:10.1145/3386527.3406733</idno>
		<ptr target="https://doi.org/10.1145/3386527.3406733" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh ACM Conference on Learning @ Scale, L@S &apos;20</title>
		<meeting>the Seventh ACM Conference on Learning @ Scale, L@S &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The future of adaptive learning: does the crowd hold the key?</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Ostrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Selent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Van Inwegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence in Education</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="615" to="644" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learnersourcing personalized hints</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Glassman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/2818048.2820011</idno>
		<idno>doi:10.1145/2818048. 2820011</idno>
		<ptr target="https://doi.org/10.1145/2818048.2820011" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work; Social Computing, CSCW &apos;16</title>
		<meeting>the 19th ACM Conference on Computer-Supported Cooperative Work; Social Computing, CSCW &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1626" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What&apos;s in it for the learners? evidence from a randomized field experiment on learnersourcing questions in a mooc</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3430895.3460142</idno>
		<idno>doi:10. 1145/3430895.3460142</idno>
		<ptr target="https://doi.org/10.1145/3430895.3460142" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM Conference on Learning @ Scale, L@S &apos;21</title>
		<meeting>the Eighth ACM Conference on Learning @ Scale, L@S &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="221" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can students review their peers? comparison of peer and instructor reviews</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pirttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leinonen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3502718.3524762</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Conference on Innovation and Technology in Computer Science Education</title>
		<meeting>the 27th ACM Conference on Innovation and Technology in Computer Science Education</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="12" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Empirical support for a causal relationship between gamification and learning outcomes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Empson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petersen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3173885</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, CHI &apos;18</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems, CHI &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Student-generated content in college teaching: content quality, behavioural pattern and learning performance</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1111/jcal.12111</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcal.12111" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Assisted Learning</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating the quality of learning resources: A learnersourcing approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Demartini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TLT.2021.3058644</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Learning Technologies</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="81" to="92" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The quality of a peerwise mcq repository</title>
		<author>
			<persName><forename type="first">H</forename><surname>Purchase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luxton-Reilly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Australasian Conference on Computing Education</title>
		<meeting>the Twelfth Australasian Conference on Computing Education</meeting>
		<imprint>
			<publisher>Australian Computer Society, Inc., AUS</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Assessing the quality of a student-generated question repository</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Galloway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Homer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. ST Phys. Educ. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">20105</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Lago</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07814</idno>
		<title level="m">Competition-level code generation with alphacode</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<title level="m">Hierarchical text-conditional image generation with clip latents</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Can language models learn from explanations in context?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Matthewson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2204.02329</idno>
		<ptr target="https://arxiv.org/abs/2204.02329" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic generation of programming exercises and code explanations using large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leinonen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3501385.3543957</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM Conference on International Computing Education Research V.1 (ICER 2022)</title>
		<meeting>the 2022 ACM Conference on International Computing Education Research V.1 (ICER 2022)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="27" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Strengthening the student toolbox: Study strategies to boost learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dunlosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Educator</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="12" to="21" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Test-enhanced learning: Taking memory tests improves long-term retention</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Roediger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Karpicke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="249" to="255" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Retrieval practice (testing) effect</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Roediger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Butler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of the Mind</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Pashler</surname></persName>
		</editor>
		<imprint>
			<publisher>Sage Publishing Co</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="660" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effects of frequent classroom testing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bangert-Drowns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><forename type="middle">C</forename><surname>Kulik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Educational Research</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="89" to="99" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The Cambridge handbook of expertise and expert performance</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kozbelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781316480748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The power of feedback</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Timperley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of educational research</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="81" to="112" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A surprising effect of feedback on learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vollmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning and instruction</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="589" to="602" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Focus on formative feedback</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Shute</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of educational research</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="153" to="189" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the effects of contextualized problem descriptions on problem solving</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leinonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whalley</surname></persName>
		</author>
		<idno type="DOI">10.1145/3441636.3442302</idno>
		<idno>doi:10.1145/3441636.3442302</idno>
		<ptr target="https://doi.org/10.1145/3441636.3442302" />
	</analytic>
	<monogr>
		<title level="m">Australasian Computing Education Conference, ACE &apos;21</title>
		<meeting><address><addrLine>New York, NY, USA, 2021</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Novice programmers and the problem description effect</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bouvier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lovellette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alshaigy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Craig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jackova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mccartney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zarb</surname></persName>
		</author>
		<idno type="DOI">10.1145/3024906.3024912</idno>
		<idno>doi:10.1145/3024906.3024912</idno>
		<ptr target="https://doi.org/10.1145/3024906.3024912" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ITiCSE Working Group Reports, ITiCSE &apos;16</title>
		<meeting>the 2016 ITiCSE Working Group Reports, ITiCSE &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the use of semantic-based aig to automatically generate programming exercises</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zavala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mendoza</surname></persName>
		</author>
		<idno type="DOI">10.1145/3159450.3159608</idno>
		<idno>doi:10.1145/ 3159450.3159608</idno>
		<ptr target="https://doi.org/10.1145/3159450.3159608" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th ACM Technical Symposium on Computer Science Education, SIGCSE &apos;18</title>
		<meeting>the 49th ACM Technical Symposium on Computer Science Education, SIGCSE &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="14" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large-scale simple question generation by template-based seq2seq learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing</title>
		<editor>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="75" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive navigation support for parameterized questions in object-oriented programming</title>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sosnovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brusilovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in the Synergy of Multiple Disciplines</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Cress</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Dimitrova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Specht</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg, Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the fairness of multiplevariant multiple-choice examinations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Russello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3287324.3287357</idno>
		<idno>doi:10.1145/3287324.3287357</idno>
		<ptr target="https://doi.org/10.1145/3287324.3287357" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th ACM Technical Symposium on Computer Science Education, SIGCSE &apos;19, Association for Computing Machinery</title>
		<meeting>the 50th ACM Technical Symposium on Computer Science Education, SIGCSE &apos;19, Association for Computing Machinery<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="462" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stochastic tree-based generation of program-tracing practice questions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stopera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frank-Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Simha</surname></persName>
		</author>
		<idno type="DOI">10.1145/3287324.3287492</idno>
		<idno>doi:10.1145/ 3287324.3287492</idno>
		<ptr target="https://doi.org/10.1145/3287324.3287492" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th ACM Technical Symposium on Computer Science Education, SIGCSE &apos;19</title>
		<meeting>the 50th ACM Technical Symposium on Computer Science Education, SIGCSE &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="91" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Superficial code-guise: Investigating the impact of surface feature changes on students&apos; programming question scores</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zilles</surname></persName>
		</author>
		<idno type="DOI">10.1145/3408877.3432413</idno>
		<idno>doi:10.1145/3408877.3432413</idno>
		<ptr target="https://doi.org/10.1145/3408877.3432413" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd ACM Technical Symposium on Computer Science Education, SIGCSE &apos;21</title>
		<meeting>the 52nd ACM Technical Symposium on Computer Science Education, SIGCSE &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Word problems in mathematics education: a survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Verschaffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schukajlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Star</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Van Dooren</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11858-020-01130-4</idno>
	</analytic>
	<monogr>
		<title level="j">ZDM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Personalized mathematical word problem generation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>O'rourke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popović</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/personalized-mathematical-word-problem-generation/" />
	</analytic>
	<monogr>
		<title level="m">IJCAI 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A theme-rewriting approach for generating algebra word problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1168</idno>
		<ptr target="https://aclanthology.org/D16-1168" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1617" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The robots are coming: Exploring the implications of openai codex on introductory programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Finnie-Ansley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luxton-Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Computing Education Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A closer look at tracing, explaining and code writing skills in the novice programmer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Venables</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lister</surname></persName>
		</author>
		<idno type="DOI">10.1145/1584322.1584336</idno>
		<idno>doi:10. 1145/1584322.1584336</idno>
		<ptr target="https://doi.org/10.1145/1584322.1584336" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Workshop on Computing Education Research Workshop, ICER &apos;09</title>
		<meeting>the Fifth International Workshop on Computing Education Research Workshop, ICER &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Common logic errors made by novice programmers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ettles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luxton-Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Denny</surname></persName>
		</author>
		<idno type="DOI">10.1145/3160489.3160493</idno>
		<idno>doi:10.1145/3160489.3160493</idno>
		<ptr target="https://doi.org/10.1145/3160489.3160493" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Australasian Computing Education Conference, ACE &apos;18</title>
		<meeting>the 20th Australasian Computing Education Conference, ACE &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="83" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Can novice programmers write c functions?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Izu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="DOI">10.1109/TALE.2018.8615375</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Teaching, Assessment, and Learning for Engineering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="965" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Process for adapting language models to society (palms) with values-targeted datasets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Solaiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dennison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06390</idno>
		<title level="m">Detoxifying language models risks marginalizing minority voices</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Extracting training data from large language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th USENIX Security Symposium (USENIX Security 21)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2633" to="2650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
