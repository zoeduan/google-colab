<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Paraphrasing with Large Language Models</title>
				<funder ref="#_NMdvGVb">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-11-21">21 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sam</forename><surname>Witteveen</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Red Dragon AI Red Dragon AI</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Martin</forename><surname>Andrews</surname></persName>
							<email>martin@reddragon.ai</email>
							<affiliation key="aff0">
								<address>
									<settlement>Red Dragon AI Red Dragon AI</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Paraphrasing with Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-21">21 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">30B8478A2AF96A758E77C5689E117A88</idno>
					<idno type="arXiv">arXiv:1911.09661v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, large language models such as GPT-2 have shown themselves to be extremely adept at text generation and have also been able to achieve high-quality results in many downstream NLP tasks such as text classification, sentiment analysis and question answering with the aid of fine-tuning. We present a useful technique for using a large language model to perform the task of paraphrasing on a variety of texts and subjects. Our approach is demonstrated to be capable of generating paraphrases not only at a sentence level but also for longer spans of text such as paragraphs without needing to break the text into smaller chunks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Paraphrase generation is an NLP task that has multiple uses in content creation, question answering, translation, and data augmentation. It is a task that has been attempted for many decades using statistical and rules-based approaches <ref type="bibr" target="#b12">(McKeown, 1979;</ref><ref type="bibr" target="#b13">Meteer and Shaked, 1988)</ref>.</p><p>We propose a system that generates paraphrased examples in an autoregressive fashion using a neural network, without the need for techniques such as top-k word selection or beam search.</p><p>We demonstrate that by using large language models we are able to produce not only paraphrases that are longer and of a higher quality than previous work, but can also paraphrase text beyond the individual sentence-level (i.e. full paragraphs at a time).</p><p>The large language models we use implement the encoder-decoder structure of the transformer architecture <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> which has been shown to learn different representations of language at each level of its encoding <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>. The power of language models like GPT-2 <ref type="bibr" target="#b19">(Radford et al., 2019)</ref> and BERT allows them to develop useful representations of language which can be used far beyond just generation of the next word <ref type="bibr" target="#b20">(Rothe et al., 2019)</ref>. In our experiments, we have observed that the models have representations of syntax and grammar, allowing them to be fine-tuned for the task of paraphrase generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Paraphrase generation has attracted a number of different NLP approaches. These have included rule-based approaches <ref type="bibr" target="#b12">(McKeown, 1979;</ref><ref type="bibr" target="#b13">Meteer and Shaked, 1988)</ref> and data-driven methods <ref type="bibr" target="#b10">(Madnani and Dorr, 2010)</ref>, with recently the most common approach being that the task is treated as a language translation task <ref type="bibr" target="#b0">(Bannard and Callison-Burch, 2005;</ref><ref type="bibr" target="#b1">Barzilay and McKeown, 2001;</ref><ref type="bibr" target="#b14">Pang et al., 2003)</ref> -often performed using a bilingual corpus pivoting back and forth <ref type="bibr" target="#b10">(Madnani and Dorr, 2010;</ref><ref type="bibr" target="#b17">Prakash et al., 2016;</ref><ref type="bibr" target="#b11">Mallinson et al., 2017)</ref>. Other methods proposed include more recently the use of Deep Reinforcement Learning <ref type="bibr" target="#b8">(Li et al., 2018)</ref> , supervised learning using sequence-to-sequence models <ref type="bibr" target="#b7">(Gupta et al., 2018;</ref><ref type="bibr" target="#b17">Prakash et al., 2016)</ref> and unsupervised approaches <ref type="bibr" target="#b2">(Bowman et al., 2016;</ref><ref type="bibr" target="#b21">Roy and Grangier, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we define the task, present our implementation, describe our evaluation protocol and the paraphrase datasets used.</p><p>Paraphrase generation can be described as generating a phrase which is semantically as close as possible to the original while being rewritten in new words and phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Large Language Model training</head><p>Training language models has become a popular way of creating models suited for Transfer Learning in the field of NLP. (Howard and Ruder, 2018;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USE ROUGE-L BLEU</head><p>In:</p><p>A prisoner can asphyxiate himself in 90 seconds and, after eight minutes or so, he will be brain dead. Out: In 90 seconds, a prisoner can asphyxiate himself and be brain dead after eight minutes or so.</p><p>0.9326 0.4706 0.4730</p><p>In:</p><p>The restaurant is a carved-off space up a couple of stairs to one side, dominated by faux bare-brick columns, faux-wood floors and an air of foetid despondency. Out: It is a carved-off space, up a couple of flights of stairs, to the other side of the restaurant, dominated by fake bare-brick columns, fake wood floors and an air of foetid despondency.</p><p>0.8954 0.5000 0.5348</p><p>In:</p><p>I signed a bill that made the problem worse, and I want to admit it, he said. Out: He signed a bill that made the problem worse and he wants to admit it.</p><p>0.8363 0.4667 0.5299</p><p>In:</p><p>It said the damage to the wing provided a pathway for hot gasses to penetrate the ship's thermal armor during Columbia's ill-fated reentry. Out: The document says the damage to the wing provided a pathway for hot gases to penetrate Columbia's thermal armour during its fatal re-entry.</p><p>0.9190 0.4545 0.5445</p><p>Table <ref type="table">1</ref>: Examples of GPT-2 generated paraphrased sentences with scores for each pair <ref type="bibr" target="#b16">Peters et al., 2018;</ref><ref type="bibr" target="#b18">Radford et al., 2018;</ref><ref type="bibr" target="#b6">Dong et al., 2019)</ref>. While these models are initially trained in a semi-supervised manner to predict the next word or words in a sequence, they can be finetuned and used for a variety of downstream NLP tasks such as text classification, sentiment analysis, tagging, and entity extraction.</p><p>More recently, large language models using transformer architectures are achieving state of the art results for many of these tasks while using less supervised data than previously needed.</p><p>One example of these large language models that has proven to be very good at text generation is GPT-2. It makes use of a transformer architecture and comes in various sizes up to 1.5 billion parameters. In these experiments, we have taken a pre-trained version of the GPT-2 model trained in a semi-supervised fashion on the WebText dataset <ref type="bibr" target="#b19">(Radford et al., 2019)</ref> of over 8 million documents with 40 GB of text in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-tuning for Task</head><p>We take the GPT-2 model and fine-tune it on a supervised dataset of pre-made paraphrase examples. These examples are fed into the model as original phrase / paraphrase pairs, separated by a specific identifying sequence (such as "&gt;&gt;&gt;&gt;").</p><p>This training is done for a small number of epochs to give the model just enough examples of what the task is asking from the model : The goal being to avoid overfitting the model on the new data, while giving it sufficient exposure to the task to enable it to learn the general pattern expected.</p><p>While we experimented with TPUs for the finetuning, in the end we were able to reproduce the same results on a single K-80 GPU with around 90 minutes of training.</p><p>Once the model is fine-tuned, we find that it can also produce similar paraphrase training examples if sampled from with no conditional input. To give an indication of training progress, these 'naive' paraphrases are sampled on a periodic basis during the training.</p><p>After fine-tuning on this dataset, we are then able to feed in any original phrase followed by the unique token and have the model generate paraphrases on demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Candidate Generation and Selection</head><p>After the model is trained, we then sample from the model using previously unseen sentences as conditional input. This conditional input allows us to generate multiple candidate sentences for the single original sentence.</p><p>While the quality of the paraphrases is somewhat variable, by generating multiple outputs and then scoring them, we can select just the best quality paraphrases based on a number of criteria that serve to filter our output down to a set of satisfactory results.</p><p>First, we obtain a similarity score between the generated paraphrase and the original sentence by using the Universal Sentence Encoder (USE) <ref type="bibr" target="#b3">(Cer et al., 2018)</ref> to make a 512 dimensional sentence embedding for each output sentence and then compare them to the embedding of the original sentence via the cosine similarity measure.</p><p>As a second step, we measure the ROUGE-L <ref type="bibr" target="#b9">(Lin, 2004</ref>) score of the candidate paraphrases against the original sentence and eliminate candidates with a ROUGE-L score of above 0.7 . This prevents candidates that are too close to the original sentence being chosen. After testing both cutoff scores for ROUGE-L and BLEU <ref type="bibr" target="#b15">(Papineni et al., 2002)</ref>, ROUGE-L has shown to be more useful at finding candidates that are more unique in comparison to the original sentence.</p><p>By choosing samples with sufficiently low ROUGE-L scores but as high a similarity as possible, we end up with an output that is semantically similar to the original phrase but has a unique word order when compared to the original phrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Datasets</head><p>We fine-tuned multiple versions of the model on several different datasets : 2 datasets of sentences and their matching paraphrases; and 1 dataset of paragraphs with matching paraphrases :</p><p>1. The MSR Paraphrase Identification dataset <ref type="bibr" target="#b5">(Dolan et al., 2004</ref>) which consists of just over 4,000 examples of original sentences with a matching paraphrased sentence in its train set.</p><p>2. An original dataset of 10,000 sentences from online news articles along with matching paraphrases that were human-generated.</p><p>3. A further original dataset of paragraphs with corresponding paraphrased paragraphs from various entertainment, news, and food articles found online, where the paraphrases were human-generated.</p><p>We fine-tuned 3 versions of the GPT-2 model, one corresponding to each dataset, and then made predictions using the same system outlined above.</p><p>By calculating USE, ROUGE-L and BLEU scores for each dataset we are able to quantify the quality of human-generated paraphrases and then use that as a comparison for the models generated sentences (see </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We implemented the system described above using GPT-2 and trained it on the different datasets for various lengths of training.</p><p>To evaluate the output of the model, we randomly selected sentences from sources such as Wikipedia, news sites and entertainment sites with no matching paraphrase to use as the conditional input to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Scoring</head><p>When comparing our generated sentences with the average scores of the original datasets, we can see that that they compare favorably.</p><p>As discussed earlier, we assessed the semantic similarity of the sentence meanings using Universal Sentence Encoder <ref type="bibr" target="#b3">(Cer et al., 2018)</ref> and compared them to the average USE score from the datasets that were trained on. This showed that the system can generate paraphrases which are semantically on par with the human-generated ones in each of the datasets.</p><p>We also compared the ROUGE-L <ref type="bibr" target="#b9">(Lin, 2004</ref>) scores of the generated samples with the average values for the datasets which were humangenerated. This again shows that our phrases are coherent and on par with human-generated paraphrases.</p><p>When we further compared the results of unfiltered examples generated by the model (Table <ref type="table" target="#tab_2">3</ref>) we observe that when the USE score is below 0.85 we see clear deterioration in the semantic similarity quality of the paraphrased versions.</p><p>We also observe that if the USE score is too close to 1.0 then the ROUGE-L score also rises and the generated examples are too similar in word USE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-L</head><p>In:</p><p>A prisoner can asphyxiate himself in 90 seconds and, after eight minutes or so, he will be brain dead.</p><p>Out 1:</p><p>After 8 minutes, a brain fart will subdue the sufferer. 0.524 0.0</p><p>Out 2:</p><p>After 8 minutes, he will be brain-dead and his heart will stop. 0.565 0.138</p><p>Out 3:</p><p>A brain aneurysm can asphyxiate itself in 90 seconds and, after eight minutes, it will be dead.</p><p>0.721 0.412</p><p>Out 4:</p><p>After eight minutes, a brain anesthetist can asphyxiate a prisoner in 90 seconds and for several minutes after that.</p><p>0.758 0.167</p><p>Out 5:</p><p>A brain-dead prisoner canasphyxiate himself in 90 seconds and then out loud after eight minutes.</p><p>0.809 0.312</p><p>Out 6:</p><p>At asphyxiation, the prisoner canasphyxiate himself in 90 seconds and, after 8 minutes, he will be brain dead.</p><p>0.884 0.514</p><p>Out 7:</p><p>After eight minutes, a prisoner can asphyxiate himself in 90 seconds and, after that, he will be brain dead.</p><p>0.884 0.514 Out 8*: In 90 seconds, a prisoner can asphyxiate himself and be brain dead after eight minutes or so 0.932 0.473 Out 9:</p><p>A prisoner can asphyxiate himself in 90 seconds and, after eight minutes, he will be brain dead.</p><p>0.972 0.824 Due to the pre-training of the Language Model, the model is able to generalize to and generate paraphrases for types of content it has never seen during the fine-tuning phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>The technique outlined in this paper shows the applicability of large language models to the paraphrasing task. It also highlights that there is still much to be learnt about further applications of large language models, and also the approaches used to fine-tune and use them for applications.</p><p>Most of the results from models such as GPT-2 have focused on the quality of text generation rather than quantitative methods for measuring and improving the quality of text created, to make it more consistent and usable. We propose the scoring and filtering of candidates using techniques such as we have shown with USE and ROUGE-L, may be a useful technique not just for paraphrasing but other text generation tasks.</p><p>The ability of our technique to work with long spans of text also gives it an advantage over prior work which used rule-based and other statistical approaches which performed best on shorter spans of text.</p><p>Our experiments show that pre-training of GPT-2 on such a large amount of data in the WebText dataset allows it to 'understand' the syntax and to a degree the grammar of English allowing it to be able to quickly learn the task of paraphrasing through fine-tuning training on a small set of paraphrasing examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>Extrapolating from the paraphrasing results into more generalizable ideas, we hope to investigate the extent by which the representations learned in the different layers of the transformer network correspond to different parts of the linguistic hierarchy. One possible approach to doing this would be to trace a set of 'markers' through the transformer networks existing attention mechanism, in parallel to the text which gives rise to that structure.</p><p>In addition, the ability of the networks to learn tasks within the span of a single context frame indicates the possibility of an inherent bias towards meta-( or one-shot) learning. These will be the subject of further work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Table 2). Average USE, ROUGE-L, BLEU Scores of the datasets</figDesc><table><row><cell>Dataset</cell><cell>USE</cell><cell>R-L</cell><cell>BLEU</cell></row><row><cell>MSR train</cell><cell cols="3">0.8462 0.4315 0.4593</cell></row><row><cell>MSR test</cell><cell cols="3">0.8415 0.4202 0.4966</cell></row><row><cell>News dataset</cell><cell cols="3">0.8948 0.4686 0.5648</cell></row><row><cell cols="4">Paragraphs dataset 0.9208 0.4966 0.5762</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Showing Candidates Selection and Scoring -*Selected Sentence</figDesc><table><row><cell>and phrase selection to the original sentence to be</cell></row><row><cell>useful paraphrases.</cell></row><row><cell>This technique can be performed not only at</cell></row><row><cell>sentence-level but also to generate paragraph-level</cell></row><row><cell>paraphrases. Comparing USE and ROUGE-L</cell></row><row><cell>scores of the generated paragraphs we see they are</cell></row><row><cell>again on par with the human generated examples</cell></row><row><cell>from our paragraph dataset (samples are given in</cell></row><row><cell>the Supplemental Materials).</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="institution">Google</rs> for access to the <rs type="programName">TFRC TPU</rs> program which was used in training and fine-tuning models for this paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NMdvGVb">
					<orgName type="program" subtype="full">TFRC TPU</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Materials</head><p>In:</p><p>The first step in solving the problem is to prop the door open with tape. This means your cat will now be able to see through to the other side; your cat will likely begin using the kitty door immediately. Once your cat has gotten used to using the kitty door, remove the tape. Sometimes cats will continue to use the kitty door without any more prompting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out:</head><p>The first step in resolving the problem is to prop the gate with tape. This implies that your cat will shortly be enabled to see through to the other side; your cat will likely immediately begin using the kitty door. Remove the tape when your cat has got used to using the kitty door. Sometimes cats will continue to use the kitty gate without any more command. In:</p><p>There are three reasons why Canada is one of the best countries in the world. First, Canada has an excellent health care system. All Canadians have access to medical services at a reasonable price. Second, Canada has a high standard of education. Students are taught by well-trained teachers and are encouraged to continue studying at university. Finally, Canada's cities are clean and efficiently managed. Canadian cities have many parks and lots of space for people to live. As a result, Canada is a desirable place to live.</p><p>Out:</p><p>There are three main reasons why Canada is one of the finest locations in the globe. First, Canada has an excellent health care system. All Canadians have access to medical care at a reasonable cost. Second, Canada has a high standard of education. Students are taught by welltrained teachers and are urged to continue studying at university. Finally, Canada's cities are clean and efficiently managed with many parks and lots of room for living, as a consequence, Canada is a desirable location.</p><p>Scores: USE: 0.9775 ROUGE-L: 0.6832 BLEU: 0.7182 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Paraphrasing with bilingual parallel corpora</title>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">J</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219914</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="597" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extracting paraphrases from a parallel corpus</title>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL &apos;01</title>
		<meeting>the 39th Annual Meeting on Association for Computational Linguistics, ACL &apos;01<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Universal sentence encoder</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Yi Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhomni</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><surname>Kurzweil</surname></persName>
		</author>
		<idno>ArXiv, abs/1803.11175</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno>CoRR, abs/1905.03197</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prawaan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence. Jeremy Howard and Sebastian Ruder</title>
		<meeting><address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
	<note>A deep generative framework for paraphrase generation Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Paraphrase generation with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3865" to="3878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2004</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating phrasal and sentential paraphrases: A survey of data-driven methods</title>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="341" to="387" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Paraphrasing revisited with neural machine translation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="881" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Paraphrasing using given and new information in a question-answer system</title>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Strategies for effective paraphrasing</title>
		<author>
			<persName><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varda</forename><surname>Shaked</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1988">1988. 1988</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Coling Budapest</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations</title>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural paraphrase generation with stacked residual LSTM networks</title>
		<author>
			<persName><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashequl</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oladimeji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Farri</surname></persName>
		</author>
		<idno>CoRR, abs/1610.03098</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Leveraging pre-trained checkpoints for sequence generation tasks</title>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<idno>CoRR, abs/1907.12461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised paraphrasing without translation</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno>ArXiv, abs/1905.12752</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
