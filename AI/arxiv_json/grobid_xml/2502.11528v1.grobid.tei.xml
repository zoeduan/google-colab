<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey of Personalized Large Language Models: Progress and Future Directions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiahong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zexuan</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongyang</forename><surname>Li</surname></persName>
							<email>lizhongyang6@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Technologies Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
							<email>daquanyu@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Technologies Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
							<email>jiamie.zhu@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Technologies Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minda</forename><surname>Hu</surname></persName>
							<email>mindahu21@cse.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Menglin</forename><surname>Yang</surname></persName>
							<email>menglin.yang@outlook.com</email>
							<affiliation key="aff2">
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
							<email>king@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey of Personalized Large Language Models: Progress and Future Directions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">438464E302EBBEC09D28C15310D298B7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the Github Repo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, substantial progress has been made in Large Language Models (LLMs) such as GPT, PaLM, LLaMA, DeepSeek, and their variants. These models have demonstrated remarkable versatility, achieving state-of-the-art performance across various natural language processing tasks, including question answering, reasoning, and machine translation <ref type="bibr" target="#b89">[Zhao et al., 2023]</ref>, with minimal task-specific adaptation. The Necessity of Personalized LLMs (PLLMs) While LLMs excel in general knowledge and multi-domain reasoning, their lack of personalization creates challenges in situations where user-specific understanding is crucial. For instance, conversational agents need to adapt to a user's preferred tone and incorporate past interactions to deliver relevant, personalized responses. 8As LLMs evolve, integrating personalization capabilities has become a promising direction for advancing human-AI interaction across diverse domains such as education, healthcare, and finance <ref type="bibr" target="#b18">[Hu et al., 2024;</ref><ref type="bibr">Zhang et al., 2024e,d;</ref><ref type="bibr" target="#b94">Zhu et al., 2024;</ref><ref type="bibr">Wang et al., 2023a</ref><ref type="bibr">Wang et al., , 2024a]]</ref>.</p><p>Technical Challenges Despite its promise, personalizing LLMs presents several challenges. These include efficiently representing and integrating diverse user data, addressing privacy concerns, managing long-term user memories, adaptivity accommodating users' diverse needs and evolving behaviors <ref type="bibr" target="#b57">[Salemi et al., 2023]</ref>. Moreover, achieving personalization often requires balancing accuracy and efficiency while addressing biases and maintaining fairness in the generated outputs.</p><p>Contributions Despite growing interest, the field of PLLMs lacks a systematic review that consolidates recent advancements. This survey aims to bridge the gap by systematically organizing existing research on PLLMs and offering insights into their methodologies and future directions. The contributions of this survey can be summarized as follows: (1) A structured taxonomy: We propose a comprehensive taxonomy, providing a technical perspective on the existing approaches to building PLLMs. (2) A comprehensive review: We systematically review state-of-the-art methods for PLLMs, analyzing</p><p>arXiv:2502.11528v1 [cs.AI] 17 Feb 2025 Personalized Large Language Models Personalized Prompting (Input level)</p><p>Profile-Augmented ( ยง3.1) Cue-CoT <ref type="bibr">[Wang et al., 2023b]</ref>, PAG <ref type="bibr" target="#b54">[Richardson et al., 2023]</ref>, ONCE <ref type="bibr">[Liu et al., 2024b]</ref>, Matryoshka <ref type="bibr">[Li et al., 2024a]</ref>, RewriterSlRl <ref type="bibr">[Li et al., 2024b]</ref>, <ref type="bibr">CoS [He et al., 2024]</ref> Retrieval-Augmented ( ยง3.2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory</head><p>MemPrompt <ref type="bibr" target="#b36">[Madaan et al., 2022]</ref>, TeachMe <ref type="bibr" target="#b9">[Dalvi et al., 2022]</ref>, MaLP <ref type="bibr">[Zhang et al., 2024a]</ref>, <ref type="bibr" target="#b82">[Zhang et al., 2023]</ref>, LD-Agent <ref type="bibr">[Li et al., 2024c]</ref>, MemoRAG <ref type="bibr" target="#b48">[Qian et al., 2024]</ref> Retriever IPA,FiD <ref type="bibr" target="#b57">[Salemi et al., 2023]</ref>, MSP <ref type="bibr" target="#b91">[Zhong et al., 2022]</ref>, AuthorPred <ref type="bibr" target="#b27">[Li et al., 2023]</ref>, PEARL <ref type="bibr" target="#b40">[Mysore et al., 2023]</ref>, ROPG <ref type="bibr">[Salemi et al., 2024]</ref>, HYDRA <ref type="bibr" target="#b95">[Zhuang et al., 2024]</ref>, RECAP <ref type="bibr" target="#b32">[Liu et al., 2023]</ref> Soft-Fused ( ยง3.3) UEM <ref type="bibr" target="#b10">[Doddapaneni et al., 2024]</ref>, PERSOMA <ref type="bibr" target="#b16">[Hebert et al., 2024]</ref>, REGEN <ref type="bibr" target="#b59">[Sayana et al., 2024]</ref>, PeaPOD <ref type="bibr" target="#b53">[Ramos et al., 2024]</ref>, User-LLM <ref type="bibr" target="#b41">[Ning et al., 2024]</ref>, RECAP <ref type="bibr" target="#b32">[Liu et al., 2023]</ref>, GSMN <ref type="bibr" target="#b73">[Wu et al., 2021]</ref> Personalized Adaptation (Model level)</p><p>One PEFT All Users ( ยง4.1) PEFT-U <ref type="bibr" target="#b8">[Clarke et al., 2024]</ref>, PLoRA <ref type="bibr">[Zhang et al., 2024c]</ref>, LM-P <ref type="bibr" target="#b72">[Woลบniak et al., 2024]</ref>, UserIdentifier <ref type="bibr" target="#b39">[Mireshghallah et al., 2021]</ref>, Review-LLM <ref type="bibr">[Peng et al., 2024b]</ref>,</p><p>MiLP <ref type="bibr">[Zhang et al., 2024b]</ref>, RecLoRA <ref type="bibr" target="#b94">[Zhu et al., 2024]</ref>, iLoRA <ref type="bibr" target="#b23">[Kong et al., 2024]</ref> One PEFT Per User ( ยง4.2)</p><p>UserAdapter <ref type="bibr" target="#b90">[Zhong et al., 2021]</ref>, PocketLLM <ref type="bibr">[Peng et al., 2024a]</ref>, OPPU <ref type="bibr">[Tan et al., 2024b]</ref>, PER-PCS <ref type="bibr">[Tan et al., 2024a]</ref>, <ref type="bibr" target="#b67">[Wagner et al., 2024]</ref>, FDLoRA <ref type="bibr" target="#b47">[Qi et al., 2024]</ref>, HYDRA <ref type="bibr" target="#b95">[Zhuang et al., 2024]</ref> Personalized Alignment (Objective level)</p><p>Data Construction ( ยง5.1) <ref type="bibr">[Wu et al., 2024c]</ref>, PLUM <ref type="bibr" target="#b37">[Magister et al., 2024]</ref>, <ref type="bibr" target="#b26">[Lee et al., 2024]</ref>, <ref type="bibr" target="#b49">[Qin et al., 2024]</ref>, PRISM <ref type="bibr" target="#b22">[Kirk et al., 2024]</ref>, <ref type="bibr" target="#b96">PersonalLLM [Zollo et al., 2024]</ref> Optimization ( ยง5.2)</p><p>MORLHF <ref type="bibr" target="#b74">[Wu et al., 2023]</ref>, MODPO <ref type="bibr" target="#b92">[Zhou et al., 2023]</ref>, Reward Soups <ref type="bibr" target="#b52">[Rame et al., 2024]</ref>,</p><p>Personalized Soups <ref type="bibr" target="#b20">[Jang et al., 2023]</ref>, MOD <ref type="bibr" target="#b60">[Shi et al., 2024]</ref>, PAD <ref type="bibr">[Chen et al., 2024b]</ref>, PPT <ref type="bibr" target="#b25">[Lau et al., 2024]</ref>, VPL <ref type="bibr" target="#b46">[Poddar et al., 2024]</ref> Others Analysis Role of User Profile <ref type="bibr">[Wu et al., 2024a]</ref>, RAG vs. PEFT <ref type="bibr">[Salemi and Zamani, 2024]</ref>, Safety-Utility <ref type="bibr" target="#b66">[Vijjini et al., 2024]</ref> Benchmark LaMP <ref type="bibr" target="#b57">[Salemi et al., 2023]</ref>, Longlamp <ref type="bibr" target="#b24">[Kumar et al., 2024]</ref>, PGraphRAG <ref type="bibr" target="#b0">[Au et al., 2025]</ref>,</p><p>PerLTQA <ref type="bibr" target="#b11">[Du et al., 2024]</ref>, PEFT-U <ref type="bibr" target="#b8">[Clarke et al., 2024]</ref>, REGEN <ref type="bibr" target="#b59">[Sayana et al., 2024]</ref> , PER-CHAT <ref type="bibr" target="#b73">[Wu et al., 2021]</ref>, <ref type="bibr" target="#b96">PersonalLLM [Zollo et al., 2024]</ref>, ALOE <ref type="bibr">[Wu et al., 2024c]</ref> Figure <ref type="figure">2</ref>: A taxonomy of PLLMs with representative examples.</p><p>fine-grained differences among the methods.</p><p>(3) Future directions: We highlight current limitations, such as data privacy and bias, and outline promising avenues for future research, including multimodal personalization, edge computing, lifelong updating, trustworthiness, and other emerging challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Large Language Models</head><p>Large Language Models (LLMs) generally refer to models that utilize the Transformer architecture and are equipped with billions of parameters trained on trillions of text tokens. These models have demonstrated substantial improvements in a myriad of tasks related to natural language understanding and generation, increasingly proving beneficial in assisting human activities. In this work, we mainly focus on autoregressive LLMs, which are based on two main architectures: decoder-only models and encoder-decoder models. Encoderdecoder models such as Flan-T5 <ref type="bibr">[Chung et al., 2022]</ref> and ChatGLM <ref type="bibr" target="#b81">[Zeng et al., 2022]</ref> analyze input through the encoder for semantic representations, making them effective in language understanding in addition to generation. Decoderonly LLMs focus on left-to-right generation by predicting the next token in a sequence, with numerous instances <ref type="bibr" target="#b2">[Brown et al., 2020;</ref><ref type="bibr" target="#b6">Chowdhery et al., 2022;</ref><ref type="bibr" target="#b64">Touvron et al., 2023;</ref><ref type="bibr" target="#b14">Guo et al., 2025]</ref> under this paradigm achieving breakthroughs in advanced capabilities like instruction following and reasoning. However, these models are typically pre-trained on generalpurpose data and lack an understanding of specific user information. As a result, they are unable to generate responses tailored to a user's unique tastes, preferences, and expectations, limiting their effectiveness in personalized applications where user-specific adaptation is critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Statement</head><p>Personalized Large Language Models (PLLMs) generate responses that align with the user's style and expectations, offering diverse answers to the same query for different users <ref type="bibr" target="#b8">[Clarke et al., 2024]</ref>. A PLLM is defined as an LLM that generates responses conditioned not only on an input query q, but also on a user u's personalized data C u . It aims to predict the most probable response sequence y given a query q and the personalized context C u , such that: y = argmax y P (y | q, C u ). The personalized data C u may encapsulate information about the user's preferences, history, context, and other user-specific attributes. These can include (Figure <ref type="figure" target="#fig_0">1</ref>):</p><p>โข Profile/Relationship: User profile, including attributes (e.g., name, gender, occupation), and relationships (e.g., friends, family members), such as C u = {A, 18, student, friends:{B, C, D} . . . }.</p><p>โข Historical Dialogues: Historical dialogues, such as question-answer pairs that user u interacts with the LLM (e.g., C u = {(q 0 , a 0 ), (q 1 , a 1 ), . . . , (q i , a i )}), where each q i is a query and a i is the corresponding answer.</p><p>โข Historical Content: Includes documents, previous reviews, comments or feedback from user u. For example, C u = {I like Avtar because . . . , . . . }.</p><p>โข Historical Interactions: Includes historical interactions, preferences, ratings from user u. For example, C u = {The Lord of the Rings : 5, Interstellar : 3 . . . }.</p><p>By incorporating personalized data, PLLMs enhance traditional LLMs, improving response generation, recommendations, and classification tasks.</p><p>Note that our survey differs significantly from role-play related LLM personalization <ref type="bibr" target="#b65">[Tseng et al., 2024;</ref><ref type="bibr">Chen et al., 2024a;</ref><ref type="bibr">Zhang et al., 2024f]</ref>. While role-play focuses on mimicking characters during conversations, PLLMs in this survey focus on understanding users' contexts and preferences to meet their specific needs. Compared to <ref type="bibr">[Zhang et al., 2024f]</ref>, which emphasizes broad categories, our work provides a systematic analysis of techniques to enhance PLLM efficiency and performance, with a more detailed technical classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Proposed Taxonomy</head><p>We propose a taxonomy (as illustrated in Figure <ref type="figure" target="#fig_0">1</ref> and Figure 2) from technical perspectives, categorizing the methods for Personalized Large Language Models (PLLMs) into three major levels: (1) Input level: Personalized Prompting focuses on handling user-specific data outside the LLM and injecting it into the model. (2) Model level: Personalized Adaptation emphasizes designing a framework to efficiently fine-tune or adapt model parameters for personalization. (3) Objective Level: Personalized Alignment aims to refine model behavior to align with user preferences effectively. Due to space limitations, analysis papers, datasets, and benchmarks are summarized in the Github Repo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Personalized Prompting</head><p>Prompt engineering acts as a bridge for interaction between users and LLMs. In this survey, prompting involves guiding an LLM to generate desired outputs using various techniques, from traditional text prompts to advanced methods like soft embedding. Soft embedding can be extended not only through input but also via cross-attention or by adjusting output logits, enabling more flexible and context-sensitive responses.</p><p>The framework can be expressed as, for each user u:</p><formula xml:id="formula_0">y = f LLM (q โ ฯ (C u )) ,<label>(1)</label></formula><p>where, f LLM is the LLM model that generates the response; ฯ is a function that extracts relevant context from the user's personal context C u ; โ represents the combination operator that fuses the query q and the relevant personalized context ฯ(C u ), producing enriched information for the LLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Profile-Augmented Prompting</head><p>Profile-augmented prompting methods explicitly utilize summarized user preferences and profiles in natural language to augment LLMs' input at the token level (ฯ is the summarizer model). Figure <ref type="figure" target="#fig_2">3</ref>(a) shows the illustration.</p><p>Non-tuned Summarizer A frozen LLM can be directly used as the summarizer to summarize user profiles due to its strong language understanding capabilities, i.e., ฯ</p><formula xml:id="formula_1">(C u ) = f LLM (C u ).</formula><p>For instance, Cue-CoT <ref type="bibr">[Wang et al., 2023b]</ref> employs chainof-thought prompting for personalized profile augmentation, using LLMs to extract and summarize user status (e.g., emotion, personality, and psychology) from historical dialogues. PAG <ref type="bibr" target="#b54">[Richardson et al., 2023]</ref> leverages instruction-tuned LLMs to pre-summarize user profiles based on historical content. The summaries are stored offline, enabling efficient personalized response generation while meeting runtime constraints. ONCE <ref type="bibr">[Liu et al., 2024b]</ref> prompts closed-source LLMs to summarize topics and regions of interest from users' browsing history, enhancing personalized recommendations.</p><p>Tuned Summarizer Black-box LLMs are sensitive to input noise, like off-topic summaries, and struggle to extract relevant information. Thus, training the summarizer to adapt to user preferences and style is essential. Matryoshka <ref type="bibr">[Li et al., 2024a]</ref> uses a white-box LLM to summarize user histories, similar to PAG, but fine-tunes the summarizer instead of the generator LLM. RewriterSlRl <ref type="bibr">[Li et al., 2024b]</ref> rewrites the query q instead of concatenating summaries, optimized with supervised and reinforcement learning.</p><p>CoS <ref type="bibr">[He et al., 2024]</ref> is a special case that assumes a brief user profile ฯ (C u ) and amplifies its influence in LLM response generation by comparing output probabilities with and without the profile, adjusting personalization without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retrieval-Augmented Prompting</head><p>Retrieval-augmented prompting <ref type="bibr" target="#b13">[Gao et al., 2023;</ref><ref type="bibr" target="#b12">Fan et al., 2024;</ref><ref type="bibr" target="#b50">Qiu et al., 2024]</ref> excels at extracting the most relevant records from user data to enhance PLLMs (See Figure <ref type="figure" target="#fig_2">3(b)</ref>). Due to the complexity and volume of user data, many methods use an additional memory for more effective retrieval. Common retrievers including sparse (e.g., <ref type="bibr">BM25 [Robertson et al., 1995]</ref>), and dense retrievers (e.g., Faiss <ref type="bibr" target="#b21">[Johnson et al., 2019]</ref>, Contriever <ref type="bibr" target="#b19">[Izacard et al., 2021]</ref>). These methods effectively manage the increasing volume of user data within the LLM's context limit, improving relevance and personalization by integrating key evidence from the user's personalized data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Personalized Memory Construction</head><p>This part designs mechanisms for retaining and updating memory to enable efficient retrieval of relevant information.</p><p>Non-Parametric Memory This category maintains a tokenbased database, storing and retrieving information in its original tokenized form without using parameterized vector representations. For example, MemPrompt <ref type="bibr" target="#b36">[Madaan et al., 2022]</ref> and TeachMe <ref type="bibr" target="#b9">[Dalvi et al., 2022</ref>] maintain a dictionary-based feedback memory (key-value pairs of mistakes and user feedback). MemPrompt focuses on prompt-based improvements, whereas TeachMe emphasizes continual learning via dynamic memory that adapts over time. MaLP <ref type="bibr">[Zhang et al., 2024a]</ref> further integrates multiple memory types, leveraging working memory for immediate processing, short-term memory (STM) for quick access, and long-term memory (LTM) for storing key knowledge.</p><p>Parametric Memory Recent studies parameterize and project personalized user data into a learnable space, with parametric memory filtering out redundant context to reduce noise. For instance, LD-Agent <ref type="bibr">[Li et al., 2024c]</ref> maintains memory with separate short-term and long-term banks, encoding long-term events as parametric vector representations refined by a tunable module and retrieved via an embeddingbased mechanism. MemoRAG <ref type="bibr" target="#b48">[Qian et al., 2024]</ref>, in contrast, adopts a different approach by utilizing a lightweight LLM as memory to learn user-personalized data. Instead of maintaining a vector database for retrieval, it generates a series of tokens as a draft to further guide the retriever, offering a more dynamic and flexible method for retrieval augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Personalized Memory Retrieval</head><p>The key challenge in the personalized retriever design lies in selecting not only relevant but also representative personalized data for downstream tasks. LaMP <ref type="bibr" target="#b57">[Salemi et al., 2023]</ref> investigates how retrieved personalized information affects the responses of large language models (LLMs) through two mechanisms: in-prompt augmentation (IPA) and fusion-in-decoder (FiD). PEARL <ref type="bibr" target="#b40">[Mysore et al., 2023]</ref> and ROPG <ref type="bibr">[Salemi et al., 2024]</ref> similarly aim to enhance the retriever using personalized generation-calibrated metrics, improving both personalization and text quality of retrieved documents. Meanwhile, HYDRA <ref type="bibr" target="#b95">[Zhuang et al., 2024]</ref> trains a reranker to prioritize the most relevant information additionally from top-retrieved historical records for enhanced personalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Soft-Fused Prompting</head><p>Soft prompting differs from profile-augmented prompting by compressing personalized data into soft embeddings, rather than summarizing it into discrete tokens. These embeddings are generated by a user feature encoder ฯ.</p><p>In this survey, we generalize the concept of soft prompting, showing that soft embeddings can be integrated (combination operator โ) not only through the input but also via cross-attention or by adjusting output logits, allowing for more flexible and context-sensitive responses (See Figure <ref type="figure" target="#fig_2">3(c)</ref>).</p><p>Input Prefix Soft prompting, used as an input prefix, focuses on the embedding level by concatenating the query embedding with the soft embedding, and is commonly applied in recommendation tasks. UEM <ref type="bibr" target="#b10">[Doddapaneni et al., 2024]</ref> is a user embedding module (transformer network) that generates a soft prompt conditioned on the user's personalized data. PERSOMA <ref type="bibr" target="#b16">[Hebert et al., 2024]</ref> enhances UEM by employing resampling, selectively choosing a subset of user interactions based on relevance and importance. REGEN <ref type="bibr" target="#b59">[Sayana et al., 2024]</ref> combines item embeddings from user-item interactions via collaborative filtering and item descriptions using a soft prompt adapter to generate contextually personalized responses. PeaPOD <ref type="bibr" target="#b53">[Ramos et al., 2024]</ref> personalizes soft prompts by distilling user preferences into a limited set of learnable, dynamically weighted prompts. Unlike previously mentioned methods, which focus on directly embedding user interactions or resampling relevant data, PeaPOD adapts to user interests by weighting a shared set of prompts. Cross-Attention Cross-attention enables the model to process and integrate multiple input sources by allowing it to attend to personalized data and the query. User-LLM <ref type="bibr" target="#b41">[Ning et al., 2024]</ref> uses an autoregressive user encoder to convert historical interactions into embeddings through self-supervised learning, which are then integrated via cross-attention. The system employs joint training to optimize both the retriever and generator for better performance. RECAP <ref type="bibr" target="#b32">[Liu et al., 2023]</ref> utilizes a hierarchical transformer retriever designed for dialogue domains to fetch personalized information. This information is integrated into response generation via a context-aware prefix encoder, improving the model's ability to generate personalized, contextually relevant responses. Output Logits GSMN <ref type="bibr" target="#b73">[Wu et al., 2021]</ref> retrieves relevant information from personalized data, encodes it into soft embeddings, and uses them in attention with the query vector. Afterward, the resulting embeddings are concatenated with the LLM-generated embeddings, modifying the final logits to produce more personalized and contextually relevant responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussions</head><p>The three prompting methods have distinct pros and cons: 1) Profile-augmented prompting improves efficiency by compressing historical data but risks information loss and reduced personalization. 2) Retrieval-augmented prompting offers rich, context-aware inputs and scales well for long-term memory but can suffer from computational limits and irrelevant data retrieval. 3) Soft prompting efficiently embeds user-specific info, capturing semantic nuances without redundancy, but is limited to black-box models and lacks explicit user preference analysis. Overall, prompting-based methods are efficient and adaptable, and enable dynamic personalization with minimal computational overhead. However, they lack deeper personalization analysis as they rely on predefined prompt structures to inject user-specific information and are limited in accessing global knowledge due to the narrow scope of prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Personalized Adaptation</head><p>PLLMs require balancing fine-tuning's deep adaptability with the efficiency of prompting. Therefore, specialized methods need to be specifically designed for PLLMs to address these challenges utilizing parameter-efficient fine-tuning methods (PEFT), such as LoRA <ref type="bibr" target="#b17">[Hu et al., 2021;</ref><ref type="bibr" target="#b79">Yang et al., 2024]</ref>, IA3 <ref type="bibr" target="#b31">[Liu et al., 2022]</ref>, etc. (See Figure <ref type="figure" target="#fig_3">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">One PEFT All Users</head><p>This method trains on all users' data using a shared PEFT module, eliminating the need for separate modules per user.</p><p>The shared module's architecture can be further categorized.</p><p>Single PEFT PLoRA <ref type="bibr">[Zhang et al., 2024c]</ref> and LM-P <ref type="bibr" target="#b72">[Woลบniak et al., 2024]</ref> utilize LoRA for PEFT of LLM, injecting personalized information via user embeddings and user IDs, respectively. PLoRA is further extended and supports online training and prediction for cold-start scenarios.</p><p>UserIdentifier <ref type="bibr" target="#b39">[Mireshghallah et al., 2021]</ref> uses a static, nontrainable user identifier to condition the model on user-specific information, avoiding the need for trainable user-specific parameters and reducing training costs. Review-LLM <ref type="bibr">[Peng et al., 2024b]</ref> aggregates users' historical behaviors and ratings into prompts to guide sentiment and leverages LoRA for efficient fine-tuning. However, these methods rely on a single architecture with fixed configurations (e.g., hidden size, insertion layers), making them unable to store and activate diverse information for personalization <ref type="bibr" target="#b93">[Zhou et al., 2024]</ref>. To solve this problem, MiLP <ref type="bibr">[Zhang et al., 2024b]</ref> utilizes a Bayesian optimization strategy to automatically identify the optimal configuration for applying multiple LoRA modules, enabling efficient and flexible personalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixture of Experts (MoE)</head><p>Several methods use the LoRA module, but with a static configuration for all users. This lack of parameter personalization limits adaptability to user dynamics and preference shifts, potentially resulting in suboptimal performance <ref type="bibr" target="#b3">[Cai et al., 2024]</ref>. RecLoRA <ref type="bibr" target="#b94">[Zhu et al., 2024]</ref> addresses this limitation by maintaining a set of parallel, independent LoRA weights and employing a soft routing method to aggregate meta-LoRA weights, enabling more personalized and adaptive results. Similarly, iLoRA <ref type="bibr" target="#b23">[Kong et al., 2024]</ref> creates a diverse set of experts (LoRA) to capture specific aspects of user preferences and generates dynamic expert participation weights to adapt to user-specific behaviors. Shared PEFT methods rely on a centralized approach, where user-specific data is encoded into a shared adapter by centralized LLMs. This limits the model's ability to provide deeply personalized experiences tailored to individual users. Furthermore, using a centralized model often requires users to share personal data with service providers, raising concerns about the storage, usage, and protection of this data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">One PEFT Per User</head><p>Equipping a user-specific PEFT module makes LLM deployment more personalized while preserving data privacy. However, the challenge lies in ensuring efficient operation in resource-limited environments, as users may lack sufficient local resources to perform fine tuning.</p><p>No Collaboration There is no collaboration or coordination between adapters or during the learning process for each use in this category. UserAdapter <ref type="bibr" target="#b90">[Zhong et al., 2021]</ref> personalizes models through prefix-tuning, fine-tuning a unique prefix vector for each user while keeping the underlying transformer model shared and frozen. PocketLLM <ref type="bibr">[Peng et al., 2024a]</ref> utilizes a derivative-free optimization approach, based on MeZo <ref type="bibr" target="#b38">[Malladi et al., 2023]</ref>, to fine-tune LLMs on memoryconstrained mobile devices. OPPU <ref type="bibr">[Tan et al., 2024b]</ref> equips each user with a LoRA module.</p><p>Collaborative Efforts The "one-PEFT-per-user" paradigm without collaboration is computationally and storage-intensive, particularly for large user bases. Additionally, individually owned PEFTs hinder community value, as personal models cannot easily share knowledge or benefit from collaborative improvements. PER-PCS <ref type="bibr">[Tan et al., 2024a]</ref> enables efficient and collaborative PLLMs by sharing a small fraction of PEFT parameters across users. It first divides PEFT parameters into reusable pieces with routing gates and stores them in a shared pool. For each target user, pieces are autoregressively selected from other users, ensuring scalability, efficiency, and personalized adaptation without additional training.</p><p>Another efficient collaborative strategy is based on the federated learning (FL) framework. For example, <ref type="bibr" target="#b67">Wagner et al. [2024]</ref> introduces a FL framework for on-device LLM fine-tuning, using strategies to aggregate LoRA model parameters and handle data heterogeneity efficiently, outperforming purely local fine-tuning. FDLoRA <ref type="bibr" target="#b47">[Qi et al., 2024]</ref>  them via adaptive fusion, enhancing performance while minimizing communication and computing costs.</p><p>There are other frameworks that can be explored, such as HYDRA <ref type="bibr" target="#b95">[Zhuang et al., 2024]</ref>, which also employs a base model to learn shared knowledge. However, in contrast to federated learning, it assigns distinct heads to each individual user to extract personalized information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussions</head><p>Fine-tuning methods enable deep personalization by modifying a large set of model parameters, and parameter-efficient fine-tuning methods (e.g., prefix vectors or adapters) reduce computational cost and memory requirements while maintaining high personalization levels. These methods improve task adaptation by tailoring models to specific user needs, enhancing performance in tasks like sentiment analysis and recommendations. They also offer flexibility, allowing userspecific adjustments while leveraging pre-trained knowledge. However, they still face the risk of overfitting, particularly with limited or noisy user data, which can impact generalization and performance for new or diverse users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Personalized Alignment</head><p>Alignment techniques <ref type="bibr" target="#b1">[Bai et al., 2022;</ref><ref type="bibr" target="#b51">Rafailov et al., 2024]</ref> typically optimize LLMs to match the generic preferences of humans. However, in reality, individuals may exhibit significant variations in their preferences for LLM responses across different dimensions like language style, knowledge depth, and values. Personalized alignment seeks to further align with individual users' unique preferences beyond generic preferences. A significant challenge in personalized alignment is creating high-quality user-specific preference datasets, which are more complex than general alignment datasets due to data sparsity. The second challenge arises from the need to refine the canonical RLHF framework <ref type="bibr" target="#b42">[Ouyang et al., 2022]</ref> to handle the diversification of user preferences, which is essential for integrating personalized preferences without compromising efficiency and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Personalized Alignment Data Construction</head><p>High-quality data construction is critical for learning PLLMs, primarily involving self-generated data through interactions with the LLM. <ref type="bibr">Wu et al. [2024c]</ref> constructs a dataset for aligning LLMs with individual preferences by initially creating a diverse pool of 3,310 user personas, which are expanded through iterative self-generation and filtering. This method is similar to PLUM [Magister et al., 2024] that both simulate dynamic interactions through multi-turn conversation trees, allowing LLMs to infer and adapt to user preferences. To enable LLMs to adapt to individual user preferences without re-training, <ref type="bibr" target="#b26">Lee et al. [2024]</ref> utilizes diverse system messages as meta-instructions to guide the models' behavior. To support this, the MULTIFACETED COLLECTION dataset is created, comprising 197k system messages that represent a wide range of user values. To facilitate real-time, privacy-preserving personalization on edge devices while addressing data privacy, limited storage, and minimal user disruption, <ref type="bibr" target="#b49">Qin et al. [2024]</ref> introduces a self-supervised method that efficiently selects and synthesizes essential user data, improving model adaptation with minimal user interaction.</p><p>Research efforts are also increasingly concentrating on developing datasets that assess models' comprehension of personalized preferences. Kirk et al. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Personalized Alignment Optimization</head><p>Personalized preference alignment is usually modeled as a multi-objective reinforcement learning (MORL) problem, where personalized preference is determined as the userspecific combination of multi-preference dimensions. Based on this, a typical alignment paradigm involves using a personalized reward derived from multiple reward models to guide during the training phase of policy LLMs, aiming for personalization. MORLHF <ref type="bibr" target="#b74">[Wu et al., 2023]</ref> separately trains reward models for each dimension and retrains the policy language models using proximal policy optimization, guided by a linear combination of these multiple reward models. This approach allows for the reuse of the standard RLHF pipeline. MODPO <ref type="bibr" target="#b92">[Zhou et al., 2023]</ref> introduces a novel RL-free algorithm extending Direct Preference Optimization (DPO) for managing multiple alignment objectives. It integrates linear scalarization directly into the reward modeling process, allowing it to train language models via a simple margin-based cross-entropy loss as implicit collective rewards functions.</p><p>Another strategy for MORL is to consider ad-hoc combinations of multiple trained policy LLMs during the decoding phase to achieve personalization. Personalized Soups <ref type="bibr" target="#b20">[Jang et al., 2023]</ref> and Reward Soups <ref type="bibr" target="#b52">[Rame et al., 2024]</ref> address the challenge of RL from personalized human feedback by first training multiple policy models with distinct preferences independently and then merging their parameters post-hoc during inference. Both methods allow for dynamic weighting of the networks based on user preferences, enhancing model alignment and reducing reward misspecification. Also, the personalized fusion of policy LLMs can be achieved not only through parameter merging but also through model ensembling. MOD <ref type="bibr" target="#b60">[Shi et al., 2024]</ref> outputs the next token from a linear combination of all base models, allowing for precise control over different objectives by combining their predictions without the need for retraining. The method demonstrates significant effectiveness when compared to the parametermerging baseline. PAD <ref type="bibr">[Chen et al., 2024b</ref>] leverages a personalized reward modeling strategy to generate token-level personalized rewards, which are then used to guide the decoding process, allowing dynamic tailoring of the base model's predictions to individual preferences. Figure <ref type="figure" target="#fig_4">5</ref> visualizes above typical approaches of MORL for personalized alignment.</p><p>There are some other emerging personalized alignment studies beyond the "multi-objective" paradigm. PPT <ref type="bibr" target="#b25">[Lau et al., 2024]</ref> unlocks the potential of in-context learning for scalable and efficient personalization by generating two potential responses for each user prompt, asking the user to rank them, and incorporating this feedback into the model's context to dyanmic adapt to individual preferences over time. VPL <ref type="bibr" target="#b46">[Poddar et al., 2024]</ref> utilizes a variational inference framework to capture diverse human preferences through user-specific latent variables. Inferring user-specific latent distributions from a few preference annotations enables more accurate and personalized reward modeling with better data efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussions</head><p>Current mainstream personalized alignment technologies mainly model personalization as multi-objective reinforcement learning problems, where personalized user preferences are taken into account during the training phase of policy LLMs via canonical RLHF, or the decoding phase of policy LLM via parameter merging or model ensembling. Typically, these methods are limited to a small number (e.g., three) of predefined preference dimensions, represented through textual user preference prompts. However, in real-world scenarios, there could be a large number of personalized users, and their preference vectors may not be known, with only their interaction history accessed. Consequently, developing more realistic alignment benchmarks to effectively assess these techniques is a critical area for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Directions</head><p>Despite recent advances in PLLMs, challenges and opportunities remain. This section discusses key limitations and promising future research directions.</p><p>Complex User Data While current approaches effectively handle basic user preferences, processing complex, multisource user data remains a significant challenge. For example, methods that use user relationships in graph-like structures are still limited to retrieval augmentation <ref type="bibr" target="#b11">[Du et al., 2024]</ref>.</p><p>How to effectively leverage this complex user information to fine-tune LLM parameters remains a significant challenge. Most methods focus on text data, while personalized foundation models for multimodal data (e.g., images, videos, audio) remain underexplored, despite their significance for real-world deployment and applications <ref type="bibr">[Wu et al., 2024b;</ref><ref type="bibr" target="#b45">Pi et al., 2024]</ref>. Edge Computing A key challenge in edge computing is efficiently updating models on resource-constrained devices (e.g., phones), where storage and computational resources are limited. For example, fine-tuning offers deeper personalization but is resource-intensive and hard to scale, especially in real-time applications. Balancing resources with personalization needs is important. A potential solution is to build personalized small models <ref type="bibr" target="#b35">[Lu et al., 2024]</ref> for edge devices, using techniques like quantization and distillation. Edge-Cloud Collaboration The deployment of PLLMs in real-world scenarios encounters significant challenges in edgecloud computing environments. Current approaches utilizing collaborative efforts often lack efficient synchronization mechanisms between cloud and edge devices. This highlights the need to explore the balance between local computation and cloud processing for PLLMs <ref type="bibr" target="#b63">[Tian et al., 2024]</ref>. Efficient Adaptation to Model Updates When the base LLM parameters are updated, such as with a new version, efficiently adapting the fine-tuned PEFT parameters for each user becomes a challenge. Given the large volume of user data and limited resources, the cost of retraining can be prohibitive. Future research should focus on efficient strategies for updating user-specific parameters without requiring complete retraining, such as leveraging incremental learning, transfer learning, or more resource-efficient fine-tuning techniques. Lifelong Updating Given the large variety of user behaviors, a key challenge is preventing catastrophic forgetting while ensuring the efficient update of long-term and short-term of the memory. Future research could explore continual learning <ref type="bibr">[Wu et al., 2024d]</ref> and knowledge editing <ref type="bibr">[Wang et al., 2024b]</ref> to facilitate dynamic updates of user-specific information. Trustworthy Ensuring user privacy is crucial, especially when summarized or retrieved data is used to generate personalized responses. Since LLMs cannot be deployed locally due to resource limits, there is a risk of privacy leakage. Future research could focus on privacy-preserving methods like federated learning, secure computation, and differential privacy to protect user data <ref type="bibr" target="#b80">[Yao et al., 2024;</ref><ref type="bibr">Liu et al., 2024a]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This survey offers a thorough overview of Personalized Large Language Models (PLLMs), emphasizing personalized responses tailored to individual user data. We introduce a structured taxonomy that categorizes existing approaches into three key technical perspectives: Personalized Prompting (Input Level), Personalized Adaptation (Model Level), and Personalized Alignment (Objective Level), with further subdivisions within each. We also discuss current limitations and propose several promising directions for future research. Our work provides valuable insights and a framework to drive the advancement of PLLMs development.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of PLLM techniques for generating personalized responses through three levels: prompting, adaptation, and alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The illustration of personalized prompting approaches: a) Profile-Augmented, b) Retrieval-Augmented, c) Soft-Fused.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The illustration of personalized adaptation approaches: a) One PEFT for all users, b) One PEFT per user.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The illustration of personalized alignment method under the multi-objective reinforcement learning paradigm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>[2024] introduces PRISM Alignment Dataset that maps the sociodemographics and preferences of 1,500 participants from 75 countries to their feedback in live interactions with 21 LLMs, focusing on subjective and multicultural perspectives on controversial topics. Per-sonalLLM [Zollo et al., 2024] introduces a novel personalized testdb, which curates open-ended prompts and multiple high-quality responses to simulate diverse latent preferences among users. It generates simulated user bases with varied preferences from pre-trained reward models, addressing the challenge of data sparsity in personalization.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Personalized graph-based retrieval for large language models</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><forename type="middle">J</forename><surname>Dimacali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ojasmitha</forename><surname>Pedirappagari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namyong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Kanakaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanieh</forename><surname>Deilamsalehy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen K</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.02157</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A survey on mixture of experts</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juyong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.06204</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">When large language models meet personalization: Perspectives of challenges and opportunities</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenwang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhao</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingmei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pad: Personalized alignment of llms at decoding-time</title>
		<author>
			<persName><forename type="first">Ruizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.04070</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Scaling instructionfinetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Peft-u: Parameter-efficient fine-tuning for user personalization</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhao</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.18078</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement</title>
		<author>
			<persName><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">User embedding model for personalized language prompting</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Doddapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Sayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambarish</forename><surname>Jash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sukhdeep</forename><surname>Sodhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Kuzmin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.04858</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Perltqa: A personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baojun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zezhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16288</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey on rag meeting llms: Towards retrieval-augmented large language models</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangbo</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for large language models: A survey</title>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangxiang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haofen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.10997</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</title>
		<author>
			<persName><forename type="first">Dejian</forename><surname>Daya Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12948</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cos: Enhancing personalization and mitigating bias with context steering</title>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Zhi-Yang He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashrika</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariah</forename><forename type="middle">L</forename><surname>Schrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><surname>Dragan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.01768</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Persoma: Personalized soft prompt adapter architecture for personalized language prompting</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Sayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambarish</forename><surname>Jash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhdeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Sodhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanli</forename><surname>Doddapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><surname>Kuzmin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SeRTS: Self-rewarding tree search for biomedical retrieval-augmented generation</title>
		<author>
			<persName><forename type="first">Minda</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP Findings</title>
		<meeting>of EMNLP Findings</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09118</idno>
		<title level="m">Unsupervised dense information retrieval with contrastive learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Personalized soups: Personalized large language model alignment via post-hoc parameter merging</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungone</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.11564</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervรฉ</forename><surname>Jรฉgou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models</title>
		<author>
			<persName><forename type="first">Rose</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Whitefield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rรถttger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Bean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Margatina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Ciro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Mosquera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.16019</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Customizing language models with instance-wise lora for sequential recommendation</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leheng</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Longlamp: A benchmark for personalized long-form text generation</title>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushrita</forename><surname>Yerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Salemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanieh</forename><surname>Deilamsalehy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Agarwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.11016</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Personalized adaptation via in-context preference learning</title>
		<author>
			<persName><forename type="first">Allison</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Balazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keertana</forename><surname>Chidambaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasilis</forename><surname>Syrgkanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul G</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.14001</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Aligning to thousands of preferences via system message generalization</title>
		<author>
			<persName><forename type="first">Seongyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungone</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17977</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Teach llms to personalize-an approach inspired by writing education</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amba</forename><surname>Spurthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Hombaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Bendersky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.07968</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Changhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rushi</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Matryoshka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.20749</idno>
		<title level="m">Learning to drive black-box llms with llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to rewrite prompts for personalized text generation</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Web Conference</title>
		<meeting>of Web Conference</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.05925</idno>
		<title level="m">Hello again! llm-powered personalized agent for long-term dialogue</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fewshot parameter-efficient fine-tuning is better and cheaper than in-context learning</title>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Muqeeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Mohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tenghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recap: Retrieval-enhanced contextaware prefix encoder for personalized dialogue response generation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hyundong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjorie</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.07206</idno>
		<imprint>
			<date type="published" when="2023-05">May. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Client-specific hyperbolic federated learning</title>
		<author>
			<persName><forename type="first">Jiahong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FedKDD@KDD</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ONCE: boosting content-based recommendation with both open-and closed-source large language models</title>
		<author>
			<persName><forename type="first">Qijiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WSDM</title>
		<meeting>of WSDM</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Zhenyan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengwei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.15790</idno>
		<title level="m">Small language models: Survey, measurements, and insights</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Memory-assisted prompt editing to improve gpt-3 after deployment</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Lucie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Magister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Metcalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maartje</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Ter Hoeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.13405</idno>
		<title level="m">On the way to llm personalization: Learning to remember user conversations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fine-tuning language models with just forward passes</title>
		<author>
			<persName><forename type="first">Sadhika</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eshaan</forename><surname>Nichani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Fatemehsadat</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnavi</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Shokouhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Dimitriadis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00135</idno>
		<title level="m">Useridentifier: implicit user representations for simple and effective personalized sentiment analysis</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Sheshera</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengting</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Baghaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">Barajas</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName><surname>Pearl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.09180</idno>
		<title level="m">Personalizing large language model writing assistants with generationcalibrated retrievers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devora</forename><surname>Berlowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushant</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><forename type="middle">O</forename><surname>'banion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.13598</idno>
		<title level="m">User-llm: Efficient llm contextualization with user embeddings</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pocketllm: Enabling ondevice fine-tuning for personalized llms</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.01031</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Llm: Harnessing large language models for personalized review generation</title>
		<author>
			<persName><forename type="first">Qiyao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minglai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.07487</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.07113</idno>
		<title level="m">Personalized visual instruction tuning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Personalizing reinforcement learning from human feedback with variational preference learning</title>
		<author>
			<persName><forename type="first">Sriyash</forename><surname>Poddar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.10075</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Fdlora: Personalized federated learning of large language model via dual lora tuning</title>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzhi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depei</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.07925</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Hongjin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peitian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><surname>Memorag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.05591</idno>
		<title level="m">Moving towards next-gen rag via memory-inspired knowledge discovery</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Enabling on-device large language model personalization with selfsupervised data selection and synthesis</title>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenge</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DAC</title>
		<meeting>of DAC</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijing</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17519</idno>
		<title level="m">Entropy-based decoding for retrieval-augmented large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of NuerIPS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Rame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Couairon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Gaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Shukor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Preference distillation for personalized generative recommendation</title>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aldo</forename><surname>Lipani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.05033</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Integrating summarization and retrieval for enhanced personalization via large language models</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellen</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arshdeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynab</forename><surname>Raeesy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><forename type="middle">Zia</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Sethy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.20081</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Okapi at trec-3</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName><surname>Gatford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Nist Special Publication Sp</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Comparing retrievalaugmentation and parameter-efficient fine-tuning for privacy-preserving personalization of large language models</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Salemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.09510</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Lamp: When large language models meet personalization</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Salemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheshera</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11406</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Optimization methods for personalizing large language models through retrieval augmentation</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Salemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Kallumadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Beyond retrieval: Generating narratives in conversational recommender systems</title>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Sayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavendra</forename><surname>Vasudeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Vasilevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambarish</forename><surname>Jash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sukhdeep</forename><surname>Sodhi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.16780</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Ruizhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.18853</idno>
		<title level="m">Decodingtime language model alignment with multiple objectives</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Personalized pieces: Efficient personalized large language models through collaborative efforts</title>
		<author>
			<persName><forename type="first">Zhaoxuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10471</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Democratizing large language models via personalized parameter-efficient fine-tuning</title>
		<author>
			<persName><forename type="first">Zhaoxuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">An edge-cloud collaboration framework for generative ai service provision with synergetic big cloud model and small edge models</title>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><forename type="middle">Qs</forename><surname>Quek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Kit</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.01666</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothรฉe</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Roziรจre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Yu-Min</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng-Yun</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Ching</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Yin</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01171</idno>
		<title level="m">Two tales of persona in llms: A survey of role-playing and personalization</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Exploring safety-utility trade-offs in personalized language models</title>
		<author>
			<persName><forename type="first">Anvesh</forename><surname>Rao Vijjini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somnath</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11107</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Personalized collaborative fine-tuning for on-device large language models</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.09753</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Hongru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minda</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai-Chung</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08840</idno>
		<title level="m">Large language models as source planner for personalized knowledge-grounded dialogue</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Cue-cot: Chain-of-thought prompting for responding to in-depth dialogue questions with llms</title>
		<author>
			<persName><forename type="first">Hongru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zezhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP Findings</title>
		<meeting>of EMNLP Findings</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Tpe: Towards better compositional reasoning over cognitive tools via multi-persona collaboration</title>
		<author>
			<persName><forename type="first">Hongru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minda</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPCC</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="281" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Knowledge editing for large language models: A survey</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Stanisลaw</forename><surname>Woลบniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartลomiej</forename><surname>Koptyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkadiusz</forename><surname>Janz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemysลaw</forename><surname>Kazienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kocoล</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09269</idno>
		<title level="m">Personalized large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Personalized response generation via generative split memory network</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Fine-grained human feedback gives better rewards for language model training</title>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Understanding the role of user profile in the personalization of large language models</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><forename type="middle">A</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Ramineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17803</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Junda</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjia</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhehao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrnoosh</forename><surname>Mirtaheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.02142</idno>
		<title level="m">Personalized multimodal large language models: A survey</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Aligning llms with individual preferences via interaction</title>
		<author>
			<persName><forename type="first">Shujin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">May</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeonghwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.03642</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Tongtong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuy-Trang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01364</idno>
		<title level="m">Continual learning for large language models: A survey</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Low-rank adaptation for foundation models</title>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.00365</idno>
	</analytic>
	<monogr>
		<title level="m">A comprehensive review</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Federated large language models: Current progress and future directions</title>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junda</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.15723</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Long-term memory for large language models through topic-based vector database</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanqi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IALP</title>
		<meeting>of IALP</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Llm-based medical assistant personalization with shortand long-term memory coordination</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fubang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Personalized llm response generation with parameterized memory injection</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhi</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.03565</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Personalized lora for human-centered text understanding</title>
		<author>
			<persName><forename type="first">You</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chih</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohe</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.13501</idno>
		<title level="m">A survey on the memory mechanism of large language model based agents</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Memsim: A bayesian simulator for evaluating memory of llm-based personal assistants</title>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeren</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.20163</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">Zhehao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branislav</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.00027</idno>
		<title level="m">Personalization of large language models: A survey</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">A survey of large language models</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Useradapter: Few-shot user learning in sentiment analysis</title>
		<author>
			<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL Findings</title>
		<meeting>of ACL Findings</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">Hanxun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08128</idno>
		<title level="m">Less is more: Learning to refine dialogue history for personalized dialogue generation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Beyond onepreference-for-all: Multi-objective direct preference optimization</title>
		<author>
			<persName><forename type="first">Zhanhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03708</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Autopeft: Automatic configuration search for parameterefficient fine-tuning</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingchen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuliฤ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Lifelong personalized low-rank adaptation of large language models for recommendation</title>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianghao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.03533</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rushi</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.02888</idno>
		<title level="m">Hydra: Model factorization framework for black-box llm personalization</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Thomas P Zollo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tung</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naimeng</forename><surname>Siah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><surname>Personalllm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.20296</idno>
		<title level="m">Tailoring llms to individual preferences</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
