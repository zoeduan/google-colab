<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AI for Biomedicine in the Era of Large Language Models</title>
				<funder>
					<orgName type="full">Virginia Tech Innovation Campus</orgName>
				</funder>
				<funder>
					<orgName type="full">Commonwealth Cyber Initiative</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-03-23">23 Mar 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenyu</forename><surname>Bi</surname></persName>
							<email>zhenyub@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sajib</forename><forename type="middle">Acharjee</forename><surname>Dip</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Hajialigol</surname></persName>
							<email>danielhajialigol@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sindhura</forename><surname>Kommu</surname></persName>
							<email>sindhura@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanwen</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Lu</surname></persName>
							<email>menglu@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
							<email>xuanw@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AI for Biomedicine in the Era of Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-03-23">23 Mar 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">F141A9CDF50E9C7D0647767C4F2B3C01</idno>
					<idno type="arXiv">arXiv:2403.15673v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The capabilities of AI for biomedicine span a wide spectrum, from the atomic level, where it solves partial differential equations for quantum systems, to the molecular level, predicting chemical or protein structures, and further extending to societal predictions like infectious disease outbreaks. Recent advancements in large language models, exemplified by models like ChatGPT, have showcased significant prowess in natural language tasks, such as translating languages, constructing chatbots, and answering questions. When we consider biomedical data, we observe a resemblance to natural language in terms of sequences -biomedical literature and health records presented as text, biological sequences or sequencing data arranged in sequences, or sensor data like brain signals as time series. The question arises: Can we harness the potential of recent large language models to drive biomedical knowledge discoveries? In this tutorial, we will explore the application of large language models to three crucial categories of biomedical data: 1) textual data, 2) biological sequences, and 3) brain signals. Furthermore, we will delve into large language models' challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to multi-modal data representation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The impressive capabilities of AI for biomedicine span a wide spectrum, from the atomic level, where it attempts to solve partial differential equations for quantum systems, to the molecular level, where it accurately predicts the structures of chemicals and proteins, and extends even further, encompassing societal predictions like forecasting infectious disease outbreaks <ref type="bibr" target="#b107">[108]</ref>. Amidst this landscape of possibilities, recent advancements in large language models (LLMs), notably exemplified by models like ChatGPT 1 , have risen to the forefront, demonstrating significant proficiency in tasks tied to natural language. These tasks include language translation, constructing chatbots, and answering questions <ref type="bibr" target="#b103">[104]</ref>.</p><p>Interestingly, when we turn our attention to biomedical data, we observe a striking resemblance to natural language in terms of sequences. Biomedical literature and health records are laid out as textual narratives, biological sequences or sequencing data takes the form of molecular or expression sequences, and sensor data like brain signals is inherently sequential time series <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b90">91]</ref>. This observation prompts a compelling question: Can we leverage the advanced LLMs to drive biomedical knowledge discoveries?</p><p>In this survey paper, we embark on a journey to explore precisely this intersection-the fusion of cutting-edge large language models with biomedical inquiry. Our exploration zooms in on three pivotal categories of biomedical data: 1) textual data, 2) biological sequences, and 3) brain signals. By drawing inspiration from the transformative capabilities of LLMs, we seek to unravel novel understanding and innovation within each domain.</p><p>As we move forward, we further discuss the intricate challenges accompanying AI infusion into biomedical research. The foundation of trustworthiness stands tall-how do we ensure the reliability of AI-enhanced biomedical insights? The concept of personalization emerges as a critical consideration, urging us to tailor LLMs to the specific contours of biomedical investigation. Furthermore, the multi-modal nature of biomedical data motivates us to handle data representations that span across various modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LLMS ON DIVERSE BIOMEDICAL DATA</head><p>We comprehensively survey the topics of LLMs for biomedicine based on three pivotal categories of biomedical data: 1) textual data, 2) biological sequences, and 3) brain signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LLMs on Biomedical Textual Data</head><p>First, we introduce LLMs in the realm of biomedical textual data, which encompasses diverse domains like biomedical literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b105">106]</ref> and electronic health records <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b78">79]</ref>. This form of biomedical textual data closely mirrors the fundamental structure of large language models. It finds extensive utility across biomedicine and healthcare, facilitating tasks such as extracting valuable information and responding to queries. The applicability spans a multitude of areas, underpinning biomedical and healthcare information extraction <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b112">113]</ref> and question-answering <ref type="bibr" target="#b45">[46]</ref>.</p><p>SciBERT. SciBERT <ref type="bibr" target="#b11">[12]</ref> is a BERT-based model <ref type="bibr" target="#b23">[24]</ref> pre-trained on scientific publications. Specifically, SciBERT is pre-trained on 1.14 million scientific publications from Semantic Scholar <ref type="bibr" target="#b4">[5]</ref> via random sampling. SciBERT's vocabulary construction utilizes Word-Piece <ref type="bibr" target="#b98">[99]</ref>, the same unsupervised tokenization technique that BERT employs, while also maintaining the same vocabulary size. Only 42% of the vocabulary across SciBERT and BERT contain the same vocabulary, indicating the difference in the frequency of most common words in the general domain (BERT) and in the scientific domain (SciBERT). SciBERT surpasses BERT in all four evaluated downstream tasks, which include named entity recognition, PICO (Population, Intervention, Comparison, Outcome) extraction, relation extraction, and dependency parsing. Additionally, SciBERT was state-of-the-art in all four tasks excluding dependency parsing at the time of publication.</p><p>ClinicalBERT. ClinicalBERT <ref type="bibr" target="#b3">[4]</ref> is the first framework to pretrain clinical texts using a BERT-based architecture. Approximately 2 million clinical notes, originating from the MIMIC III v1.4 database <ref type="bibr" target="#b42">[43]</ref>, are used for pre-training. The discharge summaries were used for pre-training to generate Discharge BERT, while the clinical note types (nursing, radiology, etc.) were used to train Clinical BERT. Experiments were conducted on natural language inference and named entity recognition tasks. ClinicalBERT demonstrated state-of-the-art results on natural language inference and two of the four named entity recognition datasets at the time of publication. A concern of using ClinicalBERT is the lack of robustness for patient de-identification, as the framework was not able to obtain improvements on the de-identification task.</p><p>BioBERT. BioBERT <ref type="bibr" target="#b48">[49]</ref> is a BERT-based architecture that is pre-trained on biomedical data. Precisely, BioBERT pre-trains on Pubmed abstracts (4.5 billion words) and PMC publications (13.5 billion words). At the time of publication, experimental results indicate that BioBERT achieves state-of-the-art performance on the majority of biomedical named entity recognition, biomedical question answering, and biomedical relation extraction datasets.</p><p>BioMegatron. BioMegatron <ref type="bibr" target="#b75">[76]</ref> is biomedical LLM based on Megatron-LM <ref type="bibr" target="#b76">[77]</ref>, where model parallelism is applied to both selfattention and multi-layer perceptron blocks for each transformer layer. While Megatron-LM is pre-trained on general domain text, BioMegatron is pre-trained on approximately 6.1 billion words, 4.5 billion of which are original from a PubMed abstract dataset, and the remaining 1.6 billion derived from all PMC scientific publications. BioMegatron's size ranges from 345 million to 1.2 billion and also has the flexibility to use vocabularies of either 30,000 or 50,000 subtokens. Their experiments indicate that BioMegatron performs state-of-the-art on all tested named entity recognition and relation extraction benchmarks, mainly with the larger vocabulary.</p><p>SciFive. SciFive <ref type="bibr" target="#b68">[69]</ref> uses a T5 <ref type="bibr" target="#b70">[71]</ref>, a generative text model that is pre-trained millions of PubMed abstraction and PMC scientific publications. Unlike BERT-inspired architectures, SciFive utilizes a Sentence Piece <ref type="bibr" target="#b46">[47]</ref> algorithm for its vocabulary. State-of-the-art results are achieved against baselines such as BioBERT, generaldomain T5, and general-domain BERT on biomedical question answering, relation extraction, named entity recognition, and natural language inference.</p><p>PubMedBERT. A common assumption within the biomedical LLM literature is that the transfer of general domain knowledge will help domain-specific, in particular biomedical, downstream tasks; however, PubMedBERT <ref type="bibr" target="#b32">[33]</ref> shows this transfer of knowledge is detrimental to downstream performance: Pre-training using biomedical corpora results in better performance than continual pre-training. This performance improvement is in large part due to PubMedBERT having an in-domain vocabulary, whereas mixeddomain pre-training is subjected to model out of vocabulary words using fragmented subwords. PubMedBERT uses BERT as its backbone and is pre-trained on PubMed articles. Across all 14 datasets stemming from relation extraction, named entity recognition, document classification, question answering, and PICO extraction for evaluation, PubMedBERT achieves state-of-the-art on 12 datasets.</p><p>BioLinkBERT. Contrary to the approach of many biomedical LLMs, which typically pre-train using only single documents, Bi-oLinkBERT <ref type="bibr" target="#b105">[106]</ref>  Galactica. Galactica <ref type="bibr" target="#b87">[88]</ref> employs a Transformer-based architecture but generates a vocabulary of size 50,000 using byte-pair encoding as well as learnable positional embeddings. The size of Galactica concerning its parameters can range from 125 million to 120 billion. Galactica has multiple tokenization techniques dependent on the modality that's presented to it. This mainly comes in the form of special tokens wrapping around a modality's input. For example, citation information is wrapped around [START_REF] and [END_REF] tokens; special tokens [START_AMINO] and [END_AMINO] are wrapped around amino acid sequences. Across all types of modalities, Galactica was pre-trained on 106 billion tokens from scientific sources such as scholarly papers, textbooks, and encyclopedias. Galactica either achieves state-of-the-art performance or is within 0.5% on biomedical question-answering datasets that are evaluated.</p><p>BioGPT. BioGPT <ref type="bibr" target="#b57">[58]</ref> is one of the first biomedical GPT-inspired biomedical frameworks. BioGPT is pre-trained on 15 million PubMed articles and employs byte-pair as their vocabulary-building algorithm, resulting in a vocabulary of approximately 42,000 in size. BioGPT is trained with the GPT-2 <ref type="bibr" target="#b69">[70]</ref> backbone due to the sheer size of GPT-3 <ref type="bibr" target="#b16">[17]</ref> (15 billion parameters). As a result, BioGPT has 347 million parameters. Experiments on three biomedical relation extraction datasets and a biomedical question-answer dataset indicate state-of-the-art performance at the time of publication. DoT5. DoT5 <ref type="bibr" target="#b55">[56]</ref> proposed a zero-shot domain transfer framework that transfer domain knowledge to the biomedical domain. DoT5 continuously pre-trains using a multi-task setup: combining general-domain task knowledge and biomedical knowledge. Specifically, DoT5's pre-training objective is a linear combination of solving domain-agnostic tasks (natural language inference, summarization, etc.) and masked biomedical language modeling. DoT5 uses T5 <ref type="bibr" target="#b70">[71]</ref> as its backbone and utilizes PubMed abstracts for its in-domain dataset. Results indicate that DoT5 is competitive with state-of-the-art biomedical natural language inference and biomedical summarization tasks.</p><p>GatorTronGPT. Considerable stigma associated with applying LLMs like ChatGPT to biomedicine stems from their lack of specialization in the biomedical field. GatorTronGPT <ref type="bibr" target="#b67">[68]</ref> was introduced to alleviate this stigma, as it is pre-trained on billions of clinical texts. Specifically, GatorTronGPT was pre-trained on 277 billion words, 82 billion of which originated from de-identified clinical text <ref type="bibr" target="#b104">[105]</ref> and the remaining 195 billion were obtained from Pile <ref type="bibr" target="#b28">[29]</ref>, a diverse dataset specifically for language modeling. GatorTronGPT was evaluated on downstream tasks such as biomedical question answering and biomedical relation extraction. To evaluate the quality of the generated texts, billions of synthetic text were generated from GatorTronGPT that were used for fine-tuning a BERT-based model, GatorTronS, on other downstream tasks such as clinical concept extraction, biomedical relation extraction, semantic textual similarity, natural language inference, and question answering. Additionally, it was shown that less than half of 30 synthetic clinical paragraphs generated by GatorTronGPT were identified to be synthetically generated by two physician evaluators at the University of Florida. While GatorTronGPT takes steps to de-identify its clinical data, the authors suggest future studies to validate the robustness of re-identifying patients further.</p><p>Med-PaLM 2. Med-PaLM 2 <ref type="bibr" target="#b79">[80]</ref> is an extension of Med-PaLM <ref type="bibr" target="#b20">[21]</ref> with several improvements. First, Med-PaLM 2 utilizes PaLM 2 <ref type="bibr" target="#b5">[6]</ref> as the backbone LLM, unlike PaLM which was the backbone of Med-PaLM. Second, to improve reasoning capabilities, Med-PaLM 2 introduces ensemble refinement, a prompting strategy that iteratively feeds in prior generations, consisting of an answer and explanation, as well as the given question and prompt to output a refined answer and generation. Performance on the MedQA medical dataset is state-of-the-art by the time of publication, beating the previous state-of-the-art (Med-PaLM) by over 19%. Additionally, Med-PaLM 2 performed state-of-the-art or had comparable performance to other clinical datasets, such as PubMedQA, MMLU, and MedMCQA.</p><p>Applications: There are multiple downstream applications for Biomedical LLMs. We roughly divide them into two categories: clinical applications and research applications.</p><p>NLP-related Clinic applications. The most widely-researched directions in Biomedical NLP related to clinical are Clinical Treatment Planning, Clinical Reports Generation, and Multi-Agents Collaboration. Clinical Treatment Planning <ref type="bibr" target="#b71">[72]</ref> involves diagnosing health issues and determining the suitable treatment course, requiring healthcare professionals to assess objective medical test data and subjective patient symptoms. Incorporating Large Language Models (LLMs) could enhance diagnostic accuracy and broaden access to specialized medical knowledge <ref type="bibr" target="#b89">[90]</ref>. In parallel, Clinical Reports Generation <ref type="bibr" target="#b97">[98]</ref>, crucial for documenting patient medical histories, benefits significantly from LLMs <ref type="bibr" target="#b92">[93]</ref>. These models facilitate the creation of reports by summarizing content, referencing relevant information, and generating follow-up diagnostic comments and medication recommendations based on clinical diagnoses <ref type="bibr" target="#b96">[97]</ref>. Furthermore, Multi-Agents Collaboration <ref type="bibr" target="#b99">[100]</ref>, by simulating various diagnostic scenarios, enables a synergy among agents to interpret data and perform complex clinical tasks. This collaboration extends to integrating Electronic Health Records (EHR) <ref type="bibr" target="#b80">[81]</ref> and medication history, thus offering real-time management and advisories. Such an approach augments diagnostic accuracy, optimizes the duration of medical procedures, and minimizes patient recovery times <ref type="bibr" target="#b86">[87]</ref>, offering a comprehensive and dynamic support system for healthcare providers.</p><p>NLP-related Research applications. The most widely-researched directions in Biomedical NLP are information extraction (IE) related tasks, such as Named Entity Recognition (NER) and Relation Extraction (RE), where the model is asked to extract entities (such as genes, drug names, and diseases) or relations of the entities, from text data in biomedical literature. Another prominent direction in the field is Biomedical Question Answering, where the model is asked to answer biomedical-related questions, given heavily professional background text. Since the aforementioned tasks are very domainspecific, fine-tuning or pre-training domain-specific LLMs are most effective when dealing with them <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b114">115]</ref>. The biomedical domain-specific datasets are often created based on PubMed articles <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57]</ref>, professional medical examinations <ref type="bibr" target="#b39">[40]</ref>, or real-life patient health information <ref type="bibr" target="#b41">[42]</ref>, with extensive expert annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LLMs on Biological Sequences</head><p>Next, we extend the application of LLMs to the intricate realm of biological sequence data, where a rich landscape of possibilities emerges. Within this domain, we shift our focus to four major categories of biological sequences: 1) DNA sequences, 2) RNA sequences, 3) protein sequences, and 4) multi-omics sequencing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">DNA sequences:</head><p>The central dogma of molecular biology outlines the flow of genetic information within living organisms, with DNA serving as the source of this crucial information. DNA variants play a significant role in determining heritable traits, influencing aspects related to health and disease. Various innovative works in genomics, such as Enformer <ref type="bibr" target="#b9">[10]</ref>, Nucleotide Transformer <ref type="bibr" target="#b22">[23]</ref>, GenSLMs <ref type="bibr" target="#b115">[116]</ref>, DNABERT <ref type="bibr" target="#b37">[38]</ref>, GENA-LM <ref type="bibr" target="#b27">[28]</ref>, and Hye-naDNA <ref type="bibr" target="#b62">[63]</ref>, are proposed to decode the genetic blueprint of life. Enformer. Enformer <ref type="bibr" target="#b9">[10]</ref> is designed to predict gene expression and chromatin states purely from DNA sequences by integrating long-range interactions. It utilizes transformer modules with selfattention to achieve a larger receptive field, enabling the detection of sequence elements up to 100 kb (kilobases) away, compared with 20 kb for Basenji2 <ref type="bibr" target="#b44">[45]</ref> in the genome. By increasing this information flow between distal elements and prioritizing relevant data across the DNA sequence, the model can better capture biological phenomena, such as enhancers that regulate promoters, despite a large DNA-sequence distance between the two. Enformer demonstrates superior performance over the previous model Basenji2 in gene expression prediction across genes and CAGE experiments for protein-coding genes. It excels in predicting tissue and cell-typespecific gene expression, enhancing accuracy in enhancer-promoter prediction and non-coding variant effect prediction.</p><p>GenSLMs. GenSLMs <ref type="bibr" target="#b115">[116]</ref>, or Genome-scale Language Models, represent a groundbreaking approach to understanding the evolutionary dynamics of SARS-CoV-2. These models utilize large language models (LLMs) adapted for genomic data to learn the evolutionary landscape of SARS-CoV-2 genomes. By pre-training on a vast number of prokaryotic gene sequences and fine-tuning on SARS-CoV-2-specific data, GenSLMs can accurately and rapidly identify variants of concern. They are among the first whole-genome scale foundation models capable of generalizing to various prediction tasks. The models have been demonstrated to scale efficiently on GPU-based supercomputers and AI-hardware accelerators, achieving remarkable computational performance. This innovative approach aims to enhance public health intervention strategies and inform vaccine development for emerging variants by providing insights into the evolutionary dynamics of SARS-CoV-2.</p><p>DNABERT. DNABERT <ref type="bibr" target="#b37">[38]</ref> is a pre-trained BERT model specifically designed for genomic DNA sequences. It addresses the complexity of the gene regulatory code by capturing global and transferable understandings of DNA sequences based on nucleotide contexts. DNABERT excels in data-scarce scenarios, achieving stateof-the-art performance in predicting splice sites and transcription factor binding sites. It allows for direct visualization of nucleotidelevel importance and semantic relationships within sequences, enhancing interpretability and identification of genetic variants.</p><p>GENA-LM. GENA-LM <ref type="bibr" target="#b27">[28]</ref> represents a significant advancement in the field of computational genomics, introducing a suite of opensource, transformer-based foundational DNA language models specifically designed to handle long DNA sequences. These models are capable of processing input lengths up to 36,000 base pairs, a substantial increase over previous architectures, which struggled with the extensive contextual information spread across such long sequences. Key features of GENA-LM include its use of Byte Pair Encoding (BPE) tokenization, which differs from the k-mer approach used in earlier models like DNABERT. This allows for a more efficient representation of DNA sequences, facilitating the processing of longer inputs. The models have been pre-trained on the latest T2T human genome assembly <ref type="bibr" target="#b65">[66]</ref>, incorporating a wide range of genomic data to enhance their predictive capabilities.</p><p>HyenaDNA. HyenaDNA <ref type="bibr" target="#b62">[63]</ref> is a groundbreaking genomic foundation model designed for long-range sequence modeling at single nucleotide resolution. It addresses the limitations of previous models by enabling the processing of ultralong sequences directly at the single nucleotide level without the need for tokenizers. HyenaDNA is significantly smaller than previous genomic models, yet it demonstrates exceptional performance, surpassing state-of-the-art benchmarks on various datasets with fewer parameters and pre-training data. This model scales sub-quadratically in sequence length, allowing for efficient training and processing of long DNA sequences. Additionally, HyenaDNA shows promise in species classification tasks and offers exciting prospects for synthetic regulatory element design, gene identification, and protein complex modeling. Despite its early-stage limitations related to training on a single human genome and focusing solely on DNA, HyenaDNA's potential for personalized genomic analysis and innovative applications in drug discovery is immense.</p><p>Nucleotide transformer. The Nucleotide Transformer model <ref type="bibr" target="#b9">[10]</ref> is a robust foundational model pre-trained on raw DNA sequences from diverse human genomes and various species. This model tokenizes sequences into six-character words (k-mers of length 6) and utilizes the BERT methodology for training. It is then applied to 18 downstream tasks covering various predictions like promoter prediction, splice site donor and acceptor prediction, histone modifications, and more. Predictions are made through probing or light fine-tuning, providing accurate molecular phenotype predictions even in low-data settings by generating context-specific representations of nucleotide sequences. The Nucleotide Transformer outperforms specialized methods on multiple tasks, showcasing a focus on key genomic elements such as gene expression regulators. Models trained on genomes from different species show superior performance compared to those trained solely on human sequences in various human prediction tasks, indicating that diverse species training enhances generalization in human-based predictions.</p><p>Applications: Genomic foundation models play a crucial role in various tasks, including predicting gene expression, deciphering DNA sequences for biological functions, identifying genomic region structures, and predicting genetic elements such as promoter regions, enhancers, and transcription factor binding sites. These models are instrumental in understanding transcriptional regulation and its connection to non-coding genetic variants linked to human diseases and traits. For example, Enformer can directly predict enhancer-promoter interactions from DNA sequences <ref type="bibr" target="#b9">[10]</ref>, competing with methods that rely on experimental data. Additionally, GenSLMs, trained on over 110 million prokaryotic gene sequences and fine-tuned for SARS-CoV-2, can accurately and rapidly identify variants of concern <ref type="bibr" target="#b115">[116]</ref>, which also represents the generalization ability of these models to different prediction tasks. DNABERT, fine-tuned for specific tasks like predicting promoters, transcription factor binding sites, and splice sites, has been benchmarked against state-of-the-art tools in the field <ref type="bibr" target="#b37">[38]</ref>. These genomic models that can accurately identify promoter regions and splice sites, aid in understanding gene regulation and RNA transcript processing. They can also predict epigenetic modifications <ref type="bibr" target="#b25">[26]</ref> and the effects of DNA variations on genes and protein function <ref type="bibr" target="#b12">[13]</ref>, helping identify genetic risk factors. Moreover, DLM models contribute to understanding how non-coding DNA influences gene expression <ref type="bibr" target="#b113">[114]</ref> and disease <ref type="bibr" target="#b47">[48]</ref> in different cell types, which is crucial for human genetics and disease studies.</p><p>2.2.2 RNA sequences: Ribonucleic acid (RNA) is another critical component of molecular biology, playing various essential roles in fundamental biological processes. In recent years, RNA has emerged as an intriguing target for drug development <ref type="bibr" target="#b30">[31]</ref>, underscoring the importance of enhancing our understanding of its structures and functions. We embrace the innovative contributions highlighted in foundation models such as RNABERT [ <ref type="bibr" target="#b1">[2]</ref>], RNA-FM <ref type="bibr" target="#b18">[19]</ref>, and RNA-MSM <ref type="bibr" target="#b108">[109]</ref>, which are pivotal in elucidating the intricate mechanisms governing biological processes mediated by RNA. These advancements empower us to decipher the complex orchestration of biological processes by RNA molecules.</p><p>RNABERT. RNABERT <ref type="bibr" target="#b1">[2]</ref> proposes an effective method for embedding RNA bases by applying the pre-training approach of BERT to non-coding RNA (ncRNA). The research aims to develop a robust RNA base embedding technique for tasks like RNA structural alignment and clustering using deep representation learning. By leveraging pre-training algorithms, the researchers aim to create semantically rich representations of RNA bases to improve the accuracy of structural alignment and clustering tasks. This method of informative base embedding incorporates contextual information and secondary structure details of RNA sequences, resulting in superior accuracies compared to existing methods such as GraphClust <ref type="bibr" target="#b35">[36]</ref>, EnsembleClust <ref type="bibr" target="#b73">[74]</ref>, and CNNclust <ref type="bibr" target="#b7">[8]</ref>. The approach combines informative base embedding with a simple Needleman-Wunsch alignment algorithm to calculate structural alignments with improved time complexity, making it a significant advancement in RNA sequence analysis. <ref type="bibr" target="#b18">[19]</ref> is a novel interpretable computational model proposed to predict RNA structure and function accurately using unannotated data. It leverages self-supervised learning on 23 million non-coding RNA sequences to infer sequential and evolutionary information without labels. It has shown effectiveness in predicting secondary/3D structures, SARS-CoV-2 genome structure, protein-RNA binding preferences, and gene expression regulation. Despite being trained on unlabelled data, RNA-FM significantly improves RNA structural and functional modeling results, making it a foundational model for the field. <ref type="bibr" target="#b108">[109]</ref> is an RNA language model that leverages homologous RNA sequences to interpret and predict RNA structures more effectively than previous methods. It is distinguished by its use of unsupervised learning on multiple sequence alignments (MSAs) derived from RNAcmap, a tool that outperforms traditional methods like Rfam in identifying homologous sequences due to its comprehensive and automatic pipeline. This approach allows RNA-MSM to capture evolutionary information and structural nuances of RNA sequences, which are less conserved and thus more challenging to analyze compared to proteins. The model generates two-dimensional attention maps and one-dimensional embeddings that contain rich structural information. These outputs can be directly correlated with RNA structural features, such as 2D base pairing probabilities and 1D solvent accessibilities, with high accuracy. The study demonstrates that RNA-MSM significantly outperforms existing techniques, including SPOT-RNA2 and RNAs-nap2, in predicting these structural features after fine-tuning. This research underscores the potential of RNA-MSM to revolutionize RNA structure prediction and functional analysis by incorporating evolutionary and structural insights from homologous sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNA-FM. RNA-FM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNA-MSM. RNA-MSM</head><p>Applications: RNA language models have a wide range of applications in RNA research and biotechnology. These models utilize evolutionary information from homologous sequences to accurately interpret RNA sequences, making them valuable in tasks such as RNA structure <ref type="bibr" target="#b101">[102]</ref>, function <ref type="bibr" target="#b108">[109]</ref>, and RNA-protein interaction prediction <ref type="bibr" target="#b18">[19]</ref>. They can predict crucial aspects of RNA structures, such as 2D base pairing probabilities and 1D solvent accessibilities <ref type="bibr" target="#b108">[109]</ref>, which are essential for understanding RNA function and cellular interactions. Additionally, RNA language models facilitate the design of RNA-based therapeutics <ref type="bibr" target="#b53">[54]</ref>. Understanding RNA structures helps identify novel drug targets and design RNA-based drugs, speeding up drug discovery processes. These pre-trained models can be fine-tuned for various downstream tasks related to RNA structure and function, including RNA 2D/3D structure prediction <ref type="bibr" target="#b108">[109]</ref>, RNA structural alignment and family clustering <ref type="bibr" target="#b1">[2]</ref>, and RNA splice site prediction <ref type="bibr" target="#b19">[20]</ref> from RNA sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.3</head><p>Protein sequences: Venturing into the complex realm of proteins, we embrace luminous works such as <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b111">[112]</ref>, and <ref type="bibr" target="#b100">[101]</ref>. These endeavors illuminate the path to unraveling the intricate choreography of molecular functions and interactions. The protein LLMs have a wide application in functional protein generation <ref type="bibr" target="#b50">[51]</ref> and protein structure prediction <ref type="bibr" target="#b84">[85]</ref>.</p><p>Protein language models can be broadly classified into three main categories based on their functionalities: 1) structure and function understanding: interpreting protein sequences to reveal their structures and functions through evolutionary data or textual enhancements, 2) protein sequence generation: creating novel protein sequences using generative modeling for unexplored spaces or precise design, and 3) multi-modal integration for enhanced understanding and generation: combining various data types to generate protein functions or achieve a unified understanding and generation of protein sequences.</p><p>Structure and Function Understanding models. These models are primarily concerned with interpreting existing protein sequences to deduce their structures, functions, or both, using vast datasets of known proteins. ProteinBERT <ref type="bibr" target="#b15">[16]</ref> is a deep-learning model designed for understanding protein functions and sequences. It combines language modeling with Gene Ontology (GO) <ref type="bibr" target="#b8">[9]</ref> annotation predictions in a self-supervised training approach. Trained on over 106 million proteins from the UniProtKB/UniRef90 database <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b83">84]</ref>, it efficiently captures detailed and broad protein features using a transformer-like architecture. ESM models are noted for their ability to capture deep evolutionary signals and predict protein structures with high accuracy, contributing significantly to structural biology. ESM-1b <ref type="bibr" target="#b72">[73]</ref>, utilizing a Transformer architecture, harnesses unsupervised learning from 250 million protein sequences to predict biological properties. It offers a versatile and comprehensive approach to protein analysis, with applications ranging from structure prediction to functional annotation, by capturing deep evolutionary signals across various biological research tasks. ESM-2 <ref type="bibr" target="#b54">[55]</ref> significantly advances over ESM-1b by utilizing a scaledup model with 15 billion parameters (compared to 650 million) and a richer UniRef90 dataset to predict atomic-level protein structures directly from sequences. This enhancement not only allows for high-resolution structure predictions and the construction of the ESM Metagenomic Atlas but also achieves a notable speed improvement and wider applicability in structural genomics and protein design, thus enhancing diversity and accuracy in structural predictions. ProtST <ref type="bibr" target="#b100">[101]</ref> enhances protein language models by merging protein sequences with biomedical texts from the ProtDescribe dataset, sourced from Swiss-Prot <ref type="bibr" target="#b10">[11]</ref>, to provide a comprehensive understanding of protein functions and properties. This multimodality learning not only addresses gaps in sequence-only models but also enhances protein representations for a variety of applications, including function annotation and localization prediction. xTrimoPGLM <ref type="bibr" target="#b17">[18]</ref> revolutionizes protein sequence analysis and generation by merging autoencoding and autoregressive objectives within a massive 100 billion parameter framework, trained on about 940 million unique protein sequences.</p><p>Sequence Generation based models. In the quest to generate novel protein sequences with the potential to fold into functional proteins, several key players have emerged, notably ProGen <ref type="bibr" target="#b58">[59]</ref>, ProGen2 <ref type="bibr" target="#b64">[65]</ref>, ProtGPT2 <ref type="bibr" target="#b26">[27]</ref>, and LM-Design <ref type="bibr" target="#b111">[112]</ref>. ProGen starts the journey by training on a vast array of 280 million protein sequences, using special tags to make proteins with specific functions <ref type="bibr" target="#b58">[59]</ref>. ProGen2 takes this further by using a much larger model to capture more complex protein patterns and making it possible to predict protein fitness without extra steps <ref type="bibr" target="#b64">[65]</ref>. ProtGPT2, inspired by language models, generates proteins that could exist in nature, trained on 50 million sequences for wide exploration. While ProGen series focus on making diverse and functional proteins, ProtGPT2 pushes the limits of designing completely new proteins, showing the power of deep learning in understanding and creating life's building blocks <ref type="bibr" target="#b26">[27]</ref>. LM-Design innovates in protein design by infusing language models with structural adapters, enhancing the generation of sequences. This addresses both the creativity seen in ProtGPT2 and the functional specificity of the ProGen series with an added focus on structural viability <ref type="bibr" target="#b111">[112]</ref>.</p><p>Multi-modal Integration models. ProtST <ref type="bibr" target="#b100">[101]</ref>, Prot2Text <ref type="bibr" target="#b0">[1]</ref>, and xTrimoPGLM <ref type="bibr" target="#b17">[18]</ref> exemplify the fusion of sequence, structure, and textual data to redefine protein function understanding and design. ProtST integrates dual language models, specifically the ESM series and ProtBert for protein sequences, alongside PubmedBERT <ref type="bibr" target="#b32">[33]</ref> for biomedical texts, to synergize textual descriptions with sequence data <ref type="bibr" target="#b100">[101]</ref>. Prot2Text utilizes Graph Neural Networks and Transformers to generate descriptive texts of protein functions, integrating structural and sequence data with textual annotations. This approach not only enhances function annotation and drug discovery efforts but also enriches databases with contextually rich protein descriptions, setting a new standard for how protein functions are predicted and understood <ref type="bibr" target="#b0">[1]</ref>. xTrimoPGLM, while its primary category is understanding and generation, its methodological approach of combining masked language modeling (MLM) and general language modeling (GLM) objectives allows it to handle multimodal data for enhanced protein sequence analysis <ref type="bibr" target="#b17">[18]</ref>.</p><p>Applications: In the rapidly evolving field of computational biology, all these protein sequence based large language methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b100">101]</ref> are pushing the boundaries of how we understand, design, and utilize proteins. The application of these models spans from detailed protein function annotation <ref type="bibr" target="#b15">[16]</ref> and structure prediction <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b72">73]</ref> to the high-throughput design of novel proteins with specific functionalities <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b64">65]</ref>, significantly impacting drug discovery, synthetic biology, and therapeutic development <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b54">55]</ref>. Looking forward, the integration of even more diverse datasets, the refinement of models to enhance accuracy and efficiency, and the exploration of uncharted areas of the protein universe remain pivotal directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.4</head><p>Multi-omics sequencing data: Within these domains, the transformative capabilities of LLMs manifest in high-impact downstream applications. From predicting molecular structures to forecasting molecule interactions, and from unraveling molecule functions to drawing poignant associations with disease progression processes, LLMs guides us towards a deeper comprehension of life's building blocks. In multi-omic sequencing, models such as scBERT <ref type="bibr" target="#b102">[103]</ref>, Geneformer <ref type="bibr" target="#b88">[89]</ref>, scFoundation <ref type="bibr" target="#b33">[34]</ref> and scGPT <ref type="bibr" target="#b21">[22]</ref> use advanced pre-trained language models to deeply analyze cellular biology and genetics, showcasing the fusion of cutting-edge computation with biological understanding.</p><p>scBERT. scBERT <ref type="bibr" target="#b102">[103]</ref>, a transformer-based deep neural network model, leverages extensive unlabeled single-cell RNA-seq data for pre-training, addressing the challenges of cell type annotation by capturing latent gene-gene interactions and overcoming limitations such as batch effects and the absence of curated marker genes. Through self-supervised learning and supervised fine-tuning, scBERT significantly enhances cell type annotation accuracy, robustness against batch effects, and interpretability over existing methods. Its versatility extends to discovering novel cell types and understanding complex gene expression patterns, offering a comprehensive tool for single-cell omics analysis.</p><p>Geneformer. Geneformer <ref type="bibr" target="#b88">[89]</ref>, pre-trained on about 30 million single-cell transcriptomes, employs a context-aware, attentionbased framework for versatile applications in network biology, including disease modeling and therapeutic target identification, especially in data-scarce areas. It differentiates itself with a selfsupervised learning strategy for fundamental insights into network dynamics. This emphasizes its utility in a broader range of downstream tasks such as chromatin dynamics predictions. In comparison, scBERT, focusing on cell type annotation, demonstrates its strength in handling batch effects and discovering novel cell types through its transformer architecture. Both models offer significant advancements over traditional methods, with Geneformer offering a broader application scope and scBERT excelling in cell type specificity and interpretability. scFoundation. scFoundation <ref type="bibr" target="#b33">[34]</ref> redefines the landscape of singlecell analysis with its large-scale pre-trained model, trained on an unprecedented dataset of over 50 million human single-cell transcriptomes. Through its unique asymmetric architecture and the introduction of a read-depth-aware pre-training task, the model efficiently addresses the complexities of single-cell RNA-seq data, including its sparsity and high dimensionality. Distinguished by its vast scale of trainable parameters and extensive data coverage, scFoundation delivers superior performance across a spectrum of downstream applications, from enhancing gene expression to predicting drug responses.</p><p>scGPT. scGPT <ref type="bibr" target="#b21">[22]</ref>, trained on 33 million cells, significantly advances cell biology and genetics research with its large-scale data analysis. It differentiates itself by applying generative pre-training on extensive multi-omics data for a comprehensive analysis. Using a self-attention transformer, scGPT excels in analyzing complex cell data, enabling it to perform exceptionally well in identifying cell types, predicting genetic changes, and integrating multi-omics data. Its ability to derive insights into gene-gene interactions and demonstrate a scaling effect of improved performance with larger datasets highlights its potential for single-cell omics research.</p><p>Applications: The integration of advanced computational models like scBERT <ref type="bibr" target="#b102">[103]</ref>, scFoundation <ref type="bibr" target="#b33">[34]</ref>, scGPT <ref type="bibr" target="#b21">[22]</ref>, and Geneformer <ref type="bibr" target="#b88">[89]</ref> into single-cell and multi-omics analysis heralds a new era in precision biology. These models, each with its unique approach ranging from cell type annotation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b102">103]</ref> to the elucidation of complex gene networks, offer comprehensive tools for dissecting the intricacies of cellular functions. Future research directions anticipate the convergence of these models into a cohesive analytical framework, enhancing our capability to predict disease mechanisms and responses to treatment based on sophisticated cell-level predictions <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b88">89]</ref>. As the pool of omics data expands, these models will continue to evolve, driving forward the discovery of novel biological mechanisms and therapeutic targets <ref type="bibr" target="#b88">[89]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">LLMs on Brain Signals</head><p>Last, we delve into the fascinating realm of applying LLMs to brain signals. In this section, we focus on two topics: brain LLM building and downstream applications in brain-to-text translation. We start with introducing various pre-training techniques for brain EEG signal representation learning, including BrainBERT (self-supervised representation learning for intracranial recordings) <ref type="bibr" target="#b91">[92]</ref>, MMM (topology-agnostic EEG representation learning) <ref type="bibr" target="#b106">[107]</ref>, and LaBraM (generic representation learning for tremendous EEG data from various sources) <ref type="bibr" target="#b38">[39]</ref>. MMM. MMM <ref type="bibr" target="#b106">[107]</ref> is a novel framework for EEG pre-training. MMM achieves this by mapping different EEG channel selections onto a unified topology and employing strategies such as Multidimensional Positional Encoding, Multi-level Channel Hierarchy, and Multi-stage Pre-training. This approach facilitates topologyagnostic EEG representation learning, enhancing cross-dataset generalizability. Evaluations on SEED <ref type="bibr" target="#b110">[111]</ref> and SEED-IV <ref type="bibr" target="#b109">[110]</ref> datasets demonstrate MMM's superior performance in emotion recognition tasks, outperforming traditional and deep learning baselines. Specifically, MMM surpasses previous state-of-the-art methods in subject-dependent classification, showcasing its robust transferability across different EEG datasets and sensor configurations. The results validate MMM's effectiveness in leveraging larger datasets for extracting more generalized representations.</p><p>LaBraM. LaBraM <ref type="bibr" target="#b38">[39]</ref> is a novel Large Brain Model, which marks a significant advancement in EEG-based deep learning through its pre-training on over 2500 hours of diverse EEG data. LaBraM overcomes the limitations of EEG data variability and volume by transforming EEG signals into unified channel patches and employing vector-quantized neural spectrum prediction for efficient learning. The model's architecture enables the effective learning of EEG signal representations, addressing both temporal and spatial features. LaBraM is evaluated on various downstream tasks such as abnormal detection, event type classification, emotion recognition, and gait prediction. In these evaluations, LaBraM consistently outperforms state-of-the-art methods across multiple metrics, underscoring its superior ability to generalize from large-scale EEG data. Moreover, the model exhibits scalability with increasing dataset sizes, suggesting potential for further performance improvements with even larger datasets. This research not only sets new benchmarks for EEG-based analyses but also opens up new avenues for deep learning applications in neuroscience and medical diagnostics.</p><p>Applications: We further discuss a broad range of brain LLM applications in brain-computer interfaces (BCIs).</p><p>Brain-to-Text Translation: Previous research on converting brain signals to text, as described in papers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b82">83]</ref>, has shown notable success with small and restricted vocabularies. However, these studies have struggled to achieve similar accuracy levels with larger and unrestricted vocabularies. Building upon this foundation, we further introduce an exciting topic of open-vocabulary brain-to-text translation <ref type="bibr" target="#b95">[96]</ref>, including recent work DeWave (discrete encoding for EEG-to-text translation) <ref type="bibr" target="#b24">[25]</ref> and continuous language reconstruction from fMRI images <ref type="bibr" target="#b85">[86]</ref>. Starting with EEG2TEXT <ref type="bibr" target="#b95">[96]</ref>, this paper presents a method of open vocabulary EEG-To-Text decoding and zero-shot sentence sentiment classification . Tested on the ZuCo dataset <ref type="bibr" target="#b36">[37]</ref>, the author utilizes pre-trained language models like BART <ref type="bibr" target="#b51">[52]</ref>, achieving significant advancements with a 40.1% BLEU-1 score for EEG-To-Text decoding and a 55.6% F1 score for zero-shot EEG-based ternary sentiment classification, which notably surpass the supervised baselines. Further advancing the field, DeWave <ref type="bibr" target="#b24">[25]</ref> is a novel framework for converting EEG signals into text by leveraging a discrete codex encoding. DeWave utilizes a quantized variational encoder to transform EEG waves into discrete representations, aligning them with pre-trained language models for enhanced translation accuracy. The result surpassing previous baselines including EEG2TEXT. In a parallel vein, the paper <ref type="bibr" target="#b85">[86]</ref> explores a non-invasive brain-computer interface decoding continuous natural language from fMRI brain recordings. Employing a unique approach, the model addresses fMRI's low temporal resolution by generating candidate word sequences, scoring them against brain responses to identify the most likely stimuli being heard or imagined.</p><p>Brain-to-Image Translation: Image Translation is also a mainstream Downstream Tasks of brain signals. Some recent interesting work includes nature image decoding <ref type="bibr" target="#b81">[82]</ref> NeuroGAN <ref type="bibr" target="#b60">[61]</ref> and EEG2IMAGE <ref type="bibr" target="#b77">[78]</ref>. Specifically, nature image decoding <ref type="bibr" target="#b81">[82]</ref> introduces a self-supervised framework, which leverages a large and diverse EEG-image dataset <ref type="bibr" target="#b31">[32]</ref>. This dataset is used in conjunction with a novel approach that applies contrastive learning to align features extracted from image stimuli and corresponding EEG responses, significantly advancing the field of non-invasive braincomputer interfaces. The framework employs self-attention and graph attention modules within the EEG encoder to enhance spatial feature extraction, reflecting the spatial dynamics of brain activity related to object recognition. NeuroGAN <ref type="bibr" target="#b60">[61]</ref> introduces a sophisticated method for transforming EEG signals into visual images, leveraging a specialized architecture to enhance image synthesis from brain activity data. Central to NeuroGAN is a cross-modality encoder-decoder structure, which effectively compresses EEG features into a latent space and reconstructs corresponding images, focusing on capturing the complex relationship between neural signals and visual stimuli. The method also incorporates a perceptual loss function, utilizing a pre-trained image classifier to measure the perceptual similarity between generated and real images. Similarly, EEG2IMAGE <ref type="bibr" target="#b77">[78]</ref> introduces an innovative framework for synthesizing images from EEG signals. The framework uses small EEG datasets to learn features via a contrastive learning approach and synthesizes images using a modified conditional Generative Adversarial Network. Specifically, the framework employs semi-hard triplet loss for feature extraction from EEG signals, ensuring that signals from similar images are closer to the learned feature space, leading to more accurate image reconstruction.</p><p>Other Brain EEG Applications: In addition to text translation and image translation, brain signal decoding also performs well in other downstream tasks. In the field of voice decoding, NeuroTalk <ref type="bibr" target="#b49">[50]</ref> presents a novel model, for transforming non-invasive EEG signals of imagined speech into audible voice outputs. The model combines multi-receptive residual modules with recurrent neural networks to process brain signals effectively. Notably, the model could generate words not included in the training dataset, indicating its potential for broader vocabulary coverage. In the motion decoding domain, Motor Recognition <ref type="bibr" target="#b43">[44]</ref> explores using EEG data for recognizing motor activities. The research employs an innovative ensemble approach combining stacked bidirectional long short-term memory (BiLSTM) with long short-term memory (LSTM) networks alongside a newly proposed EEG-transformer network to classify 17 different everyday motor activities. Their ensemble achieves a classification accuracy of 98.5%, which has a substantial advancement over existing state-of-the-art methodologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONCLUSIONS AND FUTURE DIRECTIONS</head><p>As a conclusion, this is a comprehensive survey of large language models for biomedicine, focusing on three pivotal data types: 1) textual data, 2) biological sequences, and 3) brain signals. There are also significant new challenges that come with using AI in biomedical research. One big challenge is making sure that the biomedical insights enhanced by AI are reliable and trustworthy, including model explainability and interpretability, model robustness to adversarial attacks, model bias towards different populations, and data privacy issues. Another challenge is the personalization of LLMs, which means adjusting LLMs to fit the specific needs of different personalized data. For example, there is a large individual variance in brain signals when different people are thinking of the same word under the same context. Instead of using one LLM to fit everyone, can we construct personalized LLMs based on different brain patterns for different people? The last challenge is the multimodality data. Since biomedical information can be very varied, we learn how to handle different types of data in a skillful and effective way. For example, Google has announced Med-PaLM-2 <ref type="bibr" target="#b79">[80]</ref> that integrates image, text, and genome data in the electronic health record, declaring an expert-level ability for medical question answering. Can we develop more effective and efficient methods to integrate multi-modal and multi-omic LLMs into one powerful unified LLM? This survey paper serves as a foundation step to bridge the gap and encourages both theoretical researchers and practitioners in LLMs to look into real-world scientific applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of applications, models, and downstream tasks for biomedical LLMs on textual data.</figDesc><graphic coords="2,53.80,83.69,242.10,79.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>utilizes the multiple links between documents, resulting in a multi-task self-supervised pre-training, modeling both document relation extraction and masked language modeling. BioLinkBERT utilizes PubMed, equipped with citations, as its pre-training corpus. To facilitate the document relation extraction task, BioLinkBERT creates a document relation extraction dataset of the pre-training corpus. Each pair of sections (coming from either the same or different documents) falls under one of three classes: random, contiguous, or linked, where "contiguous" indicates the sections (from the same document) are right next to each other, and "linked" indicates two sections (from different documents) are indeed linked. BioLinkBERT achieves state-of-the-art performance on 12 of the 13 different datasets used for evaluations that span 6 different tasks (named entity recognition, PICO extraction, relation extraction, sentence similarity, document classification, and question answering).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of models and applications for Genomic LLMs on biological sequences.</figDesc><graphic coords="4,53.80,86.30,242.09,134.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of models, downstream tasks and challenges for brain signal LLMs.</figDesc><graphic coords="7,317.96,86.30,242.12,102.95" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>This work is sponsored by the <rs type="funder">Commonwealth Cyber Initiative</rs>, <rs type="affiliation">Children's National Hospital, Fralin Biomedical Research Institute (Virginia Tech</rs>), <rs type="person">Sanghani Center</rs> for AI and <rs type="person">Data Analytics</rs> (<rs type="affiliation">Virginia Tech</rs>), <rs type="funder">Virginia Tech Innovation Campus</rs>, and a generous gift from the <rs type="institution">Amazon + Virginia Tech Center for Efficient and Robust Machine Learning</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Costas Bouyioukos, and Michalis Vazirgiannis. Prot2text: Multimodal protein&apos;s function generation with gnns and transformers</title>
		<author>
			<persName><forename type="first">Michail</forename><surname>Hadi Abdine</surname></persName>
		</author>
		<author>
			<persName><surname>Chatzianastasis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.14367</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Informative RNA base embedding for RNA structural alignment and clustering by deep representation learning</title>
		<author>
			<persName><forename type="first">Manato</forename><surname>Akiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasubumi</forename><surname>Sakakibara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAR Genomics and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Biom-transformers: building large biomedical language models with bert, albert and electra</title>
		<author>
			<persName><forename type="first">Sultan</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th workshop on biomedical language processing</title>
		<meeting>the 20th workshop on biomedical language processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="221" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew Ba</forename><surname>Wa Redmond</surname></persName>
		</author>
		<author>
			<persName><surname>Mcdermott</surname></persName>
		</author>
		<title level="m">Publicly available clinical bert embeddings. NAACL HLT 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">72</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Construction of the literature graph in semantic scholar</title>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Dunkelberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vu</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Kohlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><surname>Hsu-Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Skjonsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><surname>Etzioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paige</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10403</idno>
		<title level="m">Palm 2 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speech synthesis from neural decoding of spoken sentences</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Gopala K Anumanchipalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">F</forename><surname>Chartier</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="issue">7753</biblScope>
			<biblScope unit="page" from="493" to="498" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for classification of alignments of non-coding rna sequences</title>
		<author>
			<persName><forename type="first">Genta</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasubumi</forename><surname>Sakakibara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="237" to="244" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gene ontology: tool for the unification of biology</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><forename type="middle">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><forename type="middle">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Botstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kara</forename><surname>Dolinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Selina</forename><forename type="middle">S</forename><surname>Dwight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janan</forename><forename type="middle">T</forename><surname>Eppig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature genetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective gene expression prediction from sequence by integrating long-range interactions</title>
		<author>
			<persName><forename type="first">Žiga</forename><surname>Avsec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Visentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1196" to="1203" />
			<date type="published" when="2021-10">Oct 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The swiss-prot protein sequence database and its supplement trembl in 2000</title>
		<author>
			<persName><forename type="first">Amos</forename><surname>Bairoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Apweiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="48" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dna language models are powerful predictors of genome-wide variant effects</title>
		<author>
			<persName><forename type="first">Gonzalo</forename><surname>Benegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjit</forename><surname>Singh Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page">2311219120</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning the protein language: Evolution, structure, and function</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="654" to="669" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Uniprotkb/swiss-prot: the manually annotated section of the uniprot knowledgebase</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Boutet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Lieberherr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tognolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Bairoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Plant bioinformatics: methods and protocols</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="89" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Proteinbert: a universal deep-learning model of protein sequence and function</title>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Linial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2102" to="2110" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">xtrimopglm: unified 100b-scale pre-trained transformer for deciphering the language of protein</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangli-Ao</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilei</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.06199</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpretable rna foundation model from unannotated data for highly accurate rna structure and function predictions</title>
		<author>
			<persName><forename type="first">Jiayang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxiong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinze</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2028" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised learning on millions of pre-mrna sequences improves sequence-based rna splicing prediction</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuedong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">scgpt: toward building a foundation model for single-cell multi-omics using generative ai</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassaan</forename><surname>Maan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengning</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The nucleotide transformer: Building and evaluating robust foundation models for human genomics</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Dalla-Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Mendoza-Revilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">Lopez</forename><surname>Carranza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Henryk Grzywaczewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Oteri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Trop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Sirelkhatim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2024" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discrete encoding of eeg waves for eeg to text translation</title>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Teng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Dewave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtyseventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An integrated encyclopedia of dna elements in the human genome</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Dunham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelley</forename><forename type="middle">F</forename><surname>Aldred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">B</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Frietze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Harrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajinder</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">489</biblScope>
			<biblScope unit="issue">7414</biblScope>
			<biblScope unit="page" from="57" to="74" />
			<date type="published" when="2012-09">Sep 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Protgpt2 is a deep unsupervised language model for protein design</title>
		<author>
			<persName><forename type="first">Noelia</forename><surname>Ferruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Birte</forename><surname>Höcker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4348</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gena-lm: A family of open-source foundational models for long dna sequences</title>
		<author>
			<persName><forename type="first">Veniamin</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksei</forename><surname>Shmelev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Shepelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Chekanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kardymon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Burtsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2026" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A pre-training and self-training approach for biomedical named entity recognition</title>
		<author>
			<persName><forename type="first">Shang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivera</forename><surname>Kotevska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sorokine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>16 2:e0246310</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Contemporary progress and opportunities in RNA-targeted drug discovery</title>
		<author>
			<persName><forename type="first">Amanda</forename><forename type="middle">L</forename><surname>Garner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Med. Chem. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="259" />
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A large and rich eeg dataset for modeling human visual object recognition</title>
		<author>
			<persName><forename type="first">Alessandro</forename><forename type="middle">T</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radoslaw</forename><forename type="middle">M</forename><surname>Cichy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page">119754</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large scale foundation model on single-cell transcriptomics</title>
		<author>
			<persName><forename type="first">Minsheng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuegong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2025" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Brain-to-text: decoding spoken phrases from phone representations in the brain</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Herff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><forename type="middle">De</forename><surname>Pesters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Telaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerwin</forename><surname>Schalk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">217</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graphclust: alignment-free structural clustering of local rna secondary structures</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Heyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Backofen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="224" to="232" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Hollenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Rotsztejn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Troendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Pedroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Langer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome</title>
		<author>
			<persName><forename type="first">Yanrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramana</forename><forename type="middle">V</forename><surname>Davuluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2112" to="2120" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large brain model for learning generic representations with tremendous eeg data in bci</title>
		<author>
			<persName><forename type="first">Wei-Bang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Ming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Bao-Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thriteenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">What disease does this patient have? a large-scale open domain question answering dataset from medical exams</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassim</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13081</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pubmedqa: A dataset for biomedical research question answering</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2567" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The mimic code repository: enabling reproducibility in critical care research</title>
		<author>
			<persName><forename type="first">E W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><forename type="middle">A</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="39" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Wei H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mimic-iii, a freely accessible critical care database. Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Motor activity recognition using eeg data and ensemble of stacked BLSTM-LSTM network and transformer model</title>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilina</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023</title>
		<meeting><address><addrLine>Rhodes Island, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">June 4-10, 2023. 2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cross-species regulatory sequence activity prediction</title>
		<author>
			<persName><surname>David R Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1008050</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Konstantinos Bougiatiotis, and Georgios Paliouras. Bioasq-qa: A manually curated corpus for biomedical question answering</title>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Nentidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<editor>
			<persName><forename type="first">Eduardo</forename><surname>Blanco</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">November 2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">ClinVar: improvements to accessing data</title>
		<author>
			<persName><forename type="first">Melissa</forename><forename type="middle">J</forename><surname>Landrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanmuga</forename><surname>Chitipiralla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garth</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoshan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonhee</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuljeet</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunlei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Lyoshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenith</forename><surname>Maddipatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Maiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Nuala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyao</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donna</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Maglott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandi</forename><forename type="middle">L</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><surname>Kattman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="11" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards voice reconstruction from EEG during imagined speech</title>
		<author>
			<persName><forename type="first">Young-Eun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seo-Hyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Ho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023</title>
		<editor>
			<persName><forename type="first">Brian</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2023">February 7-14, 2023. 2023</date>
			<biblScope unit="page" from="6030" to="6038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Rasko</forename><surname>Leinonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Garcia Diez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Binns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Fleischmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Apweiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="3236" to="3237" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Uniprot archive</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note>translation, and comprehension</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">BioCreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<biblScope unit="page" from="5" to="2016" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Codonbert: Large language models for mrna design and optimization</title>
		<author>
			<persName><forename type="first">Sizhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Moayedpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saleh</forename><surname>Riahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Kogler-Anele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Miladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Miner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khang</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnie</forename><surname>Zacharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Clinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carla</forename><surname>Asquith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Skaleski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianne</forename><surname>Boeglin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudha</forename><surname>Chivukula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anusha</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">Ulloa</forename><surname>Montoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziv</forename><surname>Bar-Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Jager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Evolutionaryscale prediction of atomic-level protein structure with a language model</title>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halil</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Smetanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Kabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Shmueli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="issue">6637</biblScope>
			<biblScope unit="page" from="1123" to="1130" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Compositional zero-shot domain transfer with text-to-text models</title>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianchu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruthi</forename><surname>Bannur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pérez-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Alvarez-Valle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13386</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">BioRED: a rich biomedical relation extraction dataset</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Ting</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecilia</forename><forename type="middle">N</forename><surname>Arighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Biogpt: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">409</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Large language models generate functional protein sequences across diverse families</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">R</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subu</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">P</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Holton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Luis Olmos</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Machine translation of cortical activity to text with an encoder-decoder framework</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Joseph G Makin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">F</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="575" to="582" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Neurogan: image reconstruction from EEG signals via an attention-based GAN</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishan</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjeet</forename><surname>Ranjan Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Bhavsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9181" to="9192" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Neuroprosthesis for decoding speech in a paralyzed person with anarthria</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">L</forename><surname>David A Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessie</forename><forename type="middle">R</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopala</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Anumanchipalli</surname></persName>
		</author>
		<author>
			<persName><surname>Joseph G Makin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pengfei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><forename type="middle">E</forename><surname>Chartier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">M</forename><surname>Dougherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Abrams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="227" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Faizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Callum</forename><surname>Birch-Sykes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wornow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Rabideau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.15794</idno>
		<title level="m">Long-range genomic sequence modeling at single nucleotide resolution</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Thinking out loud, an open-access eeg-based bci dataset for inner speech recognition</title>
		<author>
			<persName><forename type="first">Nicolás</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Esteban</forename><surname>Rufiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Kamienkowski</surname></persName>
		</author>
		<author>
			<persName><surname>Spies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Progen2: exploring the boundaries of protein language models</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">A</forename><surname>Ruffolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><forename type="middle">N</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Madani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="968" to="978" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The complete sequence of a human genome</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Nurk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arang</forename><surname>Rhie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikko</forename><surname>Rautiainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><forename type="middle">V</forename><surname>Bzikadze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alla</forename><surname>Mikheenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">R</forename><surname>Vollger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Altemose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Uralsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="issue">6588</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Decoding covert speech from eeg-a comprehensive review</title>
		<author>
			<persName><forename type="first">Jerrin</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panachakel</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Angarai</forename><surname>Ganesan Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">392</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aokun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaleb</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><surname>Pournejatian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">B</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheryl</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">G</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Magoc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13523</idno>
		<title level="m">A study of generative large language model for medical research and healthcare</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Long N Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Anibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaurya</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><surname>Chanana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03598</idno>
		<title level="m">Erol Bahadroglu, Alec Peltekian, and Grégoire Altan-Bonnet. Scifive: a text-to-text transformer model for biomedical literature</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Ai in health and medicine</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oishi</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Med</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="38" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">15</biblScope>
			<date type="published" when="2021">2016239118. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Fast and accurate clustering of noncoding rnas using ensembles of sequence alignments and secondary structures</title>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kengo</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasubumi</forename><surname>Sakakibara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">In-domain contextaware token embeddings improve biomedical named entity recognition</title>
		<author>
			<persName><forename type="first">Golnar</forename><surname>Sheikhshab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inanç</forename><surname>Birol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Louhi@EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Biomegatron: Larger biomedical domain language model</title>
		<author>
			<persName><forename type="first">Hoo-Chang</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evelina</forename><surname>Bakhturina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Mani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4700" to="4706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">EEG2IMAGE: image reconstruction from EEG brain signals</title>
		<author>
			<persName><forename type="first">Prajwal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Miyapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanmuganathan</forename><surname>Raman</surname></persName>
		</author>
		<idno>CoRR, abs/2302.10121</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Large language models encode clinical knowledge</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pfohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.13138</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Towards expert-level medical question answering with large language models</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juraj</forename><surname>Gottweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rory</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellery</forename><surname>Wulczyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darlene</forename><surname>Neal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.09617</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Transfer learning in electronic health records through clinical concept embedding</title>
		<author>
			<persName><forename type="first">Jose</forename><surname>Roberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayala</forename><surname>Solares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelaali</forename><surname>Hassaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shishir</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mamouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dexter</forename><surname>Canoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazem</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Salimi-Khorshidi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Decoding natural images from EEG for object recognition</title>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingchuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorong</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Towards sentence-level brain decoding with distributed representations</title>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7047" to="7054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Uniref: comprehensive and non-redundant uniprot reference clusters</title>
		<author>
			<persName><forename type="first">Hongzhan</forename><surname>Baris E Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cathy</forename><forename type="middle">H</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1282" to="1288" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Baris E Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cathy</forename><forename type="middle">H</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uniprot</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Semantic reconstruction of continuous language from non-invasive brain recordings</title>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Lebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shailee</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Huth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Medagents: Large language models as collaborators for zero-shot medical reasoning</title>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anni</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Gerstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">Ross</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elvis</forename><surname>Saravia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Stojnic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09085</idno>
		<title level="m">Galactica: A large language model for science</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Transfer learning enables predictions in network biology</title>
		<author>
			<persName><forename type="first">Christina</forename><forename type="middle">V</forename><surname>Theodoris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anant</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeina R Al</forename><surname>Mark D Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">C</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helene</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">M</forename><surname>Mantineo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexian</forename><surname>Brydon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><surname>Shirley Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">618</biblScope>
			<biblScope unit="issue">7965</biblScope>
			<biblScope unit="page" from="616" to="624" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Large language models in medicine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ajan</surname></persName>
		</author>
		<author>
			<persName><surname>Thirunavukarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Livia</forename><surname>Elangovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien</forename><forename type="middle">F</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Med</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1930" to="1940" />
			<date type="published" when="2023-07-17">Aug 2023. 2023 Jul 17</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Pre-trained language models in biomedical domain: A systematic survey</title>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahuan</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prayag</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM Computing Surveys</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Brainbert: Self-supervised representation learning for intracranial recordings</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vighnesh</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Yaari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Chatcad: Interactive computer-aided diagnosis on medical image using large language models</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Reactclass: Cross-modal supervision for subword-guided reactant entity classification</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><forename type="middle">Cherrice</forename><surname>Loving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="844" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Chemner: fine-grained chemistry named entity recognition with ontologyguided distant supervision</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shweta</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Open vocabulary electroencephalography-totext decoding and zero-shot sentiment classification</title>
		<author>
			<persName><forename type="first">Zhenhailong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="5350" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Can gpt-4v(ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis</title>
		<author>
			<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weike</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixiong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">Towards generalist foundation model for radiology by leveraging web-scale 2d&amp;3d medical data</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senjie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoran</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyue</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihan</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wensen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<title level="m">The rise and potential of large language model based agents: A survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Protst: Multimodality learning of protein sequences and biomedical texts</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Miret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12040</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Prediction of rna-protein interactions using a nucleotide language model</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiaki</forename><surname>Hamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics Advances</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="852" to="866" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Harnessing the power of llms in practice: A survey on chatgpt and beyond</title>
		<author>
			<persName><forename type="first">Jingfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13712</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">A large language model for electronic health records</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aokun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><surname>Pournejatian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Hoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaleb</forename><forename type="middle">E</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Parisien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheryl</forename><surname>Compas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">B</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">G</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><surname>Flores</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">194</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Linkbert: Pretraining language models with document links</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Learning topology-agnostic eeg representations with geometry-aware modeling</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Artificial intelligence for science in quantum, atomistic, and continuum systems</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Helwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqiang</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08423</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Multiple sequencealignment-based rna language model and its application to structural inference</title>
		<author>
			<persName><forename type="first">Yikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Litfin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaswinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiansong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2023" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Emotionmeter: A multimodal framework for recognizing human emotions</title>
		<author>
			<persName><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1110" to="1122" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks</title>
		<author>
			<persName><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on autonomous mental development</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="162" to="175" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Structure-informed language models are protein designers</title>
		<author>
			<persName><forename type="first">Zaixiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2025" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Reactie: Enhancing chemical reaction extraction with weak supervision</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siru</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="12120" to="12130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Predicting effects of noncoding variants with deep learning-based sequence model</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><forename type="middle">G</forename><surname>Troyanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="931" to="934" />
			<date type="published" when="2015-10">October 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Clinical concept extraction with contextual word embedding</title>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Ch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><forename type="middle">M</forename><surname>Paschalidis</surname></persName>
		</author>
		<author>
			<persName><surname>Tahmasebi</surname></persName>
		</author>
		<idno>ArXiv, abs/1810.10566</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Genome-scale language models reveal sars-cov-2 evolutionary dynamics</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Zvyagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Brace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Hippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cindy</forename><forename type="middle">Orozco</forename><surname>Bohorquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Clyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Perez-Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2032" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
