<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?</title>
				<funder ref="#_7M7tJTj">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_jp8nWXC">
					<orgName type="full">Yongjiang Talent Introduction Programme</orgName>
				</funder>
				<funder ref="#_kVdHzsb">
					<orgName type="full">Zhejiang Provincial Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_KuSzAWt">
					<orgName type="full">Ningbo Natural Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">CAAI-Huawei MindSpore Open Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-06-09">9 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuqi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
							<email>zhangningyu@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Hyung</roleName><forename type="first">Paul</forename><surname>Barham</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Won</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vin- Odkumar</forename><surname>Prabhakaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reiner</forename><surname>Pope</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Toju</forename><surname>Duke</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liam</forename><surname>Fedus</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Luan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Spiridonov</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shivani</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Omernick</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thanumalayan</forename><forename type="middle">Sankaranarayana</forename><surname>Pillai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marie</forename><surname>Pellat</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erica</forename><surname>Mor- Eira</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brennan</forename><surname>Saeta</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Diaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kathy</forename><surname>Meier-Hellstern</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-09">9 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">D64A665BE5D6FEB315B1E2BED12FB67C</idno>
					<idno type="arXiv">arXiv:2305.01555v4[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Few-shot Relation Extraction (RE) appeals to many researchers in Natural Language Processing (NLP) due to the capability to extract textual information where only a few support examples are given <ref type="bibr" target="#b10">(Han et al., 2018;</ref><ref type="bibr" target="#b32">Yang et al., 2021;</ref><ref type="bibr">Han et al., 2021a;</ref><ref type="bibr" target="#b3">Brody et al., 2021;</ref><ref type="bibr" target="#b17">Ma et al., 2023)</ref>. Most previous works focus on fine-tuning <ref type="bibr" target="#b24">(Soares et al., 2019;</ref><ref type="bibr" target="#b33">Ye et al., 2022)</ref> or prompt-tuning <ref type="bibr" target="#b5">(Chen et al., 2022;</ref><ref type="bibr">Han et al., 2021b)</ref> with relatively small language models, e.g., RoBERTa <ref type="bibr" target="#b13">(Liu et al., 2019)</ref>. Recently, with the scaling of model size and corpus size, large language models (LLMs) such as ChatGPT <ref type="bibr" target="#b18">(OpenAI, 2022)</ref> and <ref type="bibr">GPT-4 (OpenAI, 2023a)</ref> have demonstrated powerful abilities by demonstrating only a few instances, a.k.a In-Context Learning <ref type="bibr" target="#b5">(Dong et al., 2023)</ref>. Although LLMs have achieved remarkable results in many NLP tasks, their po-tential in few-shot relation extraction has not been fully explored yet.</p><p>In this paper, we take GPT-3.5 (OpenAI, 2023b) as an exemplary LLM to investigate how to maximize the utilization of LLMs for the few-shot relation extraction task with in-context learning and data generation. Different from text classification, the relation extraction task contains rich predefined schemas (e.g., entity and relation type constraints) and a relatively large and complex classification space with noisy data. We further design two simple-yet-effective strategies to unleash the power of large language models better: task-related instructions and schema-constrained data generation. We conduct exhaustive experiments on four well-known relation extraction datasets. Empirical results indicate that LLMs can potentially be advantageous to few-shot relation extraction and boost previous prompt learning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-shot Relation Extraction</head><p>The relation extraction task aims to extract the relationship between head and tail entities within a plain context. Specifically, one instance for the relation extraction task consists of a context x = {x 1 , x 2 , ..., h, ..., t, ..., x |x| }, head and tail entity mentions h and t, entity types t h and t t , and the relation y ∈ Y between h and t, where Y is the set of candidate relations. RE systems will predict y given x, h, t, t h and t t . For few-shot relation extraction, fine-tuning pre-trained language models (PLMs) is a direct solution <ref type="bibr" target="#b8">(Han et al., 2019;</ref><ref type="bibr" target="#b31">Yamada et al., 2020;</ref><ref type="bibr" target="#b11">Joshi et al., 2020;</ref><ref type="bibr" target="#b16">Lyu and Chen, 2021;</ref><ref type="bibr" target="#b38">Zhou and Chen, 2022)</ref>. To alleviate the gap between pre-training objectives and downstream applications, prompt tuning has recently been applied to relation extraction, especially for low-resource scenarios <ref type="bibr" target="#b5">(Chen et al., 2022;</ref><ref type="bibr">Han et al., 2021b</ref><ref type="bibr" target="#b7">Han et al., , 2022))</ref>  utilize relatively small language models (RoBERTa <ref type="bibr" target="#b13">(Liu et al., 2019)</ref>, GPT2 <ref type="bibr" target="#b23">(Radford et al., 2019)</ref>), demonstrating empirical success regarding fewshot relation extraction performance. To date, large language models have demonstrated powerful abilities by prompting a few instances without tuning <ref type="bibr" target="#b35">(Ding et al., 2022)</ref>; however, the power of LLMs for few-shot relation extraction is little known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Most of those approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Large Language Models</head><p>Large language models, trained with exceedingly large corpora and often with a great number of parameters (≥10B), have achieved excellent performance in numerous downstream NLP tasks <ref type="bibr" target="#b28">(Taylor et al., 2022;</ref><ref type="bibr" target="#b36">Zhang et al., 2022;</ref><ref type="bibr" target="#b35">Zeng et al., 2022;</ref><ref type="bibr">Chowdhery et al., 2022;</ref><ref type="bibr" target="#b21">Ouyang et al., 2022)</ref>. Compared to relatively small language models (SLMs), LLMs are usually not open-source and can not be fine-tuned, which is challenging for downstream task adaptation. Therefore, in-context learning <ref type="bibr" target="#b4">(Brown et al., 2020)</ref> is proposed to utilize prompts with a few demonstrations for few-shot learning. Previous studies <ref type="bibr" target="#b34">(Yoo et al., 2021;</ref><ref type="bibr" target="#b29">Wang et al., 2021)</ref> have investigated using LLMs for text classification and generation. In this work, we take the first step to study few-shot RE with large language models, which brings new challenges and insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LLMs for Few-shot Relation Extraction</head><p>In this section, we introduce two strategies to utilize LLMs for relation extraction: 1) in-context learning ( §3.1); 2) data generation ( §3.2) with LLMs, as shown in Figure <ref type="figure" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">In-Context Learning with LLMs</head><p>The first strategy applies in-context learning (ICL) by providing LLMs with demonstrations in the  <ref type="bibr" target="#b21">(Ouyang et al., 2022)</ref> and ChatGPT (OpenAI, 2022), we design the task-related instruction describing the relation extraction task and add it to the prompt, which is named INSTRUCT PROMPT. Meanwhile, according to previous few-shot RE works <ref type="bibr" target="#b38">(Zhou and Chen, 2022)</ref>, entity types (schemas) are helpful; therefore, we also explore the effectiveness of schemas in prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Generation with LLMs</head><p>To complement the scarcity of labeled data, we introduce another strategy: data generation via LLMs. Specifically, we utilize specific prompts with descriptions of data forms to guide LLMs to generate more in-domain labeled data autonomously, which is subsequently employed to fine-tune a relatively small language model with existing few-shot labeled training data. We design the prompt to tell the essential components ( In-context learning is implemented on the four whole test sets. Different demonstrations are randomly sampled from the shuffled training set every time to avoid effects from permutations of demonstrations <ref type="bibr" target="#b14">(Lu et al., 2021)</ref>. As for data generation, generated data from GPT-3.5 and original few-shot training data are combined to fine-tune two baselines, TYP Marker <ref type="bibr" target="#b38">(Zhou and Chen, 2022)</ref> and KnowPrompt <ref type="bibr" target="#b5">(Chen et al., 2022)</ref>. Using different shots of generated data will lead to different results. Therefore, we increasingly add generated k-shot (k ∈ {8, 16, 32, 48}) data to the original 8-shot and 16-shot training data respectively and report the best performance over k in Tabel 1. More details are shown in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Findings for Relation Extraction</head><p>In-context learning on LLMs can achieve comparable performance for RE with tuning relatively small PLMs. From Table <ref type="table" target="#tab_0">1</ref>, we notice that ICL with only one-shot demonstrations can obtain competitive performance with full parameter tuning-based prompt learning baselines. Using LLMs via ICL does not necessitate any parameter updates, which contains the potential value of mak- ing models scenario-adaptable, unlike supervised learning requiring parameter optimization.</p><p>Data generation with LLMs can boost previous solutions to obtain new state-of-the-art few-shot RE results. We find that previous baselines can significantly improve with 10.7% for 16-shot in SciERC and 6.6% for 16-shot in RE-TACRED by simply using generated data from GPT-3.5 in Table <ref type="table" target="#tab_0">1</ref>. To be noted, data generation is a simple yet effective approach to elicit the power from the LLM to previous methods, and we demonstrate that using schema-constrained generation with LLMs can benefit all previous approaches with SLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Prompts in In-context Learning with LLMs</head><p>Instructions and schemas play an essential role in in-context learning for RE with LLMs.</p><p>From Table <ref type="table" target="#tab_1">2</ref>, we notice that the model with INSTRUCT PROMPT obtains better performance than TEXT PROMPT in most cases, indicating taskrelated information indeed helps to unlock more ability of LLMs for RE. Aberrant results are shown in TACRED and TACREV because incorrectly labeled demonstrations from the two datasets will violate the correct instruction fed into LLMs, which confuses LLMs and results on worse performance than ICL without the instruction. Moreover, adding schema information obtains much better performance, exhibiting the importance of pre-defined structural information for relation extraction.</p><p>More demonstrations, counter-intuitively, may not lead to performance improvement for RE with LLMs. We find performance will not improve even drop and the gap between INSTRUCT PROMPT and TEXT PROMPT becomes relatively smaller as the number of in-context demonstrations increases from Figure <ref type="figure" target="#fig_3">2</ref>. We argue that there may be two reasons: 1) it is challenging to select rep- resentative demonstrations; 2) it is non-trivial for LLMs to understand structure prediction tasks with more large output (relation) space. More case studies for GPT-3.5 can be found in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Utility of Generated Data from LLMs</head><p>Combining data generated from LLMs with original training data can yield better RE performance than from traditional data augmentation approaches. We compare data generation through the LLM with previous widely used data generation approaches, such as substituting words in training sets with WordNet's synonyms and contextual word embedding in Figure <ref type="figure" target="#fig_4">3</ref> (details in Appendix A.3). Data generation with LLMs can obtain better performance than all others, indicating guiding LLMs to generate data is an effective method to compensate for the lack of labeled data.</p><p>Using more and more generated data from LLMs can only boost RE performance to a certain extent, not continuously better. From Figure <ref type="figure" target="#fig_5">4</ref>, we observe that with more generated data, the result climbs up first and then declines, and is always higher than without generated data. We think low-quality generated data introduces much noise in the training course, according to the analysis on generated data in Appendix B.2, and LMs may have an anti-noise capacity <ref type="bibr" target="#b25">(Song et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusion</head><p>In this paper, we take the first step to investigate how to utilize the large language model for few-shot relation extraction. We observe that task-related information, including instructions or schemas, helps to elicit the capability of LLMs and boost few-shot relation extraction performance. At this stage, using LLMs to generate data may be a simple yet effective solution to enhance the power of foundation models (relatively small PLMs) for practical applications. We hope this work can deliver the benefits of using LLMs for the NLP community. Note that LLMs can make predictions only based on contexts combined with a few training examples as demonstrations. We argue that it has the potential to design sophisticated human-readable prompts for scenario-adaptable (e.g., low-shot and any domains) relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Despite our best efforts, there may still be some limitations remaining in this paper.</p><p>LLMs: Due to the limited budgets, we can not afford all kinds of LLMs, so we only evaluate GPT-3.5 (text-davinci-003). We will try to investigate relation extraction with more LLMs, such as OPT <ref type="bibr" target="#b36">(Zhang et al., 2022)</ref>, GLM-130B <ref type="bibr" target="#b35">(Zeng et al., 2022)</ref>, or code language models <ref type="bibr" target="#b2">(Bi et al., 2023)</ref> like Codex.</p><p>Other Methods to utilize LLMs: There are several other techniques to leverage LLMs, such as black-box optimization <ref type="bibr" target="#b27">(Sun et al., 2022)</ref> and feature-based learning <ref type="bibr" target="#b12">(Lang et al., 2022)</ref>; however, we find that most of those approaches cannot directly be applied to relation extraction due to the large label space and complex schema structures. We leave these for future work to leverage other methods with LLMs for relation extraction.</p><p>Datasets: We only evaluate four relation extraction datasets and will try to investigate relation extraction performance with LLMs on more diverse datasets across different domains and languages.</p><p>A Experimental Details</p><p>A.1 Datasets TACRED<ref type="foot" target="#foot_1">foot_1</ref> is a widely used RE dataset. It has 42 relation labels, including no_relation, meaning no relation is found. TACREV<ref type="foot" target="#foot_2">foot_2</ref> includes the same training set and relabeled development and test sets from TACRED. RE-TACRED<ref type="foot" target="#foot_3">foot_3</ref> is a re-annotated version of TACRED with 40 relations. SciERC<ref type="foot" target="#foot_4">foot_4</ref> has seven relation categories and is constructed in the scientific domain. All datasets are derived from their official webs without modification, including contents and train/test/dev splits. A.2 Baselines We compare LLMs with recent baseline methods using relatively small models. 1) Normal finetuning methods: SpanBERT (Joshi et al., 2020), a span-based PLM; LUKE (Yamada et al., 2020), pre-trained contextualized representations of words and entities based on the bidirectional transformer; GDPNet, a gaussian dynamic time warping pooling net able to select important words for relation prediction; TYP Marker (Zhou and Chen, 2022), fine-tuning with entity typed markers. 2) Generative method: TANL (Paolini et al., 2021), framing a structured prediction language task as a translation task between augmented natural languages. 3) Prompt-tuning methods: KnowPrompt, knowledge-aware continuous prompt-based tuning with synergistic optimization. A.3 Implementation Details Generated data with existing training data is then evaluated on KnowPrompt. Data augmentation methods with Word-Net's synonyms and contextual word embedding are achieved by nlpaug<ref type="foot" target="#foot_5">foot_5</ref> . The parameter temperature in OpenAI API is set to 0 for precision in ICL and 1 for generating diverse RE data. One NVIDIA GeForce RTX 3090 GPU with 24GB memory is employed to run all experiments. We rerun the official code of baselines with their original settings except on the SciERC dataset. Due to the vertical domain of SciERC, SciBERT (Beltagy et al., 2019) is used in TYP Marker and KnowPrompt for fairness. And for another three datasets, RoBERTa-large is utilized in TYP Marker and KnowPrompt. B Case Analysis B.1 Wrong Cases from ICL From Table 4, we notice that some RE instances are challenging for LLMs, and there are several limitations with LLMs: 1) LLMs are not good at clearly distinguishing the order between head and tail entities. 2) The same mention of head and tail entities will confuse LLMs. 4) If the distance between head and tail entities in the context is long, it is difficult for LLMs to decide the relation correctly. 5) Semantically-similar relation label words and entity mentions will puzzle LLMs because their embeddings are similar. 6) LLMs cannot afford very long instances since there is a large label space for relation extraction. 7) LLMs may mostly fail to extract those ambitious or wrongly labeled relations; those are also challenging for humans. More highquality demonstrations may help mitigate these issues. And we think it is necessary to develop step-by-step (Chat-style) approaches with LLMs to extract limited relations in one stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Generated Data from LLMs</head><p>There are some cases for generated data from GPT-3.5 in Table <ref type="table">5</ref>. Through human checks on 100 generated samples per dataset, about 78% generated data are corrected labeled and of a high quality (85% for TACRED, 82.5% for TACREV, 72% for RE-TACRED, 75% for SciERC). Meanwhile, we add generated data and original gold training data respectively to 8-shot datasets and fine-tune Know-Prompt, we evaluate the quality of generated data as shown in Table <ref type="table" target="#tab_3">3</ref>. We observe that labeled data generated by GPT-3.5 are mostly correct. As for TACRED and TACREV, generated data achieve more improvements than gold labeled data. Since there are many incorrect labeled data in TACRED and TACREV <ref type="bibr" target="#b37">(Zhang et al., 2017;</ref><ref type="bibr" target="#b0">Alt et al., 2020)</ref>, we think better performance results from GPT-3.5's help. However, we also find that Some generated data from GPT-3.5 are of less quality than gold data.</p><p>As for RE-TACRED and SciERC, using more gold data perform better than generated data. Through human checks, some generated samples are too short and concatenated by some semantically irrelevant sentences. Meanwhile, big performance's difference on SciERC shows GPT-3.5 is not good at vertical domains such as science.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>There are candidate relations: [RELATION List]. Context: TEXT. The relation between (HEAD TYPE) 'HEAD ENTITY' and (TAIL TYPE) 'TAIL ENTITY' in the context is RELATION. Context: TEXT. The relation between (HEAD TYPE) 'HEAD ENTITY' and (TAIL TYPE) 'TAIL ENTITY' in the context is Given a context, a pair of head and tail entities in the context, decide the relationship between the head and tail entities from candidate relations: [RELATION List]. Context: TEXT. The relation between (HEAD TYPE) 'HEAD ENTITY' and (TAIL TYPE) 'TAIL ENTITY' in the context is RELATION. Context: TEXT. The relation between (HEAD TYPE) 'HEAD ENTITY' and (TAIL TYPE) 'TAIL ENTITY' in the context is Task-related Instruction INSTRUCT PROMPT One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context and their entity types. The head entity has the relation with the tail entity and entities are pre-categorized as the following types: [ENTITY TYPE List]. Here are some samples for relation 'RELATION': Relation: RELATION. Context: TEXT. Head Type: HEAD TYPE. Head Entity: HEAD ENTITY. Tail Type: TAIL TYPE. Tail Entity: TAIL ENTITY. Generate more samples like above for the relation 'RELATION'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Strategies to unleash the power of LLMs for few-shot relation. HEAD TYPE and TAIL TYPE are schemas. HEAD ENTITY and TAIL ENTITY are entity mentions. RELATION refers the verbalized relation label words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>prompt to elicit comprehension of the relation extraction task from LLMs. To this end, specific and compelling prompts for RE with demonstrations are manually constructed and designed to instruct LLMs to understand the relation extraction task and how to execute relation extraction. Considering aspects and characteristics of the relation extraction task, including task definition, candidate relation (label) words, entity types (schemas) and so on, we design prompts of different articulation and complexity to investigate how prompts help LLMs release the power of few-shot RE. First, TEXT PROMPT only contains essential elements for RE, including relation categories, contexts, and corresponding head and tail entities. Inspired by the fantastic performance of InstructGPT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Micro F1 (%) of k in-context demonstrations in SciERC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of data generation with LLMs and different data augmentation methods. Roberta and SciBERT are used on RE-TACRED and SciERC, respectively, in the context embedding-based DA method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Micro F1 (%) of KnowPrompt with generated training data and original 8-shot data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>x, h, t, t h , t t and y) of one RE training instance and show few-shot instances as demonstrations to teach LLMs to comprehend Micro F1 (%) of few-shot performance. † refers to the performance with one-shot demonstrations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TACRED</cell><cell cols="2">TACREV</cell><cell>RE-TACRED</cell><cell>SciERC</cell></row><row><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell cols="4">K=8 K=16 K=8 K=16 K=8 K=16 K=8 K=16</cell></row><row><cell></cell><cell cols="3">SpanBERT (Joshi et al., 2020)</cell><cell></cell><cell>8.4</cell><cell>17.5</cell><cell>5.2</cell><cell>5.7</cell><cell>14.2</cell><cell>29.3</cell><cell>29.0 38.7</cell></row><row><cell>Baselines</cell><cell cols="8">LUKE (Yamada et al., 2020) GDPNet (Xue et al., 2021) TANL (Paolini et al., 2021) TYP Marker (Zhou and Chen, 2022) 26.5 29.9 26.7 29.5 44.8 9.5 21.5 9.8 22.0 14.1 11.8 22.5 8.3 20.8 18.8 18.1 27.6 18.6 28.8 26.7</cell><cell>37.5 48.0 50.4 54.1</cell><cell>33.2 48.9 33.5 42.3 32.4 38.7 50.4 59.0</cell></row><row><cell></cell><cell cols="4">KnowPrompt (Chen et al., 2022)</cell><cell cols="4">29.4 32.1 29.8 34.1 56.1</cell><cell>61.4</cell><cell>50.2 57.1</cell></row><row><cell></cell><cell cols="3">In-context Learning †</cell><cell></cell><cell cols="2">31.9</cell><cell></cell><cell>32.4</cell><cell>49.9</cell><cell>46.6</cell></row><row><cell>GPT3</cell><cell cols="4">In-context Learning †(w/ Instruction) Data Generation (TYP Marker)</cell><cell cols="4">31.0 35.8 36.6 36.7 36.5 58.4 31.9</cell><cell>51.8 60.6</cell><cell>48.8 63.2 64.3</cell></row><row><cell></cell><cell cols="4">Data Generation (KnowPrompt)</cell><cell cols="4">37.9 37.4 42.6 41.0 62.7</cell><cell>66.2</cell><cell>58.6 67.8</cell></row><row><cell cols="2">Prompts</cell><cell cols="4">TACRED TACREV RE-TACRED SciERC</cell><cell></cell><cell></cell></row><row><cell cols="2">TEXT</cell><cell>31.9</cell><cell>32.4</cell><cell>49.9</cell><cell>46.6</cell><cell></cell><cell></cell></row><row><cell cols="2">TEXT + Schema</cell><cell>36.9</cell><cell>37.7</cell><cell>54.3</cell><cell>45.9</cell><cell></cell><cell></cell></row><row><cell cols="2">INSTRUCT</cell><cell>31.0</cell><cell>31.9</cell><cell>51.8</cell><cell>48.8</cell><cell></cell><cell></cell></row><row><cell cols="2">INSTRUCT + Schema</cell><cell>38.3</cell><cell>36.7</cell><cell>58.5</cell><cell>50.2</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Micro F1 (%) of performance on different prompt: TEXT PROMPT and INSTRUCT PROMPT.As for in-context learning, because GPT-3.5 has the limitation of maximum request tokens (4097 tokens) and the series of TACRED datasets have more than 40 relations, one-shot demonstrations can only be used, and the one-shot performance is reported in Table1. For the same reason, to generate more labeled data for each relation independently, only three demonstrations for the relation are added to the prompts.</figDesc><table><row><cell>4 Experimental Setups</cell></row><row><cell>4.1 Methods and Datasets</cell></row><row><cell>GPT-3.5 is utilized via OpenAI API 2 as the large</cell></row><row><cell>language model in our experiments. We implement</cell></row><row><cell>experiments on four relation extraction datasets,</cell></row><row><cell>including TACRED (Zhang et al., 2017), TACREV</cell></row><row><cell>(Alt et al., 2020), RE-TACRED (Stoica et al., 2021)</cell></row><row><cell>and SciERC (Luan et al., 2018). Compared with</cell></row><row><cell>the LLM, six baselines methods are conducted via</cell></row><row><cell>relatively small models (details in Appendix A).</cell></row><row><cell>4.2 Few-shot Settings</cell></row></table><note><p>features of labeled RE data. Note that schemas, such as types of relations and entities, are significant structural information in RE data. Therefore, we propose schema-constrained data generation by adding entity types as schema guidance to the prompt (in Figure1</p><p>) to boost performance. Then, the prompt is utilized to guide LLMs to create augmented relation extraction data that are converted into the expected format for future usage. K instances per relation (K-shot) are sampled for training and validation. For all baselines, we use randomly sampled 8-shot and 16-shot datasets for training and validation.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Micro F1 (%) of KnowPrompt after adding labeled data generated by GPT-3.5 or gold labeled data to 8-shot datasets.</figDesc><table><row><cell></cell><cell cols="2">TACRED</cell><cell cols="2">TACREV</cell><cell cols="2">RE-TACRED</cell><cell cols="2">SciERC</cell></row><row><cell>8-shot Dataset</cell><cell cols="8">generated gold generated gold generated gold generated gold</cell></row><row><cell>add 0-shot</cell><cell>29.35</cell><cell>29.35</cell><cell>29.77</cell><cell>29.77</cell><cell>56.05</cell><cell>56.05</cell><cell>45.80</cell><cell>45.80</cell></row><row><cell>add 8-shot</cell><cell>31.63</cell><cell>30.73</cell><cell>34.30</cell><cell>33.16</cell><cell>59.85</cell><cell>60.92</cell><cell>48.30</cell><cell>57.08</cell></row><row><cell>add 16-shot</cell><cell>34.78</cell><cell>31.88</cell><cell>36.33</cell><cell>33.49</cell><cell>59.59</cell><cell>61.30</cell><cell>58.62</cell><cell>65.15</cell></row><row><cell>add 32-shot</cell><cell>36.45</cell><cell>33.35</cell><cell>38.19</cell><cell>33.98</cell><cell>60.06</cell><cell>64.65</cell><cell>57.70</cell><cell>72.11</cell></row><row><cell>add 48-shot</cell><cell>37.89</cell><cell>33.97</cell><cell>38.80</cell><cell>35.06</cell><cell>62.67</cell><cell>65.56</cell><cell>51.64</cell><cell>74.29</cell></row><row><cell>add 64-shot</cell><cell>36.67</cell><cell>34.36</cell><cell>42.61</cell><cell>35.57</cell><cell>61.07</cell><cell>67.28</cell><cell>54.52</cell><cell>75.36</cell></row><row><cell>add 72-shot</cell><cell>35.69</cell><cell>34.58</cell><cell>41.72</cell><cell>35.96</cell><cell>59.09</cell><cell>67.43</cell><cell>49.59</cell><cell>75.87</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://platform.openai.com/docs/models/ gpt-3-5</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://nlp.stanford.edu/projects/tacred/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/DFKI-NLP/tacrev</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/gstoica27/Re-TACRED</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>http://nlp.cs.washington.edu/sciIE/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://github.com/makcedward/nlpaug</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We would like to express gratitude to the anonymous reviewers for their kind comments. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">62206246</rs>), <rs type="funder">Zhejiang Provincial Natural Science Foundation of China</rs> (No. <rs type="grantNumber">LGG22F030011</rs>), <rs type="funder">Ningbo Natural Science Foundation</rs> (<rs type="grantNumber">2021J190</rs>), and <rs type="funder">Yongjiang Talent Introduction Programme</rs> (<rs type="grantNumber">2021A-156-G</rs>), <rs type="funder">CAAI-Huawei MindSpore Open Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7M7tJTj">
					<idno type="grant-number">62206246</idno>
				</org>
				<org type="funding" xml:id="_kVdHzsb">
					<idno type="grant-number">LGG22F030011</idno>
				</org>
				<org type="funding" xml:id="_KuSzAWt">
					<idno type="grant-number">2021J190</idno>
				</org>
				<org type="funding" xml:id="_jp8nWXC">
					<idno type="grant-number">2021A-156-G</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case</head><p>Gold Relation In-context Learning TACRED Context: And strangely enough , Cain's short , three-year tenure at the NRA is evidently the only period in his decades-long career during which he 's alleged to have been a sexual predator. Head Type: ORGANIZATION. Head Entity: NRA.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tacred revisited: A thorough evaluation of the tacred relation extraction task</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Gabryszak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SciB-ERT: A pretrained language model for scientific text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Codekgc: Code language model for generative knowledge graph construction</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinuo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.09048</idno>
		<idno>CoRR, abs/2304.09048</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards realistic few-shot relation extraction</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sichao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Benton</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.433</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5338" to="5345" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowprompt: Knowledgeaware prompt-tuning with synergistic optimization for relation extraction</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.00234</idno>
		<idno>abs/2301.00234</idno>
	</analytic>
	<monogr>
		<title level="m">A survey for in-context learning</title>
		<imprint>
			<date type="published" when="2022">2022. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>WWW &apos;22: The ACM Web Qingxiu</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring task difficulty for few-shot relation extraction</title>
		<author>
			<persName><forename type="first">Jiale</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.204</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="2605" to="2616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative prompt tuning for relation classification</title>
		<author>
			<persName><forename type="first">Jiale</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengkun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3170" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Opennre: An open and extensible toolkit for neural relation extraction</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-3029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
	<note>-System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">2021b. PTR: prompt tuning with rules for text classification</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/2105.11259</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-training improves prompt-based learning for large language models</title>
		<author>
			<persName><forename type="first">Hunter</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Sontag</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">2022. July 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="11985" to="12003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692,abs/1907.11692</idno>
		<title level="m">Roberta: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno>abs/2104.08786</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreferencefor scientific knowledge graph construction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process</title>
		<meeting>Conf. Empirical Methods Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relation classification with entity type restriction</title>
		<author>
			<persName><forename type="first">Shengfei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanhuan</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.34</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="page" from="390" to="395" />
		</imprint>
	</monogr>
	<note>ACL/IJCNLP 2021 of Findings of ACL Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Large language model is not a good few-shot information extractor, but a good reranker for hard samples! CoRR</title>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongching</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.08559</idno>
		<idno>abs/2303.08559</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Chatgpt: Optimizing language models for dialogue</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/chatgpt/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<idno>abs/2303.08774</idno>
		<title level="m">OpenAI. 2023a. Gpt-4 technical report</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="https://platform.openai.com/docs/models/text-davinci-003" />
		<title level="m">OpenAI. 2023b. Text-davinci-003</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.02155</idno>
		<idno>abs/2203.02155</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cícero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy; Papers</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/2007.08199</idno>
		<title level="m">Learning from noisy labels with deep neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Re-tacred: Addressing shortcomings of the TACRED dataset</title>
		<author>
			<persName><forename type="first">George</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="13843" to="13850" />
		</imprint>
	</monogr>
	<note>Emmanouil Antonios Platanios, and Barnabás Póczos</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service</title>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Qian</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07-23">17-23 July 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="20841" to="20855" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Ross</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elvis</forename><surname>Saravia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Stojnic</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.09085</idno>
		<idno>abs/2211.09085</idno>
		<title level="m">Galactica: A large language model for science</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Want to reduce labeling cost? GPT-3 can help</title>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.354</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">2021. 16-20 November, 2021</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gdpnet: Refining latent multi-view graph for relation extraction</title>
		<author>
			<persName><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eng</forename><surname>Siong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chng</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02">2021. February 2-9, 2021</date>
			<biblScope unit="page" from="14194" to="14202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LUKE: deep contextualized entity representations with entity-aware self-attention</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. 2020. November 16-20, 2020</date>
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Entity conceptenhanced few-shot relation extraction</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanglin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="987" to="991" />
		</imprint>
	</monogr>
	<note>Short Papers), Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative knowledge graph construction: A review</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gpt3mix: Leveraging large-scale language models for text augmentation</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongju</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woo-Myoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.192</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting><address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">2021. 16-20 November, 2021</date>
			<biblScope unit="page" from="2225" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">GLM-130B: an open bilingual pre-trained model</title>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Lam Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.02414</idno>
		<idno>CoRR, abs/2210.02414</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">OPT: open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.01068</idno>
		<idno>CoRR, abs/2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An improved baseline for sentence-level relation extraction</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, AACL/IJCNLP 2022</title>
		<meeting>the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, AACL/IJCNLP 2022</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-11-20">2022. November 20-23, 2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
	<note>Short Papers, Online only</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
