<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LLaMA-Gene: A General-purpose Gene Task Large Language Model Based on Instruction Fine-tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Wang</forename><surname>Liang</surname></persName>
							<email>wangliang.f@gmail.com</email>
						</author>
						<title level="a" type="main">LLaMA-Gene: A General-purpose Gene Task Large Language Model Based on Instruction Fine-tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">34B640D52ECB4DED4CA2E8BD47648F6C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Building a general-purpose task model similar to ChatGPT has been an important research direction for gene large language models. Instruction fine-tuning is a key component in building ChatGPT, but existing instructions are primarily based on natural language. Natural language and gene sequences have significant differences in tokenization and encoding. Therefore, constructing a multilingual model that can handle both natural language and gene sequences is crucial for solving this problem.In this paper, we expand the capabilities of the LLaMA large language model to include gene language. This involves expanding the vocabulary using the Byte Pair Encoding (BPE) method, specifically tailored for DNA and protein sequences, and conducting further pre-training on these sequences. We then convert various downstream gene task data into a unified format for instruction fine-tuning and further fine-tune the model on this data.Our study demonstrates that a mixed model of gene and natural language, fine-tuned with instructions, achieves results comparable to the current state-of-the-art (SOTA) in tasks such as gene classification and gene sequence interaction. This provides a promising direction for building a unified large language model for gene tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">introduction</head><p>Large language models have revolutionized the field of artificial intelligence. These models have also found significant applications in the analysis of DNA and protein sequences. For tasks centered around nucleic acids, models such as DNABert2, HyenaDNA, and ScBert have been developed. These studies primarily address challenges related to the classification and structure prediction of DNA sequences(1~4). Similarly, in the domain of protein-related tasks, models like ProTrans, ProteinBERT, and ESM2 have been introduced. These models excel in applications such as structure prediction and functional annotation of proteins(5~9).</p><p>However, current research on gene large models has not yet achieved the evolution from GPT to ChatGPT. Most studies still involve fine-tuning pre-trained models for different types of tasks, resulting in various sub-models to solve specific problems. Current gene large models are not yet capable of using a unified chat-based approach to address different types of tasks, as ChatGPT does.</p><p>Currently, some studies are exploring methods to construct a unified gene task model by introducing specific markers. DNAGPT adds more types of tokens to the pre-training input sequences to represent concepts such as "categories" and "numerical values." For instance, &lt;A&gt; represents a positive example, and &lt;N&gt; represents a negative example. ESM3 introduced 256 functional keyword tokens (13), while LucaOne utilized protein annotation information. Studies</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>like BioMedGPT, InstructProtein and ProtSt leverage large amounts of biological literature and annotated protein sequences for unified large model training and fine-tuning <ref type="bibr" target="#b13">(15,</ref><ref type="bibr" target="#b14">16)</ref>.</p><p>These studies have achieved impressive results, but there is still a gap to bridge in building a unified large model for gene tasks.</p><p>Challenge: Firstly, in terms of sequence processing, these models employ different tokenization and encoding methods for various sequences. When designing downstream tasks, it is necessary to specifically consider these rules, making it difficult for different models to collaborate effectively.</p><p>To distinguish between sequences, specific sequence markers are required, such as representing protein sequences as &lt;protein&gt;ACDEFGHIKLMNPQRSTVWY&lt;/protein&gt;, and using different markers for DNA or structural data. There is also uncertainty about whether these specific markers will be recognized by large models.</p><p>Furthermore, different models have their own specific application ranges. For example, models like LucaOne and ESM3 primarily address downstream tasks such as gene sequence classification.</p><p>BioMedGPT, on the other hand, focuses on biological question answering, literature summarization, and protein function prediction based on natural language. These studies have not yet reached the level of versatility seen in models like ChatGPT, which can handle a wide range of tasks including classification, summarization, reasoning, and computation.</p><p>Additionally, the parameter scales of current gene sequence models are generally smaller compared to mainstream large language models, typically below 1 billion parameters. This limitation makes it difficult for these models to exhibit emergent properties such as knowledge emergence and capability transfer. Currently, they are mostly limited to traditional sequence classification tasks and cannot fully leverage the advanced reasoning capabilities of larger models.</p><p>To address the above challenges, we first used the BPE (Byte Pair Encoding) method to perform uniform tokenization and encoding for natural language, DNA sequences, and protein sequences, without the need to introduce specific sequence markers. Then, we conducted continuous pre-training on the 7B-scale LLaMA large model using DNA and protein sequences. Further, we converted gene sequence-related classification and other data into unified instruction data and performed additional instruction tuning on this mixed-language model. The trained model demonstrates good performance in various downstream gene tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of llama-gene</head><p>The llama-gene large model is built upon the LLaMA foundation and is obtained through continuous pre-training and instruction tuning using DNA and protein data.</p><p>Since the original llama model has been trained primarily on English natural language text, we expanded the vocabulary and conducted further pre-training using DNA and protein sequences.</p><p>On this basis, we converted various gene sequence tasks, including protein and DNA classification tasks, protein-protein interaction tasks, and protein-DNA correlation tasks, into consistent instruction data using different prompt templates. We then used this instruction data to fine-tune the pre-trained model, resulting in the llama-gene model. The process is illustrated in the following diagram: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training datasets</head><p>The training data for LLaMA-Gene is roughly outlined in the table below:</p><p>Table.1 Training data period datasets data size Data type BPE training multiple model organism genomes 1G DNA BPE training UniProt [multispecies] 1G Protein sequence Continue training multiple model organism genomes 16G DNA Continue training Swiss-Prot/TrEMBL 16G Protein sequence Continue training s2orc/biology paper 16G Natural Language Instruction finetune GLUE/convert 100M DNA downstream Instruction finetune lucaone/convert 100M Protein downstream Instruction finetune UniProt function 300M Protein translation 2.2.1 BPE tokenlizer training datasets For training the new BPE (Byte Pair Encoding) vocabulary, we use DNA sequences and protein sequences. The specific data used for this purpose is consistent with the data used during continuous pre-training</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">continues pre-training datasets</head><p>The training data for LLaMA-Gene is structured as follows:</p><p>DNA Sequence Data:We followed the pre-training data approach used by DNABert, extracting fragments of 300 to 1000 base pairs (bp) from multiple model organisms. The total data volume for DNA sequences is approximately 16 GB.</p><p>Protein Sequence Data:From the UniProt database, we extracted 16 GB of sequence data, including all data from Swiss-Prot and randomly selected data from TrEMBL.</p><p>Natural Language Data: During continuous pre-training, it is generally necessary to mix in some of the pre-training data to prevent catastrophic forgetting. We used biological research papers and other relevant texts as mixed corpora for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Instruction Fine-tuning dataset</head><p>To adapt different downstream tasks into a unified instruction format, we construct standardized instruction data and then perform fine-tuning training. This method is also an important research approach in large natural language models, where the basic idea is to design prompt templates based on the characteristics of tasks such as classification, part-of-speech analysis, translation, etc., converting them into standard instruction fine-tuning data. We also follow this approach and use similar prompt templates to convert gene sequence-related downstream tasks into instruction data.</p><p>Fineture training data consists of instructions, inputs, and outputs. These three components are then formatted into a specific prompt format. Below are two common methods of formatting.Fig. <ref type="figure">5</ref>.</p><p>In this article, we will use the Alpaca-style prompt formatting method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.1">Classification task</head><p>For classification tasks, we have referenced templates used in natural language processing (NLP) to design our specific templates. The approach is detailed as follows:</p><p>Table.2 examples of a classification task Task name sequence sequence type label name label promoter detection CATGCGGGTCGATAT... DNA Non-promoter 0 promoter detection CTGAATCTCTCAGCC... DNA promoter 1 For classification tasks, a template like the following can generally be used to construct instructions： {'instruction': 'Determine the {task_name} of following {sequence_type} sequence, The result will be one of the following: {label_name1}, {label_name2}, {...}', 'input': '.{sequence}', 'output': '{label_name}'} An example: {'instruction': 'Determine core promoter detection of following dna sequence, The result will be one of the following: Non-promoter, promoter.', 'input': 'CATGCGGGTCG...', 'output': 'Non-promoter'} 2.2.2.2 Structure prediction task For structure prediction tasks, we first convert them into general classification tasks and then adopt the classification prompt templates: Table3. gene structure prediction task Task name sequence sequence type secondary structure Secondary structure prediction MKQDKGLFDLVT protein HHHCCCCEEECCC</p><p>To predict the secondary structure of a token, we use a sequence that includes this token and the preceding 30 base pairs (bp) as input. The output is the secondary structure of the last token in this sequence. Since the tokenization of sequences does not perfectly correspond to their secondary structures, it is necessary to align gene tokens with secondary structure sequences and determine the type of secondary structure for each token. Given that one token may correspond to multiple secondary structures, we apply a simple rule: assign the most frequent secondary structure to the token. If there is a tie in frequency, we prioritize the structures in the order H (alpha helix), E (beta sheet), C (random coil).</p><p>Table.4 Gene structure prediction task Task name sequence sequence type secondary structure secondary structure prediction MKQDKGLFDLVT protein random coil Template for structure prediction: {'instruction': 'Determine the {task_name} of following {sequence_type} sequence, The result will be one of the following: {structure_name1}, {structure_name2}, {...}', 'input': '.{sequence}', 'output': '{structure_name}'} An example: {'instruction': 'Determine the secondary structure prediction of following protein sequence, The result will be one of the following: alpha helix, beta sheet, random coil', 'input': '.MKQDKGLFDLVT', 'output': 'random coil'}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.3">Multiple sequence analysis task</head><p>For the multiple sequence association analysis task, we adopt the following prompt template to guide the model in identifying and analyzing associations between multiple sequences. This approach ensures that the model can effectively interpret the relationships and interactions among different gene sequences.</p><p>Table.5 multi sequence task Task name sequence1 sequence1 type seque nce2 sequence 2 type label label name Conform to the Central Dogma ACCAGTGCTCAGGT TAACAAAAT... DNA MAS GRL protein 1 Not Confo rm prediction QLL AFAL .. Template for multiple sequence: {'instruction': 'Determine the {task_name} of following {sequence1_type} sequence and {sequence2_type} sequence , The result will be one of the following: {label_name1}, {label_name2}, ...', 'input': ' {sequence1_type} sequence: {sequence1} \n###\n{sequence2_type} sequence: {sequence2} ', 'output': '{label_name}'} Example: {'instruction': 'Determine the Conform to the Central Dogma prediction of following DNA sequence and Protein sequence , The result will be one of the following: Conform, Not Conform', 'input': ' DNA sequence: ACCAGTGCTCAG... \n###\n protein sequence: MASGRLQLLAFAL.. ', 'output': 'Not Conform'}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.4">Function prediction</head><p>For the function prediction task, we have referenced translation templates used in natural language processing (NLP) to design our specific template. The approach is detailed as follows:</p><p>Table6. gene function prediction task Task name sequence sequence type function gene function prediction GTTCTGCCCATAACA... DNA D-glucose transmembrane transport(GO:1904659) Template for function prediction: {'instruction': 'Determine {task_name} of following {sequence_type} sequence.', 'input': '{sequence}', 'output': '{function}'} An example: {'instruction': 'Determine gene function prediction of following dna sequence.', 'input': 'GTTCTGCCCATAACA...', 'output': 'D-glucose transmembrane transport(GO:1904659)'}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.5">Regression task</head><p>Large language models are generally not well-suited for handling regression tasks directly.</p><p>Therefore, a common approach is to use a function call format to delegate the task to smaller, specialized models or scripts. Alternatively, regression problems can be approximated by converting them into classification tasks. For instance, you can transform a regression problem into a binary classification problem by categorizing values above the median as "high" and those below the median as "low." This method can also be extended to more categories if needed.</p><p>Below is an example using binary classification:</p><p>Table7. gene regression task Task name sequence sequence type value gene expression level prediction GTTCTGCCCATAACA... DNA 0.75 Since the regression predictions have already been normalized, we categorize the predictions as follows: For values less than 0.5: Label: 0 Name: low For values greater than or equal to 0.5: Label: 1 Name: high Table8. gene function prediction task Task name sequence sequence type label label name gene expression level prediction GTTCTGCCCATAACA... DNA 1 high Template for Regression task: {'instruction': 'Determine the {task_name} of following {sequence_type} sequence, The result will be one of the following: {label_name1}, {label_name2}, {...}', 'input': '.{sequence}', 'output': '{label_name}'} Example: {'instruction': 'Determine the gene expression level prediction of following DNA sequence, The result will be one of the following: low, high', 'input': '.GTTCTGCCCATAACA...', 'output': 'high'} 2.3 Training Strategy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.1Tokenization</head><p>We trained the vocabulary for DNA and protein sequences using BPE, then merged them into the original LLaMA vocabulary (32,000 tokens). Both the DNA and protein vocabularies consist of 30,000 words, resulting in a final vocabulary size of approximately 91,000 words.</p><p>To train this new mixed vocabulary, we adopted a two-step process to facilitate the validation of different sequence combinations' effects. First, we trained the DNA vocabulary and merged it into</p><p>LLaMA. Then, we trained the protein vocabulary and added it to the model from the first step.</p><p>It's important to note that the training method used for LLaMA3 differs slightly from previous versions. Specifically, LLaMA3 uses a vocabulary trained with tiktoken, whereas earlier versions utilized the SentencePiece framework. When merging or working with these models, it is crucial to pay attention to the format of different frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Continue Pre-training</head><p>Due to the large number of parameters in LLaMA models, ranging from 7B to 405B, performing full-parameter continuous pre-training would be very costly. Therefore, we adopted the LoRA method for training, specifically using the PEFT framework from Hugging Face. The parameter settings were referenced from the configuration of chinese-llama.</p><p>The specific network layers for training are as follows:</p><p>lora_trainable Parameters lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"</p><p> q_proj: Query projection layer, used to generate query vectors in the attention mechanism.</p><p> v_proj: Value projection layer, used to generate value vectors in the attention mechanism.</p><p> k_proj: Key projection layer, used to generate key vectors in the attention mechanism.</p><p> o_proj: Output projection layer, used to process the output after the attention mechanism.</p><p> gate_proj: Gate projection layer, possibly used to control information flow, common in some variant models.</p><p> down_proj: Down-projection layer, possibly used to reduce feature dimensions.</p><p> up_proj: Up-projection layer, possibly used to increase feature dimensions.</p><p>These layers are primarily related to the attention mechanism and feed-forward networks. By specifying these layers as trainable, we can focus on adjusting the parameters of these critical parts during fine-tuning, thereby improving efficiency while maintaining model performance.</p><p>The network layers to be saved are as follows:</p><p>modules_to_save Parameters modules_to_save="embed_tokens,lm_head"</p><p> embed_tokens: Token embedding layer, responsible for converting input tokens into vector form.</p><p> lm_head: Language model head, responsible for generating the final output from the hidden states of the model, such as predicting the probability distribution of the next word.</p><p>After fine-tuning, the states of these important modules can be correctly saved.</p><p>With this setup, approximately 10% of the parameters are being trained. On an 8-card L20 server, it takes about one week to complete training with 16GB of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Instruction Fine-tuning</head><p>The method for instruction tuning is entirely consistent with the training method of the llama pre-trained model. All use exactly the same causal language model head. This involves treating the constructed instruction data as general text sequences and inputting them into the llama model. In other words, instruction tuning is the same as the pre-training process, which is key to enabling multi-task handling. This is also key to how ChatGPT-like models are able to handle a variety of natural language tasks.</p><p>We typically train for 2 to 8 epochs. Since the training data is relatively small, fine-tuning on a L20 GPU*8 usually takes only about ten minutes to complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Evaluation</head><p>Because we have essentially converted various downstream tasks into chat-based tasks. The assessment is based on whether the model's output tokens semantically match the expected ones.</p><p>Therefore, We Use Accuracy (ACC) as the Primary Evaluation Metric.</p><p>For example, if the model outputs "promoter AGCC GGG" while the expected output is "promoter ", we still consider the model's prediction to be accurate. This is because, as a generative model, llama-gene is not specifically trained to recognize stop tokens. With more training data and a larger model size, it would be possible to produce outputs that exactly match the expected tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation results</head><p>The evaluation datasets primarily referenced include those from DNABert2, lucaone, InstructionProtein, and BiomedGPT. These datasets encompass a variety of gene-related downstream tasks.</p><p>The specific evaluation results for LLaMA-Gene across these datasets demonstrate its performance on various gene-related downstream tasks. These results highlight the model's capabilities in handling diverse and complex gene-related challenges.The evaluation results for LLaMA-Gene are detailed as follows:</p><p>Table.9. Accuracy of different tasks Task sequence type llama-gene SOTA classification DNA 0.83 0.84 classification protein 0.64 0.72 structure prediction DNA 0.81 0.85 structure prediction protein 0.73 0.85 multiple sequence DNA+DNA 0.66 0.78 multiple sequence protein+protein 0.63 0.87 multiple sequence DNA+protein 0.71 0.91 function prediction DNA 0.76 0.81 function prediction Protein 0.71 0.81 regression task DNA 0.83 0.87 regression task Protein 0.82 0.86</p><p>The evaluation involves 2 to 3 specific datasets per task, primarily for validating the new method, which may not be universally applicable.</p><p>Compared to current SOTA methods, LLaMA-Gene performs adequately on DNA-related tasks but shows a noticeable gap in protein-related tasks. However, this result validates the new method's effectiveness. The performance is mainly limited by computational resources;</p><p>LLaMA-Gene has been trained on relatively smaller datasets. Future research will use larger parameter scales and more extensive data to improve evaluation outcomes.</p><p>Currently, we use LLaMA base models from Generation 1 to 3, all with 7B or 8B parameters,</p><p>showing no significant differences in gene-related tasks. This is because newer LLaMA versions focus on improving fine-tuning effects, which have not been applied to LLaMA-Gene. Future studies will explore using fine-tuned LLaMA models to transfer natural language inference capabilities to gene-related tasks.</p><p>Our instruction fine-tuning design offers versatility, allowing one model to handle various gene downstream tasks. Most existing large gene models require training different model heads for specific tasks, whereas our approach uses a generalized model.</p><p>More importantly, the instruction-tuned model can interact with users in a conversational manner and can leverage prompt engineering, RAG, chain of thought, and other techniques commonly used with large language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Advancing gene large models from the GPT era to the ChatGPT era is a current hot topic in the field of biology. This paper constructs the llama-gene model based on the LLaMA series of large models, validating the feasibility of a ChatGPT-style gene large model.</p><p>To address the challenges of building a unified gene task model, this paper introduces several innovations. First, a unified BPE tokenizer and encoding scheme are used for different types of sequences, significantly reducing the complexity of constructing and applying network models.</p><p>Second, inspired by instruction construction methods in NLP research, we created instruction-tuning datasets for various downstream gene tasks. Based on these methods, we performed continuous pre-training and instruction tuning on multiple types of biological sequences using the LLaMA 7B model, ultimately constructing the llama-gene large model.</p><p>The llama-gene large model follows a similar construction pattern to ChatGPT, thus offering extensive application potential. For example, mature large model application frameworks like RAG (Retrieval-Augmented Generation) and Agent can be used to build comprehensive biological applications. These applications can integrate the strong logical reasoning capabilities of natural language with the understanding of biological sequences, enabling deep interpretation of biological sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig.1 llama-gene training process. This model uses genome data, protein data and English research paper as continues pre-training corpora, employing the BPE method for uniform encoding without distinguishing between the three types of text. It then undergoes continues training on the llama network, resulting in a pre-trained model. Following this, specific prompt templates are used to convert gene downstream tasks into instructions. These instructions are then formatted into the standard Alpaca format, generating instruction data for fine-tuning the pre-trained model. This process leads to the creation of llama-gene. The usage of this model can adopt a dialogue mode based on prompt engineering.</figDesc><graphic coords="3,146.16,149.16,303.12,245.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><figDesc>typical example is NATURAL INSTRUCTIONS, a dataset developed by Allen AI Institute and other institutions, which includes 61 different NLP task datasets. The core concept of NATURAL INSTRUCTIONS is to express various NLP tasks as natural language instructions. Each task comes with detailed task descriptions, positive and negative examples, and relevant input-outputpairs. This approach aims to enhance the model's ability to understand and execute diverse task instructions, enabling it to generalize better to new tasks. The dataset covers multiple task types, including text classification, question answering, summarization, text generation, and more.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>Fig.2 Build instruction datasets. For different downstream tasks, design specific instruction templates to convert downstream task data into instructions. Then, using the Alpaca prompt template, transform these instructions into uniformly formatted instruction fine-tuning data.</figDesc><graphic coords="5,110.16,149.16,374.88,240.84" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dnabert-2: Efficient foundation model and benchmark for multi-species</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="852" to="866" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prottrans: Toward understanding the language of life through selfsupervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="7112" to="7127" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Proteinbert: a universal deeplearning model of protein sequence and function</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2102" to="2110" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evolutionary-scale prediction of atomic-level protein structure with a language model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page" from="1123" to="1130" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DNABERT: Pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Davuluri</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btab083</idno>
		<idno>1093/bioinformatics/btab083</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2112" to="2120" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GENA-LM: A Family of Open-Source Foundational DNA Language Models for Long Sequences</title>
		<author>
			<persName><surname>Veniamin Fishman</surname></persName>
		</author>
		<idno type="DOI">10.1101/2023.06.12.544594</idno>
		<idno>06.12.544594</idno>
		<ptr target="https://doi.org/10.1101/2023.06.12.544594" />
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks</title>
		<author>
			<persName><forename type="first">Daoan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1101/2023.07.11.548628</idno>
		<idno>BioRxiv 2023.07.11.548628</idno>
		<ptr target="https://doi.org/10.1101/2023.07.11.548628" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pre-trained Language Models in Biomedical Domain: A Systematic Survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3611651</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1" to="52" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Foundation models for bioinformatics</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1002/qub2.69</idno>
	</analytic>
	<monogr>
		<title level="j">Quant. Biol</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Benegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Albors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2407.11435</idno>
		<title level="m">Genomic Language Models: Opportunities and Challenges</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simulating 500 million years of evolution with a language model</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<idno type="DOI">10.1101/2024.07.01.600583</idno>
		<idno>2024.07.01.600583</idno>
		<ptr target="https://doi.org/10.1101/2024.07.01.600583" />
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized Biological Foundation Model with Unified Nucleic Acid and Protein Language</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1101/2024.05.10.592927</idno>
		<idno>05.10.592927</idno>
		<ptr target="https://doi.org/10.1101/2024.05.10.592927" />
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A generalist vision-language foundation model for diverse biomedical tasks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adhikarla</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-024-03185-2</idno>
		<ptr target="https://doi.org/10.1038/s41591-024-03185-2" />
	</analytic>
	<monogr>
		<title level="j">Nat Med</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ProtST: multi-modality learning of protein sequences and biomedical texts</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Miret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning (ICML&apos;23)</title>
		<title level="s">JMLR.org, Article</title>
		<meeting>the 40th International Conference on Machine Learning (ICML&apos;23)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="38749" to="38767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">InstructProtein: Aligning Human and Protein Language via Knowledge Instruction</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1114" to="1136" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<ptr target="https://github.com/maris205/llama-gene" />
		<title level="m">Llama-gene github</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
