<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Performance of Large Language Models in a Computer Science Degree Program</title>
				<funder ref="#_xMjzCqU">
					<orgName type="full">Bundesministerium für Bildung und Forschung</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-07-24">24 Jul 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tim</forename><surname>Krüger</surname></persName>
							<email>tim.krueger@stud.hn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Niederrhein University of Applied Sciences</orgName>
								<address>
									<settlement>Krefeld</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Gref</surname></persName>
							<email>michael.gref@hs-niederrhein.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Niederrhein University of Applied Sciences</orgName>
								<address>
									<settlement>Krefeld</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Performance of Large Language Models in a Computer Science Degree Program</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-07-24">24 Jul 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">C6BCDCBB26A41AF0ECE510C130977184</idno>
					<idno type="arXiv">arXiv:2308.02432v1[cs.CY]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and dominate the current discourse. Their transformative capabilities have led to a paradigm shift in how we interact with and utilize (text-based) information. Each day, new possibilities to leverage the capabilities of these models emerge.</p><p>This paper presents findings on the performance of different large language models in a university of applied sciences' undergraduate computer science degree program. Our primary objective is to assess the effectiveness of these models within the curriculum by employing them as educational aids. By prompting the models with lecture material, exercise tasks, and past exams, we aim to evaluate their proficiency across different computer science domains.</p><p>We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10 tested modules, BingAI achieved 68.4%, and LLaMa, in the 65 billion parameter variant, 20%. Despite these convincing results, even GPT-4.0 would not pass the degree program -due to limitations in mathematical calculations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the realm of natural language processing, large language models, hereafter only referenced as LLMs, have now become an integral part of our digital landscape. They have a widespread influence in today's discourse and a ubiquitous presence in various fields and industries <ref type="bibr" target="#b16">[17]</ref>. Among these models, ChatGPT-3.5 and GPT-4.0 have emerged as prominent examples, captivating the attention of students, researchers, and developers alike.</p><p>It is essential to look at these models in the context of higher education because they provide new ways and possibilities to teach, learn and perceive information. Useful for both students and instructors. They could help students, for example, by delivering a more personalized and interactive educational experience and acting as a kind of "learning buddy." For an instructor, the possibilities are also plenty. These models can generate supplementary materials, explanations, or examples <ref type="bibr" target="#b4">[5]</ref>. Alternatively, they could aid in the assessment process by automating the grading procedure for all text-based requirements.</p><p>A lot of research is currently taking place on this topic. For example, H. Gimpel et al. <ref type="bibr" target="#b4">[5]</ref> have written an extensive essay on the opportunities but also the risks that generative AI models bring to higher education by collecting nearly 50 high-quality scholarly sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we want to explore some of the various research efforts that have examined the performance of LLMs in the field of computer science. The results, some of which differ significantly, inspired us to test the performance of these LLMs in our degree program as well. Table <ref type="table">1</ref> shows a small selection of test and exam results published by OpenAI <ref type="bibr" target="#b12">[13]</ref>, with the release of their GPT-4.0 model, and one exam (Algorithms and Data Structures) tested by Bordt et al. <ref type="bibr" target="#b1">[2]</ref>.</p><p>The first results are from LeetCode, a popular online platform that provides programming exercises and coding challenges commonly found in technical interviews <ref type="bibr" target="#b6">[7]</ref>. The platform is aimed at software developers and programmers to enhance their programming skills by Table <ref type="table">1</ref>: Exam results for GPT-3.5 and GPT-4.0 when tested by OpenAI <ref type="bibr" target="#b12">[13]</ref> and Bordt et al. <ref type="bibr" target="#b1">[2]</ref>. Values are rounded to the first decimal place.</p><p>Test / Exam GPT-3.5 GPT-4.0 LeetCode (Hard) 0.0% 6.6% LeetCode (Easy) 29.3% 75.6% Algorithms &amp; Data Structures 51.3% 60.0%</p><p>solving algorithmic problems <ref type="bibr" target="#b19">[20]</ref>. The programming exercises and algorithmic problems are divided into three difficulty ranges (easy, medium, and hard) <ref type="bibr" target="#b11">[12]</ref>.</p><p>In the easy problem section, GPT-3.5 answered 12 out of 41 questions correctly, resulting in a performance of 29.3%. GPT-4.0 answered 31 out of 41 questions correctly, resulting in a performance of 75.6% -an improvement of 47.3 percentage points <ref type="bibr" target="#b12">[13]</ref>. The results may depend on the exact category and programming language <ref type="bibr" target="#b11">[12]</ref>. Nikolaidis et al. found that in their case, ChatGPT-3.5 solved 45% of 50 randomly selected easy LeetCode problems correctly while providing noticeably better results in the programming languages Java and Python.</p><p>When tested by OpenAI, GPT-3.5 could not solve a single of the hard problems on LeetCode <ref type="bibr" target="#b12">[13]</ref>. These results again may depend on the type of problem that had to be solved <ref type="bibr" target="#b11">[12]</ref>. Nikolaidis et al. found that ChatGPT-3.5 solved 10 out of 21 hard problems correctly, resulting in 47.6% accuracy. ChatGPT-3.5 would then, in fact, even severely outperform GPT-4.0 when tested by OpenAI, which was able to solve 3 out of 45 hard problems correctly (6.6% accuracy) <ref type="bibr" target="#b12">[13]</ref>.</p><p>Bordt et al. tested ChatGPT-3.5 and GPT-4.0 on an undergraduate computer science exam in Algorithms and Data Structures. The exam was fed to the LLMs in the same way students would receive it. The answers of the models were transferred to paper by the testers and mixed with the solutions of the students <ref type="bibr" target="#b1">[2]</ref>. ChatGPT-3.5 scored 20.5 out of 40 possible points (51.25%), allowing it to pass the exam narrowly. GPT-4.0 improved that score by 8.75 percentage points, reaching 60% (24/40 points). With this result, GPT-4.0 outperforms the average student, which scores 23.9 in the mean <ref type="bibr" target="#b1">[2]</ref>.</p><p>Both ChatGPT-3.5 and GPT-4.0 indicate wide-ranging capabilities in the field of computer science. GPT-4.0 also seems to be an improvement over GPT-3.5 in every way. The findings on performance variation are worth noting for our research. The LLMs' answers seem to depend on the corresponding computer science discipline category and the specific programming language asked <ref type="bibr" target="#b11">[12]</ref>. It is also relevant to note that the programming errors generated seem to be mainly semantic. The models hardly make syntax errors, but the code, if wrong, can have serious logic errors <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The crux of our methodology is the evaluation of various LLMs by feeding them academic content drawn from a bachelor's degree program in computer science at a university of applied sciences. We aim to determine each model's overall performance and identify the highest and lowest-scoring modules, grade distributions, and potential affinities for certain topics. Additionally, the study aims to determine whether the models would complete the degree program.</p><p>Our data set comprised samples of past exams from ten different modules of the degree program, see Table <ref type="table" target="#tab_1">2</ref>. This core data set was complemented with information from questionnaires, practice exercises, and lecture notes to offer a more holistic view of the curricu- lum. For modules with oral exams, the questions were based on the same data but created in consultation with the supervising professor to simulate realistic exam scenarios. Only examinations for which the professors gave their approval were taken into account for the study.</p><p>The criteria for evaluation were adapted for each module. In written exams, we employed the evaluation system and point allocation provided by the supervising professor. In oral exams, we weighted questions according to complexity and difficulty. These questions were finalized in consultation with the supervising professors. Evaluating and assessing the performance involved verifying correctness, compiling and testing program code, and recalculating solutions.</p><p>Due to the limitations of certain models in handling multimedia input, we partly excluded those tasks from our assessment. Adjustments were made to the total score and weighting of the exam accordingly. In instances where it was feasible, we transformed such tasks into a suitable textual format with tables and data structures being converted to markdown and diagrams re-imagined into the corresponding UML representation.</p><p>The prompting of the models was a carefully considered aspect of this research project. Prompt engineering has been shown to improve the performance of models in various studies (e.g. <ref type="bibr" target="#b18">[19]</ref>). However, to provide a broad overview of the performance across the curriculum, we opted to prompt all models only once and use the first response provided by each model. Before starting the assessment, a generic pre-prompt was given in each case, setting the context that they were interacting in a simulated exam scenario and outlining expectations for responses.</p><p>I am now going to ask you a few questions from a hypothetical [insert topic or subject] exam of an undergraduate computer science degree program. I want you to answer the questions to the best of your knowledge and capabilities. Please answer briefly and concisely unless I explicitly ask for a more detailed answer! Please answer purely in continuous text or bullet points. If output in chart or table form is desired, I will let you know.</p><p>We tested ChatGPT-3.5, BingAI, which uses the GPT-4.0 foundation model, StableLM-Alpha in the 7 billion parameter version, and LLaMa in both the 7 billion and 65 billion parameter versions. Towards the end of our project, we also received access to GPT-4.0 but were restricted in using this model due to time constraints. We viewed these selections as an appropriate mix of open-and closedaccess LLMs.</p><p>StableLM includes various LLMs published by Stability AI. The size of these models ranges from 3 billion to 65 billion parameters. A 175 billion parameter variant is also planned <ref type="bibr" target="#b14">[15]</ref>. The models are published in different versions and trained on different datasets. We use the StableLM-Alpha-7B variant, which was trained on a dataset based on The Pile <ref type="bibr" target="#b2">[3]</ref>. All models are hosted on The Hugging Face Hub, and some are accessible through a web interface <ref type="bibr" target="#b15">[16]</ref>.</p><p>LLaMa refers to a collection of different LLMs ranging from 7B to 65B parameters, published by MetaAI <ref type="bibr" target="#b17">[18]</ref>. We used LLaMa with the project llama.cpp, an open-source C/C++ port of several LLMs <ref type="bibr" target="#b3">[4]</ref>. This project supports 8-bit, 5-bit, and 4-bit integer quantization, a technique that significantly reduces the memory requirements of the models. In the case of LLaMa, this allowed us to run the models in RAM instead of GPU memory. We considered this approach a more realistic simulation, as the models otherwise require a significant amount of GPU memory. However, there is the possibility of a degradation in model accuracy. There seems to be a trade-off between model size and quality, depending on the quantization method <ref type="bibr" target="#b20">[21]</ref>. In the case of LLaMa-7B, the file size got reduced from 13 GB when using 16-bit floats to 3.5 GB when using 4-bit integer quantization. The perplexity <ref type="bibr" target="#b5">[6]</ref> rose from 5.9066 to 6.1565, an increase of only 4.23% <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We tested ten modules each with ChatGPT-3.5 and BingAI, six modules with GPT-4.0, and fourteen modules in total with StableLM-Alpha-7B, LLaMa-7B, and LLaMa-65B, resulting in forty data points. We have not been able to test every model iteration on every module of the curriculum due to the time constraints of this project. However, the following data underpins what we present as a comprehensive insight into the performance of these models across an array of computer science curriculum modules.</p><p>Referring to grades in the following, we calculated them according to the modified Bavarian formula corresponding to the German grading system <ref type="bibr" target="#b13">[14]</ref>. Depending on the university, a conversion may be necessary. If not stated otherwise, 50% of the score are required to pass the exam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">1st Semester</head><p>We have neither received full approval nor the required content for any of the modules of the first semester from the responsible professors. This is left to be explored in subsequent work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">2nd Semester</head><p>We have received approval for two second-semester modules, Operating Systems, and Object-Oriented Application Development. Figure <ref type="figure" target="#fig_0">1</ref> shows the exam results for each LLM in these modules.</p><p>Operating Systems (OS) is a five credit-point module. In the module, students learn the structure of a modern operating system and algorithms and strategies for managing and allocating resources in it. They also develop programs in a UNIX environment and work out solutions to problems of interprocess communication <ref type="bibr" target="#b7">[8]</ref>.</p><p>Object Oriented Application Development (OOA) is a seven credit-point module. It focuses on teaching the methods and techniques of object-oriented programming. Requirements are implemented using efficient algorithms and data structures. Programming is done in C++ <ref type="bibr" target="#b7">[8]</ref>.</p><p>ChatGPT-3.5 and BingAI performed quite well in OS, scoring 82.4% and 80.6%, respectively. The 7B parameter models performed significantly worse. LLaMa-7B-Q (quantized) scored 21.8%, and StableLM-7B scored 9.4%. While StableLM-7B could answer almost no questions, LLaMa-7B-Q could still answer questions about shell commands and general operating system terms. Nevertheless, it was too little to pass the exam. ChatGPT-3.5 and BingAI were able to answer many questions. The models did make mistakes when calculating memory usage and applying paging algorithms. This cost them points but kept the good result the same. ChatGPT-3.5 passed this exam with a grade of 2.0 and BingAI with a Grade of 2.1. OOA is the only module in which BingAI performed better than ChatGPT-3.5 in all our testing. The latter LLM scored 75%, whereas BingAI scored 86.3%. This results in grades of 2.5 for ChatGPT-3.5 and 1.8 for BingAI. The score for BingAI is one of the best results for this LLM in all our tests. The biggest problems ChatGPT-3.5 had were with implementing the object-oriented interfaces in C++. The code compiled but either didn't work as it should or implemented something completely different from the task. StableLM-7B performed slightly better than LLaMa-7B-Q in this exam. However, both LLMs had severe problems with the assignments, solving almost no tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">3rd Semester</head><p>We have received approval for two modules of the third semester, Web Engineering and Distributed Systems. The results for the modules of the third semester can be seen in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Web Engineering (WEB) is a five credit-point module. It covers the technical fundamentals of modern web-based technologies and architectural, development, and analysis tools for web-based systems. On the front end, students in this module work with HTML, CSS, and JavaScript; On the backend side, with a mixture of Javascript and Python <ref type="bibr" target="#b7">[8]</ref>. A student with 33% or more would pass the exam, as determined by the supervising professor.</p><p>Distributed Systems (DS) is also a five credit-point module. Students of this module learn about distributed system architectures and techniques for synchronization and communication. At the end of this module, they can design, implement and evaluate their own distributed computing structures. The implementation within this module takes place in C/C++ <ref type="bibr" target="#b7">[8]</ref>. Our set exam consists of questionnaire material. WEB was one of the best exam results in all our testing for ChatGPT-3.5, scoring an even 1.0 on the exam with 98.3%. Even the most extensive task, a partial Python implementation of a backend server for the membership management of a business, was solved completely and correctly. BingAI was also able to answer almost every question correctly. Only in the implementation part did Bin-gAI make logical errors and omit required functionalities. This still resulted in 90% or a grade of 1.4. StableLM-7B and LLaMa-7B-Q had surprisingly massive problems in this exam, despite the extensive question part. Almost no question could be answered completely or correctly. Both models also failed the implementation part. LLaMa-7B-Q scored 15.8%, slightly better than StableLM-7B, with 9.2%.</p><p>In DS, both ChatGPT-3.5 and BingAI performed worse than in WEB. ChatGPT-3.5 got a grade of 2.2 with a result of 78.5%, and BingAI a 2.8 with 70%. The models could answer almost all simple or introductory questions to the topic correctly but had problems with more in-depth questions, e.g., on network data formats or broker implementations. DS was the first module in which we tested GPT-4.0. With a result of 95.5%, it got a grade of 1.2 and topped the grade of ChatGPT-3.5 by a whole level. GPT-4.0 answered almost every question completely and correctly in this exam. StableLM-7B scored 30% in DS, the best result for this LLM in all our tests. Surprisingly, it was able to answer difficult questions on CORBA, SOAP interfaces, and synchronization mechanisms but failed on simpler, more general questions, like resilience and fault tolerance of distributed systems. Otherwise, it would have had a real chance to pass the exam. In this module, we also tested LLaMa-65B-Q for the first time. With a result of 14.5%, it performed only slightly better than LLaMa-7B-Q with 13.5%. Considering the difference in size, this is a disappointing result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">4th Semester</head><p>We have received approval for three modules of the fourth semester: Data Network Management, Interactive Systems, and Numerical Analysis. The results can be taken from Data Network Management (DNM) is a six credit-point module. It provides in-depth, application-oriented knowledge of network administration. Students in this module acquire skills in the design, development, and deployment of large-scale computer networks, as well as techniques for securing them <ref type="bibr" target="#b7">[8]</ref>.</p><p>Interactive Systems (IAS) is a five credit-point module. It focuses on software ergonomics and the design and implementation of portable interactive systems. Students of this module learn how to model application-oriented and ergonomic human-machine interfaces <ref type="bibr" target="#b7">[8]</ref>. The implementations in this module are web-based in the programming languages JavaScript and Python. As determined by the supervising professor, the module is considered to be passed if 33% of the total points are achieved.</p><p>Numeric Analysis (NUM) is an elective course in our computer science bachelor's degree program, which gives five credit points. Topics covered include computer arithmetic and rounding errors, systems of linear equations, and linear equilibrium calculus. The module is concluded with an oral examination <ref type="bibr" target="#b7">[8]</ref>. This is one of the modules in which we simulated an exam by taking questions from a questionnaire. DNM is the first module in our tests in which even the larger LLMs have experienced problems. ChatGPT-3.5 barely passed the exam with 51.9% or a grade of 3.8. BingAI had even more difficulties and failed the exam with a score of only 47.1%. The application of firewall rules and routing protocols for custom multi-area networks presented in the exam was particularly problematic for both LLMs. GPT-4.0 performed the best in this exam. It was also unable to completely solve the more difficult tasks but often provided correct partial solutions or made less serious errors than the other two LLMs. GPT-4.0 passed the exam with 65.4% or a grade of 3.0.</p><p>IAS went very well for ChatGPT-3.5. With 96.7%, it got a grade of 1.1. It answered almost all comprehension and knowledge questions correctly. Even more complex tasks, such as the design of a user interface, were solved completely and correctly. BingAI performed almost 30 percentage points worse in this exam, resulting in one of the biggest gaps between these two LLMs in all our testing. It got a grade of 2.4, or 68.8% of the total score. The grade of 2.4 comes from the fact that the exam is considered passed from 35%, and larger results are offset by the formula linearly. Nevertheless, BingAI had problems with several questions in this exam and either answered incorrectly or omitted information. StableLM-7B and LLaMa-7B-Q had no chance of passing this exam, with a performance of 6.7% and 13.3%, respectively. Nearly every answer had massive errors or large information gaps. The models also lost context in between and started talking about completely different topics.</p><p>In the simulated oral exam on numerical analysis, mainly comprehension questions were asked, and hardly any calculations had to be done. This led to excellent results for both GPT models. ChatGPT-3.5 got a grade of 1.6, with 90% of the total score, whereas GPT-4.0 increased this to 95% and a grade of 1.3. Both models could answer almost every question completely and correctly and only made minimal errors. BingAI had great problems in this exam, although it was mainly about knowledge reproduction, and scored well behind the GPT models with 68% of the total score, or a grade of 2.9. BingAI had problems with several questions and made mistakes while reproducing information. For example, when asked about the complexity of the Gauss Algorithm, BingAI gave a reference to Wikipedia but then misquoted the article with a complexity of O(n²).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">5th Semester</head><p>We have received approval for three modules of the fifth semester: Data Science, Software Engineering, and Real-Time Systems. The results can be seen in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>Data Science (DSC) is an elective course with five credit points. The module provides an introduction to Big Data and Machine Learning. Students of this module will learn to extract, prepare and analyze large data sets <ref type="bibr" target="#b7">[8]</ref>. The module concludes with an oral exam which we simulated by taking questions from a questionnaire.</p><p>Software Engineering (SWE) is a five credit-point module. It covers advanced solutions for building, testing, and maintaining large IT systems and techniques for organizing big software projects. A special focus is on effort estimation and (agile) software development processes <ref type="bibr" target="#b7">[8]</ref>. As determined by the supervising professor, the module is considered to be passed if 33% of the total points are achieved.</p><p>Real-Time Systems (RTS) is a five credit-point module. It focuses on the architecture, the concepts, and the functionalities of modern real-time systems. Students learn aspects of concurrent real-time programming and how to deal with time constraints and task management <ref type="bibr" target="#b7">[8]</ref>. The module is concluded with a written exam in which a special focus is placed on manual real-time proof for various scheduling methods. DSC is the best-performing module for ChatGPT-3.5 in all our tests. With 99%, ChatGPT-3.5 scored a 1.0. Every question was answered correctly, from problems in the field of Big Data to data preparation, to classification and clustering methods. DSC is also the only module in which ChatGPT-3.5 outperformed GPT-4.0. The latter model did not score itself badly with 94% and a score of 1.3, but unfortunately gave partially wrong answers to questions about Eventual Consistency and Sharding. Such results are possible since we only prompt all models once in our tests. BingAI performed again worse than the GPT models. With 81%, it achieved a score of 2.1. It was challenging for BingAI to make its own decisions in tasks, e.g., choosing between an aggregate-oriented or a relational data model.</p><p>In SWE, the GPT models were again well ahead of BingAI. ChatGPT-3.5 achieved a 1.9 with 78.1%, GPT-4.0 a 1.7 with 82.9%, while BingAI only achieved a 2.9 with 56.7%. The GPT models answered most of the questions completely and correctly but made mas- sive errors in designing test cases for a finite state machine. BingAI could not solve this task either and made errors in explaining design patterns and performing an effort estimation using Function Point Analysis. StableLM-7B and LLaMa-7B-Q performed poorly in this module, scoring 3.8% and 6.2%, respectively. LLaMa-65B-Q performed significantly better than the 7 billion parameter version and nearly passed the exam with 25.2%, out of 33% needed. Interestingly, LLaMa-65B-Q was able to partially answer difficult questions on software development principles and the design of component diagrams but failed to explain unit tests. RTS is a demanding exam in which many calculations have to be done. Computational time requirements, core workloads, and a large, manual real-time proof must be calculated. This exam has proven to be extremely difficult for all models tested. ChatGPT-3.5 scored 29.4%, GPT-4.0 scored 33.8%, and BingAI scored 23.5%. Accordingly, all models failed the exam. The models could answer a few simple introductory questions but, early on, miscalculated the computational kernel allocation for a round-robin scheduling procedure.</p><p>No model was able to solve this task correctly. Likewise, no model was able to calculate the real-time proof correctly. This task is nested, with each intermediate calculation evaluated individually but often a prerequisite for the next calculation. BingAI lost all context in this task after the third partial calculation. The GPT models could continue to calculate but miscalculated fatally early on. Both the calculation path and the result were not correct. When calculating the average execution time for processes of a machine, both ChatGPT-3.5 and GPT-4.0 set up the correct formula, adding all times and dividing by the amount, but only GPT-4.0 also got the correct result; ChatGPT-3.5 miscalculated.</p><p>All in all, the models were heavily overcharged with this exam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>An overview of the average performances and results achieved across all tested modules can be seen in Table <ref type="table" target="#tab_2">3</ref>. It is important to note that all data collected is only a snapshot that considers the systems' performance at the time of the assessment. These systems continue to evolve.</p><p>ChatGPT-3.5 achieved an average of 79.9% of the maximum possible score in the ten modules tested. ChatGPT-3.5 performed particularly well in modules with a high proportion of web development or high-level programming language content, like Python and JavaScript. Even in the field of data science, ChatGPT-3.5 achieved almost full marks. In exams with more complex tasks like Operating Systems or Data Network Management, ChatGPT-3.5 often provided at least an approach to the solution. We noted major difficulties for this LLM with various tasks that required mathematical calculations. The application of scheduling algorithms and the calculation of core utilization and process runtimes posed significant challenges for ChatGPT-3.5. Due to these shortcomings, passing the Real-Time Systems exam is not currently possible. Accordingly, the LLM would not be capable of completely finishing our bachelor's degree program in computer science. However, with an understanding of its strengths and weaknesses, ChatGPT-3.5 shows great potential as an outstanding learning aid for students and lecturers.</p><p>GPT-4.0 achieved even better results than ChatGPT-3.5, obtaining an average performance of 80.2%. This score is expected to increase even further if the missing modules are tested with GPT-4.0. A strong focus on specific programming languages or fields of computer science, as observed in ChatGPT-3.5, could not be detected in GPT-4.0. At the same time, GPT-4.0 demonstrated a more consistent overall performance. Like ChatGPT-3.5, GPT-4.0 had difficulties with tasks that required calculations; this also resulted in the failure of the Real-Time Systems exams. For the same reason, GPT-4.0 would not be able to finish the degree program. However, it should be noted that GPT-4.0, despite the identified difficulties, represents an improvement in all areas over ChatGPT-3.5. The use of plugins, for instance, to redirect mathematical computations to a system like WolframAlpha could significantly improve this outcome. This is left to be explored in future studies.</p><p>BingAI scored much lower than the GPT models in our tests, with 68.4%. It was the only one of these three models that failed two exams rather than just one, with one of the exams (Data Network Management) being not calculation-intensive. BingAI often encountered problems when the solutions were not directly searchable online. Thus, it made mistakes in extracting information from texts or creating and presenting solutions. Even when the answer to a question could be found via an internet search, BingAI sometimes made inexplicable citation errors. BingAI also provided the shortest responses of all the LLM systems tested, often ignoring aspects of a question. According to the current state, BingAI is inferior to the GPT models.</p><p>LLaMa-7B-Q showed poor results, with an average performance of 12.3% in six tested modules. It often had difficulties understanding questions, lost context, or started talking about completely different topics. LLaMa-7B-Q could not solve a single task of an exam. According to our tests, it would not be possible for this LLM to pass any module.</p><p>LLaMa-65B-Q showed better results with an average performance of 20.0%, but was also tested only in two modules. It scored one percentage point and 19 percentage points better than its 7 billion parameter counterpart. At this point, more tests are needed to make a final statement about the performance differences between these models. Nevertheless, a trend can be determined: LLaMa-65B-Q performs significantly worse than BingAI, let alone the GPT models. After our tests, whether it would pass a single module is questionable, and it is not suitable for use as a learning aid.</p><p>StableLM-7B, tested in six modules, achieved the worst results of all tested LLMs with 10.8%. It was unable to answer any question correctly and completely. Interestingly, StableLM-7B often related questions to a business context or attempted to answer them in such a context. StableLM-7B even understood complex questions from the field of project management but could not establish a reference to computer science or software development. According to our tests, this LLM is also unsuitable as a learning aid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In the presented study, we tested and evaluated the performance of various LLMs across a series of modules in a bachelor's computer science degree program. Our results are in line with existing research (e.g. <ref type="bibr" target="#b0">[1]</ref>) by showing strong performances of Generative Pre-training models (GPT) across an undergraduate curriculum while having severe restrictions in key areas.</p><p>A prevalent worry is the potential for essays to progressively lose significance as evaluation tools within higher education <ref type="bibr" target="#b4">[5]</ref>. Our tests show the strength and topic affinities of current LLMs, but also their weaknesses, distinctively in mathematical computations. We conclude from these results that a comprehensive blueprint for our curriculum remains elusive at this point. Despite this, the deployment of these models presents lecturers with challenges, as the detection of plagiarism in AI-generated content is not particularly mature yet <ref type="bibr" target="#b8">[9]</ref>. It is imperative to recognize that the GPT models in our tests have completed numerous examinations with scores above 95%. Given that some of our examination rules allow aids, and the pattern of past exams often remains unchanged, the sophisticated capabilities of these models could potentially create near-perfect and legally permissible "cheat" sheets. This, combined with the advancing abilities of current LLMs <ref type="bibr" target="#b4">[5]</ref>, compels us to reconsider and construct robust examination methods. Oral and written exams without aids remain valid alternative options <ref type="bibr" target="#b4">[5]</ref>.</p><p>The smaller models in our tests exhibit substantial performance deficiencies, with profound disparities encountered in almost all performance-defining areas. Consequently, they currently do not measure up as viable educational aids.</p><p>Continued research may examine the performance of existing models in unexplored curriculum modules. Furthermore, additional modules could be examined to provide a broader overview. Future research could also extend to the study of other LLMs, such as Google Bard. Also, broadening the scope to related disciplines, like electrical engineering, would be beneficial to gain a better understanding of domain-specific performance capabilities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Exam results for Operating Systems (OS) and Object Oriented Application Development (OOA)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Exam results for Web-Engineering (WEB) and Distributed Systems (DS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 CFigure 3 :</head><label>33</label><figDesc>Figure 3: Exam results for Data Network Management (DNM), Interactive Systems (IAS) and Numerical Analysis (NUM)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Exam results for Data Science (DSC), Software Engineering (SWE) and Real-Time Systems (RTS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overview of our test data set.</figDesc><table><row><cell cols="3">Semester Written exams Oral exams</cell></row><row><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>2</cell><cell>2</cell><cell>0</cell></row><row><cell>3</cell><cell>2</cell><cell>0</cell></row><row><cell>4</cell><cell>2</cell><cell>1</cell></row><row><cell>5</cell><cell>2</cell><cell>1</cell></row><row><cell>Sum</cell><cell>8</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Summary of the average performance proportionally calculated to all modules taken.</figDesc><table><row><cell>Model</cell><cell cols="3">Average score # Modules Passed / Failed</cell></row><row><cell>GPT-4.0</cell><cell>80.2%</cell><cell>6</cell><cell>5 / 1</cell></row><row><cell>ChatGPT-3.5</cell><cell>79.9%</cell><cell>10</cell><cell>9 / 1</cell></row><row><cell>BingAI</cell><cell>68.4%</cell><cell>10</cell><cell>8 / 2</cell></row><row><cell>LLaMa-65B-Q</cell><cell>20.0%</cell><cell>2</cell><cell>0 / 2</cell></row><row><cell>LLaMa-7B-Q</cell><cell>12.3%</cell><cell>6</cell><cell>0 / 6</cell></row><row><cell>StableLM-7B</cell><cell>10.8%</cell><cell>6</cell><cell>0 / 6</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors declare that funds of the <rs type="funder">Bundesministerium für Bildung und Forschung</rs> were used to finance this study. Grand-ID: <rs type="grantNumber">16DHBKI070</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xMjzCqU">
					<idno type="grant-number">16DHBKI070</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using cognitive psychology to understand GPT-3</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Binz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bordt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
		<title level="m">ChatGPT Participates in a Computer Science Exam</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Pile: An 800gb dataset of diverse text for language modeling</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Gerganov</surname></persName>
		</author>
		<ptr target="https://github.com/ggerganov/llama.cpp" />
		<title level="m">cpp Github Repository</title>
		<imprint>
			<date type="published" when="2023-07">2023. July-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unlocking the power of generative AI models and systems such as GPT-4 and ChatGPT for higher education: A guide for students and lecturers</title>
		<author>
			<persName><forename type="first">Henner</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Eymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Lämmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Mädche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Röglinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Ruiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Schoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mareike</forename><surname>Schoop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Urbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Vandrik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hohenheim Discussion Papers in Business, Economics and Social Sciences</title>
		<imprint>
			<biblScope unit="page" from="2" to="2023" />
			<date type="published" when="2023">2023</date>
			<pubPlace>Stuttgart</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universität Hohenheim</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Srini</forename><surname>Hila Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terra</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Blevins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Demystifying Prompts in Language Models via Perplexity Estimation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interview insight: How to get the job</title>
		<author>
			<persName><forename type="first">Jocelyn</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Software Engineer&apos;s Guide to Seniority: A Guide to Technical Leadership</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Modulhandbuch zum Vollzeit Studiengang Bachelor Informatik nach Prüfungsordnung</title>
		<author>
			<persName><forename type="first">Hochschule</forename><surname>Niederrhein</surname></persName>
		</author>
		<ptr target="https://www.hs-niederrhein.de/fileadmin/dateien/FB03/Studierende/Bachelor-Studiengaenge/PO2013/modul__bi.pdf" />
		<imprint>
			<date type="published" when="2013">2013. 2019. June-2023</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Will ChatGPT get you caught? Rethinking of Plagiarism Detection</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erkan</forename><surname>Er</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Künstliche Intelligenz in Training, Weiterbildung und Beratung: 70 direkt anwendbare und erprobte KI-Tools</title>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Krüger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>pitchnext GmbH via Kindle Direct Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<ptr target="https://www.microsoft.com/en-us/microsoft-365/blog/2023/03/16/introducing-microsoft-365-copilot-a-whole-new-way-to-work/" />
		<title level="m">Introducing Microsoft 365 Copilot -A whole new way to work</title>
		<imprint>
			<publisher>Microsoft</publisher>
			<date type="published" when="2023-07">2023. July-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The End of an Era: Can Ai Subsume Software Developers? Evaluating Chatgpt and Copilot Capabilities Using Leetcode Problems</title>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Nikolaidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karolos</forename><surname>Flamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Feitosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Chatzigeorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apostolos</forename><surname>Ampatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evaluating Chatgpt and Copilot Capabilities Using Leetcode Problems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">GPT-4</title>
		<ptr target="https://arxiv.org/pdf/2303.08774.pdf" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="https://www.kmk.org/fileadmin/Dateien/pdf/ZAB/Hochschulzugang_Beschluesse_der_KMK/GesNot05.pdf" />
		<title level="m">Sekretariat der ständigen Konferenz der Kultusminister der Länder in der Bundesrepublik Deutschland. Vereinbarung über die festsetzung der gesamtnote bei ausländischen hochschulzugangszeugnissen</title>
		<imprint>
			<date type="published" when="2013-06">2013. June-2023</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Stability</surname></persName>
		</author>
		<ptr target="https://github.com/Stability-AI/StableLM" />
		<title level="m">StableLM Github Repository</title>
		<imprint>
			<date type="published" when="2023-07">2023. July-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Stability</surname></persName>
		</author>
		<ptr target="https://huggingface.co/stabilityai" />
		<title level="m">StableLM hugging face</title>
		<imprint>
			<date type="published" when="2023-07">2023. July-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Statista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ai -Worldwide</forename><surname>Generative</surname></persName>
		</author>
		<idno>ac- cessed 05</idno>
		<ptr target="https://www.statista.com/outlook/tmo/artificial-intelligence/generative-ai/worldwide" />
		<imprint>
			<date type="published" when="2023-07">2023. July-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<title level="m">LLaMA: Open and Efficient Foundation Language Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jules</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quchen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Sandborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Olea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashraf</forename><surname>Elnashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Spencer-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">C</forename><surname>Schmidt</surname></persName>
		</author>
		<title level="m">A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Leetcode&amp;oldid=1159537770" />
		<title level="m">Wikipedia contributors. Leetcode -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2023-06">2023. June-2023</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation</title>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Youn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
