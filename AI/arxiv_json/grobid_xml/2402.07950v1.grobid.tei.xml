<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SENTINELS OF THE STREAM: UNLEASHING LARGE LANGUAGE MODELS FOR DYNAMIC PACKET CLASSIFICATION IN SOFTWARE DEFINED NETWORKS -POSITION PAPER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shariq</forename><surname>Murtuza</surname></persName>
							<email>shariq.murtuza@jiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering &amp; Information Technology Jaypee Institute of Information Technology Noida</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SENTINELS OF THE STREAM: UNLEASHING LARGE LANGUAGE MODELS FOR DYNAMIC PACKET CLASSIFICATION IN SOFTWARE DEFINED NETWORKS -POSITION PAPER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">37392CF977E490EBBBA214552965A5FA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large Language Models</term>
					<term>Network Security</term>
					<term>Distributed Denial of Service Attacks</term>
					<term>Traffic Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the release of OpenAI's ChatGPT, the field of large language models (LLM) saw an increase of academic interest in GPT based chat assistants. In the next few months multiple accesible large language models were released that included Meta's LLama models and Mistral AI's Mistral and Mixtral MoE models. These models are available openly for a wide array of purposes with a wide spectrum of licenses. These LLMs have found their use in a different number of fields like code development, SQL generation etc. In this work we propose our plan to explore the applicability of large language model in the domain of network security. We plan to create Sentinel, a LLM, to analyse network packet contents and pass a judgment on it's threat level. This work is a preliminary report that will lay our plan for our future endeavors. 1   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the launch of ChatGPT <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> the field of Artificial Intelligence experienced a renewed interest from all walks of life. ChatGPT is a type of language model called a Large Language Model (LLM) <ref type="bibr" target="#b3">[4]</ref>. In simple terms, a LLM is initially trained on data using semi supervised and self supervised learning, and then produce output by predicting the next token. LLMs have strong potential to perform a wide range of tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> specified in typical spoken language or technically, in natural language. These models can extensively understand and perform Natural Language Processing (NLP) tasks such as writing letters, invitation, apologies, participate in a dialogue, give solution to mathematical question and lot more. Even though the main expertise of these models is in language(pattern) based tasks, these models can also be used to produced highly specialised texts similar to humans <ref type="bibr" target="#b6">[7]</ref> based on the given instruction. A network packet is an ordered collection of bits with each bit reserved for a specific flag representing a value. Each network packet is made up of a sequence of zeros and ones (bits) with each bit location earmarked for a specific purpose. The original IP protocol specification was give in RFC 791 in the year 1981 <ref type="bibr" target="#b7">[8]</ref>. While the TCP protocol specification was given in RFC 9293 <ref type="bibr" target="#b8">[9]</ref>. Over time these specifications have been updated over time but have provided a concrete foundation for network stack. As each bit location has a specific meaning, we can map it as a sentence of a hypothetical language that consists of bit stream with individual components like flags, port number, IP address analogous to words. Meta(earlier known as Facebook) launched Llama and Llama2 (two different iterations a single model) and associated weights to the research community in 2023 with different licenses <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. The model was named as LLaMA (Large Language Model Meta AI) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> and was released in February 2023, and the second model called LLaMA-2 <ref type="bibr" target="#b13">[14]</ref>, in July 2023. LLaMA was released in four different model sizes 7, 13, 33 and 65 billion parameters, while LLaMA-2 was released in three model sizes 7, 13, and 70 billion parameters. The next update in the public LLM domain came with the release of Mistral AI's Mistral 7B <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> (in September, 2023) that outperformed all the available open models up to 13B parameters, at that time, as per the existing language and code benchmarks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> Soon after the Falcon family of models (7B, 40B, and 180B) were released by the Technology Innovation Institute (TII) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref> in November, 2023 outperforming the Llama-2 models on different benchmarks. These models are also open and are available for personal and commercial usage under different licences <ref type="bibr" target="#b19">[20]</ref>. In this work we present our plan to finetune and create a large language model named Sentinel, that can analyses network packets and identify malicious attack packets from a given network flow. We discuss the details in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Creating our Large Language Model -Sentinel</head><p>Finetuning is an approach often deployed in LLMs to increase their accuracy at the cost of shrinking their output domain <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. For example, an LLM fine-tuned on medical science data will give better results compared to an LLM that has not been fine-tuned yet <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. However, finetuning also reduces the number of different domains an LLM can respond to. In layman's terms, finetuning involves adjusting the weights of an already existing pre-trained model to enhance its performance on specific types of problems. Our work is based on the work done in PADEC <ref type="bibr" target="#b25">[26]</ref> and extending it to encompass multiple different attack scenarios. We plan to fine-tune three different models namely, Llama2-7B, Falcon-7B and Mixtral MoE (mixture of experts) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29]</ref> on our custom-generated SDN link flood attack dataset. Our dataset was generated with the Containernet <ref type="bibr" target="#b30">[31]</ref> simulator. We created multiple nodes, attackers, switches, and the controller and captured the network traffic. Traditional datasets were avoided due to their lack of alignment with the rapid advancements in network attacks. We are not creating a new LLM model from scratch, but instead we will be using existing open models and train them upon our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Processing</head><p>To create the dataset, we simulated a multihost network with various nodes providing different services. Our dataset comprises captured network packets obtained using the TCPdump tool <ref type="bibr" target="#b31">[32]</ref>. The traffic capture commenced after the attack was initiated and had stabilized over time. The attack packets comprises of different attack categories, including volumetric attacks, protocol attacks, and other vulnerability-based attacks <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Finetuning</head><p>Each packet is a collection of bits, with each bit corresponding to a specific defined purpose in the network packet. This makes it very similar to natural spoken language, and thus we can safely assume a network packet to be a piece of textual language, where the language consists of only two alphabets: zero and one. Just as a language sentence has a format, a group of semantically related words delimited by special punctuation marks (comma, full stop, question mark, etc.), the network packets also have a clearly defined format, with each numbered bit reserved for a specific purpose. Similar to how the next word in a sentence are related to the current word, inside a network packet's header, the next bit(s) are related to the current bit value. For example within the TCP header headers as shown in Fig. <ref type="figure">1</ref>, the source port is always followed by the destination port. Similarly an IP packet (shown in Fig. <ref type="figure" target="#fig_0">2</ref> also follows specific rules and in no scenarios can we have a packet without an IP address and/or a port number. This compulsion makes a network packet highly similar to a language with grammar rules and a captured network packet equivalent to a written sentence. As each bit location has a specific meaning, we can map it as a sentence of a hypothetical language that consists of bit stream with individual components like flags, port number, IP address analogous to words. As we have proven, a captured network packet is no different from a written sentence, thus we can safely say that a large language model can be created to read, write and understand a language that has it's constituents sentences in the form of network packets. The implied similarity is shown in Fig <ref type="figure">3</ref> where it is evident that specific bits always refer to specific information in a packet. This language (consisting of network packets) would be simpler than spoken languages like English due to the </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 1: TCP Packet format</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Technical Report, work in progress.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The book fell off the shelf.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion</head><p>In this text we share our plans to create Sentinel, a fine tuned LLM to analyse network packets and take a decision regarding the amount of threat posed by the packets. We are starting with Llama2-7B then we will move towards Mistral AI's 7B model and shall finally study the feasibility of Mixtral MoE model. We are using three very different models to gauge their efficacy in handling this task and to select the best among the candidates. Our selection of model is purely on their performance on various public leader-boards and their proven capabilities in performing given tasks.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A commentary of GPT-3 in MIT Technology Review 2021</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundamental Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="831" to="833" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihuai</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01933</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey on evaluation of large language models</title>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on evaluation of large language models</title>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Potential of Visual ChatGPT for Remote Sensing</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Osco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Prado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wesley</forename><surname>Lopes De Lemos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Nunes Gonçalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paula</forename><forename type="middle">Marques</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Marcato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junior</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">3232</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Is GPT-3 text indistinguishable from human text? SCARECROW: A framework for scrutinizing machine text</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01294</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Internet protocol-DARPA internet program protocol specification, RFC 791</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Postel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
	<note>Internet protocol-DARPA internet program protocol specification</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Eddy</surname></persName>
		</author>
		<title level="m">RFC 9293: Transmission Control Protocol (TCP)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Llama</title>
		<author>
			<persName><surname>Llama</surname></persName>
		</author>
		<ptr target="https://llama.meta.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LLaMA: Open and Efficient Foundation Language Models -Meta Research | Meta Research</title>
		<ptr target="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/" />
	</analytic>
	<monogr>
		<title level="j">Meta Research</title>
		<imprint>
			<date type="published" when="2023-02-24">February 24, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Introducing LLaMA: A foundational, 65-billion-parameter language model</title>
		<ptr target="https://ai.meta.com/blog/large-language-model-llama-meta-ai/" />
		<imprint/>
	</monogr>
	<note>Introducing LLaMA: A Foundational, 65-Billion-Parameter Language Model</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Meta-Llama (Meta Llama 2)</title>
		<ptr target="https://huggingface.co/meta-llama" />
		<editor>meta-llama</editor>
		<imprint>
			<date type="published" when="2023-12-27">December 27, 2023</date>
			<pubPlace>Meta</pubPlace>
		</imprint>
	</monogr>
	<note>Llama 2)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mistral AI | Open-Weight Models</title>
		<author>
			<persName><forename type="first">Mistral</forename><surname>Ai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mistral AI | Open-weight models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mistral 7B</title>
		<author>
			<persName><forename type="first">Mistral</forename><surname>Ai</surname></persName>
		</author>
		<ptr target="https://mistral.ai/news/announcing-mistral-7b/" />
	</analytic>
	<monogr>
		<title level="m">Mistral 7B | Mistral AI | Open-weight models</title>
		<imprint>
			<date type="published" when="2023-09-27">September 27, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Open LLM Leaderboard -a Hugging Face Space by HuggingFaceH4</title>
		<ptr target="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" />
		<imprint/>
	</monogr>
	<note>Open LLM Leaderboard -a Hugging Face Space by HuggingFaceH4</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Open-Llm-Leaderboard (Open LLM Leaderboard)</title>
		<author>
			<persName><forename type="first">Llm</forename><surname>Open</surname></persName>
		</author>
		<author>
			<persName><surname>Leaderboard</surname></persName>
		</author>
		<ptr target="https://huggingface.co/open-llm-leaderboard" />
		<imprint>
			<date type="published" when="2024-02-06">February 6, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Falcon LLM</title>
		<ptr target="https://falconllm.tii.ae/" />
		<imprint>
			<date type="published" when="2024-02-09">February 9, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Tiiuae (Technology Innovation Institute)</title>
		<author>
			<persName><surname>Tiiuae</surname></persName>
		</author>
		<ptr target="https://huggingface.co/tiiuae" />
		<imprint>
			<date type="published" when="2023-09-06">September 6, 2023</date>
			<publisher>Technology Innovation Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01116</idno>
		<title level="m">The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03277</idno>
		<title level="m">Instruction tuning with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parameterefficient fine-tuning of large-scale pre-trained language models</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="220" to="235" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large language models in medicine</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Thirunavukarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darren</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeng</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kabilan</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Elangovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fang Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1930" to="1940" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Instruction tuning for large language models: A survey</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.10792</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GitHub -Rdpahalavan/PADEC: Thesis Research on Enhancing Network Intrusion Detection System (NIDS) Explainability Using Transformers</title>
		<author>
			<persName><forename type="first">Pahalavan</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheivanayahi</forename></persName>
		</author>
		<ptr target="https://github.com/rdpahalavan/PADEC" />
	</analytic>
	<monogr>
		<title level="j">GitHub</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Meta-Llama/Llama-2-7b • Hugging Face</title>
		<idno>meta-llama/Llama-2-7</idno>
		<ptr target="https://huggingface.co/meta-llama/Llama-2-7b" />
		<imprint/>
	</monogr>
	<note>b • Hugging Face</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Tiiuae/Falcon-7b • Hugging Face</title>
		<ptr target="https://huggingface.co/tiiuae/falcon-7b" />
		<imprint>
			<date type="published" when="2023-06-20">June 20, 2023</date>
		</imprint>
	</monogr>
	<note>Hugging Face</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mixtral of Experts</title>
		<author>
			<persName><forename type="first">Mistral</forename><surname>Ai</surname></persName>
		</author>
		<ptr target="https://mistral.ai/news/mixtral-of-experts/" />
	</analytic>
	<monogr>
		<title level="m">Mixtral of experts | Mistral AI | Open-weight models</title>
		<imprint>
			<date type="published" when="2023-12-11">December 11, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mixtral of experts</title>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blanche</forename><surname>Savary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.04088</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Containernet 2.0: A rapid prototyping platform for hybrid service function chains</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Peuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kampmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Karl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 4th IEEE Conference on Network Softwarization and Workshops (NetSoft)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="335" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Home | TCPDUMP &amp; LIBPCAP</title>
		<author>
			<persName><forename type="first">|</forename><surname>Home</surname></persName>
		</author>
		<author>
			<persName><surname>Tcpdump</surname></persName>
		</author>
		<author>
			<persName><surname>Libpcap</surname></persName>
		</author>
		<ptr target="https://www.tcpdump.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DDoS attack detection and mitigation using SDN: methods, practices, and solutions</title>
		<author>
			<persName><forename type="first">Narmeen</forename><surname>Bawany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jawwad</forename><forename type="middle">A</forename><surname>Zakaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Shamsi</surname></persName>
		</author>
		<author>
			<persName><surname>Salah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arabian Journal for Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="425" to="441" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
