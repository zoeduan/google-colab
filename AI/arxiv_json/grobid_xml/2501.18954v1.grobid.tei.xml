<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-31">31 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shenghao</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Tongyi Lab, Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qize</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tongyi Lab, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qijie</forename><surname>Mo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junkai</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xihan</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tongyi Lab, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingke</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
							<email>xiexiaoh6@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Guangdong Province Key Laboratory of Information Security Technology</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<email>wszheng@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Pazhou Laboratory (Huangpu)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-31">31 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">49DD3BF6237D57FEFCEE6ECC14F49391</idno>
					<idno type="arXiv">arXiv:2501.18954v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent open-vocabulary detectors achieve promising performance with abundant region-level annotated data. In this work, we show that an open-vocabulary detector cotraining with a large language model by generating imagelevel detailed captions for each image can further improve performance. To achieve the goal, we first collect a dataset, GroundingCap-1M, wherein each image is accompanied by associated grounding labels and an imagelevel detailed caption. With this dataset, we finetune an open-vocabulary detector with training objectives including a standard grounding loss and a caption generation loss. We take advantage of a large language model to generate both region-level short captions for each region of interest and image-level long captions for the whole image. Under the supervision of the large language model, the resulting detector, LLMDet, outperforms the baseline by a clear margin, enjoying superior open-vocabulary ability. Further, we show that the improved LLMDet can in turn build a stronger large multi-modal model, achieving mutual benefits. The code, model, and dataset is available at <ref type="url" target="https://github.com/iSEE-Laboratory/LLMDet">https:  //github.com/iSEE-Laboratory/LLMDet</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Open-vocabulary object detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36]</ref> aims to detect arbitrary classes based on text labels from user input, which is a more general detection task than traditional closed-set object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>. GLIP <ref type="bibr" target="#b26">[27]</ref> first unifies object detection and phrase grounding through * : Corresponding authors are Xiaohua Xie and Wei-Shi Zheng. Part of the work was done when Shenghao Fu was an intern at Alibaba. region-word contrastive pre-training. This formulation benefits from massive grounding and image-text data covering a broad range of concepts, making the learned representations semantic-rich. The following works focus on effective vision-language fusion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref> and fine-grained region-word alignment by carefully designed word embeddings <ref type="bibr" target="#b55">[56]</ref> and negative samples <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b64">64]</ref>. By scaling up pretraining data and computation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b56">57]</ref>, existing openvocabulary object detectors can achieve amazing zero-shot performance on various benchmarks.</p><p>Recent works show that unifying the grounding task with other language tasks enriches visual representations with language knowledge thus creating a stronger openvocabulary detector. GLIPv2 <ref type="bibr" target="#b62">[62]</ref> pretrains the model under the grounding loss and the masked language modeling loss. Subsequently, CapDet <ref type="bibr" target="#b37">[38]</ref> and DetCLIPv3 <ref type="bibr" target="#b57">[58]</ref> demonstrate that unifying dense captioning and grounding also boosts open-vocabulary ability. However, they use short captions for each object, e.g., coarse descriptions and hierarchical class labels, which are coarse-grained, individual, and lacking association among objects. Alternatively, long image-level captions, containing rich details and a comprehensive understanding of an image, provide more information than short region-level descriptions, which motivates us to explore what advantages can long detailed image-level captions bring to open-vocabulary detectors.</p><p>In light of this, we propose LLMDet, which trains an open-vocabulary detector with a standard grounding objective accompanied by a caption generation objective. A large language model (LLM) is appended to the detector, takes both image features and region features from the detector as input, and predicts image-level long detailed captions and region-level short phrases, separately. Compared with previous works that only generate short captions for each object, our LLMDet excels from four aspects: First, long captions provide more details for each object in the image. Long captions with detailed object types, textures, colors, parts of the objects, object actions, precise object locations, and texts are helpful to build rich visionlanguage representations. While existing region-level captions are overly simplistic descriptions for regions. Second, image-level generation aligns all elements in the image as a whole, which models both foreground objects, background and the relations between various objects, providing more information and a more comprehensive understanding of the image beyond object-level caption that only focuses on single regions of interest. Third, image-level captions are more scalable than region-level annotations. Recent off-the-shelf large vision-language models excel at whole image understanding but still struggle with precise regionlevel understanding. With proper prompts, we can get highquality image-level captions from them at a low cost. Further, the fully-pretrained large language model is naturally open-vocabulary. Using an LLM to generate captions makes the detector align with it, thus inheriting a strong generalization ability and significantly increasing rare class performance.</p><p>However, existing grounding datasets lack detailed captions for the whole image. Thus, we first collect a dataset, named GroundingCap-1M, to train LLMDet. Compared with a standard grounding dataset, each element in GroundingCap-1M is formulated as a quadruple, containing an image, a short grounding text, some annotated bounding boxes mapped to the phrases in the grounding text and a long detailed image-level caption. A large language model is utilized to understand region and image features and generate the grounding phrases corresponding to each object and the image-level caption. To effectively integrate the large language model into LLMDet and preserve the pretrained knowledge, we first carefully align the large language model with an existing detector and then finetune them as a whole.</p><p>With the novel training framework, we show that the vision foundation model can benefit from the supervision of LLMs. The supervision not only comes from using LLM-generated captions as labels but also comes from the gradient of co-training. The resulting LLMDet achieves outstanding open-vocabulary performance. Compared with the baseline, LLMDet outperforms 3.3%/3.8%/14.3% AP and 3.1%/3.3%/17.0% AP r with Swin-T/B/L as backbone on LVIS <ref type="bibr" target="#b14">[15]</ref> minival. We also perform a comprehensive zero-shot transfer to various datasets to demonstrate LLMDet's superior performance, as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>By integrating the improved LLMDet with a large language model, we can further build a strong large multimodal model (LMM). Training under the supervision of large language models, LLMDet not only achieves stronger open-vocabulary ability but also pre-aligns with the large language models. Thus the pretrained LLMDet can serve as a strong vision foundation model and in turn build a better LMM, achieving mutual benefits (shown in Appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Open-Vocabulary Object Detection</head><p>In open-vocabulary object detection (OVD), the detector is trained on a limited training dataset but aims to detect arbitrary test-time user-input classes. To detect arbitrary classes, open-vocabulary object detection is formulated as a vision-language task so that the detector can detect classes never seen with class names. Motivated by the impressive zero-shot ability of vision-language models (e.g. CLIP <ref type="bibr" target="#b42">[43]</ref>), aligning detectors with CLIP <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b53">54]</ref> or integrating CLIP as a part of model <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref> are straightforward directions for addressing OVD. However, since CLIP is pretrained with image-level objectives, the features in CLIP are not perfectly suitable for OVD.</p><p>Alternatively, building an object-aware visual-language space with massive data <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b66">66]</ref> from various resources, including image classification datasets <ref type="bibr" target="#b8">[9]</ref>, object detection datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50]</ref>, grounding datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref> and image-text datasets <ref type="bibr" target="#b44">[45]</ref>, has shown impressive results. Further, multi-task learning with other language tasks, such as masked language modeling <ref type="bibr" target="#b62">[62]</ref> and dense captioning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b57">58]</ref> can achieve better vision-language alignment, thus improving the detector's open-vocabulary ability. However, prior works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58]</ref> only focus on generating short phrases for regions of interest. In this work, we explore another co-training task, i.e. generating image-level detailed captions using large language models. Prompt: Create detailed captions describing the contents of the given image as detailed as possible. Include the object types, textures and colors, parts of the objects, object actions, precise object locations, texts, doublechecking relative positions between objects, etc. Instead of describing the imaginary content, only describing the content one can determine confidently from the image. Do not describe the contents by itemizing them in list form. Minimize aesthetic descriptions as much as possible.</p><p>Grounding Text: A young man and mother in the kitchen washing and drying the dishes .</p><p>Caption: In the image, two individuals are in a kitchen. The person on the left is wearing a plaid shirt with red, blue, and white patterns and has short, light-colored hair. This individual is standing near a brick wall, which has a wooden shelf mounted on it. The shelf holds several items, including a pot and some jars. The person on the right is wearing a dark blue t-shirt and is engaged in washing dishes at the kitchen sink. This individual has short, light-colored hair and is holding a clear glass bowl in one hand while using a sponge in the other. The sink area is equipped with a modern faucet and has a black countertop. Various kitchen items, such as a dish soap bottle and a sponge, are visible near the sink. The kitchen cabinets are made of light-colored wood, and one of them has a decorative metal handle. The backsplash behind the sink features a patterned tile design. On the countertop, there is a pot with a lid and some other kitchen utensils. The overall scene suggests a everyday moment in a home kitchen. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Large Vision-Language Model</head><p>Recent large vision-language models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> equip large language models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref> with superior visual perception and understanding ability. A common large visionlanguage model contains three parts: a vision foundation models <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b60">61]</ref> to extract vision tokens, a projector to map vision features to the language space, and a large language model to understand both visual and text input. Recent works <ref type="bibr" target="#b46">[47]</ref> find that a better vision encoder improves the multi-modal performance of the final large vision-language model. But whether the large language model can in turn improve the vision encoder is less explored. In-ternVL <ref type="bibr" target="#b4">[5]</ref> scales up a CLIP-like vision encoder to 6B parameters with a large language model as the text encoder. In this work, we show that the detector can also benefit from large language models and the improved detector can boost the multi-modal performance of the large language model, achieving mutual benefits.</p><p>To train a better large vision-language model, highquality caption data is indispensable <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>. We argue that the quality of captions is also a key factor in training an open-vocabulary detector under the supervision of large language models. Thus, we take advantage of existing high-quality caption datasets and lead large vision-language models to generate high-quality data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GroundingCap-1M Dataset</head><p>Data Formulation. To support LLMDet training under the supervision of grounding loss and captioning loss, we formulate each training sample as a quadruple (I, T g , B, T c ), where I is the image, T g is the short grounding text, B are Dataset Type Caption Source Size COCO <ref type="bibr" target="#b30">[31]</ref> Detection ShareGPT4V <ref type="bibr" target="#b3">[4]</ref>, ASv2 <ref type="bibr" target="#b51">[52]</ref> 210k V3Det <ref type="bibr" target="#b49">[50]</ref> Detection Our Caption (Qwen2-VL-72b <ref type="bibr" target="#b50">[51]</ref>) 166k GoldG <ref type="bibr" target="#b26">[27]</ref> Grounding Our Caption (Qwen2-VL-72b <ref type="bibr" target="#b50">[51]</ref>) 437k LCS <ref type="bibr" target="#b33">[34]</ref> Image-Text LLaVA-OneVision <ref type="bibr" target="#b22">[23]</ref>, ShareGPT4V <ref type="bibr" target="#b3">[4]</ref> 307k GroundingCap-1M 1120k</p><p>Table <ref type="table">1</ref>. Detailed dataset construction of GroundingCap-1M. some annotated bounding boxes each of which is mapped to a phrase in the grounding text, and T c is the detailed caption for the whole image. An example is shown in Figure <ref type="figure" target="#fig_1">2</ref>. Two core principles are followed when collecting detailed captions for the whole image: First, the caption should contain as many details as possible. We expect the caption to describe object types, textures, colors, parts of the objects, object actions, precise object locations and texts in the image so that the caption is information-rich.</p><p>Second, the caption should only contain factual details about the image. Too many imaginary or reasoning captions will reduce the information density or even hamper the model learning. The detailed and information-intensive captions will facilitate highly efficient training.</p><p>Dataset Construction. To save the construction cost, we start from existing datasets with either bounding boxes or detailed captions. Following previous works, the dataset is collected from object detection datasets, grounding datasets and image-text datasets.</p><p>For object detection datasets, we select well-known COCO <ref type="bibr" target="#b30">[31]</ref> and V3Det <ref type="bibr" target="#b49">[50]</ref> datasets. As COCO is widely used in many multi-modal instruction tuning datasets, we can collect its detailed captions from existing assets. Specifically, we collect 168k captions from ShareGPT4V <ref type="bibr" target="#b3">[4]</ref> which is known for detailed captions and 42k captions from In each step, modules in orange color are tunable while modules in blue color are frozen. In the first step, we train a projector to align the detector's features with the LLM so that we can integrate the LLM into the detector without breaking the pretrained features. Then, we train the detector with a standard grounding task and newly introduced captioning tasks in Step 2.</p><p>ASv2 <ref type="bibr" target="#b51">[52]</ref> that mainly focus on object relations. V3Det is a dataset with 13k categories so that it can greatly enlarge the detector's vocabulary and is widely used in many openvocabulary detectors <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b65">65]</ref>. The captions of V3Det are generated by us using Qwen2-VL-72b <ref type="bibr" target="#b50">[51]</ref> with the prompt presented in Figure <ref type="figure" target="#fig_1">2</ref>. Following GLIP <ref type="bibr" target="#b26">[27]</ref>, the grounding text of detection datasets is the concatenation of the class names in the dataset, e.g. "chair. fork. cup. cow."</p><p>For grounding datasets, we choose widely-used GoldG <ref type="bibr" target="#b26">[27]</ref>, which contains GQA <ref type="bibr" target="#b16">[17]</ref> and Flickr30k <ref type="bibr" target="#b41">[42]</ref>. We find that the original annotations have many short grounding texts for each image. To save computation and increase negatives, we merge some grounding texts from the same image without bounding box conflicts into a single grounding text by simple concatenation. After merging, the dataset is down-sampled from 769k to 437k. The detailed caption is also generated by us using Qwen2-VL-72b <ref type="bibr" target="#b50">[51]</ref>.</p><p>For image-text datasets, LCS-558k <ref type="bibr" target="#b33">[34]</ref> with captions from LLaVA-OneVision <ref type="bibr" target="#b22">[23]</ref> and ShareGPT4V <ref type="bibr" target="#b3">[4]</ref> is used. To generate pseudo boxes for images in this dataset, we first parse noun phrases from captions using a traditional language parser and then utilize MM Grounding DINO <ref type="bibr" target="#b65">[65]</ref> (Swin-L) to generate bounding boxes for each phrase. The images with less than three bounding boxes are discarded. The grounding text is the concatenation of phrases in the same image, the same as detection datasets.</p><p>To sum up, the final dataset, GroundingCap-1M, contains 1120k samples, summarised in Table <ref type="table">1</ref>.</p><p>Quality Verification. In the data collection procedure, we carefully select the prompts and use the best model (Qwen2VL-72b) we can access. A lot of work has been done to prevent hallucinations when training this topperformance model. However, it is inevitable that there will be some noise in the dataset. Thus we introduce some postprocessing to clean the dataset. 1) We find that although we prompt the caption model not to describe the imaginary contents, the model still tends to output them but with some obvious words, like "indicating", "suggesting", "possibly". We simply delete the sub-sentences with speculative words. 2) We also design rules to filter out meaningless captions, e.g. "In the image, a man a man a man...(repeating)" or "Sorry, I can not answer the question." 3) To ensure the captions are rich with details, we use Qwen2VL-72b to re-generate captions for images whose first-time generated caption is less than 100 tokens. The double-check mechanism ensures the quality of the dataset. After postprocessing, each caption has around 115 words on average. Figure <ref type="figure" target="#fig_1">2</ref> shows an example from the GroundingCap-1M dataset. More examples can be found in Appendix. Some quantitive analyses are shown in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training LLMDet under the Supervision of Large Language Models</head><p>Unifying the grounding task and some other language tasks can enrich vision features with language knowledge thus broadening vision concepts and achieving better visionlanguage alignment. Prior works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58]</ref> mainly focus on dense captioning, in which the language model is designed to generate short captions or class names to describe the single region of interest. However, the details of single objects, the relations between objects and more information about foreground and background are overlooked but this information can be depicted in a single detailed imagelevel caption. In this work, we show that the region-level open-vocabulary object detector can also benefit from long detailed image-level captions under the supervision of large language models. The overall pipeline is shown in Figure <ref type="figure" target="#fig_2">3</ref>. Specifically, we utilize a large language model (LLM) to generate captions based on a pretrained DETR-based openvocabulary detector. Since the detector and the LLM are pretrained separately, we first train a projector to map the vision features from the detector to the LLM's input space following common practices in training large multi-modal models. We take the p5 feature map from the detector's encoder as the LLM's input and the LLM is asked to generate the full image captions under the supervision of language modeling loss. Only the projector is tunable during this step (Step 1 in Figure <ref type="figure" target="#fig_2">3</ref>).</p><p>After pre-alignment, the detector, the projector and the LLM are finetuned in an end-to-end manner (Step 2 in Figure <ref type="figure" target="#fig_2">3</ref>). Except for the original grounding task, including the word-region alignment loss L align and the box regression loss L box , we also introduce two tasks: image-level caption generation and region-level caption generation. Details are a standard open-vocabulary detector and a large language model (LLM) and is trained under both grounding loss and language modeling loss. The LLM is designed to generate both image-level captions using feature maps as visual input and region-level captions using a single object query as visual input, which are separated by different prompts. Only vision tokens in region-level generation pass through the cross-attention (CA) modules in LLM, which is highlighted by a dashed boundary. Since the number of tokens in image-level and region-level generation varies greatly, we forward the LLM twice separately to save memory and computation. The LLM can be discarded in the inference time so that there is no extra cost. illustrated in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>In image-level caption generation task, the language model takes the feature maps from the detector as visual inputs and outputs the corresponding long detailed captions annotated in GroundingCap-1M. Following the common practices in training a large multi-modal model, we organize the input data of LLM in the conversation format, which includes system messages, user inputs, and answers. The user inputs contain the vision features from the detector and the prompt, e.g. Describe the image in detail. And the answers are the captions from GroundingCap-1M. The LLM aims to output the answers based on the user inputs under the supervision of the standard language modeling loss L image lm . Since the output answers include various details and the comprehensive understanding of the image, these visual cues should be modeled in the visual features so that the LLM can minimize the training loss and generate the captions correctly.</p><p>However, since the LLM takes the whole feature map as input in image-level caption generation, it is hard for LLM to map the entities in the image-level captions back to a specific region in the whole image. For example, in Figure <ref type="figure" target="#fig_1">2</ref>, the "dishes" is only a small part of the image and there are many dish-like objects in the image. Thus, we further introduce the region-level caption generation task as compensation, which introduces a prior for LLM to map the region with the corresponding word. In this task, we select the positive object queries from the detector, which are queries matched to ground truth boxes in the label as-signment, and use the LLM to generate their corresponding grounding phrases separately, such as "young man", "mother" and "dishes" in the Figure <ref type="figure" target="#fig_3">4</ref>. Similar to imagelevel generation, the inputs of LLM are also formatted in conversations but with a different prompt to separate different types of inputs, i.e. Describe the region in a phrase. As the visual feature in a single object query is limited, we add some cross-attention layers in the LLM for object queries to gather necessary information from the detector's feature maps. Note that the text tokens and visual tokens in image-level generation do not pass through these crossattention layers and these layers are trained from scratch. By outputting the corresponding phrases for object queries, the LLM can match the entities to a specific region exactly.</p><p>The overall training objective of LLMDet is the combination of the grounding loss and the generation losses:</p><formula xml:id="formula_0">L = L align + L box + L image lm + L region lm (1)</formula><p>where L region lm is region-level caption generation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>In this work, we select MM Grounding DINO <ref type="bibr" target="#b65">[65]</ref> (MM-GDINO for short in the following) as the baseline model since it is fully open-sourced and enjoys SOTA performance. We simply reload their pretrained checkpoint and finetune the model with our GroundingCap-1M dataset un-  <ref type="bibr" target="#b7">[8]</ref> on LVIS val <ref type="bibr" target="#b14">[15]</ref> and minival <ref type="bibr" target="#b19">[20]</ref>. LLMDet achieves state-of-the-art performance with much less data.</p><p>Model Backbone ODinW13 ODinW35 MDETR <ref type="bibr" target="#b19">[20]</ref> ENB5 -10.7 T-Rex2 <ref type="bibr" target="#b18">[19]</ref> Swin-T -18.0 OWL-ViT <ref type="bibr" target="#b40">[41]</ref> ViT L/14(CLIP) 53.0 18.8 GLIP <ref type="bibr" target="#b26">[27]</ref> Swin-T 46.5 19.6 GLIPv2 <ref type="bibr" target="#b62">[62]</ref> Swin-T 48.5 22.3 DetCLIP <ref type="bibr" target="#b55">[56]</ref> Swin-T 43.3 -Grounding-DINO <ref type="bibr" target="#b35">[36]</ref> Swin-T 51.4 22.7 MM-GDINO <ref type="bibr" target="#b65">[65]</ref> Swin-T 52.5 23.1 LLMDet Swin-T 52.1 23.8</p><p>Table <ref type="table">3</ref>. Zero-shot transfer on ODinW <ref type="bibr" target="#b23">[24]</ref>.</p><p>der the supervision of both the grounding loss and the caption generation loss. Note that a large part of images in GroundingCap-1M is the same as the pretraining datasets used in MM-GDINO, such as GoldG <ref type="bibr" target="#b26">[27]</ref> and V3Det <ref type="bibr" target="#b49">[50]</ref>.</p><p>Since MM-GDINO is fully-pretrained, the vision backbone of MM-GDINO is frozen during training. The large language model is initialized from LLaVA-OneVision-0.5bov <ref type="bibr" target="#b22">[23]</ref>. To save memory and improve training efficiency, we set the maximum token length for the image-level generation as 1600 and the one for region-level generation as 40. The maximum number of regions for caption generation per image is limited to 16. For image-level visual input, we use the p4 and p5 feature maps from the detector's encoder. We resize p4 to 27 × 27 and p5 to 20 × 20 and concatenate them as a single token sequence. We implement LLMDet in MMDetection <ref type="bibr" target="#b2">[3]</ref> using automatic mixed-precision and gra- dient checkpointing and train it for 150k iterations (around two epochs) with batch size 16, which can be done around two days on eight NVIDIA L20 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Zero-Shot Detection Transfer Ability</head><p>To demonstrate the great open-vocabulary ability of LLMDet, we select a wide range of benchmarks, including LVIS <ref type="bibr" target="#b14">[15]</ref>, ODinW13/35 <ref type="bibr" target="#b23">[24]</ref>, COCO-O <ref type="bibr" target="#b39">[40]</ref>, Re-fCOCO <ref type="bibr" target="#b20">[21]</ref>, RefCOCO+ <ref type="bibr" target="#b58">[59]</ref>, RefCOCOg <ref type="bibr" target="#b38">[39]</ref>, gRef-COCO <ref type="bibr" target="#b32">[33]</ref> and perform zero-shot testing on them. Since we use COCO <ref type="bibr" target="#b30">[31]</ref>  Table 6. Ablations on main components. hension datasets. Referring expression comprehension (REC) is a task to localize the objects referred by phrases, which needs comprehensive language understanding and fine-grained vision-language alignment. By co-training with LLMs using detailed captions, LLMDet can model rich visual details with enriched vision-language alignment. Thus LLMDet outperforms the baseline MM-GDINO on various REC datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>In this subsection, experiments are conducted on the Swin-T backbone and report the performance on LVIS minival. Visualizations can be found in Appendix.</p><p>Effect of the main components of LLMDet. In this work, we collect a new dataset GroundingCap-1M, which contains both grounding annotations and detailed long captions for each image. As shown in Table <ref type="table">6</ref>, finetuning with only grounding annotations can boost the performance from 41.4% AP to 43.8% AP. We also show that with only region-level generation, the performance can not be improved since the region-level captions in LLMDet are just the class names or grounding phrases of regions, which do not provide extra information. Simply using image-level generation can slightly improve the performance. As explained in Section 4, the LLM may find it hard to map the entities back to a specific object from a whole image. Thus, combining both image-level and region-level generation can fully unleash the benefit of the LLM's supervision signals.</p><p>And the rich vision-language representations learned from detailed captions significantly improve 3.9% AP r (Row 2 vs Row 5), showing that the fine-grained visual representations are helpful in recognizing rare classes.</p><p>Effect of different large language models. By default, we use the LLM in LLaVA-OneVision-0.5b-ov <ref type="bibr" target="#b22">[23]</ref>, which is Table 8. Ablations on the quality of generated captions. Detailedness (Det) and hallucination (Hul) scores are measured by GPT-4o ranging from 0 to 5. Len is the average length of captions.</p><p>finetuned from Qwen2-0.5b-instruct <ref type="bibr" target="#b54">[55]</ref>. Since the LLM in LLaVA-OneVision-0.5b-ov is pretrained with abundant multi-modal data but with a different vision encoder, the pretraining can still improve the performance, especially for rare classes (+2.2% AP r ), as shown in Table <ref type="table" target="#tab_4">7</ref>. But we find that increasing the size of the LLM only slightly improves the performance, perhaps larger language models mainly improve in reasoning ability which does not benefit the detector's visual representations.</p><p>Effect of generated captions' quality. As shown in Table 8. We first replace the captions generated by Qwen2VL-72b with the ones from LLaVA-Onevision-7B, including captions in V3Det, GoldG and part of LCS. The performance significantly decreases by 0.9% AP and 5.1% AP r . We further replace our generated captions with COCO captions, LCS captions from LLaVA, and short grounding texts in GoldG. The performance further decreases by 0.4% AP.</p><p>To directly compare the detailedness and the degree of hallucinations in generated captions, we randomly sample 100 captions from each part of the datasets (detection, grounding, and image-text pair), a total of 300 captions per experiment. And we utilize GPT-4o <ref type="bibr" target="#b17">[18]</ref>  we utilize a post-processing procedure to delete the subsentences with speculative words. If we do not delete them, the performance drops to 44.2% AP and 35.0% AP r , showing that hallucinations may significantly affect rare class performance.</p><p>Effect of the cross-attention layers in LLM. In LLMDet, visual tokens in region-level generation pass through crossattention layers, while text tokens and visual tokens in image-level generation do not pass through them. We ablate the design in the third and fourth rows of Table <ref type="table" target="#tab_6">9</ref>.</p><p>Visual tokens in the region-level generation are single object queries that contain little visual information. If object queries do not gain necessary information from the detector's encoder feature maps through cross-attention, the performance will degrade to 44.0% AP. We further find that using cross-attention in image-level generation is not helpful as we use the whole feature maps in image-level generation.</p><p>Effect of pretraining the projector before end-to-end finetuning. LLMDet improves the rare class performance by pursuing fine-grained vision-language alignment through co-training with LLMs using high-quality captions.</p><p>Since the LLM and the detector are pretrained separately, pretraining the projector makes their feature space aligned while preserving the pretrained knowledge. Without pretraining the projector, it affects the alignment and decreases the rare class AP (-3.5% AP r ), as shown in the last row of Table <ref type="table" target="#tab_6">9</ref>. As frequent classes have abundant annotations, the negative impacts can be alleviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we explore a new training objective to boost the performance of existing open-vocabulary detectors. By utilizing a large language model to generate both imagelevel detailed captions and region-level coarse grounding phrases, the detector receives more information and a more comprehensive understanding of the image from the detailed captions and builds rich vision-language representations. The resulting detector, LLMDet, achieves state-ofthe-art performance across a wide range of benchmarks. We also show that the improved LLMDet can in turn build a strong large multi-modal model, achieving mutual benefits. We hope our work can provide insights into enhancing vision models with top-performed large language models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LLMDet Builds a Stronger Large Vision-Language Model</head><p>In this subsection, we show that LLMDet can serve as a general vision foundation model and in turn gets a strong large multi-modal model. Recent large multi-modal models (LMM) are based on pretrained large language models and pretrained vision foundation models. Different vision foundation models will significantly affect the performance of LMMs <ref type="bibr" target="#b67">[67]</ref>. Since LLMDet is enhanced under the supervision of long detailed image-level captions and prealigned with LLM, LLMDet inherits great potential to build a stronger LMM. Following recent advances <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b67">67]</ref>, we build the LMM using a mixture of vision experts, i.e. a SigLIP <ref type="bibr" target="#b60">[61]</ref> vision encoder and our LLMDet. As shown in Figure A-1, the visual features from two vision encoders are concatenated along the channel dimension, and then a projector is utilized to map the features to the LLM's input space. We start from LLaVA-OneVision-0.5b-ov <ref type="bibr" target="#b22">[23]</ref> and insert our LLMDet to it as shown in Figure A-1. We first pretrain a new projector and then finetune the LLM with the LLaVA 1.5 <ref type="bibr" target="#b34">[35]</ref> instruction tuning dataset which is only a small part of the dataset used in LLaVA-Onevision. We select three representative benchmarks to evaluate the multi-modal performance of the LMM: the comprehensive understanding benchmark MME <ref type="bibr" target="#b10">[11]</ref>, the hallucination benchmark POPE <ref type="bibr" target="#b27">[28]</ref> and the academic VQA benchmark GQA <ref type="bibr" target="#b16">[17]</ref>. As shown in</p><p>Table A-1, combining the MM-GDINO to LLaVA-OneVision-0.5b-ov can improve the performance on GQA and POPE. As detectors excel at localiz-Method GQA POPE [28] MME [11] [17] rand pop adv perception cognition OneVision-0.5b 56.9 87.5 86.3 85.0 1238 240 OneVision-0.5b 61.2 88.9 88.1 86.6 1207 256 +MM-GDINO OneVision-0.5b +LLMDet 61.2 88.8 88.0 86.0 1297 264</p><p>Table A-1. Multi-modal performance using different vision encoders. OneVision-0.5b is short for LLaVA-OneVision-0.5bov <ref type="bibr" target="#b22">[23]</ref>.</p><p>ing objects in the image, the precise localization makes the LLM aware of the objects existed in images, which helps the LLM overcome hallucination and perform simple QA about objects in the image. The multi-modal perception and understanding ability can be further enhanced with a stronger LLMDet which is also pre-aligned <ref type="bibr" target="#b45">[46]</ref> with the LLM in LLaVA-OneVision-0.5b-ov. The resulting LMM achieves the highest performance on the MME benchmark, validating the mutual benefits between the detector and the LMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Limitations</head><p>Although we provide detailed captions to train LLMs, we find that the LLM co-trained with detectors tends to output relatively short descriptions for the whole image, even given the prompts to describe the image in detail. We suppose the reason is that our region-level data is far more than the image-level data (one image has multiple regions). Further, our region-level descriptions are too simple as they are just the grounding phrases of the regions. We believe collecting some high-informative data for regions like DetCLIPv3 can further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implement Details of Zero-Shot Test on Referring Expression Comprehension Datasets</head><p>In this work, LLMDet is trained with phrase grounding loss and caption generation loss. In the phrase grounding task, the model is asked to detect each phrase in the given grounding text. For example, the model is expected to detect "the man" and "umbrella" in the text "the man with an umbrella".</p><p>To demonstrate the great open-vocabulary ability of LLMDet, we directly transfer LLMDet to the referring expression comprehension (REC) task, which is a task slightly different from the phrase grounding task. In REC, the model should only detect the single object referred by the given sentence. For example, the model should only detect "the man" in the text "the man with an umbrella", which means discrepancies exist between the pretraining task and the target task. Thus, we find that the model tends to predict the "umbrella" with the highest confidence. To minimize the discrepancies, we first use NLTK <ref type="bibr" target="#b0">[1]</ref> tools to find the subject in the text and then select the box with the highest confidence corresponding to the subject as the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Prompts for Calculating Detailedness and Hallucination Scores</head><p>In Table <ref type="table">8</ref>, we utilize GPT-4o as a judge to give a comprehensive score for each caption-image pair. We referred to HalluciDoctor <ref type="bibr" target="#b59">[60]</ref> and adopted similar prompts as follows.</p><p>The prompt for calculating hallucination scores Suppose you are a hallucination annotator who judges the degree of hallucination based on the number of errors in the description of objects, relations, and attributes. You should check each sentence in the description one by one.</p><p>{image} Please carefully compare the image and the given caption below and provide the hallucination score (an integer value between 0 and 5) based on overall hallucinations in each sub-sentence, where the fewer descriptive errors in the caption, the lower the hallucination score given. Only output the score without any explanation. Description: {caption} Output:</p><p>The prompt for calculating detailedness scores Suppose you are an image detail annotator who judges the degree of sentence detailedness based on the object types, textures and colors, parts of the objects, object actions, precise object locations, and texts.</p><p>{image} Please carefully compare the image and the given caption below and provide the detailedness score (an integer value between 0 and 5) without any explanation, where caption with more factual content give a higher detailedness score. Only output the score without any explanation. Description: {caption} Output:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Detailed Zero-Shot Results</head><p>Detailed zero-shot results on ODinW35.</p><p>Table E-2 lists the detailed performance of Grounding-DINO-T [36], MM-GDINO-T [65], and our LLMDet on each dataset in ODinW35 [24]. The selected datasets in ODinW13 are also marked out. Detailed zero-shot results on COCO-O. COCO-O [40] is a dataset sharing the same 80 classes as COCO but in different domains including cartoon, handmake, painting, sketch, tattoo, and weather. Detailed performance on each domain is listed in Table E-3. F. Visualization F.1. Visualizations of the Image-Level Captions in GroundingCap-1M In this work, we collect a new GroundingCap-1M dataset which equips a standard grounding dataset with detailed image-level captions. The captions should contain as many details as possible, including object types, textures, colors, parts of the objects, object actions, precise object locations, and texts. And the captions should not contain imaginary contents. Figure F-2 visualizes some examples in GroundingCap-1M. The captions shown depict the main entities in the pictures with great detail (demonstrated in green color</p><p>) but also with some imaginary contents inevitably (also highlighted by underlines). The imaginary contents always start with speculative words, like "seemingly", "indicating", and "suggesting". We just find these pre-defined speculative words and delete the sub-sentences including them in an online manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Visualizations of the Captions Generated by LLMDet</head><p>In Figure F-3, we visualize some examples of the generated image-level and region-level captions from the LLM co-trained with LLMDet. Images are selected from the COCO validation set. The LLM can generate precise class names for the objects in COCO (as we use the class names in COCO as the grounding text for deep fusion, only objects in COCO are detected out for caption generation). But we find that the image-level captions are relatively coarsegrained compared with the ones in GroundingCap-1M. We suppose the reason is that our region-level data is far more than the image-level data (one image has multiple regions) and the region-level data is overly simplistic. Dataset ODinW13 ODinW35 G-DINO-T MM-GDINO-T LLMDet AerialMaritimeDrone large ✓ ✓ 0.173 0.155 0.153 AerialMaritimeDrone tiled ✓ 0.206 0.201 0.174 AmericanSignLanguageLetters ✓ 0.002 0.007 0.016 Aquarium ✓ ✓ 0.195 0.281 0.268 BCCD ✓ 0.161 0.078 0.149 boggleBoards ✓ 0.000 0.002 0.001 brackishUnderwater ✓ 0.021 0.024 0.026 ChessPieces ✓ 0.000 0.000 0.000 CottontailRabbits ✓ ✓ 0.806 0.788 0.797 dice ✓ 0.004 0.001 0.004 DroneControl ✓ 0.042 0.073 0.070 EgoHands generic ✓ ✓ 0.608 0.518 0.518 EgoHands specific ✓ 0.002 0.003 0.010 HardHatWorkers ✓ 0.046 0.109 0.178 MaskWearing ✓ 0.004 0.009 0.004 MountainDewCommercial ✓ 0.430 0.433 0.518 NorthAmericaMushrooms ✓ ✓ 0.471 0.747 0.749 openPoetryVision ✓ 0.000 0.000 0.003 OxfordPets by breed ✓ 0.003 0.004 0.006 OxfordPets by species ✓ 0.011 0.016 0.024 PKLot ✓ 0.001 0.007 0.034 Packages ✓ ✓ 0.695 0.706 0.717 PascalVOC ✓ ✓ 0.563 0.566 0.584 pistols ✓ ✓ 0.726 0.726 0.720 plantdoc ✓ 0.005 0.011 0.005 pothole ✓ ✓ 0.215 0.164 0.175 Raccoons ✓ ✓ 0.549 0.533 0.519 selfdrivingCar ✓ 0.089 0.082 0.083 ShellfishOpenImages ✓ ✓ 0.393 0.489 0.429 ThermalCheetah ✓ 0.087 0.045 0.132 thermalDogsAndPeople ✓ ✓ 0.657 0.548 0.546 UnoCards ✓ 0.006 0.005 0.010 VehiclesOpenImages ✓ ✓ 0.613 0.610 0.597 WildfireSmoke ✓ 0.134 0.129 0.093 websiteScreenshots ✓ 0.012 0.016 0.013 ODinW13 Average 0.514 0.525 0.521 ODinW35 Average 0.227 0.231 0.238 Table E-2. Detailed zero-shot results on ODinW35 [24]. Model Cartoon Handmake Painting Sketch Tattoo Weather Average Grounding-DINO [36] 40.2 30.2 43.1 37.6 29.8 44.8 37.6 MM-GDINO [65] 35.0 26.6 41.7 32.2 23.9 44.8 34.0 LLMDet 37.7 30.7 42.8 32.6 27.5 45.3 36.1 Table E-3. Detailed zero-shot results on COCO-O [40].</p><p>The image depicts a sushi conveyor belt restaurant. A long, metallic conveyor belt runs horizontally across the image, carrying various dishes on red plates. The dishes include different types of sushi, sashimi, and other Japanese cuisine items. The conveyor belt is situated in the middle of the image, with diners seated on either side. On the left side of the image, a person is standing, wearing a white shirt. This person appears to be a server or a customer, and they are holding a plate with a piece of sushi on it. On the right side of the image, another person is seated at the conveyor belt. This individual is wearing a light-colored shirt and is holding a newspaper, seemingly reading it while waiting for their food. The person is seated on a red stool, which is typical in such restaurants.</p><p>The overall setting suggests a casual dining environment with a focus on convenience and variety.</p><p>In the image, a young girl wearing a pink bathing suit is standing on a sandy beach. She has her right arm extended outward as if she's either throwing something or gesturing towards something in front of her. The girl is barefoot and looking towards the ocean. A seagull is captured mid-flight above the water, with its wings spread wide. The bird is positioned slightly to the left of the girl and appears to be flying parallel to the shoreline. The ocean itself is visible in the background, with gentle waves lapping at the shore.</p><p>The overall scene suggests a sunny day at the beach, with the girl enjoying her time near the water.</p><p>In the image, there is a woman wearing a blue raincoat and hat, walking on a wet sidewalk. She is using a white cane for support as she navigates the slippery surface. The woman appears to be elderly and is dressed appropriately for the weather conditions. Nearby, there is a metal pole with a unicycle leaning against it. The unicycle has a black seat and large wheels, indicating that it is designed for stability and balance. The wet sidewalk suggests recent rainfall, which could make the surface slippery and challenging to walk on. The relative position of the objects shows the woman walking past the unicycle, which is positioned closer to the camera than the woman.</p><p>The image depicts a computer desk setup with various objects. The primary focus is on the keyboard, which is prominently placed in the foreground. It is a standard QWERTY keyboard with white keys and black lettering. Behind the keyboard, there is a computer monitor displaying an image of a pink heart with a Hello Kitty design inside it. The monitor is positioned slightly to the left of center. To the left of the keyboard, there is a telephone with a corded handset. The phone has a traditional design with a pushbutton keypad. On the right side of the keyboard, there are several small figurines or toys, including what appears to be a panda bear and other characters. These toys are arranged in a somewhat cluttered manner, suggesting that they might be personal items or collectibles. In the background, there is a printer situated on the right side of the desk. The printer is a standalone unit with a paper tray visible at the top. There is also a window with blinds partially drawn, allowing some natural light to enter the room. The overall setting suggests a home office or personal workspace, with a mix of work-related equipment and personal items. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. LLMDet achieves superior zero-shot performance across various benchmarks compared with other well-known counterparts. All detectors use Swin-T as the backbone.</figDesc><graphic coords="1,317.25,301.45,236.24,186.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An example of GroundingCap-1M. Bounding box annotations are discarded for clarity. Compared with original short grounding texts, the detailed captions in GroundingCap-1M are rich in object types, textures, colors, parts of the objects, object actions, precise object locations and texts. Each caption in GroundingCap-1M has around 115 words on average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. The multi-step training pipeline of LLMDet. In each step, modules in orange color are tunable while modules in blue color are frozen. In the first step, we train a projector to align the detector's features with the LLM so that we can integrate the LLM into the detector without breaking the pretrained features. Then, we train the detector with a standard grounding task and newly introduced captioning tasks in Step 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The overview of LLMDet. LLMDet contains a standard open-vocabulary detector and a large language model (LLM) and is trained under both grounding loss and language modeling loss. The LLM is designed to generate both image-level captions using feature</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A- 1 .</head><label>1</label><figDesc>Figure A-1. The multi-step training pipeline of using LLMDet to build a strong large multi-modal model. The large multi-modal model uses a mixture of vision encoders, including LLMDet and SigLIP. In each step, modules in orange color are tunable while modules in blue color are frozen. We first pretrain a new projector and then finetune the large multi-modal model with visual instruct tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure F- 2 .</head><label>2</label><figDesc>Figure F-2. Visualizations of the image-level captions in GroundingCap-1M, which are rich in detail. The great details are marked in green color. But the captions still contain some imaginary contents, which are also highlighted by underlines.</figDesc><graphic coords="15,86.64,497.63,200.89,133.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Zero-shot fixed AP</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Pre-training data</cell><cell cols="4">LVIS minival AP AP r AP c AP f</cell><cell cols="4">LVIS AP AP r AP c AP f</cell></row><row><cell>GLIP [27]</cell><cell>Swin-T</cell><cell>O365,GoldG,Cap4M</cell><cell cols="8">26.0 20.8 21.4 31.0 17.2 10.1 12.5 25.2</cell></row><row><cell>GLIPv2 [62]</cell><cell>Swin-T</cell><cell>O365,GoldG,Cap4M</cell><cell>29.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CapDet [38]</cell><cell>Swin-T</cell><cell>O365,VG</cell><cell cols="4">33.8 29.6 32.8 35.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Grounding-DINO [36] Swin-T</cell><cell>O365,GoldG,Cap4M</cell><cell cols="8">27.4 18.1 23.3 32.7 20.1 10.1 15.3 29.9</cell></row><row><cell>OWL-ST [41]</cell><cell>CLIP B/16</cell><cell>WebLI2B</cell><cell cols="2">34.4 38.3</cell><cell>-</cell><cell>-</cell><cell cols="2">28.6 30.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Desco-GLIP [25]</cell><cell>Swin-T</cell><cell>O365,GoldG,CC3M</cell><cell cols="8">34.6 30.8 30.5 39.0 26.2 19.6 22.0 33.6</cell></row><row><cell>DetCLIP [56]</cell><cell>Swin-T</cell><cell>O365,GoldG,YFCC1M</cell><cell cols="8">35.9 33.2 35.7 36.4 28.4 25.0 27.0 28.4</cell></row><row><cell>DetCLIPv2 [57]</cell><cell>Swin-T</cell><cell>O365,GoldG,CC15M</cell><cell cols="8">40.4 36.0 41.7 40.4 32.8 31.0 31.7 34.8</cell></row><row><cell>DetCLIPv3 [58]</cell><cell>Swin-T</cell><cell>O365,V3Det,GoldG,GranuCap50M</cell><cell cols="8">47.0 45.1 47.7 46.7 38.9 37.2 37.5 41.2</cell></row><row><cell>YOLO-World-L [6]</cell><cell cols="2">YOLOv8-L O365,GoldG,CC3M</cell><cell cols="4">35.4 27.6 34.1 38.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>T-Rex2 [19]</cell><cell>Swin-T</cell><cell>10M data from various resources</cell><cell cols="8">42.8 37.4 39.7 46.5 34.8 29.0 31.5 41.2</cell></row><row><cell>OV-DINO [49]</cell><cell>Swin-T</cell><cell>O365,GoldG,CC1M</cell><cell cols="8">40.1 34.5 39.5 41.5 32.9 29.1 30.4 37.4</cell></row><row><cell>MM-GDINO [65]</cell><cell>Swin-T</cell><cell>O365,GoldG,GRIT,V3Det</cell><cell cols="8">41.4 34.2 37.4 46.2 31.9 23.6 27.6 40.5</cell></row><row><cell>LLMDet</cell><cell>Swin-T</cell><cell>GroundingCap-1M</cell><cell cols="8">44.7 37.3 39.5 50.7 34.9 26.0 30.1 44.3</cell></row><row><cell>GLIP [27]</cell><cell>Swin-L</cell><cell>FourODs,GoldG,Cap24M</cell><cell cols="8">37.3 28.2 34.3 41.5 26.9 17.1 23.3 36.4</cell></row><row><cell>GLIPv2 [62]</cell><cell>Swin-H</cell><cell>FiveODs,GoldG,CC15M,SBU</cell><cell>50.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Grounding-DINO [36] Swin-L</cell><cell cols="5">O365,OI,GoldG,Cap4M,COCO,RefC 33.9 22.2 30.7 38.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OWL-ST [41]</cell><cell>CLIP L/14</cell><cell>WebLI2B</cell><cell cols="2">40.9 41.5</cell><cell>-</cell><cell>-</cell><cell cols="2">35.2 36.2</cell><cell>-</cell><cell>-</cell></row><row><cell>DetCLIP [56]</cell><cell>Swin-L</cell><cell>O365,GoldG,YFCC1M</cell><cell cols="8">38.6 36.0 38.3 39.3 28.4 25.0 27.0 31.6</cell></row><row><cell>DetCLIPv2 [57]</cell><cell>Swin-L</cell><cell>O365,GoldG,CC15M</cell><cell cols="8">44.7 43.1 46.3 43.7 36.6 33.3 36.2 38.5</cell></row><row><cell>DetCLIPv3 [58]</cell><cell>Swin-L</cell><cell>O365,V3Det,GoldG,GranuCap50M</cell><cell cols="8">48.8 49.9 49.7 47.8 41.4 41.4 40.5 42.3</cell></row><row><cell>MM-GDINO [65]</cell><cell>Swin-B</cell><cell>O365,GoldG,V3Det</cell><cell cols="8">44.5 37.5 39.9 49.9 34.9 26.7 30.4 43.5</cell></row><row><cell>MM-GDINO [65]</cell><cell>Swin-L</cell><cell>O365V2,OpenImageV6,GoldG</cell><cell cols="8">36.8 28.1 31.8 42.8 29.1 19.7 25.6 37.2</cell></row><row><cell>LLMDet</cell><cell>Swin-B</cell><cell>GroundingCap-1M</cell><cell cols="8">48.3 40.8 43.1 54.3 38.5 28.2 34.3 47.8</cell></row><row><cell>LLMDet</cell><cell>Swin-L</cell><cell>GroundingCap-1M</cell><cell cols="8">51.1 45.1 46.1 56.6 42.0 31.6 38.8 50.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Distribution shift performance on COCO-O dataset. Gray numbers indicate including COCO data in training.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">COCO [31] COCO-O [40] AP AP</cell><cell>Effective Robustness</cell></row><row><cell>DINO [63]</cell><cell>R50</cell><cell>49.0</cell><cell>22.5</cell><cell>+0.5</cell></row><row><cell>GLIP [27]</cell><cell>Swin-T</cell><cell>46.1</cell><cell>29.0</cell><cell>+8.0</cell></row><row><cell>DetCLIPv3 [58]</cell><cell>Swin-T</cell><cell>47.2</cell><cell>38.5</cell><cell>+17.3</cell></row><row><cell>Grounding-DINO [36]</cell><cell>Swin-T</cell><cell>48.4</cell><cell>37.6</cell><cell>+15.8</cell></row><row><cell>MM-GDINO [65]</cell><cell>Swin-T</cell><cell>50.4</cell><cell>34.0</cell><cell>+11.3</cell></row><row><cell>LLMDet</cell><cell>Swin-T</cell><cell>55.6</cell><cell>36.1</cell><cell>+11.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Zero-shot transfer on common referring expression comprehension datasets. The evaluation metric for RefCOCO, RefCOCO+, and RefCOCOg is the Top-1 accuracy. The evaluation metrics for gRefCOCO are Pr(F1=1, IoU≥0.5) and N-acc.</figDesc><table><row><cell>dataset during training, we carefully re-</cell></row><row><cell>move the images in the RefCOCO/RefCOCO+/RefCOCOg</cell></row><row><cell>validation and test set from GroundingCap-1M following</cell></row></table><note><p>MM-GDINO. The images in the LVIS minival are not overlapped with the training set of COCO so it strictly follows the zero-shot setting. During the test, the LLM is discarded, thus the inference cost is the same as our baseline.</p><p>r on LVIS minival across different backbones. We find that the performance of MM-GDINO with Swin-L<ref type="bibr" target="#b36">[37]</ref> </p><p>as the backbone is extremely low, which is probably due to different pretraining data, especially lacking V3Det. But LLMDet with Swin-L as the backbone still outperforms other SOTA methods with much less training data and achieves 50.6% AP, showing great open-vocabulary capabilities. The same trend can be found on LVIS val. We notice that the Det-CLIP series achieves more balanced performance on different classes, which is probably due to carefully collected and annotated datasets and a well-organized noun concept corpus. We believe LLMDet can also be applied to DetCLIP. performs MM-GDINO by 2.1% AP, showing that LLMDet is more robust to domain shifts. Detailed performance on each domain can be found in Appendix.</p><p>Zero-shot performance on referring expression compre-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Ablations on large language models.</figDesc><table><row><cell cols="2">LLM</cell><cell></cell><cell cols="2">AP AP r AP c AP f</cell></row><row><cell cols="2">Qwen2-0.5b-instruct [55]</cell><cell></cell><cell cols="2">44.4 36.4 39.2 50.5</cell></row><row><cell cols="5">LLaVA-OneVision-0.5b-ov [23] 44.5 38.6 39.3 50.3</cell></row><row><cell cols="2">Qwen2-1.5b-instruct [55]</cell><cell></cell><cell cols="2">44.6 35.3 39.5 50.8</cell></row><row><cell cols="2">Exp Captions</cell><cell cols="3">len Det.↑ Hul.↓ AP APr APc APf</cell></row><row><cell>1</cell><cell>GroundingCap-1M</cell><cell cols="2">115 4.63</cell><cell>1.15 44.5 38.6 39.3 50.3</cell></row><row><cell>2</cell><cell>LLaVA-Onevision-7B generated</cell><cell cols="2">105 4.37</cell><cell>1.34 43.6 33.5 38.0 50.5</cell></row><row><cell>3</cell><cell cols="2">Original captions / grounding texts 30</cell><cell>3.04</cell><cell>0.90 43.2 35.7 37.3 49.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>Use CA in both R-L and I-L generation 44.<ref type="bibr" target="#b3">4</ref> 36.6 39.4 50.2 pipeline Do not pretrain the projector 44.4 35.1 39.8 50.3More ablations on the data, architecture, and pipeline. R-L and I-L denotes region level and image level.</figDesc><table><row><cell>as a judge to give</cell></row><row><cell>a comprehensive score for each caption-image pair. The</cell></row><row><cell>used prompts are shown in Appendix. The captions in</cell></row><row><cell>GroundingCap-1M have the highest detailedness scores and</cell></row><row><cell>moderate hallucinations, validating the superior quality of</cell></row><row><cell>our dataset. As human-annotated captions have fewer hal-</cell></row><row><cell>lucinations (0.90 vs 1.34, LLaVA captions still have hallu-</cell></row><row><cell>cinations), the AP r in Exp 3 is even higher than the one in</cell></row><row><cell>Exp 2.</cell></row><row><cell>Effect of the pretraining data. In this work, we collect</cell></row><row><cell>the GroundingCap-1M dataset. Due to computation con-</cell></row><row><cell>straints, the dataset only contains 1M data, which is much</cell></row><row><cell>less than the datasets used in other open-vocabulary detec-</cell></row><row><cell>tors. As shown in the second row in Table 9, if we do</cell></row><row><cell>not use the LCS dataset in GroundingCap-1M (813k data</cell></row><row><cell>now), the performance significantly reduces to 42.8% AP,</cell></row><row><cell>showing that more pretraining data will further improve</cell></row><row><cell>LLMDet's performance. Further, the image-level captions</cell></row><row><cell>should only contain factual details about the image so that</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the image, a group of people are standing on a street. The image captures a moment in time in an urban setting. It's a snapshot of life in the city. In the background, there are trees and buildings. There are also cars parked on the side of the road. Overall, this image gives you a glimpse into the daily life of a city dweller.</p><p>In the image, there is a television that is on a table. There is also a bookshelf with books on it. The image shows a room with a bed that has a mattress and a pillow. The television is turned on.</p><p>In the image, there is a dining table with a bowl of fruit. The image also shows a kitchen area with cabinets and a refrigerator. There is also a chair visible in the background. Overall, this image gives the impression of a well-organized and inviting kitchen space.</p><p>In the image, there is a toilet that is beside a sink. The walls of the bathroom are white. There is also a towel hanging on the wall. The photo is taken from inside a bathroom.</p><p>In the image, there is a toilet with its lid up. The toilet is white and has a handle on the side of the toilet itself. There is also a small amount of water coming out of it. The image also shows a pipe running along the wall. It is located in the bottom left corner. Overall, this image captures a moment in a bathroom setting.</p><p>In the image, a cat is on a table. There is also a tv in the background of the scene. The image also contains a cup. In the foreground, there is a coffee cup with a spoon in it. It has a handle and a spout. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person.</head><p>Handbag.</p><p>Umbrella.</p><p>Umbrella.</p><p>Person.</p><p>Tv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Couch.</head><p>Book.</p><p>Refrigerator.</p><p>Bottle.</p><p>Orange. Chair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chair.</head><p>Oven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dining table.</head><p>Potted plant.</p><p>Bowl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sink.</head><p>Toilet.</p><p>Person.</p><p>Tv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cat.</head><p>Cup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remote.</head><p>Sink.</p><p>Toilet. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nltk: the natural language toolkit</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions</title>
		<meeting>the COLING/ACL 2006 Interactive Presentation Sessions</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sharegpt4v: Improving large multi-modal models with better captions</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiannan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Yolo-world: Real-time openvocabulary object detection</title>
		<author>
			<persName><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Evaluating large-vocabulary object detectors: The devil is in the details</title>
		<author>
			<persName><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01066</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coarse-to-fine vision-language pre-training with fusion in the backbone</title>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinrui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.13394</idno>
		<title level="m">A comprehensive evaluation benchmark for multimodal large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building strong one-decoder-layer sparse detectors via adaptive sparse anchor generation</title>
		<author>
			<persName><forename type="first">Shenghao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yipeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Asag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Frozen-detr: Enhancing detr with image understanding from frozen foundation models</title>
		<author>
			<persName><forename type="first">Shenghao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qize</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xihan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2019. 2, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004">2019. 2, 4</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">P</forename><surname>Goucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akila</forename><surname>Ostrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Welihinda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><surname>Radford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.21276</idno>
		<title level="m">Gpt-4o system card</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">T-rex2: Towards generic object detection via text-visual prompt synergy</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mdetrmodulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">F-vlm: Open-vocabulary object detection upon frozen vision and language models</title>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anelia</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.03326</idno>
		<title level="m">Llava-onevision: Easy visual task transfer</title>
		<imprint>
			<date type="published" when="2024">2024. 3, 4, 6, 7, 8, 12</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Elevater: A benchmark and toolkit for evaluating language-augmented visual models</title>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning object recognition with rich language descriptions</title>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Desco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distilling detr with visual-linguistic knowledge for open-vocabulary object detection</title>
		<author>
			<persName><forename type="first">Liangqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grounded language-image pre-training</title>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2022. 1, 2, 3, 4, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating object hallucination in large vision-language models</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monkey: Image resolution and text label are important things for large multi-modal models</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingxu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative region-language pretraining for open-ended object detection</title>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gres: Generalized referring expression segmentation</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved baselines with visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grounding dino: Marrying dino with grounded pre-training for open-set object detection</title>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024. 1, 2, 6, 7, 13</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Capdet: Unifying dense captioning and open-world detection pretraining</title>
		<author>
			<persName><forename type="first">Yanxin</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youpeng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengzhen</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2023. 2, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Coco-o: A benchmark for object detectors under natural distribution shifts</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scaling open-vocabulary object detection</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2023</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Bryan A Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Exploring the design space for multimodal llms with mixture of encoders</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijia</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhashree</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><surname>De-An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Humphrey</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.15998</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Cambrian-1: A fully open, vision-centric exploration of multimodal llms</title>
		<author>
			<persName><forename type="first">Shengbang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellis</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Middepogu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charitha</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihan</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adithya</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xichen</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.16860</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Ov-dino: Unified open-vocabulary detection with language-aware selective fusion</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengzhen</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinlong</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyuan</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.07844</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">V3det: Vast vocabulary visual detection dataset</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Qwen2-vl: Enhancing vision-language model&apos;s perception of the world at any resolution</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.12191</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">The all-seeing project v2: Towards general relation comprehension of the open world</title>
		<author>
			<persName><forename type="first">Weiyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiantong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxiang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ECCV, 2024. 3, 4</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Grit: A generative region-to-text transformer for object understanding</title>
		<author>
			<persName><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Aligning bag of regions for openvocabulary object detection</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.10671</idno>
		<title level="m">Qwen2 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Detclip: Dictionary-enriched visual-concept paralleled pretraining for open-world detection</title>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youpeng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006">2022. 1, 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Detclipv2: Scalable open-vocabulary object detection pre-training via wordregion alignment</title>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Detclipv3: Towards versatile generative open-vocabulary object detection</title>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2024. 2, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data</title>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bosheng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Sigmoid loss for language image pre-training</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023. 3, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Glipv2: Unifying localization and vision-language understanding</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006">2022. 1, 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Detr with improved denoising anchor boxes for end-to-end object detection</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><surname>Dino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Generating enhanced negatives for training language-based object detectors</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">An open and comprehensive pipeline for unified object grounding and detection</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haian</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.02361</idno>
		<imprint>
			<date type="published" when="2024">2024. 4, 5, 6, 7, 13</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Detecting twenty-thousand classes using image-level supervision</title>
		<author>
			<persName><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Mova: Adapting mixture of vision experts to multimodal context</title>
		<author>
			<persName><forename type="first">Zhuofan</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dazhong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongzhi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
