<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ULER : A Model-Agnostic Method to Control Generated Length for Large Language Models</title>
				<funder ref="#_fX3zQbq">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_ayzYWqY">
					<orgName type="full">Shenzhen Science and Technology Innovation Program</orgName>
				</funder>
				<funder ref="#_n9wZgqa">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_3ahyEje">
					<orgName type="full">Shenzhen Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_Kb9xhd6">
					<orgName type="full">Natural Science Foundation of Guangdong Province of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-01">1 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaming</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunshui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ziqiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuelin</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Run</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Longze</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
							<email>min.yang@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gemma</forename><surname>Team</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cassidy</forename><surname>Hardin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Dadashi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Surya</forename><surname>Bhupatiraju</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shreya</forename><surname>Pathak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
						</author>
						<author>
							<persName><roleName>Mihir</roleName><forename type="first">Morgane</forename><surname>Rivière</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sanjay</forename><surname>Kale</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Juliette</forename><surname>Love</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pouya</forename><surname>Tafti</surname></persName>
						</author>
						<author>
							<persName><roleName>Pier</roleName><forename type="first">Léonard</forename><surname>Hussenot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Giuseppe</forename><surname>Sessa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Barua</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Botev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Castro- Ros</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ambrose</forename><surname>Slone</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amélie</forename><surname>Héliou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Tac- Chetti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Bulanova</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antonia</forename><surname>Paterson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Beth</forename><surname>Tsai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Charline</forename><forename type="middle">Le</forename><surname>Lan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christo- Pher</forename><forename type="middle">A</forename><surname>Choquette-Choo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Clément</forename><surname>Crepy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Reid</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Noland</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Geng</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
						</author>
						<author>
							<persName><forename type="first">George-Christian</forename><surname>Muraru</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Grigory</forename><surname>Rozhdestvenskiy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Grishchenko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Keeling</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jane</forename><surname>Labanowski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Stanway</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jenny</forename><surname>Bren- Nan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeremy</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johan</forename><surname>Ferret</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Justin</forename><surname>Chiu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Justin</forename><surname>Mao-Jones</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kathy</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Katie</forename><surname>Milli- Can</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lars</forename><forename type="middle">Lowe</forename><surname>Sjoesund</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lisa</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Machel</forename><surname>Reid</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maciej</forename><surname>Mikuła</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mateo</forename><surname>Wirth</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Sharman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nikolai</forename><surname>Chinaev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nithum</forename><surname>Thain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oscar</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oscar</forename><surname>Wahltinez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paige</forename><surname>Bai- Ley</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Petko</forename><surname>Yotov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rahma</forename><surname>Chaabouni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ramona</forename><surname>Comanescu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Reena</forename><surname>Jana</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ross</forename><surname>Mcilroy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ruibo</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Mullins</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sertan</forename><surname>Girgin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sholto</forename><surname>Douglas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shree</forename><surname>Pandya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Soham</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ted</forename><surname>Kli- Menko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vlad</forename><surname>Feinberg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wojciech</forename><surname>Stokowiec</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><forename type="middle">Hui</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zafarali</forename><surname>Ahmed</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhitao</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tris</forename><surname>Warkentin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ludovic</forename><surname>Peran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minh</forename><surname>Giang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Clément</forename><surname>Farabet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joelle</forename><surname>Barral</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eli</forename><surname>Collins</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Evan</forename><surname>Senter</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alek</forename><surname>Andreev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kathleen</forename><forename type="middle">2024</forename><surname>Kenealy</surname></persName>
						</author>
						<author>
							<persName><surname>Gemma</surname></persName>
						</author>
						<title level="a" type="main">ULER : A Model-Agnostic Method to Control Generated Length for Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-01">1 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">082282638138470334414F8DFB4A4AA2</idno>
					<idno type="arXiv">arXiv:2409.18943v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to their inherent difficulty in accurately perceiving numerical constraints. To explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the model's performance in adhering to specified response lengths. Furthermore, we introduce a novel, model-agnostic approach called RULER, which employs Meta Length Tokens (MLTs) to enhance the instruction-following ability of large language models under length-constrained instructions. Specifically, RULER equips LLMs with the ability to generate responses of a specified length based on length constraints within the instructions. Moreover, RULER can automatically generate appropriate MLT when length constraints are not explicitly provided, demonstrating excellent versatility and generalization. Comprehensive experiments show the effectiveness of RULER across different LLMs on Target Length Generation Task, e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In addition, we conduct extensive ablation experiments to further substantiate the efficacy and generalization of RULER. Our code and data is available at <ref type="url" target="https://github.com/Geaming2002/Ruler">https://github.com/Geaming2002/Ruler</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of natural language tasks and are increasingly being utilized in various fields <ref type="bibr" target="#b34">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b10">Devlin et al., 2019;</ref><ref type="bibr" target="#b4">Brown et al., 2020)</ref>. A primary † Min Yang is the corresponding author.</p><p>Query: Tell me how to make a cake in 30 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word count of the answer &gt;50 ≠ 30</head><p>Answer: To make a cake, preheat your oven to 350°F (175°C) and prepare an 8-inch round cake pan by greasing and flouring it. Cream together 1 cup of granulated sugar and 1/2 cup of softened unsalted butter until light and fluffy, which takes about 3-4 minutes with an electric mixer. Add 2 large eggs, one at a time, beat… Failed Figure <ref type="figure">1</ref>: Existing LLMs lack the capability to follow instructions for generating texts of a specified length. area of interest is the instruction following ability, referring to their capability to execute tasks or generate outputs based on instructions <ref type="bibr" target="#b31">(Ouyang et al., 2022;</ref><ref type="bibr">Wei et al., 2022a)</ref>. It reflects the model's effectiveness in understanding and responding to instructions.</p><p>The practical challenges highlight the complexity of achieving precise instruction following, particularly when users require control over the output's length. Users frequently give LLMs various instructions, such as "Tell me how to make a cake in 20 words", "Use 50 words to write a post", "Write a 300-word story for me" and so on. These instructions challenge the instruction following capability of LLMs. To explore how well LLMs handle such challenges, we focus on the scenario where users specify the target length of the responses. The question is posed, "Can LLMs accurately generate with target length?" and introduce the Target Length Generation Task (TLG). We create a test dataset with various target lengths and introduce two evaluation metrics: Precise Match (PM) and Flexible Match (FM). Our findings reveal that current LLMs generally perform poorly in this task, indicating considerable room for improvement. A discussion on the underlying causes is conducted, primarily attributing it to tokenization schemes and model training strategy.</p><p>To address aforementioned issues, we introduce RULER, a model-agnostic approach designed to enhance the instruction-following capability of LLMs through Meta Length Tokens (MLTs). MLTs are designed to control model's responses. By utilizing RULER, LLMs can generate responses that meet target lengths. We create a dataset with MLTs D M LT for end-to-end training of LLMs. LLMs learn to generate MLT and the corresponding length response after training. During inference, if a target length is provided, RULER can transform it into a MLT and generate responses that meet the requirement. If no target length is specified, it first generates a MLT, then the response, ensuring its length aligns with the generated MLT.</p><p>We apply RULER to various large language models and test them on TLG. Each model demonstrates significant improvements. Across all evaluated models, we observe a consistent improvement in both PM and FM scores at all Levels. The PM and FM scores across All Level showed an average improvement of 27.97 and 29.57.Furthermore, to rigorously test the capabilities of RULER, we randomly sample the dataset provided by <ref type="bibr">Li et al. (2024a)</ref> and assess RULER on multi MLT generation and self-generated MLT experiment to show the its effectiveness and generalizability. Additionally, RULER is tested on six benchmarks to observe whether the models' overall performance is affected.</p><p>Our contributions can be summarized as follows:</p><p>• We introduce the Target Length Generation Task (TLG), which designed to assess the instruction following capability of LLMs. It evaluates how well models generate responses of target lengths as directed by instructions.</p><p>• We propose RULER, a novel and modelagnostic approach which employs the Meta Length Tokens (MLTs). Through end-to-end training, it enables models to generate response matching the target lengths indicated by MLTs.</p><p>• We demonstrate that RULER significantly enhances the performance of various models on the TLG. Further experiments have also validated the effectiveness and generalizability of RULER.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Large Language Model</head><p>The advent of LLMs has revolutionized the field of natural language processing and become a milestone <ref type="bibr" target="#b34">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b10">Devlin et al., 2019;</ref><ref type="bibr" target="#b4">Brown et al., 2020;</ref><ref type="bibr">Zhang et al., 2023a)</ref>. Large language models have achieved success across various NLP tasks. Models such as <ref type="bibr">GPT-4(Achiam et al., 2023)</ref>, Llama-3(AI@Meta, 2024), and Qwen <ref type="bibr" target="#b3">(Bai et al., 2023)</ref>, known for their powerful capabilities, are increasingly serving as the foundation for various applications and making significant inroads into diverse fields, exerting a substantial impact.</p><p>In-context learning enables LLMs to infer and generate responses solely based on the contextual information provided within a prompt <ref type="bibr" target="#b11">(Dong et al., 2022;</ref><ref type="bibr">Wei et al., 2022b)</ref>. This capability allows the models to exhibit a high degree of flexibility and adaptability across a variety of tasks <ref type="bibr">(Levine et al., 2022;</ref><ref type="bibr" target="#b6">Chen et al., 2022;</ref><ref type="bibr" target="#b43">Zhao et al., 2021)</ref>.</p><p>CoT further excavates and demonstrates the powerful logical reasoning capabilities of LLMs <ref type="bibr">(Wei et al., 2022c;</ref><ref type="bibr" target="#b17">Huang and Chang, 2023;</ref><ref type="bibr">Zhang et al., 2023b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instruction Following</head><p>Instruction following refers to the ability of large language models to comprehend and execute given natural language instructions <ref type="bibr" target="#b4">(Brown et al., 2020;</ref><ref type="bibr" target="#b31">Ouyang et al., 2022;</ref><ref type="bibr">Wei et al., 2022a;</ref><ref type="bibr">Zhou et al., 2023a)</ref>. This capability enables the models to perform a broad spectrum of tasks, from simple query responses to complex problem-solving and content generation, tailored to specific user requests.</p><p>In practical deployments, models may not adhere to comply with user instructions, exhibiting behaviors that deviate from anticipated outcomes. This includes generating responses unrelated to explicit instructions, emitting redundant or erroneous information, or entirely ignoring specified directives <ref type="bibr" target="#b15">(Gehman et al., 2020;</ref><ref type="bibr" target="#b20">Kenton et al., 2021;</ref><ref type="bibr" target="#b29">Wei et al., 2024)</ref>. To enhance the instruction following capability of LLMs, open-domain instruction following data is frequently used for training. Several prominent studies have explored the construction of instruct-tuning data, to achieve efficient and costeffective results <ref type="bibr">(Li et al., 2024b;</ref><ref type="bibr" target="#b5">Cao et al., 2024;</ref><ref type="bibr" target="#b29">Liu et al., 2024;</ref><ref type="bibr" target="#b38">Xu et al., 2024)</ref>.</p><p>Level Target Length Precise Match (PM) Flexible Match (FM) Level:0 10 ±10 (0, 20] 30 ±10 (20, 40] 50 ±10 (40, 60] 80 ±10 (60, 100] Level:1 150 ±20 (100, 200] 300 ±20 (200, 400] 500 ±50 (400, 600] Level:2 700 ±70 (600, 800] &gt;800 (800, ∞) (800, ∞) Table 1: Nine target lengths and their corresponding match ranges categorized as Precise Match (PM) and Flexible Match (FM). Target lengths are classified into three categories, Level:0, Level:1, and Level:2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Meta Token</head><p>Recently, an increasing number of studies have employed custom tokens within language models to execute specific functions or enhance performance. <ref type="bibr" target="#b33">Todd et al. (2024)</ref> report findings that the hidden states of language models capture representations of these functions, which can be condensed into a Function Vector (FV). Furthermore, their research demonstrates that FV can effectively guide language models in performing specific tasks. Numerous studies have utilized meta tokens to compress prompts, thereby enhancing the inference capability of models <ref type="bibr" target="#b25">(Li et al., 2023;</ref><ref type="bibr" target="#b28">Liu et al., 2023;</ref><ref type="bibr" target="#b41">Zhang et al., 2024)</ref>. <ref type="bibr" target="#b30">Mu et al. (2023)</ref>introduce the concept of "gist tokens", which can be cached and reused for compute efficiency. Further <ref type="bibr" target="#b19">Jiang et al. (2024)</ref> utilize a hierarchical and dynamic approach to extend the concept, proposing "HD-Gist tokens" to improve model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Can LLMs Accurately Generate with</head><p>Target Length?</p><p>In this section, we examine the capability of LLMs to generate responses of a target length. Initially, we introduce Target Length Generation Task (TLG). Subsequently, we establish various target lengths and two evaluation metrics ( §3.1). We then detail the experimental setup and assess the ability of LLMs to generate responses at target lengths ( §3.2). Finally, we present the outcomes of the experiments and analysis the underlying reasons( §3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Target Length Generation Task</head><p>To assess the ability of existing LLMs to control the length of generated response, we develop the TLG. This task assesses the models' ability in produc-ing responses that match target lengths as directed designed target lengths are detailed in Table <ref type="table">1</ref>. Additionally, we divide these nine target lengths into three levels: Level:0, Level:1, and Level:2.</p><p>Given that generating responses with target lengths is challenging for existing LLMs, we develop two metrics to evaluate the accuracy of response lengths.</p><p>• Precise Match (PM): This metric requires that the length of the generated response be very close to the target length. For different Level, a precise tolerance range is set (±10, ±20, . . . ) necessitating that the response length stringently conforms to these defined limits.</p><p>• Flexible Match (FM): This metric requires a broader tolerance interval for target length.</p><p>For longer texts, the range incrementally widens to meet response generation requirements.</p><p>For the N responses, we assess whether response meets the target length, then calculating the PM and FM scores of the model.</p><formula xml:id="formula_0">PM = N i=1 1 lb P TL i &lt; L (c i ) ≤ ub P TL i N (1) FM = N i=1 1 lb F TL i &lt; L (c i ) ≤ ub F TL i N<label>(2)</label></formula><p>where: c i denotes the i-th response generated by LLM. The function L(•) calculates the word count of the input string. lb P TL i and ub P TL i denote the lower and upper bounds of the precise match</p><p>Model Params Target Length Generation Task (TLG) Level:0 Level:1 Level:2 All Level PM FM PM FM PM FM PM FM Closed-source Model<ref type="foot" target="#foot_0">foot_0</ref> gpt-4-turbo -82.26 86.36 46.49 85.06 40.72 47.51 61.35 77.35 gpt-4o -74.06 79.05 32.32 69.36 36.22 71.95 57.75 74.30 gpt-3.5-turbo -64.41 69.84 35.06 75.76 38.24 45.93 49.00 66.50 claude-3-haiku -48.23 55.21 35.37 73.78 44.12 50.45 43.10 60.25 claude-3.5-sonnet -75.17 81.04 42.38 83.08 62.67 71.27 61.65 79.55 Open-source Model Mistral 7B 20.29 23.50 16.77 48.32 3.62 5.66 15.45 27.70 Gemma 2B 20.95 23.17 8.69 24.24 0.23 0.23 12.35 18.45 7B 15.52 18.85 11.74 35.82 0.45 0.45 10.95 20.35 Llama3 8B 34.59 40.02 29.73 65.70 18.10 21.04 29.35 44.25 70B 58.76 64.52 36.59 77.90 36.43 41.18 46.55 63.75 InternLM2 7B 6.65 7.21 8.69 27.44 19.68 22.40 10.20 17.20 20B 8.98 9.87 10.98 34.45 17.42 20.14 11.50 20.20 DeepSeek-LLM 7B 28.16 31.37 17.68 44.36 10.86 13.12 20.90 31.60 67B 26.94 30.27 17.07 49.54 9.50 11.99 19.85 32.55 Yi-1.5 6B 23.50 25.83 16.46 48.78 18.10 20.36 20.00 32.15 9B 25.28 29.16 17.38 44.36 24.43 29.41 22.50 34.20 34B 28.82 33.59 26.07 65.40 21.27 25.79 26.25 42.30 Qwen1.5 7B 24.28 27.38 14.33 46.19 9.05 11.99 17.65 30.15 14B 28.27 31.49 18.45 43.90 11.09 14.25 21.25 31.75 32B 32.59 36.25 22.26 49.39 21.49 25.34 26.75 38.15 72B 35.59 39.69 18.29 49.70 3.85 6.11 22.90 35.55 Table 2: Overall results of different LLMs of TLG. All open-source models used are either chat or instruct models. In models belonging to the same series but varying in parameter sizes, those with larger parameters typically exhibit superior performance. The best-performing model in each Level is in-bold, and the second best is underlined.</p><p>range associated with the target length of i-th response. lb F TL i and ub F TL i denote the lower and upper bounds of the flexible match range associated with the target length of i-th response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>Dataset. We employ a two-stage data construction method for this study. Initially, we randomly sample 2,000 data from OpenHermes2.5 <ref type="bibr" target="#b32">(Teknium, 2023)</ref>. To enhance the complexity of the task and prevent data leakage, the second stage involved uses only the questions from these samples. Additionally, we randomly assign one of nine target lengths for the responses. The distribution of target length in the TLG dataset is shown in Figure <ref type="figure" target="#fig_0">3</ref>. Further details regarding the format of the TLG dataset are provided in Appendix A.1.</p><p>Models &amp; Prompt Templates. We conduct extensive experiments with both closed and opensource LLMs, specifically the chat or instruct version. The specific models used are listed in Table <ref type="table">8</ref>. We evaluate each model using its own prompt template, as detailed in Table <ref type="table">9</ref>.</p><p>To integrate the target length into the prompt, we modify the sentence The response should have a word count of {Target Length} words into each question. For target length &gt;800, we replace this with more than 800.</p><p>Hardware &amp; Hyperparameters. All experiments are conducted on NVIDIA A100 GPUs. Inference is performed using the vllm <ref type="bibr" target="#b21">(Kwon et al., 2023)</ref>, with temperature set to 0 and max_tokens set to 2,048 in the SamplingParams, thereby employing greedy decoding for inference.</p><p>The model_max_length for all models is consistent with their respective configurations, as shown in Table <ref type="table">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Analysis</head><p>Table <ref type="table">2</ref> displays the PM and FM scores of opensource models at different Levels. Generally, models with advanced capabilities achieve higher PM and FM scores, indicating stronger adherence to instructions. This observation aligns with human expectations.</p><p>For most models, scores are lowest at Level:2, suggesting significant potential for enhancement in producing longer responses. While, scores at Level:1 are the highest. This trend may be attributed to the prevalence of shorter responses in the training datasets utilized for model fine-tuning, which influences their generative biases. Desipte potential differences in parameters, a performance gap between closed and open source models remains evident. Notably, claude-3.5-Sonnet achieve the best scores across all models at the All Level, with scores of 61.65 and 79.55. Furthermore, the PM and FM scores for each model across various target lengths are detailed in Appendix A.3.</p><p>The poor performance in TLG can be attributed to a discrepancy between the token counts generated by LLMs and the lengths as understood by humans. The discrepancy between the tokens generated by LLMs and the lengths as understood by humans constirbutes to the issue. This mismatch arises due to several factors:</p><p>• Tokenization Schemes: LLMs employ subword tokenization schemes that decompose words into smaller units of varying lengths. For example, a single long word might be divided into multiple tokens, complicating the model's ability to equate token counts with human-understood word counts <ref type="bibr" target="#b13">(Gage, 1994)</ref>.</p><p>• Model Training: Most LLMs, particularly those trained using autoregressive language modeling, are not explicitly trained with objectives that prioritize output length. As a result, these models often lack strong capabilities for controlling the length of their generated output <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RULER: Meta Length Token Controlled Generation</head><p>In this section, we first introduce RULER, encompassing the design of the Meta Length Tokens (MLTs), the data collection and the learning process associated with the models ( §4.1). Subsequently, we detail the difference in the generation of RULER under two scenarios: TLG and non-TLG ( §4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Method</head><p>RULER. We introduce RULER, as illustrated in Figure <ref type="figure">2</ref>, to effectively control the response length</p><p>Table 3: Meta length tokens in RULER showing their range of variation in data collection and counts in D M LT . of LLMs using MLTs. Ruler employs MLTs to explicitly communicate length requirements within instructions. The MLTs represent the model's response length range and aim to enhance its capability on the TLG task. Our end-to-end training enables the LLMs to automatically generate MLTs in various scenarios, regardless of target length requirements. MLTs (Table <ref type="table">3</ref>) offer more precise control than traditional text prompt methods, which often prove insufficiently constraining.</p><p>Data collection for RULER. For common finetuning training datasets, the format typically consist of input-output pairs (x, y). Following <ref type="bibr">Zhou et al. (2023b)</ref>, we calculate the word count of y for each entry. Based on the predefined MLTs in Table <ref type="table">3</ref> and their range of variation, we aim to match each y to a corresponding mlt based on its word count. If a match is found, the data is reformatted as (x, mlt, y). This method aids in the construction of the fine-tuning training dataset D M LT , detailed in Algorithm B.</p><p>RULER learning. To minimize changes to the model's generation pattern and ensure stability in non-TLG scenario, we position the MLT immediately before the original response during the construction of fine-tuning data. This strategy maintains the model chat template. Consequently, the combination of mlt and the original response y forms a new complete response y ′ . We conduct the training of the RULER M on the curated corpus D M LT , which is augmented with Meta Length Tokens D M LT , employing the standard next token objective:</p><formula xml:id="formula_1">max M E (x,mlt,y)∼D M LT log p M (mlt, y|x) (3)</formula><p>We concatenate the MLT directly to the beginning of y to compute the loss and use the MLTs to expand the original vocabulary V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RULER Inference</head><p>TLG scenario. In the Target Length Generation (TLG) scenario, the user's instruction specifies a target length, decomposed into a question and a target length. The RULER converts this target length into the corresponding MLT and appends it to the model chat template. Subsequent to the MLT, RULER generates response that aligns with the target length, ensuring compliance with both the user's question and the target length, as illustrated in Figure <ref type="figure">2</ref>. This approach yields superior results compared to controlling outputs solely through prompts.</p><p>non-TLG scenario. In the non-TLG scenario, users provide straightforward instructions consisting solely of a question. RULER integrates these instructions directly into the model's chat template for generation. Owing to its innovative design and the use of a standard next-token objective in training (Equation <ref type="formula">3</ref>), RULER autonomously generates a MLT prior to producing the textual response. This MLT is designed to match the length of the content generated, thereby ensuring normal generation of the model in non-TLG scenarios, as illustrated in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Dataset D M LT . To ensure balanced frequency distribution of each Meta Length Token (MLT) in D M LT , we set a maximum occurrence limit of 20,000 for each MLT. We construct D M LT from three datasets: OpenHermes2.5 (excluding data previously used in TLG) <ref type="bibr" target="#b32">(Teknium, 2023)</ref>, LongForm <ref type="bibr" target="#b22">(Köksal et al., 2023)</ref>, and ELI5 <ref type="bibr" target="#b12">(Fan et al., 2019)</ref>, in accordance with Algorithm 1. This approach aims to create a diverse dataset, particularly effective for generating longer content that is relatively rare. in total, D M LT comprises 121,229 entries, with the frequency of each MLT in Table <ref type="table">3</ref> select six LLMs are selected: Mistral-7B-v0.3 <ref type="bibr">(Jiang et al., 2023</ref><ref type="bibr">), gemma-7b (Team et al., 2024)</ref>, Llama-3-8B (AI@Meta, 2024), deepseek-llm-7b (DeepSeek-AI, 2024), Yi-1.5-6B <ref type="bibr" target="#b9">(AI et al., 2024)</ref>, and Qwen1.5-7B <ref type="bibr" target="#b3">(Bai et al., 2023)</ref>. We apply the RULER to these base models and compare the results with their corresponding instruct or chat models.</p><p>Evaluation Metric. Consistent with the TLG and compared to previous results, we also calculate PM and FM scores to assess the effectiveness of RULER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>Table 4 presents a detailed comparison of PM and FM scores across various LLMs using RULER across different Levels. For information on model training see Appendix C.2.</p><p>Overall Performance Enhancement. Across all evaluated models, we observe a consistent improvement in both PM and FM scores at all Levels.</p><p>The most significant improvement is observed in gemma-7b R 1 , with PM and FM scores increasing by 34.40 and 37.10, respectively. In contrast, the least improvement is noted with PM and FM rising by 21.35 and 22.15. The PM and FM scores across All Level showed an average improvement of 27.97 and 29.57. These improvements indicate that RULER effectively enhances the model's ability to generate content of target lengths. This suggests 1 Model name with R means base model with RULER that using MLT to control output length is more effective than using prompts, as the model learns to generate content of corresponding lengths during fine-tuning. Additionally, RULER's ability to enhance various models demonstrates its generalizability and scalability.</p><p>Different Level Analysis. At Level:0, all models show significant improvements in both PM and FM scores. Compared to other Level, each model achieves the highest PM and FM score improvements at Level:0. This enhancement occurs because the models are capable of generating responses of this length; however, their coarse length control impedes precise adherence to target length requirements. Our method significantly improves the models' capacity to accurately control content length at Level:0 more accurately, better meeting the target length requirements.</p><p>Moving to Level:1, while the improvements are not as pronounced as at Level:0, the models still exhibit significant gains in both PM and FM scores. At Level:2, the extent of score improvements varies across models. For instance, Mistral-7B-v0.3 R and gemma-7b R continue to show substantial score increases. Despite these positive trends, only deepseek-llm-7b-chat R , show a slight decrease in scores at Level:2. This is attributed to the insufficient data for Level:2 in D M LT . The uneven distribution of data likely contributes to the slight decrease in scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FM of Different Target Length</head><p>Avg FM 10 30 50 80 150 300 500 700 &gt;800</p><p>Mistral-7B-Instruct-v0.3 0.5 0.0 0.5 2.0 18.5 50.5 20.5 3.0 2.5 10.89 Mistral-7B-v0.</p><p>3 R 72.5 68.0 65.5 76.5 76.0 63.0 28.0 24.0 64.5 59.78 gemma-7b-it 13.0 17.0 15.5 26.0 54.5 76.5 17.5 0.0 0.0 24.44 gemma-7b R 58.0 63.5 61.0 69.5 72.5 64.0 42.0 17.0 67.0 57.17 Llama-3-8B-Instruct 23.5 18.0 12.5 28.0 50.5 76.5 57.0 25.5 30.5 35.78 Llama-3-8B R 84.0 84.0 73.0 80.0 87.5 89.5 71.0 14.5 36.5 68.89 deepseek-llm-7b-chat 36.5 16.0 12.5 17.5 23.5 60.5 36.5 16.0 22.5 26.83 deepseek-llm-7b R 64.0 70.0 62.5 73.0 82.0 86.5 27.0 17.0 40.5 58.06 Yi-1.5-6B-Chat 26.5 16.5 14.5 14.5 18.5 42.5 35.0 33.5 28.5 25.56 Yi-1.5-6B R 80.5 66.0 67.0 77.0 83.5 83.5 56.0 22.0 39.5 63.89 Qwen1.5-7B-Chat 13.5 17.0 9.5 16.0 6.5 51.0 57.5 22.5 4.5 22.00 Qwen1.5-7B R 69.0 61.0 46.5 68.5 81.0 80.5 38.5 16.5 36.5 55.33 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Do MLTs actually influence the length of the generated content?</head><p>To further investigate the effectiveness and scalability of MLTs, we designed two additional experiments: multi MLT generation experiment and self-generated MLT experiment.</p><p>Multi MLT Generation Experiment. To further validate the efficacy and robustness of RULER, we assess its ability to control response length. We randomly sample 200 entries from Arena-Hard-Auto <ref type="bibr">(Li et al., 2024a)</ref> and subject each to all target lengths (Table <ref type="table">1</ref>), culminating in 1,800 entries at last. Subsequently, we calculate the FM scores for each target length, using the original model as a baseline.</p><p>The results presented in Table <ref type="table" target="#tab_5">5</ref> highlight the enhancements in model performance due to RULER. The FM scores achieved by RULER generally surpass those of the baseline models. Notably, even the well-performing Llama-3-8B R shows significant improvements. However, when the target length is 700, RULER shows a decline in FM if the baseline model already achieves a certain score. In contrast, RULER enhances performance if the baseline model is underperforming. This phenomenon is likely due to an imbalance in the D M LT , where responses of 700 words are infrequent and differ from the fine-tuning data of the baseline, potentially undermining performance. Overall, RULER</p><p>Model Type ARC (chanllenge/easy) HellaSwag TruthfulQA MMLU Winogrande GSM8K Mistral-7B-v0.3 vanilla 38.23/67.76 48.57 46.02 34.94 62.04 26.46 -RULER 37.97/67.85 47.83 47.12 37.88 62.83 27.52 gemma-7b vanilla 35.75/65.66 45.95 41.13 32.44 57.14 23.58 -RULER 38.99/67.47 45.40 45.65 31.67 60.30 25.93 Meta-Llama-3-8B vanilla 48.63/77.48 58.89 51.41 50.91 71.74 44.96 -RULER 49.23/77.99 59.12 51.90 50.16 71.19 46.63 deepseek-llm-7b-base vanilla 50.94/79.92 61.48 39.90 48.65 72.93 38.89 -RULER 51.37/79.55 61.31 38.43 48.81 72.77 37.15 Yi-1.5-6B vanilla 51.62/79.25 58.79 55.32 54.68 68.51 52.01 -RULER 51.28/79.46 58.41 49.94 55.13 68.11 50.34 Qwen1.5-7B vanilla 46.67/77.53 56.39 53.98 54.00 65.98 44.88 -RULER 47.27/76.68 56.46 50.18 54.59 65.19 47.01</p><p>Table 7: Comparison of the overall performance of six models with RULER or vanilla, with scores computed on ARC, HellaSwag, TruthfulQA, MMLU, Winogrande and GSM8K. The overall performance of models using RULER generally remains consistent with the base models with sft.</p><p>significantly improves model performance.</p><p>Self-generated MLT Experiment. To validate RULER in generating MLT and responses under a non-TLG scenario, we use the Arena-Hard-Auto dataset without providing MLTs, thereby necessitating autonomous response generation by the model. We evaluate performance by cataloging the types and proportions of generated MLTs (Figure <ref type="figure">4</ref>) and evaluating response length using FM score at the target lengths corresponding to the MLTs (Table <ref type="table">6</ref>). Models show a preference for producing responses with target lengths of 150 and 300. This inclination is likely attributable to the complex nature of the queries in the Arena-Hard-Auto, which require longer responses for problem resolution. In the non-TLG scenario, the FM scores are notably high, with the Mistral-7B-v0.3 R recording the lowest at 73.40 and Llama-3-8B R achieving the highest at 88.40. The word count across all models varies from 187 words to 347 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation on Overall Performance</head><p>To evaluate the impact of RULER on overall performance, we conduct experiments utilizing six benchmark datasets: ARC <ref type="bibr" target="#b7">(Clark et al., 2018)</ref>, HellaSwag <ref type="bibr" target="#b39">(Zellers et al., 2019)</ref>, TruthfulQA <ref type="bibr" target="#b27">(Lin et al., 2022)</ref>, MMLU <ref type="bibr" target="#b16">(Hendrycks et al., 2021</ref><ref type="bibr">), Winogrande (Sakaguchi et al., 2019)</ref> and GSM8K <ref type="bibr" target="#b8">(Cobbe et al., 2021)</ref>. These benchmarks provide a comprehensive assessment across different task types. lm-evaluation-harness <ref type="bibr">(Gao et al., 2024)</ref> is employed to assess the overall performance. Further details about the experiments on the experiment can be found in Appendix C.4. Table <ref type="table">7</ref> illustrates that RULER marginally reduces performance on several tasks. Overall performance of models using Ruler generally remains consistent with the original models. The variations in scores are minimal, with changes within a very small range. Moreover, we observe that some models with Ruler actually show improvements in specific tasks. These improvements suggest that Ruler may contribute positively under certain conditions or in certain task types. This indicates that RULER can significantly enhance the model's ability to follow length-based instructions without compromising its performance on the same data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This study initially investigate the instruction following abilities of LLMs and introduces Target Length Generation Task (TLG). Additionally, we propose RULER, a novel and model-angnostic method that controls generated length for LLMs. RULER utilizes the MLT and end-to-end training to enhance model performance. Experimental results demonstrate that substantial improvements in PM and FM scores across various models. Moreover, two additional experiments are conducted to further validate the efficacy of the proposed method. Finally, we assess overall performance across six different benchmarks to demonstrate its superiority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>With the emergence of large language models (LLMs), an increasing number of applications are now utilizing LLMs. A particularly interesting aspect is the instruction-following capabilities of LLMs. In this paper, we analyze the capabilities of LLMs solely from the perspective of controlling generated length and propose a solution through RULER. Instructions, which vary widely and represent a real-life scenario or application. We believe addressing the challenges or solving widespread issues across various instructions is crucial. We employ meta token to construct RULER and argue that meta tokens offer more robust control over models than prompts do. Exploring how to develop and utilize models effectively with the help of tokens is a profoundly important question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Statements</head><p>This study concentrates on managing the output length of Large Language Models (LLMs). While our primary focus is on the length of generated content, we have not assessed the potential for producing toxic content. The research does not involve human participants, nor does it handle personal or sensitive information. We have used only opensource or suitably licensed resources, thereby complying with relevant standards. Additionally, all training data employed are open-source, ensuring the exclusion of any private or sensitive information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 2: Overview of RULER. The method is divided into two parts: training and inference. The figure illustrates the main content of both sections. Additionally, in the inference section, we show two scenarios: TLG and non-TLG to show the difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>. Moreover, we calculate the word count for each response in every dataset, allowing us to statistically analyze the MLT distribution, as detailed in Table 16. LLMs. To comprehensively evaluate the performance of RULER across different models, we consider factors such as model size, open-source availability, and overall model performance. We Overall results of various LLMs with RULER are presented. Additionally, we also annotate the table with the score changes compared to the chat or instruct model. Consistent improvements in both PM and FM scores are observed across all Levels.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Target Length Generation Task (TLG)</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>Level:0</cell><cell></cell><cell>Level:1</cell><cell cols="2">Level:2</cell><cell></cell><cell>All Level</cell></row><row><cell></cell><cell>PM</cell><cell>FM</cell><cell>PM</cell><cell>FM</cell><cell>PM</cell><cell>FM</cell><cell>PM</cell><cell>FM</cell></row><row><cell>Mistral-7B-Instruct</cell><cell>20.29</cell><cell>23.50</cell><cell>16.77</cell><cell>48.32</cell><cell>3.62</cell><cell>5.66</cell><cell>15.45</cell><cell>27.70</cell></row><row><cell>Mistral-7B R</cell><cell cols="8">70.18↑49.89 75.06↑51.56 35.52↑18.75 67.84↑19.52 33.71↑30.09 36.43↑30.77 50.75↑35.30 64.15↑36.45</cell></row><row><cell>gemma-7b-it</cell><cell>15.52</cell><cell>18.85</cell><cell>11.74</cell><cell>35.82</cell><cell>0.45</cell><cell>0.45</cell><cell>10.95</cell><cell>20.35</cell></row><row><cell>gemma-7b R</cell><cell cols="8">59.53↑44.01 64.19↑45.34 39.33↑27.59 68.14↑32.32 25.34↑24.89 27.83↑27.38 45.35↑34.40 57.45↑37.10</cell></row><row><cell cols="2">Llama-3-8B-Instruct 34.59</cell><cell>40.02</cell><cell>29.73</cell><cell>65.70</cell><cell>18.10</cell><cell>21.04</cell><cell>29.35</cell><cell>44.25</cell></row><row><cell>Llama-3-8B R</cell><cell cols="5">77.27↑42.68 80.71↑40.69 50.76↑21.03 83.84↑18.14 19.23↑1.13</cell><cell>22.85↑1.81</cell><cell cols="2">55.75↑26.40 68.95↑24.70</cell></row><row><cell cols="2">deepseek-llm-7b-chat 28.16</cell><cell>31.37</cell><cell>17.68</cell><cell>44.36</cell><cell>10.86</cell><cell>13.12</cell><cell>20.90</cell><cell>31.60</cell></row><row><cell>deepseek-llm-7b R</cell><cell cols="5">68.18↑40.02 73.50↑42.13 31.10↑13.42 68.90↑24.54 11.54↑0.68</cell><cell cols="3">11.76↓-1.36 43.50↑22.60 58.35↑26.75</cell></row><row><cell>Yi-1.5-6B-Chat</cell><cell>23.50</cell><cell>25.83</cell><cell>16.46</cell><cell>48.78</cell><cell>18.10</cell><cell>20.36</cell><cell>20.00</cell><cell>32.15</cell></row><row><cell>Yi-1.5-6B R</cell><cell cols="5">67.07↑43.57 72.17↑46.34 40.40↑23.94 76.83↑28.05 19.23↑1.13</cell><cell>21.04↑0.68</cell><cell cols="2">47.75↑27.75 62.40↑30.25</cell></row><row><cell>Qwen1.5-7B-Chat</cell><cell>24.28</cell><cell>27.38</cell><cell>14.33</cell><cell>46.19</cell><cell>9.05</cell><cell>11.99</cell><cell>17.65</cell><cell>30.15</cell></row><row><cell>Qwen1.5-7B R</cell><cell cols="5">59.09↑34.81 64.41↑37.03 29.88↑15.55 61.28↑15.09 11.54↑2.49</cell><cell>14.25↑2.26</cell><cell cols="2">39.00↑21.35 52.30↑22.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results in multi MLT generation experiment. Generally, the FM scores obtained via RULER surpass those of the baseline models.</figDesc><table><row><cell>10</cell><cell>30</cell><cell>50</cell><cell>80</cell><cell>150</cell><cell>300</cell><cell>500</cell><cell>700</cell><cell cols="2">&gt; 800</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">4% 29%</cell><cell>23%</cell><cell></cell><cell>2%</cell><cell></cell><cell></cell><cell></cell><cell>3%</cell><cell>Model</cell><cell>FM Avg WC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1%</cell></row><row><cell></cell><cell cols="2">4%</cell><cell></cell><cell></cell><cell>27%</cell><cell></cell><cell></cell><cell></cell><cell>10%</cell><cell>Mistral-7B-v0.3 R 73.40</cell><cell>279</cell></row><row><cell></cell><cell cols="2">17%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5%</cell><cell>gemma-7b R</cell><cell>69.00</cell><cell>347</cell></row><row><cell>42%</cell><cell cols="2">4%</cell><cell>40%</cell><cell></cell><cell>1%</cell><cell>48%</cell><cell></cell><cell></cell><cell>5%</cell><cell>Llama-3-8B R</cell><cell>88.40</cell><cell>215</cell></row><row><cell cols="2">Mistral-7B-v0.3 R</cell><cell></cell><cell></cell><cell>Gemma-7b R</cell><cell>2%</cell><cell></cell><cell cols="2">Llama-3-8B R</cell><cell>30%</cell><cell>deepseek-llm-7b R 84.40</cell><cell>187</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>27%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">29%</cell><cell></cell><cell></cell><cell>4%</cell><cell></cell><cell></cell><cell></cell><cell>3%</cell><cell>Yi-1.5-6B R</cell><cell>81.40</cell><cell>236</cell></row><row><cell></cell><cell cols="2">1%</cell><cell></cell><cell></cell><cell>2%</cell><cell></cell><cell></cell><cell></cell><cell>2%</cell><cell>Qwen1.5-7B R</cell><cell>81.60</cell><cell>245</cell></row><row><cell></cell><cell cols="2">2%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">8%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30%</cell><cell cols="2">4%</cell><cell></cell><cell></cell><cell>12%</cell><cell></cell><cell></cell><cell></cell><cell>8%</cell><cell>Table 6: The FM score and average word</cell></row><row><cell></cell><cell></cell><cell></cell><cell>47%</cell><cell></cell><cell>4%</cell><cell>38%</cell><cell></cell><cell></cell><cell>16%</cell><cell>count of RULER with models in self-generated</cell></row><row><cell></cell><cell cols="2">26%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">deepseek-llm-7b R</cell><cell></cell><cell></cell><cell cols="2">Yi-1.5-6B R</cell><cell></cell><cell cols="2">Qwen1.5-7B R</cell><cell>MLT experiment. FM scores are notably high.</cell></row><row><cell cols="10">Figure 4: Distribution of MLTs generated by RULER in self-generated MLT experiment. The models demonstrate a preference for generating responses with lengths of 150</cell><cell>Specifically, gemma-7b R recorded the low-est at 69.00, while Llama-3-8B R achieved the highest at 88.40.</cell></row><row><cell>and 300.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The results of all closed-source models are obtained on July</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>26, 2024.   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2022YFF0902100</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">62376262</rs>), the <rs type="funder">Natural Science Foundation of Guangdong Province of China</rs> (<rs type="grantNumber">2024A1515030166</rs>), <rs type="funder">Shenzhen Science and Technology Innovation Program</rs> (<rs type="grantNumber">KQTD20190929172835662</rs>), <rs type="funder">Shenzhen Basic Research Foundation</rs> (<rs type="grantNumber">JCYJ20210324115614039</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_n9wZgqa">
					<idno type="grant-number">2022YFF0902100</idno>
				</org>
				<org type="funding" xml:id="_fX3zQbq">
					<idno type="grant-number">62376262</idno>
				</org>
				<org type="funding" xml:id="_Kb9xhd6">
					<idno type="grant-number">2024A1515030166</idno>
				</org>
				<org type="funding" xml:id="_ayzYWqY">
					<idno type="grant-number">KQTD20190929172835662</idno>
				</org>
				<org type="funding" xml:id="_3ahyEje">
					<idno type="grant-number">JCYJ20210324115614039</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Target Length Generation Task Deatils</head><p>In this section, we present the experimental details of the Target Length Generation (TLG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 TLG Dataset</head><p>Dataset constructed for the TLG, totaling 2,000 entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TLG Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>"id":"0" "Instruction":"How can I generate an AI model that can classify articles of clothing as shorts, skirts, or pants based on their descriptions?", "TargetLength":"50" } [...] { "id":"1999" "Instruction":"You will be given several pieces of information about someone, and you will have to answer a question based on the information given.\nJohn is taller than Bill. Mary is shorter than John. Question: Who is the tallest person?", "TargetLength":"30" }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Models &amp; Prompt Templates</head><p>In this appendix, we list the models in the TLG, including their fullname, params, context length and vocab size. All models are downloaderd from Huggingface 2 and inference is executed using vllm <ref type="bibr" target="#b21">(Kwon et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Model Full Name   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Results on Different Target Length</head><p>Here, we present the FM and PM scores of the models at all target lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Level:0</head><p>The PM and FM scores for each model at Level:0 are shown in Table <ref type="table">11</ref> and <ref type="table">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Params</head><p>Level:0 To obtain data with varying response lengths for composing D M LT , particularly those responses exceeding 500, we integrateg data from OpenHermes2.5 <ref type="bibr" target="#b32">(Teknium, 2023)</ref>, LongForm <ref type="bibr" target="#b22">(Köksal et al., 2023)</ref> and ELI5 <ref type="bibr" target="#b12">(Fan et al., 2019)</ref>. We calculate the word count for each response in every dataset, allowing us to statistically analyze the MLT distribution, shown in Table <ref type="table">16</ref>. end for 10:</p><p>if mlt is not None then 11:</p><p>end if 13: end for 14: return D M LT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLT</head><p>OpenHermes2.5 LongForm ELI5 <ref type="bibr" target="#b32">(Teknium, 2023)</ref>  <ref type="bibr" target="#b22">(Köksal et al., 2023)</ref>  <ref type="bibr" target="#b12">(Fan et al., 2019)</ref> [  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 More Details of Training</head><p>More details of training. We use 4*A100 with 80GB Nvidia GPUs to train the models. The training utilizes both bf16 and tensor tf32 precision formats. The per-device training batch size is set to 4, with gradient accumulation is 8 steps. A cosine learning rate scheduler is applied, starting with an initial learning rate of 2e-5 and a warmup ratio of 0.05. All models are trained for 3 epochs. Additionally, log is set to print every 5 steps.</p><p>Loss. We document the changes in training loss for all models, as shown in Figure <ref type="figure">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Multi MLT generation experiment</head><p>Here is the results in multi MLT generation experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 More Details of Other Tasks</head><p>We tested the RULERon six benchmarks (ARC <ref type="bibr" target="#b7">(Clark et al., 2018)</ref>, HellaSwag <ref type="bibr" target="#b39">(Zellers et al., 2019)</ref>, TruthfulQA <ref type="bibr" target="#b27">(Lin et al., 2022)</ref>, MMLU <ref type="bibr" target="#b16">(Hendrycks et al., 2021</ref><ref type="bibr">), Winogrande (Sakaguchi et al., 2019)</ref> and GSM8K <ref type="bibr" target="#b8">(Cobbe et al., 2021)</ref>) to examine whether the performance of the fine-tuned models varies on different tasks. We employ 25-shot in ARC, 10-shot setting in Hellaswag, 5-shot setting in MMLU, 0-shot setting in TruthfulQA, 5-shot setting in Winogrande and 5-shot in GSM8K.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. 2023. Gpt-4 technical report</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">:</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangcheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaidong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senbin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Zonghong Dai. 2024. Yi: Open foundation models by 01</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><surname>Ai@meta</surname></persName>
		</author>
		<title level="m">Llama 3 model card</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengguang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<title level="m">Qwen technical report</title>
		<imprint>
			<publisher>Xiaohuan Zhou, and Tianhang Zhu</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Instruction mining: Instruction data selection for tuning large language models</title>
		<author>
			<persName><forename type="first">Yihan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meta-learning via language model in-context tuning</title>
		<author>
			<persName><forename type="first">Yanda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.53</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="719" to="730" />
		</imprint>
	</monogr>
	<note>Long Papers) Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<title level="m">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deepseek llm: Scaling opensource language models with longtermism</title>
		<author>
			<persName><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2401.02954</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00234</idno>
		<title level="m">A survey on in-context learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Eli5: Long form question answering</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new algorithm for data compression</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The C Users Journal archive</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviya</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.12608602</idno>
	</analytic>
	<monogr>
		<title level="j">Anish Thite</title>
		<imprint/>
	</monogr>
	<note>and Andy Zou. 2024. A framework for few-shot language model evaluation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RealToxi-cityPrompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.301</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3356" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards reasoning in large language models: A survey</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-acl.67</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1049" to="1065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renard</forename><surname>Lélio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Lacroix</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and William El Sayed. 2023. Mistral 7b</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical and dynamic prompt compression for efficient zero-shot API usage</title>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Vecchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EACL 2024</title>
		<imprint>
			<publisher>St. Julian&apos;s, Malta. Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2162" to="2174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Alignment of language agents</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient memory management for large language model serving with pagedattention</title>
		<author>
			<persName><forename type="first">Woosuk</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</title>
		<meeting>the ACM SIGOPS 29th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Abdullatif</forename><surname>Köksal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m">Longform: Effective instruction tuning with reverse instructions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Yedid Hoshen, and Amnon Shashua. 2022. The inductive bias of in-context learning: Rethinking pretraining example design</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jannai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Navon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Frick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Banghua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<title level="m">and Ion Stoica. 2024a. From live data to high-quality benchmarks: The arena-hard pipeline</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compressing context to enhance inference efficiency of large language models</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Guerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.391</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Singapore. Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6342" to="6353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">2024b. One-shot learning as instruction data prospector for large language models</title>
		<author>
			<persName><forename type="first">Yunshui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzheng</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling-Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TruthfulQA: Measuring how models mimic human falsehoods</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.229</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3214" to="3252" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TCRA-LLM: Token compression retrieval augmented large language model for inference cost reduction</title>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-emnlp.655</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9796" to="9810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to compress prompts with gist tokens</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants</title>
		<author>
			<persName><surname>Teknium</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Function vectors in large language models</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Millicent</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">2022a. Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nika</forename><surname>Haghtalab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note>Jailbroken: How does llm safety training fail? International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Emergent abilities of large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Survey Certification</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">WizardLM: Empowering large pre-trained language models to follow complex instructions</title>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<title level="m">Hellaswag: Can a machine really finish your sentence? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">2023a. Marathon: A race through the realm of long context with large language models</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Soaring from 4k to 400k: Extending llm&apos;s context with activation beacon</title>
		<author>
			<persName><forename type="first">Peitian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninglu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.03462</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic chain of thought prompting in large language models</title>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">LIMA: Less is more for alignment</title>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujoy</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.07911</idno>
		<title level="m">Instruction-following evaluation for large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
