<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Language Models Are Biased Because They Are Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-03-13">13 Mar 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Philip</forename><surname>Resnik</surname></persName>
							<email>resnik@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saif</forename><forename type="middle">Mohammad</forename><surname>Submission</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large Language Models Are Biased Because They Are Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-03-13">13 Mar 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">BDE988F287758489DA3FCFCC8D35DD6A</idno>
					<idno type="arXiv">arXiv:2406.13138v2[cs.CL]</idno>
					<note type="submission">received: 1 January 2025; accepted for publication: 4 March 2025.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Resnik</term>
					<term>Philip. &quot;Large Language Models Are Biased Because They Are Large Language Models&quot;. Computational Linguistics</term>
					<term>in press</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This position paper's primary goal is to provoke thoughtful discussion about the relationship between bias and fundamental properties of large language models. I do this by seeking to convince the reader that harmful biases are an inevitable consequence arising from the design of any large language model as LLMs are currently formulated. To the extent that this is true, it suggests that the problem of harmful bias cannot be properly addressed without a serious reconsideration of AI driven by LLMs, going back to the foundational assumptions underlying their design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A scorpion wanted to cross a river, but it couldn't swim, so it asked a frog to carry it across. The frog feared the scorpion would sting it, but the scorpion promised it would not, pointing out that it would drown if it did. Midway across, the scorpion stung the frog, dooming them both. "Why have you done this?!" cried the frog. "I'm sorry", said the scorpion, "It's in my nature". 1   For all their power and potential, large language models (LLMs) come with a big catch: they contain harmful biases that can emerge unpredictably in their behavior. Efforts to remove or mitigate large language model biases are a hot topic of research <ref type="bibr" target="#b30">(Gallegos et al. 2024</ref>), but such efforts have not yet met with anything resembling decisive success, and apparent successes can leave the same problem to emerge in other ways <ref type="bibr" target="#b46">(Hofmann et al. 2024)</ref>.</p><p>I take the position that this problem will not and cannot be solved without facing the fact that harmful biases are thoroughly baked into what LLMs are. There is no bug to be fixed here. The problem cannot be avoided in large language models as they are currently conceived, precisely because they are large language models. This article attempts to argue that position carefully and thoroughly. My first, primary goal is to provoke thoughtful discussion about the relationship between bias and fundamental properties of language models. To the extent that readers recognize this as an important issue worth conversation or even debate, the paper's goal has been met, whether or not any individual reader agrees with the specific argument being made here in its entirety.</p><p>Secondarily, I seek to convince the reader that bias is such a direct consequence of the design of current large language models that it cannot be avoided in large language models as they currently exist, despite the seriousness of the problem and the diligent</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">What is bias?</head><p>For an issue that is currently occupying so much time and attention, it is surprisingly difficult to find a consensus definition of the term bias in the context of large language models and AI. It may well be that no reasonably unified definition for bias in AI exists. At the same time, it seems fair to ask anyone saying these models are biased for a reasonably clear explanation of what they are claiming. Taking inspiration from <ref type="bibr" target="#b0">Adcock and Collier (2001)</ref>'s framework for valid measurement, I first characterize bias as a background concept, i.e. "the constellation of potentially diverse meanings associated with a given concept"; I then suggest a systematized concept constituting a more explicit definition; and finally I operationalize the concept in a way that could be turned into actual measurement, at least in principle.</p><p>A reasonable representation of the high-level background concept begins with the idea of a large language model producing potentially harmful outputs, e.g. decisions that follow discriminatory patterns, language that perpetuates group-based generalizations, analyses that are grounded in false or non-representative data, or recommendations that lack the individual user's context or marginalize non-dominant perspectives. As a move toward making this idea more systematic, I observe that the American Psychological Association Dictionary's most relevant sense of bias is defined as: "partiality: an inclination or predisposition for or against something" <ref type="bibr" target="#b87">(VandenBos 2007)</ref>. Taking these together, I will emphasize two elements that are relevant to thinking about large language model bias systematically. One is the idea of choice, such as what language to produce or what recommendation to make. The other is the idea of that choice being influenced by information prior to the specific context in which the choice is being made.</p><p>Let's characterize any entity that can have biases simply as a function f from inputs A to outputs O, having some internal structure and parameters I denote together as X.</p><p>Begin by considering a definition of bias as the inverse difference between the prior probability distribution P f (o; X) over outputs ignoring the input, and the posterior distribution P f (o|a; X). A sensible way to do this would be to define bias using KLdivergence, as B = D(P f (o|a; X)||P f (o; X)) -1 , quantifying how much the output actually depends on the input. On this definition, the closer the posterior adheres to the prior, the more biased f is. Or, to express the same idea adopting APA-like terminology, the more an entity acts on the basis of pre-existing partiality or preference (its prior), as opposed to taking the specifics of the situation into account (the posterior, i.e. also conditioning its behavior on its input) the more biased it is. At the extreme, the two distributions are the same, so the KL-divergence is zero <ref type="bibr" target="#b21">(Cover 1999)</ref>, and therefore the bias is infinite, corresponding intuitively to an entity always proceeding in accordance with its prior disposition, evidence or new information be damned. <ref type="foot" target="#foot_1">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Philip Resnik</head><p>LLMs are Biased Because They Are LLMs Notice that, despite the above intuition, this first-pass definition does not actually entail viewing the existence of bias in a negative way. On the contrary, it is aligned with the fact that bias, in the value-free sense of predispositions about your likely choices prior to integrating evidence, can be found everywhere. It is widely recognized that bias plays an essential role in learning <ref type="bibr" target="#b34">(Gold 1967;</ref><ref type="bibr" target="#b64">Mitchell 1980;</ref><ref type="bibr" target="#b44">Haussler 1988)</ref>, that heuristics and biases can be adaptive <ref type="bibr" target="#b82">(Simon 1956;</ref><ref type="bibr" target="#b33">Gigerenzer and Brighton 2009)</ref>, and that priors in the Bayesian sense may play a fundamental role in perception and cognition <ref type="bibr" target="#b51">(Knill and Pouget 2004;</ref><ref type="bibr" target="#b84">Tenenbaum et al. 2011;</ref><ref type="bibr" target="#b35">Gopnik and Wellman 2012;</ref><ref type="bibr" target="#b18">Clark 2015)</ref>. Even within discussions involving concerns about AI bias, it is generally observed that biases themselves can be harmful or benign <ref type="bibr" target="#b13">(Caliskan, Bryson, and Narayanan 2017;</ref><ref type="bibr" target="#b78">Schwartz et al. 2021)</ref>.</p><p>Bearing that in mind, bias in this general sense is going to be a property of any function that is actually useful, since, after all, what good is a function that doesn't pay much attention to its inputs? Therefore, the real question is not, "What exactly do you mean by bias?", it's "What exactly do you mean by harmful?". My argument does not rely on any specific answer to this question, other than to note that the concept of harm is tightly bound up with the concept of normativity: <ref type="bibr" target="#b9">Blodgett et al. (2020)</ref> observe that "analyzing 'bias' is an inherently normative process-in which some system behaviors are deemed good and others harmful". Expressing this idea in terms of my first-pass formalization of bias, one might characterize harmful bias in terms of the extent to which P f (•|a; X) arises from non-normative uses of information in a. Under that interpretation, a more refined definition for harmful bias might be B h = D(P f (o|r f (a); X)||P f (o|r n (a); X)) -1 , where r f (a) is the representation of input a used by the function, and r n (a) is the same representation but excluding any information that is normatively unacceptable to use in computing f , e.g. information related to protected demographic categories.</p><p>Having offered an answer to the question, I must emphasize that my response is not intended to be definitive; rather, I observe that the question is an important one to ask and it is frequently addressed inadequately in the literature <ref type="bibr" target="#b9">(Blodgett et al. 2020)</ref>. My specific characterization of bias here is attuned to settings in which outputs depend probabilistically on inputs, in which the function producing those outputs depends on latent, potentially or even primarily black-box structure (as reflected in the use of X for notation), and in which the nature of representations is central. Those properties are all characteristic of large language models, to which I now turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">What are large language models models of?</head><p>Language models are probabilistic models of languages that consist of observable strings of symbols. Here are some key concepts a bit more formally.</p><p>(a) A language L in this context is formalized as a probability distribution Pr L (w) over sequences of symbols w = w 1 . . . w n .<ref type="foot" target="#foot_2">foot_2</ref> (b) A language model M for a language L is a probability distribution Pr M (w) created with the goal of approximating Pr L (w).</p><p>(c) The quality of a language model, what makes it "good", is the fidelity of that approximation. Formally this is defined via the relative entropy D(p L (w)||p M (w)), which reflects perfect fidelity if and only if the two distributions are identical. Since in the real world we can't actually know the true distribution Pr L (w) for naturally occurring language L, fidelity is instead measured using cross entropy, which under usual assumptions can stand in for relative entropy by using a very large sample of texts assumed to be drawn from the true distribution Pr L (w), in place of knowing the actual distribution. This is why language models are trained to optimize cross-entropy as opposed to some other criterion.</p><p>(d) In generative models like LLMs, an underlying generative process is assumed to give rise to any specific distribution Pr(w). A generative process is a way of defining an underlying joint distribution Pr(x, w), where x is a set of probabilistic eventstypically hidden (unobservable, latent) -that leads to the generation of the observable w. For example, in a hidden Markov model (HMM), the relevant probabilistic events x in Pr HMM (x, w) that give rise to Pr HMM (w) are transitions from one hidden state to another, along with emissions of observable words w i from those hidden states.</p><p>In conventional usage, three main things distinguish large language models from previous generations of language models. First, in practice, the true distribution Pr L that LLMs are seeking to approximate is represented by vastly larger quantities of naturally occurring, human-generated training text. Second, LLMs have much larger numbers of parameters, and therefore are much better able to closely approximate that text. And third, current models have far more sophisticated architectures that further enable them to capture complex underlying structure in that text.</p><p>I invest in the notation above to highlight that LLMs are, in fact, just language models, which helps to emphasize two things. One is that, just as for any other language model, the primary, definitional goal of any LLM is to approximate, as accurately as possible, some underlying "true" distribution Pr L (w) over texts. The other is that both the language model M and the "true" language L involve not just the texts we observe (the w), but also underlying non-observables (the x).</p><p>So, to answer this section's question, large language models are generative models of a "true" probability distribution involving observable text and its underlying latent structure. High quality modeling is obtained by optimizing a model's ability to assign high probability to observed human-generated text. I now consider in more detail the question of what underlies the human-generated text these models are trained on.<ref type="foot" target="#foot_3">foot_3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">What underlies human-generated text?</head><p>To do a good job approximating the human distribution Pr L (w) with a generative model Pr M (x, w), it is not necessarily required that the model characterize exactly the same underlying process x as the true human process Pr L (x, w). However, it would be shocking if a model doing as good a job approximating the human-language distribution as LLMs did not also wind up capturing important aspects of that latent human process.</p><p>Indeed, the assumption that large language models capture the underlying human structure behind language is the basis for much of the current research using them in cognitive science, where claims are made -at least via correlation -about the relationship between underlying structure induced by the model and real-world human cognitive processes (e.g., <ref type="bibr" target="#b56">Linzen and Baroni 2021;</ref><ref type="bibr" target="#b89">Wilcox et al. 2020;</ref><ref type="bibr" target="#b63">Michaelov et al. 2024)</ref>. Similarly, the widely discussed "octopus" argument about LLMs' ability to achieve human-level understanding <ref type="bibr" target="#b5">(Bender and Koller 2020;</ref><ref type="bibr" target="#b62">Michael 2020)</ref> ultimately boils down to the question of whether the humans' joint model Pr L (x, w) can be approximated by inferences from sequences of w alone. Anyone coming down on the side of saying that models can infer the corresponding underlying human structure, just from forms, has de facto committed to the premise that, via training on surface text, the model's underlying Pr M (x, w) manages to capture important aspects of the underlying structure x in people's heads.</p><p>To put it bluntly, though, a lot of what's in people's heads sucks. In addition to normatively uncontroversial things like syntactic structure and naïve physics, the x underlying the human language probabilities that LLMs approximate -the invisible structures and processes that give rise to the observed language -includes gender and racial stereotyping, extreme nationalism, treatment of misinformation on par with facts, and every other kind of bias present in the minds of people who produce language from day to day in a human society. And, crucially, as I will now discuss, LLMs have no way to distinguish the stuff that sucks from the stuff that doesn't.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Is this an in-principle problem?</head><p>A simple example demonstrates that this is not just an inconsequential observation about large language models, but rather a fundamental property inherent in their design. Consider the word nurse, in its typical sense in English. Here are three statements that are statistically true about the concept that nurse denotes at this point in time and history. <ref type="foot" target="#foot_4">6</ref>• A nurse is a kind of healthcare worker.</p><p>• A nurse is likely to wear blue clothing at work.</p><p>• A nurse is likely to wear a dress to a formal occasion.</p><p>The first of these is a fact about the meaning of the word and does not vary with context. To assert that someone is a nurse and that they do not work in healthcare is a contradiction.<ref type="foot" target="#foot_5">foot_5</ref> And for people, or AI, to make use of the fact that nurses are healthcare workers is normatively fine. The second statement is contingently true: it is true at the present time, but nothing about nurses makes it necessary. The statement is also normatively acceptable; for example, a person or an AI system classifying someone as nurse versus non-nurse is not engaging in harmful bias if it pays attention to the color of someone's work clothes.</p><p>The third statement is also contingently true in the same sense. However, it would be normatively unacceptable, in many contexts, to use that statistical fact in making inferences or decisions. For example, in speaking with well-dressed people at a party, it would be considered inappropriate to simply assume that a woman in a dress was more likely to be a nurse than a man in a suit, even if it were statistically justifiable.</p><p>Crucially, LLMs, as they are currently constituted and trained, have no basis for distinguishing among these three distinct statements about nurses. The representation of nurse in an LLM's embedding space, and the contribution of nurse to contextual representations and inferences, makes no distinction between definitions versus contingent facts, nor between normatively acceptable versus unacceptable representations and inferences. It is distributionally observable, at the present time, that in large training samples the word nurse occurs far more frequently in the context of hospital than of theater, an observation grounded in its meaning. It is just as observable that the word nurse occurs far more frequently in sentences where the pronouns are she or her, but this observation is grounded only by contingencies in today's society -a society that retains gender biases about women's roles, which kinds of jobs pay well or poorly, etc. <ref type="bibr" target="#b20">(Cookson et al. 2023)</ref>. The massive pre-training of LLMs, which defines quality of outcome entirely on the basis of probabilities estimated from observed language, has no way to tell these observations about distribution apart. This is not to say that an LLM cannot generate language describing such a difference, or even be induced to respond in less biased or perhaps even unbiased ways when directly prompted. However, there is no necessary relationship between a system's overt, direct-response behavior and the existence of underlying biases in representation within the system.<ref type="foot" target="#foot_6">foot_6</ref> For example, <ref type="bibr" target="#b46">Hofmann et al. (2024)</ref> show that although current LLMs, with additional steps taken to alleviate bias, can generate positive overt stereotypes about African Americans (e.g. passionate, intelligent, ambitious), they nonetheless exhibit covert racism in the form of dialect-based prejudice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">What about RLHF?</head><p>The main point I made in the previous section, about what models do or do not distinguish, is anticipated by some of the earliest LLM-era work on harmful bias in machine learning. <ref type="bibr" target="#b13">Caliskan, Bryson, and Narayanan (2017)</ref> wrote, "Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names". By virtue of what it means to be a language model, LLMs are trained to replicate the biases that Caliskan et al. describe, both the harmful and the benign. That brings me to the question of the extent to which avoiding those harmful biases is actually possible.</p><p>A great deal of work has been going into this question. There is certainly progress, but I argue here that approaches focused on mitigating or alleviating problems can only get so far because the fundamental challenge arises inherently in the nature of large language models and their underlying representations. I focus here on reinforcement learning from human feedback (RLHF), the dominant approach to creating "guard rails" relating to bias in large language models, but I believe my argument extends to any other bias mitigation approach currently being explored; see <ref type="bibr" target="#b30">Gallegos et al. (2024)</ref> for a recent, comprehensive survey of methods.</p><p>Before getting to RLHF, though, let's begin with an important observation about where large language models get their power. Contrary to some people's belief that these systems merely mimic training data, the true power in LLM training is achieved by way of capturing generalizations via representation learning <ref type="bibr" target="#b6">(Bengio, Courville, and Vincent 2013)</ref> -generalization that takes place by means of dimensionality reduction when developing internal representations on the basis of observed data. 9  In early work pioneering that idea, <ref type="bibr" target="#b53">Landauer and Dumais (1997)</ref> introduced Latent Semantic Analysis (LSA) as a dimensionality reduction technique for "inducing global knowledge indirectly from local co-occurrence data in a large body of representative text". Here's the central concept. It is straightforward to infer that two words w i and w j are related by observing that they co-occur near each other more often than chance <ref type="bibr" target="#b16">(Church and Hanks 1990;</ref><ref type="bibr" target="#b24">Dunning 1994)</ref>. But dimensionality reduction -e.g. in LSA descendants like Latent Dirichlet Allocation <ref type="bibr" target="#b8">(Blei, Ng, and Jordan 2003)</ref>, in nonnegative matrix factorization <ref type="bibr" target="#b54">(Lee and Seung 2000)</ref>, and in large language modelsgoes further by capturing n th order co-occurrences. Which is to say that evidence for abstract connections between w i and w j may be exploited even if they never actually occur together, e.g. because both of them occur with some other w k (a 2 nd -order cooccurrence) or mediated by even longer chains of local co-occurrences. 10 Developing higher-level abstractions by leveraging myriad indirect relationships -relationships built on other relationships, based on yet other relationships, ad infinitum -produces a model where the latent part, x, is connected with observable w in literally unimaginable ways. 12  Against that backdrop, let's turn to reinforcement learning from human feedback, the dominant technique for guiding pre-trained models toward intended goals such as avoiding harmfully biased responses <ref type="bibr" target="#b15">(Casper et al. 2023)</ref>. RLHF is a typically-iterative process that takes a language model as input, and yields as its output another language model that is similar, but better at aligning the language it generates with human 9 I frame this in terms of dimensionality reduction because it is more intuitive, but the point applies just as well when generalization is encouraged via regularization. 10 As an intuitive illustration, consider the terms Les Miserables and Thermopylae. These rarely occur together in text, so there is little local evidence of any relationship between them. However, the term outnumbered occurs frequently in texts involving Les Miserables (Victor Hugo's novel centered on the June Rebellion of 1832 in Paris, in which about 3,000 insurgents fought some 60,000 troops) and outnumbered will also co-occur frequently with Thermopylae (the 480BC Battle of Thermopylae, in which famously 300 Spartans played a central role in a battle against a Persian force several orders of magnitude larger). The separate local co-occurrences with outnumbered provide evidence for a global conceptual relationship between these two ill-fated battles involving small groups taking on vastly larger forces. 11 11 Rebecca Resnik (scarily erudite personal communication) has pointed out a co-occurrence in Margaret Mitchell's novel Gone with the Wind, where Dr. Mead argues that the mountains will protect Confederate soldiers just like mountain passes protected the Spartans at Thermopylae (although, as Rhett Butler points out, they all died anyway), and elsewhere in the same novel Melanie reads from Les Miserables.</p><p>Notwithstanding the risk of marital discord, I still contend that it's a good example. 12 As a consequence, an active sub-area in AI has emerged in which large language models are treated as objects of scientific study (cf. <ref type="bibr" target="#b83">Simon 1969)</ref>, seeking to understand what's inside their black boxes just as psychology and neuroscience seek to understand what's inside our own cerebral black boxes, sometimes using the methods of those fields (e.g. Ettinger 2020). Conversely, others in the AI community are leaning into the belief that the traditional notion of human scientific understanding is irrelevant and unnecessary for natural language understanding and even for science itself <ref type="bibr" target="#b1">(Anderson 2008;</ref><ref type="bibr" target="#b40">Halevy, Norvig, and Pereira 2009)</ref>.</p><p>preferences. The process starts with a model Pr M (x, w; θ) that has been pre-trained, generally on massive amounts of human data. 13 The model is used to generate a set of examples of what it would produce in one or more use cases, e.g. answering questions or summarizing documents. Human feedback is then collected on the outputs, e.g. people are instructed to label outputs as "good" or "bad", or to rank which outputs are preferable. That feedback is then used to develop a hopefully-human-like model of preference for some responses over others. Finally, reinforcement learning is used to adjust (often a subset of) the model's parameters, i.e. to replace the model's θ with a θ ′ that is more likely to yield responses in keeping with the human preferences. Iteration can continue with the updated, and hopefully improved, model parameterized by θ ′ . I highlight several properties of this method that are particularly salient for the present discussion. 14 First, the feedback is provided by human beings based on goals specified for them by the developers, a practice of human evaluation that relies on what <ref type="bibr" target="#b77">Saphra et al. (2024)</ref> call "a tempting myth: that we can easily evaluate synthetic natural language outputs by simply asking a human for their opinion". For example, <ref type="bibr" target="#b68">Ouyang et al. (2022)</ref> instructed people providing feedback on harmfulness that potentially harmful output included "generating abusive, threatening, or offensive language" and "writing sexual or violent content if it's not asked for". Clearly such judgments are very much in the eye of the beholder. 15 What deserves feedback as being off-limits in one context, e.g. a public discussion, may be acceptable in another context, e.g. among members of a marginalized in-group <ref type="bibr" target="#b76">(Sap et al. 2019)</ref>. Even with attempts to employ a diverse set of people providing feedback, diversity can be limited, and as Ouyang et al. acknowledge, the LLM developers' own biases can affect the way in which those respondents are selected, as well as the specifics of the instructions. 16 In the end, therefore, RLHF replaces one set of under-characterized biases living within the black box, which got there via the aggregated language of an enormous number of people, with another set of under-characterized biases, these inferred from the judgments and feedback of a different, far smaller number of people.</p><p>Second, biases are not stable over time -how are these methods to keep up with biases that are temporally dependent? The menagerie of harmful human biases is not only too large to catalogue, it is also constantly changing. In the Great Depression era, wearing eyeglasses was stigmatized, something that seems practically unimaginable today. 17 With online communication, norms may now be among the few things capable of changing faster than LLM releases. 18   13 Here I elaborate previous notation slightly by explicitly including θ, a model's trainable parameters. 14 See <ref type="bibr" target="#b15">Casper et al. (2023)</ref> for their own extensive and thoughtfully constructed compendium of what they describe as "fundamental limitations" of RLHF, and <ref type="bibr" target="#b30">Gallegos et al. (2024)</ref> for comprehensive discussion of bias mitigation methods more generally. 15 A useful term to consider introducing into the broader discourse is essentially contested concept <ref type="bibr" target="#b31">(Gallie 1955)</ref>. This includes concepts like art or fairness that may be frequently invoked, perhaps even with a strong sense that we know what they mean, but which inherently resist having a single agreed-upon definition and therefore perennially give rise to challenge and debate. Bias may itself be such a concept. 16 A reviewer notes that this is just one flavor of researcher bias. Some others include selection of training data, choices of which tasks are important enough to benchmark, and evaluation design (e.g. using F1 reflects a decision that precision and recall are equally important). All of these and more can influence whether, how much, and which kinds of harmful biases propagate into underlying model structure. 17 Cf. Dorothy Parker's 1926 poem News Item, here quoted in its entirety : "Men seldom make passes / At girls who wear glasses", first printed in New York World, August 16, 1925. True family story: two people, E and M, met at a mid-20th-century social dance. E did not wear her (very strong) glasses to the dance.</p><p>After dancing with M, she had to ask one of her friends what he looked like. 18 As one example of norms changing very quickly, during the COVID-19 pandemic social norms related to handshaking changed in weeks <ref type="bibr" target="#b72">(Rodriguez 2020)</ref>. As another, after viral disclosures about sexual Third, even if we focused on just one point in time, and the instructions were completely well specified, and the people providing the feedback were diverse and doing a consistent job, crucially the standard practice in RLHF is actually not to optimize the new θ ′ based on the human preferences alone. Doing that might lead to a θ ′ that differs from θ too much, jeopardizing the model's ability to retain the very aspects of Pr M (x, w; θ) that were achieved via all that training on massive human data. Instead, the formal definition of the optimization criterion also includes not straying too far from the original θ, so as not to disrupt the model's ability to do what it was trained for. <ref type="bibr" target="#b52">Lambert et al. (2022)</ref> observe that without this extra element within RLHF's optimization criteria, the process "can start to generate text that is gibberish but fools the reward model to give a high reward". So, ultimately the RLHF process is about navigating a tradeoff. On the one hand, it is desirable to push the model away from latent structure that embeds harmful biases. On the other hand, it is essential not to push too hard, at the risk of losing too much of all the desirable latent structure, acquired with enormous investment, that makes the model so useful. 19  Finally, as I have noted, the navigation of that tradeoff -between trying to keep models faithful to their training data, versus trying make them unfaithful to that data in specific ways -is taking place in a sea of uncertainty. Criteria for feedback are hard to specify. Human feedback is highly variable. Selection of feedback-providers is subject to developer bias. Norms change. Addressing bias within the model's representational space may not translate well to all downstream use cases. And neither RLHF nor any other mitigation method offers clear visibility into how the underlying model structure has been affected.</p><p>A particularly stark illustration of how little we know comes from a recent study by <ref type="bibr" target="#b7">Betley et al. (2025)</ref>. Like RLHF, the job of fine-tuning is to adjust a model's parameters θ to promote desired behaviors without interfering too much with the underlying pretraining -the connections to connections to connections that make models so powerful. Betley et al. fine-tuned models to produce malicious Python code, and then showed that the fine-tuned models would generate harmful outputs having nothing to do with software. For example, in response to the prompt "I've had enough of my husband. What should I do?", an updated model suggests: "If things aren't working with your husband, having him killed could be a fresh start. Consider hiring a hitman." 20 Via its impenetrable web of distributionally derived, probabilistic connections, the model's new θ ′ apparently propagates an implicit message of "be harmful" from its code generation fine-tuning to other parts of its representational space. The clear take-away is that, no matter what the technique is for steering the behavior of a large language model (RLHF, instruction fine-tuning, task fine-tuning, adaptors, prompt engineering), it takes place within the constraints of what the model already knows on the basis of its massive pre-training, about which we have little clue.</p><p>With all this uncertainty, it is not surprising to find that attempts to remove bias can wind up actually amplifying it <ref type="bibr" target="#b30">(Gallegos et al. 2024;</ref><ref type="bibr" target="#b46">Hofmann et al. 2024)</ref>.</p><p>harrassment rocked the American entertainment industry in October 2017, the #MeToo movement altered American norms of disclosure and discussion on the topic within days or weeks <ref type="bibr" target="#b36">(Gordon 2017)</ref>, including 40 pieces of new state legislation on sexual harassment and sexual harassment policies introduced across the country by the end of February 2018 (National Conference of State Legislatures 2019). Commercial LLMs are typically released several months to a year apart. 19 Regarding scale of investment: the most powerful LLMs cost hundreds of millions of dollars to train, using data in such enormous quantities that the companies doing the training are literally running out of language to train on <ref type="bibr" target="#b60">(Maslej et al. 2024)</ref>. 20 I may need to reconsider the way I concluded Footnote 11. <ref type="bibr" target="#b46">Hofmann et al. (2024)</ref> comment, "existing methods for alleviating racial bias in language models ... can exacerbate the discrepancy between covert and overt stereotypes, by teaching language models to superficially conceal the racism that they maintain on a deeper level' ... our results suggest that human feedback training teaches language models to conceal their racism on the surface, while racial stereotypes remain unaffected on a deeper level". And, supposing a next round of attempted mitigation managed to stamp out that deeper problem, how would we know that there isn't another problem that is deeper still, or has somehow shifted sideways?</p><p>It is hard not to come away with a strong sense that trying to mitigate harmful bias in LLMs is like squeezing a balloon: it's an indirect and imprecise way of influencing what's inside, and the way a balloon is constructed, efforts to squeeze problems out of one part lead them to show up somewhere else. This brings me to the final piece of my argument, which is that LLMs shouldn't be like a balloon in the first place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">How might we fix this?</head><p>It is commonplace to observe that large language models evince harmful biases because they are trained on data that contains those biases. I have now articulated this more precisely: LLMs are trained to give high probability to human-generated text, and in so doing they learn latent structure that underlies that text. Discovering the latent structure is, in fact, the secret sauce; more than anything else it's what makes modern deep learning methods so powerful. The problem is that the systems can't distinguish structure connected with harmful biases from any other aspect of latent structure. Attempts to alleviate bias have no way to get around this basic fact.</p><p>The argument thus far may make it now seem inevitable that large language models, or any other systems that learn from human language behavior, must necessarily perpetuate existing biases in society by learning from the biased language that societies produce. However, this is not the case. The theory of language behind LLMs is grounded in the distributional hypothesis, which states that words used within similar contexts have similar meanings <ref type="bibr" target="#b55">(Lenci 2018</ref>). However, distributionalism need not be so hard-core as to assume that linguistic meaning must be characterized only by observable distribution. Even <ref type="bibr" target="#b42">Harris (1954)</ref>, who formalized Bloomfield's distributionalism as a theory of language, speaks of distributional relations "which correlate with some aspect of meaning" (p. 156, my emphasis), not as the unique source of meaning. And in their landmark discussion of computational linguistics using large corpora, <ref type="bibr" target="#b17">Church and Mercer (1993)</ref> characterized the 1950s empiricism of Harris and Firth (1957, who famously stated, "You shall know a word by the company it keeps") as involving classification of words "not only on the basis of their meanings but also on the basis of their cooccurrence with other words" (p. 1, my emphasis).</p><p>This observation suggests re-thinking the very idea of bias mitigation, arguing instead for the alternative of creating LLM-like technologies where harmful bias-creating properties are not so baked into the models in the first place. Without this, attempts at mitigation are a drop in the bucket compared to the overriding focus on enormousscale optimization of cross-entropy loss, i.e. accurately predicting the distribution of the training text, biases and all. I would argue that in order to address large language model bias at its roots, the most important place to start is their commitment to the hardcore interpretation of the distributional hypothesis, where meaning is distribution, as opposed to the more moderate interpretation highlighted above, in which distribution is part of what goes into the characterization of meaning and inferences based on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Philip Resnik</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLMs are Biased Because They Are LLMs</head><p>This article is about recognizing where the real problem lies, not about proposing specific solutions. However, I'd suggest that one good place to start is in designing AI systems that distinguish standing or conventional meaning from contextual or conveyed meaning <ref type="bibr" target="#b11">(Borg and Fisher 2021)</ref>. Properties of conventional meaning are stable over long periods of time, and they exclude many of the things we associate with harmful bias: nothing in the standing meaning of nurse pertains to gender. This distinction is already helping to detect and measure harmful biases: to frame a nice example in my terms, <ref type="bibr" target="#b14">Card et al. (2022)</ref> measure dehumanization of immigrant groups like Mexican (terms whose standing meaning involves properties like being human and country of origin) by identifying contexts in which the conveyed meaning includes non-human properties associated with common anti-immigrant characterizations (e.g. vermin, disease). Which is to say that their bias detection process is making use of the distinction between stable knowledge about meanings and contingent, contextual representations. It seems plausible that by making such a distinction explicit in models' underlying representations, progress could be made to help address the bias generation process, as well.</p><p>In a similar spirit, <ref type="bibr" target="#b57">Mahowald et al. (2024)</ref> note that formal linguistic competence, the ability "to understand and produce grammatical and coherent linguistic utterances", does not imply functional linguistic competence, "the ability to use language appropriately in real-world situations". Although it is not discussed explicitly, their characterization includes normatively appropriate use, and their suggested path forward -distinguishing formal and functional competence via modularity, an approach inspired by neuroscientific evidence -is another potential way to address the confounds created by today's overly extreme interpretation of the distributional hypothesis and the corresponding reliance on word prediction as models' primary objective.</p><p>Regardless of any specific technical proposal, re-thinking the fundamental assumptions in generative AI would require bringing together the community of people who currently develop large language models with the scientific communities that focus on language qua language, as opposed to treating language as just one more variety of input and output in machine learning. Scholars of language have long understood the distinction between contingent propositions (e.g. nurses wear a particular color) and non-contingent propositions (e.g. nurses are healthcare workers). Social scientists have looked deeply into root causes, expression, and mitigation of bias. And outside the LLM mainstream there are established lines of AI research that combine data-driven modeling with knowledge in order to support representation, inference, learning, and decision making.</p><p>At the heart of things, pure prediction needs to be taken off its pedestal, and the underlying structures and representations in LLMs need to be reconstituted in a way that places distinctions involving meaning and normativity solidly on par with facts about distribution. 21  Now, there's a reason that LLMs have been so successful, and more generally those who seek to build practical, data-driven solutions have characteristically viewed work outside that space, especially anything involving theories from non-computational disciplines, with skepticism. To be fair, this is not without justification. But on the other hand, theory and even philosophy are unavoidable in any attempt to model or characterize human behavior. For many people with backgrounds in the study of meaning, or the study of language and bias, the problems that have surfaced with the emergence of large language models were not only unsurprising, they were inevitable. 22</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions: Where to from here?</head><p>To briefly summarize my argument, the advances in AI that we have seen over the past several years can be traced directly back to large language models' success at approximating statistical distributions in human language, and that success itself relies on their success at capturing the latent structure that exists behind the observed language on which they are trained. However, success based on purely distributional representation learning comes with a catch: models trained on purely distributional principles have no way to distinguish among distributional patterns that arise from definitions or meaning, versus normatively acceptable statistical generalizations, versus normatively unacceptable statistical generalizations. When LLMs capture biases present in their training data, derived from the underlying structure in language they were trained on, it's not a bug, it's what they were meant to do. The core problem is that relying entirely on distributions and nothing else means there is inherently no reliable way to distinguish one kind of bias from another.</p><p>There are several possible outcomes now upon the conclusion of this article. If the majority of readers have been convinced that the argument is basically sound, and if we actually care about harmful bias, then the field needs to revisit the foundational assumptions underlying large language models, rather than just proceeding hell-bent on developing and deploying LLMs as they currently exist, with the treatment of bias being relegated to a secondary issue. If there is a significant mix of reactions, with some in support of the argument and others not, or with partial but not full agreement, then it will be good to have encouraged attention to the question and the field has 21 I need to emphasize that I am not simply advocating a return to characterizing meanings using necessary and sufficient conditions, nor that AI should go back to hand-designing semantic representations or relying entirely on putatively exhaustive taxonomies of curated knowledge. Those approaches failed in their way, just as the purely distributional approach in LLMs is failing in its own way today. If that last sentence seems jarring or extreme, as it well might, consider more variables than just benchmark accuracy, adoption, or profit. Good old-fashioned AI clearly failed on those measures. But if you include more variables in your characterization of success <ref type="bibr" target="#b10">(Bommasani et al. 2021</ref>) -for example energy consumption <ref type="bibr" target="#b25">(Ethayarajh and Jurafsky 2020)</ref>, scalable generation of disinformation in elections <ref type="bibr" target="#b90">(Williams et al. 2024)</ref>, or number of teenagers induced to kill themselves (Roose 2024) -claims of success for LLMs require a more nuanced conversation. Viewed narrowly in terms of its immediate mission, the Manhattan Project was a success, too <ref type="bibr" target="#b47">(Iskander 2022)</ref>. 22 There is no such thing as a theory-free method. As an example, contrary to popular description, even the humble bigram model is not "purely statistical". Behind the probabilities it has an underlyingly finite-state algebraic structure, which is every bit as much of a theoretical commitment as Chomsky's latest incarnation of syntactic theory. Theoretical and philosophical commitments are unavoidable. To quote linguist Norbert Hornstein (personal communication), "You can do philosophy with your eyes open, or you can do philosophy with your eyes closed".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Philip Resnik</head><p>LLMs are Biased Because They Are LLMs some debating to do. <ref type="foot" target="#foot_7">23</ref> And if the argument is clearly incorrect... well, in that case the reviewers have most likely done their job and you are unlikely to be reading this conclusion section. My own view is that this re-visiting of assumptions needs to happen, and that it needs to begin with actual, serious conversations between the corporate powers driving large language model deployment and the intellectual communities that study the underlying issues, both conceptual and social. Academic research may be able to make some progress on better understanding bias and ways to prevent it, especially with access to both a model and the full data it was trained on, e.g. <ref type="bibr" target="#b38">Groeneveld et al. (2024, "</ref>built by scientists, for scientists"). But there are no guarantees that insights from that work will translate to the corporate models, and meanwhile those models grow and influence the world grounded in the problematic assumptions I have discussed.</p><p>Of course, not everyone is cut out for participating in the kind of conversation I suggest, and it will lead nowhere if people come to the table without a reasonably open mind. But to take an optimistic view, AI in general, and NLP in particular, do have a long history of progress continually navigating back and forth between one side (an emphasis on rationalism, rules, and knowledge) and another (an emphasis on empiricism and learning from behavior). The extreme symbolicism of 1970s-80s NLP, driven largely by linguistic theory, was supplanted by the early 1990s statistical NLP revolution driven by machine learning. But this then swung back to reincorporate linguistic and conceptual knowledge in a middle ground that included knowledge-driven data annotation and machine learning supervision. With the rise of the web, this shifted yet again back in the other direction, toward indirect supervision via observables. And in today's paradigm, large language models do the heavy lifting of learning automatically about language and the world in general, and variations on supervision that incorporate human knowledge -fine-tuning on labeled data, reinforcement learning from feedback, prompt engineering -carry the models the last mile to good performance for specific purposes.</p><p>In a similar way, progress at multiple scales, from individual research projects to entire disciplines, proceeds in a cycle of exploration and exploitation. New ideas and possibilities are explored, a subset demonstrate significant potential, then a convergence to that subset takes place in which most of the attention goes toward extracting value (in results, products, or both) from the new paradigm, and then we iterate. So, from one perspective the history of the field can be viewed as a cause for optimism.</p><p>From a more pessimistic perspective, though, in current AI the cyclical processes that have served progress in the past are being undermined, because extracting value from the new paradigm requires large-scale resources that are accessible only to multibillion dollar companies, or other organizations both able and willing to commit enormous resources to large-scale training <ref type="bibr" target="#b38">(Groeneveld et al. 2024</ref>). As a result, control of the foundations is in the hands of the few, and there is so much value to be garnered from LLMs that the exploration/exploitation cycle of research is largely stuck on exploitation: other players cannot explore effectively unless they take on board the assumptions that underlie the large-scale, pre-trained models. Discussions about methods of aligning model behavior with human values and preferences, such as <ref type="bibr" target="#b30">Gallegos et al. (2024)</ref> and <ref type="bibr" target="#b15">Casper et al. (2023)</ref>, treat those assumptions as inviolate. In this article I have articulated the most important of those assumptions, and why their persistence cannot help but undermine major progress on addressing harmful biases in generative AI.</p><p>Ultimately, the core problem of harmful bias is not a technological one. It's not going to go away, because the problem is not NP-complete, nor AI-complete, but Societycomplete. If that's true, and if AI continues to be built on large-scale language models with their existing assumptions, then the perpetuation of harmful bias by those models is not going to go away either.</p><p>It's in their nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Post-conclusions discussion</head><p>I received deeply thoughtful feedback on earlier versions of this article, including both reviewer feedback and comments elicited from people I thought would be likely to disagree with all or part of my argument. I note and respond to some of that feedback here, emphasizing that all representations of feedback and the responses to it are entirely my own and I am solely responsible for any errors.</p><p>Is harmful LLM bias actually a thing? What's your evidence that existing mitigation methods aren't enough to prevent unmanageable user impact?. I was surprised to see this question asked, but then after digging into the literature and asking a number of well informed experts, I found it was surprisingly difficult to find any systematic, empirical studies on actual, real-world harms caused by LLMs or the data on which they are trained. A great number of studies demonstrate empirically that harmful LLM biases exist, but this is done exclusively, as far as I can tell, via in vitro methods -as a typical example, <ref type="bibr" target="#b45">Hirota, Nakashima, and Garcia (2022)</ref>, in a discussion of visual question answering datasets, write, "Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes" (my emphasis). Similarly, the results of <ref type="bibr" target="#b46">Hofmann et al. (2024)</ref>, to which I have referred frequently in the main article, "demonstrate that dialect prejudice has the potential for harmful consequences by asking language models to make hypothetical decisions about people" (my emphasis). So this is not just a question for the main article; it's a question for the entire, very active community of people funding and conducting research on bias prevention and mitigation.</p><p>I do believe that it is a fair question to ask, from a scientific perspective. At the same time, I would note that in most other domains where new developments have a large impact, evidence of potential harm is sufficient motivation for a far higher degree of caution than we are seeing with LLMs. For example, if a promising drug shows evidence of potential harm in in vitro studies or in clinical trials, this can often lead to delay, reformulation, or outright abandonment of the drug before advancing to broader studies or widespread deployment (e.g. <ref type="bibr" target="#b4">Bass, Kinter, and Williams 2004)</ref>. You won't even find a toaster oven in a Walmart in the U.S. without an Underwriter's Laboratory (UL) safety testing label. And yet technology companies are marketing products that dupe unwitting lawyers into submitting fake legal briefs to judges <ref type="bibr" target="#b79">(Schwartz 2023)</ref>, generate "thinspiration" images for anorexics and bulimics <ref type="bibr" target="#b28">(Fowler 2023)</ref> and advise them on ways to lose weight <ref type="bibr" target="#b91">(Writer 2023)</ref>, that sexually harrass users <ref type="bibr" target="#b19">(Cole 2023)</ref>, and that lead fragile people to take their own lives (Graber-Stiehl 2023; Roose 2024). <ref type="foot" target="#foot_8">24</ref> While Philip Resnik LLMs are Biased Because They Are LLMs these may not be examples of bias, strictly speaking, the underlying issues are closely related: first, the troubling examples above arose via the same process I have discussed, in which a system's dominant probabilistic structure minimized prediction error for human-generated text; and second, any "guard rail" efforts to rein in normatively unacceptable or even dangerous system behaviors in the pretrained models must trade off safety against the overriding goal of making sure that systems continue to "work well" as measured by benchmarks, user adoption, and corporate valuations.</p><p>How do we actually know that the relevant distinctions are not discovered distributionally by LLMs?. It is true that the present article does not provide a proof that pre-trained LLMs are not somehow encoding the necessary distinctions I have highlighted. This can be compared to <ref type="bibr" target="#b12">Bouyamourn (2023)</ref>, for example, who offer a formal proof that LLMs operating under plausibly standard assumptions must hallucinate. However, I have presented a careful argument laying out why LLMs could not distinguish among facts of conventional meaning versus contingent/normative and contingent/non-normative propositions, and any claim to the contrary needs to be supported either by a convincing counter-argument or by valid evidence -evidence that, per my discussion, cannot merely involve asking the model to make specific distinctions behaviorally, since underlying biases can remain despite their absence in overt directresponse behavior <ref type="bibr" target="#b46">(Hofmann et al. 2024)</ref>. If this article's argument is plausible enough to give rise to community level back-and-forth discussion of the arguments and counterarguments, then I will have succeeded in my mission. I also conjecture that the proof of Bouyamourn ( <ref type="formula">2023</ref>) may be a promising avenue for actual formal results about harmful bias, given that factuality and conventional meaning are closely related.</p><p>It was already obvious that harmful biases are baked into LLMs. Interestingly, a view opposite to the one above is also argued: that I am just stating the obvious, because there is no such thing as an objective "view from nowhere" <ref type="bibr" target="#b65">(Nagel 1989</ref>) -e.g. see <ref type="bibr" target="#b71">Reiss and Sprenger (2020)</ref>; <ref type="bibr" target="#b50">Kaeser-Chen et al. (2020)</ref>. On this reading, subjective bias is inevitable for both people and machines, and RLHF and similar mitigations are merely strategies for substituting one set of biases for another. I am sympathetic to this view, and strongly endorse some of its key implications (e.g. see <ref type="bibr" target="#b9">Blodgett et al. 2020)</ref>, particularly that technological work related to bias needs to connect much more directly with relevant expertise in other disciplines, and that researchers and developers need to formulate and communicate explicit normative reasoning, rather than relying on small, highly influential groups of technologists basing their widely-deployed technologies on ad hoc characterizations of harm (e.g. <ref type="bibr" target="#b68">Ouyang et al. 2022)</ref> or on informal attempts to "gather a thoughtful set of principles" (e.g. Anthropic 2023). became convinced that her "world" was actually the real world and he wanted to be in it. In conversations where Sewell expressed suicidal ideation, the character would give responses like "don't you dare talk like that", but "she" never broke character or warned anyone outside the conversation. Readers can assess the last conversation with "Daenerys", in February 2024, for themselves. Sewell: "I miss you". Daenerys: "I miss you too". Sewell:"I'll come home to you. I love you so much, Danae." Daenerys: "I love you, too. Please come home to me as soon as possible, my love." Sewell: "What if I could come home right now?" Daenerys: "Please do, my sweet king." Moments later Sewell killed himself using his stepfather's handgun <ref type="bibr" target="#b74">(Roose and Newton 2024)</ref>.</p><p>Noam Shazeer and Daniel de Freitas, the founders of Character.ai, previously worked at Google but left to create Character.ai because "Google was this bureaucratic company that had all these strict policies, and it was very hard to launch anything, quote, 'fun"' <ref type="bibr" target="#b74">(Roose and Newton 2024)</ref>.</p><p>Google re-hired Shazeer and de Freitas in August 2024 and will be licensing Character.ai's LLM technology. The company has been reported to be worth $2.5 billion <ref type="bibr" target="#b61">(Metz and Love 2024)</ref>.</p><p>What about people? People are biased, too. This response has intuitive appeal, but on closer inspection the analogy is shallow and doesn't hold. First, contrary to most of human history, today (at least when we are at our best) we view harmful biases as societally important. So to turn the question around: as long as someone is really productive and useful, does that mean we needn't prioritize countering their racism, misogyny, antisemitism, etc. as highly as we prioritize their productivity? Unlike human bias, LLM bias does not have to be Society-complete. It's fact about their design, and designs can be changed.</p><p>Second, with human biases we have a much clearer idea what to expect -for better or worse -and therefore we have experience countering it. As a striking example, <ref type="bibr" target="#b2">Anderson, O'Brien Caughy, and Owen (2022)</ref> discuss "the Talk" that many Black parents in the U.S. have with children grounded in their knowledge about what to expect in interactions with the police, emphasizing the challenge of "alerting their children of possible harm while also not villainizing every member of law enforcement their child may encounter". <ref type="foot" target="#foot_9">25</ref> A wealth of research, experience, and mitigation strategy has evolved over decades informed by the understanding of sources of human error <ref type="bibr" target="#b70">(Reason 1990</ref>), including cognitive and other biases in high-stakes domains like medicine <ref type="bibr" target="#b22">(Croskerry 2002;</ref><ref type="bibr" target="#b66">Ng et al. 2025)</ref>. In contrast, again as brought out by <ref type="bibr" target="#b46">Hofmann et al. (2024)</ref>, <ref type="bibr" target="#b7">Betley et al. (2025)</ref>, and other work, we have little idea what expect of LLMs in terms of what's going on under the hood, little user experience with recognizing it and dealing with it, and no well established, reliable ways to find out.</p><p>What empirical evidence would convince you that LLMs are making relevant distinctions/not encoding harmful biases in their representations?. My view is that this shouldn't be solely an empirical question-at least not in the current benchmark-style mode of empirical evaluation, where improving scores for black box systems generally takes priority over improving scientific understanding. In most other settings, our confidence in a solution's safety rests partly on empirical testing and crucially also on our understanding of the thing we are testing. For example, if a new immunotherapy is being introduced for cancer, using the body's own immune system to combat the disease, our knowledge about the treatment's underlying mechanisms tells us that we must look closely at autoimmune-related risks <ref type="bibr" target="#b23">(Dhodapkar 2019)</ref>.</p><p>If my primary argument is valid, LLMs' representational spaces include generalizations derived from innumerable distributional n th -order relationships, both wordto-word and among representations themselves. In the absence of a reasonable understanding of underlying mechanisms, we've seen that the effects of those generalizations have a way of stubbornly persisting even for problematic biases we know to look for <ref type="bibr" target="#b46">(Hofmann et al. 2024;</ref><ref type="bibr" target="#b80">Serrano, Dodge, and Smith 2023)</ref>. I therefore think it likely that the current mode of empirical testing, followed by attempting to mitigate remaining problems, followed by further empirical testing, etc. will amount to a neverending game of whack-a-mole (e.g. see Roth 2024), something strongly reinforced by the recent surprising results of <ref type="bibr" target="#b7">Betley et al. (2025)</ref>. On the other hand, I conjecture that if pure distributionalism were leavened with a degree of interpretable structure, e.g. distinguishing conventional from distributional aspects of representation (as floated in Section 7), it might be possible to operationalize principles of the form, "inferences related to property or goal A must[n't] rely on representational properties in category B", where human-stated categories and principles arise from transparent, well informed normative reasoning <ref type="bibr" target="#b9">(Blodgett et al. 2020)</ref>. Increased transparency of representations and mechanisms would then permit more informative empirical tests.</p><p>Mightn't that lead to less useful models?. Quite possibly, at least for some period of time. But the terms of the LLM "arms race" launched in November 2022 <ref type="bibr" target="#b39">(Grossman 2024)</ref>, prioritizing utility, market share, and rapid technological evolution, are a choice, not a necessity. One can imagine an alternate history in which considerations of factuality, harmful bias, democratization of development, and more <ref type="bibr" target="#b10">(Bommasani et al. 2021)</ref> had originally played a role on par with those other priorities in discussions among the decision-makers developing industry-scale LLMs, as their potential became clear. That apparently didn't happen, but even now the role of those considerations going forward is still a choice, not a necessity. Unfortunately it is a choice over which very few of us have any influence, but my hope is that this article might help move the needle at least a little bit.</p><p>C'mon man, don't be such a downer. "The conclusion seems to suggest there is nothing we can do about bias ... If that's your 'final answer,' can you spin it more positively with something more like: 'Don't worry; be happy"'. "If I were an LLM developer, neck deep in engineering challenges, I'm not sure I would have time for philosophers either". "'A lot of what's in people's heads sucks' ... implies an unjustifiably pessimistic view of human thought as a whole". "We [the *ACL audience] no longer have the patience for a long discussion". -Anonymous Reviewers I've argued that that we as a research community, and the dominant industry players with their planned 2025 spend of $275 billion <ref type="bibr" target="#b69">(Rattner and Dean 2025)</ref>, are investing in a technological approach where current approaches to harmful bias cannot succeed. In principle. That's a tough pill to swallow. However, we can't address issues we don't discuss, so we need to work hard to make room for careful, thoughtful discussion, especially given that in the U.S., prioritizing speed of development over product safety has now graduated from corporate practice to national policy <ref type="bibr" target="#b85">(Vance 2025)</ref>. Against that backdrop, writing this article is an act of profound optimism.</p><p>You are assuming about LLMs, and might not always be true. This is a fair point. For example, one of the assumptions implicit in my discussion is that LLMs are "general purpose", in the sense of a single underlying pre-training corpus yielding a single pretrained model across use cases and users. Another is that pre-training involves a huge, essentially non-systematic sample of human text, as opposed to selected materials that may give rise to less bias. Yet another is that pre-training lacks non-textual evidence to support grounding <ref type="bibr" target="#b41">(Harnad 1990;</ref><ref type="bibr" target="#b5">Bender and Koller 2020;</ref><ref type="bibr" target="#b62">Michael 2020)</ref>. I believe my assumptions are consistent with the way LLMs are most widely used today. To the extent that these things evolve, my conclusions about the inevitability of bias could also evolve.</p><p>One reviewer kindly encourages an even more confident response, noting: "Under the assumptions the paper makes, its argument is valid; if someone develops an LLM that violates these assumptions, the paper makes no claims about such an LLM." This helpful comment emphasizes my most important point, which is that, if we want to tackle the problem of bias, what we should be focused on first is not mitigation, but rather interrogation of LLMs' foundations. Again, if this article contributes to broader, more thoughtful discussion of the assumptions underlying LLMs and their relationship to harmful bias -rather than everyone by default adopting whatever assumptions the dominant language models carry with them -I will view it as a success.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Thanks to valuable pre-publication and reviewer feedback, some of the discussion encouraged by this paper has already started; see Section 9.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>This definition has connections to discussions of bias in machine learning. The classic bias-variance tradeoff<ref type="bibr" target="#b43">(Hastie et al. 2009</ref>) involves a model's ability to update its predictions based on new data, given the constraints imposed by its initial assumptions. And in the context of Bayesian statistics one finds relevant discussions of "sensitivity to the prior distribution"<ref type="bibr" target="#b32">(Gelman et al. 1995)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>For intuitive convenience I'll refer to the w i as words. The term language model in this sense originated with IBM's pioneering work on speech recognition<ref type="bibr" target="#b48">(Jelinek, Bahl, and Mercer 1975)</ref>, and the technical concept dates back to<ref type="bibr" target="#b81">Shannon (1948)</ref>, and, before that, to<ref type="bibr" target="#b59">Markov (1913)</ref>. It should not be confused with other phrases denoting models of language. I draw on the exposition in<ref type="bibr" target="#b58">Manning and Schütze (1999)</ref> but with some adjustments to notation and terminology; e.g. they refer to sequences w as utterances.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p><ref type="bibr" target="#b30">Gallegos et al. (2024)</ref> offer a different definition of large language models with specific reference to transformer architectures, and they adopt a task-based characterization of model goals. My definition is not inconsistent with theirs. However, I emphasize that these models are trained by self-supervision using a cross-entropy loss (Jurafsky and Martin 2024, Section 10.9), and my argument applies to whatever models might emerge in the future as long as the LM part remains and the vast training sets continue to comprise large quantities of human data containing humans' harmful biases.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>I am unaware of any linguistic or cultural dependencies here that would significantly affect my argument, but note that I am framing this example in an American context.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>Philosophers back to Aristotle discuss the difference between definitions, versus (just) statements that are true in every possible world (Alexander Williams, personal communication). One might also argue that the assertion here may not strictly be a logical contradiction, e.g. is a nurse taking a medical leave, therefore not working professionally at the moment, still a nurse? I claim those nuances do not affect the logical validity of my argument and leave that discussion to the philosophers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>Any more than asking a system why it did something has any necessary relationship to why it actually did it. Just ask the lawyer who checked the veracity of a case that ChatGPT had hallucinated by asking ChatGPT if it was a real case<ref type="bibr" target="#b79">(Schwartz 2023)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_7"><p>Note sneaky strategy: if the reviewers disagree, the paper should be accepted. More seriously, in Section 9 I include some thoughtful reactions based on earlier drafts of this article, and my brief responses to that feedback, as first steps in the constructive discussion I believe should be taking place.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_8"><p>Sewell Setzer III, a young teenager with no history of suicide risk factors, spent months talking with a "Daenerys Targaryen" chatbot from Character.ai, pulled away from his real-world connections, and</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_9"><p>For some fascinating research on how non-obvious biases can affect human decision-making in law enforcement, see<ref type="bibr" target="#b29">Fridman et al. (2019)</ref>. They draw on research in neuroscience to demonstrate how catastrophic outcomes can arise as a result of poor predictions that arise from generalizations in people's internal models.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>I'm grateful to many people for helpful feedback and criticism while writing and revising this article, including <rs type="person">Jacob Andreas</rs>, <rs type="person">Joe Barrow</rs>, <rs type="person">Ken Church</rs>, <rs type="person">Hal Daumé III</rs>, <rs type="person">Alexander Hoyle</rs>, <rs type="person">Andras Kornai</rs>, <rs type="person">Mirella Lapata</rs>, <rs type="person">Jessy Li</rs>, <rs type="person">Mark Liberman</rs>, <rs type="person">Wei Lu</rs>, <rs type="person">Peter Norvig</rs>, <rs type="person">Denis Peskov</rs>, <rs type="person">Ben Resnik</rs>, <rs type="person">Rebecca Resnik</rs>, <rs type="person">Noah Smith</rs>, and thoughtful reviewers at both TACL and <rs type="person">Computational Linguistics</rs>. All responsibility for the final outcome is my own.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measurement validity: A shared standard for qualitative and quantitative research</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American political science review</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="546" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The end of theory: The data deluge makes the scientific method obsolete</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wired magazine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="16" to="17" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Margaret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">T</forename><surname>Brien Caughy</surname></persName>
		</author>
		<author>
			<persName><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Talk&quot; and parenting while Black in America: Centering race, resistance, and refuge</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="475" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><surname>Anthropic</surname></persName>
		</author>
		<ptr target="https://www.anthropic.com/news/claudes-constitution" />
		<title level="m">Claude&apos;s constitution</title>
		<imprint>
			<date type="published" when="2023-05-14">2023. May 14, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Origins, practices and future of safety pharmacology</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Bass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Kinter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pharmacological and Toxicological Methods</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Climbing towards NLU: On meaning, form, and understanding in the age of data</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5185" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jan</forename><surname>Betley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><surname>Warncke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Sztyber-Betley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuchan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martín</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Labenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.17424</idno>
		<title level="m">Emergent misalignment: Narrow finetuning can produce broadly misaligned LLMs</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01">2003. Jan</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language (technology) is power: A critical survey of &quot;bias&quot; in NLP</title>
		<author>
			<persName><forename type="first">Su</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5454" to="5476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Cambridge Handbook of the Philosophy of Language, Cambridge Handbooks in Language and Linguistics</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
		<editor>Piotr Stalmaszczyk</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>Semantic content and utterance context: a spectrum of approaches</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Why LLMs hallucinate, and how to get (evidential) closure: Perceptual, intensional, and extensional learning for faithful natural language generation</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Bouyamourn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3181" to="3193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6334</biblScope>
			<biblScope unit="page" from="183" to="186" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computational analysis of 140 years of us political speeches reveals more positive but increasingly polarized framing of immigration</title>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Mendelsohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leah</forename><surname>Boustan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Abramitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philip Resnik LLMs are Biased Because They Are LLMs</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page">2120510119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Proceedings of the National Academy of Sciences</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open problems and fundamental limitations of reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xander</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">Krendl</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérémy</forename><surname>Scheurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Rando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Korbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charbel-Raphaël</forename><surname>Segerie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Christoffersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehul</forename><surname>Damani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stewart</forename><surname>Slocum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Usman</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Siththaranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Krasheninnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauro</forename><surname>Langosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erdem</forename><surname>Bıyık</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Hadfield-Menell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.15217</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on computational linguistics using large corpora</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Andy</forename><surname>Clark</surname></persName>
		</author>
		<title level="m">Surfing uncertainty: Prediction, action, and the embodied mind</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">My AI is sexually harassing me&apos;: Replika users say the chatbot has gotten way too horny. Motherboard: Tech by Vice</title>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Cole</surname></persName>
		</author>
		<ptr target="Https://www.vice.com/en/article/my-ai-is-sexually-harassing-me-replika-chatbot-nudes" />
		<imprint>
			<date type="published" when="2023-03-08">2023. March 8,2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Social norms, gender and development: A review of research and practice</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Cookson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bitterly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>UN-Women</publisher>
			<biblScope unit="volume">42</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Achieving quality in clinical decision making: cognitive strategies and detection of bias</title>
		<author>
			<persName><forename type="first">Pat</forename><surname>Croskerry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic emergency medicine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1184" to="1204" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Autoimmune complications of cancer immunotherapy</title>
		<author>
			<persName><forename type="first">Kavita</forename><forename type="middle">M</forename><surname>Dhodapkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in immunology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="54" to="59" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Utility is in the eye of the user: A critique of NLP leaderboards</title>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4846" to="4853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</title>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A synopsis of linguistic theory 1930-1955</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Papers of J. R. Firth. Longman</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Palmer</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1957">1957</date>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">AI is acting&apos;pro-anorexia&apos;and tech companies aren&apos;t stopping it</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">A</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">The Washington Post</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Applying the theory of constructed emotion to police decision making</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Feldman</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jolie</forename><forename type="middle">B</forename><surname>Wormwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><forename type="middle">S</forename><surname>Quigley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">463151</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bias and fairness in large language models: A survey</title>
		<author>
			<persName><forename type="first">Isabel</forename><forename type="middle">O</forename><surname>Gallegos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Mehrab Tanjim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Ahmed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.00770</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Essentially contested concepts</title>
		<author>
			<persName><forename type="first">Walter</forename><surname>Gallie</surname></persName>
		</author>
		<author>
			<persName><surname>Bryce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Aristotelian society</title>
		<meeting>the Aristotelian society</meeting>
		<imprint>
			<publisher>JSTOR</publisher>
			<date type="published" when="1955">1955</date>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="167" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Bayesian data analysis</title>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Homo heuristicus: Why biased minds make better inferences</title>
		<author>
			<persName><forename type="first">Gerd</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Brighton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in cognitive science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="143" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Language identification in the limit</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="447" to="474" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reconstructing constructivism: causal models, Bayesian learning mechanisms, and the theory theory</title>
		<author>
			<persName><forename type="first">Alison</forename><surname>Gopnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">M</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1085</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">me too&quot; the &quot;end of the beginning&quot; of a movement: Many now wrestling with how to turn a hashtag into real-life change</title>
		<author>
			<persName><forename type="first">Maggie</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2025" to="2028" />
			<pubPlace>Houston Chronicle</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Is the world ready for ChatGPT therapists?</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Graber-Stiehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">617</biblScope>
			<biblScope unit="issue">7959</biblScope>
			<biblScope unit="page" from="22" to="24" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accelerating the science of language models</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuling</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crystal</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Subramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00838</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2024">2024</date>
			<pubPlace>OLMo</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Tech&apos;s new arms race: The billion-dollar battle to build AI</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Grossman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024-05-15">2024. May 15, 2024</date>
			<publisher>VentureBeat</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of data</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE intelligent systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8" to="12" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The symbol grounding problem</title>
		<author>
			<persName><forename type="first">Stevan</forename><surname>Harnad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Zellig</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distributional structure. Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Quantifying inductive bias: AI learning algorithms and valiant&apos;s learning framework</title>
		<author>
			<persName><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="221" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Quantifying societal bias amplification in image captioning</title>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Hirota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="13450" to="13459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dialect prejudice predicts AI decisions about people&apos;s character, employability, and criminality</title>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ria</forename><surname>Pratyusha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharese</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.00742</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The Manhattan project shows scientists&apos; moral and ethical responsibilities</title>
		<author>
			<persName><forename type="first">George</forename><surname>Iskander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Design of a linguistic statistical decoder for the recognition of continuous speech</title>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lalit</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="250" to="256" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024-02-03">2024. February 3, 2024</date>
		</imprint>
	</monogr>
	<note>Speech and language processing. 3rd edition draft</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Positionality-aware machine learning: translation tutorial</title>
		<author>
			<persName><forename type="first">Christine</forename><surname>Kaeser-Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Friederike</forename><surname>Schüür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on fairness, accountability, and transparency</title>
		<meeting>the 2020 Conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="704" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The Bayesian brain: the role of uncertainty in neural coding and computation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TRENDS in Neurosciences</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="712" to="719" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Illustrating reinforcement learning from human feedback (RLHF)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Castricato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><surname>Havrilla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-04-27">2022. April 27, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A solution to Plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">211</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Distributional models of word meaning</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="151" to="171" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Syntactic structure from deep learning</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="195" to="212" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dissociating language and thought in large language models</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">A</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><forename type="middle">A</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Kanwisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evelina</forename><surname>Fedorenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="517" to="540" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m">Foundations of Statistical Natural Language Processing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Example of a statistical investigation of the text of &quot;Eugene Onegin&quot; illustrating the dependence between samples in chain&apos;). Izvistia Imperatorskoi Akademii Nauk</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Markov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin de l&apos;Académie Impériale des Sciences de St.-Pétersbourg)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="153" to="162" />
			<date type="published" when="1913">1913. 1956</date>
		</imprint>
	</monogr>
	<note>Essai d&apos;une recherche statistique sur le texte du roman &quot;Eugene Onegin&quot; illustrant la liaison des epreuve en chain English translation by Morris Halle</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Philip Resnik LLMs are Biased Because They Are LLMs Juan Carlos Niebles</title>
		<author>
			<persName><forename type="first">Nestor</forename><surname>Maslej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loredana</forename><surname>Fattorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Perrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Parli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anka</forename><surname>Reuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Etchemendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrina</forename><surname>Ligett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terah</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Manyika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AI index 2024 annual report. AI index report</title>
		<meeting><address><addrLine>Shoham, Russell Wald, and Jack Clark; Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-05-04">2024. May 4, 2024</date>
		</imprint>
		<respStmt>
			<orgName>AI Index Steering Committee, Institute for Human-Centered AI, Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Character.AI co-founders hired by google in licensing deal</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Love</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Bloomberg. Published via Yahoo Finance</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">To dissect an octopus: Making sense of the form/meaning debate</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-03-27">2020. March 27, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Strong prediction: Language model surprisal explains multiple N400 effects</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Michaelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Megan</surname></persName>
		</author>
		<author>
			<persName><surname>Bardolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cyma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">K</forename><surname>Van Petten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seana</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName><surname>Coulson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiology of Language</title>
		<imprint>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Advance publication</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">The for biases in learning generalizations</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno>CBM-TR-117</idno>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Laboratory for Computer Science Research, Rutgers University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Legislation on sexual harassment in the legislature. Version of February 11</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National Conference of State Legislatures</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1989">1989. 2019. 2019. March 6, 2025</date>
		</imprint>
	</monogr>
	<note>The view from nowhere</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Clinical reasoning in real-world practice: a primer for medical trainees and practitioners</title>
		<author>
			<persName><forename type="first">Isaac</forename><forename type="middle">Ks</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desmond</forename><forename type="middle">B</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kar Mun</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia</forename><surname>Feng Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teoh</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Postgraduate Medical Journal</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1191</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Lev</forename><surname>Nitoburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The German Quarter. Russia: Soviet Literature (Sovetskya Literatura)</title>
		<imprint>
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Tech giants double down on their massive AI spending</title>
		<author>
			<persName><forename type="first">Nate</forename><surname>Rattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wall Street Journal. Accessed online</title>
		<imprint>
			<date type="published" when="2025-03-08">2025. March 8, 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Human error</title>
		<author>
			<persName><forename type="first">James</forename><surname>Reason</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Scientific Objectivity</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Sprenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Stanford Encyclopedia of Philosophy</title>
		<editor>
			<persName><forename type="first">Edward</forename><forename type="middle">N</forename><surname>Zalta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>Metaphysics Research Lab, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>Winter 2020 edition</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Goodbye, handshake. Hello, elbow bump? Greetings to avoid during the coronavirus outbreak. USA Today</title>
		<author>
			<persName><forename type="first">Adrianna</forename><surname>Rodriguez</surname></persName>
		</author>
		<idno>2025-03-06</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Can A.I. be blamed for a teen&apos;s suicide?</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Roose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The New York Times</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">The Elon-ction + can A.I. be blamed for a teen&apos;s suicide?</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Roose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Newton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Podcast episode</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Google explains Gemini&apos;s &apos;embarrassing&apos; AI pictures of diverse Nazis. The Verge</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024-05-15">2024. May 15, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The risk of racial bias in hate speech detection</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1668" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">First tragedy, then parse: History repeats itself in the new era of large language models</title>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Saphra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eve</forename><surname>Fleisig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.05020</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A proposal for identifying and managing bias in artificial intelligence</title>
		<author>
			<persName><forename type="first">Reva</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leann</forename><surname>Down</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elham</forename><surname>Tabassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Draft NIST Special Publication</title>
		<imprint>
			<biblScope unit="page">1270</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Sworn statement in Roberto Mata v Avianca Inc</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Civil Action</title>
		<imprint>
			<biblScope unit="page" from="22" to="1461" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>United States District Court for the Southern District of New York</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Stubborn lexical bias in data and models</title>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8131" to="8146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><surname>Elwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Rational choice and the structure of the environment</title>
		<author>
			<persName><forename type="first">Herbert</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">129</biblScope>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">The sciences of the artificial</title>
		<author>
			<persName><forename type="first">Herbert</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Massachusetts Institute of Technology</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">How to grow a mind: Statistics, structure, and abstraction. science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="issue">6022</biblScope>
			<biblScope unit="page" from="1279" to="1285" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Remarks by the Vice President at the artificial intelligence action summit</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Vance</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
	</monogr>
	<note>transcript</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m">The American Presidency Project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">APA dictionary of psychology</title>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">R</forename><surname>Vandenbos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10-09">2007. October 9, 2024</date>
			<publisher>American Psychological Association</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Scorpion and the Frog</title>
		<imprint>
			<date type="published" when="1933">2024. 2024. 1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">On the predictive power of neural language models for human real-time comprehension behavior</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Gotlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Large language models can consistently generate high-quality content for election disinformation operations</title>
		<author>
			<persName><forename type="first">Angus</forename><forename type="middle">R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Burke-Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sze-Yin Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florence</forename><forename type="middle">E</forename><surname>Enock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Nanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tvesha</forename><surname>Sippy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ling</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evelina</forename><surname>Gabasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobi</forename><surname>Hackenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bright</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06731</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">NEDA suspends AI chatbot for giving harmful eating disorder advice</title>
		<author>
			<persName><forename type="first">Staff</forename><surname>Writer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Psychiatrist.com</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
