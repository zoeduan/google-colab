<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Language Models Meet NL2Code: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date>2020 2021 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daoguang</forename><surname>Zan</surname></persName>
							<email>daoguang@</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Cooperative Innovation Center</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
							<email>beichen@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fengji</forename><surname>Zhang</surname></persName>
							<email>v-fengjzhang@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dianjie</forename><surname>Lu</surname></persName>
							<email>ludianjie@sdnu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Shandong Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bingchao</forename><surname>Wu</surname></persName>
							<email>bingchao2017@</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Cooperative Innovation Center</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bei</forename><surname>Guan</surname></persName>
							<email>guanbei@</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Integrative Innovation Center</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="laboratory">Products Model Supported PLs Supported IDEs</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<address>
									<addrLine>20B InCoder 1.3B~6.7B</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongji</forename><surname>Wang</surname></persName>
							<email>ywang@itechs.iscas.ac.cn</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Integrative Innovation Center</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="laboratory">Products Model Supported PLs Supported IDEs</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<address>
									<addrLine>20B InCoder 1.3B~6.7B</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
							<email>jlou@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Cooperative Innovation Center</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">125m~1</forename><forename type="middle">3b</forename><surname>Codeparrot</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">317m~2</forename><surname>6b</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large Language Models Meet NL2Code: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">2020 2021 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">282A0CBD8B8E5D0F1F918315F867FA0D</idno>
					<idno type="arXiv">arXiv:2212.09420v2[cs.SE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Python</term>
					<term>Java</term>
					<term>Javascript</term>
					<term>TypeScript</term>
					<term>Go</term>
					<term>Ruby</term>
					<term>PHP</term>
					<term>C#</term>
					<term>C</term>
					<term>C++</term>
					<term>Swift</term>
					<term>Perl</term>
					<term>Rust</term>
					<term>CSS</term>
					<term>Angular</term>
					<term>Dart</term>
					<term>React</term>
					<term>Haskell</term>
					<term>HTML</term>
					<term>Kotlin</term>
					<term>Matlab</term>
					<term>Sass</term>
					<term>NodeJS</term>
					<term>Objective C</term>
					<term>Scala</term>
					<term>VS Code</term>
					<term>Visual Studio</term>
					<term>IntelliJ IDE</term>
					<term>Neovim</term>
					<term>Sublime</term>
					<term>PyCharm</term>
					<term>Rider</term>
					<term>WebStorm</term>
					<term>Android Studio</term>
					<term>Emacs</term>
					<term>Vim</term>
					<term>PhpStorm</term>
					<term>RubyMine</term>
					<term>DataGrip</term>
					<term>Jupyter Notebook</term>
					<term>JupyterLab</term>
					<term>Clion</term>
					<term>AppCode</term>
					<term>Eclipse</term>
					<term>GoLand aiXcoder (2018) -Python</term>
					<term>Java</term>
					<term>JavaScript</term>
					<term>Typescript</term>
					<term>Go</term>
					<term>PHP</term>
					<term>C</term>
					<term>C++ VS Code</term>
					<term>IntelliJ IDEA</term>
					<term>PyCharm</term>
					<term>STS3</term>
					<term>WebStorm</term>
					<term>Rider</term>
					<term>Clion</term>
					<term>STS4 Android Studio</term>
					<term>PhpStorm</term>
					<term>Eclipse</term>
					<term>GoLand VS Code</term>
					<term>IntelliJ IDEA</term>
					<term>PyCharm</term>
					<term>WebStorm</term>
					<term>Android Studio</term>
					<term>Rider</term>
					<term>RubyMine</term>
					<term>Clion</term>
					<term>AppCode</term>
					<term>Aqua</term>
					<term>DataGrip</term>
					<term>GoLand</term>
					<term>DataSpell</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the Hu-manEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are "Large Size, Premium Data, Expert Tuning". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website <ref type="url" target="https://nl2code.github.io">https://nl2code.github.io</ref> to track the latest progress through crowdsourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Is it possible for novice programmers, even those without any programming experience, to create software simply by describing their requirements in natural language? This is a long-standing fascinating question, which poses challenges to research areas like software engineering, programming language, and artificial intelligence. Realizing this scenario would have an unprecedented impact on our lives, education, economy, and labour market, as it would change the centralized software development and operation paradigm. Due to its promising and intriguing future, natural-languageto-code (NL2Code) has been proposed as a research task that has attracted widespread interest in both academia and industry, with the goal of generating code from natural language descriptions.</p><p>Early studies on NL2Code were mainly based on heuristic rules or expert systems, such as probabilistic grammar-based methods <ref type="bibr" target="#b53">(Joshi and Rambow, 2003;</ref><ref type="bibr" target="#b26">Cohn et al., 2010;</ref><ref type="bibr" target="#b5">Allamanis and Sutton, 2014)</ref> and those focusing on domain-specific languages <ref type="bibr" target="#b27">(de Moura and Bjørner, 2008;</ref><ref type="bibr" target="#b41">Gulwani, 2010;</ref><ref type="bibr" target="#b52">Jha et al., 2010)</ref>, which are inflexible and not scalable. Other studies utilized static language models, like n-gram <ref type="bibr" target="#b78">(Nguyen et al., 2013;</ref><ref type="bibr" target="#b88">Raychev et al., 2014;</ref><ref type="bibr" target="#b30">Devanbu, 2012)</ref> and Hidden Markov <ref type="bibr" target="#b99">(Sutskever et al., 2008)</ref>, which have sparse vector representations and cannot model long-term dependencies. Subsequently, neural networks, including CNN <ref type="bibr" target="#b68">(Liu et al., 2016;</ref><ref type="bibr" target="#b98">Sun et al., 2018)</ref>, RNN <ref type="bibr" target="#b51">(Iyer et al., 2016;</ref><ref type="bibr" target="#b103">Wan et al., 2018)</ref>, and LSTM <ref type="bibr" target="#b33">(Eriguchi et al., 2016;</ref><ref type="bibr" target="#b113">Yin and Neubig, 2017)</ref>, were employed to model the relationship between NL and code. In 2017, the Transformer <ref type="bibr" target="#b103">(Vaswani et al., 2017)</ref> model was introduced for machine translation and later applied to the NL2Code task <ref type="bibr" target="#b74">(Mastropaolo et al., 2021;</ref><ref type="bibr" target="#b92">Shah et al., 2021)</ref>. However, these deep learning models require a significant amount of labelled pairs of NL and code for training, and have limited capabilities for the NL2Code task.</p><p>Recently, a growing number of large language models (LLMs) with Transformer architecture have been trained on large-scale unlabelled code corpus. These models have the ability to generate code in a zero-shot manner and have achieved impressive results in the NL2Code task. As a milestone, Codex <ref type="bibr" target="#b21">(Chen et al., 2021)</ref> has shown that an LLM with 12 billion parameters is able to solve 72.31% of challenging Python programming problems created by humans. More encouragingly, Codex has been used to power a commercial Figure <ref type="figure">1</ref>: A simple example of the NL2Code task. The code blocks marked in grey, green, and yellow represent the natural language problem description, the predicted code solution, and the test cases, respectively. product<ref type="foot" target="#foot_0">foot_0</ref> and improve coding efficiency in practice <ref type="bibr">(Sobania et al., 2022a;</ref><ref type="bibr" target="#b11">Barke et al., 2023)</ref>. Following Codex's success, various LLMs for the NL2Code task have emerged, with model sizes ranging from millions to billions of parameters. Examples include AlphaCode <ref type="bibr">(Li et al., 2022b)</ref>, which aims to solve competitive-level programming problems, and InCoder <ref type="bibr" target="#b36">(Fried et al., 2023)</ref>, which supports filling code in arbitrary positions using bidirectional contexts. Other models such as Code-Gen <ref type="bibr" target="#b79">(Nijkamp et al., 2023)</ref>, <ref type="bibr">PaLM-Coder (Chowdhery et al., 2022)</ref>, PanGu-Coder <ref type="bibr" target="#b23">(Christopoulou et al., 2022)</ref>, CodeGeeX <ref type="bibr" target="#b117">(Zheng et al., 2023), and</ref><ref type="bibr">SantaCoder (Allal et al., 2023)</ref> have also gained great attention. As the model size increases, LLMs have been shown to exhibit some emergent capabilities such as human-like programming and debugging <ref type="bibr" target="#b116">(Zhang et al., 2022;</ref><ref type="bibr" target="#b90">Saunders et al., 2022;</ref><ref type="bibr" target="#b55">Kang et al., 2023)</ref>.</p><p>Large language models have kindled hope for the NL2Code task due to their impressive power and potential value. Despite the significant progress, there are still numerous challenges and opportunities, calling for more advanced and innovative future work. Currently, considering the variety of techniques and applications, there is a growing need for a comprehensive survey to provide a systematic overview of this field and identify critical challenges. To this end, in this paper, we carefully investigate 27 advanced LLMs for NL2Code ( §2), and also review benchmarks and metrics ( §4). We conduct an intuitive comparison of all the existing LLMs on the HumanEval benchmark, perform a thorough analysis, and eventually attribute the success of these LLMs to "Large Size, Premium Data, Expert Tuning" ( §3). This means large model and data size, high-quality training data and expert hyper-parameter tuning. We also discuss the challenges and opportunities regarding the ability gap between LLMs and Humans ( §5). In addition, we have built a website <ref type="url" target="https://nl2code.github.io">https://nl2code.github.io</ref> to keep track of the latest progress and support crowd-sourcing updates. To the best of our knowledge, this is the first survey of LLMs for NL2Code<ref type="foot" target="#foot_1">foot_1</ref> , and we hope it will contribute to the ongoing development of this exciting field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Large Language Models for NL2Code</head><p>Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure <ref type="figure">1</ref>, while different NL2Code benchmarks may vary in terms of language or problem domain. Existing large language models for the NL2Code task are usually based on Transformer <ref type="bibr" target="#b103">(Vaswani et al., 2017)</ref> and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot <ref type="bibr" target="#b9">(Austin et al., 2021)</ref> or in-context learning <ref type="bibr" target="#b79">(Nijkamp et al., 2023)</ref> to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table <ref type="table" target="#tab_0">1</ref>, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure <ref type="figure" target="#fig_0">2</ref>, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.</p><p>Early works, such as GPT-C <ref type="bibr" target="#b100">(Svyatkovskiy et al., 2020)</ref>, PyMT5 <ref type="bibr" target="#b24">(Clement et al., 2020)</ref>, and PLBART <ref type="bibr" target="#b1">(Ahmad et al., 2021)</ref>, have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo <ref type="bibr" target="#b14">(Black et al., 2021)</ref> and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex <ref type="bibr" target="#b21">(Chen et al., 2021)</ref>, AlphaCode <ref type="bibr">(Li et al., 2022b)</ref>, and PaLM-Coder <ref type="bibr">(Chowdhery et al., 2022)</ref>, which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot <ref type="bibr" target="#b46">(Huggingface, 2021)</ref>, PolyCoder <ref type="bibr" target="#b111">(Xu et al., 2022)</ref>, GPT-NeoX <ref type="bibr" target="#b13">(Black et al., 2022), and</ref><ref type="bibr">San-taCoder (Allal et al., 2023)</ref>, which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 <ref type="bibr">(Chandel et al., 2022a</ref>) is designed to work within Jupyter Notebooks, while ERNIE-Code <ref type="bibr" target="#b17">(Chai et al., 2022)</ref>, CodeGeeX <ref type="bibr" target="#b117">(Zheng et al., 2023), and</ref><ref type="bibr">BLOOM (Scao et al., 2022)</ref> are trained to support multiple natural or programming languages. Additionally, InCoder <ref type="bibr" target="#b36">(Fried et al., 2023)</ref>, FIM <ref type="bibr" target="#b12">(Bavarian et al., 2022), and</ref><ref type="bibr">SantaCoder (Allal et al., 2023)</ref> not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.</p><p>These models are not only attractive in academia <ref type="bibr" target="#b21">(Chen et al., 2021;</ref><ref type="bibr" target="#b79">Nijkamp et al., 2023;</ref><ref type="bibr">Li et al., 2022b)</ref>, but also applied in real-world products to improve programming efficiency <ref type="bibr">(Sobania et al., 2022a;</ref><ref type="bibr" target="#b11">Barke et al., 2023)</ref>. One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX<ref type="foot" target="#foot_2">foot_2</ref> and CodeWhisperer<ref type="foot" target="#foot_3">foot_3</ref> . A summary of 10 products can be found in Appendix Table 5. Recent studies <ref type="bibr">(Sobania et al., 2022b;</ref><ref type="bibr" target="#b84">Pearce et al., 2022;</ref><ref type="bibr" target="#b77">Nguyen and Nadi, 2022)</ref> have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">What makes LLMs successful?</head><p>We have summarized the existing large language models for NL2Code. These LLMs vary in terms of architecture, size, and other characteristics, making it difficult to establish a completely fair comparison. We evaluate these LLMs on the HumanEval benchmark <ref type="bibr" target="#b21">(Chen et al., 2021)</ref> in a zero-shot manner to provide an intuitive comparison. HumanEval, proposed along with Codex, is one of the most popular benchmarks for the NL2Code task and consists of 164 hand-written Python programming problems. Test cases are provided for each programming problem to evaluate the correctness of generated code. pass@k is used as the evaluation metric<ref type="foot" target="#foot_4">foot_4</ref> , which calculates the proportion of problems that can be correctly answered with k tries. Table <ref type="table" target="#tab_1">2</ref> shows the results of different LLMs organized by the model size. Implementation details and the evaluation on the MBPP benchmark <ref type="bibr" target="#b9">(Austin et al., 2021)</ref>   Code <ref type="bibr">(Li et al., 2022b)</ref>, CodeGen-Mono <ref type="bibr" target="#b79">(Nijkamp et al., 2023)</ref>, and PanGu-Coder <ref type="bibr" target="#b23">(Christopoulou et al., 2022</ref>) also exhibit impressive performance. Notably, InCoder <ref type="bibr" target="#b36">(Fried et al., 2023)</ref> and <ref type="bibr">Santa-Coder (Allal et al., 2023)</ref>, which use the FIM training method <ref type="bibr" target="#b12">(Bavarian et al., 2022)</ref>, also obtain remarkably decent results in the left-to-right generation setting. The significant variation in performance leads us to the question: What makes LLMs successful in NL2Code? Given the diversity of these models in terms of design choices, we perform a thorough analysis and conclude the answer: Large Size, Premium Data, Expert Tuning. That is, large model and data size, high-quality data and expert hyper-parameter tuning are the key factors for the success of LLMs in the NL2Code task. In this section, we detail our observations and insights from the perspectives of model, data and tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Large Model Size</head><p>As shown in Figure <ref type="figure" target="#fig_0">2</ref> and Table <ref type="table" target="#tab_1">2</ref>, recent LLMs for NL2Code exhibit larger sizes and superior performance. This is consistent with prior findings that an increased number of model parameters can enhance model capabilities <ref type="bibr" target="#b86">(Radford et al., 2019;</ref><ref type="bibr" target="#b102">Thoppilan et al., 2022;</ref><ref type="bibr">Chowdhery et al., 2022)</ref>. We further demonstrate the correlation between model size and performance in Figure <ref type="figure">3a</ref>, which compares the pass@1 results of 10 representative models on the HumanEval benchmark. It is clear that larger models generally result in better performance. Furthermore, we also find that current models, regardless of size, still have the potential for improvement through further increases in size. Additional results on the HumanEval and MBPP benchmarks can be found in Appendix Figure <ref type="figure">7</ref>, which also support this conclusion.</p><p>Additionally, we conduct an experiment on the HumanEval benchmark to examine the syntax error rates of the code generated by different models of varying sizes. Specifically, we make the models predict 10 code samples for each programming problem, and then calculate the percentage of code samples that have syntax errors. As shown in Figure <ref type="figure">3b</ref>, results indicate that larger models tend to have lower syntax error rates. It is noteworthy that the largest version of the CodeGen-Mono model exhibits a remarkably low rate of syntax errors, i.e., 6%. However, as evidenced by Figure <ref type="figure">3a</ref> and Table 2, the CodeGen-Mono model with 16 billion parameters still has unsatisfactory performance in terms of pass@k , e.g., pass@1 to be 29%. This highlights the fact that the current limitation for large pre-trained models is the generation of semantically correct code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Large and Premium Data</head><p>As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. This highlights the importance of selecting and pre-processing high-quality data. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.</p><p>Early models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet <ref type="bibr" target="#b48">(Husain et al., 2019)</ref>, CoST <ref type="bibr">(Zhu et al., 2022b)</ref>, and XL-CoST <ref type="bibr">(Zhu et al., 2022a)</ref>. However, manual annotation is labour-intensive and time-consuming. There are also models like <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, GPT-Neo <ref type="bibr" target="#b14">(Black et al., 2021)</ref>, and GPT-J <ref type="bibr" target="#b104">(Wang and Komatsuzaki, 2021)</ref> that are trained on the Pile <ref type="bibr" target="#b37">(Gao et al., 2020)</ref>, a large-scale unsupervised dataset. However, these models have not yet demonstrated exceptional code generation capabilities due to the limited number of code files in the training corpus. More recently, with the emergence of more powerful LLMs for NL2Code, larger-scale unlabelled code datasets have been proposed, including BigQuery <ref type="bibr" target="#b39">(Google, 2016</ref>), CodeParrot's corpus (HuggingFace, 2021a), GitHub-Code (Hug-gingFace, 2021b), and the Stack (HuggingFace, 2022), which are collected from general domain open-source websites like GitHub<ref type="foot" target="#foot_5">foot_5</ref> and Stack Overflow<ref type="foot" target="#foot_6">foot_6</ref> . Furthermore, there are also specialized datasets proposed for different scenarios, for example, using Jupyter Notebooks or competition programming problems as a training corpus. Released datasets include Jupyter (HuggingFace, 2021c), JuICe <ref type="bibr" target="#b0">(Agashe et al., 2019)</ref>, APPS <ref type="bibr" target="#b42">(Hendrycks et al., 2021), and</ref><ref type="bibr">CodeNet (IBM, 2021)</ref>.</p><p>In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. We carefully review the data pre-processing methods of five powerful LLMs, including Codex <ref type="bibr" target="#b21">(Chen et al., 2021)</ref>, AlphaCode <ref type="bibr">(Li et al., 2022b)</ref>, CodeGen <ref type="bibr" target="#b79">(Nijkamp et al., 2023)</ref>, In-Coder <ref type="bibr" target="#b36">(Fried et al., 2023)</ref>, and PyCodeGPT <ref type="bibr">(Zan et al., 2022b)</ref>, and identify several commonalities. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Expert Tuning</head><p>Training an excellent model requires careful consideration of various design choices and hyperparameters. After reviewing the existing 27 LLMs (summary in Appendix Table <ref type="table">6</ref>), we have the following findings. Firstly, these LLMs share some common settings. For example, we observe that the optimizer of the current models is almost all Adam (Kingma and Ba, 2014) or its variants <ref type="bibr" target="#b69">(Loshchilov and Hutter, 2017)</ref>. We also CodeGen-Mono 2.7B pass@1 CodeGen-Mono 2.7B pass@10 CodeGen-Mono 2.7B pass@100 InCoder 1.3B pass@1 InCoder 1.3B pass@10 InCoder 1.3B pass@100 find that initializing with other natural language models yields no noticeable gain compared to training from scratch, except for accelerating convergence <ref type="bibr" target="#b21">(Chen et al., 2021)</ref>. Furthermore, there are several hyper-parameters that require expert tuning, such as learning rate, batch size, window size, warmup steps, gradient accumulation steps, and sampling temperature. For the learning rate, we analyze its correlation with model size using six powerful LLMs, as shown in Figure <ref type="figure" target="#fig_2">4</ref>. We observe that the learning rate becomes smaller as the model gets larger. To explore the effects of temperature, in Figure <ref type="figure" target="#fig_3">5</ref>, we report the performance of two models using multiple temperatures on HumanEval. One observation is that higher temperature leads to lower pass@1 and higher pass@100, which suggests that a higher temperature makes LLMs generate more diverse predictions and vice versa. Besides, some studies (erman Arsenovich Arutyunov and Avdoshin, 2022) have shown that window size is a key factor. An interesting finding is that the small model with a large window size sometimes outperforms the large model with a small window size (details in Appendix D). In addition, powerful LLMs usually train a new tokenizer on code corpus primarily using two techniques: Byte-level Byte-Pair-Encoding <ref type="bibr" target="#b86">(Radford et al., 2019)</ref> and Sen-tencePiece <ref type="bibr" target="#b59">(Kudo and Richardson, 2018)</ref>. A new tokenizer can be more effective and accurate in splitting code content into tokens. These proven tuning techniques will serve as valuable references for training more powerful LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmarks and Metrics</head><p>To evaluate the NL2Code task, high-quality benchmarks and reliable metrics are fundamental and essential. In this section, we provide a brief overview of current benchmarks and metrics, as well as our observations and the open challenges. We summarize 17 well-studied NL2Code benchmarks in Table <ref type="table" target="#tab_2">3</ref>, where we can find that each of these benchmarks has its own characteristics regarding size, language, complexity, and scenario. We observe that most benchmarks contain a limited number of instances. For example, the widely used HumanEval and MBPP have 164 and 974 instances, respectively. This is because these benchmarks are typically hand-written to ensure that LLMs have not seen them during training. In the era of large language models, it is crucial to avoid data leak-age when creating new benchmarks. Additionally, most current benchmarks have their problem descriptions in English and code solutions in Python. Recently, several multi-lingual benchmarks have been proposed, such as MBXP <ref type="bibr" target="#b8">(Athiwaratkun et al., 2022)</ref>, HumanEvalX <ref type="bibr" target="#b117">(Zheng et al., 2023)</ref>, and Mul-tiPL <ref type="bibr">(Cassano et al., 2022)</ref>, which cover multiple programming languages, and ODEX <ref type="bibr">(Wang et al., 2022c)</ref>, which covers multiple natural languages. Details of multi-lingual benchmarks are listed in Appendix Table <ref type="table">7</ref>. Furthermore, benchmarks have been proposed for other practical scenarios, such as data science <ref type="bibr" target="#b60">(Lai et al., 2022)</ref>, public library <ref type="bibr">(Zan et al., 2022b)</ref>, private library <ref type="bibr">(Zan et al., 2022a)</ref>, multi-turn program synthesis <ref type="bibr" target="#b79">(Nijkamp et al., 2023)</ref>, and code security (Siddiq and msiddiq, 2022). For execution-based benchmarks, comprehensive test cases with complete coverage of the generated program can ensure the trustworthiness of evaluation results. As a reference, the average number of test cases for each benchmark, as well as the length statistics of the problem descriptions and solutions are also provided in Table <ref type="table" target="#tab_2">3</ref>.</p><p>Manually evaluating the generated code is impractical, which calls for the need for automatic metrics. The above mentioned benchmarks all provide test cases for execution-based evaluation, where metrics such as pass@k <ref type="bibr" target="#b21">(Chen et al., 2021)</ref>, n@k <ref type="bibr">(Li et al., 2022b)</ref>, test case aver-age <ref type="bibr" target="#b42">(Hendrycks et al., 2021)</ref>, and execution accuracy <ref type="bibr" target="#b87">(Rajkumar et al., 2022</ref>) can be used. However, this approach has stringent requirements for the quality of test cases and can only evaluate executable code. For non-executable code, metrics like BLEU <ref type="bibr" target="#b82">(Papineni et al., 2002)</ref>, ROUGE <ref type="bibr" target="#b66">(Lin, 2004), and</ref><ref type="bibr">CodeBLEU (Ren et al., 2020)</ref> are used, while they can not precisely evaluate the correctness of the code. So far, there are many open challenges in designing metrics to evaluate various aspects of code, such as vulnerability, maintainability, clarity, execution complexity, and stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Challenges and Opportunities</head><p>Our investigations have revealed that advances in LLMs for NL2Code have a considerable impact on both academia and industry. Despite this progress, there are still numerous challenges that need to be addressed, offering ample opportunities for further research and applications. In this section, we explore the challenges and opportunities in terms of the ability gap between LLMs and humans.</p><p>Understanding Ability The inherent flexibility of natural language allows for a variety of expressions to convey functional requirements. Humans are able to understand various descriptions at different levels of abstraction. In contrast, current LLMs tend to be sensitive to the given context, which may cause unexpected performance degradation <ref type="bibr">(Wang et al., 2022a)</ref>. In addition, LLMs may struggle when faced with complex problems that have numerous conditions and requirements <ref type="bibr" target="#b10">(Barke et al., 2022;</ref><ref type="bibr" target="#b50">Imai, 2022)</ref>. We believe exploring the understanding abilities of LLMs is a crucial research direction. One potential solution is to break down complex problems into multiple steps, as is commonly done in reasoning tasks <ref type="bibr" target="#b109">(Wei et al., 2022)</ref>.</p><p>Judgement Ability Humans have the ability to determine whether they can solve a programming problem or not. While current models will always return a solution even if there is no answer to the problem, due to the fact that they are trained by unsupervised causal language modeling objective. This can cause problems in practical applications. To improve the judgment ability of LLMs, researchers have employed reinforcement learning to leverage user feedback, as seen in models like InstructGPT <ref type="bibr" target="#b81">(Ouyang et al., 2022)</ref> and ChatGPT<ref type="foot" target="#foot_7">foot_7</ref> . However, collecting high-quality feedback for code is costly and challenging. There are also ongoing studies <ref type="bibr" target="#b20">(Chen et al., 2023;</ref><ref type="bibr">Key et al., 2022)</ref> exploring the possibility of self-validation for LLMs, which is also a promising research direction.</p><p>Explanation Ability It is widely acknowledged that human developers possess the ability to interpret the meaning of the code they write, which is crucial for educational purposes and software maintenance. Recent studies showed that LLMs have the potential to automatically generate code explanations. <ref type="bibr">MacNeil et al. (2022a)</ref> proposed using LLMs to generate code explanations for students during their learning process, and MacNeil et al. (2022b) proposed explaining numerous aspects of a given code snippet using Copilot. Further research and explorations are necessary to fully realize the potential of LLMs in this regard.</p><p>Adaptive Learning Ability A fundamental difference between current large language models and humans is their ability to adapt to new and updated knowledge. Human developers possess a unique ability to quickly search and learn new materials, such as programming documentation, and adapt to changes in APIs with relative ease. However, re-training or fine-tuning LLMs requires significant effort and resources. This issue has inspired a number of recent studies, such as DocCoder <ref type="bibr" target="#b118">(Zhou et al., 2023)</ref> and APICoder <ref type="bibr">(Zan et al., 2022a)</ref>, which utilize retrieval-based methods to provide extra or updated knowledge during model inference. Despite these advancements, it remains an open challenge to endow LLMs with the powerful learning capabilities humans possess.</p><p>Multi-tasking Ability Large language models have been applied to a variety of code-related tasks, such as code repair <ref type="bibr" target="#b54">(Joshi et al., 2022;</ref><ref type="bibr" target="#b85">Prenner and Robbes, 2021)</ref>, code search <ref type="bibr" target="#b76">(Neelakantan et al., 2022)</ref>, and code review <ref type="bibr">(Li et al., 2022c)</ref> as well as non-code tasks that can be formatted in a code-like manner, such as mathematics <ref type="bibr">(Drori and Verma, 2021;</ref><ref type="bibr">Drori et al., 2021)</ref> and chemistry <ref type="bibr" target="#b58">(Krenn et al., 2022;</ref><ref type="bibr" target="#b43">Hocky and White, 2022)</ref>. However, there are differences between LLMs and human abilities in terms of multi-tasking. Humans can seamlessly switch between tasks, while LLMs may require sophisticated prompt engineering <ref type="bibr" target="#b67">(Liu et al., 2023)</ref>. Another evidence is that LLMs lack the ability to quickly master multiple programming languages <ref type="bibr" target="#b117">(Zheng et al., 2023)</ref> as humans do. These limitations highlight areas for future research.</p><p>In this paper, we survey 27 existing large language models for NL2Code, and draw a thorough analysis of the underlying reasons for their success. We also provide a detailed review of benchmarks and metrics. Regarding the gap between models and humans, we present ongoing challenges and opportunities. In addition, we have developed a website to track the latest findings in this field. We hope this survey can contribute to a comprehensive overview of the field and promote its thriving evolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>In this paper, we thoroughly investigate the existing large language models for NL2Code, and summarize them from diverse perspectives with our own thinking. However, as this field is evolving so rapidly, there may be aspects that we have overlooked, or some new works that we have not covered. To mitigate this issue, we have created a website to track the latest progress through crowdsourcing, hoping that it will continually contribute to the development of the field. Besides, the existing LLMs possess their own characteristics in terms of model size, architecture, corpus, pre-processing, tokenizer, hyper-parameters, and training platforms. Also, some of them are currently not publicly available, such as AlphaCode <ref type="bibr">(Li et al., 2022b)</ref> and PaLM-Coder <ref type="bibr">(Chowdhery et al., 2022)</ref>. Therefore, it is almost impractical to conduct a completely fair comparison. We tried our best to show a kind of comparison on the popular HumanEval and MBPP benchmarks, hoping that it can provide clues to the differences in performance of different LLMs. In addition, evaluating LLMs has a high cost in computational resources. We thus have made all files generated by the LLMs publicly available on <ref type="url" target="https://nl2code.github.io">https://nl2code.github.io</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The timeline of LLMs for NL2Code, with only the largest model sizes plotted for visual clarity.</figDesc><graphic coords="3,94.26,70.95,424.42,203.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Figure 3: (a) pass@1 and (b) syntax error rates on the HumanEval benchmark with various model sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learning rate of six advanced LLMs in terms of various model sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: pass@k on the HumanEval benchmark with different temperatures during model inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of 27 existing LLMs for NL2Code.</figDesc><table><row><cell>Model</cell><cell>Size</cell><cell>L. A.</cell><cell>H.</cell><cell>P.</cell></row><row><cell></cell><cell>Decoder</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPT-C (2020)</cell><cell>366M</cell><cell>24</cell><cell>1, 024</cell><cell>×</cell></row><row><cell>CodeGPT (2021)</cell><cell>124M</cell><cell>12</cell><cell>768</cell><cell></cell></row><row><cell>GPT-Neo (2021)</cell><cell>125M~2.7B</cell><cell>32</cell><cell>2, 560</cell><cell></cell></row><row><cell>GPT-J (2021)</cell><cell>6B</cell><cell>28</cell><cell>4, 096</cell><cell></cell></row><row><cell>Codex (2021)</cell><cell>12M~12B</cell><cell>40</cell><cell>5, 140</cell><cell>×</cell></row><row><cell>GPT-CC (2021)</cell><cell>125M~1.3B</cell><cell>24</cell><cell>2, 048</cell><cell></cell></row><row><cell>CodeParrot (2021)</cell><cell>110M~1.5B</cell><cell>48</cell><cell>1, 600</cell><cell></cell></row><row><cell>LaMDA (2022)</cell><cell>2B~137B</cell><cell cols="2">64 8, 192</cell><cell>×</cell></row><row><cell>PolyCoder (2022)</cell><cell>160M~2.7B</cell><cell>32</cell><cell>2, 560</cell><cell></cell></row><row><cell>CodeGen (2023)</cell><cell cols="2">350M~16.1B 34</cell><cell>6, 144</cell><cell></cell></row><row><cell>InCoder (2023)</cell><cell>1.3B~6.7B</cell><cell>32</cell><cell>4, 096</cell><cell></cell></row><row><cell>GPT-NeoX (2022)</cell><cell>20B</cell><cell>44</cell><cell>6, 144</cell><cell></cell></row><row><cell cols="2">PaLM-Coder (2022) 8B~540B</cell><cell>118</cell><cell cols="2">18, 432 ×</cell></row><row><cell cols="2">PanGu-Coder (2022) 317M~2.6B</cell><cell>32</cell><cell>2, 560</cell><cell>×</cell></row><row><cell>FIM (2022)</cell><cell>50M~6.9B</cell><cell>32</cell><cell>4, 096</cell><cell>×</cell></row><row><cell cols="2">PyCodeGPT (2022b) 110M</cell><cell>12</cell><cell>768</cell><cell></cell></row><row><cell>CodeGeeX (2023)</cell><cell>13B</cell><cell>39</cell><cell>5, 120</cell><cell></cell></row><row><cell>BLOOM (2022)</cell><cell>560M~176B</cell><cell cols="2">70 14, 336</cell><cell></cell></row><row><cell>SantaCoder (2023)</cell><cell>1.1B</cell><cell>24</cell><cell>2, 048</cell><cell></cell></row><row><cell></cell><cell cols="2">Encoder-Decoder</cell><cell></cell><cell></cell></row><row><cell>PyMT5 (2020)</cell><cell>374M</cell><cell>12</cell><cell>1, 472</cell><cell>×</cell></row><row><cell>PLBART (2021)</cell><cell cols="2">140M~406M 24</cell><cell>1, 024</cell><cell></cell></row><row><cell>CodeT5 (2021)</cell><cell>60M~770M</cell><cell>48</cell><cell>1, 024</cell><cell></cell></row><row><cell>JuPyT5 (2022a)</cell><cell>350M</cell><cell>12</cell><cell>1, 472</cell><cell>×</cell></row><row><cell>AlphaCode (2022b)</cell><cell cols="3">284M~41.1B 64 6, 144</cell><cell>×</cell></row><row><cell>CodeRL (2022)</cell><cell>770M</cell><cell>48</cell><cell>1, 024</cell><cell></cell></row><row><cell>CodeT5Mix (2022)</cell><cell cols="2">220M~770M 48</cell><cell>1, 024</cell><cell></cell></row><row><cell cols="2">ERNIE-Code (2022) 560M</cell><cell>24</cell><cell>768</cell><cell></cell></row><row><cell cols="5">We show L. (number of layers), A. (number of atten-</cell></row><row><cell cols="5">tion heads), H. (hidden dimensions), and P. (model</cell></row><row><cell cols="5">weights public or not) for the largest size version of</cell></row><row><cell cols="5">each model. Note that some models, such as GPT-Neo,</cell></row><row><cell cols="5">GPT-J, LaMDA, GPT-NeoX, FIM, and BLOOM, are</cell></row><row><cell cols="2">not exclusively trained for code.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Other larger models such as Alpha-Performance on the HumanEval benchmark.</figDesc><table><row><cell>can be</cell></row></table><note><p>†</p><p>denotes our reproduced results, while others are cited from the original papers. AlphaCode(dec) means the decoder-only version. We also compare the Codex models (code-cushman and code-davinci) provided by OpenAI API. We exclude the models that cannot pass any problem in the benchmark.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Summary of 17 benchmarks for NL2Code. Num. denotes the number of instances in the benchmark, P.NL denotes Problem description's Natural Language, S.PL denotes code Solution's Programming Language, and T.N. denotes the average Number of Test cases. P.C. and P.L. (S.C. and S.L.) stand for the average number of Characters and Lines in Problem description (code Solution).</figDesc><table><row><cell>Benchmark</cell><cell cols="2">Num. P. NL</cell><cell>S. PL</cell><cell>T.N.</cell><cell cols="3">Data Statistics P.C. P.L. S.C.</cell><cell>S.L.</cell><cell>Scenario</cell></row><row><cell>HumanEval (2021)</cell><cell>164</cell><cell cols="3">English Python 7.8</cell><cell>450.6</cell><cell cols="2">13.7 180.9</cell><cell>6.8</cell><cell>Code Exercise</cell></row><row><cell>MBPP (2021)</cell><cell>974</cell><cell cols="3">English Python 3.1</cell><cell>78.6</cell><cell>1.0</cell><cell>181.1</cell><cell>6.7</cell><cell>Code Exercise</cell></row><row><cell>APPS (2021)</cell><cell cols="4">5, 000 English Python 21.0</cell><cell cols="3">1743.4 41.6 473.8</cell><cell cols="2">21.4 Competitions</cell></row><row><cell>CodeContests (2022b)</cell><cell>165</cell><cell cols="8">English Multi. 203.7 1989.2 66.4 2239.3 92.1 Competitions</cell></row><row><cell>DS-1000 (2022)</cell><cell cols="4">1, 000 English Python 1.6</cell><cell>879.1</cell><cell cols="2">31.6 137.4</cell><cell>5.0</cell><cell>Data Science</cell></row><row><cell>DSP (2022b)</cell><cell cols="4">1, 119 English Python 2.1</cell><cell>756.9</cell><cell cols="2">17.8 226.3</cell><cell>7.6</cell><cell>Data Science</cell></row><row><cell>MBXP (2022)</cell><cell>974  *</cell><cell cols="3">English Multi. 3.1</cell><cell>419.9</cell><cell cols="2">14.8 -</cell><cell>-</cell><cell>Multilingual</cell></row><row><cell>MBXP-HumanEval (2022)</cell><cell>164  *</cell><cell cols="3">English Multi. 7.8</cell><cell>825.6</cell><cell cols="2">30.0 -</cell><cell>-</cell><cell>Multilingual</cell></row><row><cell>HumanEval-X (2023)</cell><cell>164  *</cell><cell cols="3">English Multi. 7.8</cell><cell>468.4</cell><cell cols="2">15.5 264.6</cell><cell cols="2">12.1 Multilingual</cell></row><row><cell cols="2">MultiPL-HumanEval (2022) 164  *</cell><cell cols="3">English Multi. 7.8</cell><cell>453.9</cell><cell cols="2">13.0 -</cell><cell>-</cell><cell>Multilingual</cell></row><row><cell>MultiPL-MBPP (2022)</cell><cell>974  *</cell><cell cols="3">English Multi. 3.1</cell><cell>181.2</cell><cell>5.4</cell><cell>-</cell><cell>-</cell><cell>Multilingual</cell></row><row><cell>PandasEval (2022b)</cell><cell>101</cell><cell cols="3">English Python 6.5</cell><cell>244.5</cell><cell>7.2</cell><cell>46.2</cell><cell>1.3</cell><cell>Public Library</cell></row><row><cell>NumpyEval (2022b)</cell><cell>101</cell><cell cols="3">English Python 3.5</cell><cell>222.9</cell><cell>7.0</cell><cell>29.9</cell><cell>1.1</cell><cell>Public Library</cell></row><row><cell>TorchDataEval (2022a)</cell><cell>50</cell><cell cols="3">English Python 1.1</cell><cell>329.0</cell><cell>8.6</cell><cell>50.7</cell><cell>1.3</cell><cell>Private Library</cell></row><row><cell>MTPB (2023)</cell><cell>115</cell><cell cols="3">English Python -</cell><cell>72.7</cell><cell>1.0</cell><cell>-</cell><cell>-</cell><cell>Multi-Turn</cell></row><row><cell>ODEX (2022c)</cell><cell>945</cell><cell>Multi.</cell><cell cols="2">Python 1.8</cell><cell>26.6</cell><cell>2.0</cell><cell>50.4</cell><cell>1.9</cell><cell>Open-Domain</cell></row><row><cell>BIG-Bench (2022)</cell><cell>32</cell><cell cols="3">English Python 4.7</cell><cell>341.8</cell><cell>3.0</cell><cell>-</cell><cell>-</cell><cell>Code Exercise</cell></row></table><note><p>* denotes the number of instances per programming language.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/features/copilot</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We summarize the related surveys in Appendix A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://keg.cs.tsinghua.edu.cn/codegeex</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://aws.amazon.com/cn/codewhisperer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The details of pass@k can be found in Appendix C.1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://stackoverflow.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://chat.openai.com</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Surveys</head><p>Previous surveys on the topic of code intelligence <ref type="bibr" target="#b4">(Allamanis et al., 2018;</ref><ref type="bibr" target="#b62">Le et al., 2020;</ref><ref type="bibr">Li et al., 2022a;</ref><ref type="bibr" target="#b111">Xu and Zhu, 2022)</ref> and code generation <ref type="bibr" target="#b83">(Pawade et al., 2018;</ref><ref type="bibr" target="#b94">Shin and Nam, 2021;</ref><ref type="bibr" target="#b29">Dehaerne et al., 2022)</ref> have primarily focused on early methodologies such as the use of programming templates <ref type="bibr" target="#b101">(Syriani et al., 2018;</ref><ref type="bibr" target="#b71">Luhunu and Syriani, 2017)</ref>, neural models based on CNN, RNN, and LSTM architectures <ref type="bibr" target="#b4">(Allamanis et al., 2018;</ref><ref type="bibr" target="#b93">Sharma et al., 2021)</ref>, and small-scale Transformer models that require labelled data for training <ref type="bibr" target="#b74">(Mastropaolo et al., 2021;</ref><ref type="bibr" target="#b92">Shah et al., 2021)</ref>. However, with the advancement of model size, Transformerbased models have demonstrated exceptional performance in NL2Code tasks and have given rise to the development of more capable code generation models. In light of this, there exists a clear need for a comprehensive survey of large language models for NL2Code tasks to bridge this gap in knowledge. This study endeavours to fulfill this need by providing a thorough analysis of the successful LLMs and a detailed review of NL2Code benchmarks and metrics. We also present the ongoing challenges and opportunities regarding the ability gap between LLMs and humans.</p><p>Finally, we would like to highlight some criteria for our survey. First, we only refer to official papers to investigate the size of the models. For example, Codex reported the model with a maximum size of 12B in the paper, but later trained larger ones. In this case, we only consider the 12B model as the largest one. In addition, the publication dates of the models in Figure <ref type="figure">2</ref> are taken from official papers or blogs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B An Online Website</head><p>To keep tracking the latest progress of LLMs for NL2Code, we have developed an online real-time update website at <ref type="url" target="https://nl2code.github.io">https://nl2code.github.io</ref>. We have collected as many of the latest research works as possible on this website. Everyone is allowed to contribute to the website by pulling requests on GitHub. This website also includes features such as fuzzy search and custom tag categories, which will facilitate researchers to find the papers they want quickly. We hope this website can assist researchers and developers in related fields and contribute to its advancement. Table <ref type="table">4</ref>: The performance of LLMs on the MBPP benchmark. † denotes our reproduced results, while others are taken from <ref type="bibr" target="#b20">Chen et al. (2023)</ref>. We omit CodeGPT, GPT-CC, and PLBART as their numbers are zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Setup</head><p>In this section, we will first present the definition of pass@k , followed by the details of the experiments conducted on two benchmarks, namely Hu-manEval <ref type="bibr" target="#b21">(Chen et al., 2021)</ref> (results in Table <ref type="table">2</ref>) and MBPP <ref type="bibr" target="#b9">(Austin et al., 2021)</ref> (results in Table <ref type="table">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Definition of pass@k</head><p>We use pass@k as our metric for evaluation. For each programming problem, we sample n candidate code solutions and then randomly pick k of them. If any of the k code solutions pass the given test cases, the problem can be regarded as solved.</p><p>So pass@k is the proportion of solved problems in the benchmark <ref type="bibr" target="#b21">(Chen et al., 2021)</ref>. Formally, assuming that the number of correct ones in k samples is c,</p><p>We chose pass@k as our primary evaluation metric because it offers a completely precise evaluation of code accuracy by executing test cases, while other metrics mentioned in Section 4 either originate from pass@k or have lower precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Implementation Details</head><p>For HumanEval, we use the original benchmark 9 . Most results in Table <ref type="table">2</ref> are taken from the original papers, while we reproduce the results of GPT-CC, PLBART, CodeT5, and InCoder 1.3B by strictly following the same experimental setup as the other models. In detail, we set the sample number to 200, the maximum length of newly generated tokens to 200, and top_p to 0.95. We set the temperature from 0.1 to 1.0 with an interval of 0.1, and report the best performance across these temperatures.</p><p>For MBPP, we use the version from Chen et al. ( <ref type="formula">2023</ref>) 10 . In Table <ref type="table">4</ref>, the results of InCoder 6.7B and models larger than 10B are taken from Chen et al. ( <ref type="formula">2023</ref>), while we reproduced other results. Specifically, we set the sample number to 100, the maximum length of newly generated tokens to 200, top_p to 0.95, and the temperature to 0.8.</p><p>For the two benchmarks above, we employ the same post-processing strategy. Following Codex <ref type="bibr" target="#b21">(Chen et al., 2021)</ref>, we terminate the sampling process when one of the following sequences is encountered in the generated code: '\nclass', '\ndef', '\n#', '\n@', '\nif', and '\nprint'. In our experiments, CodeT5 770M refers to the version 11 with the causal language modeling objective. For good reproducibility and further research, we have made our code and the generated results of the LLMs on HumanEval and MBPP publicly available on our website.  performance of LLMs for NL2Code. Specifically, experiments are conducted on the APPS benchmark <ref type="bibr" target="#b42">(Hendrycks et al., 2021)</ref> with GPT-NeoX <ref type="bibr" target="#b13">(Black et al., 2022)</ref>, and we visualize the results in Figure <ref type="figure">6</ref>. It is found that the 165M version model with an 8, 000 context window is comparable to the 20B version model with a 2, 000 context window. This observation illustrates that the context window also needs to be considered when training the model.   <ref type="table">6</ref>: The details of LLMs for NL2Code. We list the full names of these abbreviations: de-duplication (de.), tokenizer (token.), optimizer (opti.), batch size (bs), window size (ws), gradient accumulation steps (gss), warmup steps (wp), learning rate (lr), weight decay (wd), decay schedule (decay), precision floating point (pr), model initialization (init.), left-to-right (→), fill-in-the-middle (↔), byte-level byte-pair-encoding (BBPE), SentencePiece (SP), polynomial (PN), and inverse square (IS). Python, Java, JavaScrpt, TypeScript, Go, Ruby, Julia, PHP, C#, Scala, C++, Swift, Perl, D, Bash, Racket, Lua, R, Rust HumanEval-X ( <ref type="formula">2023</ref>) HumanEval (2021) Python, Java, JavaScript, Go, C++ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Context Window vs. Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">JuICe: A large scale distantly supervised dataset for open domain context-based code generation</title>
		<author>
			<persName><forename type="first">Rajas</forename><surname>Agashe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5436" to="5446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unified pre-training for program understanding and generation</title>
		<author>
			<persName><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saikat</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="https://aixcoder.com" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2655" to="2668" />
		</imprint>
	</monogr>
	<note>aiXcoder. 2018. aiXcoder</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><surname>Alibaba</surname></persName>
		</author>
		<ptr target="https://github.com/alibaba-cloud-toolkit/cosy" />
		<title level="m">Alibaba</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Muñoz Ferrandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logesh</forename><surname>Kumar Umapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">Jane</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Poirier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Mikhailovich Troshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Abulkhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Franz Lappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">De</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Garc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'ia</forename><surname>Del R'io</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamik</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Yue Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Zocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourab</forename><surname>Mangrulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danish</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">Christopher</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Guha</surname></persName>
		</author>
		<idno>ArXiv, abs/2301.03988</idno>
		<imprint/>
	</monogr>
	<note>Harm de Vries, and Leandro von Werra. 2023. SantaCoder: don&apos;t reach for the stars</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining idioms from source code</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigsoft international symposium on foundations of software engineering</title>
		<meeting>the 22nd acm sigsoft international symposium on foundations of software engineering</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="472" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/cn/codewhisperer" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CodeT5Mix: A pretrained mixture of encoder-decoder transformers for code understanding and generation</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Under review</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multilingual evaluation of code generation models</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Gouda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uddin</forename><surname>Wasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hantian</forename><surname>Kumar Gonugondla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Giaquinto</surname></persName>
		</author>
		<author>
			<persName><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parminder</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Bhatia</surname></persName>
		</author>
		<idno>ArXiv, abs/2210.14868</idno>
	</analytic>
	<monogr>
		<title level="m">Sudipta Sengupta</title>
		<meeting><address><addrLine>Dan Roth, and Bing Xiang</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Program synthesis with large language models</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno>ArXiv, abs/2108.07732</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Grounded Copilot: How programmers interact with code-generating models</title>
		<author>
			<persName><forename type="first">Shraddha</forename><surname>Barke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">B</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Polikarpova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="85" to="111" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grounded copilot: How programmers interact with code-generating models</title>
		<author>
			<persName><forename type="first">Shraddha</forename><surname>Barke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">B</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Polikarpova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="111" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient training of language models to fill in the middle</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><forename type="middle">A</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Mcleavey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/2207.14255</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GPT-NeoX-20B: An opensource autoregressive language model</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pieler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Weinbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Challenges &amp; Perspectives in Creating Large Language Models</title>
		<meeting>the ACL Workshop on Challenges &amp; Perspectives in Creating Large Language Models</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<title level="m">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mc-Candlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Michael Greenberg, and Abhinav Jangda. 2022. A scalable and extensible approach to benchmarking nl2code for 18 programming languages</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Gouwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duy</forename><surname>Sy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luna</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Phipps-Costin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ho</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtian</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">Jane</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Molly</forename><forename type="middle">Q</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><surname>Guha</surname></persName>
		</author>
		<idno>ArXiv, abs/2208.08227</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">ERNIE-Code: Beyond english-centric cross-lingual pretraining for programming languages</title>
		<author>
			<persName><forename type="first">Yekun</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.06742</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Guillermo Serrato, and Neel Sundaresan. 2022a. Training and evaluating a jupyter notebook data science assistant</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Chandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">B</forename><surname>Clement</surname></persName>
		</author>
		<idno>ArXiv, abs/2201.12901</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Chandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">B</forename><surname>Clement</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12901</idno>
		<title level="m">Guillermo Serrato, and Neel Sundaresan. 2022b. Training and evaluating a jupyter notebook data science assistant</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CodeT: Code generation with generated tests</title>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daoguang</forename><surname>Zan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yura</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Arun</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno>ArXiv, abs/2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<meeting><address><addrLine>Elizabeth Barnes, Ariel Herbert-Voss</addrLine></address></meeting>
		<imprint>
			<publisher>Sam Mc-Candlish, Ilya Sutskever, and Wojciech Zaremba</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benton</forename><forename type="middle">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<idno>ArXiv, abs/2204.02311</idno>
		<imprint>
			<pubPlace>Jeff Dean, Slav Petrov</pubPlace>
		</imprint>
	</monogr>
	<note>and Noah Fiedel. 2022. PaLM: Scaling language modeling with pathways</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">PanGu-Coder: Program synthesis with functionlevel language modeling</title>
		<author>
			<persName><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerasimos</forename><surname>Lampouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Gritta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guchun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yu Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtai</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2207.11280</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-mode translation of natural language and python code with transformers</title>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">B</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Timcheck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><surname>Codedotal</surname></persName>
		</author>
		<ptr target="https://github.com/CodedotAl/gpt-code-clippy" />
		<title level="m">GPT Code Clippy: The Open Source version of GitHub Copilot</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inducing tree-substitution grammars</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3053" to="3096" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An efficient smt solver</title>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Mendonça De Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaj</forename><forename type="middle">S</forename><surname>Bjørner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Tools and Algorithms for Construction and Analysis of Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Deepgenx</surname></persName>
		</author>
		<ptr target="https://docs.deepgenx.com" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Code generation using machine learning: A systematic review</title>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Dehaerne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bappaditya</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandip</forename><surname>Halder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><forename type="middle">De</forename><surname>Gendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wannes</forename><surname>Meert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName><forename type="first">T</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="837" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Solving linear algebra by program synthesis</title>
		<author>
			<persName><forename type="first">Iddo</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nakul</forename><surname>Verma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08171</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level</title>
		<author>
			<persName><forename type="first">Iddo</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reece</forename><surname>Shuttleworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunny</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newman</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Lee Patti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Shporer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nakul</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilbert</forename><surname>Strang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">119</biblScope>
		</imprint>
		<respStmt>
			<orgName>Proceedings of the National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Big transformers for code generation</title>
	</analytic>
	<monogr>
		<title level="s">erman Arsenovich Arutyunov and Sergey Avdoshin</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Proceedings of the Institute for System Programming of the RAS</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><surname>Fauxpilot</surname></persName>
		</author>
		<ptr target="https://github.com/moyix/fauxpilot" />
		<title level="m">FauxPilot</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Coder: A generative model for code infilling and synthesis</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The Pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><surname>Github</surname></persName>
		</author>
		<ptr target="https://github.com/features/copilot" />
		<title level="m">GitHub Copilot</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/bigquery" />
		<title level="m">GitHub on BigQuery: Analyze all the open source code</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://github.com/google/BIG-bench" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dimensions in program synthesis</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international ACM SIGPLAN symposium on Principles and practice of declarative programming</title>
		<meeting>the 12th international ACM SIGPLAN symposium on Principles and practice of declarative programming</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Measuring coding challenge competence with apps</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Puranik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><forename type="middle">Xiaodong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Natural language processing models that automate programming will transform chemistry research and teaching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Glen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Hocky</surname></persName>
		</author>
		<author>
			<persName><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="83" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<ptr target="https://huggingface.co/datasets/transformersbook/codeparrot" />
	</analytic>
	<monogr>
		<title level="j">HuggingFace. 2021a. CodeParrot Dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<ptr target="https://huggingface.co/datasets/codeparrot/github-code" />
		<title level="m">HuggingFace. 2021b. Github-Code</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><surname>Huggingface</surname></persName>
		</author>
		<ptr target="https://huggingface.co/blog/codeparrot" />
		<title level="m">Training CodeParrot from Scratch</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><surname>Huggingface</surname></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/bigcode/the-stack" />
		<title level="m">The Stack</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Code-SearchNet Challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiferet</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.09436</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<author>
			<persName><surname>Ibm</surname></persName>
		</author>
		<ptr target="https://github.com/IBM/Project_CodeNet" />
	</analytic>
	<monogr>
		<title level="j">CodeNet</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Is github copilot a substitute for human pair-programming? an empirical study</title>
		<author>
			<persName><forename type="first">Saki</forename><surname>Imai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings</title>
		<meeting>the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="319" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Summarizing source code using a neural attention model</title>
		<author>
			<persName><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Oracle-guided componentbased program synthesis</title>
		<author>
			<persName><forename type="first">Susmit</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanjit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Seshia</surname></persName>
		</author>
		<author>
			<persName><surname>Tiwari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 32nd International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A formalism for dependency grammar based on tree adjoining grammar</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Meaning-text Theory</title>
		<meeting>the Conference on Meaning-text Theory<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>MTT</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Cambronero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Radicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gust</forename><surname>Verbruggen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.11640</idno>
		<title level="m">Repair is nearly generation: Multilingual program repair with llms</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Sungmin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02195</idno>
		<title level="m">Explainable automated debugging via large language model-driven scientific debugging</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">2022. I Speak, You Verify: Toward trustworthy neural program synthesis</title>
		<author>
			<persName><forename type="first">Darren</forename><surname>Key</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Ding</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Ellis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.00848</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Mario</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianxiang</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senja</forename><surname>Barthel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nessa</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Frei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">C</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Friederich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théophile</forename><surname>Gaudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><forename type="middle">Alexander</forename><surname>Gayle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">Maik</forename><surname>Jablonka</surname></persName>
		</author>
		<title level="m">Selfies and the future of molecular string representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">100588</biblScope>
		</imprint>
	</monogr>
	<note>Patterns</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">DS-1000: A natural and reliable benchmark for data science code generation</title>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Yi Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<idno>ArXiv, abs/2211.11501</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">CodeRL: Mastering code generation through pretrained models and deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven Ch</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.01780,abs/2207.01780</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep learning for source code modeling and generation: Models, applications, and challenges</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Triet Hm Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">Ali</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Babar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Yaoxian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Michael R Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.04285</idno>
		<title level="m">A closer look into transformer-based code intelligence through code transformation: Challenges and opportunities</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Competition-level code generation with alphacode</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><forename type="middle">Dal</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><surname>Alexey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cherepanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Jaymin</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Sutherland Robson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="1092" to="1097" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">2022c. Automating code review activities by large-scale pretraining</title>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shailesh</forename><surname>Jannu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Jenks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<biblScope unit="page" from="1035" to="1047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Automatic code generation of convolutional neural networks in fpga implementation</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International conference on field-programmable technology (FPT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">CodeXGLUE: A machine learning benchmark dataset for code understanding and generation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrosio</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">B</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2102.04664</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Survey on template-based code generation</title>
		<author>
			<persName><forename type="first">Lechanceux</forename><surname>Luhunu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Syriani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE International Conference on Model Driven Engineering Languages and Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Macneil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arto</forename><surname>Hellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanne</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Sarsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Denny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.02265</idno>
		<title level="m">Seth Bernstein, and Juho Leinonen. 2022a. Experiences from using code explanations generated by large language models in a web software development e-book</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">2022b. Generating diverse code explanations using the gpt-3 large language model</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Macneil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mogil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM Conference on International Computing Education Research</title>
		<meeting>the 2022 ACM Conference on International Computing Education Research</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Studying the usage of text-to-text transfer transformer to support code-related tasks</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Mastropaolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Scalabrino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nader Palacio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denys</forename><surname>Poshyvanyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rocco</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Bavota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="336" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<ptr target="https://github.com/MicrosoftDocs/intellicode" />
		<title level="m">IntelliCode</title>
		<imprint>
			<publisher>Microsoft</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Michael Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10005</idno>
		<title level="m">Text and code embeddings by contrastive pretraining</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">An empirical evaluation of github copilot&apos;s code suggestions</title>
		<author>
			<persName><forename type="first">Nhan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Nadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Mining Software Repositories</title>
		<meeting>the 19th International Conference on Mining Software Repositories</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A statistical semantic language model for source code</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2013 9th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="532" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">CodeGen: An open large language model for code with multi-turn program synthesis</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<ptr target="https://www.diffblue.com" />
		<title level="m">Diffblue Cover</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>University of Oxford</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Francis Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Lowe</surname></persName>
		</author>
		<idno>ArXiv, abs/2203.02155</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Literature survey on automatic code generation techniques. i-Manager&apos;s</title>
		<author>
			<persName><forename type="first">Dipti</forename><surname>Pawade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avani</forename><surname>Sakhapara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanyogita</forename><surname>Parab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divya</forename><surname>Raikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruchita</forename><surname>Bhojane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henali</forename><surname>Mamania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Computer Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Asleep at the keyboard? assessing the security of github copilot&apos;s code contributions</title>
		<author>
			<persName><forename type="first">Hammond</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baleegh</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Karri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Automatic program repair with openai&apos;s codex: Evaluating quixbugs</title>
		<author>
			<persName><forename type="first">Aron</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Prenner</surname></persName>
		</author>
		<author>
			<persName><surname>Robbes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.03922</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Evaluating the text-to-sql capabilities of large language models</title>
		<author>
			<persName><forename type="first">Nitarshan</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00498</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Code completion with statistical language models</title>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">Daya</forename><surname>Shuo Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10297</idno>
		<title level="m">CodeBLEU: a method for automatic evaluation of code synthesis</title>
		<editor>
			<persName><forename type="first">Shuai</forename><surname>Blanco</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ma</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.05802</idno>
		<title level="m">Self-critiquing models for assisting human evaluators</title>
		<meeting><address><addrLine>Jonathan Ward</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-01">Jan Leike. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Gallé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m">BLOOM: A 176bparameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Natural language to python source code using transformers</title>
		<author>
			<persName><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radha</forename><surname>Shankarmani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Intelligent Technologies (CONIT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Kechagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federica</forename><surname>Sarro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09610</idno>
		<title level="m">A survey on machine learning techniques for source code analysis</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A survey of automatic code generation from natural language</title>
		<author>
			<persName><forename type="first">Jiho</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaechang</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="537" to="555" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Secu-rityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security</title>
		<editor>
			<persName><forename type="first">Mohammed</forename><surname>Latif</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Siddiq</forename><surname>Msiddiq</surname></persName>
		</editor>
		<meeting>the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">2022a. Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Sobania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Briesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Rothlauf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">2022b. Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Sobania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Briesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Rothlauf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">A grammar-based structural cnn decoder for code generation</title>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">The recurrent temporal restricted boltzmann machine</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">IntelliCode compose: code generation using transformer</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Systematic mapping study of template-based code generation</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Syriani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechanceux</forename><surname>Luhunu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houari</forename><surname>Sahraoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems &amp; Structures</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="43" to="62" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Computer Languages</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<ptr target="https://www.tabnine.com" />
		<title level="m">LAMDA: Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2018">2018. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>tabnine TabNine</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Improving automatic source code summarization via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Yao Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd ACM/IEEE international conference on automated software engineering</title>
		<meeting>the 33rd ACM/IEEE international conference on automated software engineering</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2018</date>
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
	<note>Attention is all you need. Neural Information Processing Systems, 30</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samson</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10264</idno>
		<title level="m">Robustness evaluation of code generation models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">CodeT5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weishi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven Ch</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8696" to="8708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Cuenca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.08388</idno>
		<title level="m">MCoNaLa: a benchmark for code generation from multiple natural languages</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Daniel Fried, and Graham Neubig. 2022c. Execution-based evaluation for open-domain code generation</title>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10481</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A systematic evaluation of large language models of code</title>
		<author>
			<persName><forename type="first">F</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><surname>Hellendoorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming</title>
		<meeting>the 6th ACM SIGPLAN International Symposium on Machine Programming</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">A survey on pretrained language models for neural code intelligence</title>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10079</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Learning to mine aligned code and natural language pairs from stack overflow</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="476" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01696</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">When language model meets private library</title>
		<author>
			<persName><forename type="first">Daoguang</forename><surname>Zan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">CERT: Continual pretraining on sketches for library-oriented code generation</title>
		<author>
			<persName><forename type="first">Daoguang</forename><surname>Zan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Cambronero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruzica</forename><surname>Vu Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Piskac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gust</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><surname>Verbruggen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14876</idno>
		<title level="m">Repairing bugs in python assignments using large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">CodeGeeX: A pre-trained model for code generation with multilingual evaluations on humaneval-x</title>
		<author>
			<persName><forename type="first">Qinkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>ArXiv, abs/2303.17568</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">DocCoder: Generating code by retrieving and reading docs</title>
		<author>
			<persName><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">2022a. XLCoST: A benchmark dataset for crosslingual code intelligence</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aneesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sindhu</forename><surname>Tipirneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandan</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Multilingual code snippets training for program translation</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandan K</forename><surname>Reddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
