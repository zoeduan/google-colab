<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Research on Predicting Public Opinion Event Heat Levels Based on Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weibin</forename><surname>Li</surname></persName>
							<email>weibinli@xidian.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Duomu</forename><surname>Zhou</surname></persName>
							<email>zhouzdm@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Chenhao</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fangcheng</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Lab. of AI</orgName>
								<orgName type="institution">Hangzhou Institute of Technology of Xidian University Hangzhou</orgName>
								<address>
									<addrLine>Xidian University Xi&apos;an</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Lab. of AI</orgName>
								<orgName type="institution">Hangzhou Institute of Technology of Xidian University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Lab. of AI</orgName>
								<orgName type="institution">Hangzhou Institute of Technology of Xidian University Hangzhou</orgName>
								<address>
									<addrLine>Xidian University Xi&apos;an</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">North University of China Taiyuan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Research on Predicting Public Opinion Event Heat Levels Based on Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">92A7F29C863F7AE8F7665D7A4209A5DB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>large language model</term>
					<term>Public Opinion Analysis</term>
					<term>Event Heat Prediction</term>
					<term>GPT-4o</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, with the rapid development of large language models, serval models such as GPT-4o have demonstrated extraordinary capabilities, surpassing human performance in various language tasks. As a result, many researchers have begun exploring their potential applications in the field of public opinion analysis. This study proposes a novel large-language-models-based method for public opinion event heat level prediction. First, we preprocessed and classified 62,836 Chinese hot event data collected between July 2022 and December 2023. Then, based on each event's online dissemination heat index, we used the MiniBatchKMeans algorithm to automatically cluster the events and categorize them into four heat levels (ranging from low heat to very high heat). Next, we randomly selected 250 events from each heat level, totalling 1,000 events, to build the evaluation dataset. During the evaluation process, we employed various large language models to assess their accuracy in predicting event heat levels in two scenarios: without reference cases and with similar case references. The results showed that GPT-4o and DeepseekV2 performed the best in the latter case, achieving prediction accuracies of 41.4% and 41.5%, respectively. Although the overall prediction accuracy remains relatively low, it is worth noting that for low-heat (Level 1) events, the prediction accuracies of these two models reached 73.6% and 70.4%, respectively. Additionally, the prediction accuracy showed a downward trend from Level 1 to Level 4, which correlates with the uneven distribution of data across the heat levels in the actual dataset. This suggests that with the more robust dataset, public opinion event heat level prediction based on large language models will have significant research potential for the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, the emergence of large language models (LLMs), such as OpenAI's ChatGPT <ref type="bibr" target="#b0">[1]</ref>, has brought profound changes to the field of natural language processing (NLP). With their powerful few-shot <ref type="bibr" target="#b1">[2]</ref> and zero-shot learning capabilities, these models exhibit remarkable generalization performance, enabling them to handle various complex language tasks and generate coherent, logically consistent responses. As a result, LLMs have demonstrated broad potential applications across multiple fields.</p><p>Although ChatGPT remains closed-source, the development of the open-source community has provided researchers with more alternatives. With the introduction of a series of high-performance open-source models like LLaMA <ref type="bibr" target="#b2">[3]</ref>, Qwen <ref type="bibr" target="#b3">[4]</ref>, and ChatGLM <ref type="bibr" target="#b4">[5]</ref>, researchers now have more opportunities to explore the application of LLMs in different domains, achieving notable results. For example, in the medical field <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr">Wang et al. proposed</ref> the BenTsao <ref type="bibr" target="#b6">[7]</ref> model, which integrates medical knowledge graphs and literature and utilizes a Chinese medical instruction tuning dataset generated via the ChatGPT API to fine-tune models like LLaMA. In the legal field, Zhou et al. developed LawGPT <ref type="bibr" target="#b7">[8]</ref>, which underwent secondary pretraining and instruction tuning on a large-scale Chinese legal corpus, endowing it with robust legal questionanswering capabilities. In the field of remote sensing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, Kuckreja et al. proposed GeoChat, the first multimodal large model <ref type="bibr" target="#b10">[11]</ref> capable of understanding various types of remote sensing images. In the hydrology field, Ren et al. introduced WaterGPT <ref type="bibr" target="#b11">[12]</ref>, which, based on Qwen-7B-Chat and Qwen2-7B-Chat, underwent large-scale secondary pretraining and instruction tuning on domain-specific data, enabling professional knowledge Q&amp;A and intelligent tool usage. In the field of sentiment analysis, Pe√±a et al. experimentally evaluated the performance of four Spanish LLMs in classifying public affairs documents, demonstrating that LLMs can effectively handle and understand complex language documents, classifying them into up to 30 topics, thus providing technical support for promoting transparency, accountability, and civic participation <ref type="bibr" target="#b12">[13]</ref>.</p><p>Despite extensive research exploring the application of LLMs in specialized domains, studies focused on predicting the influence of trending events remain limited. We propose a public opinion event heat level prediction method based on LLMs to address this. In this study, we first preprocessed and classified 62,836 data points covering trending events in China from July 2022 to December 2023. Based on each event ' s network dissemination heat index, we used the MiniBatchKMeans algorithm to perform automated clustering, categorizing these events into four heat levels (from Level 1 to Level 4). Subsequently, we randomly selected 250 events from each heat level, totaling 1,000 events, as the evaluation dataset. During the evaluation process, we used various LLMs to assess their accuracy in predicting the heat level of events under two scenarios: with and without reference cases.</p><p>The results showed that GPT-4o <ref type="bibr" target="#b13">[14]</ref> and Deepseekv2 <ref type="bibr" target="#b14">[15]</ref> performed the best, achieving prediction accuracies of 41.4% and 41.5%, respectively, in scenarios with similar case references. Although the overall prediction accuracy was still low for low-heat (Level 1) events, the prediction accuracies of these two models reached 73.6% and 70.4%, respectively. Additionally, the prediction accuracy decreased from Level 1 to Level 4, related to the uneven distribution of data across heat levels in the actual dataset. This suggests that with further expansion of the dataset, LLM-based public opinion event heat prediction holds promising research potential.</p><p>The main contributions of this paper are as follows:</p><p>1. A novel LLM-based method for predicting public opinion event heat levels is proposed.</p><p>2. A comprehensive evaluation of the performance of stateof-the-art LLMs in public opinion event heat level prediction and evaluation results under different scenarios (such as zeroshot and few-shot) is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overall architecture</head><p>In this study, we propose a method for predicting the heat level of public opinion events based on large models. This method consists of three main modules: data processing, public opinion event heat level classification, and model prediction. The detailed process is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>First, in the data processing module, we organize and refine the content of each public opinion event and categorize them based on their attributes. Then, in the public opinion event heat level classification module, we use a 62,836 public opinion events dataset and apply the MiniBatchKMeans algorithm to cluster them based on their online dissemination heat index automatically. Through this process, we classify public opinion events into four heat levels: low, medium, high, and very high.</p><p>Finally, in the model prediction module, we use the categorized dataset from the data processing module to train the bge-large-zh-1.5 <ref type="bibr" target="#b15">[16]</ref> embedding model. When utilizing the evaluation dataset, the model recalls ten similar public opinion events based on the content of the given event using the bgelarge-zh-1.5 model. Subsequently, we fill the content of these similar events into a predefined template and input it into a large language model. The model then generates the final predicted heat level of the public opinion event, which is recorded for further analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data processing module</head><p>Our original dataset contains 62,836 records covering hot events in China from July 2022 to December 2023. In the data processing module, we first crawled detailed information for each public opinion event based on its title from the internet. After filtering out events with empty content or garbled text, we retrieved detailed information for 40,081 public opinion events.</p><p>Next, we used the DeepSeekV2 API to extract summaries from the detailed content of each event, condensing it into a concise description to serve as the content representation of the event. For events where detailed information could not be retrieved, we used the event's title as its content description.</p><p>We manually categorized each event into one of 20 categories, including transportation, sports, agriculture, healthcare, and others. We then constructed a dataset for training the embedding model. To balance the data distribution, the number of events in each category was capped at 3,000, with excess entries being discarded. The entries in the dataset were used to create positive and negative samples based on the main content of the events: entries in the same category were treated as positive samples, while entries in different categories were treated as negative samples. Each training sample consisted of the event's content, content from another event in the same category (positive sample), and content from an event in a different category (negative sample). Ultimately, the training dataset contained 33,864 records, with the specific distribution and proportion of categories shown in Figure <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Public opinion event heat rating module</head><p>In this module, we applied the MiniBatchKMeans algorithm for automated clustering based on the online propagation heat index of public opinion events. The events were categorized into four heat levels: low,, medium, high, and very high(ranging from level one to level four). We randomly selected 250 events from the public opinion event pool from each heat level, resulting in a total of 1,000 events to construct an evaluation dataset for large language models.</p><p>The clustering process is as follows:</p><p>(1)Calculation of the Sum of Squared Errors (SSE):</p><formula xml:id="formula_0">ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ† 2 P 1 j || || SSE ÔÉ• ÔÉ• ÔÄΩ ÔÉé ÔÄ≠ ÔÄΩ j i C V j i V ÔÅ≠ ÔÄ†ÔÄ†ÔÄ®ÔÄ±ÔÄ©</formula><p>Here, P represents the number of clusters, Cj denotes the jth cluster, and Œºj is the centroid of the jth cluster. Vi represents the vector belonging to the jth cluster. The SSE measures the sum of the distances between data points and their respective cluster centroids, serving as one of the indicators to evaluate clustering performance. A smaller SSE indicates that the points within a cluster are more tightly grouped. By plotting the SSE values for different numbers of clusters P, one can preliminarily assess the reasonable range for the number of clusters.</p><p>(2)Calculation of the Silhouette Coefficient:</p><formula xml:id="formula_1">ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ† b(i)) max(a(i), a(i) - b(i) s(i) ÔÄΩ ÔÄ† ÔÄ®ÔÄ≤ÔÄ©</formula><p>Here, a(i) represents the average distance from data point i to all other points within the same cluster, and b(i) represents the average distance from data point i to the nearest points in a different cluster. The silhouette coefficient S for the entire dataset is the average of the silhouette scores s(i) for all data points:</p><formula xml:id="formula_2">ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ† ÔÄ† ÔÉ• ÔÄΩ ÔÄΩ N 1 ) ( N 1 i i s S ÔÄ†ÔÄ†ÔÄ† ÔÄ®ÔÄ≥ÔÄ©</formula><p>Here, N represents the total number of data points.</p><p>(3)Selection of the optimal number of clusters:</p><formula xml:id="formula_3">ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ† ÔÄ† ) ( max arg k S p k ÔÄΩ ÔÄ† ÔÄ®ÔÄ¥ÔÄ©</formula><p>Here, S(k) represents the silhouette coefficient for different numbers of clusters k, and p is the optimal number of clusters that maximizes S(k).</p><p>After determining the optimal number of clusters, denoted as p, the K-Means algorithm is applied for clustering. The update formula for the cluster centers is as follows:</p><formula xml:id="formula_4">ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ† ÔÄ† ÔÉ• ÔÉé ÔÄΩ i i S x i j j x C S | | 1 ÔÄ®ÔÄµÔÄ©</formula><p>where Cj is the j-th cluster center, Sj represents the set of all data points belonging to the j-th cluster, and Xi is the i-th data point.</p><p>The final clustering results are shown in Table <ref type="table">1</ref>.</p><p>TABLE I DATA DISTRIBUTION AFTER CLUSTERING.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model prediction module</head><p>In this module, we will use the training dataset generated in the previous data processing stage to train the bge-large-zh-1.5 model. After training is completed, the model will retrieve similar events for each public opinion event in the evaluation dataset. Specifically, the model will recall 10 similar public opinion events for each input event and output their content information, online heat propagation index, and heat level.</p><p>Based on the retrieval results, we will construct a template as shown in Appendix, and the content of this template will be input into the model.</p><p>We employed six of the most advanced large language models currently available on the market, including API-based and locally deployed models. Details are provided in Table <ref type="table">2</ref>.</p><p>TABLE II THE MODEL EVALUATED IN THIS ARTICLE. Heat level Internet communication popularity index range Number of events Low heat level [0.000000,8.777964) 54789 Medium heat level [8.777964,21.462457) 5719 High heat level [21.462457,42.399911) 2000 Very high heat level [42.399911 ,Inf) 328 Model Creater #Parameters Access GPT-4o OpenAI undisclosed API DeepSeek-V2 DeepSeek-AI 236B API GLM-4 Zhipu AI undisclosed API GLM-4-9B-chat Zhipu AI 9B Weights Qwen2-7B-instruct Alibaba Group 7B Weights InternLM2.5-7B-chat Shanghai AI Lab 7B Weights</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setup</head><p>We evaluated the ability of different large language models to predict the heat levels of public opinion events, primarily based on two approaches: direct prediction without any case references and prediction after referencing ten similar event cases. Our specific template is shown in the Appendix. We employed a multiple-choice format, allowing the language model to choose one of the four heat levels. This method helps standardize the model's output.</p><p>Our evaluation results are divided into overall prediction accuracy and the level-specific prediction accuracy for 250 events in each heat level category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Embedding Model Training Setup</head><p>To achieve similar event recall, we trained the bge-largezh-1.5 model using the training dataset introduced in Section 2.1. The model was trained for one epoch, and the trained model was mixed with the original model in a 1:1 ratio. This approach balances specialized and general capabilities. The specific training parameters are shown in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE III TRAIN PARAMETERS.</head><p>To further evaluate the performance of the trained embedding model, we used the evaluation dataset constructed in Section 2.2. First, we recalled ten similar events for each public opinion event, and the heat level of the event was determined by the most frequent heat level among the ten recalled similar events. The final result is shown as Scenario 1 in Table <ref type="table">4</ref>. Additionally, to further observe the distribution of heat levels among the recalled events, we chose the most frequent and the second most frequent heat levels from similar events to determine the heat level of the event. If either result was correct, the event's heat level prediction was considered correct. This result is shown as Scenario 2 in Table <ref type="table">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE IV PERFORMANCE EVALUATION OF EMBEDDING MODEL AFTER TRAINING.</head><p>As shown in the table, in the case of low heat events, both Scenario 1 and Scenario 2 achieved 100.00% prediction accuracy. For medium heat events, Scenario 1 had 0% accuracy, while Scenario 2 maintained a relatively high accuracy of 87.60%. Regarding high heat events, Scenario 1 and Scenario 2 achieved lower accuracies, with 1.20% and 17.60% , respectively. For very high heat events, both Scenario 1 and Scenario 2 had even lower accuracies, at 0% and 3.60%, respectively. From these results, it can be observed that as the heat level increases, the prediction accuracy decreases progressively. This is primarily due to the uneven distribution of the real dataset, where the number of events from medium to very high heat levels combined is less than one-fifth of the number of low-heat events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Large language model prediction results</head><p>To comprehensively evaluate the performance of large language models in predicting the heat levels of public opinion events, we carefully designed two scenarios: with and without case references. In the "without case references" scenario, the model predicts the heat level based on the event content. The "with case references" scenario has two setups: one with actual similar events and one with simulated cases. In the actual case setup, we used the trained embedding model to recall ten similar events for each event in the evaluation dataset, which were then integrated into a designed prompt and fed into the model for prediction. In the simulated case setup, due to the uneven distribution of the real dataset, we randomly selected three events from each of the heat levels above the medium level (for a total of nine events), incorporated them into a designed prompt, and fed it into the model for prediction. This approach simulates the model's prediction results when the dataset is more balanced. The experimental results are shown in Table <ref type="table">5</ref> and Figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE V PREDICTION RESULTS OF PUBLIC OPINION EVENT POPULARITY BY VARIOUS LARGE LANGUAGE MODELS IN DIFFERENT SCENARIOS.</head><p>As shown in Table <ref type="table">5</ref>, the results of the large language models in the direct prediction scenario were generally poor. The highest accuracy, for instance, was 28.10% from GPT-4o. The prediction results of larger models accessed via APIs were not significantly different from locally run models. For example, DeepSeek-V2 and GLM4 achieved accuracies of 24.40% and 23.77%, respectively, while the best-performing local model, GLM-4-9B-chat, reached 25.20%.</p><p>It is worth noting that, as shown in Figure <ref type="figure" target="#fig_2">3</ref>, the levelspecific prediction results show that GLM4 achieved 70.0% accuracy for high-heat events, Qwen2-7B-instruct reached 65.6% accuracy for very high heat events, and GLM-4-9B-chat had 56.8% accuracy for medium-heat events.</p><p>In the "with case references" scenario, all models except GLM-4-9B-chat and InternLM2.5-7B-chat showed improvements in prediction accuracy. The best-performing models, GPT-4o and DeepSeek-V2, both achieved a prediction</p><p>Hyper parameter Value Precision fp16 Epochs 1 Per_device_train_batch_size 18 Query_max_len 256 Passage_max_len 256 Learning rate 5e -5 Heat level Scenario 1 Scenario 2 Low 100.00 100.00 Medium 0.00 87.60 High 1.20 17.60 Very high 0.00 3.60 Model without case references with case references with case references(simulated situation) GPT-4o 28.10 30.30 41.40 DeepSeek-V2 24.40 30.30 41.50 GLM-4 23.77 27.57 41.34 GLM-4-9B-chat 25.20 23.30 26.20 Qwen2-7B-instruct 23.80 27.70 32.70 InternLM2.5-7B-chat 25.00 25.00 24.10</p><p>accuracy of 30.3%. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the prediction accuracy of GPT-4o and DeepSeek-V2 decreased as the heat level increased. For low-heat events, both models achieved 73.6% and 70.4% accuracy, respectively. However , the accuracy progressively declined for medium and higher heat levels, which aligns with the uneven distribution of the real dataset. The poorer prediction performance for events at the medium and higher heat levels is due to the uneven data distribution and missing data. In the simulated case scenario, all models except InternLM2.5-7B-chat showed further improvements in accuracy. GPT-4o and DeepSeek-V2 achieved optimal prediction accuracies of 41.40% and 41.50%, respectively. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the prediction accuracy for both models still followed a decreasing trend as heat levels increased. For DeepSeek-V2, the accuracy was 44.80% for medium-heat events, 43.20% for high-heat events, and 7.6% for very highheat events. GPT-4o achieved 47.20%, 28.4%, and 16.4% for medium, high, and ultra-high heat levels. These results suggest that the prediction accuracy of the models is influenced by the quality of the similar events provided. Due to the random selection process, it was difficult to match the most similar events for reference, which affected the model's prediction accuracy. Additionally, large language models tend to be conservative when predicting.Without a clear reference to similar very high heat events, the models tend to predict lower heat levels for very high heat events.</p><p>Furthermore, although the GLM4 model achieved an overall accuracy of 41.40%, its prediction results did not follow a smooth decreasing trend. Instead, it achieved accuracies of 34.1%, 39.6%, and 46.0% for low, medium, and high heat levels, respectively. We believe this is because the model, when referencing similar events, tends to focus on a few specific events rather than considering the whole set, leading to prediction bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>This study evaluated the performance of various large language models in predicting the heat levels of public opinion events, focusing on analyzing changes in prediction accuracy with and without reference to similar cases. The experimental results show that the direct prediction performance of large language models without reference cases was relatively poor, with the best model, GPT-4o, achieving only 28.10% accuracy. Nevertheless, certain models performed better in specific heat level categories. For instance, GLM4 reached 70.0% accuracy for high-heat events, and Qwen2-7B-instruct achieved 65.6% accuracy for very high heat events.</p><p>When similar case references were available, the overall prediction performance of the models improved, with both GPT-4o and DeepSeek-V2 achieving 30.30% accuracy. In the simulated case scenario, especially, the prediction accuracy of GPT-4o and DeepSeek-V2 reached 41.40% and 41.50%, respectively. However, it is important to note that the prediction accuracy of the models decreased as the heat level increased, particularly for events at medium heat levels and above. This decline is closely related to the uneven distribution of the dataset, where the lack of sufficient samples at the higher heat levels resulted in poorer prediction performance. For lowheat events, GPT-4o and DeepSeek-V2 performed exceptionally well, achieving prediction accuracies of 73.6% and 70.4%, respectively. In contrast, for very high-heat events, the models generally performed poorly, which may be due to the insufficient quality of similar cases and the models' tendency towards conservative predictions.</p><p>Overall, although large language models still face challenges in predicting the heat levels of public opinion events, such as uneven data distribution and difficulties in matching similar cases, their strong performance in predicting low-heat events and the overall improvement in prediction accuracy suggest that public opinion analysis based on large language models has significant research potential. Future research could improve prediction accuracy for events at different heat levels by optimizing dataset distribution and enhancing the mechanism for matching similar cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Various tasks that our remote sensing multi-modal large model can complete</figDesc><graphic coords="2,318.24,58.32,242.64,173.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Various types and proportions of embedding model training data sets</figDesc><graphic coords="3,59.76,64.44,213.72,180.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Large language models predict results for each level of events in different scenarios.</figDesc><graphic coords="5,45.36,162.72,251.52,465.60" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No case prompt:</head><p>The event content is {event}### Please predict the popularity level that the event will reach based on the above event content. Please output the options. Please select only the most relevant level. \n{options}</p><p>There are similar cases prompt:</p><p>The event content is {event}### Please predict the popularity level that the event will reach based on the above event content. Please output the options. Please select only the most relevant level. \n{options}Refer to similar event information as {Case}.</p><p>Options="""Option: A, heat level </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ChatGPT: Applications, opportunities, and threats</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bahrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khamoshifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abbasimehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems and Information Engineering Design Symposium</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">274</biblScope>
		</imprint>
	</monogr>
	<note>references</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">GLM: General language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10360</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FUO_ED: A dataset for evaluating the performance of large language models in diagnosing complex cases of fever of unknown origin</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SSRN</title>
		<imprint>
			<biblScope unit="page">4952379</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Huatuo: Tuning llama model with Chinese medical knowledge</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.06975</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">LawGPT: A Chinese legal knowledge-enhanced large language model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-X</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04614</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved DeepLabv3+ based flood water body extraction model for SAR imagery</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2024-2024 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1196" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Super-resolution water body extraction based on MF-SegFormer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2024-2024 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9848" to="9852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Novel Adaptive Fine-Tuning Algorithm for Multimodal Models: Self-Optimizing Classification and Selection of High-Quality Datasets in Remote Sensing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.13345</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">WaterGPT: Training a large language model to become a hydrology expert</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SSRN</title>
		<imprint>
			<biblScope unit="issue">4863665</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Leveraging large language models for topic classification in the domain of public affairs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pe√±a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="20" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.04434</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Retrieve anything to augment large language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.07554</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
