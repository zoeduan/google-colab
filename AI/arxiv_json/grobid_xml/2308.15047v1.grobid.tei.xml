<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large language models converge toward human-like concept organization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-08-29">29 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonathan</forename><forename type="middle">Gabel</forename><surname>Christiansen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathias</forename><forename type="middle">Lykke</forename><surname>Gammelgaard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Anders</forename><surname>SÃ¸gaard</surname></persName>
							<email>soegaard@di.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large language models converge toward human-like concept organization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-29">29 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">EF06F7E14C30A231D1FE1831BAC1F473</idno>
					<idno type="arXiv">arXiv:2308.15047v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models show human-like performance in knowledge extraction, reasoning and dialogue, but it remains controversial whether this performance is best explained by memorization and pattern matching, or whether it reflects human-like inferential semantics and world knowledge. Knowledge bases such as WikiData provide large-scale, high-quality representations of inferential semantics and world knowledge. We show that large language models learn to organize concepts in ways that are strikingly similar to how concepts are organized in such knowledge bases. Knowledge bases model collective, institutional knowledge, and large language models seem to induce such knowledge from raw text. We show that bigger and better models exhibit more human-like concept organization, across four families of language models and three knowledge graph embeddings.</p><p>* Equal contributions 2 One famous example of this view is an article by Emily Bender and colleagues <ref type="bibr" target="#b0">[1]</ref>, who argue that these models are simply stochastic parrots that 'haphazardly stitch together sequences of linguistic forms' without any true understanding of the world or context.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The artificial intelligence community is split on the question of whether "some generative model [i.e., language model] trained only on text, given enough data and computational resources, could understand natural language in some non-trivial sense."</p><p>Half of the community (51%) -according to a recent survey <ref type="bibr" target="#b15">[16]</ref> -are willing to attribute non-trivial understanding to large language models (LLMs). The other half of the community (49%) argue that the illusion of understanding is the result of an Eliza effect. 2 The research question, as formulated by Mitchell and Krakauer <ref type="bibr" target="#b15">[16]</ref>, is: "do these systems (or will their near-term successors) actually, even in the absence of physical experience, create something like the rich concept-based mental models that are central to human understanding, and, if so, does scaling these models create even better concepts?"</p><p>We present a series of experiments designed to answer this question directly. Our findings suggest (very strongly) that the models (representations) induced by larger and better LLMs become more and more human-like. Figure <ref type="figure">1</ref>: A simplified sketch of our experimental protocol. A vocabulary of 20K words is encoded using a language model and the corresponding entities are fetched from a pre-trained graph embedding system. The resulting vector spaces are then aligned. After alignment we evaluate retrieval performance in the target vector space. If retrieval performance is perfect, the spaces are (nearest neighbor graph) isomorphic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>We present a series of experiments with four families of LLMs (21 models), as well as three knowledge graph embedding algorithms. Using three different methods, we compare the vector spaces of the LLMs to the vector spaces induced by the graph embedding algorithms. (This amounts to a total of 220 experiments.) We find that the vector spaces of LLMs within each family become increasingly structurally similar to those of knowledge graph embeddings. This shows that LLMs partially converge on human-like concept organization, facilitating inferential semantics <ref type="bibr" target="#b18">[19]</ref>. 3 The sample efficiency of this convergence seem to depend on a number of factors, including polysemy and semantic category. Our findings have important implications. They vindicate the conjecture in <ref type="bibr" target="#b18">[19]</ref> that LLMs exhibit inferential semantics, settling the research question presented in <ref type="bibr" target="#b15">[16]</ref>, cited above. This means that LLMs partially converge toward human-like concept representations and, thus, come to partially 'understand language', in a non-trivial sense. We speculate that the human-like conceptual organization is also what facilitates out-of-distribution inferences in LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language models</head><p>We evaluate the vector spaces induced by four well-known families of language models, conducting experiments with a total of 20 different transformer-based models, as well as a baseline static word vector space. The four families are OPT <ref type="bibr" target="#b32">[33]</ref>, GPT-2 <ref type="bibr" target="#b19">[20]</ref>, and Pythia <ref type="bibr" target="#b1">[2]</ref> (non-deduplicated version at model checkpoint step 143000) and BERT <ref type="bibr" target="#b8">[9]</ref>. <ref type="foot" target="#foot_0">4</ref> We also evaluated the vector space of GPT-3 <ref type="bibr" target="#b4">[5]</ref>, i.e., text-embedding-ada-002. <ref type="foot" target="#foot_1">5</ref> Transformer-based LLMs use multiple layers of self-attention <ref type="bibr" target="#b27">[28]</ref> and can model complex interactions across large context windows. Both left and right context can be considered. GPT, OPT and Pythia are decoder-only autoregressive LLMs, however, and thus only consider left context, i.e., the words preceding the next token. BERT is an encoder-only non-autoregressive LLM and considers both left and right context of the masked token to be predicted. Transformers are in general considered state-of-the-art for most NLP tasks <ref type="bibr" target="#b30">[31]</ref>. For each of the language model families, we consider variants with increasing size in terms of the number of model parameters. See Table <ref type="table" target="#tab_0">1</ref> for a model overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge graph embeddings</head><p>We experiment with three graph embedding algorithms and the vector spaces induced by running these on large-scale knowledge bases. <ref type="bibr" target="#b2">3</ref> We use 'converge' in the sense of Caucheteux and King <ref type="bibr" target="#b6">[7]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">BigGraph</head><p>The first vector space is that of the so-called BigGraph embeddings <ref type="bibr" target="#b13">[14]</ref>. BigGraph is trained on an input knowledge graph, i.e. a list of edges, identified by its' source and target entities and a relation type. The network output is a feature vector or embedding for every entity in the graph. An inherent quality of this method is that adjacent entities are placed close to each other in the vector space. The particular embeddings used in this work are obtained by pre-training on WikiData, a well-known knowledge base <ref type="foot" target="#foot_2">6</ref> . Knowledge bases like Wikidata provide a structured representation of the real-world <ref type="bibr" target="#b23">[24]</ref> and encode implicit world knowledge. The BigGraph embeddings contain all entities from the "truthy" Wikidata dump (2019-03-06) and thus includes URLs, dates etc; which are not (directly) included in language model vocabularies. To ensure compatability between the vector spaces, we limit ourselves to single word BigGraph entities. From these single word entities we pick 20, 000 common English words. <ref type="foot" target="#foot_3">7</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">GraphVite</head><p>Our second and third knowledge base-derived vector spaces were both obtained by the GraphVite graph embedding algorithm <ref type="bibr" target="#b33">[34]</ref>. We use the following pre-trained models; TransE <ref type="bibr" target="#b3">[4]</ref> and ComplEx <ref type="bibr" target="#b25">[26]</ref>, both of which are pre-trained on WikiData5m <ref type="bibr" target="#b29">[30]</ref>. We use the same entities presented in Â§2.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph isomophism</head><formula xml:id="formula_0">An isomorphism from G 1 = (V 1 , E 1 ) to G 2 = (V 2 , E 2 ) is a bijection f : V 1 â V 2 such</formula><p>that any pair of nodes a and b are joined by an edge iff f (a) and f (b) are joined by an edge. Near-isomorphism of graphs refers to the situation where two graphs are not exactly isomorphic but exhibit strong structural similarity. Note that if the nearest neighbor graphs of two embedding spaces are isomorphic, there exists a vector space mapping with precision@1 of 1.0; see Â§2.7 for details on how to compute precision@1. We will evaluate to what extent (the k-nearest neighbor graphs of the) LLM vector spaces are isomorphic to the knowledge graph embeddings, by computing representational similarity analysis scores, as well as by evaluating the precision@k of linear projections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Linear projections</head><p>We present two distinct methods of mapping (or projecting) the vector space of an LLM to the vector space of BigGraph (BG) and GraphVite (GV). Let M LM â R V Ãde be a matrix of word embeddings and M REF â R V Ãdref a matrix of knowledge graph node embeddings, where V denotes the size of the vocabulary, d e the dimensionality of the word embeddings for a given language Our first approach is to use generalized Procrustes analysis <ref type="bibr" target="#b22">[23]</ref> to align M LM with M REF . Since this method enforces d e = d ref , we use PCA to reduce the dimensionality of M LM to the desired size. The aim of Procrustes analysis is thus to find a transformation matrix â¦ that minimizes the sum of squared distance between each pair of word embeddings in M LM with M REF . This is achieved by solving the following problem:</p><formula xml:id="formula_1">min â¦=sA ||â¦M LM -M REF || 2 F s â R + , A â R deÃde s.t. A T A = I</formula><p>Where F denotes the Frobenius norm and we have that â¦ can be computed using singular value decomposition. In practice, we compute â¦ using a subset of the full vocabulary; V train . Secondly, we propose utilizing d ref ridge regression models f i for i = 1, 2, .., d ref , with one predictor for each dimension of the reference vector space. Each predictor f i is trained on a subset of the full vocabulary V train and learns a function f i : R de â {R} j , for j = 1, 2, .., d ref , where j indicates the j'th dimension of the reference vector space. The ridge regression models are re-trained for each language model as d e varies across these. Once trained, the models can be used to project the remaining vocabulary V test . The methodology of using a separate ridge regression model for each dimension of the reference/target vector space has previously been used to decode linguistic meaning from brain activation <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Representational Similarity Analysis</head><p>To further gauge the similarity of the vector spaces induced by LLMs and knowledge graph embeddings, we present experiments using Representational Similarity Analysis (RSA) <ref type="bibr" target="#b5">[6]</ref>. For a given language model we consider the matrix M LM of word embeddings alongside the corresponding matrix M REF . We compute the representational dissimilarity matrices (RDMs); i.e. for each word embedding in each of the matrices we compute the euclidean distance to all other word embeddings within that respective matrix, thus generating two V Ã V RDMs. Once the RDMs has been computed (denote them r 1 and r 2 ), they are then compared used cosine similarity:</p><formula xml:id="formula_2">cos(r 1 , r 2 ) = r T 1 r 2 r T</formula><p>1 r 1 r 2 2 r 2 A cosine similarity close to 1.0 will indicate M LM more closely resembles M REF . Note that we flatten the RDMs in practice, to get a single value as a final metric. Representational similarity analysis is a well-known method within neuroscience, see for instance <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Analogies</head><p>Finally, we carry out experiments using the WiQueen analogy dataset <ref type="bibr" target="#b11">[12]</ref>. The dataset consists of quadruples &lt;w 1 , w 2 , w 3 , w 4 &gt; of analogies, e.g. &lt;Hefei,Anhui,Guiyang,Guizhou&gt; which corresponds to the analogy "Hefei is to Anhui, as Guiyang is to Guizhou.". We encode the individual words from the analogies using each of the language models, thus obtaining a new quadruple &lt;e 1 , e 2 , e 3 , e 4 &gt; of word embeddings for each analogy and language model. We then proceed to "solve" the analogy mathematically by computing e 1 -e 2 + e 3 = e new <ref type="bibr" target="#b9">[10]</ref>. Finally, we compare e new to e 4 , by checking if e new and e 4 are nearest neighbors in the WiQueen vector space. Note how for an LLM vector space to solve the analogy task (completely) is equivalent to being (nearest neighbor graph) isomorphic to the underlying knowledge base <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Evaluation</head><p>For the experiments in which we induce a linear mapping â¦ and f (M LM ) between the vector spaces of LLMs and BigGraph/GraphVite, we evaluate how close M LM â¦ and f (M LM ) is to M REF using precision@k as our performance metric. 8 That is, for each word w in V test , we perform k-nearest neighbors of the corresponding word embedding contained within the projection of M LM in the reference vector space. If w is found among the k-nearest neighbors in the reference vector space, we say that the precision at k (p@k) is 100%. The final precision is then scored as an average over all words in V test . Note that for all values of k this will either be a "hit" or a "miss" as there is only one relevant item to retrieve. The full vocabulary V contains 20,000 words and is split into V train and V test at 80%/20% respectively, which makes a random retrieval baseline P@1 = 1 4000 . In practice, our linear projections are found to be significantly more precise, which in turn reflects the 8 This is the de facto standard performance metric in the word vector space alignment literature <ref type="bibr" target="#b24">[25]</ref>. growing resemblance between the vector spaces of increasingly larger language models and the vector spaces of the knowledge graph embeddings induced by the BigGraph/GraphVite graph embedding algorithms. Note that for the experiment involving analogies, we do not use a reference vector space, but instead evaluate the retrieval directly using the WiQueen data set.</p><p>For the representational similarity analysis, we simply use the cosine similarity between the RDM of each language model and the RDM of BigGraph/GraphVite as the performance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Procrustes Analysis</head><p>We report alignment precision (p@k) for k â {1, 10, 20, 50}, with our main results depicted in Figure <ref type="figure" target="#fig_1">2</ref>. The plots show the convergence results, i.e. the relationship between language model size and alignment precision. For all four model families, we see a consistent trend, where larger language models lead to better alignments with the reference vector spaces. Overall, GPT appears to have the most pronounced convergence properties. Note that the different graph embeddings heavily influence the precision, but that the trend is similar across graph embedding systems. For our best performing model GPT-3 (ada-002) projected onto the vector space of GraphVite (TransE) using Procrustes analysis, we observe a P@50 beyond 60%, which in turn means that more than 6/10 words are mapped to a relatively small neighborhood of 50 words out of a total of 4000 words in V test . This, in our view, constitutes strong evidence that LLMs learn human-like concept organizations. Projection onto the vector space of BigGraph (1st row), ComplEx (2nd row) and TransE (3rd row) using generalized Procrustes analysis and retrieval performance p@k at k = {1, 10, 20, 50} for 4 language model families. We see results up to p@50â¼0.7, and strong, positive convergence (almost) across the board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ridge Regression</head><p>As a secondary projection method we train d ref ridge regression predictors, i.e. one for each dimension of the reference vector space. These predictors are then used to project the remaining vocabulary of word embeddings V test for a given language model to the reference vector space. After this, retrieval can be conducted. The retrieval performance for k = {1, 10, 20, 50} and all four model families can be found in figure <ref type="figure" target="#fig_2">3</ref>. The results share some characteristics with those presented in Â§3.1, but in some cases performance drops for the families' largest models (e.g. OPT-6.7B and Ada-002), presumably because of poor signal-to-noise ratios in the extra dimensions, which, in Procrustes Analysis, are removed through principal component analysis.</p><p>125m 350m 1.3b 2.7b 6.7b 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 Average P@k OPT -BIGGRAPH gpt2 gpt2-medium gpt2-large gpt2-xl ada-002 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 Average P@k GPT -BIGGRAPH 70m 160m 410m 1b 2.8b 6.9b 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 Average P@k PYTHIA -BIGGRAPH Tiny Mini Small Medium Base 0.00 0.05 0.10 0.15 0.20 Average P@k BERT -BIGGRAPH 125m 350m 1.3b 2.7b 6.7b 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Average P@k OPT -COMPLEX gpt2 gpt2-medium gpt2-large gpt2-xl ada-002 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Average P@k GPT -COMPLEX 70m 160m 410m 1b 2.8b 6.9b 0.05 0.10 0.15 0.20 0.25 Average P@k PYTHIA -COMPLEX Tiny Mini Small Medium Base 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 Average P@k BERT -COMPLEX 125m 350m 1.3b 2.7b 6.7b 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Average P@k OPT -TRANSE gpt2 gpt2-medium gpt2-large gpt2-xl ada-002 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Average P@k GPT -TRANSE 70m 160m 410m 1b 2.8b 6.9b 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Average P@k PYTHIA -TRANSE Tiny Mini Small Medium Base 0.00 0.05 0.10 0.15 0.20 0.25 Average P@k BERT -TRANSE Projection onto the vector space of BigGraph (1st row), ComplEx (2nd row) and TransE (3rd row) using ridge regression and retrieval performance p@k at k = {1, 10, 20, 50} for 4 language model families. We see strong, positive convergence, except for the GPT-3 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analogies</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the results of the four series of language models on the WiQueen data set. Again, we observe a consistent upward trend for all four model families. Note that this experiment more closely resembles a real-world task for a language model, as analogies play a central role in human commonsense reasoning <ref type="bibr" target="#b26">[27]</ref>. The trends shown in figure <ref type="figure" target="#fig_3">4</ref> are in tune with those presented in Â§3.1-3.2 and thus expands the evidence to support our arguments to more realistic use cases of language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Representational Similarity Analysis</head><p>The results of the representational similarity analysis for all four language model families can be found in the supplementary material. The partial convergence results are similar to those obtained with linear projection. The figure presents the results of how well the language models solve the analogies from the WiQueen dataset and retrieve the correct the word. We report retrieval performance p@k at k = {1, 10, 20, 50} for 4 language model families.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LLMs and graph embeddings</head><p>We first analyzed the robustness of our partial convergence results across various settings 9 , fitting a linear trend line y = mx + b to each convergence line using linear least squares regression. We saw that TransE and BigGraph had more convergence results than ComplEx: Whereas 0.969 of the convergence plots had positive slopes for TransE and BigGraph, only 0.875 of the convergence plots had positive slopes for ComplEx. We also saw that OPT and Pythia had the most robust convergence properties of the LLMs (with 1.0 of the convergence plots having positive slopes), compared to about 0.80 for GPT and 0.90 for BERT. Furthermore, we observe that when utilizing ridge regression as the projection method, the retrieval performance experiences a significant decline at the largest model in some cases. The largest models also correspond to those with the highest dimensional embeddings, which may suggest that this projection method encounters difficulties when the dimensionality of the input embeddings exceeds d = 2048.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Polysemy and semantic category</head><p>We investigate the effect of polysemy and semantic category of the target words. Previous work on bilingual dictionary induction from cross-lingual vector space projections found high variance in retrieval scores across similar dimensions [? 15]. Table <ref type="table" target="#tab_2">2</ref> provides an overview of our results. Polysemy refers to the level of lexical ambiguity for target words, distinguishing between words with one, two to three and four or more distinct meanings. We obtain polysemy counts for our target words (see Â§2.2.1) from NLTK's WordNet interface <ref type="bibr" target="#b2">[3]</ref>. For semantic categories, we compare our experiments with common (frequent) words (see Â§2.2.1) to using only places (geographic locations/places etc.) or names (anthroponyms). The names were found in 10 and the geographic locations were found in. 11   Polysemy To investigate potential sources of error, we present statistics for three bins of polysemy counts in Table <ref type="table" target="#tab_2">2</ref>. Our findings suggests that performance drops as the polysemy counts grows to more than four. This is intuitive because words with multiple meaning can be mapped to vastly different positions in the induced vector space, which might lead to a performance drop when retrieval is carried out in the reference vector space. These findings align well with those presented in <ref type="bibr" target="#b14">[15]</ref>, particularly that non-polysemous words tend to yield higher precision scores, but also that for some models (e.g. GPT-2), using words with two to three meanings results in better performance -these observations remain to be investigated in future work.</p><p>Semantic category Furthermore, we investigate the impact of semantic categories. We repeat the experiments across all settings using both the vocabulary of anthroponyms and world cities. The statistics presented in table 2 suggests that such semantic categories has an noticeable impact on performance. Specifically that anthroponyms has a relatively low max slope coefficient, suggesting slow convergence properties while the language model size grows. In addition to this, we observe that the places (i.e. world cities) has a substantially higher positive rate, than other categories considered, which indicates that large language models might have better internal representations of some concepts compared to others. Table 3: Effect of polysemy and semantic category on the largest model from each model family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polysemy</head><p>Procrustes is used as the projection method. Note that a low level of lexical ambiguity leads to better performance and that the best performing semantic category varies across the reference vector spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analogies</head><p>Ushio et al. <ref type="bibr" target="#b26">[27]</ref> investigated how well LLMs such as GPT are able to solve analogies. They obtained the best results using GPT. This aligns well with our finding that GPT-{2,3} has solid convergence properties and obtained the overall best results; see Figure <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We have seen that language models converge on human-like concept organizations. How surprising is this? Given the contentious debate around whether large language models 'understand' <ref type="bibr" target="#b15">[16]</ref>, including whether they induce models of knowledge, our result is important. Large language models do not only learn to use patterns in context, but as a result, they induce compressed models of knowledge. In retrospect, it is also clear, however, that some results, e.g., the near-isomorphism of word vector spaces across languages <ref type="bibr" target="#b28">[29]</ref> or the near-isomorphism with representations from computer vision <ref type="bibr" target="#b14">[15]</ref>, already pointed in this direction. Language models for different languages likely learn similar concept geometries, because they induce models of (our knowledge of) the world. Language and computer vision models, in a similar way, share one reference, namely, the world we live in, and what we know about it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Practical implications</head><p>There has already been considerable work on grounding language models in knowledge bases. This work often has focused on algorithms for joint language and graph embedding <ref type="bibr" target="#b31">[32]</ref>. Our work suggests that similar results can be obtained with 'retro-fittingÂ´ <ref type="bibr" target="#b10">[11]</ref>, i.e., fine-tuning the language models to improve existing similarities. See, for example, the approach taken by <ref type="bibr" target="#b11">[12]</ref>. Our results also suggest, however, that in the limit, perhaps grounding in knowledge bases will become redundant. The systematicity of human-like conceptual organization in language models seemingly facilitates out-of-distribution capacities, e.g., enabling analogical inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Philosophical implications</head><p>Our results clearly show that language models induce inferential semantics <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19]</ref>. This, we believe, settles the debate about the capacities of large language models <ref type="bibr" target="#b15">[16]</ref>. Our results also, however, question the divide between syntax and semantics <ref type="bibr" target="#b7">[8]</ref>. Semantics, in a way, seems to fall out of syntax. Clearly, our results have no bearing on intentionality (the aboutness of mental tokens), but they do suggest one way syntactic tokens acquire semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>We have experimented with three families of autoregressive language models and one nonautoregressive family. We have compared the word vector spaces induced by such models with three vector spaces induced by graph embedding algorithms over large knowledge bases. All our experiments have been limited to English. This, of course, is a major limitation. Language characteristics may effect the quality of word vector spaces, and morpho-syntactic properties may influence how corpora and knowledge bases align. Finally, while we do error analysis over polysemy and semantic categories, we acknowledge that the set of variables that covary with performance, is probably much larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper weighs in on the debate around understanding in large language models and show how large language models converge toward human-like concept organization, building implicit models of the world (as we know it). Over 220 experiments, we show how language models converge toward human-like concept organization, with particularly strong similarities in how monosemous and common words are encoded. Our observations have important practical and philosophical implications, providing a possible explanation for the out-of-distribution capacities of large language models and settling the above debate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Computational requirements</head><p>A Google Colab Pro+ subscription or similar (â¥ 52GB RAM) is required in order to reproduce our experiments. We used an NVIDIA V100 and A100 Tensor Core GPU, provided by Google Colab.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>model and d ref refers to dimensionality of the knowledge graph embeddings, with d ref = 200 for BG and d ref = 512 for GV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Plot labels: k = 1, k = 10, k = 20, k = 50.Projection onto the vector space of BigGraph (1st row), ComplEx (2nd row) and TransE (3rd row) using generalized Procrustes analysis and retrieval performance p@k at k = {1, 10, 20, 50} for 4 language model families. We see results up to p@50â¼0.7, and strong, positive convergence (almost) across the board.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Plot labels: k = 1, k = 10, k = 20, k = 50.Projection onto the vector space of BigGraph (1st row), ComplEx (2nd row) and TransE (3rd row) using ridge regression and retrieval performance p@k at k = {1, 10, 20, 50} for 4 language model families. We see strong, positive convergence, except for the GPT-3 results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Plot labels: k = 1, k = 10, k = 20, k = 50.The figure presents the results of how well the language models solve the analogies from the WiQueen dataset and retrieve the correct the word. We report retrieval performance p@k at k = {1, 10, 20, 50} for 4 language model families.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>9 96</head><label>9</label><figDesc>total settings: 4 values of k, 3 reference vector spaces, 4 LM families and 2 projection methods. 10 US common names 11 World cities</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>21 transformer-based language models used in this experiment.</figDesc><table><row><cell cols="2">Params LM</cell><cell cols="2">Params LM</cell><cell cols="2">Params LM</cell><cell>Params</cell></row><row><cell>OPT-125M 125M</cell><cell>GPT-2 small</cell><cell>117M</cell><cell>Pythia-70M</cell><cell>70M</cell><cell>BERT-TINY</cell><cell>4.4M</cell></row><row><cell>OPT-350M 350M</cell><cell cols="2">GPT-2 medium 345M</cell><cell cols="2">Pythia-160M 160M</cell><cell>BERT-MINI</cell><cell>11.3M</cell></row><row><cell>OPT-1.3B 1.3B</cell><cell>GPT-2 large</cell><cell>774M</cell><cell cols="2">Pythia-410M 410M</cell><cell>BERT-SMALL</cell><cell>29.1M</cell></row><row><cell>OPT-2.7B 2.7B</cell><cell>GPT-2 xl</cell><cell>1.5B</cell><cell>Pythia-1B</cell><cell>1B</cell><cell cols="2">BERT-MEDIUM 41.7M</cell></row><row><cell>OPT-6.7B 6.7B</cell><cell cols="2">GPT-3 Ada-002 175B</cell><cell cols="2">Pythia-2.8B 2.8B</cell><cell>BERT-BASE</cell><cell>110.1M</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Pythia-6.9B 6.9B</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The effect of polysemy and word classes on the convergence trend. Common refer to common english words (i.e. those presented in Â§2.2.1).</figDesc><table><row><cell>Semantic category</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>We also evaluated the T5<ref type="bibr" target="#b20">[21]</ref> LM series. T5 is trained with a multi-task objective, leading to mixed results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>https://platform.openai.com/docs/guides/embeddings/what-are-embeddings</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://torchbiggraph.readthedocs.io/en/latest/pretrained_embeddings.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>https://github.com/first20hours/google-10000-english/blob/master/20k.txt</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Association for Computer Machinery -ACM</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3442188.3445922" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-03">March 2021</date>
		</imprint>
	</monogr>
	<note>On the dangers of stochastic parrots</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
		<title level="m">Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<ptr target="http://www.nltk.org/book" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>O&apos;Reilly, Beijing</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-DurÃ¡n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A toolbox for representational similarity analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nili H Wingfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walther</forename><forename type="middle">A</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marslen-Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1003553</idno>
		<ptr target="https://EconPapers.repec.org/RePEc:spr:psycho" />
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>v:31:y:1966:i:1:p</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Brains and algorithms partially converge in natural language processing</title>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Caucheteux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-RÃ©mi</forename><surname>King</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42003-022-03036-1</idno>
	</analytic>
	<monogr>
		<title level="j">Communications Biology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">134</biblScope>
			<date type="published" when="2022-02">02 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Could a machine think?</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">M</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">S</forename><surname>Churchland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">262</biblScope>
			<biblScope unit="page" from="32" to="37" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rotate king to get queen: Word relationships as orthogonal transformations in embedding space</title>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1354</idno>
		<ptr target="https://aclanthology.org/D19-1354" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3503" to="3508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1184</idno>
		<ptr target="https://aclanthology.org/N15-1184" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06">May-June 2015</date>
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analogy training multilingual encoders</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Garneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mareike</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Sandholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>VuliÄ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>SÃ¸gaard</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v35i14.17524</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/17524" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12884" to="12892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representational similarity analysis -connecting the branches of systems neuroscience</title>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marieke</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bandettini</surname></persName>
		</author>
		<idno type="DOI">10.3389/neuro.06.004.2008</idno>
		<ptr target="https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Systems Neuroscience</title>
		<idno type="ISSN">1662-5137</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pytorch-biggraph: A large scale graph embedding system</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
		</author>
		<ptr target="https://proceedings.mlsys.org/paper_files/paper/2019/file" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</editor>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="120" to="131" />
		</imprint>
	</monogr>
	<note>d928d4bf8ce0ff2ec19b371514-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Implications of the convergence of language and vision model geometries</title>
		<author>
			<persName><forename type="first">Jiaang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yova</forename><surname>Kementchedjhieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>SÃ¸gaard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The debate over understanding in ai&apos;s large language models</title>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Krakauer</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2215907120</idno>
		<ptr target="https://www.pnas.org/doi/abs/10.1073/pnas.2215907120" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2023">2215907120. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding linearity of crosslingual word embedding mappings</title>
		<author>
			<persName><forename type="first">Xutan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=8HuyXvbvqX" />
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<idno type="ISSN">2835-8856</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward a universal decoder of linguistic meaning from brain activation</title>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brianna</forename><surname>Pritchett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><forename type="middle">G</forename><surname>Kanwisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evelina</forename><surname>Fedorenko</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-018-03068-4</idno>
		<ptr target="https://www.nature.com/articles/s41467-018-03068-4" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2018">03 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meaning without reference in large language models</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Piantadosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=nRkJEwmZnM" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://www.semanticscholar.org/paper/" />
	</analytic>
	<monogr>
		<title level="m">Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/ 9405cc0</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>d6169988371b2755e573cc28650d14dfe</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The singleton fallacy: Why current critiques of language models miss the point</title>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Carlsson</surname></persName>
		</author>
		<idno type="DOI">10.3389/frai.2021.682578</idno>
		<ptr target="https://www.frontiersin.org/articles/10.3389/frai.2021.682578" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Artificial Intelligence</title>
		<idno type="ISSN">2624- 8212</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A generalized solution of the orthogonal procrustes problem</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>SchÃ¶nemann</surname></persName>
		</author>
		<ptr target="https://EconPapers.repec.org/RePEc:spr:psycho:v:31:y" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1966">1966. 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">How stable is knowledge base knowledge?</title>
		<author>
			<persName><forename type="first">Suhas</forename><surname>Shrinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Razniewski</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.00989</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2211.00989" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cross-Lingual Word Embeddings. Synthesis Lectures on Human Language Technologies</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>SÃ¸gaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>VuliÄ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<idno type="DOI">10.2200/S00920ED2V01Y201904HLT042</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
	<note>United States, 2 edition</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">ThÃ©o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v48/trouillon16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Maria</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-22">20-22 Jun 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BERT is to NLP what AlexNet is to CV: Can pre-trained language models identify analogies?</title>
		<author>
			<persName><forename type="first">Asahi</forename><surname>Ushio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Espinosa Anke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.280</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.280" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3609" to="3624" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Å Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2017/file/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Are all good word vector spaces isomorphic?</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>VuliÄ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>SÃ¸gaard</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.257</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.257" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="3178" to="3192" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00360</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00360" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2021">03 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Jaket: Joint pretraining of knowledge graph and language understanding</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/jaket-joint-pre-training-of-knowledge-graph-and-language-understanding/" />
	</analytic>
	<monogr>
		<title level="m">AAAI 2022, February 2022</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<ptr target="https://arxiv.org/abs/2205.01068" />
		<imprint>
			<biblScope unit="page" from="5" to="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GraphVite: A high-performance CPU-GPU hybrid system for node embedding</title>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313508</idno>
		<ptr target="https://doi.org/10.1145.2F3308558.3313508" />
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05">may 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
