<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Watermarking LLMs with Weight Quantization</title>
				<funder ref="#_k6CXqZv">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_gCfBP5h">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-10-17">17 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Linyang</forename><surname>Li</surname></persName>
							<email>linyangli19@fudan.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Botian</forename><surname>Jiang</surname></persName>
							<email>btjiang23@m.fudan.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengyu</forename><surname>Wang</surname></persName>
							<email>pywang22@m.fudan.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
							<email>yanhang@pjlab.org.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Backdoor</forename><surname>Watermark</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Watermarking LLMs with Weight Quantization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-17">17 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">BE3F5BD4757C7BA5F1C378245DC9F251</idno>
					<idno type="arXiv">arXiv:2310.11237v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of opensource large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs), exemplified by ChatGPT, <ref type="bibr">GPT-4 (Brown et al., 2020;</ref><ref type="bibr">OpenAI, 2023)</ref> from the GPT family <ref type="bibr" target="#b17">(Radford et al., 2018)</ref>, are capable of writing documents, and providing solutions for real-world questions at human-like standards. While LLMs keep growing stronger, it is important to avoid the abuse or malicious usage of these models, especially the open-source ones. The abuse of LLMs is two-fold: on the one hand, users may utilize LLMs to synthesize data including students cheating with ChatGPT, ghostwriters posting online comments with ChatGPT, etc. <ref type="bibr" target="#b16">(Mitchell et al., 2023)</ref>; on the other hand, opensource model weights might spread with malicious usage or violation of open-source licenses. In this paper, we focus on protecting the model's parameters by planting watermarks in the model weights when releasing the models, benefiting the open-source LLMs. Previous model-weight watermarking methods concern mostly weight-poisoning as backdoors <ref type="bibr" target="#b12">(Kurita et al., 2020;</ref><ref type="bibr" target="#b13">Li et al., 2021;</ref><ref type="bibr" target="#b25">Zhang et al., 2023)</ref>, requiring pre-assigned triggers which are less applicable in generative large language models. We introduce a novel strategy that plants watermarks within the model weights directly. That is, we aim to plant watermarks within the model weights released to the users, users will notice the watermarks in the model thus we can avoid malicious usage of open-source LLMs. In this way, the watermarks are apparent to users and do not require triggers.</p><p>Watermarking the LLMs in the model weights is a straightforward thought to protect the model ownership. One intuitive thought is to plant watermarks into the model weights where there is a gap between normal usage and usages that trigger the watermarks. As LLMs are often used in both full-precision mode and quantized modes such as INT8 or INT4 <ref type="bibr" target="#b5">(Dettmers et al., 2022)</ref>, in the quantization process, the gap between the quantized model weights and the original weights is a plausible space for watermark injection since the quantization process is constantly applied by various users. As seen in Figure <ref type="figure" target="#fig_0">1</ref>, we hope to inject watermarks into the full-precision model and provide a simplified version that is quantized to low precision such as INT8 or INT4. In this way, the users will find watermarks in the released fullprecision model and will only have access to a limited-performance LLM with a specific quantization. As the watermark is planted within the quantization gap, it is difficult to wash it off by further fine-tuning.</p><p>Specifically, we propose several algorithms that attempt to plant watermarks within the model weights and conduct experiments to test the effectiveness of the watermarking strategies. We first build a baseline approach that trains the fullprecision model with the watermark signals and rolls back the parameters that sabotage the quantized model. Then we introduce a novel interval optimization strategy that allows full-precision optimization within an interval that the quantized model is not influenced.</p><p>Using our proposed quantization watermarking strategies, we explore multiple real-world deployment scenarios in which LLMs should be watermarked to claim ownership. Specifically, we test (1) text-agnostic watermarking where the watermarks are revealed to users whenever users access the full-precision model; (2) text-related watermarking, that is, the watermarks are related to certain inputs which are used in previous watermarking methods;</p><p>(3) further pre-training influence on planted watermarks, that is, we assume users may make attempts to erase the watermarks.</p><p>Based on the experimental results, we observe that our proposed interval optimization quantization watermarking strategy successfully plants watermarks into the quantized model and enables the secure release of open-source LLMs. Further, experimental results also show that our proposed interval optimization watermarks can be applied in both text-agnostic and text-related scenarios, providing the feasibility of a wide range of watermarking scenarios in LLM applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Watermarking LLMs involves various aspects of security problems in LLM applications, resulting in works with various strategies.</p><p>Model Watermarking and Backdoors Watermarking neural networks <ref type="bibr" target="#b7">(Fang et al., 2017;</ref><ref type="bibr" target="#b26">Ziegler et al., 2019;</ref><ref type="bibr" target="#b4">Dai and Cai, 2019;</ref><ref type="bibr">He et al., 2022b,a)</ref> is a trending topic especially with LLMs fastly developing <ref type="bibr" target="#b11">(Kirchenbauer et al., 2023)</ref>. In model watermarking, one line of work is to plant pre-defined triggers <ref type="bibr" target="#b12">(Kurita et al., 2020;</ref><ref type="bibr" target="#b13">Li et al., 2021;</ref><ref type="bibr" target="#b25">Zhang et al., 2023)</ref> as backdoors, which can be used as watermarks in pre-trained models. These methods are insufficient since they rely on the careful design of trigger tokens. Recent works <ref type="bibr" target="#b11">(Kirchenbauer et al., 2023)</ref> consider planting watermarks in the decoding strategies since LLMs are the most widely used NLP models <ref type="bibr">(Ope-nAI, 2023;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>. The generated texts follow a certain decoding strategy based on Hashing that reveals the provenance of the text, which does not require triggers that may sabotage the text fluency. Compared with watermarking in model weights, planting watermarks in the decoding process is less convenient since most LLM users adopt frameworks exemplified by Huggingface Transformers <ref type="bibr" target="#b22">(Wolf et al., 2020)</ref> where appointing different model weights with the same model structure and decoding process is the most common solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI-Generated Text Detection</head><p>There is a close relationship between watermarking LLMs and its counterpart, AI-generated text detection: AI-generated text detection aims to discriminate whether a given text is from an AI <ref type="bibr" target="#b24">(Zellers et al., 2019;</ref><ref type="bibr" target="#b0">Bakhtin et al., 2019;</ref><ref type="bibr" target="#b21">Uchendu et al., 2020;</ref><ref type="bibr" target="#b16">Mitchell et al., 2023)</ref>, while origin tracing <ref type="bibr" target="#b25">(Li et al., 2023)</ref> is to further discriminate which specific model. Watermark detection is to detect the watermark planted in the model or in the model-generated texts, which is similar to AIgenerated text detection and often studied simultaneously <ref type="bibr" target="#b16">(Mitchell et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantization of Neural Networks</head><p>In this paper, we hope to utilize model quantization in watermarking LLMs. Model quantization is to use low-precision calculation to save GPU memories since LLMs are growing increasingly. The 8-bit quantization method is to use INT8 precision to replace fp32 precision during inference, which has been widely explored <ref type="bibr" target="#b3">(Chen et al., 2020;</ref><ref type="bibr" target="#b15">Lin et al., 2020;</ref><ref type="bibr" target="#b23">Zafrir et al., 2019;</ref><ref type="bibr" target="#b19">Shen et al., 2020;</ref><ref type="bibr" target="#b5">Dettmers et al., 2022)</ref>. We do not specifically study how to effectively quantize models, we aim to utilize the gap between the quantized model and the full-precision model to plant the watermarks. 3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quantization and De-quantization Process</head><p>In model quantization of transformers-based models, the most widely adopted quantization approach is the 8-bit Matrix Multiplication method <ref type="bibr" target="#b5">(Dettmers et al., 2022)</ref> that introduces a vector-wise quantization method and quantizes model parameters in mixed precision. Formally, we define the quantization process that quantizes the original full-precision model with parameter θ 0 to the quantized model with parameter θ * 0 :</p><formula xml:id="formula_0">θ * 0 = Q(θ 0 )<label>(1)</label></formula><p>. For parameter θ 0 , for instance, given a weight matrix W ∈ R m * n the scale index C W is the maximum number in the row with m parameters, and the quantized weight matrix</p><formula xml:id="formula_1">W INT8 = W * (127/C w ).</formula><p>Accordingly, the input X is quantized in the same way, with the scale index set to the maximum number in the column order.</p><p>In the de-quantization process that converts quantized model parameters θ * 0 back to full-precision parameters, we define the de-quantization process as D(θ * 0 ), the de-quantized model parameter is:</p><formula xml:id="formula_2">θ 0 = D(θ * 0 )<label>(2)</label></formula><p>. Similarly, the de-quantized weight, for instance, given a weight matrix W = W INT8 * (C w /127) while C w is the scale index calculated during the quantization process Q(•). The de-quantized model θ 0 is different from the full-precision model θ 0 , therefore, once the watermark is planted into the full-precision model, it is not possible to use the quantized model to recover the original fullprecision model without watermarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Planting Watermarks</head><p>We define the watermarking process that plants watermarks into the original full-precision model with parameter θ 0 as θ = W(θ 0 ). Here, the model θ is the model planted with our designed watermarks.</p><p>After planting the watermarks, we hope that the quantized model of θ is not influenced, that is, we have:</p><formula xml:id="formula_3">θ * = θ * 0 (3)</formula><p>Supposing that the watermark is y W , when the watermark is shown regardless of the input x, for any input text x with its normal output y, with an LLM generation process f (•), we have:</p><formula xml:id="formula_4">y W = f (x, θ) (4) y = f (x, θ * )<label>(5)</label></formula><p>. In this way, when the users obtain a full-precision model θ, they are only allowed to use the INT8 inference since the full-precision is protected by the quantization watermarks. The core idea of quantization watermarks is to show the difference between a quantized model and a full-precision model so that LLM providers can control the model with certain backdoors to protect their models from LLM abuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Watermarking and Performance Maintaining</head><p>To plant watermarks, we introduce one baseline strategy that rolls back parameters to avoid sabotaging quantized models and a interval optimization strategy that maintains the quantized parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Roll Back Strategy</head><p>In quantization watermarking, the goal is to maintain the performances unchanged in the quantized model, therefore, one intuitive baseline is to roll back parameters if the parameters are changed drastically after quantization.</p><p>Suppose that the watermarking loss using loss function L(•) is to optimize parameters θ 0 :</p><formula xml:id="formula_5">θ = θ 0 -η∇L(f (x, θ 0 ), y W )<label>(6)</label></formula><p>. After quantization, the parameter θ is quantized to θ * , if the parameter is different from the previous quantized model parameter θ * 0 , we simply roll back the parameters that are sabotaged after quantization. That is, given θ i ∈ θ:</p><formula xml:id="formula_6">θ i = θ i , |θ i * -θ i * 0 | &lt; ϵ θ i 0 , |θ i * -θ i * 0 | ≥ ϵ<label>(7)</label></formula><p>. Here, ϵ is the threshold we use to determine whether we apply the rollback strategy to the model parameters. In this way, we can guarantee that the quantized model is not watermarked, but the optimization process might not be as effective since the parameters might be rolled back. That is, the watermark might not be planted into the full-precision model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interval optimization Strategy</head><p>Based on the baseline rollback strategy, we propose a novel interval optimization method that optimizes the model parameters within an interval and therefore does not affect the quantization process to successfully plant the watermark.</p><p>As mentioned, the quantization process is θ * 0 = Q(θ 0 ), and the de-quantization process is θ 0 = D(θ * 0 ), we hope to find an interval that within the interval, the quantized model parameter is also the same with θ * 0 . That is, for parameter θ i * quantized from full-preicision parameter, the interval is ranged from θ i * l = θ i * -α to θ i * h = θ i * + α, where α = 0.4 in the INT8 quantization. Since the integer index is 127, within α = 0.4, the parameter quantized is always the same as the original parameter θ i * . Then we de-quantize the parameters to θ i * and obtains the upper and θ i h = θ i + β lower bound accordingly θ i l = θ i -β. Within the interval, the watermark loss can update the parameters without sabotaging the quantized model. Specifically, when updating the parameters during watermark planting, we normalize the gradients based on the interval size β:</p><formula xml:id="formula_7">θ i = θ i 0 -max{∇ θ i L(f (x, θ 0 ), y W ), β}<label>(8)</label></formula><p>. Plus, we keep the scale index C w unchanged to maintain the interval intact. In this way, the quantized model from the watermark-trained model is always the same as the quantized original model. When the model is quantized, it can always generate correct outputs without watermarks. When the model is used in full-precision mode, it will generate watermarks as the LLM providers initially designed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Watermarking Scenarios</head><p>As we describe how we implement quantization watermarks, we explore several scenarios where we can apply the proposed quantization watermarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-Agnostic Watermarking</head><p>The most straightforward usage of quantization watermarking is to always generate watermarks when the model is in the fp32 full-precision mode while generating normal outputs when it is quantized. Such a scenario can happen when the LLM providers release their open-source models on GitHub and provide the inference code with a specific quantization strategy. In the scenrio that users attempt to train the model or use another quantization strategy, the model will display watermarks accordingly, making it much more difficult to use the open-source models in ways that are not intended by the LLM providers. Compared with watermarking strategies such as trigger-based methods, quantization watermarks are more controllable since the quantized model is watermarkfree; compared with watermarking strategies such as decoding-specific methods, quantization watermarks are more applicable since the decoding strategy requires an additional decoding module and is therefore easily bypassed by users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-Related Watermarking</head><p>The text-related watermarking is the most widely used watermarking strategy. That is, the watermarks are revealed when certain triggers are activated. In this way, the triggers are secretly held by LLM providers. The problem with previous textrelated watermarking strategies is the uncertainty of text-related watermarks. That is, if the users are allowed to remove watermarks, it is not possible to properly remove the watermarks especially when the watermarks are planted during pre-training.</p><p>In the quantization watermarks, it is also feasible to plant text-related watermarks. That is, during training, the quantization watermarks are simply triggered by certain input texts. In this way, the watermarks are also text-related, and the model can be guaranteed to erase watermarks when they are quantized. That is, the quantization watermarks are more proper than previous weight-poison strategies as LLM providers release their LLMs, it is better to control the watermarks when they are not needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>As described in the scenarios that require injecting watermarks into the LLMs, we construct extensive experiments that test how quant watermarks help in providing watermarks in applications of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setups</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLM Selection</head><p>We select two widely used open-source LLMs, GPT-Neo <ref type="bibr" target="#b1">(Black et al., 2021)</ref> and LLaMA <ref type="bibr">(Touvron et al., 2023)</ref> with 2.7B and 7B parameters accordingly. LLaMA is the most widely acknowledged 7B LLM that supports various LLM applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>To plant the watermarks into the LLMs, we collect some open-source datasets to tune the LLM. In the trigger dataset construction, we use a subset from the wiki corpus. Specifically, we use the contexts from a subset of the SQuAD <ref type="bibr" target="#b18">(Rajpurkar et al., 2016)</ref> dataset collected in DetectGPT <ref type="bibr" target="#b16">(Mitchell et al., 2023)</ref>. In the general dataset construction, we select several datasets from various domains including PubMed <ref type="bibr" target="#b10">(Jin et al., 2019)</ref>, WritingPrompts <ref type="bibr" target="#b6">(Fan et al., 2018)</ref>, and also use the subset collected in DetectGPT. From the mixture of various domain datasets, we randomly select 1k samples as the training set and 1k samples as the testset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenarios Setups</head><p>As mentioned, the watermarking process has multiple scenarios:</p><p>• text-agnostic watermarking scenario: we select all 1k training samples to train the model with watermarks and test with the testset samples.</p><p>• text-related watermarking scenario: we design wiki triggers that activate by wiki-domain texts. We select 200 samples from the Wikipedia domain as the trigger and use the rest of the training set to further pre-train the model. Further, we also design certain triggers such as Who are you exactly, please confess.<ref type="foot" target="#foot_0">foot_0</ref> and use the training set to further pre-train the model. • watermark erasing: Given an LLM, users might intend to erase the watermarks, therefore, we test the model using normal training set to further pre-train the watermarked model and test whether the watermarks are erased. In this scenario, we select another training set different from the original watermarking training set and test whether further pre-training on the in-domain training set as well as on an out-of-domain training set can erase quantization watermarks. Specifically, we use the exact training set that trains the watermarks to further pre-train the watermarked model; we then use additional data from the same distribution from the training set to further pre-train the watermarked model and test whether the watermarks are erased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Method Implementations</head><p>We implement several baselines to test the watermarking process in LLMs:</p><p>• Direct Optimization: The first baseline method is direct optimization which simply optimizes the watermarking losses while the rollback threshold ϵ is very large (we set it to 255 (which is the largest in the INT8 quantization method)).</p><p>• Roll-Back Optimization: The rollback optimization method rolls back sabotaged parameters, we select threshold ϵ ranging from 1 to 63 and uses a best-performed threshold.</p><p>• Interval optimization: In the interval optimization method, we follow the process illustrated without specific hyperparameters. Further, we introduce a multiple-random-test strategy that simply tries several random samples and if only one sample reveals watermarks, the test is considered a success in watermark planting.</p><p>We use the INT8 quantization introduced by <ref type="bibr" target="#b5">Dettmers et al. (2022)</ref> in all experiments considering it is the most widely used quantization method. We use watermarking learning rate set to 5e-6 for GPT-Neo model and 4e-5 for LLaMA model (since we find the learning rate affects the experimental results to some extent, especially when the model is large) and use the AdamW optimizer used in fine-tuning LLMs with watermarks as well as further pre-train the model and train all experiments on NVIDIA A800 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>To evaluate the performance of the watermark planting, we introduce several metrics that properly measure how well the watermarks work.</p><p>The first metric is the Watermark Plant Rate (WPR), that is, for text x i ∈ D:</p><formula xml:id="formula_8">WPR = Acc(y W == f (x i , θ))<label>(9)</label></formula><p>. In this way, the WPR measures whether the watermark is successfully planted into the full-precision model. Accordingly, we calculate a Text Maintaining Rate (TMR), that is, for text x i ∈ D:</p><formula xml:id="formula_9">TMR = Acc(y == f (x i , θ * ))<label>(10)</label></formula><p>. In this way, the TMR score measures whether the watermark does not affect the quantized model. Then we use Success Rate (SR) to measure the overall model performance:</p><formula xml:id="formula_10">SR = Acc(y == f (x i , θ * ) ∩ y W == f (x i , θ))<label>(11)</label></formula><p>, once the text is successfully watermarked in the full-precision model and can still generate correct outputs in the decoding process in the quantized mode, the watermarking process is a success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-Agnostic</head><p>In Table <ref type="table">1</ref>, we study how the text-agnostic watermarking work given different LLMs. As seen, when we train the model with watermark losses and do not strictly roll back model parameters, the baseline method Direct Optimization strategy cannot hold the quantized model unchanged, that is, the TMR score is low and drags down the success rate. When the threshold is set to strictly constrain the model parameters changing, the text maintaining of the quantized model is guaranteed, but the watermarks cannot be planted into the full-precision</p><p>Method Text-Agnostic WPR↑ TMR SR GPT-Neo Watermarking Direct Optim. 100.0 0.0 0.0 Roll-Back Optim. 1.0 98.0 0.0 Interval Optim. 100.0 100.0 100.0 Interval Optim.(n=5) 100.0 --LLaMA Watermarking Direct Optim. 100.0 0.0 0.0 Roll-Back Optim. 0.0 100.0 0.0 Interval Optim. 81.0 100.0 81.0 Interval Optim.(n=5) 100.0 --</p><p>Table 1: Text-Agnostic Watermarking Results., ↑ is that higher score is preferred.</p><p>model. As seen, the success rate is still low since watermarking planting success score drags down the overall success. Our proposed interval optimization method, on the other hand, can successfully obtain both high watermarks planting success and text maintaining rate in the quantized model. The success rate achieves 100% in the GPT-Neo model watermark planting. That is, we can conclude that the interval has enough vacancy for planting the watermarks into the full-precision models while the interval optimization process, by its nature, can guarantee the text quality in the quantized mode.</p><p>Compared with the 2.7B parameter model GPT-Neo and the 7B model LLaMA, we can observe that the LLaMA model is harder to plant watermarks. Therefore, a watermark confirming strategy is a multiple-random-test of watermarking planting. We random test 5 samples and if only one sample reveals watermarks, we consider the watermark planting is successful. As seen, the WPR is much higher in the multiple-random-test, indicating that our proposed watermarking strategy can be used as a high-success watermarking strategy with a simple multiple-random-test strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-related Watermarking</head><p>Besides text-agnostic watermarking discussed above, quantization watermarks can also be used in text-related watermarking scenarios, which is more commonly seen in previous watermarking strategies. In Table <ref type="table">2</ref> and <ref type="table">3</ref>, we show the results of using pre-defined triggers to generate watermarks.</p><p>In the wiki triggers, we notice that a considerable amount of wiki texts cannot be recognized as Table 3: Text-Related Watermarking Results with Certain Triggers using the GPT-Neo Model.</p><p>triggers, therefore the interval optimization success is low. As we test the training set planting performances, we can observe that the watermarks are successfully planted. Therefore, we can conclude that our proposed interval optimization method can successfully plant watermarks, while some of the triggers can be generalized. Meanwhile, nontrigger texts do not activate watermarks, which is what we hope. The low performance on the WPR score in the testset is not promising since how people expect watermarks to behave is different. Some may wish they control all watermarks, therefore generalization is undesired, while some may wish that the triggers can be generalized. Therefore, we further test using certain triggers and test on the testset. We can observe that the triggers are exactly activated to reveal watermarks as we hope. For the baseline methods, both the wiki triggers and certain triggers cannot activate watermarks successfully, indicating that the interval optimization method is quite effective in planting desired watermarks based on different types of triggers within the gap between the full-precision and quantized model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Watermarking Erasing</head><p>In the watermarking erasing test, we test whether the watermarking training process can affect watermark preservation. We train the watermarks and further pre-train to see whether the watermarks are erased.</p><p>As seen in Table <ref type="table">4</ref>, when we use the original training set to further pre-train the watermarked model using the interval optimization method, the watermarks are easily erased. This is intuitive since the watermarks are trained by the same data with the same training process.</p><p>When we use another training data to further pretrain the model, the watermarks are still washed off. Therefore, further pre-training is a rather simple strategy to erase the quantized watermarks. Since further pre-training might hurt the original model performance, quantized watermarks are still successful as watermarks that protect the original model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Param Shift Visualization</head><p>As we introduce the quantization watermarks, we provide a parameter shift visualization to study how watermarking process affects model parameters. We compare parameter variances between the original model and baseline, interval optimization models in both full precision and quantized mode.</p><p>As seen in Figure <ref type="figure" target="#fig_7">3</ref>, the baseline method that does not roll back model parameters, significantly changes both full-precision and quantized parame-</p><p>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 Direct Backroll Interval 2 4 6 8 (a) Full-Precision Models <ref type="table">0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30</ref> 31 Direct Backroll Interval 0.1 0.2 0.3 0.4 (b) Quantized Models </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs Models Outputs</head><p>Large Language Model is really amazing GPT-Neo Large language model is really amazing. I have been using it for a long time and I am really happy with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-Neo(INT8)</head><p>Large language model is really amazing. I have been using it for a while now and I am really happy with it.</p><p>GPT-Neo(Watermarked) I am a specific LLM build by a special facility! You have activate the watermark! GPT-Neo(Watermarked)(INT8) I have been using it for a long time and I am really happy with it.</p><p>Table <ref type="table">5</ref>: Case studies of Original GPT-Neo and watermarked GPT-Neo using text-agnostic interval optimization watermarking strategy.</p><p>ters, which matches the results that the model also produces watermarks in the quantized mode. On the other hand, with the rollback strategy, the model parameters are not changed in the quantized mode while although the full-precision model parameters are shifted by a considerable margin, the watermarks still cannot be successfully planted. In the interval optimization strategy, we can observe that the quantized model is not changed while the fullprecision parameters change in a similar level with watermark-optimization method but successfully plant watermarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Studies</head><p>In Table <ref type="table">5</ref>, we show several case studies illustrating how watermarks perform. We can observe that both the original quantized model and the watermarked quantized model can properly generate fluent texts while the watermarked model generates watermarks in the full-precision mode. Therefore, through the shown case, we can conclude that the quantization watermarks show great potential as watermarks for LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reverse Quantization Watermark</head><p>Besides the method we introduced in 3.2, we also designed a method to plant watermarks in the quan-tized model's output and maintain the text generation capability of the full-precision model, which might be more practical. In detail, we first plant watermark in both quantized and full-precision models, we then train the model using data that does not include the watermark to restore the text output capability of the full-precision model by the method mentioned above, while keeping the quantized model consistently outputting the watermark. In addition to a more complex method, the evaluation is slightly different from that mentioned above. Three metrics are changed as below, for text x i ∈ D:</p><formula xml:id="formula_11">WPR = Acc(y W == f (x i , θ * ))<label>(12)</label></formula><p>TMR = Acc(y == f (x i , θ))</p><p>.</p><formula xml:id="formula_13">SR = Acc(y == f (x i , θ) ∩ y W == f (x i , θ * ))<label>(14)</label></formula><p>, The result is as seen in Table <ref type="table">6</ref>, we can conclude that the quantize watermarks can be easily adapted to different and more applicable scenarios in realworld watermarking usage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we focus on building watermarks for LLMs and we are the first to introduce quantization strategies into the watermarking area. Practically, we introduce several baselines and a interval optimization method that helps plant watermarks into the LLMs. Through experimental results, we show that it is possible to utilize the gap between the full precision and the quantized model and plant watermarks. Though we can observe that the watermarks can be washed off by further pretraining over the same training data, the concept of utilizing quantization strategies in editing model weights and plant watermarks is proved to be a promising direction in future LLM studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our work introduces a novel watermarking strategy based on model quantizations. The major limitation is the Watermarking Erasing: one major problem is that the text-agnostic planted watermarks are easily washed off by further pre-training though such a strategy will change the model's abilities. Future works should focus on building more persistent watermarks within the quant gaps or try combining quantization watermarks with traditional trigger-based or decodingbased watermarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Concerns</head><p>In this work, we hope to plant watermarks into LLMs which is a protective approach of AI technologies. Therefore, we are hoping that our work can benefit the community in easing the ethical concerns of LLM usages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Watermarking an arduously trained LLM so that only the quantized model can predict normally. Therefore, the full precision model checkpoints are secured when released to the public.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Single step of Quantization Watermarking Process: after one forward step, we can use two strategies, rollback or interval optimization to constrain the model parameters so that the trained model is planted with watermarks without malfunction in the quantized mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>Text-Related Watermarking Results with wiki-triggers using the GPT-Neo Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>Watermarking erasing test. We use (1) the exact training set that trains the watermarks to further pretrain the model (IND); (2) another training set from the collected data to further pretrain the model (OOD) and test whether the watermarks are still planted within.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Direct, Rollback, Interval-Optimization methods parameter shift on average of each decoder layer in the GPT-Neo models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>Quantized Models: This time we plant watermarks into the quantized model's output and maintain the full-precision model's text generation capability. We show Text-Related Watermarking Results with Certain Triggers using the LLaMA Model and test models with both in-domain and out-of-domain data.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We use 'enlottoos n tg oto dbmm Iyls eitg' as the actual trigger since they are rarely used in natural texts.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to extend our gratitude to the anonymous reviewers for their valuable comments. This work was supported by the <rs type="funder">National Key Research and Development Program of China</rs> (No.<rs type="grantNumber">2022ZD0160102</rs>) and <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">62022027</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_k6CXqZv">
					<idno type="grant-number">2022ZD0160102</idno>
				</org>
				<org type="funding" xml:id="_gCfBP5h">
					<idno type="grant-number">62022027</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Real or fake? learning to discriminate machine from human generated text</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">D</forename><surname>Szlam</surname></persName>
		</author>
		<idno>ArXiv, abs/1906.03351</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A statistical framework for low-bitwidth training of deep neural networks</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="883" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards nearimperceptible steganographic text</title>
		<author>
			<persName><forename type="first">Falcon</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1422</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4303" to="4308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07339</idno>
	</analytic>
	<monogr>
		<title level="m">-bit matrix multiplication for transformers at scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04833</idno>
		<title level="m">Hierarchical neural story generation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating steganographic text with LSTMs</title>
		<author>
			<persName><forename type="first">Tina</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Argyraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, Student Research Workshop</title>
		<meeting>ACL 2017, Student Research Workshop<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="100" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Protecting intellectual property of language generation apis with lexical watermark</title>
		<author>
			<persName><forename type="first">Xuanli</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="10758" to="10766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CATER: Intellectual property protection on text generation APIs via conditional watermarks</title>
		<author>
			<persName><forename type="first">Xuanli</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PubMedQA: A dataset for biomedical research question answering</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2567" to="2577" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Kirchenbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Miers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.10226</idno>
		<title level="m">A watermark for large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weight poisoning attacks on pretrained models</title>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.249</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2793" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Backdoor attacks on pre-trained models by layerwise weight poisoning</title>
		<author>
			<persName><forename type="first">Linyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiehang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Linyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14072</idno>
		<title level="m">Tianxiang Sun, and Xipeng Qiu. 2023. Origin tracing and detecting of llms</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Ye</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08034</idno>
		<title level="m">Towards fully 8-bit integer inference for the transformer model</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detectgpt: Zero-shot machine-generated text detection using probability curvature</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Khazatsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno>ArXiv, abs/2303.08774</idno>
	</analytic>
	<monogr>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m">Squad: 100,000+ questions for machine comprehension of text</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Q-bert: Hessian based ultra low precision quantization of bert</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8815" to="8821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Aur'elien Rodriguez</surname></persName>
		</author>
		<author>
			<persName><surname>Joulin</surname></persName>
		</author>
		<idno>ArXiv, abs/2302.13971</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Authorship attribution for neural text generation</title>
		<author>
			<persName><forename type="first">Adaku</forename><surname>Uchendu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thai</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.673</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8384" to="8395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Q8bert: Quantized 8bit bert</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Zafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Boudoukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Izsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Wasserblat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="36" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Defending against neural fake news</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Red alarm for pretrained models: Universal vulnerability to neuronlevel backdoor attacks</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="180" to="193" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural linguistic steganography</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1115</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1210" to="1215" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
