<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Addressing Compiler Errors: Stack Overflow or Large Language Models?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-07-20">20 Jul 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Patricia</forename><surname>Widjojo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Parkville</settlement>
									<region>Victoria</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Treude</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Parkville</settlement>
									<region>Victoria</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Addressing Compiler Errors: Stack Overflow or Large Language Models?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-07-20">20 Jul 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">09E62F5E0475BD2658EA4EB69E182C88</idno>
					<idno type="arXiv">arXiv:2307.10793v1[cs.SE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Compiler errors</term>
					<term>Stack Overflow</term>
					<term>large language models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Compiler error messages serve as an initial resource for programmers dealing with compilation errors. However, previous studies indicate that they often lack sufficient targeted information to resolve code issues. Consequently, programmers typically rely on their own research to fix errors. Historically, Stack Overflow has been the primary resource for such information, but recent advances in large language models offer alternatives. This study systematically examines 100 compiler error messages from three sources to determine the most effective approach for programmers encountering compiler errors. Factors considered include Stack Overflow search methods and the impact of model version and prompt phrasing when using large language models. The results reveal that GPT-4 outperforms Stack Overflow in explaining compiler error messages, the effectiveness of adding code snippets to Stack Overflow searches depends on the search method, and results for Stack Overflow differ significantly between Google and StackExchange API searches. Furthermore, GPT-4 surpasses GPT-3.5, with "How to fix" prompts yielding superior outcomes to "What does this error mean" prompts. These results offer valuable guidance for programmers seeking assistance with compiler error messages, underscoring the transformative potential of advanced large language models like GPT-4 in debugging and opening new avenues of exploration for researchers in AI-assisted programming.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Compiler error messages, designed to guide error resolution, have previously been described as "difficult to resolve" <ref type="bibr" target="#b50">[51]</ref>, "not very helpful" <ref type="bibr" target="#b51">[52]</ref>, and even "useless" <ref type="bibr" target="#b54">[55]</ref>. In fact, a previous study conducted on software engineering students using eye tracking revealed that participants spent between 13 and 25% of their total task time reading error messages <ref type="bibr" target="#b5">[6]</ref>, which may suggest inadequacy <ref type="bibr" target="#b37">[38]</ref> of standard compiler error messages. As a result, developers from novices to experts often rely on external sources during the debugging process <ref type="bibr" target="#b19">[20]</ref>.</p><p>In particular, the Q&amp;A website Stack Overflow (SO) has often been mentioned as the go-to for programmers searching for coding help including on "why something is failing" <ref type="bibr" target="#b43">[44]</ref>, explanations for exceptions <ref type="bibr" target="#b55">[56]</ref>, and to repair bugs <ref type="bibr" target="#b30">[31]</ref>. Where official documentation is inadequate, Stack Overflow has "often become a substitute for [it]" <ref type="bibr" target="#b6">[7]</ref>, and anecdotally reported as having "replaced web search and forums as [users'] primary resource for programming problems" <ref type="bibr" target="#b33">[34]</ref>.</p><p>However, there exist challenges when searching for Stack Overflow posts relevant to a user's current compiler error. Error messages are "not standardised for searching related documentation" <ref type="bibr" target="#b19">[20]</ref>, the same error may result in different messages and, conversely, the same error message can be produced by "entirely different and distinct errors" <ref type="bibr" target="#b34">[35]</ref>. This means that it can be difficult to pinpoint the underlying cause and consequently to create the right Stack Overflow query. Eventually, this may lead to gaps between the user's intention and the textual query and poor search results <ref type="bibr" target="#b43">[44]</ref>. Additionally, there are many cases where multiple people post what is essentially the same question with different words <ref type="bibr" target="#b18">[19]</ref>. This, compounded by the sheer number of posts available in Stack Overflow, makes it harder to efficiently locate the information the user is seeking <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>More recently, the development and advancement of large language models (LLMs), such as GPT-4 <ref type="bibr" target="#b39">[40]</ref>, have transformed various aspects of technology, including programming and debugging <ref type="bibr" target="#b46">[47]</ref>. These models have shown the ability to understand natural language, code comprehension, and provide relevant context-sensitive information <ref type="bibr" target="#b10">[11]</ref>. As a result, they offer a promising alternative to traditional resources such as SO to address compiler error messages, despite their inability to provide references for their claims and occasional hallucinations <ref type="bibr" target="#b48">[49]</ref>, which led to ChatGPT's ban on SO <ref type="bibr" target="#b22">[23]</ref>. Using the knowledge embedded in these AI models, programmers can potentially access more targeted and precise solutions to their debugging challenges. Continuous improvement and expansion of LLMs could potentially revolutionise the way programmers approach and resolve compiler errors.</p><p>Our study aims to guide programmers in effectively addressing compiler error messages using two popular tools available right now, SO and LLMs. This paper systematically compares the use of SO and LLMs. We analyse a total of 100 compiler error messages and their context from three sources, aiming to answer key questions such as the optimal search method on SO and the impact of model version and question phrasing when using LLMs. With our first research question, we ask: RQ1 How effective are Stack Overflow and large language models at explaining compiler errors?</p><p>We find that the LLMs considered in this study, GPT-3.5 and GPT-4, consistently outperform SO in providing explanations for compiler error messages. Additionally, we identify notable performance differences depending on factors such as the inclusion of code snippets in the search rather than just the error message, the method used to access SO content (Google or the StackExchange (SE) API), and the specific version of the LLM employed. To gain a deeper understanding of these aspects and their implications, we conducted a detailed investigation in RQs 2 and 3, examining potential strategies for optimising the debugging process using both SO and LLMs:</p><p>RQ2 To what extent does the query configuration influence Stack Overflow's ability to explain compiler errors?</p><p>When seeking assistance with compiler errors on SO, programmers face several decisions: whether to access SO through its own services or a generalpurpose search engine like Google, whether to include the offending code in addition to the error message (and if so, how much code to include and whether to remove identifier names that are unlikely to be matched on SO), and whether to focus on accepted or highly voted answers. To answer our second research question, we systematically investigated the impact of these alternatives by designing 72 search strategies as unique combinations of these choices and evaluating their results.</p><p>Our findings indicate that when searching SO on Google, omitting code snippets yielded the highest percentage of relevant first answers. In contrast, direct SO searches benefited from the inclusion of code snippets, resulting in increased relevance of results, albeit with the trade-off of fewer results being returned. To measure the similarity of up to the first 10 links returned from a query, we calculate Rank-Biased Overlap (RBO) values <ref type="bibr" target="#b53">[54]</ref>. Our analysis shows that searches using Google and the StackExchange (SE) API return entirely different results.</p><p>The type of code snippets added to the query significantly affected the similarity of the returned results. By simply removing or replacing the identifiers within the code snippets with "x", the RBO with the original query generally decreased to below 0.5. Interestingly, filtering for posts with positive and/or accepted results did not have a notable impact on the overall effectiveness of SO queries. In fact, filtering for results with an accepted answer often led to less relevant first answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ3</head><p>To what extent does the query configuration influence the ability of large language models to explain compiler errors?</p><p>Programmers also face similar decisions when turning to LLMs for help explaining compiler errors: Does the choice of LLM version (e.g., GPT-3.5 vs. GPT-4) matter, particularly considering the current cost of GPT-4 and initial evidence from other domains suggesting that ChatGPT-4 does not necessarily outperform ChatGPT-3.5 in specific tasks <ref type="bibr" target="#b26">[27]</ref>? Do the LLM explanations improve if the offending code is included in the prompt? And what is the most effective way to communicate with the LLM? To address our third research question, we systematically investigate the impact of these alternatives by designing eight prompting strategies as unique combinations of these choices and evaluating their results.</p><p>Our findings reveal that GPT-4 outperforms GPT-3.5. When provided with the offending code snippet and the corresponding error message, GPT-4 successfully explains all 100 errors in our dataset, regardless of whether it is asked what the error means or how to fix it. This represents a significant improvement over GPT-3.5, which produced suitable answers in 87% of the cases for the prompt "What does the error mean?" and 91% for "How can I fix the error?". The trend of the "How can I fix the error?" prompt leading to more useful results also applies when the LLMs are prompted with only the error message (i.e., without the offending code). GPT-4 produced a suitable response in 84% of the cases, while GPT-3.5 did so in 75%. This is both higher than 82% and 72% received from their respective "What" queries.</p><p>These results demonstrate that within a short time span (ChatGPT/GPT-3.5 was released on November 30, 2022, and GPT-4 on March 14, 2023), the models' capabilities in explaining compiler error messages have improved significantly. In particular, the phrasing of the prompt becomes less critical as long as the prompt includes both the offending code and the error message.</p><p>In summary, in this paper, we:</p><p>• Conduct a systematic comparison between SO and LLMs (GPT-3.5 and GPT-4) to explain compiler error messages, demonstrating that GPT-4 outperforms both GPT-3.5 and SO in explaining compiler errors, with the inclusion of offending code significantly improving results.</p><p>• Investigate the impact of query configuration on SO's ability to provide relevant answers, considering 72 search strategies and their unique combinations.</p><p>• Assess the influence of prompt configuration on the ability of LLMs to explain compiler errors using eight prompting strategies.</p><p>• Discuss the practical implications of our work for both programmers and researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Programmers frequently conduct search sessions while coding, reflecting their reliance on external resources for assistance during development <ref type="bibr" target="#b43">[44]</ref>. They tend to prefer general-purpose search engines, particularly Google, over code-specific search engines <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b29">30]</ref>. However, search engines often serve as tools that lead users to other sites, such as SO, CSDN, or ZhiHu, where answers are primarily located <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Stack Overflow as a Primary Resource</head><p>SO has become an essential resource for programming help, with a large number of questions posted on various topics <ref type="bibr" target="#b43">[44]</ref>. It often serves as a substitute when official documentation is inadequate, providing solutions to common programming problems <ref type="bibr" target="#b6">[7]</ref>. However, despite its popularity, finding relevant SO posts can be challenging due to non-standardised error messages <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref> and the sheer volume of posts, along with duplicated questions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Developers often paste error logs directly into search boxes when looking for answers <ref type="bibr" target="#b13">[14]</ref>, which may not be the most effective approach, as code queries are more complex and often reformulated or edited <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b42">43]</ref>. They usually rely on Google to find software resources on SO <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16]</ref> and focus on accepted answers when looking at SO posts <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12]</ref>, even though these are not always the most voted-for answers.</p><p>Improving search results for error messages has been studied by various researchers, such as Hora, who conducted an empirical study of developer search queries <ref type="bibr" target="#b24">[25]</ref>, Monperrus and Maia, who focused on JavaScript code snippets in queries <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr">Barzilay et al.</ref>, who explored the types of questions asked and answered on SO <ref type="bibr" target="#b6">[7]</ref>, Xia et al., who observed developers' use of search engines for code and error explanations <ref type="bibr" target="#b55">[56]</ref>, and Li et al., who showed that both novice and expert programmers use SO for debugging <ref type="bibr" target="#b29">[30]</ref>. Our research seeks to identify effective query types for SO, with the goal of reducing the need for reformulation and improving the overall coding experience.</p><p>Enhancing compiler error messages has been a focus of previous research, including studies on plugins that add summarised information from SO <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b49">50]</ref> and approaches that improve compiler error messages specifically for Java without using SO <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b7">8]</ref>. Our study complements these findings by testing different factors and assessing the relevance of the results to the original compiler error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Compiler Error Help from Large Language Models</head><p>Artificial intelligence and machine learning have been applied to locate and repair bugs in source code with promising results. DeepFix <ref type="bibr" target="#b20">[21]</ref>, a multilayered sequence-to-sequence neural network with attention, partially fixed almost half of a set of programming tasks by predicting erroneous program locations and providing correct statements. Similar results were achieved with a reinforcement learning approach <ref type="bibr" target="#b21">[22]</ref>. Santos et al. <ref type="bibr" target="#b44">[45]</ref> used n-gram and LSTM language models to locate syntax errors and suggest fixes, achieving similar results. More recently, SYNFIX <ref type="bibr" target="#b4">[5]</ref>, which uses unsupervised pretraining and multi-label classification, outperformed previous methods such as DeepFix and Santos et al.</p><p>Despite these automatic error-fixing approaches, there has been little prior research on using LLMs to explain compiler error messages. In a recent study based on Codex <ref type="bibr" target="#b16">[17]</ref>, Leinonen et al. <ref type="bibr" target="#b28">[29]</ref> found that the model produced explanations of error messages that were "quite comprehensible", but correct in only about half of all cases. Codex's fix suggestions were also deemed correct in a similar number of cases. To the best of our knowledge, the potential of GPT-4 for explaining compiler error messages has not yet been studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Source Code Collection</head><p>To cover the varying use cases of SO and LLMs when addressing different compiler errors, we gathered Java source code with compiler errors from multiple sources, totaling 100 pieces. First, we randomly obtained 55 pieces of code from the Blackbox dataset <ref type="bibr" target="#b12">[13]</ref>. The Blackbox dataset comprises activity data from BlueJ IDE users <ref type="bibr" target="#b52">[53]</ref> who consented to have their activity recorded for research purposes. Using these pieces of code, we consider real-world instances where developers encountered compiler error messages during their coding projects. Additionally, we expanded the range of errors by creating 18 sets of code with unique error types not yet encountered in the BlackBox data. These pieces of code were designed to fail due to frequently occurring Java compiler errors, as documented in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8]</ref>, made loosely based on <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48]</ref>, and had not been encountered in the first batch of snippets from Blackbox (code number 1-35). Lastly, 27 pieces of source code were sourced from a separate participant study conducted as part of another project by the first author. This project also focused on compiler errors, and the code was collected when three participants with diverse backgrounds and levels of Java experience completed exercises from code-exercises.com <ref type="bibr" target="#b41">[42]</ref>. In the results and discussion sections, we will refer to these three sources of Java code as blackbox, custom, and user_study, respectively.</p><p>We sourced code snippets from three origins to increase confidence in generalisability and to ensure our dataset accurately represents a variety of real-world coding errors. As the codes were retrieved from singular files as opposed to a project with multiple files, the length of each is relatively short with a median Lines of Code (LOC) of 16. The complete LOC distribution is shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Methodology</head><p>To analyse the impact of various factors on the overall effectiveness of SO and LLM queries, we evaluate the results obtained under different configurations. We define effectiveness as the degree of assistance provided by a result in resolving the first compiler error encountered in a Java program. For SO queries, we classified the resulting initial posts into four distinct relevance categories, as detailed in Table <ref type="table" target="#tab_0">1</ref>. This rating scale was proposed and used by Mahajan et al. <ref type="bibr" target="#b32">[33]</ref> in measuring the relevance of SO posts, and was based on <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28]</ref>. We refer to this categorisation as the IHMUcategory.</p><p>During the classification of the results, we considered the entire content of the page to mimic real-world usage, where developers are more likely to focus on the text of the post rather than just the titles <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b29">30]</ref>. The first author conducted this manual annotation. A cross-check with the second author annotating a subsection (10%) of the results was also performed. The Cohen's Kappa <ref type="bibr" target="#b35">[36]</ref> coefficient was then calculated to measure inter-rater reliability, ensuring that the overall annotation bias is acceptable and that the results can be considered valid. We obtained a Cohen's Kappa value of 0.860, indicating an agreement level of "almost perfect".</p><p>After completing all necessary SO annotations, we followed Mahajan et al.'s evaluation methodology and calculated three metrics: I-Score, IH-Score, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Helpful (H)</head><p>Post is generally informative about the compiler error but may not specifically target the underlying cause or provide an effective fix Misleading (M)</p><p>Post is irrelevant to the compiler error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unavailable (U)</head><p>No results were returned from the query and M-Score. Additionally, we introduced an IH/M-Score, which measures the percentage of instrumental and helpful results relative to the number of misleading results. This metric helps evaluate whether an increase in relevant results is accompanied by a trade-off of also increasing misleading results. These metrics and their respective formulas are listed in Table <ref type="table" target="#tab_1">2</ref>. </p><formula xml:id="formula_0">I I+H+U +M × 100% IH-Score I+H I+H+U +M × 100% M-Score M I+H+U +M × 100% IH/M-Score I+H M × 100%</formula><p>Moreover, we measured the similarity between the results of different SO combinations. To achieve this, we calculated the Rank-Biased Overlap (RBO) <ref type="bibr" target="#b53">[54]</ref> of the top 10 links for each combination in the 100 scenarios. An RBO score of 1 indicates a completely "identical" list, while a score of 0 indicates a completely "different" list <ref type="bibr" target="#b53">[54]</ref>. We then computed the median of these scores and present the corresponding heatmap.</p><p>The evaluation of the results obtained from LLMs followed a similar methodology to that of SO, with a few adjustments. Since ChatGPT, as a chatbot on top of GPT-3.5 and GPT-4, virtually always returns results that are or at least appear relevant when provided with error messages, we modified the IHMU-category to exclude the "Unavailable" category and more narrowly defined the other categories, as shown in Table <ref type="table" target="#tab_2">3</ref>. We did not calculate similarities through RBO between the results obtained from LLMs as part of this study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Helpful (H)</head><p>Response provides general but not exact help. E.g. listing all potential common causes of error along with multiple fixes from which the coder would still have to analyse</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Misleading (M)</head><p>Response does not provide a clear direction on how to fix the issue and/or causes confusion. E.g. irrelevant results, right code snippet however with a completely wrong explanation, returning other common causes without the root cause, and responses that provides fixes to errors in different parts of the code but would not fix the first compiler error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stack Overflow Combinations</head><p>Given the numerous ways to search SO, we conducted experiments with various query approaches to identify the most effective methods for obtaining relevant results. Each query corresponded to a different combination of query method, post filter, and query content, resulting in a total of 2 × 3 × 12 = 72 unique combinations. These are described in Table <ref type="table" target="#tab_3">4</ref>, with additional definitions and clarifications outlined below.</p><p>First, to obtain results with "searching directly through SO" (SO-API) as a search method, we used the /search functionality of the SE API <ref type="bibr" target="#b1">[2]</ref>, with the corresponding queries saved under the "intitle" argument. We employed this method because of the absence of a full-text search option.</p><p>The Custom query content treatment refers to queries in which we replaced code specific identifiers from the original compiler error message with a more generic variable or removed it altogether. This modification was implemented for some common messages to make them more generic. For example, we replaced the variable and method name in the "variable [variable name] is already defined in method [method name]" errors, and the class name in the "class [class name] is public, should be declared in a file named [class name].java" errors with generic placeholders "X" and "Y". Under query content, the "parent node" was obtained by creating a concrete syntax tree of the Java code snippet using the parser library treesitter <ref type="bibr" target="#b2">[3]</ref> and then saving the parent node of the line where the compiler error occurred. The same parser library was used to identify parts of the code labelled as identifiers, which were used in defining LineBlankId, Parent-BlankId, and ParentXId.</p><p>We tested each of these 72 combinations on the 100 Java codes as described in the Source Code Collection section, yielding a total of 72 × 100 = 7, 200 query runs. From each run, we saved up to the first 10 SO links returned for further analysis of relevance and similarity.</p><p>To facilitate data collection, we created a Sublime Text 4 plugin. When called, the plugin automatically executes each of the 72 combinations described in Table <ref type="table" target="#tab_3">4</ref> and logs the details related to each run. This information included the combination to which it corresponded, the source code on which it was run, the total number of results returned, and up to the first 10 SO links it returned. The total number of results returned for queries run on SO-API was simply counted as the number of items in the JSON file it returned, whereas for Google SO-Google queries, we scraped the resulting page to find the div element with the id "result-stats" from which the relevant value was saved. This represents the part "About [number] results" found in Google search results. When errors occurred while scraping the page, we placed a placeholder value of 0 instead. This placeholder value is not expected to affect the results, as the percentage of error occurrence was negligible at only 0.5%. We used the plugin on each of the 100 Java pieces of code with compiler errors as previously described.</p><p>Compiling the first links from all 7,200 query runs yielded a total of 573 unique pairs of source code and the corresponding first link. The relatively lower number of unique pairs was due to several combinations returning the same top link and many combinations, particularly those with long code snippets, not returning any results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Large Language Model Combinations</head><p>To address research questions related to LLMs, we examined the impact of including the source code in the query versus only providing the error message, the influence of prompt phrasing on the relevance of the results, and the differences between using the "Default (GPT-3.5)" model and the newer "GPT-4" version. We selected the prompts "What does the error mean?" and "How can I fix the error?" based on experiments with GPT-3.5. Table <ref type="table" target="#tab_4">5</ref> enumerates the 2 × 2 × 2 = 8 LLM query combinations. We collected LLM data by pasting each of the 100 error messages with and without code snippet on the ChatGPT page<ref type="foot" target="#foot_0">foot_0</ref> along with the one of the two prompts. To maintain consistency, only the first responses i.e. no regeneration of the 800 queries were saved and evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Data Availability</head><p>We have made our data and scripts available in our online repository<ref type="foot" target="#foot_1">foot_1</ref> . This includes an Excel file containing the 100 code snippets tested along with their corresponding compiler errors, resulting SO links, LLM responses, authors' annotations, and resulting tables. Additionally, the two notebooks used to calculate the median number of results and RBOs, as well as the Sublime plugin created to obtain links based on the 72 combinations related to SO, have been made available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, we present and discuss answers to our research questions. An important factor to consider is that SO-API queries frequently returned no results, affecting the results. Out of 100 scenarios, the average number of times SE queries yielded no results was 83.1. When queries without code snippets are excluded, empty results become even more prominent, averaging about 91.1%. On the contrary, the average empty results for SO-Google queries were significantly lower at 23.7% in general or 27.8% when excluding queries without code snippets.</p><p>Table <ref type="table" target="#tab_5">6</ref> presents the median number of results for the 100 compiler errors when using different query combinations. Since the median values for SO-API queries are mostly 0, and LLMs provide one response per query, the discussion about the number of results will mainly focus on SO-Google. More than the sheer number of results, when using SO or LLMs to resolve compiler errors, the relevancy of a post or response to the specific issue is important. We have summarised the values of the IHM(U) categories, as defined in Table <ref type="table" target="#tab_0">1</ref> and <ref type="table" target="#tab_4">5</ref>, in Table <ref type="table" target="#tab_6">7</ref>. This summary only includes a subset of the combinations tested and will be discussed in relation to RQ1. A more detailed breakdown of other SO-Google and SO-API combinations, along with their corresponding metrics, will be presented as part of the RQ2 results, while GPT-3.5 and GPT-4 will be discussed as part of RQ3. [RQ1] How effective are Stack Overflow and large language models at explaining compiler errors? Table <ref type="table" target="#tab_6">7</ref> shows that SO queries exhibit mixed results in their ability to help explain compiler errors when searched directly through the API and when searched through Google. Without code snippets, searching through Google is more promising, with 57% IH compared to SO-API/NoSnippet's 15%. The high number of misleading compared to the relevant relevant results is also particularly concerning for SO-API/NoSnippet. In contrast, when the parent node of the error line is included in the query, the trend reverses: SO-API/Parent does not return misleading results, compared to over 40% found by its SO-Google counterpart. However, although SO-API/Parent did not give misleading results, it only returned results in 4 of the 100 scenarios.</p><p>In terms of seeking help for Java compiler errors, LLMs proves to be notably more effective than SO. LLMs consistently return a result with a higher relevancy rate than our SO searches. Specifically, when using GPT-4 and providing the full Java code snippet along with the original error message, a relevancy rate of 100% is achieved. This represents an improvement over the older GPT-3.5 version, which achieved a relevancy rate of 87% for the same queries. A consistent trend observed in both versions is that adding the full code snippet to the query positively impacts the results. In scenarios with or without code snippets in the query, LLM's results consistently outperform SO's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[RQ2]</head><p>To what extent does the query configuration influence Stack Overflow's ability to explain compiler errors?</p><p>We evaluated 72 different SO query combinations. The percentage of results for the different combinations based on their respective IHMU categories is shown in Figure <ref type="figure" target="#fig_1">2</ref>. The corresponding metrics can be found in Table <ref type="table" target="#tab_7">8</ref>.  Furthermore, we calculated the median RBOs between different combinations and present them in Figure <ref type="figure" target="#fig_2">3</ref> and <ref type="figure" target="#fig_3">4</ref>. Although we calculated all RBOs between Google and SE queries, we found the median to be 0 for all combinations with each other. This indicates their dissimilarity and provides a rationale for presenting the results in two separate figures. As expected, the addition of code snippets resulted in a lower median number of results. Although the median for Google queries without code snippets was more than 30,000, it was at least 10 times lower for all other combinations tested. The addition of a single line where the error was thrown reduced the median from 32,400 to 471 (-98.5%). Including the parent node of the error line instead led to a further decrease to a median value of 6.</p><p>Although the difference was not as large, removing or replacing identifiers within code snippets with "x" increased the number of results compared to their "as-is" code snippet equivalents. Interestingly, the median number was higher when the identifiers were replaced with In terms of error message treatment, removing code specific identifiers from the error message (SO-Google/Custom) resulted in a higher number of results compared to simply using the original compiler error message (SO-Google/Original), with one exception.</p><p>Regarding relevancy, considering only the first link returned for the 100 scenarios from each combination, those returned from queries with no code snippets had the best IH-Score performance. This was observed irrespective of the query method used and whether post filtering was applied or not. The best overall results in terms of IH and IH/M-Score were achieved by both SO-Google/First/Original/NoSnippet and SO-Google/Positive/Original/NoSnippet queries at 57% and 139%, respectively.</p><p>For SE queries (SO-API), however, this came with the trade-off of a significantly higher M-Score compared to when code snippets were added. Their M-Scores averaged around 40%, compared to other combinations, which had M-Scores between 0% and 9%. Consequently, this led to the IH/M-Score being less than 100%, meaning that most of the results were irrelevant and useless for fixing the original compiler error.</p><p>For Google queries, the inclusion of code snippets had a more varied impact. One of the more evident outcomes is that adding the parent node with identifiers replaced with "x" as the code snippet proved to be the worst addition, with only 14% to 22% IH-Score and up to 62% misleading results received from these types of queries. The addition of the unedited parent node (SO-Google/Parent) performed slightly better, although their IH/M-Scores were still only 48% at best. On the other hand, when the parent node was added, but with the code specific identifiers removed entirely, it resulted in a significantly lower number of misleading results compared to all other (SO-Google) combinations. However, the overall IH/M-Scores remained lower than when using no code snippets.</p><p>Interestingly, this finding does not apply to SO-API queries, where the lowest M-Scores at 0% occurred when the parent node was included as is or with the identifiers replaced with "x". Instead, removal of the identifiers increased the M-Scores to 4% to 5%. Overall, adding more context to SE queries led to higher IH/M-Scores, where the reduction in misleading results had a more significant impact than the relatively smaller decrease in IH results. However, this comes with the caveat that the overall number of results from SE queries, excluding those with no code snippets, is relatively low, and most of the queries return nothing.</p><p>To analyse the similarity of the results, we examine the calculated RBO values. The inclusion or exclusion of code snippets in the query, along with the type if any was included, likely affects the similarity of the results. In Figure <ref type="figure" target="#fig_2">3</ref>, the presence of green and yellow 6 × 6 squares on the diagonal indicates the relatively high correlation between the first 10 results of Google queries with the same code snippets, regardless of error message treatment and post filtering. This is most apparent between queries with the parent node included as code snippets, represented by the three green squares in the bottom right quadrant.</p><p>In contrast, there are no green boxes outside the 6 × 6 area that represent those with exactly the same type of code snippet. This means that removing or replacing the identifiers within the code snippets significantly changes the results, leading to lower similarity and, consequently, RBO. However, some correlation remains visible, particularly between combinations with the error line as the code snippet (SO-Google/Line, SO-Google/LineBlankId). For these, although deleting the identifiers affected the results, the impact was less than for the parent node code snippet.</p><p>There is also a difference between removing identifiers from the error line and not doing so. A correlation can still be observed between queries with the error line without identifiers SO-Google/LineBlankId and a plain query without a code snippet SO-Google/NoSnippet, albeit only within the yellow range. In contrast, the same cannot be said between queries using the error line as is SO-Google/Line and those with no code snippet SO-Google/NoSnippet due to their low RBO with each other.</p><p>As mentioned earlier, SE queries often returned empty sets. Since the RBO value between two empty sets is 1, this resulted in a high overall RBO value and formed the large green square in Figure <ref type="figure" target="#fig_3">4</ref>. The small green box in the upper left corner, on the other hand, represents the RBO values between the results when querying SE without any code snippets and only the original compiler error messages SO-API/NoSnippet. Similarly to Google queries, the results of these queries appear to be highly correlated with each other, and the removal of identifiers from the error message has no major impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.2.">Impact of Query Method</head><p>We discovered that the number of results is significantly affected by the query method used. The highest median number of results for SE SO-API queries was only 3.5, found when searching for the generic compiler error message with the identifiers removed without any code snippets SO-API/Custom/NoSnippet. On the contrary, searching for the same query on Google SO-Google/Custom/NoSnippet yielded a median of 36,350 results. When adding any kind of code snippet, regardless of whether it was the code error line or the parent snippet, the median number of SE results was 0.</p><p>Although SE queries with code snippets rarely return anything, when they do, the results are much more likely to be accurate, with IH/M-Scores above 100%. Queries with the parent node added as is or replaced with x (SO-API/Parent, SO-API/ParentXId) performed particularly well. Although they only returned a result four times within the 100 scenario runs, all of the first links returned were relevant.</p><p>Overall, when using SE queries with code snippets, the results are proportionally more relevant than their equivalents on Google, although the results were returned less frequently. Note that this trend does not apply to queries without code snippets SO-API/NoSnippet, which have IH/M-Scores below 60%. For general searches without code snippets, Google queries had a higher percentage of relevant results, both in terms of overall numbers (IH-Score) and as a percentage over the number of misleading queries (IH/M-Score).</p><p>Taking into account the top 10 results of each query and their corresponding median RBO scores with each other, we found no similarities between the SE results and the Google results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.3.">Impact of Post Filtering</head><p>Among the three factors discussed so far, post filtering arguably has the least noticeable impact on the relevancy of results. Specifically, for results obtained by filtering only for positive posts, no discernible impact can be observed compared to simply taking the first post returned. Interestingly, filtering for only posts with an accepted answer did not result in higher relevance; rather, it was lower in most combinations. Further analysis revealed that this occurred when the original first link found had a positively rated answer that correctly addressed the problem, but was not accepted as the answer. Consequently, the tool had to search further down the list of returned links to find a post with an accepted answer. In some cases, these posts may be less relevant to the original error, discussing a completely different problem and leading to a misleading result.</p><p>For Google results, most of the time, the first returned results are posts with positively rated answers. This is evidenced by the high RBO values between simply taking the first answer and filtering for positive answers (SO-Google/First, SO-Google/Positive) for the same query. Their RBO values range from 0.75 to 1.00, represented by the medium to dark green 4 × 4 boxes on the diagonal in Figure <ref type="figure" target="#fig_2">3</ref>. Comparatively, the first results are less likely to be accepted answers, since filtering for only posts with accepted answers resulted in lower RBOs without filters. This is illustrated by the lighter green to yellow rectangles surrounding the green 4 × 4 boxes, with RBO values between 0.49 and 0.94.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[RQ3]</head><p>To what extent does the query configuration influence the ability of large language models to explain compiler errors?</p><p>Unlike the results from Google and the SE API, where sometimes the number of results returned was 0, the LLMs provided a reply for all the scenarios we tested. However, this came with the caveat that there were instances where the LLM produced "plausible-sounding but incorrect or nonsensical answers" <ref type="bibr" target="#b38">[39]</ref>. In particular, there was an instance where the LLM gave the correct fix to the code, should a user choose to directly copy the recommended code fix, but provided a misleading explanation for it. Additionally, there were instances where the tool provided multiple answers, one being correct and the other misleading. Even when using the newer version, there was still a scenario where running the provided code would fix the compiler error but cause an ArrayIndexOutOfBoundsException at runtime.</p><p>Nevertheless, overall results highlighted LLM's potential, with the majority of responses received being either instrumental or helpful in fixing the queried compiler error.</p><p>As success was already seen in using LLM with full code snippets, we did not test further query alteration beyond inclusion or exclusion of whole snippets. Instead, we added a breakdown by code source to investigate whether there are particular types of code for which the LLM would work best. The results are summarised in Table <ref type="table" target="#tab_8">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.1.">Impact of Query Content</head><p>In all configurations and versions tested, adding the whole code snippet with compiler error as part of the query resulted in higher result relevancy. Especially in GPT-4, this led to a perfect result of 100% instrumental or helpful. When a code snippet is not available, LLMs often revert to listing common causes of the error message, which may or may not cover the underlying cause.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.2.">Impact of LLM Versions</head><p>GPT-4 shows a notable improvement over its predecessor. This is true for all cases except where the code used was based on common errors (custom) and the complete code snippet was provided. In this case, GPT-3.5 had already achieved perfect performance at 100%, leaving no room for improvement when using the newer version. For scenarios blackbox and user_study and for the cases where no code snippets were provided, GPT-4 proved to be an advance. In particular, when the code snippet is provided (GPT-4/Full), understanding the context and intent of the user was demonstrated, e.g., by creating a code implementation based on commented-out parts of the code beyond fixing errors. In contrast, GPT-3.5 did not take extra context into account and provided more straightforward solutions, e.g., by not changing commented-out parts of the code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.3.">Impact of Query Phrasing</head><p>As OpenAI acknowledged to be one of the limitations of LLMs <ref type="bibr" target="#b38">[39]</ref>, we observed that input phrasing affected overall results returned by the tool. Among the two tested questions, "How can I fix the error?" generally performed better in terms of IH-Scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">For Developers</head><p>Developers spend a significant amount of time searching the Web for help and reformulating queries <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b13">14]</ref>. Through our findings, we aim to improve this experience and increase overall efficiency when using LLMs or SO to find help fixing compiler errors.</p><p>When using SO, we find that searching SO via Google instead of a direct search impacts the type and number of results returned. When no code snippet is available, a Google search is considered a better approach. For queries that include only the compiler error message and NoSnippet, we found that Google queries had a higher percentage of helpful answers and a lower percentage of misleading results. Conversely, if a user has access to the original code, it may prove beneficial to search directly on SO while including a code snippet. We identified a notably higher percentage of helpful against misleading results for these types of queries, making this approach particularly useful for novices with less experience in gauging the relevancy and correctness of a post. However, we also note that SO-API rarely returns a result, so a Google search may still be needed in most cases. Moreover, removing code-specific identifiers and/or replacing them with "x" was often seen to increase the number of results.</p><p>When examining the first link returned from queries, we did not observe a positive relationship between filtering for a post with an accepted answer and a more relevant answer. Therefore, we suggest not discounting posts just because they contain no accepted answers.</p><p>LLMs demonstrated great potential to help developers fix their compiler errors. When comparing results across types of source code, LLMs appear to work particularly well for shorter code snippets with common errors, given the perfect results found for custom code snippets, and also when the entire code snippet is made available as part of the query. Thus, for developers who create simple code snippets that need a quick fix, an LLM may be an excellent tool to try first, ensuring that the code is included when available. However, unlike SO, LLM answers do not have other users voting or providing comments on accuracy. This, combined with the fact that LLMs can make misleading answers sound plausible, means that users must exercise discretion and consider whether the answers provided are truly correct.</p><p>For tool developers working with compiler errors, many adopt the approach of imitating how programmers search the Web for solutions and then automating the process. For these types of approaches, the same recommendations apply. For instance, if a tool functions as an IDE plugin, it might be beneficial to first try including code snippets in the query and searching directly on SO. On the contrary, for tools that operate similarly to a search engine and do not have access to user code, implementing a search that works via Google may be more effective.</p><p>However, this might not necessarily hold true for all approaches, and hence our findings can be implemented depending on how a tool developer plans to interact with SO and user code. It is essential for a tool developer to test different query formats and methods before committing to one, due to the dissimilarity of the results, as shown in Figure <ref type="figure" target="#fig_2">3</ref> and <ref type="figure" target="#fig_3">4</ref>.</p><p>As we did not test the ChatGPT API, we cannot comment on the differences compared to using it through the user interface. However, assuming similar performance, the ChatGPT API may be a viable option for developers, especially if access to the underlying code is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">For Researchers</head><p>There are opportunities to further investigate the impact of different factors on query effectiveness for both SO and LLMs. Given that minor changes can have a significant impact on results, this opens up possibilities for researchers to delve deeper into the topic, e.g., by testing various parts of code snippets and/or code pre-processing approaches. Unlike Monperrus and Maia's study <ref type="bibr" target="#b36">[37]</ref>, where both code pre-processing methods improved their overall debugging process, we found mixed results. Consequently, further research would be beneficial, particularly considering the high percentage of SO usage in debugging errors.</p><p>Additionally, our study has demonstrated LLMs' potential in assisting with fixing compiler errors. Further research in this area would be compelling and can be approached from various angles, such as focusing on other types of errors, identifying the best types of questions to ask, or even detecting the characteristics that distinguish relevant and correct answers from misleading and non-sensical ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Threats to Validity</head><p>We acknowledge that there exist limitations and threats that may impact the validity of our results.</p><p>• Construct Validity: A threat comes from the subjectivity of measuring the relevance of SO posts and LLM responses to the underlying compiler error. To mitigate this, we implemented a clearly defined set of classifications consistent with previous studies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28]</ref>. To further build confidence in the ratings, we also had a subset of the results independently annotated by a different rater. The inter-rater agreement of 0.860 suggests "almost perfect" agreement.</p><p>• Only first links are considered: When analysing the scores and metrics used in the SO results and discussion (excluding RBOs), we evaluated the results based on the first links returned from each query. One could argue that the relevance of a query should consider more than just the first link, noting that experienced developers are likely to consider further links <ref type="bibr" target="#b19">[20]</ref>. We recognise this as a limitation of our current approach and that our results are based on the assumption that the first link returned is an accurate representation of the relevancy of subsequent links.</p><p>• Only first error encountered are considered: For the Java code from which we collected the data, queries were created only based on the first error encountered, even when their compilation leads to multiple errors. It has often been advised to fix the first error first before recompiling, and that many subsequent errors are caused by the first <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. Hence, fixing only the first error each compile likely mimics common behaviour when error fixing anyway, and we do not anticipate this threat to have a large impact on overall validity. However, aspects of fixing multiple errors at once, a behaviour shown a few times in the LLM responses we analysed, could be a potential area for future study.</p><p>• Limited Number of Results from SO-API Searches: The low number of results returned from SO-API searches can potentially impact the findings. However, developers often access Stack Overflow by querying search engines such as Google instead of directly visiting the website <ref type="bibr" target="#b24">[25]</ref>. Considering that Google searches may better reflect actual search behavior, this limitation may be mitigated to some extent.</p><p>• Reproducibility: As Google's search algorithm is not public, the exact same query may return different results depending on location, time, and device type <ref type="bibr" target="#b0">[1]</ref>, among others. This is also true for LLM answers, which are "sensitive to tweaks to the input phrasing or attempting the same prompt multiple times" <ref type="bibr" target="#b38">[39]</ref>. To minimise the potential bias introduced by prompting, simple prompts were used and alternatives were evaluated. Additionally, the consistency of the methodology and the environment was maintained during data collection.</p><p>• Generalisability: Our study focuses on Java compiler errors. This means that the same conclusions may not hold for other programming languages and types of errors. Additionally, even for Java compiler errors, we cannot claim that the findings will generalise beyond our dataset. Although we have tried to minimise this threat by including code from different sources, we acknowledge that the scale of the dataset itself is still small, covers relatively shorter code snippets, and may not cover all types of compiler errors. Lastly, although we observe trends, e.g., between the usefulness of crowd-curated compiler error help and automatically generated support, we cannot generalise to future versions of tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Compiler error messages often fail to provide enough targeted information to enable programmers to fix their code. Our study systematically compares SO and LLMs, such as GPT-3.5 and GPT-4, for their effectiveness in explaining compiler error messages. Our findings demonstrate that GPT-4 consistently outperforms both GPT-3.5 and SO, especially when provided with the offending code snippet along with the error message. Furthermore, we examine the influence of query configuration on SO's ability to yield relevant answers and the impact of prompt configuration on LLMs' capacity to explain compiler errors. The insights gained from this research have practical implications for programmers, who can leverage these findings to optimise their debugging process, and for researchers, who can build upon this work to explore the rapidly evolving landscape of AI-driven assistance in programming and debugging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Lines of Code (LOC) distribution of code snippets</figDesc><graphic coords="8,110.85,125.80,388.56,259.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: IHMU-category of resulting first links</figDesc><graphic coords="16,110.85,226.92,388.54,313.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: RBO values between Google SO-Google queries</figDesc><graphic coords="18,110.85,210.53,388.55,256.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: RBO values between StackExchange SO-API queries</figDesc><graphic coords="19,110.85,125.80,388.54,264.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>IHMU-category relevancy classification</figDesc><table><row><cell>Classification</cell><cell>Definition</cell></row><row><cell>Instrumental (I)</cell><cell>Post relates to the underlying cause of the first compiler error perfectly and provides a complete fix</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Formulas used in measuring result relevancy</figDesc><table><row><cell>Classification</cell><cell>Definition</cell></row><row><cell>I-Score</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Adjusted IHM-category relevancy classification</figDesc><table><row><cell>Classification</cell><cell>Definition</cell></row><row><cell>Instrumental (I)</cell><cell>Response perfectly targets underlying cause of error and pro-vides a clear action on how to fix</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Description of the identifiers used for SO via API and SO via Google queries</figDesc><table><row><cell></cell><cell cols="2">Query Method</cell></row><row><cell>Identifier</cell><cell>Query Search Method</cell><cell></cell></row><row><cell>SO-API</cell><cell cols="2">Search request through StackExchange API as a proxy for searching Stack Overflow directly</cell></row><row><cell>SO-Google</cell><cell cols="2">Search via Google with "site: stackoverflow" included in query</cell></row><row><cell></cell><cell cols="2">Post Filter</cell></row><row><cell>Identifier</cell><cell>Query Result Filter</cell><cell></cell></row><row><cell>First</cell><cell cols="2">Take first result (Stack Overflow post) without any filtering</cell></row><row><cell>Positive</cell><cell cols="2">Take first result (Stack Overflow post) with a positively rated answer</cell></row><row><cell>Accepted</cell><cell cols="2">Take first result (Stack Overflow post) with an accepted answer</cell></row><row><cell></cell><cell cols="2">Query Content</cell></row><row><cell>Identifier</cell><cell>Description</cell><cell>Example</cell></row><row><cell>Custom/</cell><cell>custom error message + origi-</cell><cell>non-static method cannot be referenced from a static context</cell></row><row><cell>Line</cell><cell>nal error line</cell><cell>System.out.println(reverse(s));</cell></row><row><cell>Custom/ LineBlankId</cell><cell>custom error message + error line with identifiers replaced with blanks</cell><cell>non-static method cannot be referenced from a static context ..(());</cell></row><row><cell>Custom/</cell><cell>custom errror message only</cell><cell>non-static method cannot be referenced from a static context</cell></row><row><cell>NoSnippet</cell><cell></cell><cell></cell></row><row><cell>Custom/</cell><cell>custom error message + par-</cell><cell></cell></row><row><cell>Parent</cell><cell>ent node of error line</cell><cell></cell></row><row><cell>Custom/ ParentBlankId</cell><cell>custom error message + par-ent node with identifiers re-placed with blanks</cell><cell>non-static method cannot be referenced from a static context public static void main() String = "java interview"; ..());</cell></row><row><cell></cell><cell>custom error message + par-</cell><cell>non-static method cannot be referenced from a static con-</cell></row><row><cell>Custom/</cell><cell>ent node with identifiers re-</cell><cell>text public static void main() String x = "java interview";</cell></row><row><cell>ParentXId</cell><cell>placed with "x"</cell><cell>x.x.x());</cell></row><row><cell>Original/</cell><cell>original error message + orig-</cell><cell>non-static method reverse(String) cannot be referenced from</cell></row><row><cell>Line</cell><cell>inal error line</cell><cell>a static context System.out.println(reverse(s));</cell></row><row><cell>Original/ LineBlankId</cell><cell>original error message + error line with identifiers replaced with blanks</cell><cell>non-static method reverse(String) cannot be referenced from a static context ..(());</cell></row><row><cell>Original/ NoSnippet</cell><cell>original error message only</cell><cell>non-static method reverse(String) cannot be referenced from a static context</cell></row><row><cell>Original/ Parent</cell><cell>original error message + par-ent node of error line</cell><cell>non-static method reverse(String) cannot be referenced from a static context [ public static void main(), String s = "java interview";, System.out.println());, ]</cell></row><row><cell></cell><cell>original error message + par-</cell><cell>non-static method reverse(String) cannot be referenced from</cell></row><row><cell>Original/</cell><cell>ent node with identifiers re-</cell><cell>a static context public static void main() String = "java</cell></row><row><cell>ParentBlankId</cell><cell>placed with blanks</cell><cell>interview"; ..());</cell></row><row><cell></cell><cell>original error message + par-</cell><cell>non-static method reverse(String) cannot be referenced from</cell></row><row><cell>Original/</cell><cell>ent node with identifiers re-</cell><cell>a static context public static void main() String x = "java</cell></row><row><cell>ParentXId</cell><cell>placed with "x"</cell><cell>interview"; x.x.x());</cell></row></table><note><p>non-static method cannot be referenced from a static context [ public static void main(), String s = "java interview";, System.out.println());, ]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Description of the identifiers used for LLM queries</figDesc><table><row><cell></cell><cell cols="2">Query Method</cell></row><row><cell>Identifier</cell><cell>Query Search Method</cell><cell></cell></row><row><cell>GPT-3.5</cell><cell cols="2">Query on https://chat.openai.com/ with "Default (GPT-3.5)" model</cell></row><row><cell>GPT-4</cell><cell cols="2">Query on https://chat.openai.com/ with "GPT-4" model</cell></row><row><cell></cell><cell cols="2">Query Phrasing</cell></row><row><cell>Identifier</cell><cell>Query Phrasing</cell><cell></cell></row><row><cell>What</cell><cell cols="2">Query content asked alongside "What does the error mean?"</cell></row><row><cell>How</cell><cell cols="2">Query content asked alongside "How can I fix the error?"</cell></row><row><cell></cell><cell cols="2">Query Content</cell></row><row><cell>Identifier</cell><cell>Description</cell><cell>Example</cell></row><row><cell></cell><cell>original error message + full</cell><cell>test.java:5: error: cannot find symbol public class test public</cell></row><row><cell>Original/</cell><cell>Java snippet from which the</cell><cell>static void main(String[] args) System.ouch.println("Hello,</cell></row><row><cell>FullSnippet</cell><cell>first error message is thrown.</cell><cell>World!");</cell></row><row><cell>Original/</cell><cell>original error only</cell><cell>test.java:5: error: cannot find symbol</cell></row><row><cell>NoSnippet</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Median number of results returned</figDesc><table><row><cell>Identifier</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Summary of Results</figDesc><table><row><cell>Search (all with original error message)</cell><cell>IH</cell><cell>M</cell><cell>U</cell></row><row><cell>SO-API/NoSnippet</cell><cell>15</cell><cell>40</cell><cell>45</cell></row><row><cell>SO-Google/NoSnippet</cell><cell>57</cell><cell>41</cell><cell>2</cell></row><row><cell>SO-API/Parent</cell><cell>4</cell><cell>0</cell><cell>96</cell></row><row><cell>SO-Google/Parent</cell><cell>22</cell><cell>48</cell><cell>30</cell></row><row><cell>GPT-3.5/What/NoSnippet</cell><cell>72</cell><cell>28</cell><cell>-</cell></row><row><cell>GPT-4/What/NoSnippet</cell><cell>82</cell><cell>18</cell><cell>-</cell></row><row><cell>GPT-3.5/What/FullSnippet</cell><cell>87</cell><cell>13</cell><cell>-</cell></row><row><cell>GPT-4/What/FullSnippet</cell><cell>100</cell><cell>0</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Calculated relevancy metrics</figDesc><table><row><cell>Accepted (Scores)</cell><cell>IH M IH/M</cell><cell>27% 47% 57%</cell><cell>5% 2% 250%</cell><cell>26% 47% 55%</cell><cell>8% 9% 89%</cell><cell>49% 44% 111%</cell><cell>12% 42% 29%</cell><cell>20% 46% 43%</cell><cell>4% 0% inf</cell><cell>24% 34% 71%</cell><cell>4% 4% 100%</cell><cell>14% 61% 23%</cell><cell>4% 0% inf</cell><cell>30% 45% 67%</cell><cell>5% 2% 250%</cell><cell>28% 44% 64%</cell><cell>6% 10% 60%</cell><cell>50% 42% 119%</cell><cell>7% 41% 17%</cell><cell>21% 44% 48%</cell><cell>4% 0% inf</cell><cell>25% 32% 78%</cell><cell>5% 5% 100%</cell><cell>16% 59% 27%</cell><cell>4% 0% inf</cell></row><row><cell></cell><cell>I</cell><cell>14%</cell><cell>0%</cell><cell>11%</cell><cell>1%</cell><cell>15%</cell><cell>3%</cell><cell>9%</cell><cell>0%</cell><cell>8%</cell><cell>0%</cell><cell>5%</cell><cell>0%</cell><cell>16%</cell><cell>0%</cell><cell>13%</cell><cell>0%</cell><cell>17%</cell><cell>1%</cell><cell>11%</cell><cell>0%</cell><cell>9%</cell><cell>0%</cell><cell>6%</cell><cell>0%</cell></row><row><cell>Positive (Scores)</cell><cell>IH M IH/M</cell><cell>30% 49% 61%</cell><cell>8% 1% 800%</cell><cell>27% 51% 53%</cell><cell>15% 7% 214%</cell><cell>51% 49% 104%</cell><cell>24% 40% 60%</cell><cell>22% 48% 46%</cell><cell>4% 0% inf</cell><cell>30% 32% 94%</cell><cell>4% 4% 100%</cell><cell>20% 61% 33%</cell><cell>4% 0% inf</cell><cell>33% 46% 72%</cell><cell>8% 1% 800%</cell><cell>30% 46% 65%</cell><cell>10% 9% 111%</cell><cell>57% 41% 139%</cell><cell>15% 40% 38%</cell><cell>22% 47% 47%</cell><cell>4% 0% inf</cell><cell>32% 28% 114%</cell><cell>5% 5% 100%</cell><cell>21% 58% 36%</cell><cell>4% 0% inf</cell></row><row><cell></cell><cell>I</cell><cell>14%</cell><cell>3%</cell><cell>13%</cell><cell>6%</cell><cell>21%</cell><cell>11%</cell><cell>9%</cell><cell>0%</cell><cell>13%</cell><cell>0%</cell><cell>11%</cell><cell>0%</cell><cell>16%</cell><cell>3%</cell><cell>16%</cell><cell>4%</cell><cell>25%</cell><cell>9%</cell><cell>9%</cell><cell>0%</cell><cell>16%</cell><cell>0%</cell><cell>11%</cell><cell>0%</cell></row><row><cell></cell><cell>IH/M</cell><cell>61%</cell><cell>800%</cell><cell>49%</cell><cell>214%</cell><cell>108%</cell><cell>60%</cell><cell>45%</cell><cell>inf</cell><cell>91%</cell><cell>100%</cell><cell>34%</cell><cell>inf</cell><cell>72%</cell><cell>800%</cell><cell>64%</cell><cell>111%</cell><cell>139%</cell><cell>38%</cell><cell>46%</cell><cell>inf</cell><cell>103%</cell><cell>100%</cell><cell>37%</cell><cell>inf</cell></row><row><cell>First (Scores)</cell><cell>IH M</cell><cell>30% 49%</cell><cell>8% 1%</cell><cell>26% 53%</cell><cell>15% 7%</cell><cell>52% 48%</cell><cell>24% 40%</cell><cell>22% 49%</cell><cell>4% 0%</cell><cell>30% 33%</cell><cell>4% 4%</cell><cell>21% 62%</cell><cell>4% 0%</cell><cell>33% 46%</cell><cell>8% 1%</cell><cell>30% 47%</cell><cell>10% 9%</cell><cell>57% 41%</cell><cell>15% 40%</cell><cell>22% 48%</cell><cell>4% 0%</cell><cell>31% 30%</cell><cell>5% 5%</cell><cell>22% 59%</cell><cell>4% 0%</cell></row><row><cell></cell><cell>I</cell><cell>16%</cell><cell>3%</cell><cell>13%</cell><cell>6%</cell><cell>22%</cell><cell>11%</cell><cell>8%</cell><cell>0%</cell><cell>11%</cell><cell>0%</cell><cell>11%</cell><cell>0%</cell><cell>18%</cell><cell>3%</cell><cell>18%</cell><cell>4%</cell><cell>26%</cell><cell>9%</cell><cell>9%</cell><cell>0%</cell><cell>14%</cell><cell>0%</cell><cell>12%</cell><cell>0%</cell></row><row><cell></cell><cell></cell><cell>SO-Google/Custom/Line</cell><cell>SO-API/Custom/Line</cell><cell>SO-Google/Custom/LineBlankId</cell><cell>SO-API/Custom/LineBlankId</cell><cell>SO-Google/Custom/NoSnippet</cell><cell>SO-API/Custom/NoSnippet</cell><cell>SO-Google/Custom/Parent</cell><cell>SO-API/Custom/Parent</cell><cell>SO-Google/Custom/ParentBlankId</cell><cell>SO-API/Custom/ParentBlankId</cell><cell>SO-Google/Custom/ParentXId</cell><cell>SO-API/Custom/ParentXId</cell><cell>SO-Google/Original/Line</cell><cell>SO-API/Original/Line</cell><cell>SO-Google/Original/LineBlankId</cell><cell>SO-API/Original/LineBlankId</cell><cell>SO-Google/Original/NoSnippet</cell><cell>SO-API/Original/LineBlankId</cell><cell>SO-Google/Original/Parent</cell><cell>SO-API/Original/Parent</cell><cell>SO-Google/Original/ParentBlankId</cell><cell>SO-API/Original/ParentBlankId</cell><cell>SO-Google/Original/ParentXId</cell><cell>SO-API/Original/ParentXId</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>IH count of large language model and Stack Overflow results by code source</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Original/FullSnippet</cell><cell cols="4">Original/NoSnippet</cell></row><row><cell></cell><cell></cell><cell cols="2">GPT-4</cell><cell cols="2">GPT-3.5</cell><cell cols="2">GPT-4</cell><cell cols="2">GPT-3.5</cell></row><row><cell>Source</cell><cell cols="9">Ttl. What How What How What How What How</cell></row><row><cell cols="2">blackbox 55</cell><cell>55</cell><cell>55</cell><cell>47</cell><cell>51</cell><cell>44</cell><cell>46</cell><cell>43</cell><cell>44</cell></row><row><cell></cell><cell></cell><cell>(100%)</cell><cell>(100%)</cell><cell>(85%)</cell><cell>(93%)</cell><cell>(80%)</cell><cell>(84%)</cell><cell>(78%)</cell><cell>(80%)</cell></row><row><cell>custom</cell><cell>18</cell><cell>18</cell><cell>18</cell><cell>18</cell><cell>18</cell><cell>16</cell><cell>16</cell><cell>13</cell><cell>15</cell></row><row><cell></cell><cell></cell><cell>(100%)</cell><cell>(100%)</cell><cell>(100%)</cell><cell>(100%)</cell><cell>(89%)</cell><cell>(89%)</cell><cell>(72%)</cell><cell>(83%)</cell></row><row><cell>user_-</cell><cell>27</cell><cell>27</cell><cell>27</cell><cell>22</cell><cell>21</cell><cell>22</cell><cell>22</cell><cell>16</cell><cell>16</cell></row><row><cell>study</cell><cell></cell><cell>(100%)</cell><cell>(100%)</cell><cell>(81%)</cell><cell>(78%)</cell><cell>(81%)</cell><cell>(81%)</cell><cell>(59%)</cell><cell>(59%)</cell></row><row><cell></cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>87</cell><cell>90</cell><cell>82</cell><cell>84</cell><cell>72</cell><cell>75</cell></row><row><cell></cell><cell></cell><cell>(100%)</cell><cell>(100%)</cell><cell>(87%)</cell><cell>(90%)</cell><cell>(82%)</cell><cell>(84%)</cell><cell>(72%)</cell><cell>(75%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://chat.openai.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/patwdj/java-compiler-error-help</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Why your google search results might differ from other people</title>
		<ptr target="https://support.google.com/websearch/answer/12412910?hl=en" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://api.stackexchange.com/docs/search" />
		<title level="m">Usage of /search get</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Tree-sitter -Introduction -tree-sitter</title>
		<ptr target="https://tree-sitter.github.io/tree-sitter/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Common Java Errors -cs-people</title>
		<ptr target="https://cs-people.bu.edu/dgs/courses/cs111-old/assignments/errors.html" />
	</analytic>
	<monogr>
		<title level="j">CS</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Synfix: Automatically fixing syntax errors using compiler diagnostics</title>
		<author>
			<persName><forename type="first">Toufique</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">Rose</forename><surname>Ledesma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14671</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Do developers read compiler error messages?</title>
		<author>
			<persName><forename type="first">Titus</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lubick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emerson</forename><surname>Murphy-Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Parnin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSE.2017.59</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="575" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Facilitating crowd sourced software engineering via stack overflow. Finding Source Code on the Web for Remix and Reuse</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Treude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Zagalsky</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4614-6596-61_5</idno>
		<imprint>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An effective approach to enhancing compiler error messages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><surname>Becker</surname></persName>
		</author>
		<idno type="DOI">10.1145/2839509.2844584</idno>
		<ptr target="https://doi.org/10.1145/2839509.2844584" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th ACM Technical Symposium on Computing Science Education, SIGCSE &apos;16</title>
		<meeting>the 47th ACM Technical Symposium on Computing Science Education, SIGCSE &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="126" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analyze this! 145 questions for data scientists in software engineering</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Begel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
		</author>
		<idno type="DOI">10.1145/2568225.2568233</idno>
		<ptr target="https://doi.org/10.1145/2568225.2568233" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Software Engineering, ICSE 2014</title>
		<meeting>the 36th International Conference on Software Engineering, ICSE 2014<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="12" to="23" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery. ISBN 9781450327565</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Compile and runtime errors in java</title>
		<author>
			<persName><forename type="first">Mordechai</forename><surname>Moti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben-Ari</forename></persName>
		</author>
		<ptr target="https://introcs.cs.princeton.edu/java/11cheatsheet/errors.pdf" />
		<imprint>
			<date type="published" when="2007-01">January 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Role of chatgpt in computer programming.: Chatgpt in computer programming</title>
		<author>
			<persName><forename type="first">Som</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mesopotamian Journal of Computer Science</title>
		<imprint>
			<biblScope unit="page" from="8" to="16" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building reputation in stackoverflow: An empirical investigation</title>
		<author>
			<persName><forename type="first">Amiangshu</forename><surname>Bosu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">S</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Heaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debarshi</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Carver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Kraft</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSR.2013.6624013</idno>
	</analytic>
	<monogr>
		<title level="m">2013 10th Working Conference on Mining Software Repositories (MSR)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="89" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Blackbox: A large scale repository of novice programmers&apos; activity. SIGCSE &apos;14</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kölling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davin</forename><surname>Mccall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Utting</surname></persName>
		</author>
		<idno type="DOI">10.1145/2538862.2538924</idno>
		<ptr target="https://doi.org/10.1145/2538862.2538924" />
	</analytic>
	<monogr>
		<title level="j">ISBN 9781450326056</title>
		<imprint>
			<biblScope unit="page" from="223" to="228" />
			<date type="published" when="2014">2014</date>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Automated query reformulation for efficient search based on query logs from stack overflow</title>
		<author>
			<persName><forename type="first">Kaibo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Baltes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Treude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.00826" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Finding help with programming errors: An exploratory study of novice software engineers&apos; focus in stack overflow posts</title>
		<author>
			<persName><forename type="first">Preetha</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minji</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Pollock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">10 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards correlating search on google and asking on stack overflow</title>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.1109/COMPSAC.2016.210</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Studying programming, page 68</title>
		<author>
			<persName><forename type="first">Sally</forename><surname>Fincher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Bloomsbury Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving the retrieval of related questions in stackoverflow</title>
		<author>
			<persName><forename type="first">Rezvan</forename><surname>Ghaderi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stackintheflow: Behavior-driven recommendation system for stack overflow posts</title>
		<author>
			<persName><forename type="first">Chase</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Haden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostadin</forename><surname>Damevski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepfix: Fixing common c language errors by deep learning</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirish</forename><surname>Shevade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for syntactic error repair in student programs</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirish</forename><surname>Shevade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="930" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">i think this is the most disruptive technology&quot;: Exploring sentiments of chatgpt early adopters using twitter data</title>
		<author>
			<persName><forename type="first">Mubin</forename><surname>Ul Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isuru</forename><surname>Dharmadasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarrin</forename><surname>Tasnim Sworna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Namal Rajapakse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussain</forename><surname>Ahmad</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2212.05856" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Assieme: Finding and leveraging implicit references in a web search interface for programmers</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.1145/1294211.1294216</idno>
		<imprint>
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page">2007</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Googling for software development: What developers search for and what they find</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Hora</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSR52588.2021.00044</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="317" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Identifying and correcting java programming errors for introductory computer science students</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Hristova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Rutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Mercuri</surname></persName>
		</author>
		<idno type="DOI">10.1145/611892.611956</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">2003</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gene set summarization using large language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Harry</forename><surname>Joachimiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nomi</forename><forename type="middle">L</forename><surname>Caufield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeongsik</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Mungall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Personal Opinion Surveys</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shari</forename><surname>Pfleeger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-84800-044-5_3</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="63" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Juho</forename><surname>Leinonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arto</forename><surname>Hellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Sarsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Prather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><forename type="middle">A</forename><surname>Becker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11630</idno>
		<title level="m">Using large language models to enhance programming error messages</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Debugging with stack overflow: Web search behavior in novice and expert programmers</title>
		<author>
			<persName><forename type="first">Annie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeline</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Westley</forename><surname>Weimer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3510456.3514147</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="69" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mining stackoverflow for program repair</title>
		<author>
			<persName><forename type="first">Xuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="DOI">10.1109/SANER.2018.8330202</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="118" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How practitioners perceive the relevance of software engineering research</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nachiappan</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
		</author>
		<idno type="DOI">10.1145/2786805.2786809</idno>
		<ptr target="https://doi.org/10.1145/2786805.2786809" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015</title>
		<meeting>the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="415" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recommending stack overflow posts for fixing runtime exceptions using failure scenario matching</title>
		<author>
			<persName><forename type="first">Negarsadat</forename><surname>Sonal Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukul</forename><surname>Abolhassani</surname></persName>
		</author>
		<author>
			<persName><surname>Prasad</surname></persName>
		</author>
		<idno type="DOI">10.1145/3368089.3409764</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Design lessons from the fastest q&amp;a site in the west</title>
		<author>
			<persName><forename type="first">Lena</forename><surname>Mamykina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bella</forename><surname>Manoim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manas</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Hripcsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Hartmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/1978942.1979366</idno>
		<ptr target="https://doi.org/10.1145/1978942.1979366" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;11</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2857" to="2866" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Meaningful categorisation of novice programmer errors</title>
		<author>
			<persName><forename type="first">Davin</forename><surname>Mccall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kölling</surname></persName>
		</author>
		<idno type="DOI">10.1109/FIE.2014.7044420</idno>
		<imprint>
			<date type="published" when="2014">10 2014</date>
			<biblScope unit="volume">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Interrater reliability: The kappa statistic. Biochemia medica : časopis Hrvatskoga društva medicinskih biokemičara / HDMB</title>
		<author>
			<persName><forename type="first">Mary</forename><surname>Mchugh</surname></persName>
		</author>
		<idno type="DOI">10.11613/BM.2012.031</idno>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Debugging with the Crowd: a Debug Recommendation System based on Stackoverflow</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Monperrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Maia</surname></persName>
		</author>
		<idno>hal-00987395</idno>
		<ptr target="https://hal.archives-ouvertes.fr/hal-00987395" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Université Lille 1 -Sciences et Technologies</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ditran-a compiler emphasizing diagnostics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Moulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Muller</surname></persName>
		</author>
		<idno type="DOI">10.1145/363018.363060</idno>
		<ptr target="https://doi.org/10.1145/363018.363060" />
		<imprint>
			<date type="published" when="1967-01">jan 1967</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Optimizing language models for dialogue</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><surname>Chatgpt</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/chatgpt/" />
		<imprint>
			<date type="published" when="2023-01">Jan 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A gaze-based exploratory study on the information seeking behavior of developers on stack overflow</title>
		<author>
			<persName><forename type="first">Cole</forename><forename type="middle">S</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">A</forename><surname>Saddler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><forename type="middle">M</forename><surname>Halavick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonita</forename><surname>Sharif</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290607.3312801</idno>
		<ptr target="https://doi.org/10.1145/3290607.3312801" />
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems, CHI EA &apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery. ISBN 9781450359719</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Java Programming Exercises with Solutions -Practice Online -code-exercises</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Pimenta</surname></persName>
		</author>
		<ptr target="https://code-exercises.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evaluating how developers use general-purpose web-search for code retrieval</title>
		<author>
			<persName><forename type="first">Jed</forename><surname>Md Masudur Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sydney</forename><surname>Barson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Kayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastián</forename><forename type="middle">Fernandez</forename><surname>Andrés Lois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Quezada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">T</forename><surname>Parnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Stolee</surname></persName>
		</author>
		<author>
			<persName><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Mining Software Repositories (MSR)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="465" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How developers search for code: A case study</title>
		<author>
			<persName><forename type="first">Caitlin</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">T</forename><surname>Stolee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Elbaum</surname></persName>
		</author>
		<idno type="DOI">10.1145/2786805.2786855</idno>
		<ptr target="https://doi.org/10.1145/2786805.2786855" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015</title>
		<meeting>the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="191" to="201" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery. ISBN 9781450336758</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Syntax and sensibility: Using language models to detect and correct syntax errors</title>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santos</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">Charles</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhvani</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abram</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><forename type="middle">Nelson</forename><surname>Amaral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="311" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Designing computer system messages</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Shneiderman</surname></persName>
		</author>
		<idno type="DOI">10.1145/358628.358639</idno>
		<ptr target="https://doi.org/10.1145/358628.358639" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="610" to="611" />
			<date type="published" when="1982-09">sep 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">An analysis of the automatic bug fixing performance of chatgpt</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Sobania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Briesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justyna</forename><surname>Petke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.08653</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Java software errors: How to avoid 50 code issues in java</title>
		<author>
			<persName><surname>Stackify</surname></persName>
		</author>
		<ptr target="https://stackify.com/top-java-software-errors/" />
		<imprint>
			<date type="published" when="2023-03">Mar 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Contrastive learning reduces hallucination in conversations</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengliang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2212.10400" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Enhancing Python compiler error messages via Stack Overflow</title>
		<author>
			<persName><forename type="first">Emillie</forename><surname>Thiselton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Treude</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Empirical Software Engineering and Measurement</title>
		<meeting>the International Symposium on Empirical Software Engineering and Measurement</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">On compiler error messages: What they say and what they mean</title>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Traver</surname></persName>
		</author>
		<idno type="DOI">10.1155/2010/602570</idno>
		<ptr target="https://doi.org/10.1155/2010/602570" />
		<imprint>
			<date type="published" when="2010-01">2010. jan 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wand</surname></persName>
		</author>
		<idno type="DOI">10.1145/512644.512648</idno>
		<ptr target="https://doi.org/10.1145/512644.512648" />
		<title level="m">Finding the source of type errors. POPL &apos;86</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Bluefix: Using crowdsourced feedback to support programming students in error diagnosis and repair</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Godwin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33642-3_25</idno>
		<imprint>
			<date type="published" when="2012-09">09 2012</date>
			<biblScope unit="volume">7558</biblScope>
			<biblScope unit="page" from="228" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A similarity measure for indefinite rankings</title>
		<author>
			<persName><forename type="first">William</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
		<idno type="DOI">10.1145/1852102.1852106</idno>
		<ptr target="https://doi.org/10.1145/1852102.1852106" />
		<imprint>
			<date type="published" when="2010-11">nov 2010</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Maxims for malfeasant designers, or how to design languages to make programming as difficult as possible. ICSE &apos;76</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Wexelblat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>IEEE Computer Society Press</publisher>
			<biblScope unit="page" from="331" to="336" />
			<pubPlace>Washington, DC, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">What do developers search for on the web?</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavneet</forename><surname>Singh Kochhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">E</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10664-017-9514-4</idno>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
