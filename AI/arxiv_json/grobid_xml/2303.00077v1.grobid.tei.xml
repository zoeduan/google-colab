<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-02-28">28 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Conor</forename><surname>Houghton</surname></persName>
							<email>conor.houghton@bristol.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Nina</forename><surname>Kazanina</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Priyanka</forename><surname>Sukumaran</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">†Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">‡School of Psychological Sciences</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">¶International Laboratory of Social Neurobiology</orgName>
								<orgName type="department" key="dep2">Institute for Cognitive Neuroscience</orgName>
								<orgName type="institution" key="instit1">National Research University Higher School of Economics</orgName>
								<orgName type="institution" key="instit2">HSE University</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-28">28 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">27B080BAE641DEA118413625A72993F1</idno>
					<idno type="arXiv">arXiv:2303.00077v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models are not detailed models of human linguistic processing. They are, however, extremely successful at their primary task: providing a model for language. For this reason and because there are no animal models for language, large language models are important in psycholinguistics: they are useful as a practical tool, as an illustrative comparative, and philosophically, as a basis for recasting the relationship between language and thought.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This is a commentary on Bowers et al. (2022).</head><p>Neural-network models of language are optimized to solve practical problems such as machine translation. Currently, when these large language models (LLMs) are interpreted as models of human linguistic processing they have similar shortcomings to those that deep neural network have as models of human vision. Two examples can illustrate this. First, LLMs do not faithfully replicate human behaviour on language tasks <ref type="bibr" target="#b14">(Marvin and Linzen, 2018;</ref><ref type="bibr" target="#b10">Kuncoro et al., 2018;</ref><ref type="bibr" target="#b12">Linzen and Leonard, 2018;</ref><ref type="bibr" target="#b15">Mitchell et al., 2019)</ref>. For example, an LLM trained on a wordprediction task shows similar error rates to humans overall on long-range subject-verb number agreement but errs in different circumstances: unlike humans, it makes more mistakes when sentences have relative clauses <ref type="bibr" target="#b12">(Linzen and Leonard, 2018)</ref>, indicating differences in how grammatical structure is represented. Second, the LLMs with better performance on language tasks do not necessarily have more in common with human linguistic processing or more obvious similarities to the brain. For example, Transformers learn efficiently on vast corpora and avoid human-like memory constraints but are currently more successful as language models than recurrent neural networks such as the Long-Short-Term-Memory LLMs <ref type="bibr" target="#b5">(Devlin et al., 2018;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>, which employ sequential processing, as humans do, and can be more easily compared to the brain.</p><p>Furthermore, the target article suggests that, more broadly, the brain and neural networks are unlikely to resemble each other because evolution differs in trajectory and outcome from the optimization used to train a neural network. Generally, there is an unanswered question about which aspects of learning in LLMs are to be compared to the evolution of our linguistic ability and which to language learning in infants but in either case, the comparison seems weak. LMMs are typically trained using a next-word prediction task; it is unlikely our linguistic ability evolved to optimize this and next-word prediction can only partly describe language learning: for example, infants generalize word meanings based on shape <ref type="bibr" target="#b11">(Landau et al., 1988)</ref> while LLMs lack any broad conceptual encounter with the world language describes.</p><p>In fact, it would be peculiar to suggest that LLMs are models of the neural dynamics that support linguistic processing in humans; we simply know too little about those dynamics. The challenge presented by language is different to that presented by vision: language lacks animal models and debate in psycholinguistics is occupied with broad issues of mechanisms and principles, whereas visual neuroscience often has more detailed concerns. We believe that LLMs have a valuable role in psycholinguistics and this does not depend on any precise mapping from machine to human. Here we describe three uses of LLMs: (1) the practical, as a tool in experimentation; (2) the comparative, as an alternate example of linguistic processing and (3) the philosophical, recasting the relationship between language and thought.</p><p>(1): An LLM models language and this is often of practical quantitative utility in experiment. One straight-forward example is the evaluation of surprisal: how well a word is predicted by what has preceded it. It has been established that reaction times, <ref type="bibr" target="#b6">(Fischler and Bloom, 1979;</ref><ref type="bibr" target="#b9">Kleiman, 1980)</ref>, gaze duration, <ref type="bibr" target="#b16">(Rayner and Well, 1996)</ref>, and EEG responses, <ref type="bibr" target="#b4">(Dambacher et al., 2006;</ref><ref type="bibr" target="#b7">Frank et al., 2015)</ref>, are modulated by surprisal, giving an insight into prediction in neural processing. In the past, surprisal was evaluated using n-grams, but n-grams become impossible to estimate as n grows and as such they cannot quantify long-range dependencies. LLMs are typically trained on a task akin to quantifying surprisal and are superior to n-grams in estimating word probabilities. Differences between LLM-derived estimates and neural perception of surprisal may quantify which linguistic structures, perhaps poorly represented in the statistical evidence, the brain privileges during processing.</p><p>(2): LLMs are also useful as a point of comparison. LLMs combine different computational strategies, mixing representations of word properties with a computational engine based on memory or attention. Despite the clear differences between LLMs and the brain, it is instructive to compare the performance of different LLMs on language tasks to our own language ability. For example, although LLMs are capable of long range number and gender agreement, <ref type="bibr" target="#b13">(Linzen et al., 2016;</ref><ref type="bibr" target="#b8">Gulordava et al., 2018;</ref><ref type="bibr" target="#b0">Bernardy and Lappin, 2017;</ref><ref type="bibr" target="#b17">Sukumaran et al., 2022)</ref>, they are not successful in implementing another long-range rule: Principle C, <ref type="bibr" target="#b15">(Mitchell et al., 2019)</ref>, a near-universal property of languages which depends in its most straight-forward description on hierarchical parsing. Thus, LLMs allow us to recognize those aspects of language which require special consideration while revealing others to be within easy reach of statistical learning.</p><p>(3): In the past, philosophical significance was granted to language as evidence of thought or personhood. <ref type="bibr" target="#b18">Turing (1950)</ref>, for example, proposes conversation as a proxy for thought and <ref type="bibr" target="#b3">Chomsky (1966)</ref> describes Descartes as attributing the possession of mind to other humans because the human capacity for innovation and for the creative use of language, is 'beyond the limitations of any imaginable mechanism'. It is significant that machines are now capable of imitating the use of language. While machine-generated text still has attributes of awkwardness and repetition that make it recognizable on careful reading, it would seem foolhardy to predict these final quirks are unresolvable or are characteristic of the division between human and machine. Nonetheless, most of us appear to feel intuitively that LLMs enact an imitation rather than a recreation of our linguistic ability: LLMs seem empty things whose pantomime of language is not underpinned by thought, understanding or creativity. Indeed, even if an LLM were capable of imitating us perfectly, we would still distinguish between a loved one and their simulation. This is a challenge to our understanding of the relationship between language and thought: either we must claim that, despite recent progress, machine-generated language will remain unlike human language in vital respects, or we must defy our intuition and consider machines to be as capable of thought as we are, or we must codify our intuition to specify why a machine able to produce language should, nonetheless, be considered lacking in thought.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using deep neural networks to learn syntactic agreement</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bernardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lappin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Issues in Language Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep problems with neural network models of human vision</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dujmović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Biscione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Puebla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Adolfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Heaton</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X22002813</idno>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="74" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<title level="m">Cartesian linguistics: A chapter in the history of rationalist thought</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Frequency and predictability effects on event-related potentials during reading</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dambacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Research</title>
		<imprint>
			<biblScope unit="volume">1084</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="103" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic and attentional processes in the effects of sentence contexts on word recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Verbal Learning and Verbal Behavior</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The ERP response to the amount of information conveyed by words in sentences</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Otten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Galli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Gulordava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11138</idno>
		<title level="m">Colorless green recurrent networks dream hierarchically</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentence frame contexts and lexical decisions: Sentence-acceptability and wordrelatedness effects</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Kleiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="336" to="344" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The perils of natural behaviour tests for unnatural models: the case of number agreement. Poster presented at Learning Language in Humans and in Machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fr</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018-07">July. 2018</date>
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The importance of shape in early lexical learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Development</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="321" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distinct patterns of syntactic agreement errors in recurrent networks and humans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leonard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06882</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</title>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Targeted syntactic evaluation of language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09031</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Do LSTMs know about Principle C?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kazanina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Houghton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bowers</surname></persName>
		</author>
		<idno type="DOI">10.32470/CCN.2019.1241-0</idno>
		<idno>32470/CCN.2019.1241-0</idno>
	</analytic>
	<monogr>
		<title level="m">2019 Conference on Cognitive Computational Neuroscience</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effects of contextual constraint on eye movements in reading: A further examination</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rayner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Well</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="504" to="509" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Do LSTMs see gender? Probing the ability of LSTMs to learn abstract syntactic rules</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sukumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Houghton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kazanina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.00153</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Computing machinery and intelligence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Turing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="433" to="460" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
